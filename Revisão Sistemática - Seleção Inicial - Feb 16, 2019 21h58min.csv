"name","abstract","authors","year","base","analysis","review"
"Refactoring edit history of source code","This paper proposes a concept for refactoring an edit history of source code and a technique for its automation. The aim of our history refactoring is to improve the clarity and usefulness of the history without changing its overall effect. We have defined primitive history refactorings including their preconditions and procedures, and large refactorings composed of these primitives. Moreover, we have implemented a supporting tool that automates the application of history refactorings in the middle of a source code editing process. Our tool enables developers to pursue some useful applications using history refactorings such as task level commit from an entangled edit history and selective undo of past edit operations.","S. Hayashi and T. Omori and T. Zenmyo and K. Maruyama and M. Saeki",2012,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA1, CA3","Aceito: CA1, CA3"
"Supporting Merge Conflict Resolution by Using Fine-Grained Code Change History","Modern version control systems facilitate concurrent work in software development by providing a mechanism to merge revisions that are independently modified by multiple programmers. However, merge conflicts might emerge due to concurrent modifications, and their resolution might require the programmers to scrutinize every modification in the revisions. This paper presents a tool that can alleviate this cumbersome task of merging conflicting revisions. Our tool exploits the fine-grained edit operation history of Java source code and extracts only the edit operations that affect the revision of a particular class member. By just replaying the extracted edit operations, it helps programmers detect merge conflicts between class members within the two revisions and understand the modifications of the conflicting class members. Moreover, it can artificially merge two snapshots that appear during the evolution of the two revisions and show the programmers a unique artificial snapshot that is consistent with both the merged snapshots. By replaying the fine-grained edits that cause merge conflicts and showing the apparent snapshots with no conflicts as hints of the merged revision, the tool reduces the burden of inspecting the code changes behind the conflicts and reconciling the conflicting revisions.","Y. Nishimura and K. Maruyama",2016,"[""IEEE""]","Aceito: CA3, CA7","Aceito: CA3"
"Integrating source code search into git client for effective retrieving of change history","In order to achieve effective development management, it is important to manipulate and understand the change histories of source code in a repository. Although general version control systems provide change history manipulation, these systems are restricted to line-based and textual operations such as grep and diff. As such, these systems cannot follow the syntax/semantics of the source code. While various studies have examined querying and searching source codes, these methods cannot follow historical changes. The key concept of this paper is the integration of a source code search technique into Git commands that manipulate historical data in a repository. This paper presents MJgit, a prototype tool for achieving the above goal. In order to evaluate the proposed tool, we conducted a performance experiment using actual software repositories.","M. Sasaki and S. Matsumoto and S. Kusumoto",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR11"
"Detecting Program Changes from Edit History of Source Code","Detecting program changes helps maintainers to figure out the evolution of the changed program. For this, several line-based difference tools have been proposed, which extract differences between two versions of the program. Unfortunately, these tools do not provide enough support to program comprehension since a single commitment stored in a version control system contains multiple changes that are intermingled with each other. Therefore, the maintainers have to untangle them by hand. This work is troublesome and time-consuming. This paper proposes a novel mechanism that automatically detects individual program changes. For this, it restores snapshots of the program from the history of edit operations for the target source code and compares class members that result from syntax analysis for respective snapshots. In addition, the mechanism provides several options of aggregating fine-grained changes detected based on the edit history. The maintainers can select their suitable levels of summarization of program changes. The paper also shows experimental results with a running implementation of the change detection tool. Through the experiment, the detection mechanism presents various kinds of summarized information on program changes, which might facilitate maintainers' activities for program comprehension.","E. Kitsu and T. Omori and K. Maruyama",2013,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA1, CA2, CA3, CA7","Aceito: CA1, CA2, CA3"
"Detecting bad smells in source code using change history information","Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately. We propose an approach to detect five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy, by exploiting change history information mined from versioning systems. We applied approach, coined as HIST (Historical Information for Smell deTection), to eight software projects written in Java, and wherever possible compared with existing state-of-the-art smell detectors based on source code analysis. The results indicate that HIST's precision ranges between 61% and 80%, and its recall ranges between 61% and 100%. More importantly, the results confirm that HIST is able to identify code smells that cannot be identified through approaches solely based on code analysis.","F. Palomba and G. Bavota and M. Di Penta and R. Oliveto and A. De Lucia and D. Poshyvanyk",2013,"[""IEEE"",""ACM"",""Engineering Village""]","Aceito: CA4","Aceito: CA4"
"Historef: A tool for edit history refactoring","This paper presents Historef, a tool for automating edit history refactoring on Eclipse IDE for Java programs. The aim of our history refactorings is to improve the understandability and/or usability of the history without changing its whole effect. Historef enables us to apply history refactorings to the recorded edit history in the middle of the source code editing process by a developer. By using our integrated tool, developers can commit the refactored edits into underlying SCM repository after applying edit history refactorings so that they are easy to manage their changes based on the performed edits.","S. Hayashi and D. Hoshino and J. Matsuda and M. Saeki and T. Omori and K. Maruyama",2015,"[""IEEE"",""Engineering Village""]","Aceito: CA0","Aceito: CA7"
"ChangeChecker: A tool for defect prediction in source code changes based on incremental learning method","In software development process, software developers may introduce defects as they make changes to software projects. Being aware of introduced defects immediately upon the completion of the change would allow software developers or testers to allocate more resources of testing and inspecting on the current risky change timely, which can shorten the process of defect finding and fixing effectively. In this paper, we propose a software tool called ChangeChecker to help software developers predict whether current source code change has any defects or not during the software development process. This tool infers the existence of defect by dynamically mining patterns of the source code changes in the revision history of the software project. It mainly consists of three components: (1) incremental feature collection and transformation, (2) real-time defect prediction for source code changes, and (3) dynamic update of the learning model. The tool has been evaluated in a large famous open source project Eclipse and applied to a real software development scenario.","Z. Yuan and C. Liu and L. Yu and L. Zhang",2013,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA1, CA2, CA5, CA7","Aceito: CA7, CA3"
"NESC Handbook, Premier Edition – Spanish version - A Discussion of the National Electrical Safety Code(R) plus COMENTARIOS EN ESPAÑOL","","",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR0, CR9"
"Linking Source Code to Untangled Change Intents","Previous work [13] suggests that tangled changes (i.e., different change intents aggregated in one single commit message) could complicate tracing to different change tasks when developers manage software changes. Identifying links from changed source code to untangled change intents could help developers solve this problem. Manually identifying such links requires lots of experience and review efforts, however. Unfortunately, there is no automatic method that provides this capability. In this paper, we propose AutoCILink, which automatically identifies code to untangled change intent links with a pattern-based link identification system (AutoCILink-P) and a supervised learning-based link classification system (AutoCILink-ML). Evaluation results demonstrate the effectiveness of both systems: the pattern-based AutoCILink-P and the supervised learning-based AutoCILink-ML achieve average accuracy of 74.6% and 81.2%, respectively.","X. Liu and L. Huang and C. Li and V. Ng",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Using source code metrics to predict change-prone web services: A case-study on ebay services","Predicting change-prone object-oriented software using source code metrics is an area that has attracted several researchers attention. However, predicting change-prone web services in terms of changes in the WSDL (Web Service Description Language) Interface using source code metrics implementing the services is a relatively unexplored area. We conduct a case-study on change proneness prediction on an experimental dataset consisting of several versions of eBay web services wherein we compute the churn between different versions of the WSDL interfaces using the WSDLDiff Tool. We compute 21 source code metrics using Chidamber and Kemerer Java Metrics (CKJM) extended tool serving as predictors and apply Least Squares Support Vector Machines (LSSVM) based technique to develop a change proneness estimator. Our experimental results demonstrates that a predictive model developed using all 21 metrics and linear kernel yields the best results.","L. Kumar and S. K. Rath and A. Sureka",2017,"[""IEEE"",""Engineering Village""]","Aceito: CA6, CA4","Aceito: CA4"
"Exploring the Effects of History Length and Age on Mining Software Change Impact","The goal of Software Change Impact Analysis is to identify artifacts (typically source-code files) potentially affected by a change. Recently, there is an increased interest in mining software change impact based on evolutionary coupling. A particularly promising approach uses association rule mining to uncover potentially affected artifacts from patterns in the system's change history. Two main considerations when using this approach are the history length, the number of transactions from the change history used to identify the impact of a change, and history age, the number of transactions that have occurred since patterns were last mined from the history. Although history length and age can significantly affect the quality of mining results, few guidelines exist on how to best select appropriate values for these two parameters. In this paper, we empirically investigate the effects of history length and age on the quality of change impact analysis using mined evolutionary couplings. Specifically, we report on a series of systematic experiments involving the change histories of two large industrial systems and 17 large open source systems. In these experiments, we vary the length and age of the history used to mine software change impact, and assess how this affects precision and applicability. Results from the study are used to derive practical guidelines for choosing history length and age when applying association rule mining to conduct software change impact analysis.","L. Moonen and S. D. Alesio and T. Rolfsnes and D. W. Binkley",2016,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA3","Aceito: CA3"
"Move-optimized source code tree differencing","When it is necessary to express changes between two source code files as a list of edit actions (an edit script), modern tree differencing algorithms are superior to most text-based approaches because they take code movements into account and express source code changes more accurately. We present 5 general optimizations that can be added to state-of-the-art tree differencing algorithms to shorten the resulting edit scripts. Applied to Gumtree, RTED, JSync, and ChangeDistiller, they lead to shorter scripts for 1898% of the changes in the histories of 9 open-source software repositories. These optimizations also are parts of our novel Move-optimized Tree DIFFerencing algorithm (MTD-IFF) that has a higher accuracy in detecting moved code parts. MTDIFF (which is based on the ideas of ChangeDistiller) further shortens the edit script for another 20% of the changes in the repositories. MTDIFF and all the benchmarks are available under an open-source license.","G. Dotzler and M. Philippsen",2016,"[""IEEE"",""ACM""]","Aceito: CA1, CA3","Aceito: CA1"
"Automatic Matching Release Notes and Source Code by Generating Summary for Software Change","To quickly locate the source code that maps to a specific change described in change history, establishing traceability links between release notes and source code is a necessary task. Current works on the traceability link recovery can be used to find out source code changes which are of higher textual similarities with the release note. However, these approaches rely on consistency of the text used in artifacts at various abstraction levels, and the completeness of text descriptions. In this paper, we propose to leverage source code change information for improving the accuracy of release note to source code traceability recovery tasks. In order to reduce the complexity of link recovery, our approach first performs change impact analysis to cluster the source code changes for the same purpose as a virtual class. After that, our approach employs a natural language generation algorithm to generate readable summary sentence for each virtual class. The traceability links are built between release notes and clusters of program entities by computing the linguistic similarity of sentences. We conduct case studies on 26 releases of 3 popular softwares to evaluate the approach, and the results indicate that our proposed method can improve the accuracy of traceability link recovery compared to other IR-based techniques.","Y. Huang and Z. Liu and X. Chen and X. Luo",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Automated Generalization and Refinement of Code Templates with Ekeko/X","Code templates are an intuitive means to specify source code snippets of interest, such as all instances of a bug, groups of snippets that need to be refactored or transformed, or instances of design patterns. While intuitive, it is not always straightforward to write a template that produces only the desired matches. A template could produce either more snippets than desired, or too few. To assist the users of EKEKO/X, our template-based search and transformation tool for Java, we have extended it with two components: The first is a suite of mutation operators that simplifies the process of modifying templates. The second is a system that can automatically suggest a sequence of mutations to a given template, such that it matches only with a set of desired snippets. In this tool paper, we highlight the key design decisions in implementing these two components of EKEKO/X, and demonstrate their use by walking through an example sequence of mutations suggested by the system.","T. Molderez and C. De Roover",2016,"[""IEEE""]","Aceito: CA3","Aceito: CA3"
"Using Version Control History to Follow the Changes of Source Code Elements","Version control systems store the whole history of the source code. Since the source code of a system is organized into files and folders, the history tells us the concerned files and their changed lines only but software engineers are also interested in which source code elements (e.g. classes or methods) are affected by a change. Unfortunately, in most programming languages source code elements do not follow the file system hierarchy, which means that a file can contain more classes and methods and a class can be stored in more files, which makes it difficult to determine the changes of classes by using the changes of files. To solve this problem we developed an algorithm, which is able to follow the changes of the source code elements by using the changes of files and we successfully applied it on the Web Kit open source system.","Z. Tóth and G. Novák and R. Ferenc and I. Siket",2013,"[""IEEE"",""Engineering Village""]","Aceito: CA1, CA3","Aceito: CA1, CA3"
"Chronos: Visualizing slices of source-code history","In this paper, we present CHRONOS-a tool that enables the querying, exploration, and discovery of historical change events to source code. Unlike traditional Revision-Control-System tools, CHRONOS allows queries across any subset of the code, down to the line-level, which can potentially be contiguous or disparate, even among multiple files. In addition, CHRONOS provides change history across all historical versions (i.e., it is not limited to a pairwise “diff”). The tool implements a zoom-able user interface as a visualization of the history of the queried code to provide both a high-level view of the changes, which supports pattern recognition and discovery, and a low-level view that supports semantic comprehension for tasks such as reverse engineering and identifying design rationale. In this paper, we describe use cases in which CHRONOS may be helpful, provide a motivating example to demonstrate the benefits brought by CHRONOS, and describe its visualization in detail.","F. Servant and J. A. Jones",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Enriching API Documentation by Relevant API Methods Recommendation Based on Version History","Application programming interfaces (APIs) enable developers to increase their productivity; however, developers have to expend much of their time to search API usages because API documentation often lacks some types of information such as co-use relationships between API methods or code examples. In this paper, we propose a technique to automatically generate enriched API documentation. Our technique identifies groups of relevant API methods by analyzing a software change history and inserts those into standard API documentation. Using our document, developers can easily find relevant API methods interactively according to their various development purposes.","Y. Arimatsu and Y. Ishida and K. Noda and T. Kobayashi",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11, CR10","Rejeitado: CR11, CR10, CR8"
"An Empirical Study on the Characteristics of Python Fine-Grained Source Code Change Types","Software has been changing during its whole life cycle. Therefore, identification of source code changes becomes a key issue in software evolution analysis. However, few current change analysis research focus on dynamic language software. In this paper, we pay attention to the fine-grained source code changes of Python software. We implement an automatic tool named PyCT to extract 77 kinds of fine-grained source code change types from commit history information. We conduct an empirical study on ten popular Python projects from five domains, with 132294 commits, to investigate the characteristics of dynamic software source code changes. Analyzing the source code changes in four aspects, we distill 11 findings, which are summarized into two insights on software evolution: change prediction and fault code fix. In addition, we provide direct evidence on how developers use and change dynamic features. Our results provide useful guidance and insights for improving the understanding of source code evolution of dynamic language software.","W. Lin and Z. Chen and W. Ma and L. Chen and L. Xu and B. Xu",2016,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA1, CA3","Aceito: CA1, CA3"
"Detecting and investigating the source code changes using logical rules","Software developers often need to examine program differences between two versions and reason about the changes. Analyzing the changes is the task. To facilitate the programmers to represent the high level source code changes, this proposed system introduces the rule-based program differencing approach to represent the changes as logical rules. This approach is instantiated with three levels: first level describes the changes in method header names and signature; second level captures change in the code level and structural dependences; and third level identifies the same set of function with different name. This approach concisely represents the systematic changes and helps the software engineers to recognize the program differences. This approach can be applied in open source project to examine the difference among program version.","E. Kodhai and B. Dhivya",2014,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA1","Aceito: CA1"
"On Automatically Generating Commit Messages via Summarization of Source Code Changes","Although version control systems allow developers to describe and explain the rationale behind code changes in commit messages, the state of practice indicates that most of the time such commit messages are either very short or even empty. In fact, in a recent study of 23K+ Java projects it has been found that only 10% of the messages are descriptive and over 66% of those messages contained fewer words as compared to a typical English sentence (i.e., 15-20 words). However, accurate and complete commit messages summarizing software changes are important to support a number of development and maintenance tasks. In this paper we present an approach, coined as Change Scribe, which is designed to generate commit messages automatically from change sets. Change Scribe generates natural language commit messages by taking into account commit stereotype, the type of changes (e.g., files rename, changes done only to property files), as well as the impact set of the underlying changes. We evaluated Change Scribe in a survey involving 23 developers in which the participants analyzed automatically generated commit messages from real changes and compared them with commit messages written by the original developers of six open source systems. The results demonstrate that automatically generated messages by Change Scribe are preferred in about 62% of the cases for large commits, and about 54% for small commits.","L. F. Cortés-Coy and M. Linares-Vásquez and J. Aponte and D. Poshyvanyk",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Querying the History of Software Projects Using QWALKEKO","We present the QwalKeko meta-programming library for Clojure that enables querying the history of versioned software projects in a declarative manner. Unique to this library is its support for regular path expressions within history queries. Regular path expressions are akin to regular expressions, except that they match a sequence of successive snapshots of a software project along which user-specified logic conditions must hold. Such logic conditions can concern the source code within a snapshot, versioning information associated with the snapshot, as well as patterns of source code changes with respect to other snapshots. We have successfully used the resulting multi-faceted queries to detect refactorings in project histories. In this paper, we discuss how applicative logic meta-programming enabled combining the heterogenous components of QwalKeko into a uniform whole. We focus on the applicative logic interface to a new implementation of a well-known change distilling algorithm. We use the problem of detecting and categorizing changes made to Selenium-based test scripts for illustration purposes.","R. Stevens and C. D. Roover",2014,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA1","Aceito: CA1, CA3"
"Intent, tests, and release dependencies: Pragmatic recipes for source code integration","Continuous integration of source code changes, for example, via pull-request driven contribution channels, has become standard in many software projects. However, the decision to integrate source code changes into a release is complex and has to be taken by a software manager. In this work, we identify a set of three pragmatic recipes plus variations to support the decision making of integrating code contributions into a release. These recipes cover the isolation of source code changes, contribution of test code, and the linking of commits to issues. We analyze the development history of 21 open-source software projects, to evaluate whether, and to what extent, those recipes are followed in open-source projects. The results of our analysis showed that open-source projects largely follow recipes on a compliance level of &gt; 75%. Hence, we conclude that the identified recipes plus variations can be seen as wide-spread relevant best-practices for source code integration.","M. Brandtner and P. Leitner and H. C. Gall",2015,"[""IEEE""]","Rejeitado: CR11","Rejeitado: CR11"
"Using Eye Gaze Data to Recognize Task-Relevant Source Code Better and More Fine-Grained","Models to assess a source code element's relevancy for a given change task are the basis of many software engineering tools, such as recommender systems, for code comprehension. To improve such relevancy models and to aid developers in finding relevant parts in the source code faster, we studied developer's fine-grained navigation patterns with eye tracking technology. By combining the captured eye gaze data with interaction data of 12 developers working on a change task, we were able to identify relevant methods with high accuracy and improve precision and recall compared to the widely used click frequency technique by 77% and 24% respectively. Furthermore, we were able to show that the captured gaze data enables to retrace which source code lines developers found relevant. Our results thus provide evidence that eye gaze data can be used to improve existing models in terms of accuracy and granularity.","K. Kevic",2017,"[""IEEE""]","Rejeitado: CR8","Rejeitado: CR8"
"Locating Source Code to Be Fixed Based on Initial Bug Reports - A Case Study on the Eclipse Project","In most software development, a Bug Tracking System is used to improve software quality. Based on bug reports managed by the bug tracking system, triagers who assign a bug to fixers and fixers need to pinpoint buggy files that should be fixed. However if triagers do not know the details of the buggy file, it is difficult to select an appropriate fixer. If fixers can identify the buggy files, they can fix the bug in a short time. In this paper, we propose a method to quickly locate the buggy file in a source code repository using 3 approaches, text mining, code mining, and change history mining to rank files that may be causing bugs. (1) The text mining approach ranks files based on the textual similarity between a bug report and source code. (2) The code mining approach ranks files based on prediction of the fault-prone module using source code product metrics. (3) The change history mining approach ranks files based on prediction of the fault-prone module using change process metrics. Using Eclipse platform project data, our proposed model gains around 20% in TOP1 prediction. This result means that the buggy files are ranked first in 20% of bug reports. Furthermore, bug reports that consist of a short description and many specific words easily identify and locate the buggy file.","P. Bangcharoensap and A. Ihara and Y. Kamei and K. Matsumoto",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Triaging incoming change requests: Bug or commit history, or code authorship?","There is a tremendous wealth of code authorship information available in source code. Motivated with the presence of this information, in a number of open source projects, an approach to recommend expert developers to assist with a software change request (e.g., a bug fixes or feature) is presented. It employs a combination of an information retrieval technique and processing of the source code authorship information. The relevant source code files to the textual description of a change request are first located. The authors listed in the header comments in these files are then analyzed to arrive at a ranked list of the most suitable developers. The approach fundamentally differs from its previously reported counterparts, as it does not require software repository mining. Neither does it require training from past bugs/issues, which is often done with sophisticated techniques such as machine learning, nor mining of source code repositories, i.e., commits. An empirical study to evaluate the effectiveness of the approach on three open source systems, ArgoUML, JEdit, and MuCommander, is reported. Our approach is compared with two representative approaches: (1) using machine learning on past bug reports, and (2) based on commit logs. The presented approach is found to provide recommendation accuracies that are equivalent or better than the two compared approaches. These findings are encouraging, as it opens up a promising and orthogonal possibility of recommending developers without the need of any historical change information.","M. Linares-Vásquez and K. Hossen and H. Dang and H. Kagdi and M. Gethers and D. Poshyvanyk",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Slicing and replaying code change history","Change-aware development environments have recently become feasible and reasonable. These environments can automatically record fine-grained code changes on a program and allow programmers to replay the recorded changes in chronological order. However, they do not always need to replay all the code changes to investigate how a particular entity of the program has been changed. Therefore, they often skip several code changes of no interest. This skipping action is an obstacle that makes many programmers hesitate in using existing replaying tools. This paper proposes a slicing mechanism that can extract only code changes necessary to construct a particular class member of a Java program from the whole history of past code changes. In this mechanism, fine-grained code changes are represented by edit operations recorded on source code of a program. The paper also presents a running tool that implements the proposed slicing and replays its resulting slices. With this tool, programmers can avoid replaying edit operations nonessential to the construction of class members they want to understand.","K. Maruyama and E. Kitsu and T. Omori and S. Hayashi",2012,"[""IEEE"",""ACM"",""Engineering Village""]","Aceito: CA0, CA1, CA3","Aceito: CA1, CA3"
"Enriching in-IDE process information with fine-grained source code history","Current studies on software development either focus on the change history of source code from version-control systems or on an analysis of simplistic in-IDE events without context information. Each of these approaches contains valuable information that is unavailable in the other case. Our work proposes enriched event streams, a solution that combines the best of both worlds and provides a holistic view on the software development process. Enriched event streams not only capture developer activities in the IDE, but also specialized context information, such as source-code snapshots for change events. To enable the storage of such code snapshots in an analyzable format, we introduce a new intermediate representation called Simplified Syntax Trees (SSTs) and build CA□RET, a platform that offers reusable components to conveniently work with enriched event streams. We implement FEEDBAG++, an instrumentation for Visual Studio that collects enriched event streams with code snapshots in the form of SSTs. We share a dataset of enriched event streams captured from 58 users and representing 915 days of work. Additionally, to demonstrate usefulness, we present three research applications that have already made use of CA□RET and FEEDBAG++.","S. Proksch and S. Nadi and S. Amann and M. Mezini",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11, CR7","Rejeitado: CR11"
"[Research Paper] The Case for Adaptive Change Recommendation","As the complexity of a software system grows, it becomes increasingly difficult for developers to be aware of all the dependencies that exist between artifacts (e.g., files or methods) of the system. Change impact analysis helps to overcome this problem, as it recommends to a developer relevant source-code artifacts related to her current changes. Association rule mining has shown promise in determining change impact by uncovering relevant patterns in the system's change history. State-of-the-art change impact mining algorithms typically make use of a change history of tens of thousands of transactions. For efficiency, targeted association rule mining focuses on only those transactions potentially relevant to answering a particular query. However, even targeted algorithms must consider the complete set of relevant transactions in the history. This paper presents ATARI, a new adaptive approach to association rule mining that considers a dynamic selection of the relevant transactions. It can be viewed as a further constrained version of targeted association rule mining, in which as few as a single transaction might be considered when determining change impact. Our investigation of adaptive change impact mining empirically studies seven algorithm variants. We show that adaptive algorithms are viable, can be just as applicable as the start-of-the-art complete-history algorithms, and even outperform them for certain queries. However, more important than the direct comparison, our investigation lays necessary groundwork for the future study of adaptive techniques and their application to challenges such as the on-the-fly style of impact analysis that is needed at the GitHub-scale.","S. Pugh and D. Binkley and L. Moonen",2018,"[""IEEE""]","Rejeitado: CR11, CR12","Rejeitado: CR11, CR12"
"Mining A change history to quickly identify bug locations : A case study of the Eclipse project","In this study, we proposed an approach to mine a change history to improve the bug localization performance. The key idea is that a recently fixed file may be fixed in the near future. We used a combination of textual feature and mining the change history to recommend source code files that are likely to be fixed for a given bug report. First, we adopted the Vector Space Model (VSM) to find relevant source code files that are textually similar to the bug report. Second, we analyzed the change history to identify previously fixed files. We then estimated the fault proneness of these files. Finally, we combined the two scores, from textual similarity and fault proneness, for every source code file. We then recommend developers examine source code files with higher scores. We evaluated our approach based on 1,212 bug reports from the Eclipse Platform and Eclipse JDT. The experimental results show that our proposed approach can improve the bug localization performance and effectively identify buggy files.","C. Tantithamthavorn and R. Teekavanich and A. Ihara and K. Matsumoto",2013,"[""IEEE""]","Aceito: CA0, CA3","Aceito: CA3"
"The Ekeko/X Program Transformation Tool","Developers often need to perform repetitive changes to source code. For instance, to repair several instances of a bug or to update all clients of a library to a newer version. Manually performing such changes is laborious and error-prone. Program transformation tools enable automating changes, but specifying changes as a program transformation requires significant expertise. Code templates are often touted as a remedy, yet have never been endorsed wholeheartedly. Their use is mostly limited to expressing the syntactic characteristics of the intended change subjects. Less familiar means have to be resorted to for expressing their structural, control flow, and data flow characteristics. In this tool paper, we introduce a decidedly template-driven program transformation tool called Ekeko/X. Its specifications feature templates for specifying all of the aforementioned characteristics of its subjects. To this end, developers can associate different directives with individual components of a template. Each matching directive imposes particular constraints on the matches for the component it is associated with. Rewriting directives, on the other hand, determine how each match should be changed. We develop Ekeko/X from the ground up, starting from its applicative logic meta-programming foundation. We highlight the key choices in this implementation and demonstrate its use through two example program transformations.","C. De Roover and K. Inoue",2014,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA1, CA3","Aceito: CA1, CA3"
"Code Change History and Software Vulnerabilities","Usually, the most critical modules of the system receive extra attention. But even these modules might be too large to be thoroughly inspected so it is useful to know where to apply the majority of the efforts. Thus, knowing which code changes are more prone to contain vulnerabilities may allow security experts to concentrate on a smaller subset of submitted code changes. In this paper we discuss the change history of functions and its impact on the existence of vulnerabilities. For this, we analyzed the commit history of two software projects widely exposed to attacks (Mozilla and Linux Kernel). Starting from security bugs, we analyzed more than 95k functions (with and without vulnerabilities), and systematized the changes in each function according to a subset of the patterns described in the Orthogonal Defects Classification. The results show that the frequency of changes can allow to distinguish functions more prone to have vulnerabilities.","M. Piancó and B. Fonseca and N. Antunes",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Automatic Algorithm Recognition of Source-Code Using Machine Learning","As codebases for software projects get larger, reaching ranges of millions of lines of code, the need for computer-aided program comprehension grows. We define one of the tasks of program comprehension to be algorithm recognition: given a piece of source-code from a file, identify the algorithm this code is implementing, such as brute-force or dynamic programming. Most research in this area is making use of pattern matching, which involves much human effort and is of questionable accuracy when the structure and semantics of programs change. Thus, this paper proposes to let go of defined patterns, and make use of simpler features, such as counts of variables and counts of different constructs to recognize algorithms. We then feed these features to a classification algorithm to predict the class or type of algorithm used in this source code. We show through experimental results that our proposed method achieves a good improvement over baseline.","M. Shalaby and T. Mehrez and A. El Mougy and K. Abdulnasser and A. Al-Safty",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8"
"Predicting bug inducing source code change patterns","A change in source code without the prior analysis of its impact may generate one or more defects. Fixing of such defects consumes maintenance time which ultimately increases the cost of software maintenance. Therefore, in the recent years, several research works have been done to develop techniques for the automatic impact analysis of changes in source code. In this paper, we propose to use Frequent Pattern Mining (FPM) technique of machine learning for the automatic impact analysis of those changes in source code which may induce bugs. Therefore, to find patterns associated with some specific types of software changes, we applied FPM's algorithms' Apriori and Predictive Apriori on the stored data of software changes of the following three Open-Source Software (OSS) projects: Mozilla, GNOME, and Eclipse. We grouped the data of software changes into two major categories: changes to meet bug fixing requirements and changes to meet requirements other than bug fixing. In the case of bug fixing requirements, we predict source files which are frequently changed together to fix any one of the following four types of bugs related to: memory (MEMORY), variables locking (LOCK), system (SYSTEM) and graphical user interface (UI). Our experimental results predict several interesting software change patterns which may induce bugs. The obtained bug inducing patterns have high confidence and accuracy value i.e., more than 90%.","A. Khan and S. N. Ahsan",2016,"[""IEEE"",""Engineering Village""]","Aceito: CA3, CA1","Aceito: CA1, CA3"
"Using Topic Model to Suggest Fine-Grained Source Code Changes","Prior research has shown that source code and its changes are repetitive. Several approaches have leveraged that phenomenon to detect and recommend change and fix patterns. In this paper, we propose TasC, a model that leverages the context of change tasks in development history to suggest fine-grained code change and fix at the program statement level. We use Latent Dirichlet Allocation (LDA) to capture the change task context via co-occurring program elements in the changes in a context. We also propose a novel technique for measuring the similarity of code fragments and code changes using the task context. We conducted an empirical evaluation on a large dataset of 88 open-source Java projects containing more than 200 thousand source files and 3.5 million source lines of code in their last revisions with 423 thousand changed methods. Our result shows that TasC relatively improves recommendation accuracy up to 130%-250% in comparison with the base models that do not use task context. Compared with other types of contexts, TasC outperforms the models using structural and co-change contexts.","H. A. Nguyen and A. T. Nguyen and T. N. Nguyen",2016,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA2, CA3","Aceito: CA2, CA3"
"A Program Slicing-Based Bayesian Network Model for Change Impact Analysis","Change impact analysis plays an important role in identifying potential affected areas that are caused by changes that are made in a software. Most of the existing change impact analysis techniques are based on architectural design and change history. However, source code-based change impact analysis studies are very few and they have shown higher precision in their results. In this study, a static method-granularity level change impact analysis, that uses program slicing and Bayesian Network technique has been proposed. The technique proposes a directed graph model that also represents the call dependencies between methods. In this study, an open source Java project with 8999 to 9445 lines of code and from 505 to 528 methods have been analyzed through 32 commits it went. Recall and f-measure metrics have been used for evaluation of the precision of the proposed method, where each software commit has been analyzed separately.","E. Ufuktepe and T. Tuglular",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"An automated tool for collection of code attributes for cross project defect prediction","This paper presents a tool that automates the process of data collection for defect or change analysis. Prediction of defects in early phases has become crucial to reduce the efforts and costs incurred due to defects. This tool extracts the information from Git Version Control System of open source projects. Two consecutive versions of one single project have been used by the tool to obtain results. The tool generates a matrix containing code churn (added lines, deleted lines, modified lines, total LoC), complexity, pre-release bugs and post-release bugs of each file of source code. The obtained software metrics can be used to measure the development process of a software and therefore in analysis and prediction purposes.","R. Malhotra and B. Bansal and C. Jain and E. Punia",2017,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA5, CA6","Aceito: CA2, CA5, CA6"
"A study on repetitiveness of code completion operations","In current software development, code completion is necessary to enhance productivity of our programming tasks. However, how developers use code completion tools on integrated development environments is still not elucidated completely. Aiming to improve such tools, we performed an investigation in terms of code completion use. We investigated developers' operation histories on an integrated development environment and found that code completion operations inserting the same text tend to be repetitively performed in a short time period. We also propose new code completion strategies to reduce such repetitive code completion.","T. Omori and H. Kuwabara and K. Maruyama",2012,"[""IEEE""]","Rejeitado: CR8","Rejeitado: CR8"
"Predicting software change-proneness with code smells and class imbalance learning","The objective of this paper is to study the relationship between different types of object-oriented software metrics, code smells and actual changes in software code that occur during maintenance period. It is hypothesized that code smells are indicators of maintenance problems. To understand the relationship between code smells and maintenance problems, we extract code smells in a Java based mobile application called MOBAC. Four versions of MOBAC are studied. Machine learning techniques are applied to predict software change-proneness with code smells as predictor variables. The results of this paper indicate that codes smells are more accurate predictors of change-proneness than static code metrics for all machine learning methods. However, class imbalance techniques did not outperform class balance machine learning techniques in change-proneness prediction. The results of this paper are based on accuracy measures such as F-measure and area under ROC curve.","A. Kaur and K. Kaur and S. Jain",2016,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA4, CA5","Aceito: CA4, CA5"
"Applying Machine Learning to Predict Software Fault Proneness Using Change Metrics, Static Code Metrics, and a Combination of Them","Predicting software fault proneness is very important as the process of fixing these faults after the release is very costly and time-consuming. In order to predict software fault proneness, many machine learning algorithms (e.g., Logistic regression, Naive Bayes, and J48) were used on several datasets, using different metrics as features. The question is what algorithm is the best under which circumstance and what metrics should be applied. Related works suggested that using change metrics leads to the highest accuracy in prediction. In addition, some algorithms perform better than others in certain circumstances. In this work, we use three machine learning algorithms (i.e., logistic regression, Naive Bayes, and J48) on three Eclipse releases (i.e., 2.0, 2.1, 3.0). The results showed that accuracy is slightly better and false positive rates are lower, when we use the reduced set of metrics compared to all change metrics set. However, the recall and the G score are better when we use the complete set of change metrics. Furthermore, J48 outperformed the other classifiers with respect to the G score for the reduced set of change metrics, as well as in almost all cases when the complete set of change metrics, static code metrics, and the combination of both were used.","Y. A. Alshehri and K. Goseva-Popstojanova and D. G. Dzielski and T. Devine",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Development History Granularity Transformations (N)","Development histories can simplify some software engineering tasks, butdifferent tasks require different history granularities. For example, a history that includes every edit that resulted in compiling code is needed when searching for the cause of a regression, whereas a history that contains only changes relevant to a feature is needed for understanding the evolution of the feature. Unfortunately, today, both manual and automated history generation result in a single-granularity history. This paper introduces the concept of multi-grained development history views and the architecture of Codebase Manipulation, a tool that automatically records a fine-grained history and manages its granularity by applying granularity transformations.","K. Muslu and L. Swart and Y. Brun and M. D. Ernst",2015,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12, CR11"
"Enhancing History-Based Concern Mining with Fine-Grained Change Analysis","Maintenance of large software projects is often hindered by cross-cutting concerns scattered over multiple modules. History-based mining techniques have been proposed to mitigate the difficultly by examining changes related to methods/functions in development history to suggest potential concerns. However, the techniques do not cope well with renamed entities and may lead to irrelevant information about concerns. The intricate procedures of the methods also make the results difficult for others to reproduce, utilize or improve. In this paper, we reinforce history-based concern mining techniques with fine-grained change analysis based on tree differencing on abstract syntax trees. Source code changes are recorded as facts over source code regions according to the RDF (Resource Description Framework) data model so that the analysis can be performed in terms of fact base queries. To show the capability of the method, we report on an experiment that emulates the state-of-the-art concern mining technique called COMMIT using our own change analysis tool called Diff/TS. A comparative case study on several open source projects written in C and Java shows that our technique improves results and overcomes the language barrier in the analysis.","M. Hashimoto and A. Mori",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11, CR12","Rejeitado: CR8"
"CROP: Linking Code Reviews to Source Code Changes","Code review has been widely adopted by both industrial and open source software development communities. Research in code review is highly dependant on real-world data, and although existing researchers have attempted to provide code review datasets, there is still no dataset that links code reviews with complete versions of the system's code base mainly because reviewed versions are not kept in the system's version control repository. Thus, we present CROP, the Code Review Open Platform, the first curated code review repository that links review data with isolated complete versions (snapshots) of the source code at the time of review. CROP currently provides data for 8 software systems, 48,975 reviews and 112,617 patches, including versions of the systems that are inaccessible in the systems' original repositories. Moreover, CROP is extensible, and it will be continuously curated and extended.","M. Paixao and J. Krinke and D. Han and M. Harman",2018,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR10, CR12"
"Visualization of fine-grained code change history","Conventional version control systems save code changes at each check-in. Recently, some development environments retain more fine-grain changes. However, providing tools for developers to use those histories is not a trivial task, due to the difficulties in visualizing the history. We present two visualizations of fine-grained code change history, which actively interact with the code editor: a timeline visualization, and a code history diff view. Our timeline and filtering options allow developers to navigate through the history and easily focus on the information they need. The code history diff view shows the history of any particular code fragment, allowing developers to move through the history simply by dragging the marker back and forth through the timeline to instantly see the code that was in the snippet at any point in the past. We augment the usefulness of these visualizations with richer editor commands including selective undo and search, which are all implemented in an Eclipse plug-in called “Azurite”. Azurite helps developers with answering common questions developers ask about the code change history that have been identified by prior research. In addition, many of users' backtracking tasks can be achieved using Azurite, which would be tedious or error-prone otherwise.","Y. Yoon and B. A. Myers and S. Koo",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Automatically Recommending Peer Reviewers in Modern Code Review","Code review is an important part of the software development process. Recently, many open source projects have begun practicing code review through “modern” tools such as GitHub pull-requests and Gerrit. Many commercial software companies use similar tools for code review internally. These tools enable the owner of a source code change to request individuals to participate in the review, i.e., reviewers. However, this task comes with a challenge. Prior work has shown that the benefits of code review are dependent upon the expertise of the reviewers involved. Thus, a common problem faced by authors of source code changes is that of identifying the best reviewers for their source code change. To address this problem, we present an approach, namely cHRev, to automatically recommend reviewers who are best suited to participate in a given review, based on their historical contributions as demonstrated in their prior reviews. We evaluate the effectiveness of cHRev on three open source systems as well as a commercial codebase at Microsoft and compare it to the state of the art in reviewer recommendation. We show that by leveraging the specific information in previously completed reviews (i.e.,quantification of review comments and their recency), we are able to improve dramatically on the performance of prior approaches, which (limitedly) operate on generic review information (i.e., reviewers of similar source code file and path names) or source coderepository data. We also present the insights into why our approach cHRev outperforms the existing approaches.","M. B. Zanjani and H. Kagdi and C. Bird",2016,"[""IEEE""]","Rejeitado: CR11, CR12","Rejeitado: CR11"
"LHDiff: Tracking Source Code Lines to Support Software Maintenance Activities","Tracking lines across versions of a file is a necessary step for solving a number of problems during software development and maintenance. Examples include, but are not limited to, locating bug-inducing changes, tracking code fragments or vulnerable instructions across versions, co-change analysis, merging file versions, reviewing source code changes, and software evolution analysis. In this tool demonstration, we present a language-independent line-level location tracker, named LHDiff, that can be used to track lines and analyze changes in various kinds of software artifacts, ranging from source code to arbitrary text files. The tool can effectively detect changed or moved lines across versions of a file, has the ability to detect line splits, and can easily be integrated with existing version control systems. It overcomes the limitations of existing language-independent techniques and is even comparable to tools that are language dependent. In addition to describing the tool, we also describe its effectiveness in analyzing source code artifacts.","M. Asaduzzaman and C. K. Roy and K. A. Schneider and M. D. Penta",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Reducing Features to Improve Code Change-Based Bug Prediction","Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine (SVM) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The SVM's improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features.","S. Shivaji and E. James Whitehead and R. Akella and S. Kim",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"APIEvolutionMiner: Keeping API evolution under control","During software evolution, source code is constantly refactored. In real-world migrations, many methods in the newer version are not present in the old version (e.g.,60% of the methods in Eclipse 2.0 were not in version 1.0). This requires changes to be consistently applied to reflect the new API and avoid further maintenance problems. In this paper, we propose a tool to extract rules by monitoring API changes applied in source code during system evolution. In this process, changes are mined at revision level in code history. Our tool focuses on mining invocation changes to keep track of how they are evolving. We also provide three case studies in order to evaluate the tool.","A. Hora and A. Etien and N. Anquetil and S. Ducasse and M. T. Valente",2014,"[""IEEE""]","Aceito: CA0, CA3","Aceito: CA3"
"Primary user detection in cognitive radio networks through quickest detection","Spectrum sensing is one of the key functionalities in cognitive radio (CR) which enables opportunistic spectrum access. How to promptly sense the presence of the primary user (PU) is a key issue to CR network. The quick detection of the PU is critical such that violation of the detection on time will cause an interference to the PU. Particularly the spectrum sensing can be evaluated by how quickly it detect the changes. This involves detecting reliably and quickly possibly weak primary user signal. In this work, we propose a cluster based quickest change detection algorithm for the spectrum sensing. The Proposed scheme utilizes one-sided CUSUM (cumulative sum) algorithm at the user level, and proposed an algorithm at the cluster level which maintain 1-slot history of the user at the cluster head (CH), which also helps in detecting the change at the cluster level. CH only collect data from the users which observe change in their slot. By exploiting the advantage of one-sided CUSUM algorithm with cluster-based approach, the proposed scheme achieve the minimum detection delay subject to false alarm constraint. With simulation results, we demonstrate that the proposed scheme has faster detection as compare to conventional detection scheme.","M. S. Khan and I. Koo",2017,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR10, CR8"
"Generating Accurate and Compact Edit Scripts Using Tree Differencing","For analyzing changes in source code, edit scriptsare used to describe the differences between two versions of afile. These scripts consist of a list of actions that, applied to thesource file, result in the new version of the file. In contrast toline-based source code differencing, tree-based approaches suchas GumTree, MTDIFF, or ChangeDistiller extract changes bycomparing the abstract syntax trees (AST) of two versions of asource file. One benefit of tree-based approaches is their abilityto capture moved (sub) trees in the AST. Our approach, theIterative Java Matcher (IJM), builds upon GumTree and aims atgenerating more accurate and compact edit scripts that capturethe developer's intent. This is achieved by improving the qualityof the generated move and update actions, which are the mainsource of inaccurate actions generated by previous approaches. To evaluate our approach, we conducted a study with 11 external experts and manually analyzed the accuracy of 2400 randomly selected editactions. Comparing IJM to GumTree and MTDIFF, the resultsshow that IJM provides better accuracy for move and updateactions and is more beneficial to understanding the changes.","V. Frick and T. Grassauer and F. Beck and M. Pinzger",2018,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA3","Aceito: CA3"
"On the Conceptual Cohesion of Co-Change Clusters","The analysis of co-change clusters as an alternative software decomposition can provide insights on different perspectives of modularity. But the usual approach using coarse-grained entities does not provide relevant information, like the conceptual cohesion of the modular abstractions that emerge from co-change clusters. This work presents a novel approach to analyze the conceptual cohesion of the source-code associated with co-change clusters of fine-grained entities. We obtain from the change history information found in version control systems. We describe the use of our approach to analyze six well established and currently active open-source projects from different domains and one of the most relevant systems of the Brazilian Government for the financial domain. The results show that co-change clusters offer a new perspective on the code based on groups with high conceptual cohesion between its entities (up to 69% more than the original package decomposition), and, thus, are suited to detect concepts pervaded on codebases, opening new possibilities of comprehension of source-code by means of the concepts embodied in the co-change clusters.","M. C. D. Oliveira and R. B. D. Almeida and G. N. Ramos and M. Ribeiro",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Mining Version Histories for Detecting Code Smells","Code smells are symptoms of poor design and implementation choices that may hinder code comprehension, and possibly increase changeand fault-proneness. While most of the detection techniques just rely on structural information, many code smells are intrinsically characterized by how code elements change overtime. In this paper, we propose Historical Information for Smell deTection (HIST), an approach exploiting change history information to detect instances of five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy. We evaluate HIST in two empirical studies. The first, conducted on 20 open source projects, aimed at assessing the accuracy of HIST in detecting instances of the code smells mentioned above. The results indicate that the precision of HIST ranges between 72 and 86 percent, and its recall ranges between 58 and 100 percent. Also, results of the first study indicate that HIST is able to identify code smells that cannot be identified by competitive approaches solely based on code analysis of a single system's snapshot. Then, we conducted a second study aimed at investigating to what extent the code smells detected by HIST (and by competitive code analysis techniques) reflect developers' perception of poor design and implementation choices. We involved 12 developers of four open source projects that recognized more than 75 percent of the code smell instances identified by HIST as actual design/implementation problems.","F. Palomba and G. Bavota and M. D. Penta and R. Oliveto and D. Poshyvanyk and A. De Lucia",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Recommending relevant code artifacts for change requests using multiple predictors","Finding code artifacts affected by a given change request is a time-consuming process in large software systems. Various approaches have been proposed to automate this activity, e.g., based on information retrieval. The performance of a particular prediction approach often highly depends on attributes like coding style or writing style of change request. Thus, we propose to use multiple prediction approaches in combination with machine learning. First experiments show that machine learning is well suitable to weight different prediction approaches for individual software projects and hence improve prediction performance.","O. Denninger",2012,"[""IEEE"",""ACM"",""Engineering Village""]","Aceito: CA1","Aceito: CA1"
"A study of repetitiveness of code changes in software evolution","In this paper, we present a large-scale study of repetitiveness of code changes in software evolution. We collected a large data set of 2,841 Java projects, with 1.7 billion source lines of code (SLOC) at the latest revisions, 1.8 million code change revisions (0.4 million fixes), 6.2 million changed files, and 2.5 billion changed SLOCs. A change is considered repeated within or cross-project if it matches another change having occurred in the history of the project or another project, respectively. We report the following important findings. First, repetitiveness of changes could be as high as 70-100% at small sizes and decreases exponentially as size increases. Second, repetitiveness is higher and more stable in the cross-project setting than in the within-project one. Third, fixing changes repeat similarly to general changes. Importantly, learning code changes and recommending them in software evolution is beneficial with accuracy for top-1 recommendation of over 30% and top-3 of nearly 35%. Repeated fixing changes could also be useful for automatic program repair.","H. A. Nguyen and A. T. Nguyen and T. T. Nguyen and T. N. Nguyen and H. Rajan",2013,"[""IEEE"",""ACM"",""Engineering Village""]","Aceito: CA0, CA1, CA3","Aceito: CA1, CA3"
"Bug Localization Based on Code Change Histories and Bug Reports","A bug report is mainly used to find a fault location in software maintenance. It contains several fields such as summary, description, status and version. The description field includes detail scenario and stack traces if exceptional messages are presented. Recently researchers have proposed several approaches for automatic bug localization by using information retrieval and data mining. We propose BLIA, a statically integrated analysis approach of IR-based bug localization by utilizing texts and stack traces in bug reports, structured information of source files, and source code change histories. We performed experiments on three open source projects, namely AspectJ, SWT and ZXing. Compared with prior tools, our experiment results showed that BLIA outperforms the existing tools in terms of mean average precision. Our approach on average improved the metric of BugLocator, BLUiR, BRTracer and AmaLgam by 34%, 23%, 17% and 8%, respectively.","K. C. Youm and J. Ahn and J. Kim and E. Lee",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Mining Change Histories for Unknown Systematic Edits","Software developers often need to repeat similar modifications in multiple different locations of a system's source code. These repeated similar modifications, or systematic edits, can be both tedious and error-prone to perform manually. While there are tools that can be used to assist in automating systematic edits, it is not straightforward to find out where the occurrences of a systematic edit are located in an existing system. This knowledge is valuable to help decide whether refactoring is needed, or whether future occurrences of an existing systematic edit should be automated. In this paper, we tackle the problem of finding unknown systematic edits using a closed frequent itemset mining algorithm, operating on sets of distilled source code changes. This approach has been implemented for Java programs in a tool called SysEdMiner. To evaluate the tool's precision and scalability, we have applied it to an industrial use case.","T. Molderez and R. Stevens and C. De Roover",2017,"[""IEEE"",""ACM""]","Aceito: CA0, CA1, CA2, CA3","Aceito: CA1, CA2, CA3"
"Lost comments support program comprehension","Source code comments are valuable to keep developers' explanations of code fragments. Proper comments help code readers understand the source code quickly and precisely. However, developers sometimes delete valuable comments since they do not know about the readers' knowledge and think the written comments are redundant. This paper describes a study of lost comments based on edit operation histories of source code. The experimental result shows that developers sometimes delete comments although their associated code fragments are not changed. Lost comments contain valuable descriptions that can be utilized as new data sources to support program comprehension.","T. Omori",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Impact of Version Management for Transactional Memories on Phase-Change Memories","Two of the major issues in current computer systems are energy consumption and how to explore concurrent systems in a correct and efficient way. Solutions for these hazards may be sought both in hardware and in software. Phase-Change Memory (PCM) is a memory technology intended to replace DRAMs (Dynamic Random Access Memories) as the main memory, providing reduced static power consumption. Their main problem is related to write operations that are slow and wear their material. Transactional Memories are synchronization methods developed to reduce the limitations of lock-based synchronization. Their main advantages are related to being high-level and allowing composition and reuse of code, besides the absence of deadlocks. The objective of this study is to analyze the impact of different versioning managers (VMs) for transactional memories in PCMs. The lazy versioning/lazy acquisition scheme for version management presented the lowest wear on the PCM in 3 of 7 benchmarks analyzed, and results similar to the alternative versioning for the other 4~benchmarks. These results are related to the number of aborts of VMs, where this VM presents a much smaller number of aborts than the others, up to 39 times less aborts in the experiment with the benchmark Kmeans with 64 threads.","F. L. Teixeira and M. L. Pilla and A. R. D. Bois and D. Mosse",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Recommending source code locations for system specific transformations","From time to time, developers perform sequences of code transformations in a systematic and repetitive way. This may happen, for example, when introducing a design pattern in a legacy system: similar classes have to be introduced, containing similar methods that are called in a similar way. Automation of these sequences of transformations has been proposed in the literature to avoid errors due to their repetitive nature. However, developers still need support to identify all the relevant code locations that are candidate for transformation. Past research showed that these kinds of transformation can lag for years with forgotten instances popping out from time to time as other evolutions bring them into light. In this paper, we evaluate three distinct code search approaches (“structural”, based on Information Retrieval, and AST based algorithm) to find code locations that would require similar transformations. We validate the resulting candidate locations from these approaches on real cases identified previously in literature. The results show that looking for code with similar roles, e.g., classes in the same hierarchy, provides interesting results with an average recall of 87% and in some cases the precision up to 70%.","G. Santos and K. V. R. Paixão and N. Anquetil and A. Etien and M. de Almeida Maia and S. Ducasse",2017,"[""IEEE""]","Aceito: CA0, CA1, CA3","Aceito: CA1, CA3"
"ChangeMacroRecorder: Recording fine-grained textual changes of source code","Recording code changes comes to be well recognized as an effective means for understanding the evolution of existing programs and making their future changes efficient. Although fine-grained textual changes of source code are worth leveraging in various situations, there is no satisfactory tool that records such changes. This paper proposes a yet another tool, called ChangeMacroRecorder, which automatically records all textual changes of source code while a programmer writes and modifies it on the Eclipse's Java editor. Its capability has been improved with respect to both the accuracy of its recording and the convenience for its use. Tool developers can easily and cheaply create their new applications that utilize recorded changes by embedding our proposed recording tool into them.","K. Maruyama and S. Hayashi and T. Omori",2018,"[""IEEE""]","Rejeitado: CR11","Rejeitado: CR11"
"When Do Changes Induce Software Vulnerabilities?","Version control systems (VCSs) have almost become the de facto standard for the management of open-source projects and the development of their source code. In VCSs, source code which can potentially be vulnerable is introduced to a system through what are so called commits. Vulnerable commits force the system into an insecure state. The farreaching impact of vulnerabilities attests to the importance of identifying and understanding the characteristics of prior vulnerable changes (or commits), in order to detect future similar ones. The concept of change classification was previously studied in the literature of bug detection to identify commits with defects. In this paper, we borrow the notion of change classification from the literature of defect detection to further investigate its applicability to vulnerability detection problem using semi-supervised learning. In addition, we also experiment with new vulnerability predictors, and compare the predictive power of our proposed features with vulnerability prediction techniques based on text mining. The experimental results show that our semi-supervised approach holds promise in improving change classification effectiveness by leveraging unlabeled data.","M. Alohaly and H. Takabi",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Code Reviewing in the Trenches: Challenges and Best Practices","Code review has been widely adopted by and adapted to open source and industrial projects. Code review practices have undergone extensive research, with most studies relying on trace data from tool reviews, sometimes augmented by surveys and interviews. Several recent industrial research studies, along with blog posts and white papers, have revealed additional insights on code reviewing “from the trenches.” Unfortunately, the lessons learned about code reviewing are widely dispersed and poorly summarized by the existing literature. In particular, practitioners wishing to adopt or reflect on an existing or new code review process might have difficulty determining what challenges to expect and which best practices to adopt for their development context. Building on the existing literature, this article adds insights from a recent large-scale study of Microsoft developers to summarize the challenges that code-change authors and reviewers face, suggest best code-reviewing practices, and discuss tradeoffs that practitioners should consider. This article is part of a theme issue on Process Improvement.","L. MacLeod and M. Greiler and M. Storey and C. Bird and J. Czerwonka",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Generalizing the Analysis of Evolutionary Coupling for Software Change Impact Analysis","Software change impact analysis aims to find artifacts potentially affected by a change. Typical approaches apply language-specific static or dynamic dependence analysis, and are thus restricted to homogeneous systems. This restriction is a major drawback given today's increasingly heterogeneous software. Evolutionary coupling has been proposed as a language-agnostic alternative that mines relations between source-code entities from the system's change history. Unfortunately, existing evolutionary coupling based techniques fall short. For example, using Singular Value Decomposition (SVD) quickly becomes computationally expensive. An efficient alternative applies targeted association rule mining, but the most widely known approach (ROSE) has restricted applicability: experiments on two large industrial systems, and four large open source systems, show that ROSE can only identify dependencies about 25% of the time. To overcome this limitation, we introduce TARMAQ, a new algorithm for mining evolutionary coupling. Empirically evaluated on the same six systems, TARMAQ performs consistently better than ROSE and SVD, is applicable 100% of the time, and runs orders of magnitude faster than SVD. We conclude that the proposed algorithm is a significant step forward towards achieving robust change impact analysis for heterogeneous systems.","T. Rolfsnes and S. D. Alesio and R. Behjati and L. Moonen and D. W. Binkley",2016,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"The atomic change set of Java programming language","Software configuration management computes the diff between the old version and new version of the program and represents the diff as added, deleted and updated text lines. The source code change represents the change type of code revisions in fine-grained levels such as class or field but rather to added, deleted and updated text lines. Atomic change is the minimal source code change. An atomic change can not be decomposed into other atomic changes. This paper defines the atomic change and the atomic change set of Java programming language. It analyzes properties of the atomic change set of Java programming language. It also emphasizes the differences of the binary compatibility and source compatibility of atomic changes of Java programming language. Finally, it gives a method for computing all source compatible atomic changes of Java programming language.","Y. Liang and L. Yansheng",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Can we predict types of code changes? An empirical analysis","There exist many approaches that help in pointing developers to the change-prone parts of a software system. Although beneficial, they mostly fall short in providing details of these changes. Fine-grained source code changes (SCC) capture such detailed code changes and their semantics on the statement level. These SCC can be condition changes, interface modifications, inserts or deletions of methods and attributes, or other kinds of statement changes. In this paper, we explore prediction models for whether a source file will be affected by a certain type of SCC. These predictions are computed on the static source code dependency graph and use social network centrality measures and object-oriented metrics. For that, we use change data of the Eclipse platform and the Azureus 3 project. The results show that Neural Network models can predict categories of SCC types. Furthermore, our models can output a list of the potentially change-prone files ranked according to their change-proneness, overall and per change type category.","E. Giger and M. Pinzger and H. C. Gall",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR11, CR12"
"Implication of N400 and P600 waves in the Linguistic Code Change in Monolinguals and Bilinguals","There is evidence of the importance of N400 and P600 waves in linguistic processes, theses brain waves are related to syntax. This work proposes to evaluate learning process through the analysis of responses generated when formulation of word is requested, an artificial grammar test (AGT) is developed and N400 and P600 peaks are taken as indicators of performance; and two different groups of subjects took the AGT, 5 monolinguals and 5 bilinguals. The AGT is composed by 30 hybrids, each hybrid defines rules to formulate words; then if this word accomplished the rules, it is considered as grammatical. The N400 and P600 waves are computed by each word letter, and the mean for all 30 hybrids is compared between both two groups by electrode. Greater amplitudes for N400 and P600 peaks was found for monolinguals in comparison with bilinguals.","D. Achanccaray and J. Astucuri and M. Hayashibe and J. Pirca and V. Espinoza",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Extracting executable transformations from distilled code changes","Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. Using examples, we demonstrate the expressiveness of specifying source code evolutions through intermediate ASTs. We also show that our approach is able to recall different implementation variants of the same source code evolution in open-source histories.","R. Stevens and C. De Roover",2017,"[""IEEE""]","Aceito: CA3","Aceito: CA3"
"[Research Paper] Which Method-Stereotype Changes are Indicators of Code Smells?","A study of how method roles evolve during the lifetime of a software system is presented. Evolution is examined by analyzing when the stereotype of a method changes. Stereotypes provide a high-level categorization of a method's behavior and role, and also provide insight into how a method interacts with its environment and carries out tasks. The study covers 50 open-source systems and 6 closed-source systems. Results show that method behavior with respect to stereotype is highly stable and constant over time. Overall, out of all the history examined, only about 10% of changes to methods result in a change in their stereotype. Examples of methods that change stereotype are further examined. A select number of these types of changes are indicators of code smells.","M. J. Decker and C. D. Newman and N. Dragan and M. L. Collard and J. I. Maletic and N. A. Kraft",2018,"[""IEEE""]","Rejeitado: CR11, CR12","Rejeitado: CR11"
"Cross-project build co-change prediction","Build systems orchestrate how human-readable source code is translated into executable programs. In a software project, source code changes can induce changes in the build system (aka. build co-changes). It is difficult for developers to identify when build co-changes are necessary due to the complexity of build systems. Prediction of build co-changes works well if there is a sufficient amount of training data to build a model. However, in practice, for new projects, there exists a limited number of changes. Using training data from other projects to predict the build co-changes in a new project can help improve the performance of the build co-change prediction. We refer to this problem as cross-project build co-change prediction. In this paper, we propose CroBuild, a novel cross-project build co-change prediction approach that iteratively learns new classifiers. CroBuild constructs an ensemble of classifiers by iteratively building classifiers and assigning them weights according to its prediction error rate. Given that only a small proportion of code changes are build co-changing, we also propose an imbalance-aware approach that learns a threshold boundary between those code changes that are build co-changing and those that are not in order to construct classifiers in each iteration. To examine the benefits of CroBuild, we perform experiments on 4 large datasets including Mozilla, Eclipse-core, Lucene, and Jazz, comprising a total of 50,884 changes. On average, across the 4 datasets, CroBuild achieves a F1-score of up to 0.408. We also compare CroBuild with other approaches such as a basic model, AdaBoost proposed by Freund et al., and TrAdaBoost proposed by Dai et al.. On average, across the 4 datasets, the CroBuild approach yields an improvement in F1-scores of 41.54%, 36.63%, and 36.97% over the basic model, AdaBoost, and TrAdaBoost, respectively.","X. Xia and D. Lo and S. McIntosh and E. Shihab and A. E. Hassan",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR12, CR11"
"Software Defect Prediction Using Semi-Supervised Learning with Change Burst Information","Software defect prediction is an important software quality assurance technique. It utilizes historical project data and previously discovered defects to predict potential defects. However, most of existing methods assume that large amounts of labeled historical data are available for prediction, while in the early stage of the life cycle, projects may lack the data needed for building such predictors. In addition, most of existing techniques use static code metrics as predictors, while they omit change information that may introduce risks into software development. In this paper, we take these two issues into consideration, and propose a semi-supervised based defect prediction approach - extRF. extRF extends the classical supervised Random Forest algorithm by self-training paradigm. It also employs change burst information for improving accuracy of software defect prediction. We also conduct an experiment to evaluate extRF against three other supervised machine learners (i.e. Logistic Regression, Naive Bayes, Random Forest) and compare the effectiveness of code metrics, change burst metrics, and a combination of them. Experimental results show that extRF trained with a small size of labeled dataset achieves comparable performance to some supervised learning approaches trained with a larger size of labeled dataset. When only 2% of Eclipse 2.0 data are used for training, extRF can achieve F-measure about 0.562, approximate to that of LR (a supervised learning approach) at labeled sampling rate of 50%. Besides, change burst metrics outperform code metrics in that F-measure rises to a peak value of 0.75 for Eclipse 3.0 and JDT.Core.","Q. He and B. Shen and Y. Chen",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Assessing Technical Debt in Automated Tests with CodeScene","Test automation promises several advantages such as shorter lead times, higher code quality, and an executable documentation of the system's behavior. However, test automation won't deliver on those promises unless the quality of the automated test code itself is maintained, and to manually inspect the evolution of thousands of tests that change on a daily basis is impractical at best. This paper investigates how CodeScene - a tool for predictive analyses and visualizations - could be used to identify technical debt in automated test code. CodeScene combines repository mining, static code analysis, and machine learning to prioritize potential code improvements based on the most likely return on investment.","A. Tornhill",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8"
"Mining Performance Regression Inducing Code Changes in Evolving Software","During software evolution, the source code of a system frequently changes due to bug fixes or new feature requests. Some of these changes may accidentally degrade performance of a newly released software version. A notable problem of regression testing is how to find problematic changes (out of a large number of committed changes) that may be responsible for performance regressions under certain test inputs. We propose a novel recommendation system, coined as PERFIMPACT, for automatically identifying code changes that may potentially be responsible for performance regressions using a combination of search-based input profiling and change impact analysis techniques. PERFIMPACT independently sends the same input values to two releases of the application under test, and uses a genetic algorithm to mine execution traces and explore a large space of input value combinations to find specific inputs that take longer time to execute in a new release. Since these input values are likely to expose performance regressions, PERFIMPACT automatically mines the corresponding execution traces to evaluate the impact of each code change on the performance and ranks the changes based on their estimated contribution to performance regressions. We implemented PERFIMPACT and evaluated it on different releases of two open-source web applications. The results demonstrate that PERFIMPACT effectively detects input value combinations to expose performance regressions and mines the code changes are likely to be responsible for these performance regressions.","Q. Luo and D. Poshyvanyk and M. Grechanik",2016,"[""IEEE"",""ACM"",""Engineering Village""]","Aceito: CA5, CA6, CA4","Aceito: CA4, CA5, CA1"
"Semantic zooming of code change history","Previously, we presented our technique for visualizing fine-grained code changes in a timeline view, designed to facilitate reviewing and interacting with the code change history. During user evaluations, it became evident that users often wanted to see the code changes at a higher level of abstraction. Therefore, we developed a novel approach to automatically summarize fine-grained code changes into more conceptual, higher-level changes in real time. Our system provides four collapse levels, which are integrated with the timeline via semantic zooming: raw level (no collapsing), statement level, method level, and type level. Compared to the raw level, the number of code changes shown in the timeline at each level is reduced by 55%, 77%, and 83%, respectively. This implies that the semantic zooming would help users better understand and interact with the history by minimizing the potential information overload.","Y. Yoon and B. A. Myers",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"A Practical Approach to the Automatic Classification of Security-Relevant Commits","The lack of reliable sources of detailed information on the vulnerabilities of open-source software (OSS) components is a major obstacle to maintaining a secure software supply chain and an effective vulnerability management process. Standard sources of advisories and vulnerability data, such as the National Vulnerability Database (NVD), are known to suffer from poor coverage and inconsistent quality. To reduce our dependency on these sources, we propose an approach that uses machine-learning to analyze source code repositories and to automatically identify commits that are security-relevant (i.e., that are likely to fix a vulnerability). We treat the source code changes introduced by commits as documents written in natural language, classifying them using standard document classification methods. Combining independent classifiers that use information from different facets of commits, our method can yield high precision (80%) while ensuring acceptable recall (43%). In particular, the use of information extracted from the source code changes yields a substantial improvement over the best known approach in state of the art, while requiring a significantly smaller amount of training data and employing a simpler architecture.","A. Sabetta and M. Bezzi",2018,"[""IEEE""]","Rejeitado: CR7, CR12","Rejeitado: CR12"
"Helping Mobile Software Code Reviewers: A Study of Bug Repair and Refactoring Patterns","Mobile Developers commonly spend a significant amount of time and effort on conducting code reviews on newly introduced and domain-specific practices, such as platform-specific feature addition, quality of service anti-pattern refactorings, and battery-related bug fixes. To address these problems, we conducted a large empirical study over the software change history of 318 open source projects and investigated platform-dependent code changes from open source projects. Our analysis focuses on what types of changes mobile application developers typically make and how they perceive, recall, and communicate changed and affected code. Our study required the development of an automated strategy to examine open source repositories and categorize platform-related refactoring edits, bug repairs, and API updates, mining 1,961,990 commit changes. Our findings call for the need to develop a new recommendation system aimed at efficiently identifying required changes such as bug fixes and refactorings during mobile application code reviews.","Z. Chen",2016,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR10, CR8"
"A History Querying Tool and Its Application to Detect Multi-version Refactorings","Version Control Systems (VCS) have become indispensable in developing software. In order to provide support for change management, they track the history of software projects. Tool builders can exploit this latent historical information to provide insights in the evolution of the project. For example, the information needed to identify when and where a particular refactoring was applied is implicitly present in the VCS. However, tool support for eliciting this information is lacking. So far, no general-purpose history querying tool capable of answering a wide variety of questions about the evolution of software exists. Therefore, we generalize the idea of a program querying tool to a history querying tool. A program querying tool reifies the program's code into a knowledge base, from which it retrieves elements that exhibit characteristics specified through a user-provided program query. Our history querying tool, QwalKeko, enables specifying the evolution of source code characteristics across multiple versions of Java projects versioned in Git. We apply QwalKeko to the problem of detecting refactorings, specified as the code changes induced by each refactoring. These specifications stem from the literature, but are limited to changes between two successive versions. We demonstrate the expressiveness of our tool by generalizing the specifications such that refactorings can span multiple versions.","R. Stevens and C. De Roover and C. Noguera and V. Jonckers",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Who should review my code? A file location-based code-reviewer recommendation approach for Modern Code Review","Software code review is an inspection of a code change by an independent third-party developer in order to identify and fix defects before an integration. Effectively performing code review can improve the overall software quality. In recent years, Modern Code Review (MCR), a lightweight and tool-based code inspection, has been widely adopted in both proprietary and open-source software systems. Finding appropriate code-reviewers in MCR is a necessary step of reviewing a code change. However, little research is known the difficulty of finding code-reviewers in a distributed software development and its impact on reviewing time. In this paper, we investigate the impact of reviews with code-reviewer assignment problem has on reviewing time. We find that reviews with code-reviewer assignment problem take 12 days longer to approve a code change. To help developers find appropriate code-reviewers, we propose RevFinder, a file location-based code-reviewer recommendation approach. We leverage a similarity of previously reviewed file path to recommend an appropriate code-reviewer. The intuition is that files that are located in similar file paths would be managed and reviewed by similar experienced code-reviewers. Through an empirical evaluation on a case study of 42,045 reviews of Android Open Source Project (AOSP), OpenStack, Qt and LibreOffice projects, we find that RevFinder accurately recommended 79% of reviews with a top 10 recommendation. RevFinder also correctly recommended the code-reviewers with a median rank of 4. The overall ranking of RevFinder is 3 times better than that of a baseline approach. We believe that RevFinder could be applied to MCR in order to help developers find appropriate code-reviewers and speed up the overall code review process.","P. Thongtanunam and C. Tantithamthavorn and R. G. Kula and N. Yoshida and H. Iida and K. Matsumoto",2015,"[""IEEE""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Explaining Why Methods Change Together","By analyzing historical information from Source Code Management systems, previous research has observed that certain methods tend to change together consistently. Co-change has been identified as a good predictor of the entities that are likely to be affected by a change, which ones might be missing modifications, and which ones might change in the future. However, existing co-change analysis provides no insight on why methods consistently co-change. Being able to identify the rationale that explains co-changes could allow to document and enforce design knowledge. This paper proposes an automatic approach to derive the reason behind a co-change. We define the reason of a (set) of co-changes as a set of properties common to the elements that co-change. We consider two kinds of properties: structural properties which indicate explicit dependencies, and semantic properties which reveal implicit dependencies. Then we attempt to identify the reasons behind single commits, as well as the reasons behind co-changes that repeatedly affect the same set of methods. These sets of methods are identified by clustering methods that tend to be modified in the same commit-transactions. We perform our analysis over the history of two open-source systems, analyzing nearly 19.000 methods and over 3700 commits. We show that it is possible to automatically extract explanations for co-changes, that the quality of such explanations improves when structural and semantic properties are taken into account, and when the methods analyzed co-change recurrently.","A. Lozano and C. Noguera and V. Jonckers",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Co-evolution analysis of production and test code by learning association rules of changes","Many modern software systems come with automated tests. While these tests help to maintain code quality by providing early feedback after modifications, they also need to be maintained. In this paper, we replicate a recent pattern mining experiment to find patterns on how production and test code co-evolve over time. Understanding co-evolution patterns may directly affect the quality of tests and thus the quality of the whole system. The analysis takes into account fine grained changes in both types of code. Since the full list of fine grained changes cannot be perceived, association rules are learned from the history to extract co-change patterns. We analyzed the occurrence of 6 patterns throughout almost 2500 versions of a Java system and found that patterns are present, but supported by weaker links than in previously reported. Hence we experimented with weighting methods and investigated the composition of commits.","L. Vidács and M. Pinzger",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"MINCE: Mining change history of Android project","An analysis of commit history of Android reveals that Android has a code base of 550K files, where on an average each file has been modified 8.7 times. 41% of files have been modified at-least once. In terms of contributors, it has an overall contributor community of 1563, with 58.5% of them having made &gt;; 5 commits. Moreover, the contributor community shows high churn levels, with only 13 of contributors continuing from 2005 to 2011. In terms of industry participation, Google &amp; Android account for 22% of developers. Intel and RedHat account for 2% of contributors each and IBM, Oracle, TI, SGI account for another 1% each. Android code can be classified into 5 sub-projects: kernel, platform, device, tools and toolchain. In this paper, we profile each of these sub-projects in terms of change volumes, contributor and industry participation. We further picked specific framework topics such as UI, security, whose understanding is required from perspective of developing apps over Android, and present some insights on community participation around the same.","V. S. Sinha and S. Mani and M. Gupta",2012,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"How Well Do Change Sequences Predict Defects? Sequence Learning from Software Changes","Software defect prediction, which aims to identify defective modules, can assist developers in finding bugs and prioritizing limited quality assurance resources. Various features to build defect prediction models have been proposed and evaluated. Among them, process metrics are one important category. Yet, existing process metrics are mainly encoded manually from change histories and ignore the sequential information arising from the changes during software evolution. Unlike traditional process metrics used for existing defect prediction models, change sequences are mostly vectors of variable length. This makes it difficult to apply such sequences directly in prediction models that are driven by conventional classifiers. To resolve this challenge, we utilize Recurrent Neural Network (RNN), which is a deep learning technique, to encode features from sequence data automatically. In this paper, we propose a novel approach called Fences, which extracts six types of change sequences covering different aspects of software changes via fine-grained change analysis. It approaches defects prediction by mapping it to a sequence labeling problem solvable by RNN. Our evaluations on 10 open source projects show that Fences can predict defects with high performance. Fences also outperforms the state-of-the-art technique which learns semantic features automatically from static code via deep learning.","M. Wen and R. Wu and S. C. Cheung",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR11, CR12"
"Learning binary code features for UAV target tracking","During target tracking, in order to obtain a higher tracking accuracy, the region we would like to track should have a good feature expression. Furthermore, we need to extract multilevel and complex features to deal with problems which are usually encountered during UAV tracking, such as the target deformation, scale change and occlusion. However, such features make tracker more complex which would seriously affect the real-time tracking. Considering the above problems, we take the advantage of random forest for features selection, and then transform the features to binary code, which can not only reduce redundancy but speed up the tracker. In order to further improve the accuracy of UAV tracking, we utilize structured SVM for online learning to distinguish object from background. In addition, we apply the scale pyramid to achieve the scale invariance of tracker, which help to obtain a more precise position of the object. We have verified the effectiveness and robustness of our method on the classical UAV object tracking dataset UAV123.","Q. Xiao and Q. Zhang and X. Wu and X. Han and R. Li",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Code Bad Smell Detection through Evolutionary Data Mining","The existence of code bad smell has a severe impact on the software quality. Numerous researches show that ignoring code bad smells can lead to failure of a software system. Thus, the detection of bad smells has drawn the attention of many researchers and practitioners. Quite a few approaches have been proposed to detect code bad smells. Most approaches are solely based on structural information extracted from source code. However, we have observed that some code bad smells have the evolutionary property, and thus propose a novel approach to detect three code bad smells by mining software evolutionary data: duplicated code, shotgun surgery, and divergent change. It exploits association rules mined from change history of software systems, upon which we define heuristic algorithms to detect the three bad smells. The experimental results on five open source projects demonstrate that the proposed approach achieves higher precision, recall and F-measure.","S. Fu and B. Shen",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation","Peer code review is a cost-effective software defect detection technique. Tool assisted code review is a form of peer code review, which can improve both quality and quantity of reviews. However, there is a significant amount of human effort involved even in tool based code reviews. Using static analysis tools, it is possible to reduce the human effort by automating the checks for coding standard violations and common defect patterns. Towards this goal, we propose a tool called Review Bot for the integration of automatic static analysis with the code review process. Review Bot uses output of multiple static analysis tools to publish reviews automatically. Through a user study, we show that integrating static analysis tools with code review process can improve the quality of code review. The developer feedback for a subset of comments from automatic reviews shows that the developers agree to fix 93% of all the automatically generated comments. There is only 14.71% of all the accepted comments which need improvements in terms of priority, comment message, etc. Another problem with tool assisted code review is the assignment of appropriate reviewers. Review Bot solves this problem by generating reviewer recommendations based on change history of source code lines. Our experimental results show that the recommendation accuracy is in the range of 60%-92%, which is significantly better than a comparable method based on file change history.","V. Balachandran",2013,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"[Journal First] On the Diffuseness and the Impact on Maintainability of Code Smells: A Large Scale Empirical Investigation","Code smells are symptoms of poor design and implementation choices that may hinder code comprehensibility and maintainability. Despite the effort devoted by the research community in studying code smells, the extent to which code smells in software systems affect software maintainability remains still unclear. In this paper we present a large scale empirical investigation on the diffuseness of code smells and their impact on code change- and fault-proneness. The study was conducted across a total of 395 releases of 30 open source projects and considering 17,350 manually validated instances of 13 different code smell types. The results show that smells characterized by long and/or complex code (e.g., Complex Class) are highly diffused, and that smelly classes have a higher change- and fault-proneness than smell-free classes.","F. Palomba and G. Bavota and M. Di Penta and F. Fasano and R. Oliveto and A. De Lucia",2018,"[""IEEE""]","Rejeitado: CR11, CR12","Rejeitado: CR8, CR10"
"Mining Version Control System for Automatically Generating Commit Comment","Commit comments increasingly receive attention as an important complementary component in code change comprehension. To address the comment scarcity issue, a variety of automatic approaches for commit comment generation have been intensively proposed. However, most of these approaches mechanically outline a superficial level summary of the changed software entities, the change intent behind the code changes is lost (e.g., the existing approaches cannot generate such comment: ""fixing null pointer exception""). Considering the comments written by developers often describe the intent behind the code change, we propose a method to automatically generate commit comment by reusing the existing comments in version control system. Specifically, for an input commit, we apply syntax, semantic, pre-syntax, and pre-semantic similarities to discover the similar commits from half a million commits, and recommend the reusable comments to the input commit from the ones of the similar commits. We evaluate our approach on 7 projects. The results show that 9.1% of the generated comments are good, 27.7% of the generated comments need minor fix, and 63.2% are bad, and we also analyze the reasons that make a comment available or unavailable.","Y. Huang and Q. Zheng and X. Chen and Y. Xiong and Z. Liu and X. Luo",2017,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Recommending Code Changes for Automatic Backporting of Linux Device Drivers","Device drivers are essential components of any operating system (OS). They specify the communication protocol that allows the OS to interact with a device. However, drivers for new devices are usually created for a specific OS version. These drivers often need to be backported to the older versions to allow use of the new device. Backporting is often done manually, and is tedious and error prone. To alleviate this burden on developers, we propose an automatic recommendation system to guide the selection of backporting changes. Our approach analyzes the version history for cues to recommend candidate changes. We have performed an experiment on 100 Linux driver files and have shown that we can give a recommendation containing the correct backport for 68 of the drivers. For these 68 cases, 73.5%, 85.3%, and 88.2% of the correct recommendations are located in the Top-1, Top-2, and Top-5 positions of the recommendation lists respectively. The successful cases cover various kinds of changes including change of record access, deletion of function argument, change of a function name, change of constant, and change of if condition. Manual investigation of failed cases highlights limitations of our approach, including inability to infer complex changes, and unavailability of relevant cues in version history.","F. Thung and X. D. Le and D. Lo and J. Lawall",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Co-evolution of Infrastructure and Source Code - An Empirical Study","Infrastructure-as-code automates the process of configuring and setting up the environment (e.g., servers, VMs and databases) in which a software system will be tested and/or deployed, through textual specification files in a language like Puppet or Chef. Since the environment is instantiated automatically by the infrastructure languages' tools, no manual intervention is necessary apart from maintaining the infrastructure specification files. The amount of work involved with such maintenance, as well as the size and complexity of infrastructure specification files, have not yet been studied empirically. Through an empirical study of the version control system of 265 Open Stack projects, we find that infrastructure files are large and churn frequently, which could indicate a potential of introducing bugs. Furthermore, we found that the infrastructure code files are coupled tightly with the other files in a project, especially test files, which implies that testers often need to change infrastructure specifications when making changes to the test framework and tests.","Y. Jiang and B. Adams",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"SENSA: Sensitivity Analysis for Quantitative Change-Impact Prediction","Sensitivity analysis determines how a system responds to stimuli variations, which can benefit important software-engineering tasks such as change-impact analysis. We present SENSA, a novel dynamic-analysis technique and tool that combines sensitivity analysis and execution differencing to estimate the dependencies among statements that occur in practice. In addition to identifying dependencies, SENSA quantifies them to estimate how much or how likely a statement depends on another. Quantifying dependencies helps developers prioritize and focus their inspection of code relationships. To assess the benefits of quantifying dependencies with SENSA, we applied it to various statements across Java subjects to find and prioritize the potential impacts of changing those statements. We found that SENSA predicts the actual impacts of changes to those statements more accurately than static and dynamic forward slicing. Our SENSA prototype tool is freely available for download.","H. Cai and S. Jiang and R. Santelices and Y. Zhang and Y. Zhang",2014,"[""IEEE""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Extracting Facts from Performance Tuning History of Scientific Applications for Predicting Effective Optimization Patterns","To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.","M. Hashimoto and M. Terai and T. Maeda and K. Minami",2015,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Code Churn: A Neglected Metric in Effort-Aware Just-in-Time Defect Prediction","Background: An increasing research effort has devoted to just-in-time (JIT) defect prediction. A recent study by Yang et al. at FSE'16 leveraged individual change metrics to build unsupervised JIT defect prediction model. They found that many unsupervised models performed similarly to or better than the state-of-the-art supervised models in effort-aware JIT defect prediction. Goal: In Yang et al.'s study, code churn (i.e. the change size of a code change) was neglected when building unsupervised defect prediction models. In this study, we aim to investigate the effectiveness of code churn based unsupervised defect prediction model in effort-aware JIT defect prediction. Methods: Consistent with Yang et al.'s work, we first use code churn to build a code churn based unsupervised model (CCUM). Then, we evaluate the prediction performance of CCUM against the state-of-the-art supervised and unsupervised models under the following three prediction settings: cross-validation, time-wise cross-validation, and cross-project prediction. Results: In our experiment, we compare CCUM against the state-of-the-art supervised and unsupervised JIT defect prediction models. Based on six open-source projects, our experimental results show that CCUM performs better than all the prior supervised and unsupervised models. Conclusions: The result suggests that future JIT defect prediction studies should use CCUM as a baseline model for comparison when a novel model is proposed.","J. Liu and Y. Zhou and Y. Yang and H. Lu and B. Xu",2017,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Four eyes are better than two: On the impact of code reviews on software quality","Code review is advocated as one of the best practices to improve software quality and reduce the likelihood of introducing defects during code change activities. Recent research has shown how code components having a high review coverage (i.e., a high proportion of reviewed changes) tend to be less involved in post-release fixing activities. Yet the relationship between code review and bug introduction or the overall software quality is still largely unexplored.","G. Bavota and B. Russo",2015,"[""IEEE""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Gated factored 3-way RBM for image transformation","The Factored 3-way Restricted Boltzmann Machine has encoded the image transformation successfully. But when utilize the code to unknown image, the result was much affected by the feature of training samples. Based on the model, we separated the transformation feature out of the hidden representation and designed a new probabilistic model with gate for learning distributed representations of image transformations. Inference in the model consists extracting the transformation, find the mapping code, training filters to fit for the affine or more general transformations. We also provide experimental results to validate the performance of our model to a various tasks.","L. Xia and A. Yadav and N. Duobiao",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Using Developer-Interaction Trails to Triage Change Requests","The paper presents an approach, namely iHDev, to recommend developers who are most likely to implement incoming change requests. The basic premise of iHDev is that the developers who interacted with the source code relevant to a given change request are most likely to best assist with its resolution. A machine-learning technique is first used to locate source code entities relevant to the textual description of a given change request. Ihdev then mines interaction trails (i.e., Mylyn sessions) associated with these source code entities to recommend a ranked list of developers. Ihdev integrates the interaction trails in a unique way to perform its task, which was not investigated previously. An empirical study on open source systems Mylyn and Eclipse Project was conducted to assess the effectiveness of iHDev. A number of change requests were used in the evaluated bench-mark. Recall for top one to five recommended developers and Mean Reciprocal Rank (MRR) values are reported. Furthermore, a comparative study with two previous approaches that use commit histories and/or the source code authorship information for developer recommendation was performed. Results show that iHDev could provide a recall gain of up to 127.27% with equivalent or improved MRR values by up to 112.5%.","M. B. Zanjani and H. Kagdi and C. Bird",2015,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Summarizing Evolutionary Trajectory by Grouping and Aggregating relevant code changes","The lifecycle of a large-scale software system can undergo many releases. Each release often involves hundreds or thousands of revisions committed by many developers over time. Many code changes are made in a systematic and collaborative way. However, such systematic and collaborative code changes are often undocumented and hidden in the evolution history of a software system. It is desirable to recover commonalities and associations among dispersed code changes in the evolutionary trajectory of a software system. In this paper, we present SETGA (Summarizing Evolutionary Trajectory by Grouping and Aggregation), an approach to summarizing historical commit records as trajectory patterns by grouping and aggregating relevant code changes committed over time. SETGA extracts change operations from a series of commit records from version control systems. It then groups extracted change operations by their common properties from different dimensions such as change operation types, developers and change locations. After that, SETGA aggregates relevant change operation groups by mining various associations among them. The proposed approach has been implemented and applied to three open-source systems. The results show that SETGA can identify various types of trajectory patterns that are useful for software evolution management and quality assurance.","Qingtao Jiang and X. Peng and Hai Wang and Z. Xing and W. Zhao",2015,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA3","Aceito: CA3"
"Mapping API Elements for Code Migration with Vector Representations","Mapping API elements has a significant role in software development, especially in code migration. A manual process of defining the migration is tedious and error-prone while recent approaches to automatically mine API mappings are limited to discover the mappings with textually similar APIs' names. This leads to the low accuracy in existing migration tools.We propose an approach to automatically mine API mappings which overcomes the lexical mismatch problem. We represent an API by its usages instead of its name.To characterize an API with its context consisting of surrounding APIs in its usages, we take advantage of Word2Vec model to project the APIs of Java JDK and C# .NET into corresponding continuous vector spaces. The semantic relations among APIs will be observed in those continuous space as the geometric arrangements between their representation vectors in two vector spaces.We use a learning approach to derive the linear (e.g., rotating and scaling) transformation function between two vector spaces. Transformation function is trained from human-defined pairs of API mappings from Java to C#. To find the C# API mapping with a given Java API, we use the learned function to compute its transformed vector in the C# vector space. Then, the C# API which has the most similar vector with the transformed vector is considered as the result. Our experiment shows that for just one suggestion, we are able to correctly derive the API in C# in almost 43% of the cases. With 5 suggestions, we can correctly suggest the correct C# API in almost 3 out of 4 cases (73.2%).","T. D. Nguyen and A. T. Nguyen and T. N. Nguyen",2016,"[""IEEE""]","Rejeitado: CR11","Rejeitado: CR11"
"When and Why Your Code Starts to Smell Bad (and Whether the Smells Go Away)","Technical debt is a metaphor introduced by Cunningham to indicate “not quite right code which we postpone making it right”. One noticeable symptom of technical debt is represented by code smells, defined as symptoms of poor design and implementation choices. Previous studies showed the negative impact of code smells on the comprehensibility and maintainability of code. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced, what is their survivability, and how they are removed by developers. To empirically corroborate such anecdotal evidence, we conducted a large empirical study over the change history of 200 open source projects. This study required the development of a strategy to identify smell-introducing commits, the mining of over half a million of commits, and the manual analysis and classification of over 10K of them. Our findings mostly contradict common wisdom, showing that most of the smell instances are introduced when an artifact is created and not as a result of its evolution. At the same time, 80 percent of smells survive in the system. Also, among the 20 percent of removed instances, only 9 percent are removed as a direct consequence of refactoring operations.","M. Tufano and F. Palomba and G. Bavota and R. Oliveto and M. D. Penta and A. De Lucia and D. Poshyvanyk",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Mining system specific rules from change patterns","A significant percentage of warnings reported by tools to detect coding standard violations are false positives. Thus, there are some works dedicated to provide better rules by mining them from source code history, analyzing bug-fixes or changes between system releases. However, software evolves over time, and during development not only bugs are fixed, but also features are added, and code is refactored. In such cases, changes must be consistently applied in source code to avoid maintenance problems. In this paper, we propose to extract system specific rules by mining systematic changes over source code history, i.e., not just from bug-fixes or system releases, to ensure that changes are consistently applied over source code. We focus on structural changes done to support API modification or evolution with the goal of providing better rules to developers. Also, rules are mined from predefined rule patterns that ensure their quality. In order to assess the precision of such specific rules to detect real violations, we compare them with generic rules provided by tools to detect coding standard violations on four real world systems covering two programming languages. The results show that specific rules are more precise in identifying real violations in source code than generic ones, and thus can complement them.","A. Hora and N. Anquetil and S. Ducasse and M. T. Valente",2013,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA1, CA3, CA2","Aceito: CA1, CA2, CA3"
"Discriminative Transfer Subspace Learning via Low-Rank and Sparse Representation","In this paper, we address the problem of unsupervised domain transfer learning in which no labels are available in the target domain. We use a transformation matrix to transfer both the source and target data to a common subspace, where each target sample can be represented by a combination of source samples such that the samples from different domains can be well interlaced. In this way, the discrepancy of the source and target domains is reduced. By imposing joint low-rank and sparse constraints on the reconstruction coefficient matrix, the global and local structures of data can be preserved. To enlarge the margins between different classes as much as possible and provide more freedom to diminish the discrepancy, a flexible linear classifier (projection) is obtained by learning a non-negative label relaxation matrix that allows the strict binary label matrix to relax into a slack variable matrix. Our method can avoid a potentially negative transfer by using a sparse matrix to model the noise and, thus, is more robust to different types of noise. We formulate our problem as a constrained low-rankness and sparsity minimization problem and solve it by the inexact augmented Lagrange multiplier method. Extensive experiments on various visual domain adaptation tasks show the superiority of the proposed method over the state-of-the art methods. The MATLAB code of our method will be publicly available at http://www.yongxu.org/lunwen.html.","Y. Xu and X. Fang and J. Wu and X. Li and D. Zhang",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Identifying Implicit Architectural Dependencies Using Measures of Source Code Change Waves","The principles of Agile software development are increasingly used in large software development projects, e.g. using Scrum of Scrums or combining Agile and Lean development methods. When large software products are developed by self-organized, usually feature-oriented teams, there is a risk that architectural dependencies between software components become uncontrolled. In particular there is a risk that the prescriptive architecture models in form of diagrams are outdated and implicit architectural dependencies may become more frequent than the explicit ones. In this paper we present a method for automated discovery of potential dependencies between software components based on analyzing revision history of software repositories. The result of this method is a map of implicit dependencies which is used by architects in decisions on the evolution of the architecture. The software architects can assess the validity of the dependencies and can prevent unwanted component couplings and design erosion hence minimizing the risk of post-release quality problems. Our method was evaluated in a case study at one large product at Saab Electronic Defense Systems (Saab EDS) and one large software product at Ericsson AB.","M. Staron and W. Meding and C. Höglund and P. Eriksson and J. Nilsson and J. Hansson",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Crosscutting revision control system","Large and medium scale software projects often require a source code revision control (RC) system. Unfortunately, RC systems do not perform well with obliviousness and quantification found in aspect-oriented code. When classes are oblivious to aspects, so is the RC system, and the crosscutting effect of aspects is not tracked. In this work, we study this problem in the context of using AspectJ (a standard AOP language) with Subversion (a standard RC system). We describe scenarios where the crosscutting effect of aspects combined with the concurrent changes that RC supports can lead to inconsistent states of the code. The work contributes a mechanism that checks-in with the source code versions of crosscutting metadata for tracking the effect of aspects. Another contribution of this work is the implementation of a supporting Eclipse plug-in (named XRC) that extends the JDT, AJDT, and SVN plug-ins for Eclipse to provide crosscutting revision control (XRC) for aspect-oriented programming.","S. Ifrah and D. H. Lorenz",2012,"[""IEEE"",""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Enhancing flexibility and portability of Execution Preserving Language Transformation using Meta programming","This paper describes flexibility and effectiveness of Execution Preserving Language Transformation (EPLT) using a meta framework. Program transformation is visualized as transforming the program written in legacy code to a more contemporary environment. Pure program transformation systems translate source code to target code preserving the functionality of legacy systems. Augmented versions of existing languages can be developed by combining good properties of two languages. In this work a meta framework is developed from C++ and Java language. The growing popularity of Java language forces the programmer to implement data structures and algorithms of other languages in Java. This meta framework enhances the conversion of unsafe source code written in C++ and Java to safe byte code. It provides a transformational scheme which unifies the syntax and semantics of existing languages and reduce the learning curves.","S. N. Beevi and D. C. Prasad and S. S. V. Chandra",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Integrated impact analysis for managing software changes","The paper presents an adaptive approach to perform impact analysis from a given change request to source code. Given a textual change request (e.g., a bug report), a single snapshot (release) of source code, indexed using Latent Semantic Indexing, is used to estimate the impact set. Should additional contextual information be available, the approach configures the best-fit combination to produce an improved impact set. Contextual information includes the execution trace and an initial source code entity verified for change. Combinations of information retrieval, dynamic analysis, and data mining of past source code commits are considered. The research hypothesis is that these combinations help counter the precision or recall deficit of individual techniques and improve the overall accuracy. The tandem operation of the three techniques sets it apart from other related solutions. Automation along with the effective utilization of two key sources of developer knowledge, which are often overlooked in impact analysis at the change request level, is achieved. To validate our approach, we conducted an empirical evaluation on four open source software systems. A benchmark consisting of a number of maintenance issues, such as feature requests and bug fixes, and their associated source code changes was established by manual examination of these systems and their change history. Our results indicate that there are combinations formed from the augmented developer contextual information that show statistically significant improvement over standalone approaches.","M. Gethers and B. Dit and H. Kagdi and D. Poshyvanyk",2012,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"A Learning Algorithm for Change Impact Prediction","Change impact analysis (CIA) consists in predicting the impact of a code change in a software application. In this paper, the artifacts that are considered for CIA are methods of object-oriented software; the change under study is a change in the code of the method, the impact is the test methods that fail because of the change that has been performed. We propose LCIP, a learning algorithm that learns from past impacts to predict future impacts. To evaluate LCIP, we consider Java software applications that are strongly tested. We simulate 6000 changes and their actual impact through code mutations, as done in mutation testing. We find that LCIP can predict the impact with a precision of 74%, a recall of 85%, corresponding to a F-score of 64%. This shows that taking a learning perspective on change impact analysis let us achieve good precision and recall in change impact analysis.","V. Musco and A. Carette and M. Monperrus and P. Preux",2016,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Evolving Requirements-to-Code Trace Links across Versions of a Software System","Trace links provide critical support for numerous software engineering activities including safety analysis, compliance verification, test-case selection, and impact prediction. However, as the system evolves over time, there is a tendency for the quality of trace links to degrade into a tangle of inaccurate and untrusted links. This is especially true with the links between source-code and upstream artifacts such as requirements - because developers frequently refactor and change code without updating the links. We present TLE (Trace Link Evolver), a solution for automating the evolution of trace links as changes are introduced to source code. We use a set of heuristics, open source tools, and information retrieval methods to detect common change scenarios across different versions of software. Each change scenario is then associated with a set of link evolution heuristics which are used to evolve trace links. We evaluate our approach through a controlled experiment and also through applying it across 27 releases of the Cassandra Database System. Results show that the trace links evolved using our approach are significantly more accurate than those generated using information retrieval alone.","M. Rahimi and W. Goss and J. Cleland-Huang",2016,"[""IEEE""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Reducing Attack Surface via Executable Transformation","Modern software development and deployment practices encourage complexity and bloat while unintentionally sacrificing efficiency and security. A major driver in this is the overwhelming emphasis on programmers' productivity. The constant demands to speed up development while reducing costs have forced a series of individual decisions and approaches throughout software engineering history that have led to this point. The current state-of-the-practice in the field is a patchwork of architectures and frameworks, packed full of features in order to appeal to: the greatest number of people, obscure use cases, maximal code reuse, and minimal developer effort. The Office of Naval Research (ONR) Total Platform Cyber Protection (TPCP) program seeks to de-bloat software binaries late in the life-cycle with little or no access to the source code or the development process.","S. Mertoguno and R. Craven and D. Koller and M. Mickelson",2018,"[""IEEE""]","Rejeitado: CR9, CR8","Rejeitado: CR9, CR8, CR10"
"A notation for non-linear program edits","We present a visual notation to support the understanding and reasoning about program edits. The graph-based representation directly supports a number of editing operations beyond those offered by a typical, linear program-edit model and makes obvious otherwise hidden states the code can reach through selectively undoing or redoing changes.","M. Erwig and K. Smeltzer and K. Xu",2014,"[""IEEE""]","Rejeitado: CR11, CR12","Rejeitado: CR11, CR12"
"Using edit distance and junction feature to detect and recognize arrow road marking","Arrow road markings usually appear on freeway surface and they convey important navigation information to autonomous driving. But detecting and recognizing them is a tough task because they suffer from numerous deviations like objects' interference and themselves' abrasion, etc. We therefore propose a novel local junction feature (L-junction) to describe each road marking as a junction string, different deviation is dispersed into different junction. We encode those junctions within a range as the same code. To measure the similarity between detected junction string and ground truth junction string, we design a weighted edit distance strategy and assign different deviation with different weight so that our framework is robust enough to deviations in arrow road marking but sensitive to non- arrow road markings' deviations. To test our framework, we collect three freeway datasets with our self-driving car: clean/dirty arrow road marking images (300 images respectively), a video dataset (arrow road marking and non-road marking images (670 images)). Another deep learning framework (Boosting+Convolutional Deep Neural Network (CDNN)) is also implemented for comparison. Extensive experimental results well demonstrate the superior performance of our framework.","uhang He and hi Chen and ifeng Pan and ai Ni",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Deploying an enterprise-class Software Lifecycle Management solution for Test Program Sets","Information Technology tools for Software Lifecycle Management have advanced in recent years to enable organizations to implement a more centralized, enterprise-level lifecycle management solution. Despite these tools having become commonly used in the software development world, the Test Program Set (TPS) community remained behind the times. This paper presents the history and implementation of TPS Lifecycle Management as deployed by the TPS community for the Consolidated Automated Support System (CASS) Family of Testers (FoT). Prior to any efforts to enact centralized TPS lifecycle (i.e. configuration) management, TPS owners controlled their software in a variety of ways. The results of their efforts could be characterized as barely effective and often problematic. The initial attempt at solving the TPS lifecycle management problem was to mandate the use of a commercially available product which was not well supported by the developer of the application. That, coupled with both political issues and technical problems, relegated this system to becoming simply a repository for data. After addressing many of the problems with the initial solution and moving to a more widely-used product, the next iteration of TPS lifecycle management has been much more successful. It has become a legitimate enterprise solution providing both version control and implementing change management principles to include traceability between software change requests and released code. Many of the specific design details are discussed in the paper and include: a flexible workflow, a defined code branching strategy, an ownership and protection scheme and a process for release. The paper concludes with a detailed example of how TPS lifecycle management has benefitted the CASS team and, in particular, those who provide support of TPSs. It also shows how those benefits can be attained by other TPS support teams looking to effectively manage their test programs on other types of Automatic Test Equipment.","T. W. Davis and G. S. Kane",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8, CR10","Rejeitado: CR8, CR10"
"Exploiting tree structures for classifying programs by functionalities","Analyzing source code to solve software engineering problems such as fault prediction, cost, and effort estimation always receives attention of researchers as well as companies. The traditional approaches are based on machine learning, and software metrics obtained by computing standard measures of software projects. However, these methods have faced many challenges due to limitations of using software metrics which were not enough to capture the complexity of programs. The aim of this paper is to apply several natural language processing techniques, which deal with software engineering problems by exploring information of programs' abstract syntax trees (ASTs) instead of software metrics. To speed up computational time, we propose a pruning tree technique to eliminate redundant branches of ASTs. In addition, the k-Nearest Neighbor (kNN) algorithm was adopted to compare with other methods whereby the distance between programs is measured by using the tree edit distance (TED) and the Levenshtein distance. These algorithms are evaluated based on the performance of solving 104-label program classification problem. The experiments show that due to the use of appropriate data structures although kNN is a simple machine learning algorithm, the classifiers achieve the promising results.","Viet Anh Phan and Ngoc Phuong Chau and Minh Le Nguyen",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Keynote 1","Customizable computing has been of interest to the research community for over three decades. The interest has intensified in the recent years as the power and energy become a significant limiting factor to the computing industry. For example, the energy consumed by the datacenters of some large internet service providers is well over 109 Kilowatt-hours. FPGA-based acceleration has shown 10-100X performance/energy efficiency over the general-purpose processors in many applications. With Intel's $17B acquisition of Altera completed in December 2015, customizable computing is going from advanced research projects into mainstream computing technologies. In this talk, I shall first present several successful examples of customizable computing from my lab on CPU+FPGA platforms in multiple application domains, including medical imaging, machine learning, and computational genomics. Programming effort remains to be a serious challenge. So, the second part of my talk discusses our ongoing effort in automating compilation with source-code level transformation and optimization coupled with high-level synthesis, as well as developing efficient runtime support for scheduling and transparent resource management for integration of FPGAs for cloud-scale acceleration.","J. Cong",2016,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR10, CR8"
"Reconciling the past and the present: An empirical study on the application of source code transformations to automatically rejuvenate Java programs","Software systems change frequently over time, either due to new business requirements or technology pressures. Programming languages evolve in a similar constant fashion, though when a language release introduces new programming constructs, older constructs and idioms might become obsolete. The coexistence between newer and older constructs leads to several problems, such as increased maintenance efforts and steeper learning curve for developers. In this paper we present a RASCAL Java transformation library that evolves legacy systems to use more recent programming language constructs (such as multi-catch and lambda expressions). In order to understand how relevant automatic software rejuvenation is, we submitted 2462 transformations to 40 open source projects via the GitHub pull request mechanism. Initial results show that simple transformations, for instance the introduction of the diamond operator, are more likely to be accepted than transformations that change the code substantially, such as refactoring enhanced for loops to the newer functional style.","R. Dantas and A. Carvalho and D. Marcílio and L. Fantin and U. Silva and W. Lucas and R. Bonifácio",2018,"[""IEEE"",""Engineering Village""]","Aceito: CA3","Aceito: CA3"
"Ensemble techniques for software change prediction: A preliminary investigation","Predicting the classes more likely to change in the future helps developers to focus on the more critical parts of a software system, with the aim of preventively improving its maintainability. The research community has devoted a lot of effort in the definition of change prediction models, i.e., models exploiting a machine learning classifier to relate a set of independent variables to the change-proneness of classes. Besides the good performances of such models, key results of previous studies highlight how classifiers tend to perform similarly even though they are able to correctly predict the change-proneness of different code elements, possibly indicating the presence of some complementarity among them. In this paper, we aim at analyzing the extent to which ensemble methodologies, i.e., machine learning techniques able to combine multiple classifiers, can improve the performances of change-prediction models. Specifically, we empirically compared the performances of three ensemble techniques (i.e., Boosting, Random Forest, and Bagging) with those of standard machine learning classifiers (i.e., Logistic Regression and Naive Bayes). The study was conducted on eight open source systems and the results showed how ensemble techniques, in some cases, perform better than standard machine learning approaches, even if the differences among them is small. This requires the need of further research aimed at devising effective methodologies to ensemble different classifiers.","G. Catolino and F. Ferrucci",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"A Source Code Recommender System to Support Newcomers","Newcomers in a software development project often need assistance to complete their first tasks. Then a mentor, an experienced member of the team, usually teaches the newcomers what they need to complete their tasks. But, to allocate an experienced member of a team to teach a newcomer during a long time is neither always possible nor desirable, because the mentor could be more helpful doing more important tasks. During the development the team interacts with a version control system, bug tracking and mailing lists, and all these tools record data creating the project memory. Recommender systems can use the project memory to help newcomers in some tasks answering their questions, thus in some cases the developers do not need a mentor. In this paper we present Mentor, a recommender system to help newcomers to solve change requests. Mentor uses the Prediction by Partial Matching (PPM) algorithm and some heuristics to analyze the change requests, and the version control data, and recommend potentially relevant source code that will help the developer in the change request solution. We did three experiments to compare the PPM algorithm with the Latent Semantic Indexing (LSI). Using PPM we achieved results for recall rate between 37% and 66.8%, and using LSI the results were between 20.3% and 51.6%.","Y. Malheiros and A. Moraes and C. Trindade and S. Meira",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8"
"An Implementation of Just-in-Time Fault-Prone Prediction Technique Using Text Classifier","Since the fault prediction is an important technique to help allocating software maintenance effort, much research on fault prediction has been proposed so far. The goal of these studies is applying their prediction technique to actual software development. In this paper, we implemented a prototype fault-prone module prediction tool using a text-filtering based technique named ""Fault-Prone Filtering"". Our tool aims to show the result of fault prediction for each change (i.e., Commits) as a probability that a source code file to be faulty. The result is shown on a Web page and easy to track the histories of prediction. A case study performed on three open source projects shows that our tool could detect 90 percent of the actual fault modules (i.e., The recall of 0.9) with the accuracy of 0.67 and the precision of 0.63 on average.","K. Mori and O. Mizuno",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Machine Learning-Based Detection of Open Source License Exceptions","From a legal perspective, software licenses govern the redistribution, reuse, and modification of software as both source and binary code. Free and Open Source Software (FOSS) licenses vary in the degree to which they are permissive or restrictive in allowing redistribution or modification under licenses different from the original one(s). In certain cases, developers may modify the license by appending to it an exception to specifically allow reuse or modification under a particular condition. These exceptions are an important factor to consider for license compliance analysis since they modify the standard (and widely understood) terms of the original license. In this work, we first perform a large-scale empirical study on the change history of over 51K FOSS systems aimed at quantitatively investigating the prevalence of known license exceptions and identifying new ones. Subsequently, we performed a study on the detection of license exceptions by relying on machine learning. We evaluated the license exception classification with four different supervised learners and sensitivity analysis. Finally, we present a categorization of license exceptions and explain their implications.","C. Vendome and M. Linares-Vásquez and G. Bavota and M. Di Penta and D. German and D. Poshyvanyk",2017,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"A case study of program comprehension effort and technical debt estimations","This paper describes a case study of using developer activity logs as indicators of a program comprehension effort by analyzing temporal sequences of developer actions (e.g., navigation and edit actions). We analyze developer activity data spanning 109,065 events and 69 hours of work on a medium-sized industrial application. We examine potential correlations between different measures of developer activity, code change metrics and code smells to gain insight into questions that could direct future technical debt interest estimation. To gain more insights into the data, we follow our analysis with commit message analysis and a developer interview. Our results indicate that developer activity as an estimate of program comprehension effort is correlated with both change proneness and static metrics for code smells.","V. Singh and L. L. Pollock and W. Snipes and N. A. Kraft",2016,"[""IEEE""]","Rejeitado: CR7","Rejeitado: CR7"
"A data-driven color feature learning scheme for image retrieval","This paper addresses content based image retrieval based on color features. Several previous works have addressed color based image retrieval based on hand-crafted features. In this paper, a data-driven learning framework is proposed for generating color based signatures. To obtain the features, a linear transformation is learned from the pixel values based on its reconstruction error. Using this linear transformation, the original pixel values are transformed into a higher dimensional space. In the higher dimensional space, a dictionary is learned to obtain the sparse codes of the pixels. A max pooling strategy is used to obtain the dominant color features of a region and the final feature vector for an image is obtained by concatenating the pooled features. We evaluate our approach following the standard evaluation criteria for the INRIA Holidays and University of Kentucky Benchmark datasets. The approach is compared with several baselines such as histograms in RGB, HSV, YUV and Lab color spaces and several other color based features proposed for addressing this problem. Our approach shows competitive results on these datasets and outperforms all the baselines.","R. Rama Varior and G. Wang",2015,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Automatic Identification of Important Clones for Refactoring and Tracking","Code cloning is a controversial software engineering practice due to contradictory claims regarding its impacts on software evolution and maintenance. While a number of studies identify some positive aspects of code clones, there is strong empirical evidence of some negative impacts of clones too. Focusing on the issues related to clones researchers suggest to manage code clones through detection, refactoring, and tracking. However, all clones in a software system are not suitable for refactoring or tracking. Thus, it is important to identify which clones we should consider for refactoring and which clones should be considered for tracking. In this research work we apply the concept of evolutionary coupling to identify clones that are important for refactoring or tracking. By mining software evolution history, we determine and analyze constrained association rules of clone fragments that evolved following a particular change pattern called Similarity Preserving Change Pattern and are important from the perspective of refactoring and tracking. According to our investigation with rigorous manual analysis on thousands of revisions of six diverse subject systems covering two programming languages, overall 13.20% of all clones in a software system are important candidates for refactoring, and overall 10.27% of all clones are important candidates for tracking. Our implemented system can automatically identify these important candidates and thus, can help us in better maintenance of code clones in terms of refactoring and tracking.","M. Mondal and C. K. Roy and K. A. Schneider",2014,"[""IEEE"",""Engineering Village""]","Aceito: CA3","Aceito: CA3"
"Impact Analysis Using Static Execute After in WebKit","Insufficient propagation of changes causes the majority of regression errors in heavily evolving software systems. Impact analysis of a particular change can help identify those parts of the system that also need to be investigated and potentially propagate the change. A static code analysis technique called Static Execute After can be used to automatically infer such impact sets. The method is safe and comparable in precision to more detailed analyses. At the same time it is significantly more efficient, hence we could apply it to different large industrial systems, including the open source Web Kit project. We overview the benefits of the method, its existing implementations, and present our experiences in adapting the method to such a complex project. Finally, using this particular analysis on the Web Kit project, we verify whether applying the method we can actually predict the required change propagation and hence reduce regression errors. We report on the properties of the resulting impact sets computed for the change history, and their relationship to the actual fixes required. We looked at actual defects provided by the regression test suite along with their fixes taken from the version control repository, and compared these fixes to the predicted impact sets computed at the changes that caused the failing tests. The results show that the method is applicable for the analysis of the system, and that the impact sets can predict the required changes in a fair amount of cases, but that there are still open issues for the improvement of the method.","J. J´sz and L. Schrettner and Á. Beszédes and C. Osztrogon´c and T. Gyimóthy",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"The Impact of Human Discussions on Just-in-Time Quality Assurance: An Empirical Study on OpenStack and Eclipse","In order to spot defect-introducing code changes during review before they are integrated into a project's version control system, a variety of defect prediction models have been designed. Most of these models focus exclusively on source code properties, like the number of added or deleted lines, or developer-related measures like experience. However, a code change is only the outcome of a much longer process, involving discussions on an issue report and review discussions on (different versions of) a patch. % Ignoring the characteristics of these activities during prediction is unfortunate, since Similar to how body language implicitly can reveal a person's real feelings, the length, intensity or positivity of these discussions can provide important additional clues about how risky a particular patch is or how confident developers and reviewers are about the patch. In this paper, we build logistic regression models to study the impact of the characteristics of issue and review discussions on the defect-proneness of a patch. Comparison of these models to conventional source code-based models shows that issue and review metrics combined improve precision and recall of the explanatory models up to 10%. Review time and issue discussion lag are amongst the most important metrics, having a positive (i.e., increasing) relation with defect-proneness.","P. Tourani and B. Adams",2016,"[""IEEE""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Automatically Extracting Instances of Code Change Patterns with AST Analysis","A code change pattern represents a kind of recurrent modification in software. For instance, a known code change pattern consists of the change of the conditional expression of an if statement. Previous work has identified different change patterns. Complementary to the identification and definition of change patterns, the automatic extraction of pattern instances is essential to measure their empirical importance. For example, it enables one to count and compare the number of conditional expression changes in the history of different projects. In this paper we present a novel approach for search patterns instances from software history. Our technique is based on the analysis of Abstract Syntax Trees (AST) files within a given commit. We validate our approach by counting instances of 18 change patterns in 6 open-source Java projects.","M. Martinez and L. Duchien and M. Monperrus",2013,"[""IEEE"",""Engineering Village""]","Aceito: CA3","Aceito: CA3"
"A dataset from change history to support evaluation of software maintenance tasks","Approaches that support software maintenance need to be evaluated and compared against existing ones, in order to demonstrate their usefulness in practice. However, oftentimes the lack of well-established sets of benchmarks leads to situations where these approaches are evaluated using different datasets, which results in biased comparisons. In this data paper we describe and make publicly available a set of benchmarks from six Java applications, which can be used in the evaluation of various software engineering (SE) tasks, such as feature location and impact analysis. These datasets consist of textual description of change requests, the locations in the source code where they were implemented, and execution traces. Four of the benchmarks were already used in several SE research papers, and two of them are new. In addition, we describe in detail the methodology used for generating these benchmarks and provide a suite of tools in order to encourage other researchers to validate our datasets and generate new benchmarks for other subject software systems. Our online appendix: http://www.cs.wm.edu/semeru/data/msr13/.","B. Dit and A. Holtzhauer and D. Poshyvanyk and H. Kagdi",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Parallel Implementation of a Combined Moment Expansion and Spherical-Multipole Time-Domain Near-Field to Far-Field Transformation","A time-domain spherical-multipole near-to-far-field algorithm running in a parallelized code version is introduced in this paper. Such an approach is employed to obtain UWB antenna radiated fields and radiation patterns directly in time domain, which is more convenient to perform a unified characterization in time and frequency domains. We propose the use of the OpenMP, an application program interface that permits the code parallelization with almost no intervention. The results show that the proposed technique allows a code running 28 times faster when using a computer with 24 processors, each one with two threads, when compared with the sequential one. It also suggested a moment expansion technique to improve the accuracy and computational efficiency when using Gaussian pulse excitation when compared with the Fourier transform.","G. L. Ramos and C. G. Rego and A. R. Fonseca",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Why are Commits Being Reverted?: A Comparative Study of Industrial and Open Source Projects","Software development is a cyclic process of integrating new features while introducing and fixing defects. During development, commits that modify source code files are uploaded to version control systems. Occasionally, these commits need to be reverted, i.e., the code changes need to be completely backed out of the software project. While one can often speculate about the purpose of reverted commits (e.g., the commit may have caused integration or build problems), little empirical evidence exists to substantiate such claims. The goal of this paper is to better understand why commits are reverted in large software systems. To that end, we quantitatively and qualitatively study two proprietary and four open source projects to measure: (1) the proportion of commits that are reverted, (2) the amount of time that commits that are eventually reverted linger within a codebase, and (3) the most frequent reasons why commits are reverted. Our results show that 1%-5% of the commits in the studied systems are reverted. Those commits that are eventually reverted linger within the studied codebases for 1-35 days (median). Furthermore, we identify 13 common reasons for reverting commits, and observe that the frequency of reverted commits of each reason varies broadly from project to project. A complementary qualitative analysis suggests that many reverted commits could have been avoided with better team communication and change awareness. Our findings made Sony Mobile's stakeholders aware that internally reverted commits can be reduced by paying more attention to their own changes. On the other hand, externally reverted commits could be minimized only if external stakeholders are involved to improve inter-company communication or requirements elicitation.","J. Shimagaki and Y. Kamei and S. McIntosh and D. Pursehouse and N. Ubayashi",2016,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Recommending Clones for Refactoring Using Design, Context, and History","Developers know that copy-pasting code (aka code cloning) is often a convenient shortcut to achieving a design goal, albeit one that carries risks to the code quality over time. However, deciding which, if any, clones should be eliminated within an existing system is a daunting task. Fixing a clone usually means performing an invasive refactoring, and not all clones may be worth the effort, cost, and risk that such a change entails. Furthermore, sometimes cloning fulfils a useful design role, and should not be refactored at al. And clone detection tools often return very large result sets, making it hard to choose which clones should be investigated and possibly removed. In this paper, we propose an automated approach to recommend clones for refactoring by training a decision tree-based classifier. We analyze more than 600 clone instances in three medium-to large-sized open source projects, and we collect features that are associated with the source code, the context, and the history of clone instances. Our approach achieves a precision of around 80% in recommending clone refactoring instances for each target system, and similarly good precision is achieved in cross-project evaluation. By recommending which clones are appropriate for refactoring, our approach allows for better resource allocation for refactoring itself after obtaining clone detection results, and can thus lead to improved clone management in practice.","W. Wang and M. W. Godfrey",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"A comparative analysis of evolutionary algorithms for the prediction of software change","Change-proneness prediction of software components has become a significant research area wherein the quest for the best classifier still persists. Although numerous statistical and Machine Learning (ML) techniques have been presented and employed in the past literature for an efficient generation of change-proneness prediction models, evolutionary algorithms, on the other hand, remain vastly unexamined and unaddressed for this purpose. Bearing this in mind, this research work targets to probe the potency of six evolutionary algorithms for developing such change prediction models, specifically for source code files. We employ apposite object oriented metrics to construct four software datasets from four consecutive releases of a software project. Furthermore, the prediction capability of the selected evolutionary algorithms is evaluated, ranked and compared against two statistical classifiers using the Wilcoxon signed rank test and Friedman statistical test. On the basis of the results obtained from the experiments conducted in this article, it can be ascertained that the evolutionary algorithms possess a capability for predicting change-prone files with high accuracies, sometimes even higher than the selected statistical classifiers.","L. Kaur and A. Mishra",2018,"[""IEEE""]","Aceito: CA6, CA7, CA3","Aceito: CA6, CA7, CA3"
"Insight into a method co-change pattern to identify highly coupled methods: An empirical study","In this paper, we describe an empirical study of a unique method co-change pattern that has the potential to pinpoint design deficiency in a software system. We automatically identify this pattern by inspecting the method co-change history using reasonable constraints on method association rules. We also investigate the effect of code clones on the method co-changes identified according to the pattern, because there is a common intuition that clone fragments from the same clone class often require corresponding changes to ensure they remain consistent with each other. According to our in-depth investigation on hundreds of revisions of seven open-source software systems considering three types of clones (Type 1, Type 2, Type 3), our identified pattern helps us detect methods that are logically coupled with multiple other methods and that exhibit a significantly higher modification frequency than other methods. We call the methods detected by the pattern MMCGs (Methods appearing in Multiple Commit Groups) considering the pattern semantic. MMCGs can be considered as the candidates for restructuring in order to minimize coupling as well as to reduce the change-proneness of a software system. According to our observation, code clones have a significant effect on method co-changes as well as on MMCGs. We believe that clone refactoring can help us minimize evolutionary coupling among methods.","M. Mondal and C. K. Roy and K. A. Schneider",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"[Research Paper] Periodic Developer Metrics in Software Defect Prediction","Defect prediction studies have proposed several data-driven approaches, and recently, this field has put more emphasis on whether the people factor is associated software defects. Developer metrics can capture experience, code ownership, coding skills and techniques, and commit activities. These metrics have so far been measured at a specified snapshot of the codebase although developer's knowledge on a source module could change over time. In this paper, we propose to measure periodic developer experience with regard to contextual knowledge on files and directories. We extract periodic experience metrics capturing the previous activities of developers on source files and investigate the explanatory effect of these metrics on defects. We also use activity-based (churn) metrics to observe the performance of both metric types on defect prediction. We used two large-scale open source projects, Lucene and Jackrabbit, for model evaluation. We calculate periodic developer experience metrics and churn metrics at two granularity levels: file level and commit level. We build the models using five popular machine learning algorithms in defect prediction literature. The models with the two best performing algorithms are assessed in terms of Precision, Recall, False Positive Rate, and F-measure. The set of metrics that explains software defects the best is also identified using correlation-based feature selection method. Results show that periodic developer experience metrics extracted at file level are good merits for defect prediction, accompanied with churn. When there is not enough data to extract the contextual knowledge of developers on source files, churn metrics play an important role on defect prediction.","S. Ozcan Kini and A. Tosun",2018,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Characterizing the roles of classes and their fault-proneness through change metrics","Many approaches to determine the fault-proneness of code artifacts rely on historical data of and about these artifacts. These data include the code and how it was changed over time, and information about the changes from version control systems. Each of these can be considered at different levels of granularity. The level of granularity can substantially influence the estimated fault-proneness of a code artifact. Typically, the level of detail oscillates between releases and commits on the one hand, and single lines of code and whole files on the other hand. Not every information may be readily available or feasible to collect at every level, though, nor does more detail necessarily improve the results. Our approach is based on time series of changes in method-level dependencies and churn on a commit-to-commit basis for two systems, Spring and Eclipse. We identify sets of classes with distinct properties of the time series of their change histories. We differentiate between classes based on temporal patterns of change. Based on this differentiation, we show that our measure of structural change in concert with its complement, churn, effectively indicates fault-proneness in classes. We also use windows on time series to select sets of commits and show that changes over short amounts of time do effectively indicate the fault-proneness of classes.","M. Steff and B. Russo",2012,"[""IEEE"",""ACM"",""Engineering Village""]","Aceito: CA0, CA4","Aceito: CA4"
"Tool for detecting standardwise differences in C++ legacy code","Programming languages are continuously evolving as the experiences are accumulated, developers face new problems and other requirements such as increasing support for multi-threading is emerging. These changes are reflected in new language standards and compiler versions. Although these changes are carefully planned to keep reverse compatibility with previous versions to keep the syntax and the semantics of earlier written code, sometimes languages break this rule. In case of silent semantic changes, when the earlier written code is recompiled with the new version, this is especially harmful. The new C++11 standard introduced major changes in the core language. This changes are widely believed to be reverse compatible, i.e. a simple recompilation of earlier written code will keep the old semantics. Recently we found examples that the backward compatibility between language versions is broken. The previously written code compiled with a new C++ compiler may change the program behaviour without any compiler diagnostic message. In a large code base such issues are very hard to catch by manual inspection, therefore some automatic tool support is required for this purpose. In this paper we propose a tool support to detect such backward incompatibilities in C++. The basic idea is to parse the source code using different standards, and then compare the abstract syntax trees. We implemented a proof of concept prototype tool to demonstrate our idea based on the LLVM/Clang compiler infrastructure.","T. Brunner and N. Pataki and Z. Porkolab",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Visualizing a Tangled Change for Supporting Its Decomposition and Commit Construction","Developers often save multiple kinds of source code edits into a commit in a version control system, producing a tangled change, which is difficult to understand and revert. However, its separation using an existing sequence-based change representation is tough. We propose a new visualization technique to show the details of a tangled change and align its component edits in a tree structure for expressing multiple groups of changes. Our technique is combined with utilizing refactoring detection and change relevance calculation techniques for constructing the structural tree. Our combination allows us to divide the change into several associations. We have implemented a tool and conducted a controlled experiment with industrial developers to confirm its usefulness and efficiency. Results show that by using our tool with tree visualization, the subjects could understand and decompose tangled changes easier, faster, and higher accuracy than the baseline file list visualization.","S. Sothornprapakorn and S. Hayashi and M. Saeki",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Improved query reformulation for concept location using CodeRank and document structures","During software maintenance, developers usually deal with a significant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique-ACER-that takes an initial query, identifies appropriate search terms from the source code using a novel term weight-CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with five closely related existing techniques in query reformulation not only validates our empirical findings but also demonstrates the superiority of our technique.","M. M. Rahman and C. K. Roy",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Method-level bug prediction","Researchers proposed a wide range of approaches to build effective bug prediction models that take into account multiple aspects of the software development process. Such models achieved good prediction performance, guiding developers towards those parts of their system where a large share of bugs can be expected. However, most of those approaches predict bugs on file-level. This often leaves developers with a considerable amount of effort to examine all methods of a file until a bug is located. This particular problem is reinforced by the fact that large files are typically predicted as the most bug-prone. In this paper, we present bug prediction models at the level of individual methods rather than at file-level. This increases the granularity of the prediction and thus reduces manual inspection efforts for developers. The models are based on change metrics and source code metrics that are typically used in bug prediction. Our experiments-performed on 21 Java open-source (sub-)systems-show that our prediction models reach a precision and recall of 84% and 88%, respectively. Furthermore, the results indicate that change metrics significantly outperform source code metrics.","E. Giger and M. D'Ambros and M. Pinzger and H. C. Gall",2012,"[""IEEE""]","Aceito: CA5, CA6","Aceito: CA5, CA6"
"Quantum cost realization of new reversible gates with transformation based synthesis technique","Reversible computing appears to be promising due its applications in emerging technologies. To compute any reversible function it is necessary to build the system with reversible gates. Simplified version of transformation technique [3,5] to synthesize new reversible gates with Fredkin and Toffoli gates network is presented in this paper. Basic and bidirectional transformation algorithms with an example are illustrated, which uncovers every step of the algorithm. The same example is used for both the algorithms to give clarity on the difference in the efficiency of the algorithms. Simple pseudo code is also presented to illustrate the steps of the algorithm. The best quantum cost obtained is listed in this paper.","Jayashree H. V and V. K. Agrawal and Shishir Bharadwaj N",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR10, CR8"
"Understanding regression failures through test-passing and test-failing code changes","Debugging and isolating changes responsible for regression test failures are some of the most challenging aspects of modern software development. Automatic bug localization techniques reduce the manual effort developers spend examining code, for example, by focusing attention on the minimal subset of recent changes that results in the test failure, or on changes to components with most dependencies or highest churn. We observe that another subset of changes is worth the developers' attention: the complement of the maximal set of changes that does not produce the failure. While for simple, independent source-code changes, existing techniques localize the failure cause to a small subset of those changes, we find that when changes interact, the failure cause is often in our proposed subset and not in the subset existing techniques identify. In studying 45 regression failures in a large, open-source project, we find that for 87% of those failures, the complement of the maximal passing set of changes is different from the minimal failing set of changes, and that for 78% of the failures, our technique identifies relevant changes ignored by existing work. These preliminary results suggest that combining our ideas with existing techniques, as opposed to using either in isolation, can improve the effectiveness of bug localization tools.","R. Sukkerd and I. Beschastnikh and J. Wuttke and S. Zhang and Y. Brun",2013,"[""IEEE""]","Rejeitado: CR12, CR12","Rejeitado: CR12"
"A Replication Study: Just-in-Time Defect Prediction with Ensemble Learning","Just-in-time defect prediction, which is also known as change-level defect prediction, can be used to efficiently allocate resources and manage project schedules in the software testing and debugging process. Just-in-time defect prediction can reduce the amount of code to review and simplify the assignment of developers to bug fixes. This paper reports a replicated experiment and an extension comparing the prediction of defect-prone changes using traditional machine learning techniques and ensemble learning. Using datasets from six open source projects, namely Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL we replicate the original approach to verify the results of the original experiment and use them as a basis for comparison for alternatives in the approach. Our results from the replicated experiment are consistent with the original. The original approach uses a combination of data preprocessing and a two-layer ensemble of decision trees. The first layer uses bagging to form multiple random forests. The second layer stacks the forests together with equal weights. Generalizing the approach to allow the use of any arbitrary set of classifiers in the ensemble, optimizing the weights of the classifiers, and allowing additional layers, we apply a new deep ensemble approach, called deep super learner, to test the depth of the original study. The deep super learner achieves statistically significantly better results than the original approach on five of the six projects in predicting defects as measured by F1 score.","S. Young and T. Abdou and A. Bener",2018,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Will This Bug-Fixing Change Break Regression Testing?","Context: Software source code is frequently changed for fixing revealed bugs. These bug-fixing changes might introduce unintended system behaviors, which are inconsistent with scenarios of existing regression test cases, and consequently break regression testing. For validating the quality of changes, regression testing is a required process before submitting changes during the development of software projects. Our pilot study shows that 48.7% bug-fixing changes might break regression testing at first run, which means developers have to run regression testing at least a couple of times for 48.7% changes. Such process can be tedious and time consuming. Thus, before running regression test suite, finding these changes and corresponding regression test cases could be helpful for developers to quickly fix these changes and improve the efficiency of regression testing. Goal: This paper proposes bug- fixing change impact prediction (BFCP), for predicting whether a bug-fixing change will break regression testing or not before running regression test cases, by mining software change histories. Method: Our approach employs the machine learning algorithms and static call graph analysis technique. Given a bug-fixing change, BFCP first predicts whether it will break existing regression test cases; second, if the change is predicted to break regression test cases, BFCP can further identify the might-be-broken test cases. Results: Results of experiments on 552 real bug-fixing changes from four large open source projects show that BFCP could achieve prediction precision up to 83.3%, recall up to 92.3%, and F-score up to 81.4%. For identifying the might-be-broken test cases, BFCP could achieve 100% recall.","X. Tang and S. Wang and K. Mao",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Synthesizing Object Transformation for Dynamic Software Updating","Dynamic software updating (DSU) can upgrade a running program on-the-fly by directly replacing the in-memory code and reusing existing runtime state (e.g., heap objects) for the updated execution. Additionally, it is usually necessary to transform the runtime state into a proper new state to avoid inconsistencies that arise during runtime states reuse among different versions of a program. However, such transformations mostly require human efforts, which is time-consuming and error-prone. This paper presents AOTES, an approach to automating object transformations for dynamic updating of Java programs. AOTES tries to generate the new state by re-executing a method invocation history and leverages symbolic execution to synthesize the history from the current object state without any recording. We evaluated AOTES on software updates taken from Apache Tomcat, Apache FTP Server and Apache SSHD Server. Experimental results show that AOTES successfully handled 47 of 57 object transformations of 18 updated classes, while two state-of-the-art approaches only handled 11 and 6 of 57, respectively.","T. Gu and X. Ma and C. Xu and Y. Jiang and C. Cao and J. Lü",2017,"[""IEEE"",""ACM""]","Rejeitado: CR10","Rejeitado: CR10"
"Leveraging historical co-change information for requirements traceability","Requirements traceability (RT) links requirements to the corresponding source code entities, which implement them. Information Retrieval (IR) based RT links recovery approaches are often used to automatically recover RT links. However, such approaches exhibit low accuracy, in terms of precision, recall, and ranking. This paper presents an approach (CoChaIR), complementary to existing IR-based RT links recovery approaches. CoChaIR leverages historical co-change information of files to improve the accuracy of IR-based RT links recovery approaches. We evaluated the effectiveness of CoChaIR on three datasets, i.e., iTrust, Pooka, and SIP Communicator. We compared CoChaIR with two different IR-based RT links recovery approaches, i.e., vector space model and Jensen-Shannon divergence model. Our study results show that CoChaIR significantly improves precision and recall by up to 12.38% and 5.67% respectively; while decreasing the rank of true positive links by up to 48% and reducing false positive links by up to 44%.","N. Ali and F. Jaafar and A. E. Hassan",2013,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Semantic Versioning versus Breaking Changes: A Study of the Maven Repository","For users of software libraries or public programming interfaces (APIs), backward compatibility is a desirable trait. Without compatibility, library users will face increased risk and cost when upgrading their dependencies. In this study, we investigate semantic versioning, a versioning scheme which provides strict rules on major versus minor and patch releases. We analyze seven years of library release history in Maven Central, and contrast version identifiers with actual incompatibilities. We find that around one third of all releases introduce at least one breaking change, and that this figure is the same for minor and major releases, indicating that version numbers do not provide developers with information in stability of interfaces. Additionally, we find that the adherence to semantic versioning principles has only marginally increased over time. We also investigate the use of deprecation tags and find out that methods get deleted without applying deprecated tags, and methods with deprecated tags are never deleted. We conclude the paper by arguing that the adherence to semantic versioning principles should increase because it provides users of an interface with a way to determine the amount of rework that is expected when upgrading to a new version.","S. Raemaekers and A. van Deursen and J. Visser",2014,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10"
"Code Vault","So, what has changed since that first NATO software engineering conference in 1968? Depending on your point of view, nothing much has changed, or everything has changed. The part that didn't change much is that we still struggle with writing code that's robust enough to trust. The part that has changed dramatically is the performance of the hardware that runs our code. This article is part of a theme issue on software engineering's 50th anniversary.","G. J. Holzmann",2018,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10"
"A Support Vector Machine Based Approach for Code Smell Detection","Code smells may be introduced in software due to market rivalry, work pressure deadline, improper functioning, skills or inexperience of software developers. Code smells indicate problems in design or code which makes software hard to change and maintain. Detecting code smells could reduce the effort of developers, resources and cost of the software. Many researchers have proposed different techniques like DETEX for detecting code smells which have limited precision and recall. To overcome these limitations, a new technique named as SVMCSD has been proposed for the detection of code smells, based on support vector machine learning technique. Four code smells are specified namely God Class, Feature Envy, Data Class and Long Method and the proposed technique is validated on two open source systems namely ArgoUML and Xerces. The accuracy of SVMCSD is found to be better than DETEX in terms of two metrics, precision and recall, when applied on a subset of a system. While considering the entire system, SVMCSD detect more occurrences of code smells than DETEX.","A. Kaur and S. Jain and S. Goel",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"SPCP-Miner: A tool for mining code clones that are important for refactoring or tracking","Code cloning has both positive and negative impacts on software maintenance and evolution. Focusing on the issues related to code cloning, researchers suggest to manage code clones through refactoring and tracking. However, it is impractical to refactor or track all clones in a software system. Thus, it is essential to identify which clones are important for refactoring and also, which clones are important for tracking. In this paper, we present a tool called SPCP-Miner which is the pioneer one to automatically identify and rank the important refactoring as well as important tracking candidates from the whole set of clones in a software system. SPCP-Miner implements the existing techniques that we used to conduct a large scale empirical study on SPCP clones (i.e., the clones that evolved following a Similarity Preserving Change Pattern called SPCP). We believe that SPCP-Miner can help us in better management of code clones by suggesting important clones for refactoring or tracking.","M. Mondal and C. K. Roy and K. A. Schneider",2015,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"An Empirical Examination of the Relationship between Code Smells and Merge Conflicts","Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both ""smelly"" and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.","I. Ahmed and C. Brindescu and U. A. Mannan and C. Jensen and A. Sarma",2017,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8"
"Classifying Code Comments in Java Mobile Applications","Developers adopt code comments for different reasons such as document source codes or change program flows. Due to a variety of use scenarios, code comments may impact on readability and maintainability. In this study, we investigate how developers of 5 open-source mobile applications use code comments to document their projects. Additionally, we evaluate the performance of two machine learning models to automatically classify code comments. Initial results show marginal differences between desktop and mobile applications.","L. Pascarella",2018,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Lase: Locating and applying systematic edits by learning from examples","Adding features and fixing bugs often require systematic edits that make similar, but not identical, changes to many code locations. Finding all the relevant locations and making the correct edits is a tedious and error-prone process for developers. This paper addresses both problems using edit scripts learned from multiple examples. We design and implement a tool called LASE that (1) creates a context-aware edit script from two or more examples, and uses the script to (2) automatically identify edit locations and to (3) transform the code. We evaluate LASE on an oracle test suite of systematic edits from Eclipse JDT and SWT. LASE finds edit locations with 99% precision and 89% recall, and transforms them with 91% accuracy. We also evaluate LASE on 37 example systematic edits from other open source programs and find LASE is accurate and effective. Furthermore, we confirmed with developers that LASE found edit locations which they missed. Our novel algorithm that learns from multiple examples is critical to achieving high precision and recall; edit scripts created from only one example produce too many false positives, false negatives, or both. Our results indicate that LASE should help developers in automating systematic editing. Whereas most prior work either suggests edit locations or performs simple edits, LASE is the first to do both for nontrivial program edits.","N. Meng and M. Kim and K. S. McKinley",2013,"[""IEEE"",""ACM"",""Engineering Village""]","Aceito: CA0, CA3","Aceito: CA3"
"Learning Feature Representations from Change Dependency Graphs for Defect Prediction","Given the heterogeneity of the data that can be extracted from the software development process, defect prediction techniques have focused on associating different sources of data with the introduction of faulty code, usually relying on handcrafted features. While these efforts have generated considerable progress over the years, little attention has been given to the fact that the performance of any predictive model depends heavily on the representation of the data used, and that different representations can lead to different results. We consider this a relevant problem, as it could be affecting directly the efforts towards generating safer software systems. Therefore, we propose to study the impact of the representation of the data in defect prediction models. To this end, we focus on the use of developer activity data, from which we structure dependency graphs. Then, instead of manually generating features, such as network metrics, we propose two models inspired by recent advances in representation learning which are able to automatically generate feature representations from graph data. These new representations are compared against manually crafted features for defect prediction in real world software projects. Our results show that automatically learned features are competitive, reaching increments in prediction performance up to 13%.","P. Loyola and Y. Matsuo",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Landfill: An Open Dataset of Code Smells with Public Evaluation","Code smells are symptoms of poor design and implementation choices that may hinder code comprehension and possibly increase change- and fault-proneness of source code. Several techniques have been proposed in the literature for detecting code smells. These techniques are generally evaluated by comparing their accuracy on a set of detected candidate code smells against a manually-produced oracle. Unfortunately, such comprehensive sets of annotated code smells are not available in the literature with only few exceptions. In this paper we contribute (i) a dataset of 243 instances of five types of code smells identified from 20 open source software projects, (ii) a systematic procedure for validating code smell datasets, (iii) LANDFILL, a Web-based platform for sharing code smell datasets, and (iv) a set of APIs for programmatically accessing LANDFILL's contents. Anyone can contribute to Landfill by (i) improving existing datasets (e.g., Adding missing instances of code smells, flagging possibly incorrectly classified instances), and (ii) sharing and posting new datasets. Landfill is available at www.sesa.unisa.it/landfill/, while the video demonstrating its features in action is available at http://www.sesa.unisa.it/tools/landfill.jsp.","F. Palomba and D. Di Nucci and M. Tufano and G. Bavota and R. Oliveto and D. Poshyvanyk and A. De Lucia",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Do historical metrics and developers communication aid to predict change couplings?","Developers have contributed to open-source projects by forking the code and submitting pull requests. Once a pull request is submitted, interested parties can review the set of changes, discuss potential modifications, and even push additional commits if necessary. Mining artifacts that were committed together during history of pull-requests makes it possible to infer change couplings among these artifacts. Supported by the Conway's Law, whom states that “organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations”, we hypothesize that social network analysis (SNA) is able to identify strong and weak change dependencies. In this paper, we used statistical models relying on centrality, ego, and structural holes metrics computed from communication networks to predict co-changes among files included in pull requests submitted to the Ruby on Rails project. To the best of our knowledge, this is the first study to employ SNA metrics to predict change dependencies from Github projects.","I. S. Wiese and R. T. Kuroda and R. Ré and R. S. Bulhóes and G. A. Oliva and M. A. Gerosa",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Extracting artifact lifecycle models from metadata history","Software developers and managers make decisions based on the understanding they have of their software systems. This understanding is both built up experientially and through investigating various software development artifacts. While artifacts can be investigated individually, being able to summarize characteristics about a set of development artifacts can be useful. In this paper we propose lifecycle models as an effective way to gain an understanding of certain development artifacts. Lifecycle models capture the dynamic nature of how various development artifacts change over time in a graphical form that can be easily understood and communicated. Lifecycle models enables reasoning of the underlying processes and dynamics of the artifacts being analyzed. In this paper we describe how lifecycle models can be generated and demonstrate how they can be applied to the code review process of a development project.","O. Baysal and O. Kononenko and R. Holmes and M. W. Godfrey",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8"
"Configuration selection using code change impact analysis for regression testing","Configurable systems that let users customize system behaviors are becoming increasingly prevalent. Testing a configurable system with all possible configurations is very expensive and often impractical. For a single version of a configurable system, sampling approaches exist that select a subset of configurations from the full configuration space for testing. However, when a configurable system changes and evolves, existing approaches for regression testing select all configurations that are used to test the old versions for testing the new version. As demonstrated in our experiments, this retest-all approach for regression testing configurable systems turns out to be highly redundant. To address this redundancy, we propose a configuration selection approach for regression testing. Formally, given two versions of a configurable system, S (old) and S' (new), and given a set of configurations C<sub>S</sub> for testing S, our approach selects a subset C<sub>S'</sub> of C<sub>S</sub> for regression testing S'. Our study results on two open source systems and a large industrial system show that, compared to the retest-all approach, our approach discards 15% to 60% of configurations as redundant. Our approach also saves 20% to 55% of the regression testing time, while retaining the same fault detection capability and code coverage of the retest-all approach.","X. Qu and M. Acharya and B. Robinson",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Effectiveness of discriminative training and feature transformation for reverberated and noisy speech","Automatic speech recognition in the presence of non-stationary interference and reverberation remains a challenging problem. The 2<sup>nd</sup> `CHiME' Speech Separation and Recognition Challenge introduces a new and difficult task with time-varying reverberation and non-stationary interference including natural background speech, home noises, or music. This paper establishes baselines using state-of-the-art ASR techniques such as discriminative training and various feature transformation on the middle-vocabulary sub-task of this challenge. In addition, we propose an augmented discriminative feature transformation that introduces arbitrary features to a discriminative feature transformation. We present experimental results showing that discriminative training of model parameters and feature transforms is highly effective for this task, and that the augmented feature transformation provides some preliminary benefits. The training code will be released as an advanced ASR baseline.","Y. Tachioka and S. Watanabe and J. R. Hershey",2013,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"An Exploratory Study on the Relationship between Changes and Refactoring","Refactoring aims at improving the internal structure of a software system without changing its external behavior. Previous studies empirically assessed, on the one hand, the benefits of refactoring in terms of code quality and developers' productivity, and on the other hand, the underlying reasons that push programmers to apply refactoring. Results achieved in the latter investigations indicate that besides personal motivation such as the responsibility concerned with code authorship, refactoring is mainly performed as a consequence of changes in the requirements rather than driven by software quality. However, these findings have been derived by surveying developers, and therefore no software repository study has been carried out to corroborate the achieved findings. To bridge this gap, we provide a quantitative investigation on the relationship between different types of code changes (i.e., Fault Repairing Modification, Feature Introduction Modification, and General Maintenance Modification) and 28 different refactoring types coming from 3 open source projects. Results showed that developers tend to apply a higher number of refactoring operations aimed at improving maintainability and comprehensibility of the source code when fixing bugs. Instead, when new features are implemented, more complex refactoring operations are performed to improve code cohesion. Most of the times, the underlying reasons behind the application of such refactoring operations are represented by the presence of duplicate code or previously introduced self-admitted technical debts.","F. Palomba and A. Zaidman and R. Oliveto and A. De Lucia",2017,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Using Co-change Histories to Improve Bug Localization Performance","A large open source software (OSS) project receives many bug reports on a daily basis. Bug localization techniques automatically pinpoint source code fragments that are relevant to a bug report, thus enabling faster correction. Even though many bug localization methods have been introduced, their performance is still not efficient. In this research, we improved on existing bug localization methods by taking into account co-change histories. We conducted experiments on two OSS datasets, the Eclipse SWT 3.1 project and the Android ZXing project. We validated our approach by evaluating effectiveness compared to the state-of-the-art approach Bug Locator. In the Eclipse SWT 3.1 project, our approach reliably identified source code that should be fixed for a bug in 72.46% of the total bugs, while Bug Locator identified only 51.02%. In the Android ZXing project, our approach identified 85.71%, while Bug Locator identified 60%.","C. Tantithamthavorn and A. Ihara and K. Matsumoto",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Domino Effect: Move More Methods Once a Method is Moved","Software refactoring is a popular technology to improve the design of existing source code, and thus it is widely used to facilitate software evolution. Moving methods is one of the most popular refactorings. It helps to reduce coupling between classes and to improve cohesion of involved classes. However, it is difficult to manually identify such methods that should be moved. Consequently, a number of approaches and tools have been proposed to identify such methods based on source code metrics, change history, and textual information. In this paper we propose a new way to identify methods that should be moved. Whenever a method is moved, the approach checks other methods within the same class, and suggests to move the one with the greatest similarity and strongest relationship with the moved method. The rational is that similar and closely related methods should be moved together. The approach has been evaluated on open-source applications by comparing the recommended move method refactorings against refactoring histories of the involved applications. Our evaluation results show that the approach is accurate in recommending methods to be moved (average precision 76%) and in recommending destinations for such methods (average precision 83%). Our evaluation results also show that for a substantial percentage (27%) of move method refactorings, the proposed approach succeeds in identifying additional refactoring opportunities.","H. Liu and Y. Wu and W. Liu and Q. Liu and C. Li",2016,"[""IEEE""]","Rejeitado: CR7","Rejeitado: CR7"
"GiveMe Trace: A Software Evolution Traceability Support Tool","Traceability is a key factor in the analysis of the changes that software undergoes throughout its evolution. The main purpose of analysis is to minimize the side effects of these changes and, when it is made to the source at a lower level of abstraction (methods) and in an integrated manner, it can provide more accurate data in order to support decision making. This article presents the GiveMe Trace tool, integrated with a multiple view interactive environment that, among other features, can generate information about the traceability between source code and artifacts its different versions. This information is based on software versions analysis from software repository. As a result, occurrences of changes in classes or methods are shown. A proof of concept was carried out through which repositories versions of two distinct real projects were analyzed. At the end, it was possible to obtain evidences on the feasibility of the use of GiveMe Trace to support traceability between the source code and versions.","C. A. Silveira Lelis and J. Fernandes Tavares and M. A. Pereira Araujo and J. M. Nazar David",2016,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Confusion Detection in Code Reviews","Code reviews are an important mechanism for assuring quality of source code changes. Reviewers can either add general comments pertaining to the entire change or pinpoint concerns or shortcomings about a specific part of the change using inline comments. Recent studies show that reviewers often do not understand the change being reviewed and its context.Our ultimate goal is to identify the factors that confuse code reviewers and understand how confusion impacts the efficiency and effectiveness of code review(er)s. As the first step towards this goal we focus on the identification of confusion in developers' comments. Based on an existing theoretical framework categorizing expressions of confusion, we manually classify 800 comments from code reviews of the Android project. We observe that confusion can be reasonably well-identified by humans: raters achieve moderate agreement (Fleiss' kappa 0.59 for the general comments and 0.49 for the inline ones). Then, for each kind of comment we build a series of automatic classifiers that, depending on the goals of the further analysis, can be trained to achieve high precision (0.875 for the general comments and 0.615 for the inline ones), high recall (0.944 for the general comments and 0.988 for the inline ones), or substantial precision and recall (0.696 and 0.542 for the general comments and 0.434 and 0.583 for the inline ones, respectively). These results motivate further research on the impact of confusion on the code review process. Moreover, other researchers can employ the proposed classifiers to analyze confusion in other contexts where software development-related discussions occur, such as mailing lists.","F. Ebert and F. Castor and N. Novielli and A. Serebrenik",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Model Change Detection With the MDL Principle","We are concerned with the issue of detecting model changes in probability distributions. We specifically consider the strategies based on the minimum description length (MDL) principle. We theoretically analyze their basic performance from the two aspects: data compression and hypothesis testing. From the view of data compression, we derive a new bound on the minimax regret for model changes. Here, the mini-max regret is defined as the minimum of the worst-case code-length relative to the least normalized maximum likelihood code-length over all model changes. From the view of hypothesis testing, we reduce the model change detection into a simple hypothesis testing problem. We thereby derive upper bounds on error probabilities for the MDL-based model change test. The error probabilities are valid for finite sample size and are related to the information-theoretic complexity as well as the discrepancy measure of the hypotheses to be tested.","K. Yamanishi and S. Fukushima",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The employment of online self-learning coding course to enhance logical reasoning ability for fifth and sixth grader students","This paper was aimed to investigate the influence of online coding courses on logical reasoning ability towards fifth and sixth graders. Students used ""Code.org online programming courses"" about three months. Through ""Raven's Standard Progressive Matrices"" before and after testing to understand the change of students' logical reasoning ability in the experiment. And based on ""Technology Acceptance Model""' researcher edit ""Code.org Learning Attitude Questionnaire"" to explore the attitude of students towards the courses and the factors of influencing students learning online courses.","K. Yang and C. Tsai and L. Lee and C. Chen",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Changing the Code® culture","Much attention is given to educating and instilling a fundamental safety culture in the electrical industry's skilled workforce; as it should. However, given the prevalence of electrically-related fires, injuries, and deaths, one could argue that there is a lack of electrical safety culture among the general public; or at least a lack in sufficient knowledge and behaviors to effectively reduce electrical incidents. This paper will explore the shortcomings of the public's electrical safety culture and their impact. Particular attention will be devoted to the National Electrical Code® (NEC) and its role in saving lives and preventing injury. With the introduction of each new version, the National Electrical Code faces resistance as adoption is delayed, enforcement is lax, and general apathy prevails to varying degrees across the United States. Additionally this paper will outline opportunities to educate and engage the public to help the NEC gain widespread support while grooming safety advocates from all industries and backgrounds.","B. Brenner",2015,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Assessing Change Proneness at the Architecture Level: An Empirical Validation","Change proneness is a characteristic of software artifacts that represents their probability to change in future. Change proneness can be assessed at different levels of granularity, ranging from classes to modules. Although change proneness can be successfully assessed at the source code level (i.e., methods and classes), it remains rather unexplored for architectures. Additionally, the methods that have been introduced at the source code level are not directly transferable to the architecture level. In this paper, we propose and empirically validate a method for assessing the change proneness of architectural modules. Assessing change proneness at the level of architectural modules requires information from two sources: (a) the history of changes in the module, as a proxy of how frequently the module itself undergoes changes; and (b) the dependencies with other modules that affect the probability of a change being propagated from one module to the other. To validate the proposed approach, we performed a case study on five open-source projects. Specifically, we compared the accuracy of the proposed approach to the use of software package metrics as assessors of modules change proneness, based on the 1061-1998 IEEE Standard. The results suggest that compared to examined metrics, the proposed method is a better assessor of change proneness. Therefore, we believe that the method and accompanying tool can effectively aid architects during software maintenance and evolution.","E. M. Arvanitou and A. Ampatzoglou and K. Tzouvalidis and A. Chatzigeorgiou and P. Avgeriou and I. Deligiannis",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Vectorization-Aware Loop Optimization with User-Defined Code Transformations","The cost of maintaining an application code would significantly increase if the application code is branched into multiple versions, each of which is optimized for a different architecture. In this work, default and vector versions of a realworld application code are refactored to be a single version, and the differences between the versions are expressed as user-defined code transformations. As a result, application developers can maintain only the single version, and transform it to its vector version just before the compilation. Although code optimizations for a vector processor are sometimes different from those for other processors, application developers can enjoy the performance of the vector processor without increasing the code complexity. Evaluation results demonstrate that vectorization-aware loop optimization for a vector processor can be expressed as user-defined code transformation rules, and thereby significantly improve the performance of a vector processor without major code modifications.","H. Takizawa and T. Reimann and K. Komatsu and T. Soga and R. Egawa and A. Musa and H. Kobayashi",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Untangling fine-grained code changes","After working for some time, developers commit their code changes to a version control system. When doing so, they often bundle unrelated changes (e.g., bug fix and refactoring) in a single commit, thus creating a so-called tangled commit. Sharing tangled commits is problematic because it makes review, reversion, and integration of these commits harder and historical analyses of the project less reliable. Researchers have worked at untangling existing commits, i.e., finding which part of a commit relates to which task. In this paper, we contribute to this line of work in two ways: (1) A publicly available dataset of untangled code changes, created with the help of two developers who accurately split their code changes into self contained tasks over a period of four months; (2) a novel approach, EpiceaUntangler, to help developers share untangled commits (aka. atomic commits) by using fine-grained code change information. EpiceaUntangler is based and tested on the publicly available dataset, and further evaluated by deploying it to 7 developers, who used it for 2 weeks. We recorded a median success rate of 91% and average one of 75%, in automatically creating clusters of untangled fine-grained code changes.","M. Dias and A. Bacchelli and G. Gousios and D. Cassou and S. Ducasse",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"CUDA multiclass change detection for remote sensing hyperspectral images using extended morphological profiles","The need for information of the Earth's surface is growing as it is the base for applications such as monitoring the land uses or performing environmental studies, for example. In this context the effective change detection (CD) among multitemporal datasets is a key process that must produce accurate results obtained by computationally efficient algorithms. Most of the CD methods are focused on binary detection (presence or absence of changes) or in the clustering of the different detected types of changes. In this paper, a CUDA scheme to perform pixel-based multiclass CD for hyperspectral datasets is introduced. The scheme combines multiclass CD with binary CD to obtain an accurate multiclass change map. The combination with the binary map contributes to reducing the execution time of the CUDA code. The binary CD is based on performing the difference among images based on Euclidean and Spectral Angle Mapper (SAM) distances and a later thresholding by Otsu's algorithm to detect the changed pixels. The multiclass CD begins with the fusion of the multitemporal data following with feature extraction by Principal Component Analysis (PCA) and incorporating spatial features by means of an Extended Morphological Profile (EMP). The resulting dataset is filtered using the binary CD map and classified pixel by pixel by the supervised algorithms Extreme Learning Machine (ELM) and Support Vector Machine (SVM). The scheme was validated in a non-synthetic multitemporal hyperspectral dataset.","J. López-Fandiño and D. B. Heras and F. Argüello and R. J. Duro",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR10, CR8"
"A longitudinal study of programmers' backtracking","Programming often involves reverting source code to an earlier state, which we call backtracking. We performed a longitudinal study of programmers' backtracking, analyzing 1,460 hours of fine-grained code editing logs collected from 21 people. Our analysis method keeps track of the change history of each abstract syntax tree node and looks for backtracking instances within each node. Using this method, we detected a total of 15,095 backtracking instances, which gives an average backtracking rate of 10.3/hour. The size of backtracking varied con-siderably, ranging from a single character to thousands of char-acters. 34% of the backtracking was performed by manually deleting or typing the desired code, and 9.5% of all backtracking was selective, meaning that it could not have been performed using the conventional undo command present in the IDE. The study results show that programmers need better backtracking tools, and also provide design implications for such tools.","Y. S. Yoon and B. A. Myers",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Malware Detection using Opcode Trigram Sequence with SVM","Malicious software also known as “Malware” is software that uses legitimate instructions or code to perform malicious actions. Malware poses a major threat for computer security and information security in general. Over the years, malware has evolved to the point that a single malware specimen can have hundreds or maybe thousands of variants using polymorphic and metamorphic transformation to change the signature of the malware variant in propagation. The common signature-based malware detection methods are no longer robust to detect these variants due to the alteration of code. Static analysis is required to obtain these signatures and anti-virus companies are required to propagate these signature updates to their software. A faster detection method is needed to compensate the exponentially increasing number of malware variants. Machine learning is a trending approach for malware detection. This removes the need to use signature-based detection and is also faster. Software companies do not need to propagate signatures as often. Machine learning algorithms using opcode sequences can recognise patterns in the malicious code that are not present in common signatures and classify them more efficiently. Therefore, a machine learning approach for malware detection should be adopted for faster and more efficient detection. Most research in malware detection using machine learning used static attributes such as network connections, processes spawned, hashes, etc., that were not that robust to changes. In this paper we introduced our novel approach in using trigrams and PE file attributes as features for malware detection. We took a text mining approach to make our detection method more robust to polymorphism and metamorphism. The instruction sequence for critical code in malware on the assembly level is basically the same across malware families. We used opcode trigram sequences as the main feature for our machine learning algorithm. We used Support Vector Machine(SVM) as our classifying algorithm which is a discriminative classifier model that gives a definite decision whether the predicted outcome belongs to the learned class or not. The above shows our novel approach that enabled us to get higher detection rates with less features.","A. I. Elkhawas and N. Abdelbaki",2018,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10"
"Investigating the Change-Proneness of Service Patterns and Antipatterns","Like any other software systems, service-based systems (SBSs) evolve frequently to accommodate new user requirements. This evolution may degrade their design and implementation and may cause the introduction of common bad practice solutions -- antipatterns -- in opposition to patterns which are good solutions to common recurring design problems. We believe that the degradation of the design of SBSs does not only affect the clients of the SBSs but also the maintenance and evolution of the SBSs themselves. This paper presents the results of an empirical study that aimed to quantify the impact of service patterns and antipatterns on the maintenance and evolution of SBSs. We measure the maintenance effort of a service implementation in terms of the number of changes and the size of changes (i.e., Code churns) performed by developers to maintain and evolve the service, two effort metrics that have been widely used in software engineering studies. Using data collected from the evolutionary history of the SBS FraSCAti, we investigate if (1) services involved in patterns require less maintenance effort, (2) services detected as antipatterns require more maintenance effort than other services, and (3) if some particular service antipatterns are more change-prone than others. Results show that (1) services involved in patterns require less maintenance effort, but not at statistically significant level, (2) services detected as antipatterns require significantly more maintenance effort than non-antipattern services, and (3) services detected as God Component, Multi Service, and Service Chain antipatterns are more change-prone (i.e., Require more maintenance effort) than the services involved in other antipatterns. We also analysed the relation between object-oriented code smells and service patterns/antipatterns and found a significant difference in the proportion of code smells contained in the implementations of service patterns and antipatterns.","F. Palma and L. An and F. Khomh and N. Moha and Y. Guéhéneuc",2014,"[""IEEE"",""Engineering Village""]","Aceito: CA4, CA6","Aceito: CA4, CA6"
"Towards model checking of computer games with Java PathFinder","We show that Java source code of computer games can be checked for bugs such as uncaught exceptions by the model checker Java PathFinder (JPF). To model check Java games, we need to tackle the state space explosion problem and handle native calls. To address those two challenges we use our extensions of JPF, jpf-probabilistic and jpf-nhandler. The former deals with the randomization in the source code of the game, which is a cause of the state space explosion problem. The latter handles native calls automatically. We show how JPF enhanced with our extensions can check games such as the text based game Hamurabi and a graphics based version of rock-paper-scissors.","N. Shafiei and F. van Breugel",2013,"[""IEEE"",""ACM""]","Rejeitado: CR10","Rejeitado: CR10"
"Fatiguing STDP: Learning from spike-timing codes in the presence of rate codes","Spiking neural networks (SNNs) could play a key role in unsupervised machine learning applications, by virtue of strengths related to learning from the fine temporal structure of event-based signals. However, some spike-timing-related strengths of SNNs are hindered by the sensitivity of spike-timing-dependent plasticity (STDP) rules to input spike rates, as fine temporal correlations may be obstructed by coarser correlations between firing rates. In this article, we propose a spike-timing-dependent learning rule that allows a neuron to learn from the temporally-coded information despite the presence of rate codes. Our long-term plasticity rule makes use of short-term synaptic fatigue dynamics. We show analytically that, in contrast to conventional STDP rules, our fatiguing STDP (FSTDP) helps learn the temporal code, and we derive the necessary conditions to optimize the learning process. We showcase the effectiveness of FSTDP in learning spike-timing correlations among processes of different rates in synthetic data. Finally, we use FSTDP to detect correlations in real-world weather data from the United States in an experimental realization of the algorithm that uses a neuro-morphic hardware platform comprising phase-change memristive devices. Taken together, our analyses and demonstrations suggest that FSTDP paves the way for the exploitation of the spike-based strengths of SNNs in real-world applications.","T. Moraitis and A. Sebastian and I. Boybat and M. Le Gallo and T. Tuma and E. Eleftheriou",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR10, CR8"
"Class Level Code Summarization Based on Dependencies and Micro Patterns","Modifications in any software need to be carries out on various entities. Change in one entity may force some changes in many other dependent entities. Complete understanding of entities and there dependencies are highly desirable to carry out modifications in an efficient manner. Automated summarization of classes in object oriented software can be a good step in this direction. This paper proposes a natural language summary based code summarization of those java classes which are more change prone. Code summary is generated by using concept of micro patterns and change proneness is identified by computing different kinds of dependencies among classes. A threshold is decided to identify the classes which are more sensitive to change. The empirical evaluation of some open source classes has been carried out which clearly indicates the usefulness of the proposed work.","M. Malhotra and J. Kumar Chhabra",2018,"[""IEEE""]","Rejeitado: CR9, CR10","Rejeitado: CR9, CR10"
"Experience report: Evaluating the effectiveness of decision trees for detecting code smells","Developers continuously maintain software systems to adapt to new requirements and to fix bugs. Due to the complexity of maintenance tasks and the time-to-market, developers make poor implementation choices, also known as code smells. Studies indicate that code smells hinder comprehensibility, and possibly increase change- and fault-proneness. Therefore, they must be identified to enable the application of corrections. The challenge is that the inaccurate definitions of code smells make developers disagree whether a piece of code is a smell or not, consequently, making difficult creation of a universal detection solution able to recognize smells in different software projects. Several works have been proposed to identify code smells but they still report inaccurate results and use techniques that do not present to developers a comprehensive explanation how these results have been obtained. In this experimental report we study the effectiveness of the Decision Tree algorithm to recognize code smells. For this, it was applied in a dataset containing 4 open source projects and the results were compared with the manual oracle, with existing detection approaches and with other machine learning algorithms. The results showed that the approach was able to effectively learn rules for the detection of the code smells studied. The results were even better when genetic algorithms are used to pre-select the metrics to use.","L. Amorim and E. Costa and N. Antunes and B. Fonseca and M. Ribeiro",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"RCLinker: Automated Linking of Issue Reports and Commits Leveraging Rich Contextual Information","Links between issue reports and their corresponding commits in version control systems are often missing. However, these links are important for measuring the quality of various parts of a software system, predicting defects, and many other tasks. A number of existing approaches have been designed to solve this problem by automatically linking bug reports to source code commits via comparison of textual information in commit messages with textual contents in the bug reports. Yet, the effectiveness of these techniques is oftentimes sub optimal when commit messages are empty or only contain minimum information, this particular problem makes the process of recovering trace ability links between commits and bug reports particularly challenging. In this work, we aim at improving the effectiveness of existing bug linking techniques by utilizing rich contextual information. We rely on a recently proposed tool, namely Change Scribe, which generates commit messages containing rich contextual information by using a number of code summarization techniques. Our approach then extracts features from these automatically generated commit messages and bug reports and inputs them into a classification technique that creates a discriminative model used to predict if a link exists between a commit message and a bug report. We compared our approach, coined as RCLinker (Rich Context Linker), to MLink, which is an existing state-of-the-art bug linking approach. Our experiment results on bug reports from 6 software projects show that RCLinker can outperform MLink in terms of F-measure by 138.66%.","T. B. Le and M. Linares-Vasquez and D. Lo and D. Poshyvanyk",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Privacy preservation in interaction history on integrated development environments","The interaction history in a software development environment allows us to analyze how developers change source code and how they use tools on the integrated development environment. Sharing the interaction history with tool providers increases the chances that developers obtain better tools. However, the interaction history sometimes contains privacy-sensitive information, which is an obstacle in collecting and using the interaction history. As an attempt to tackle this issue, this paper proposes a technique to replace sensitive text in a recorded interaction history. This paper describes the proposed technique, its current implementation, the results of a preliminary survey on how potential privacy-sensitive information exists in recorded interaction histories, and how privacy issues in sharing interaction histories can be ameliorated.","T. Omori",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Detecting and Preventing Program Inconsistencies under Database Schema Evolution","Nowadays, data-intensive applications tend to access their underlying database in an increasingly dynamic way. The queries that they send to the database server are usually built at runtime, through String concatenation, or Object-Relational-Mapping (ORM) frameworks. This level of dynamicity significantly complicates the task of adapting application programs to database schema changes. Failing to correctly adapt programs to an evolving database schema results in program inconsistencies, which in turn may cause program failures. In this paper, we present a tool-supported approach, that allows developers to (1) analyze how the source code and database schema co-evolved in the past and (2) simulate a database schema change and automatically determine the set of source code locations that would be impacted by this change. Developers are then provided with recommendations about what they should modify at those source code locations in order to avoid inconsistencies. The approach has been designed to deal with Java systems that use dynamic data access frameworks such as JDBC, Hibernate and JPA. We motivate and evaluate the proposed approach, based on three real-life systems of different size and nature.","L. Meurice and C. Nagy and A. Cleve",2016,"[""IEEE""]","Rejeitado: CR11","Rejeitado: CR11"
"Are Fix-Inducing Changes a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Prediction","Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.","S. McIntosh and Y. Kamei",2018,"[""IEEE""]","Rejeitado: CR8","Rejeitado: CR8"
"Detecting, Tracing, and Monitoring Architectural Tactics in Code","Software architectures are often constructed through a series of design decisions. In particular, architectural tactics are selected to satisfy specific quality concerns such as reliability, performance, and security. However, the knowledge of these tactical decisions is often lost, resulting in a gradual degradation of architectural quality as developers modify the code without fully understanding the underlying architectural decisions. In this paper we present a machine learning approach for discovering and visualizing architectural tactics in code, mapping these code segments to tactic traceability patterns, and monitoring sensitive areas of the code for modification events in order to provide users with up-to-date information about underlying architectural concerns. Our approach utilizes a customized classifier which is trained using code extracted from fifty performance-centric and safety-critical open source software systems. Its performance is compared against seven off-the-shelf classifiers. In a controlled experiment all classifiers performed well; however our tactic detector outperformed the other classifiers when used within the larger context of the Hadoop Distributed File System. We further demonstrate the viability of our approach for using the automatically detected tactics to generate viable and informative messages in a simulation of maintenance events mined from Hadoop's change management system.","M. Mirakhorli and J. Cleland-Huang",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Automatically Documenting Software Artifacts","Software artifacts constantly change during evolution and maintenance of software systems. One critical artifact that developers need to be able to maintain during evolution and maintenance of software systems is up-to-date and complete documentation. However, recent studies on the co-evolution of comments and code showed that the comments are rarely maintained or updated when the respective source code is changed. In order to understand developer practices regarding documenting two kinds of software artifacts, unit test cases and database-related operations, we designed two empirical studies both composed of (i) an online survey with contributors of open source projects and (ii) a mining-based analysis of method comments in these projects. Later, motivated by the findings of the studies, we proposed two novel approaches. UnitTestScribe is an approach for automatically documenting test cases, while DBScribe is an approach for automatically documenting test cases. We evaluated our tools by means of an online survey with industrial developers and graduate students. In general, participants indicated that descriptions generated by our tools are complete, concise, and easy to read.","B. Li",2016,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Predicting defects using change genealogies","When analyzing version histories, researchers traditionally focused on single events: e.g. the change that causes a bug, the fix that resolves an issue. Sometimes however, there are indirect effects that count: Changing a module may lead to plenty of follow-up modifications in other places, making the initial change having an impact on those later changes. To this end, we group changes into change genealogies, graphs of changes reflecting their mutual dependencies and influences and develop new metrics to capture the spatial and temporal influence of changes. In this paper, we show that change genealogies offer good classification models when identifying defective source files: With a median precision of 73% and a median recall of 76%, change genealogy defect prediction models not only show better classification accuracies as models based on code complexity, but can also outperform classification models based on code dependency network metrics.","K. Herzig and S. Just and A. Rau and A. Zeller",2013,"[""IEEE"",""Engineering Village""]","Aceito: CA5, CA6","Aceito: CA5, CA6"
"[Journal First] Are Fix-Inducing Changes a Moving Target?: A Longitudinal Case Study of Just-in-Time Defect Prediction","Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.","S. McIntosh and Y. Kamei",2018,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10"
"Towards Automatic Generation of Short Summaries of Commits","Committing to a version control system means submitting a software change to the system. Each commit can have a message to describe the submission. Several approaches have been proposed to automatically generate the content of such messages. However, the quality of the automatically generated messages falls far short of what humans write. In studying the differences between auto-generated and human-written messages, we found that 82% of the human-written messages have only one sentence, while the automatically generated messages often have multiple lines. Furthermore, we found that the commit messages often begin with a verb followed by an direct object. This finding inspired us to use a ""verb+object"" format in this paper to generate short commit summaries. We split the approach into two parts: verb generation and object generation. As our first try, we trained a classifier to classify a diff to a verb. We are seeking feedback from the community before we continue to work on generating direct objects for the commits.","S. Jiang and C. McMillan",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"An Empirical Validation of the Complexity of Code Changes and Bugs in Predicting the Release Time of Open Source Software","With the increasing popularity of open source software, the changes in source code are inevitable. These changes in code are due to feature enhancement, new feature introduction and bug repair or fixed. It is important to note that these changes can be quantified by using entropy based measures. The pattern of bug fixing scenario with complexity of code change is responsible for the next release as these changes will cover the number of requirements and fixes. In this paper, we are proposing a method to predict the next release problem based on the complexity of code change and bugs fixed. We applied multiple linear regression to predict the time of the next release of the product and measured the performance using different residual statistics, goodness of fit curve and R2. We observed from the results of multiple linear regression that the predicted value of release time is fitting well with the observed value of number of months for the next release.","K. K. Chaturvedi and P. Bedi and S. Misra and V. B. Singh",2013,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Automatic software architecture recovery: A machine learning approach","Automatically recovering functional architecture of the software can facilitate the developer's understanding of how the system works. In legacy systems, original source code is often the only available source of information about the system and it is very time consuming to understand source code. Current architecture recovery techniques either require heavy human intervention or fail to recover quality components. To alleviate these shortcomings, we propose use of machine learning techniques which use structural, runtime behavioral, domain, textual and contextual (e.g. code authorship, line co-change) features. These techniques will allow us to experiment with a large number of features of the software artifacts without having to establish a priori our own insights about what is important and what is not important. We believe this is a promising approach that may finally start to produce usable solutions to this elusive problem.","H. Sajnani",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Unsupervised deep hashing for large-scale visual search","Learning based hashing plays a pivotal role in large-scale visual search. However, most existing hashing algorithms tend to learn shallow models that do not seek representative binary codes. In this paper, we propose a novel hashing approach based on unsupervised deep learning to hierarchically transform features into hash codes. Within the heterogeneous deep hashing framework, the autoencoder layers with specific constraints are considered to model the nonlinear mapping between features and binary codes. Then, a Restricted Boltzmann Machine (RBM) layer with constraints is utilized to reduce the dimension in the hamming space. The experiments on the problem of visual search demonstrate the competitiveness of our proposed approach compared to the state of the art.","Z. Xia and X. Feng and J. Peng and A. Hadid",2016,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR10, CR8"
"Yestercode: Improving code-change support in visual dataflow programming environments","In this paper, we present the Yestercode tool for supporting code changes in visual dataflow programming environments. In a formative investigation of LabVIEW programmers, we found that making code changes posed a significant challenge. To address this issue, we designed Yestercode to enable the efficient recording, retrieval, and juxtaposition of visual dataflow code while making code changes. To evaluate Yestercode, we implemented our design as a prototype extension to the LabVIEW programming environment, and ran a user study involving 14 professional LabVIEW programmers that compared Yestercode-extended LabVIEW to the standard LabVIEW IDE. Our results showed that Yestercode users introduced fewer bugs during tasks, completed tasks in about the same time, and experienced lower cognitive loads on tasks. Moreover, participants generally reported that Yestercode was easy to use and that it helped in making change tasks easier.","A. Z. Henley and S. D. Fleming",2016,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Learning a cross-modal hashing network for multimedia search","In this paper, we propose a cross-modal hashing network (CMHN) method to learn compact binary codes for cross-modality multimedia search. Unlike most existing cross-modal hashing methods which learn a single pair of projections to map each example into a binary vector, we design a deep neural network to learn multiple pairs of hierarchical non-linear transformations, under which the nonlinear characteristics of samples can be well exploited and the modality gap is well reduced. Our model is trained under an iterative optimization procedure which learns a (1) unified binary code discretely and discriminatively through a classification-based hinge-loss criterion, and (2) cross-modal hashing network, one deep network for each modality, through minimizing the quantization loss between real-valued neural code and binary code, and maximizing the variance of the learned neural codes. Experimental results on two benchmark datasets show the efficacy of the proposed approach.","V. E. Liong and J. Lu and Y. Tan",2017,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"How do Multiple Pull Requests Change the Same Code: A Study of Competing Pull Requests in GitHub","GitHub is a widely used collaborative platform for global software development. A pull request plays an important role in bridging code changes with version controlling. Developers can freely and parallelly submit pull requests to base branches and wait for the merge of their contributions. However, several developers may submit pull requests to edit the same lines of code; such pull requests result in a latent collaborative conflict. We refer such pull requests that tend to change the same lines and remain open during an overlapping time period to as competing pull requests. In this paper, we conduct a study on 9,476 competing pull requests from 60 Java repositories in GitHub. The data are collected by mining pull requests that are submitted in 2017 from top Java projects with the most forks. We explore how multiple pull requests change the same code via answering four research questions, including the distribution of competing pull requests, the involved developers, the changed lines of code, and the impact on pull request integration. Our study shows that there indeed exist competing pull requests in GitHub: in 45 out of 60 repositories, over 31% of pull requests belong to competing pull requests; 20 repositories have more than 100 groups of competing pull requests, each of which is submitted by over five developers; 42 repositories have over 10% of competing pull requests with over 10 same lines of code. Meanwhile, we observe that attributes of competing pull requests do not have strong impacts on pull request integration, comparing with other types of pull requests. Our study provides a preliminary analysis for further research that aims to detect and eliminate conflicts among competing pull requests.","X. Zhang and Y. Chen and Y. Gu and W. Zou and X. Xie and X. Jia and J. Xuan",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"A coding style-based plagiarism detection","In this paper a plagiarism detection framework is proposed based on coding style. Furthermore, the typical style-based approach is improved to better detect plagiarism in programming codes. The plagiarism detection is performed in two phases: in the first phase the main features representing a coding style are extracted. In the second phase the extracted features are used in three different modules to detect the plagiarized codes and to determine the giver and takers of the codes. The extracted features for each code developer are kept in a history log, i.e. a user profile as his/her style of coding, and would be used to determine the change in coding style. The user profile allows the system to detect if a code is truly developed by the claimed developer or it is written by another person, having another style. Furthermore, the user profile allows determining the code giver and code taker when two codes are similar by comparing the codes' styles with the style of the programmers. Also if a code is copied from the internet or developed by a third party, then the style of who claims the ownership of the code is normally less proficient in coding than the third party and can be detected. The difference between the style levels is done through the style level checker module in the proposed framework. The proposed framework has been implemented and tested and the results are compared to Moss which shows comparable performance in detecting plagiarized codes.","S. Arabyarmohamady and H. Moradi and M. Asadpour",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Better backtracking support for programmers","Programmers often need to backtrack while coding, yet there is only limited support for backtracking in modern programming tools. Our study results confirmed the prevalence of backtracking and identified several problems programmers face while backtracking. To mitigate these problems, we are building an IDE plug-in aimed at providing better support for backtracking by combining a selective undo mechanism, novel visualizations, and code change history search features. We envision that this approach will help programmers perform backtracking tasks more easily.","Y. Yoon",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"An MDL-based change-detection algorithm with its applications to learning piecewise stationary memoryless sources","Kleinberg has proposed an algorithm for detecting bursts from a data sequence, which has turned out to be effective in the scenario of data mining, such as topic detection, change-detection. In this paper we extend Kleinberg's algorithm in an information-theoretic fashion to obtain a new class of algorithms and apply it into learning of piecewise stationary memoryless sources (PSMSs). The keys of the proposed algorithm are; 1) the parameter space is discretized so that discretization scale depends on the Fisher information, and 2) the optimal path over the discretized parameter space is efficiently computed using the dynamic programming method so that the sum of the data and parameter description lengths is minimized on the basis of the MDL principle. We prove that an upper bound on the total code-length for the proposed algorithm asymptotically matches Merhav's lower bound.","H. Kanazawa and K. Yamanishi",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9, CR10","Rejeitado: CR9, CR8, CR10"
"Face detection method based on histogram of sparse code in tree deformable model","Face detection is a challenging research area and crucial step of face detection system. Because of the factors of rotation, pose change, and complicated background, false faces also can be found in detection results. This paper puts forward a new approach based on the landmark localization to detect face image which includes various pose variation. Furthermore, the proposed histogram of sparse code-based method is very effective and it can capture global elastic and multi-view deformation which can be optimized easily. The proposed method achieved higher effectiveness and efficiency in comparison with the existing face detection methods on different data sets.","Q. Zhang and L. Zhou and W. Li and K. Ricanek and X. Li",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Collaborative bug triaging using textual similarities and change set analysis","Bug triaging assigns a bug report, which is also known as a work item, an issue, a task or simply a bug, to the most appropriate software developer for fixing or implementing it. However, this task is tedious, time-consuming and error-prone if not supported by effective means. Current techniques either use information retrieval and machine learning to find the most similar bugs already fixed and recommend expert developers, or they analyze change information stemming from source code to propose expert bug solvers. Neither technique combines textual similarity with change set analysis and thereby exploits the potential of the interlinking between bug reports and change sets. In this paper, we present our approach to identify potential experts by identifying similar bug reports and analyzing the associated change sets. Studies have shown that effective bug triaging is done collaboratively in a meeting, as it requires the coordination of multiple individuals, the understanding of the project context and the understanding of the specific work practices. Therefore, we implemented our approach on a multi-touch table to allow multiple stakeholders to interact simultaneously in the bug triaging and to foster their collaboration. In the current stage of our experiments we have experienced that the expert recommendations are more specific and useful when the rationale behind the expert selection is also presented to the users.","K. Kevic and S. C. Müller and T. Fritz and H. C. Gall",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8"
"Towards robust color recovery for high-capacity color QR codes","Color brings extra data capacity for QR codes, but it also brings tremendous challenges to the decoding because of color interference and illumination variation, especially for high-density QR codes. In this paper, we put forth a framework for high-capacity QR codes, HiQ, which optimizes the decoding algorithm for high-density QR codes to achieve robust and fast decoding on mobile devices, and adopts a learning-based approach for color recovery. Moreover, we propose a robust geometric transformation algorithm to correct the geometric distortion. We also provide a challenging color QR code dataset, CUHK-CQRC, which consists of 5390 high-density color QR code samples captured by different smartphones under different lighting conditions. Experimental results show that HiQ outperforms the baseline [1] by 286% in decoding success rate and 60% in bit error rate.","Z. Yang and Z. Cheng and C. C. Loy and W. C. Lau and C. M. Li and G. Li",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"SSPARED: Saliency and sparse code analysis for rare event detection in video","The problem of detecting rare and unusual events in video is critical to the analysis of large video datasets. Such events are identified as those occurrences within a sequence that cause a significant change in the scene. We propose to determine the significance of a frame, while preserving its compact representation, by introducing a saliency-driven dictionary learning technique. The derived sparse codes are then leveraged, together with the Kullback-Leibler divergence, in the design of a histogram-based metric that we use to evaluate the scene changes between consecutive frames. Our method, SSPARED, is compared with two state of the art methods for anomaly detection and shows significant improvement in detecting abnormal incidents and reduced false alarm generation.","R. Sarkar and A. Vaccari and S. T. Acton",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9, CR10","Rejeitado: CR9, CR10"
"The impact of tangled code changes","When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found up to 15% of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6% of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.","K. Herzig and A. Zeller",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Microcontroller Education: Do it Yourself, Reinvent the Wheel, Code to Learn","Microcontroller education has experienced tremendous change in recent years. This book attempts to keep pace with the most recent technology while holding an opposing attitude to the No Need to Reinvent the Wheel philosophy. The choice strategies are in agreement with the employment of today's flexible and low-cost Do-It-Yourself (DYI) microcontroller hardware, along with an embedded C programming approach able to be adapted by different hardware and software development platforms. Modern embedded C compilers employ built-in features for keeping programs short and manageable and, hence, speeding up the development process. However, those features eliminate the reusability of the source code among diverse systems. The recommended programming approach relies on the motto Code More to Learn Even More, and directs the reader toward a low-level accessibility of the microcontroller device. The examples addressed herein are designed to meet the demands of Electrical & Electronic Engineering discipline, where the microcontroller learning processes definitely bear the major responsibility. The programming strategies are in line with the two virtues of C programming language, that is, the adaptability of the source code and the low-level accessibility of the hardware system.","Dimosthenis E. Bolanakis",2017,"[""IEEE""]","Rejeitado: CR4","Rejeitado: CR4"
"Bayesian learning based multiuser detection for M2M communications with time-varying user activities","Machine-to-Machine (M2M) communication plays a significant role in supporting Internet of Thing (IoT). This paper is concerned about multiuser detection (MUD) for massive M2M supported by Low-Activity Code Division Multiple Access (LA-CDMA). In previous work, maximum likelihood (ML) and maximum a posterior probability (MAP) detectors have been developed for such system. The ML detector has exponential complexity, while the MAP detector requires perfect knowledge of user activity factor. In practice, the user activity factor may not be known and could change from time to time. To design MUD detectors addressing these problems, in this paper, we formulate multiple measurement vector (MMV) model for uplink LA-CDMA system with time-varying user activities. Since the transmitted signals have block sparse structure, we introduce the pattern coupled spare Bayesian learning (PCSBL) by using the neighbour coherence of each transmitted signal, which effectively solves the user activity factor unknown problem. Furthermore, we embed the generalized approximate message passing (GAMP) to PCSBL and develop a novel algorithm, called generalized approximate message passing pattern coupled sparse Bayesian learning (GAMP-PCSBL). The GAMP-PCSBL does not require activity factor either, and greatly reduces the computational complexity. Simulation results have shown that the proposed algorithms have superior recovery performance than the conventional algorithms.","X. Zhang and Y. Liang and J. Fang",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Identifying cryptographic functionality in Android applications","Mobile devices in corporate IT infrastructures are frequently used to process security-critical data. Over the past few years powerful security features have been added to mobile platforms. However, for legal and organisational reasons it is difficult to pervasively enforce using these features in consumer applications or Bring-Your-Own-Device (BYOD) scenarios. Thus application developers need to integrate custom implementations of security features such as encryption in security-critical applications. Our manual analysis of container applications and password managers has shown that custom implementations of cryptographic functionality often suffer from critical mistakes. During manual analysis, finding the custom cryptographic code was especially time consuming. Therefore, we present the Semdroid framework for simplifying application analysis of Android applications. Here, we use Semdroid to apply machine-learning techniques for detecting non-standard symmetric and asymmetric cryptography implementations. The identified code fragments can be used as starting points for subsequent manual analysis. Thus manual analysis time is greatly reduced. The capabilities of Semdroid have been evaluated on 98 password-safe applications downloaded from Google Play. Our evaluation shows the applicability of Semdroid and its potential to significantly improve future application analysis processes.","A. Oprisnik and D. Hein and P. Teufl",2014,"[""IEEE""]","Rejeitado: CR10, CR9","Rejeitado: CR10, CR9"
"An Empirical Study on Factors Impacting Bug Fixing Time","Fixing bugs is an important activity of the software development process. A typical process of bug fixing consists of the following steps: 1) a user files a bug report, 2) the bug is assigned to a developer, 3) the developer fixes the bug, 4) changed code is reviewed and verified, and 5) the bug is resolved. Many studies have investigated the process of bug fixing. However, to the best of our knowledge, none has explicitly analyzed the interval between bug assignment and the time when bug fixing starts. After a bug assignment, some developers will immediately start fixing the bug while others will start bug fixing after a long period. We are blind on developer's delays when fixing bugs. This paper explores such delays of developers through an empirical study on three open source software systems. We examine factors affecting bug fixing time along three dimensions: bug reports, source code involved in the fix, and code changes that are required to fix the bug. We further compare different factors by descriptive logistic regression models. Our results can help development teams better understand factors behind delays, and then improve bug fixing process.","F. Zhang and F. Khomh and Y. Zou and A. E. Hassan",2012,"[""IEEE""]","Rejeitado: CR11","Rejeitado: CR11"
"Nonlinear Discrete Hashing","In this paper, we propose a nonlinear discrete hashing approach to learn compact binary codes for scalable image search. Instead of seeking a single linear projection in most existing hashing methods, we pursue a multilayer network with nonlinear transformations to capture the local structure of data samples. Unlike most existing hashing methods that adopt an error-prone relaxation to learn the transformations, we directly solve the discrete optimization problem to eliminate the quantization error accumulation. Specifically, to leverage the similarity relationships between data samples and exploit the semantic affinities of manual labels, the binary codes are learned with the objective to: 1) minimize the quantization error between the original data samples and the learned binary codes; 2) preserve the similarity relationships in the learned binary codes; 3) maximize the information content with independent bits; and 4) maximize the accuracy of the predicted labels based on the binary codes. With an alternating optimization, the nonlinear transformation and the discrete quantization are jointly optimized in the hashing learning framework. Experimental results on four datasets including CIFAR10, MNIST, SUN397, and ILSVRC2012 demonstrate that the proposed approach is superior to several state-of-the-art hashing methods.","Z. Chen and J. Lu and J. Feng and J. Zhou",2017,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Comparison and Analysis of Governance Mechanisms Employed by Blockchain-Based Distributed Autonomous Organizations","One of the distinguishing features of blockchain-based Distributed Autonomous Organizations(DAO) is lack of a central authority. Changes to blockchain data is achieved through consensus amongst blockchain network participants, rather than through a central node's authoritative decision. Similarly, governance, i.e., changes to features and underlying source code, is achieved through a decentralized mechanism. As adoption of blockchain has increased, the need to evolve and adopt new features has grown. These changes highlight the mechanism by which the network, rather than a central node, makes decisions. One change in particular, proposed increases to the block size to address scalability limitations, has been particularly demonstrative of the governance mechanisms employed by disparate blockchains. For example, two digital currency projects, Bitcoin and Dash, employ significantly different governance mechanisms: the Dash Decentralized Governance By Blockchain (DGBB) process, and the Bitcoin Improvement Proposal (BIP) process, to decide what changes to make to their blockchains. Dash governance was able to decide to alter the block size in a matter of hours, while Bitcoin governance took several years to make the same decision. This paper evaluates the governance mechanisms of blockchain projects using the change in block size as an exemplar. Two prominent governance mechanisms are described, compared, and assessed based upon how effective they came to consensus and made the decision to change to support the disparate needs of stakeholders.","S. DiRose and M. Mansouri",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR9"
"RefDiff: Detecting Refactorings in Version Histories","Refactoring is a well-known technique that is widely adopted by software engineers to improve the design and enable the evolution of a system. Knowing which refactoring operations were applied in a code change is a valuable information to understand software evolution, adapt software components, merge code changes, and other applications. In this paper, we present RefDiff, an automated approach that identifies refactorings performed between two code revisions in a git repository. RefDiff employs a combination of heuristics based on static analysis and code similarity to detect 13 well-known refactoring types. In an evaluation using an oracle of 448 known refactoring operations, distributed across seven Java projects, our approach achieved precision of 100% and recall of 88%. Moreover, our evaluation suggests that RefDiff has superior precision and recall than existing state-of-the-art approaches.","D. Silva and M. T. Valente",2017,"[""IEEE"",""ACM"",""Engineering Village""]","Aceito: CA0, CA1, CA3","Aceito: CA1, CA3"
"SQA-Profiles: Rule-based activity profiles for Continuous Integration environments","Continuous Integration (CI) environments cope with the repeated integration of source code changes and provide rapid feedback about the status of a software project. However, as the integration cycles become shorter, the amount of data increases, and the effort to find information in CI environments becomes substantial. In modern CI environments, the selection of measurements (e.g., build status, quality metrics) listed in a dashboard does only change with the intervention of a stakeholder (e.g., a project manager). In this paper, we want to address the shortcoming of static views with so-called Software Quality Assessment (SQA) profiles. SQA-Profiles are defined as rule-sets and enable a dynamic composition of CI dashboards based on stakeholder activities in tools of a CI environment (e.g., version control system). We present a set of SQA-Profiles for project management committee (PMC) members: Bandleader, Integrator, Gatekeeper, and Onlooker. For this, we mined the commit and issue management activities of PMC members from 20 Apache projects. We implemented a framework to evaluate the performance of our rule-based SQA-Profiles in comparison to a machine learning approach. The results showed that project-independent SQA-Profiles can be used to automatically extract the profiles of PMC members with a precision of 0.92 and a recall of 0.78.","M. Brandtner and S. C. Müller and P. Leitner and H. C. Gall",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11, CR12","Rejeitado: CR11, CR12"
"Code reusability in cloud based ERP solutions","Cloud-based technology has created a new software abstraction layer above the implementation layers, and has therefore changed the way in which enterprise resource planning (ERP) systems are developed and implemented over the hardware abstraction layers. The traditional release-by-release update methodology governed by main version change (from pre-alpha to gold release) was changed to a continuous release management. Within the cloud based Software as a Service (SaaS) model, the core business logic is implied above the physical implementation layer. This scenario can predict that the software product can have a longer lifetime, because it is segregated from the always changing physical implementation layer. As the sudden change of technology is present in nowadays IT architecture, the presence of this new abstraction layer seems logical, because the basic business processes are not changing this rapidly. The SaaS type life cycle management means that the heavily technology independent part are not describing the business processes anymore. Previous lifecycle implementations from the assessment phase to the post go-live and support phase dealt the business logic as one entity with its implementation. That means, that the question of code reusability has a different role as in the standard on premise model. This paper introduces a new method of encapsulating and identifying the software parts, which can be later reused in a cloud SaaS environment.","I. Orosz and T. Orosz",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10, CR11","Rejeitado: CR10, CR12, CR11"
"Who should review this change?: Putting text and file location analyses together for more accurate recommendations","Software code review is a process of developers inspecting new code changes made by others, to evaluate their quality and identify and fix defects, before integrating them to the main branch of a version control system. Modern Code Review (MCR), a lightweight and tool-based variant of conventional code review, is widely adopted in both open source and proprietary software projects. One challenge that impacts MCR is the assignment of appropriate developers to review a code change. Considering that there could be hundreds of potential code reviewers in a software project, picking suitable reviewers is not a straightforward task. A prior study by Thongtanunam et al. showed that the difficulty in selecting suitable reviewers may delay the review process by an average of 12 days. In this paper, to address the challenge of assigning suitable reviewers to changes, we propose a hybrid and incremental approach Tie which utilizes the advantages of both Text mIning and a filE location-based approach. To do this, Tie integrates an incremental text mining model which analyzes the textual contents in a review request, and a similarity model which measures the similarity of changed file paths and reviewed file paths. We perform a large-scale experiment on four open source projects, namely Android, OpenStack, QT, and LibreOffice, containing a total of 42,045 reviews. The experimental results show that on average Tie can achieve top-1, top-5, and top-10 accuracies, and Mean Reciprocal Rank (MRR) of 0.52, 0.79, 0.85, and 0.64 for the four projects, which improves the state-of-the-art approach RevFinder, proposed by Thongtanunam et al., by 61%, 23%, 8%, and 37%, respectively.","X. Xia and D. Lo and X. Wang and X. Yang",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Deep Semantic Feature Learning for Software Defect Prediction","Software defect prediction, which predicts defective code regions, can assist developers in finding bugs and prioritizing their testing efforts. Traditional defect prediction features often fail to capture the semantic differences between different programs. This degrades the performance of the prediction models built on these traditional features. Thus, the capability to capture the semantics in programs is required to build accurate prediction models. To bridge the gap between semantics and defect prediction features, we propose leveraging a powerful representation-learning algorithm, deep learning, to learn the semantic representations of programs automatically from source code files and code changes. Specifically, we leverage a deep belief network (DBN) to automatically learn semantic features using token vectors extracted from the programs' abstract syntax trees (AST) (for file-level defect prediction models) and source code changes (for change-level defect prediction models). We examine the effectiveness of our approach on two file-level defect prediction tasks (i.e., file-level within-project defect prediction and file-level cross-project defect prediction) and two change-level defect prediction tasks (i.e., change-level within-project defect prediction and change-level cross-project defect prediction). Our experimental results indicate that the DBN-based semantic features can significantly improve the examined defect prediction tasks. Specifically, the improvements of semantic features against existing traditional features (in F1) range from 2.1 to 41.9 percentage points for file-level within-project defect prediction, from 1.5 to 13.4 percentage points for file-level cross-project defect prediction, from 1.0 to 8.6 percentage points for change-level within-project defect prediction, and from 0.6 to 9.9 percentage points for change-level cross-project defect prediction.","S. Wang and T. Liu and J. Nam and L. Tan",2018,"[""IEEE"",""Engineering Village""]","Aceito: CA5","Aceito: CA5"
"XML_DocTracker: Generating Software Requirements Specification (SRS) from XML Schema","Agile software development methodology is an iterative and incremental method in making interactions more important than process and tools. The method also emphasizes more on developing software rather than making a comprehensive documentation. Therefore, web developers like to adapt agile software development methodology in their web development. The reason is because the methodology delivers web application faster than the traditional software development methodology. As advantages of this method, web application is developed in a short time. Although these make huge benefits, the most important thing in software development life cycle has been ignored. That is, documentation process in capturing requirements and design. Therefore, this paper presents a tool named XML_DocTracker for generating the software requirements specification (SRS) from XML schema as well as addressing the versioning problems during generating the SRS. XML_DocTracker is implemented based on the framework for transformation rules from XML Schema. The framework also addresses the versioning factor using traceability for detecting the document changes. Based on the framework, XML_DocTracker is developed and the tool is able to generate the SRS from the XML schema as well as able to detect document changes in SRS due to traceability factor that is embedded inside the tool. The tool can be used for software community who want to generate the SRS from the source codes if the SRS did not exist for that particular software. This paper contribution is detecting new type of element evolution in SRS when new XML schema version is introduced.","H. Aman and R. Ibrahim",2016,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10"
"Does cloned code increase maintenance effort?","In-spite of a number of in-depth investigations regarding the impact of clones in the maintenance phase there is no concrete answer to the long lived research question, “Does the presence of code clones increase maintenance effort?”. Existing studies have measured different change related metrics for cloned and non-cloned regions, however, no study calculates the maintenance effort spent for these code regions. In this paper, we perform an in-depth empirical study in order to compare the maintenance efforts required for cloned and non-cloned code. For the purpose of our study we implement a prototype tool which is capable of estimating the effort spent by a developer for changing a particular method. It can also predict effort that might need to be spent for making some changes to a particular method. Our estimation and prediction involve automatic extraction and analysis of the entire evolution history of a candidate software system. We applied our tool on hundreds of revisions of six open source subject systems written in three different programming languages for calculating the efforts spent for cloned and non-cloned code. According to our experimental results: (i) cloned code requires more effort in the maintenance phase than non-cloned code, and (ii) Type 2 and Type 3 clones require more effort compared to the efforts required by Type 1 clones. According to our findings, we should prioritize Type 2 and Type 3 clones when making clone management decisions.","M. Mondal and C. K. Roy and K. A. Schneider",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Extracting Dependencies from Software Changes: An Industry Experience Report","Retrieving and analyzing information from software repositories and detecting dependencies are important tasks supporting software evolution. Dependency information is used for change impact analysis, defect prediction as well as cohesion and coupling measurement. In this paper we report our experience from extracting dependency information from the change history of a commercial software system. We analyzed the software system's evolution of about six years, from the start of development to the transition to product releases and maintenance. Analyzing the co-evolution of software artifacts allows detecting logical dependencies between system parts implemented with heterogeneous technologies as well as between different types of development artifacts such as source code, data models or documentation. However, the quality of the extracted dependencies relies on established development practices and conformance to a defined change process. In this paper we indicate resulting limitations and recommend further processing and filtering steps to prepare the dependency data for subsequent analysis and measurement activities.","T. Wetzlmaier and C. Klammer and R. Ramler",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Comparison of dynamic model selection with infinite HMM for statistical model change detection","In this study, we address the issue of tracking changes in statistical models under the assumption that the statistical models used for generating data may change over time. This issue is of great importance for learning from non-stationary data. One of the promising approaches for resolving this issue is the use of the dynamic model selection (DMS) method, in which a model sequence is estimated on the basis of the minimum description length (MDL) principle. Another approach is the use of the infinite hidden Markov model (HMM), which is a non-parametric learning method for the case with an infinite number of states. In this study, we propose a few new variants of DMS and propose efficient algorithms to minimize the total code-length by using the sequential normalized maximum likelihood. We compare these algorithms with infinite HMM to investigate their statistical model change detection performance, and we empirically demonstrate that one of our variants of DMS significantly outperforms infinite HMM in terms of change-point detection accuracy.","E. Sakurai and K. Yamanishi",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Neural Acceleration for General-Purpose Approximate Programs","This paper describes a learning-based approach to the acceleration of approximate programs. We describe the \emph{Parrot transformation}, a program transformation that selects and trains a neural network to mimic a region of imperative code. After the learning phase, the compiler replaces the original code with an invocation of a low-power accelerator called a \emph{neural processing unit} (NPU). The NPU is tightly coupled to the processor pipeline to accelerate small code regions. Since neural networks produce inherently approximate results, we define a programming model that allows programmers to identify approximable code regions -- code that can produce imprecise but acceptable results. Offloading approximable code regions to NPUs is faster and more energy efficient than executing the original code. For a set of diverse applications, NPU acceleration provides whole-application speedup of 2.3× and energy savings of 3.0× on average with quality loss of at most 9.6%.","H. Esmaeilzadeh and A. Sampson and L. Ceze and D. Burger",2012,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"[Research Paper] Detecting Evolutionary Coupling Using Transitive Association Rules","If two or more program entities (such as files, classes, methods) co-change (i.e., change together) frequently during software evolution, then it is likely that these two entities are coupled (i.e., the entities are related). Such a coupling is termed as evolutionary coupling in the literature. The concept of traditional evolutionary coupling restricts us to assume coupling among only those entities that changed together in the past. The entities that did not co-change in the past might also have coupling. However, such couplings can not be retrieved using the current concept of detecting evolutionary coupling in the literature. In this paper, we investigate whether we can detect such couplings by applying transitive rules on the evolutionary couplings detected using the traditional mechanism. We call these couplings that we detect using our proposed mechanism as transitive evolutionary couplings. According to our research on thousands of revisions of four subject systems, transitive evolutionary couplings combined with the traditional ones provide us with 13.96% higher recall and 5.56% higher precision in detecting future co-change candidates when compared with a state-of-the-art technique.","M. A. Islam and M. M. Islam and M. Mondal and B. Roy and C. K. Roy and K. A. Schneider",2018,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Early Conflict Detection with Mined Models","The key idea introduced in this paper consists of running a multi-branch server-side dynamic analysis at every commit operation. The analysis will execute the test cases available in the source code management (SCM) system to trace the behavior of the application and automatically derive models that capture how the program behaves according to multiple dimensions. For instance, the functional behavior of the program can be represented with method pre- and post-conditions, API usage protocols, and precedence rules among method invocations. The temporal behavior of a program can be represented with models that capture aspects, such as deadlines, periodicity, and constraints on the timing of the tasks. These models are used on the server side to run automated conflict detection. Models are derived for every version in every branch, and automatically compared every time a change is introduced. Comparing models allows identifying behavioral conflicts regardless the presence of textual conflicts, which do not need to be resolved to run the analysis. We call this analysis Behavioral Driven Continuous Integration (BDCI).","L. Mariani and D. Micucci and F. Pastore",2014,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10"
"Estimating the Accuracy of Dynamic Change-Impact Analysis Using Sensitivity Analysis","The reliability and security of software are affected by its constant changes. For that reason, developers use change-impact analysis early to identify the potential consequences of changing a program location. Dynamic impact analysis, in particular, identifies potential impacts on concrete, typical executions. However, the accuracy (precision and recall) of dynamic impact analyses for predicting the actual impacts of changes has not been studied. In this paper, we present a novel approach based on sensitivity analysis and execution differencing to estimate, for the first time, the accuracy of dynamic impact analyses. Unlike approaches that only use software repositories, which might not be available or might contain insufficient changes, our approach makes changes to every part of the software to identify actually impacted code and compare it with the predictions of dynamic impact analysis. Using this approach in addition to changes made by other researchers on multiple Java subjects, we estimated the accuracy of the best method-level dynamic impact analysis in the literature. Our results suggest that dynamic impact analysis can be surprisingly inaccurate with an average precision of 47-52% and recall of 56-87%. This study offers insights to developers into the effectiveness of existing dynamic impact analyses and motivates the future development of more accurate analyses.","H. Cai and R. Santelices and T. Xu",2014,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Time-Contrastive Networks: Self-Supervised Learning from Video","We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a triplet loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at sermanet.github.io/imitate.","P. Sermanet and C. Lynch and Y. Chebotar and J. Hsu and E. Jang and S. Schaal and S. Levine and G. Brain",2018,"[""IEEE""]","Rejeitado: CR9, CR10","Rejeitado: CR9, CR8, CR10"
"Mining frequent bug-fix code changes","Detecting bugs as early as possible plays an important role in ensuring software quality before shipping. We argue that mining previous bug fixes can produce good knowledge about why bugs happen and how they are fixed. In this paper, we mine the change history of 717 open source projects to extract bug-fix patterns. We also manually inspect many of the bugs we found to get insights into the contexts and reasons behind those bugs. For instance, we found out that missing null checks and missing initializations are very recurrent and we believe that they can be automatically detected and fixed.","H. Osman and M. Lungu and O. Nierstrasz",2014,"[""IEEE"",""Engineering Village""]","Aceito: CA0, CA5","Aceito: CA0, CA5"
"DOMdiff: Identification and Classification of Inter-DOM Modifications","Current web crawlers, document databases and change monitoring systems for web sites are commonly limited to static content and analysis of code as retrieved from the server, an approach that is not suitable for modern dynamic web applications. The canonical representation of the contents of a single web page at any given time is an instance of the Document Object Model (DOM), a tree structure that forms the basis for rendering and processing of the page within the browser and is updated when content is modified. This work presents DOMdiff, an algorithm to identify changes between two different DOM instances, as well as a method to classify these changes in terms of a ranking that represents the distance between the two trees. We compare a manually derived classifier with the results of PRank, a ranked version of the Perceptron algorithm, a simple machine learning approach that generates a multiclass classifier based on formulae in a constrained predicate logic, and the established statistical classifier C5.0. Our results indicate that DOMdiff is suitable to large-scale change identification and that entropy-based statistical classifiers are more accurate than our simple predicate-based classifier for the problem at hand, but require a larger decision tree. We additionally identify a shortcoming of PRank when handling features with low information gain/high entropy.","M. Leithner and D. E. Simos",2018,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Tracking Down Dynamic Feature Code Changes against Python Software Evolution","Python, a typical dynamic programming language, is increasingly used in many application domains. Dynamic features in Python allow developers to change the code at runtime. Some dynamic features such as dynamic type checking play an active part in maintenance activities, thus dynamic feature code is often changed to cater to software evolution. The aim of this paper is exploring and validating the characteristics of feature changes in Python. We collected change occurrences in 85 open-source projects and discovered the relationship between feature changes and bug-fix activities. Furthermore, we went into 358 change occurrences to explore the causes and behaviors of feature changes. The results show that: (1) dynamic features are increasingly used and the code is changeable; (2) most dynamic features may behave that feature code is more likely to be changed in bug-fix activities than non-bugfix activities; (3) dynamic feature code plays both positive and negative roles in maintenance activities. Our results provide useful guidance and insights for improving automatic program repair and refactoring tools.","Z. Chen and W. Ma and W. Lin and L. Chen and B. Xu",2016,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Collective Personalized Change Classification With Multiobjective Search","Many change classification techniques have been proposed to identify defect-prone changes. These techniques consider all developers' historical change data to build a global prediction model. In practice, since developers have their own coding preferences and behavioral patterns, which causes different defect patterns, a separate change classification model for each developer can help to improve performance. Jiang, Tan, and Kim refer to this problem as personalized change classification, and they propose PCC+ to solve this problem. A software project has a number of developers; for a developer, building a prediction model not only based on his/her change data, but also on other relevant developers' change data can further improve the performance of change classification. In this paper, we propose a more accurate technique named collective personalized change classification (CPCC), which leverages a multiobjective genetic algorithm. For a project, CPCC first builds a personalized prediction model for each developer based on his/her historical data. Next, for each developer, CPCC combines these models by assigning different weights to these models with the purpose of maximizing two objective functions (i.e., F1-scores and cost effectiveness). To further improve the prediction accuracy, we propose CPCC+ by combining CPCC with PCC proposed by Jiang, Tan, and Kim To evaluate the benefits of CPCC+ and CPCC, we perform experiments on six large software projects from different communities: Eclipse JDT, Jackrabbit, Linux kernel, Lucene, PostgreSQL, and Xorg. The experiment results show that CPCC+ can discover up to 245 more bugs than PCC+ (468 versus 223 for PostgreSQL) if developers inspect the top 20% lines of code that are predicted buggy. In addition, CPCC+ can achieve F1-scores of 0.60-0.75, which are statistically significantly higher than those of PCC+ on all of the six projects.","X. Xia and D. Lo and X. Wang and X. Yang",2016,"[""IEEE"",""Engineering Village""]","Aceito: CA5","Aceito: CA5"
"DataLab: A Version Data Management and Analytics System","One challenge in big data analytics is the lack of tools to manage the complex interactions among code, data and parameters, especially in the common situation where all these factors can change a lot. We present our preliminary experience with DataLab, a system we build to manage the big data workflow. DataLab improves big data analytical workflow in several novel ways. 1) DataLab manages the revision of both code and data in a coherent system, and includes a distributed code execution engine to run users' code; 2) DataLab keeps track of all the data analytics results in a data work flow graph, and is able to compare the code / results between any two versions, making it easier for users to intuitively see the results of their code change; 3) DataLab provides an efficient data management system to separate data from their metadata, allowing efficient preprocessing filters; and 4) DataLab provides a common API so people can build different applications on top of it. We also present our experience of applying a DataLab prototype in a real bioinformatics application.","Y. Zhang and F. Xu and E. Frise and S. Wu and B. Yu and W. Xu",2016,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"A modified version of Rijndael algorithm implemented to analyze the cyphertexts correlation for switched S-Boxes","There are more than eleven years since Rijndael algorithm was declared the winner of the NIST contest for the new AES election. All this time the original algorithm was analyzed and attacked by cryptanalysts and hackers in order to find its vulnerabilities. The modified version of Rijndael we analyze in this paper randomly changes the accessing order of S-Boxes implemented in the source code of the original algorithm, due to affine transformation and inverse matrix properties. The goal is to obtain two different cyphertexts, keeping the plaintext and the secret key. For this to be possible, a PRNG designed by Gorge Marsaglia was implemented in the software solution.","M. Cretu and C. Apostol",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"How do Java methods grow?","Overly long methods hamper the maintainability of software - they are hard to understand and to change, but also difficult to test, reuse, and profile. While technically there are many opportunities to refactor long methods, little is known about their origin and their evolution. It is unclear how much effort should be spent to refactor them and when this effort is spent best. To obtain a maintenance strategy, we need a better understanding of how software systems and their methods evolve. This paper presents an empirical case study on method growth in Java with nine open source and one industry system. We show that most methods do not increase their length significantly; in fact, about half of them remain unchanged after the initial commit. Instead, software systems grow by adding new methods rather than by modifying existing methods.","D. Steidl and F. Deissenboeck",2015,"[""IEEE""]","Rejeitado: CR11","Rejeitado: CR11"
"Comparison of absolute and relative strategies to encode sensorimotor transformations in tool-use","We explore different strategies to overcome the problem of sensorimotor transformation that babies face during development, especially in the case of tool-use. From a developmental perspective, we investigate a model based on absolute coordinate frames of reference, and another one based on relative coordinate frames of reference. In a situation of sensorimotor learning and of adaptation to tool-use, we perform a computer simulation of a 4 degrees of freedom robot. We show that the relative coordinate strategy is the most rapid and robust to re-adapt the neural code.","R. Braud and A. Pitti and P. Gaussier",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Case-Based Instruction of ""How Computer Works"" Courses for High School Students","The concept of binary, which is the basis of how computer works, is often taught in Taiwan's high school computer courses as the introductory unit of the class ""Introduction to Computers"". In the past, teachers used to lecture through binary concepts such as the decimal binary conversion, ASCII code transformation, and several traditional courses. However, most students did not know why they needed to learn binary concepts, and seemed less interested in learning. To enhance the effectiveness of learning, we incorporated some examples corresponding to each binary concept to link students' life experiences to the concepts in order to make learning meaningful. In this paper, we will share our teaching materials for the binary concept from the course ""Introduction to Computers"". The lesson starts from how computer stores values, text or image data to introduce basic concepts of binary data interpretation and hex conversion. Afterwards, a combination of simple logic gates adder instance is introduced to guide students to learn how to perform a binary compute operation. Finally, software CPU simulator is covered to allow students to learn and visualize how computer uses binary data and basic operations to perform some specific compute functions.","T. Wang",2015,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Learning Syntactic Program Transformations from Examples","Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.","R. Rolim and G. Soares and L. D'Antoni and O. Polozov and S. Gulwani and R. Gheyi and R. Suzuki and B. Hartmann",2017,"[""IEEE"",""ACM"",""Engineering Village""]","Aceito: CA0, CA1, CA2, CA3","Aceito: CA1, CA2, CA3"
"Switching to Git: The Good, the Bad, and the Ugly","Since its introduction 10 years ago, GIT has taken the world of version control systems (VCS) by storm. Its success is partly due to creating opportunities for new usage patterns that empower developers to work more efficiently. However, the resulting change in both user behavior and the way GIT stores changes impacts data mining and data analytics procedures [6], [13]. While some of these unique characteristics can be managed by adjusting mining and analytical techniques, others can lead to severe data loss and the inability to audit code changes, e.g. knowing the full history of changes of code related to security and privacy functionality. Thus, switching to GIT comes with challenges to established development process analytics. This paper is based on our experience in attempting to provide continuous process analysis for Microsoft product teams who switching to GIT as their primary VCS. We illustrate how GIT's concepts and usage patterns create a need for changing well-established data analytic processes. The goal of this paper is to raise awareness how certain GIT operations may damage or even destroy information about historical code changes necessary for continuous data development process analytics. To that end, we provide a list of common GIT usage patterns with a description of how these operations impact data mining applications. Finally, we provide examples of how one may counteract the effects of such destructive operations in the future. We further provide a new algorithm to detect integration paths that is specific to distributed version control systems like GIT, which allows us to reconstruct the information that is crucial to most development process analytics.","S. Just and K. Herzig and J. Czerwonka and B. Murphy",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8"
"Evolution analysis for Accessibility Excessiveness in Java","In Java programs, access modifiers are used to control the accessibility of fields and methods from other objects. Choosing appropriate access modifiers is one of the key factors to improve program quality and to reduce potential vulnerability. In our previous work, we presented a static analysis method named Accessibility Excessiveness (AE) detection for each field and method in Java program. We have also developed an AE analysis tool named ModiChecker that analyzes each field and method of the input Java programs, and reports their excessiveness. In this paper, we have applied ModiChecker to several OSS repositories to investigate the evolution of AE over versions, and identified transition of AE status and the difference in the amount of AE change between major version releases and minor ones. Also we propose when to evaluate source code with AE analysis.","K. Kobori and M. Matsushita and K. Inoue",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8"
"When and Why Your Code Starts to Smell Bad","In past and recent years, the issues related to managing technical debt received significant attention by researchers from both industry and academia. There are several factors that contribute to technical debt. One of these is represented by code bad smells, i.e., Symptoms of poor design and implementation choices. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced. To fill this gap, we conducted a large empirical study over the change history of 200 open source projects from different software ecosystems and investigated when bad smells are introduced by developers, and the circumstances and reasons behind their introduction. Our study required the development of a strategy to identify smell-introducing commits, the mining of over 0.5M commits, and the manual analysis of 9,164 of them (i.e., Those identified as smell-introducing). Our findings mostly contradict common wisdom stating that smells are being introduced during evolutionary tasks. In the light of our results, we also call for the need to develop a new generation of recommendation systems aimed at properly planning smell refactoring activities.","M. Tufano and F. Palomba and G. Bavota and R. Oliveto and M. Di Penta and A. De Lucia and D. Poshyvanyk",2015,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Automatic Clone Recommendation for Refactoring Based on the Present and the Past","When many clones are detected in software programs, not all clones are equally important to developers. To help developers refactor code and improve software quality, various tools were built to recommend clone-removal refactorings based on the past and the present information, such as the cohesion degree of individual clones or the co-evolution relations of clone peers. The existence of these tools inspired us to build an approach that considers as many factors as possible to more accurately recommend clones. This paper introduces CREC, a learning-based approach that recommends clones by extracting features from the current status and past history of software projects. Given a set of software repositories, CREC first automatically extracts the clone groups historically refactored (R-clones) and those not refactored (NR-clones) to construct the training set. CREC extracts 34 features to characterize the content and evolution behaviors of individual clones, as well as the spatial, syntactical, and co-change relations of clone peers. With these features, CREC trains a classifier that recommends clones for refactoring. We designed the largest feature set thus far for clone recommendation, and performed an evaluation on six large projects. The results show that our approach suggested refactorings with 83% and 76% F-scores in the within-project and cross-project settings. CREC significantly outperforms a state-of-the-art similar approach on our data set, with the latter one achieving 70% and 50% F-scores. We also compared the effectiveness of different factors and different learning algorithms.","R. Yue and Z. Gao and N. Meng and Y. Xiong and X. Wang and J. D. Morgenthaler",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Measuring software library stability through historical version analysis","Backward compatibility is a major concern for any library developer. In this paper, we evaluate how stable a set of frequently used third-party libraries is in terms of method removals, implementation change, the ratio of change in old methods to change in new ones and the percentage of new methods in each snapshot. We provide a motivating example of a commercial company which demonstrates several issues associated with the usage of third-party libraries. To obtain dependencies from software systems we developed a framework which extracts dependencies from Maven build files and which analyzes system and library code. We propose four metrics which provide different insights in the implementation and interface stability of a library. The usage frequency of library methods is utilized as a weight in the final metric and is obtained from a dataset of more than 2300 snapshots of 140 industrial Java systems. We finally describe three scenarios and an example of the application of our metrics.","S. Raemaekers and A. van Deursen and J. Visser",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Power control in the uplink of a wireless multi-carrier CDMA system","This paper addresses the power allocation problem in the uplink of wireless multi-carrier code-division multiple-access (MC-CDMA) in order to guarantee QoS requirements. QoS is evaluated with respect to the signal to interference-noise ratio (SINR), estimated after the detection process at the base station (BS). First, departing of the received signal model in a MC-CDMA system, the SINR is computed by considering the application of linear multiuser detectors and a common transmission power through all subcarriers. By assuming that the measured SINR must follow an objective SINR value, an open-loop solution is introduced to the power allocation problem. Next, an iterative version is derived that relies on an integral correction of the required transmission power. By applying a loop transformation and the small-gain theorem, general distributed controllers can be proposed, where specific stability and performance conditions are introduced. A simulation evaluation is presented at different load levels in the MC-CDMA, and time-varying channel gains. In all the studied cases, the resulting power allocation system was able to allocate transmission power to achieve the objective SINR.","D. U. Campos-Delgado and J. M. Luna-Rivera",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"An efficient approach for providing rationale of method change for object oriented programming","Software engineering requires modification of code during development and maintenance phase. During modification, a difficult task is to understand rationale of code changed by others. Present Integrated Development Environments (IDEs) attempt to help this by providing features integrated with different types of repositories. However, these features still consume developers' time as he has to switch from editor to another window for this purpose. Moreover, these features focus on elements available in present version of code, thus increasing the difficulty of finding rationale of an element removed or modified earlier. Leveraging different sources for providing information through code completion menus has been shown to be valuable, even when compared to standalone counterparts offering similar functionalities in literature. Literature also shows that it is one of the most used features for consuming information within IDE. Based on that, we prepare an Eclipse plug-in and a framework that allows providing reason of code change, at method granularity, across versions through a new code completion menu in IDE. These allow a software engineer to gain insight about rationale of removed or modified methods which are otherwise not available in present version of code. Professional software engineers participated in our empirical evaluation process and we observed that more than 80% participants considered this to be a useful approach for saving time and effort to understand rationale of method change.","Amit Seal Ami and M. S. Islam",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Mining Software Repositories for Accurate Authorship","Code authorship information is important for analyzing software quality, performing software forensics, and improving software maintenance. However, current tools assume that the last developer to change a line of code is its author regardless of all earlier changes. This approximation loses important information. We present two new line-level authorship models to overcome this limitation. We first define the repository graph as a graph abstraction for a code repository, in which nodes are the commits and edges represent the development dependencies. Then for each line of code, structural authorship is defined as a sub graph of the repository graph recording all commits that changed the line and the development dependencies between the commits, weighted authorship is defined as a vector of author contribution weights derived from the structural authorship of the line and based on a code change measure between commits, for example, best edit distance. We have implemented our two authorship models as a new git built-in tool git-author. We evaluated git-author in an empirical study and a comparison study. In the empirical study, we ran git-author on five open source projects and found that git-author can recover more information than a current tool (git-blame) for about 10% of lines. In the comparison study, we used git-author to build a line-level model for bug prediction. We compared our line-level model with an existing file-level model. The results show that our line-level model performs consistently better than the file-level model when evaluated on our data sets produced from the Apache HTTP server project.","X. Meng and B. P. Miller and W. R. Williams and A. R. Bernat",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Enhance embedded system E-leaming experience with sensors","Earlier research shows that using an embedded LED system motivates students to learn programming languages in massive open online courses (MOOCs) efficiently. Since this earlier approach was very successful the system should be improved to increase the learning experience for students during programming exercises. The problem of the current system is that only a static image was shown on the LED matrix controlled by students' array programming over the embedded system. The idea of this paper is to change this static behavior into a dynamic display of information on the LED matrix by the use of sensors which are connected with the embedded system. For this approach a light sensor and a temperature sensor are connected to an analog-to-digital converter (ADC) port of the embedded system. These sensors' values can be read by the students to compute the correct output for the LED matrix. The result is captured and sent back to the students for direct feedback. Furthermore, unit tests can be used to automatically evaluate the programming results. The system was evaluated during a MOOC course about Web Technologies using JavaScript. Evaluation results are taken from the student's feedback and an evaluation of the students' code executions on the system. The positive feedback and the evaluation of the students' executions, which shows a higher amount of code executions compared to standard programming tasks and the fact that students solving these tasks have overall better course results, highlight the advantage of the approach. Due to the evaluation results, this approach should be used in e-learning e.g. MOOCs teaching programming languages to increase the learning experience and motivate students to learn programming.","M. Malchow and J. Renz and M. Bauer and C. Meinel",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Pipelined implementations of polar encoder and feed-back part for SC polar decoder","In this paper, we first reveal the similarity of polar encoder and fast Fourier transform (FFT) processor. Based on this, both feed-forward and feed-back pipelined implementations of polar encoder are proposed. It is pointed out that the feedback part of SC polar decoder is nothing but a simplified version of polar encoder and therefore can be pipelined implemented also. Moreover, a general approach which uniformly constructs most pipelined polar encoders via folding transformation is proposed. Implementation results have shown that both proposed pipelined polar encoder architectures achieve more than 98.3% complexity reduction and more than 9.86% speed-up compared to the conventional implementation.","C. Zhang and J. Yang and X. You and S. Xu",2015,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"A new semantics-based android malware detection","With its high market share, the Android platform has become a growing target for mobile malware, which posed great threat to customers' safety. Meanwhile, malwares employed various techniques, take code obfuscation for example, to evade detection. The commercial mobile anti-malware products, however, are vulnerable to common code transformation techniques. This paper proposes an enhanced malware detection approach which combines advantage of static analysis and performance of ensemble learning to improve Android malware detection accuracy. The model extracts semantics-based features which can resist common obfuscation techniques, and also uses feature collection from code and app characteristics through static analysis. Real-world malware samples are used to evaluate the model and the results of experiments have proved that this approach improved the efficiency with AUC of 2.06% higher than previous approach.","Xiaohan Zhang and Zhengping Jin",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Frankencode: Creating Diverse Programs Using Code Clones","In this paper, we present an approach to detecting novel cyber attacks though a form of program diversification, similar to the use of n-version programming for fault tolerant systems. Building on extensive previous and ongoing work by others on the use of code clones in a wide variety of areas, our Functionally Equivalent Variants using Information Synchronization (FEVIS) system automatically generates program variants to berun in parallel, seeking to detect attacks through divergence in behavior. Unlike approaches to diversification that only change program memory layout and behavior, FEVIS can detect attacks exploiting vulnerabilities in execution timing, string processing, and other logic errors. We are in the early stages of research and development for this approach, but have made sufficient progress to provide a proof of concept and some lessons learned. In this paper we describe FEVIS and its application to diversifying an open-source webserver, with results on several different example classes of attack which FEVIS will detect.","H. Borck and M. Boddy and I. J. D. Silva and S. Harp and K. Hoyme and S. Johnston and A. Schwerdfeger and M. Southern",2016,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Mobile Multi-agent Systems for the Internet-of-Things and Clouds Using the JavaScript Agent Machine Platform and Machine Learning as a Service","The Internet-of-Things (IoT) gets real in today's life and is becoming part of pervasive and ubiquitous computing networks offering distributed and transparent services. A unified and common data processing and communication methodology is required to merge the IoT, sensor networks, and Cloud-based environments seamless, which can be fulfilled by the mobile agent-based computing paradigm, discussed in this work. Currently, portability, resource constraints, security, and scalability of Agent Processing Platforms (APP) are essential issues for the deployment of Multi-agent Systems (MAS) in strong heterogeneous networks including the Internet, addressed in this work. To simplify the development and deployment of MAS it would be desirable to implement agents directly in JavaScript, which is a well known and public widespread used programming language, and JS VMs are available on all host platforms including WEB browsers. The novel proposed JS Agent Machine (JAM) is capable to execute AgentJS agents in a sandbox environment with full run-time protection and Machine learning as a service. Agents can migrate between different JAM nodes seamless preserving their data and control state by using a on-the-fly code-to-text transformation in an extended JSON+ format. A Distributed Organization System (DOS) layer provides JAM node connectivity and security in the Internet, completed by a Directory-Name Service offering an organizational graph structure. Agent authorization and platform security is ensured with capability-based access and different agent privilege levels.","S. Bosse",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Machine-Learning Aided Analysis of Clone Evolution","Code clones are similar code fragments appearing in software. As software evolves, code clones may be subjected to changes as well; we term this clone evolution. There have not been many investigations into clone evolution characteristics. Therefore, we tackle this by exploring useful information associated with changes of clones during evolution. We focus on three perspectives of clone evolution, ranging from individual clone changes to characterization of clone genealogies. With the help X-means clustering, we establish associations between clone changes and life of clones. Our experimental results on two softwares show that clones are mostly stable throughout software evolution. For the relatively smaller group of “unstable” clones, changes usually happen after several versions, and consistent changes appear more frequently than inconsistent ones. We suggest that developers should pay more attention to relatively longer genealogies, and should consider applying changes consistently to clone group when a constituent clone fragment has undergone change.","F. Zhang and S. Khoo and X. Su",2017,"[""IEEE""]","Rejeitado: CR11","Rejeitado: CR11"
"A proposal of three extensions in blank element selection algorithm for Java programming learning assistant system","To assist Java programming educations, we have developed a Web-based Java Programming Learning Assistant System (JPLAS). JPLAS provides fill-in-blank problems to let students study Java grammar and basic programming skills by filling the blanked elements in a given Java code. To generate the feasible problems, we have proposed a blank element selection algorithm using the constraint graph to select as many blanks as possible such that they have grammatically correct and unique answers. In this paper, to further increase the number of blanks and control the difficulty of the generated problem, we extend this algorithm by 1) adding operators in conditional expressions for blank candidates, 2) improving the edge generation method in the constraint graph to increase the number of blanks, and 3) introducing two parameters to change the frequency of selecting blanks. To verify the effectiveness, we apply the extended algorithm to 55 Java codes for fundamental data structure or algorithms, and confirm that these extensions can increase the number of blanks and change the problem difficulty.","K. K. Zaw and N. Funabiki and M. Kuribayashi",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Apollo: Reusable Models for Fast, Dynamic Tuning of Input-Dependent Code","Increasing architectural diversity makes performance portability extremely important for parallel simulation codes. Emerging on-node parallelization frameworks such as Kokkos and RAJA decouple the work done in kernels from the parallelization mechanism, allowing for a single source kernel to be tuned for different architectures at compile time. However, computational demands in production applications change at runtime, and performance depends both on the architecture and the input problem, and tuning a kernel for one set of inputs may not improve its performance on another. The statically optimized versions need to be chosen dynamically to obtain the best performance. Existing auto-tuning approaches can handle slowly evolving applications effectively, but are too slow to tune highly input-dependent kernels. We developed Apollo, an auto-tuning extension for RAJA that uses pre-trained, reusable models to tune input-dependent code at runtime. Apollo is designed for highly dynamic applications; it generates sufficiently low-overhead code to tune parameters each time a kernel runs, making fast decisions. We apply Apollo to two hydrodynamics benchmarks and to a production multi-physics code, and show that it can achieve speedups from 1.2x to 4.8x.","D. Beckingsale and O. Pearce and I. Laguna and T. Gamblin",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Efficient Learning for Hashing Proportional Data","Spectral hashing (SH) seeks compact binary codes of data points so that Hamming distances between codes correlate with data similarity. Quickly learning such codes typically boils down to principle component analysis (PCA). However, this is only justified for normally distributed data. For proportional data (normalized histograms), this is not the case. Due to the sum-to-unity constraint, features that are as independent as possible will not all be uncorrelated. In this paper, we show that a linear-time transformation efficiently copes with sum-to-unity constraints: first, we select a small number K of diverse data points by maximizing the volume of the simplex spanned by these prototypes; second, we represent each data point by means of its cosine similarities to the K selected prototypes. This maximum volume hashing is sensible since each dimension in the transformed space is likely to follow a von Mises (vM) distribution, and, in very high dimensions, the vM distribution closely resembles a Gaussian distribution. This justifies to employ PCA on the transformed data. Our extensive experiments validate this: maximum volume hashing outperforms spectral hashing and other state of the art techniques.","Z. Xu and K. Kersting and C. Bauckhage",2012,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10"
"Mapping Bug Reports to Relevant Files: A Ranking Model, a Fine-Grained Benchmark, and Feature Evaluation","When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and improve productivity. This paper introduces an adaptive ranking approach that leverages project knowledge through functional decomposition of source code, API descriptions of library components, the bug-fixing history, the code change history, and the file dependency graph. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluate the ranking system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the learning-to-rank approach outperforms three recent state-of-the-art methods. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70 percent of the bug reports in the Eclipse Platform and Tomcat projects.","X. Ye and R. Bunescu and C. Liu",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs","Convolutional neural networks (CNNs) have been widely applied in many deep learning applications. In recent years, the FPGA implementation for CNNs has attracted much attention because of its high performance and energy efficiency. However, existing implementations have difficulty to fully leverage the computation power of the latest FPGAs. In this paper we implement CNN on an FPGA using a systolic array architecture, which can achieve high clock frequency under high resource utilization. We provide an analytical model for performance and resource utilization and develop an automatic design space exploration framework, as well as source-to-source code transformation from a C program to a CNN implementation using systolic array. The experimental results show that our framework is able to generate the accelerator for real-life CNN models, achieving up to 461 GFlops for floating point data type and 1.2 Tops for 8-16 bit fixed point.","Xuechao Wei and Cody Hao Yu and Peng Zhang and Youxiang Chen and Yuxin Wang and Han Hu and Yun Liang and J. Cong",2017,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An Empirical Study of the Effect of File Editing Patterns on Software Quality","While some developers like to work on multiple code change requests, others might prefer to handle one change request at a time. This juggling of change requests and the large number of developers working in parallel often lead to files being edited as part of different change requests by one or several developers. Existing research has warned the community about the potential negative impacts of some file editing patterns on software quality. For example, when several developers concurrently edit a file as part of different change requests, they are likely to introduce bugs due to limited awareness of other changes. However, very few studies have provided quantitative evidence to support these claims. In this paper, we identify four file editing patterns. We perform an empirical study on three open source software systems to investigate the individual and the combined impact of the four patterns on software quality. We find that: (1) files that are edited concurrently by many developers have on average 2.46 times more future bugs than files that are not concurrently edited, (2) files edited in parallel with other files by the same developer have on average 1.67 times more future bugs than files individually edited, (3) files edited over an extended period (i.e., above the third quartile) of time have 2.28 times more future bugs than other files, and (4) files edited with long interruptions (i.e., above the third quartile) have 2.1 times more future bugs than other files. When more than one editing patterns are followed by one or many developers during the editing of a file, we observe that the number of future bugs in the file can be as high as 1.6 times the average number of future bugs in files edited following a single editing pattern. These results can be used by software development teams to warn developers about risky file editing patterns.","F. Zhang and F. Khomh and Y. Zou and A. E. Hassan",2012,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Symbolic Crosschecking of Data-Parallel Floating-Point Code","We present a symbolic execution-based technique for cross-checking programs accelerated using SIMD or OpenCL against an unaccelerated version, as well as a technique for detecting data races in OpenCL programs. Our techniques are implemented in KLEE-CL, a tool based on the symbolic execution engine KLEE that supports symbolic reasoning on the equivalence between expressions involving both integer and floating-point operations. While the current generation of constraint solvers provide effective support for integer arithmetic, the situation is different for floating-point arithmetic, due to the complexity inherent in such computations. The key insight behind our approach is that floating-point values are only reliably equal if they are essentially built by the same operations. This allows us to use an algorithm based on symbolic expression matching augmented with canonicalisation rules to determine path equivalence. Under symbolic execution, we have to verify equivalence along every feasible control-flow path. We reduce the branching factor of this process by aggressively merging conditionals, if-converting branches into select operations via an aggressive phi-node folding transformation. To support the Intel Streaming SIMD Extension (SSE) instruction set, we lower SSE instructions to equivalent generic vector operations, which in turn are interpreted in terms of primitive integer and floating-point operations. To support OpenCL programs, we symbolically model the OpenCL environment using an OpenCL runtime library targeted to symbolic execution. We detect data races by keeping track of all memory accesses using a memory log, and reporting a race whenever we detect that two accesses conflict. By representing the memory log symbolically, we are also able to detect races associated with symbolically-indexed accesses of memory objects. We used KLEE-CL to prove the bounded equivalence between scalar and data-parallel versions of floating-point programs and find a number of issues in a variety of open source projects that use SSE and OpenCL, including mismatches between implementations, memory errors, race conditions and a compiler bug.","P. Collingbourne and C. Cadar and P. H. J. Kelly",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Supporting Selective Undo in a Code Editor","Programmers often need to revert some code to an earlier state, or restore a block of code that was deleted a while ago. However, support for this backtracking in modern programming environments is limited. Many of the backtracking tasks can be accomplished by having a selective undo feature in code editors, but this has major challenges: there can be conflicts among edit operations, and it is difficult to provide usable interfaces for selective undo. In this paper, we present AZURITE, an Eclipse plug-in that allows programmers to selectively undo fine-grained code changes made in the code editor. With AZURITE, programmers can easily perform backtracking tasks, even when the desired code is not in the undo stack or a version control system. AZURITE also provides novel user interfaces specifically designed for selective undo, which were iteratively improved through user feedback gathered from actual users in a preliminary field trial. A formal lab study showed that programmers can successfully use AZURITE, and were twice as fast as when limited to conventional features.","Y. Yoon and B. A. Myers",2015,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Profmig: A framework for flexible migration of program profiles across software versions","Offline program profiling is costly, especially when software update is frequent. In this paper, we initiate a systematic exploration in cross-version program profile migration, which tries to effectively reuse the valid part of the behavior profiles of an old version of a software for a new version. We explore the effects imposed on profile reusability by the various factors in program behaviors, profile formats, and impact analysis, and introduce ProfMig, a framework for flexible migrations of various profiles. We demonstrate the effectiveness of the techniques on migrating loop trip-count profiles and dynamic call graphs. The migration saves significant (48-67% on average) profiling time with less than 10% accuracy compromised for most programs.","M. Zhou and B. Wu and Y. Ding and X. Shen",2013,"[""IEEE"",""ACM""]","Rejeitado: CR10","Rejeitado: CR10"
"Technology assisted tool for learning skills development in early childhood","Importance of proper early childhood education is a fact which is often overlooked and neglected. But it is a very important phase in a child's life and it cannot be under-estimated. Experiences during this phase extensively influence physical and neurological developments, which drive biological, psychological and social responses throughout the entire human lifespan. This research introduces a technology assisted tool for the learning skills development in early childhood. The final outcome is a Tablet PC based application to help the toddlers in their learning experience at early ages. This tool is able to improve the writing and speaking skills of the toddler in an entertainment based way. This features an easy way to teach the toddlers in a productive and efficient manner. The objective is to ensure the children are trained and practiced in early days of their lives using standards and cutting edge methodologies, so that they will be ready to face the challenges in their future.","W. H. Rankothge and S. V. Sendanayake and R. G. P. Sudarshana and B. G. G. H. Balasooriya and D. R. Alahapperuma and Y. Mallawarachchi",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Domain adaptation in remote sensing through cross-image synthesis with dictionaries","This contribution studies an approach based on dictionary learning which enables the alignment of the sparse representations of two images. Set in a domain adaptation context, the purpose of this work is to re-synthesize the pixels of a remote sensing image so that, for a given land-cover class, the new values of the samples are comparable across acquisitions. Consequently, the data space of a given source image can be converted to that of a related target image, or vice-versa. After the mentioned transformation, the performance of a classifier trained on the source image and used to predict the thematic classes on the target image is expected to be more robust. A linear transformation is derived thanks to an algorithm simultaneously learning the image-specific dictionaries and the mapping function bridging them via their respective sparse codes. Experiments on knowledge transfer among two co-registered VHR images acquired with different off-nadir angles show promising results. An appropriate cross-image synthesis yields an increased land-cover model portability from one acquisition to another.","G. Matasci and F. de Morsier and M. Kanevski and D. Tuia",2014,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Detecting and Predicting Evolution in Spreadsheets - A Case Study in an Energy Network Company","The use of spreadsheets in industry is widespread and the information that they provide is often used for decisions. Research has shown that spreadsheets are error-prone, leading to the risk that decisions are made on incorrect information. Software Evolution is a well-researched topic and the results have proven to support developers in creating better software. Could this also be applied to spreadsheets? Unfortunately, the research on spreadsheet evolution is still limited. Therefore, the aim of this paper is to obtain a better understanding of how spreadsheets evolve over time and if the results of such a study provide similar benefits for spreadsheets as it does for source code. In this study, we cooperated with Alliander, a large energy network company in the Netherlands. We conducted two case studies on two different set of spreadsheets that both were already maintained for a period of three years. To have a better understanding of the spreadsheets itself and the context in which they evolved, we also interviewed the creators of the spreadsheets. We focus on the changes that are made over time in the formulas. Changes in these formulas change the behavior of the spreadsheet and could possibly introduce errors. To effectively analyze these changes we developed an algorithm that is able to detect and visualize these changes. Results indicate that studying the evolution of a spreadsheet helps to identify areas in the spreadsheet that are error-prone, likely to change or that could benefit from refactoring. Furthermore, by analyzing the frequency in which formulas are changed from version to version, it is possible to predict which formulas need to be changed when a new version of the spreadsheet is created.","B. Jansen and F. Hermans and E. Tazelaar",2018,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Surface Water Mapping by Deep Learning","Mapping of surface water is useful in a variety of remote sensing applications, such as estimating the availability of water, measuring its change in time, and predicting droughts and floods. Using the imagery acquired by currently active Landsat missions, a surface water map can be generated from any selected region as often as every 8 days. Traditional Landsat water indices require carefully selected threshold values that vary depending on the region being imaged and on the atmospheric conditions. They also suffer from many false positives, arising mainly from snow and ice, and from terrain and cloud shadows being mistaken for water. Systems that produce high-quality water maps usually rely on ancillary data and complex rule-based expert systems to overcome these problems. Here, we instead adopt a data-driven, deep-learning-based approach to surface water mapping. We propose a fully convolutional neural network that is trained to segment water on Landsat imagery. Our proposed model, named Deep-WaterMap, learns the characteristics of water bodies from data drawn from across the globe. The trained model separates water from land, snow, ice, clouds, and shadows using only Landsat bands as input. Our code and trained models are publicly available at http://live.ece.utexas.edu/research/deepwatermap/.","F. Isikdogan and A. C. Bovik and P. Passalacqua",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Supervised Machine Learning to Predict Follow-Up Among Adjuvant Endocrine Therapy Patients","Long-term adjuvant endocrine therapy patients often fail to follow-up with their care providers for the recommended duration of time. We used electronic health record data, tumor registry records, and appointment logs to predict follow-up for an adjuvant endocrine therapy patient cohort. Learning predictors for follow-up may facilitate interventions that improve follow-up rates, and ultimately improve patient care in the adjuvant endocrine therapy patient population.We selected 1455 adjuvant endocrine therapy patients at Vanderbilt University Medical Center, and modeled them as a matrix of medical-related, appointment-related, and demographic related features derived from EHR data. We built and optimized a random forest classifier and neural network to differentiate between patients that follow-up, or fail to follow-up, with their care provider for at least five years. We measured follow-up three different ways: thought appointments with any care providers, appointments with an oncologist, and adjuvant endocrine therapy medication records. Classifiers make predictions at the start of adjuvant endocrine therapy, and additionally use temporal subsets of data to learn the change in accuracy as patient data accrues.Our best model is a random forest classifier combining medical-related, appointment-related, and demographic-related features to achieve an AUC of 0.74. The most predictive features for follow-up in our random forest model are total medication counts, patient age, and median income for zip code. We suggest that reliable prediction for follow-up may be correlated with amount of care received at VUMC (i.e., VUMC primary care).This study achieved moderately accurate prediction for followup in adjuvant endocrine therapy patients from electronic health record data. Predicting follow-up can facilitate interventions for improving follow-up rates and improve patient care for adjuvant endocrine therapy cohorts. This study demonstrates the ability to find opportunities for patient care improvement from EHR data.","M. Harrell and M. Levy and D. Fabbri",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Droidsentry: Efficient Code Integrity and Control Flow Verification on TrustZone Devices","The fast evolution of mobile devices has made them the center of attention for not only the research industry, but also malicious actors, as smartphones are used to store, transmit and process sensitive information. The diversity and number of typically installed applications create windows of opportunity for attackers. Attackers can use vulnerable applications to gain control over the device or change the behavior of applications relied on to manage user's finances or store their secret data. Thus, current mobile systems need application execution verification mechanisms. In consequence, we present a framework for current ARM mobile devices that can detect application control flow manipulation attempts by looking at the history of executed control flow altering instructions on the processor. This history examination provides enough information to implement the state-of-the-art fine-grained control policies, without additional binary instrumentation. Moreover, this framework is designed to work with existing hardware and have a minimal impact on performance.","D. Suciu and R. Sion",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Modeling Changeset Topics","Topic modeling has been applied to several areas of software engineering, such as bug localization, feature location, triaging change requests, and traceability link recovery. Many of these approaches combine mining unstructured data, such as bug reports, with topic modeling a snapshot (or release) of source code. However, source code evolves, which causes models to become obsolete. In this paper, we explore the approach of topic modeling changesets over the traditional release approach. We conduct an exploratory study of four open source systems. We investigate the differences in corpora in each project, and evaluate the topic distinctness of the models.","C. S. Corley and K. L. Kashuda and D. S. May and N. A. Kraft",2014,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10, CR9, CR10"
"A method of the hidden faults elimination in FPGA projets for the critical applications","This paper addresses to a problem of the hidden faults which is inherent for the instrumentation and control safety-related systems. These systems are designed for operation in two modes: normal and emergency. The hidden faults can be accumulated throughout the long normal mode and reduce a fault tolerance of the digital components and the functional safety of systems and control objects in the most responsible emergency mode. A method of the hidden faults elimination by an increase in a checkability of the circuits constructed in the LUT-oriented architecture of FPGA projects is suggested. The method is based on the use of several versions of a program code of the FPGA project when saving its hardware implementation. Serial change of versions in a normal mode allows to control the bits of LUT memory used only in an emergency mode.","O. Drozd and M. Kuznietsov and O. Martynyuk and M. Drozd",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"COPE: Vision for a Change-Oriented Programming Environment","Software engineering involves a lot of change as code artifacts are not only created once but maintained over time. In the last 25 years, major paradigms of program development have arisen - agile development with refactorings, software product lines, moving sequential code to multicore or cloud, etc. Each is centered on particular kinds of change; their conceptual foundations rely on transformations that (semi-) automate these changes. We are exploring how transformations can be placed at the center of software development in future IDEs, and when such a view can provide benefits over the traditional view. COPE, a Change-Oriented Programming Environment, looks at 5 activities: (1) analyze what changes programmers typically make and how they perceive, recall, and communicate changes, (2) automate transformations to make it easier to apply and script changes, (3) develop tools that compose and manipulate transformations to make it easier to reuse them, (4) integrate transformations with version control to pro- vide better ways for archiving and understanding changes, and (5) develop tools that infer higher-level transformations from lower-level changes. Characterizing software development in terms of transformations is an essential step to take software engineering from manual development to (semi-) automated development of software.","D. Dig and R. Johnson and D. Marinov and B. Bailey and D. Batory",2016,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Visual tracking using learned color features","Robust object tracking is a challenging task in computer vision. Color features have been popularly used in visual tracking. However, most conventional color-based trackers either rely on luminance information or use simple color representations for image description. During the tracking sequences, the perceived color of the target may change because of the varying lighting conditions. In this paper, we learn the color patterns offline from pixels sampled from images across different camera views. In the new color feature space, the proposed tracking method performs robustly in various environment. The new color feature space is learned by learning a linear transformation and a dictionary to encode pixel values. To speedup the feature extraction, we use the marginal regression to calculate the sparse feature codes. Experimental results demonstrate that significant improvement can be achieved by using our learned color features, especially on the video sequences with complicated lighting conditions.","T. Liu and R. R. Varior and G. Wang",2015,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Problem and Project-Based Learning in Scripting Lab","Scripting language employ high-level constructs to interpret and execute one command at a time. In general scripting languages are easier to learn and faster to code than structured and compiled languages such as C and C++. Scripting languages have many important advantages over traditional programming languages. In future the usage of these languages is likely to increase. In this paper we discuss and report our experience in teaching scripting languages lab at the undergraduate level, 4th semester. Scripting language is an umbrella term used for languages like unix shell, TCL, perl, java, python and LISP. Out of these, we have chosen UNIX shell programming and python for our curriculum. The authors report various pedagogical activities like multiple assignments, peer assessment within a group, self learning through e-resources and course project that were employed during the course. The course projects were specially designed so as to make students explore the vast number of python packages. The authors found that these activities definitely enhance the learning experience and there was a remarkable change in the learning level of the students as compared to previous years as evident in the grades obtained by the students.","S. Giraddi and S. Yaligar and H. S. Kavitha",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"What if I Had No Smells?","What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.","D. Falessi and B. Russo and K. Mullen",2017,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Concern-based cohesion: Unveiling a hidden dimension of cohesion measurement","Cohesion has been avidly recognized as a key property of software modularity. Ideally, a software module is considered to be cohesive if it represents an abstraction of a single concern of the software. Modules with several concerns may be harder to understand because developers must mentally separate the source code related to each concern. Also, modules implementing several concerns are more likely to undergo changes as much as distinct development tasks may target its different concerns. The most well-known cohesion metrics are defined in terms of the syntactical structure of a module, and as a consequence fail to capture the amount of concerns realized by the module. In this context, we investigated the potential of a new metric, called Lack of Concern-based Cohesion. This metric explicitly counts the number of concerns realized by each module. We compared this metric with other five structural cohesion metrics by applying them over six open source software systems. We studied how those metrics are associated with module changes by mining over 16,000 repository revisions. Our results pointed out that the concern-based metric captured a cohesion dimension that is not reflected by structural metrics, and, as a consequence, adds to the association of cohesion and change-proneness.","B. Silva and C. Sant'Anna and C. Chavez and A. Garcia",2012,"[""IEEE""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Type-Safe Evolution of Web Services","Applications based on micro or web services have had significant growth due to the exponential increase in the use of mobile devices. However, using such kind of loosely coupled interfaces provides almost no guarantees to the developer in terms of evolution. Changes to service interfaces can be introduced at any moment, which may cause the system to fail due to mismatches between communicating parts. In this paper, we present a programming model that allows the development of web service applications, server end-points and their clients, in such a way that the evolution of services' implementation does not cause the disruption of the client. Our approach is based on a type based code slicing technique that ensures that each version only refers to type compatible code, of the same version or of a compatible version, and that each client request is redirected to the most recent type compatible version implemented by the server. We abstract the notion of version and parametrize type compatibility on the relation between versions. The relation between versions is tagged with compatibility levels, so to capture the common conventions used in software development. Our implementation allows multiple versions of a service to be deployed simultaneously, while reusing code between versions in a type safe way. We describe a prototype framework, based on code transformation, for server-side JavaScript code, and using Flow as verification tool.","J. Campinhos and J. C. Seco and J. Cunha",2017,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"An Empirical Analysis of the Docker Container Ecosystem on GitHub","Docker allows packaging an application with its dependencies into a standardized, self-contained unit (a so-called container), which can be used for software development and to run the application on any system. Dockerfiles are declarative definitions of an environment that aim to enable reproducible builds of the container. They can often be found in source code repositories and enable the hosted software to come to life in its execution environment. We conduct an exploratory empirical study with the goal of characterizing the Docker ecosystem, prevalent quality issues, and the evolution of Dockerfiles. We base our study on a data set of over 70000 Dockerfiles, and contrast this general population with samplings that contain the Top-100 and Top-1000 most popular Docker-using projects. We find that most quality issues (28.6%) arise from missing version pinning (i.e., specifying a concrete version for dependencies). Further, we were not able to build 34% of Dockerfiles from a representative sample of 560 projects. Integrating quality checks, e.g., to issue version pinning warnings, into the container build process could result into more reproducible builds. The most popular projects change more often than the rest of the Docker population, with 5.81 revisions per year and 5 lines of code changed on average. Most changes deal with dependencies, that are currently stored in a rather unstructured manner. We propose to introduce an abstraction that, for instance, could deal with the intricacies of different package managers and could improve migration to more light-weight images.","J. Cito and G. Schermann and J. E. Wittern and P. Leitner and S. Zumberi and H. C. Gall",2017,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Neural Conditional Energy Models for Multi-label Classification","Multi-label classification (MLC) is a type of structured output prediction problems where a given instance can be associated to more than one labels at a time. From the probabilistic point of view, a model predicts a set of labels y given an input vector v by learning a conditional distribution p(y|v). This paper presents a powerful model called a Neural Conditional Energy Model (NCEM) to solve MLC. The model can be viewed as a hybrid deterministic-stochastic network of which we use a deterministic neural network to transform the input data, before contributing to the energy landscape of v, y, and a single stochastic hidden layer h. Non-linear transformation given by the neural network makes our model more expressive and more capable of capturing complex relations between input and output, and using deterministic neurons facilitates exact inference. We present an efficient learning algorithm that is simple to implement. We conduct extensive experiments on 15 real-world datasets from wide variety of domains with various evaluation metrics to confirm that NCEM is significantly superior to current state-of-the-art models most of the time based on pair-wise t-test at 5% significance level. The MATLAB source code to replicate our experiments are available at https://github.com/Kublai-Jing/NCEM.","H. Jing and S. Lin",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Management of the Quality of Teaching at Universities - a course for teachers at the Technical University of Kosice developed within the operational programme “Education”","The aim of this contribution is to provide the basic information concerning the course “Management of the Quality of Teaching at Universities”. This course was prepared within the framework of the operational programme Education “Package of Innovative Elements for the Transformation of Education at TUKE” (code ITMS 26110230018). In recent weeks we finished the implementation of the pilot course that was focused to the development of professional skills of teachers and to the enhancement of quality education at TUKE (Technical University of Kosice).","M. Blaško and P. Raschman",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"How much really changes? A case study of firefox version evolution using a clone detector","This paper focuses on the applicability of clone detectors for system evolution understanding. Specifically, it is a case study of Firefox for which the development release cycle changed from a slow release cycle to a fast release cycle two years ago. Since the transition of the release cycle, three times more versions of the software were deployed. To understand whether or not the changes between the newer versions are as significant as the changes in the older versions, we measured the similarity between consecutive versions.We analyzed 82MLOC of C/C++ code to compute the overall change distribution between all existing major versions of Firefox. The results indicate a significant decrease in the overall difference between many versions in the fast release cycle. We discuss the results and highlight how differently the versions have evolved in their respective release cycle. We also relate our results with other results assessing potential changes in the quality of Firefox. We conclude the paper by raising questions on the impact of a fast release cycle.","T. Lavoie and E. Merlo",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Integrating design freeze in to large-scale construction projects in Sri Lanka","Change is inevitable in most construction projects due to uniqueness of projects and limited resources. Previous studies revealed that design changes are accountable for 78% of the total cost deviation in construction projects. Design Freeze (DF) is one method used during design development stage to mitigate the risks associated with change. This organizes and complies the design process, control changes, and force the completion of design stages on time. However, this is a novel concept to the construction industry. Hence, this study focused on investigating the DF concept in terms of its adoptability in large-scale construction projects to enhance project performance by minimizing changes. The present study adopted a mixed approach where the questionnaire survey and expert interviews helped to derive findings. Questionnaire survey was administered to hundred (100) industry professionals with a response rate of 87%. Further, an expert opinion survey was performed with nine subject matter experts. The quantitative data were analysed using SPSS software, whereas code based content analysis using Nvivo version 11 supported to analyse qualitative data. The findings revealed that the client and the consultant are the major parties causing design changes. Selecting an experienced and qualified design team, permitting adequate time frame for design development, and etc. are the significant factors to be considered when implementing a DF. Furthermore, the study identified the impracticability of adopting a 100% design freeze in large-scale construction projects, and the interviewees accepted the concept of partial design freeze by identifying strategies targeted to maximize the design percentage.","R. M. N. Hemal and K. G. A. S. Waidyasekara and E. M. A. C. Ekanayake",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Porting Existing Radiation Code for GPU Acceleration","Graphics processing units (GPUs) have proven very robust architectures for performing intensive scientific calculations, resulting in speedups as high as several hundred times. In this paper, the GPU acceleration of a radiation code for use in creating simulated satellite observations of predicted climate change scenarios is explored, particularly the prospect of porting an already existing and widely used radiation transport code to a GPU version that fully exploits the parallel nature of GPUs. The porting process is attempted with a simple radiation code, revealing that this process centers on creating many copies of variables and inlining function/subroutine calls. A resulting speedup of about 25x is reached. This is less than the speedup achieved from a radiation code built for CUDA from scratch, but it was achieved with an already existing radiation code using the PGI Accelerator to automatically generate CUDA kernels, and this demonstrates a possible strategy to speed up other existing models like MODTRAN and LBLRTM.","D. M. Coleman and D. R. Feldman",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Performance Analysis of Paralldroid Generated Programs","The advent of emergent System-on-Chip (SoCs) and multiprocessor System-on-Chip (MPSocs) opens a new era on the small mobile devices (Smartphones, Tablets, ...) in terms of computing capabilities and applications to be addressed. The efficient use of such devices, including the parallel power, is still a challenge for general purpose programmers due to the very high learning curve demanding very specific knowledge of the devices. While some efforts are currently being made, mainly in the scientific scope, the scenario is still quite far from being the desirable for non-scientific applications where very few of them take advantage of the parallel capabilities of the devices. We develop a performance analysis in several SoCs using Paralldroid. Paralldroid (Framework for Parallelism in Android), is a parallel development framework oriented to general purpose programmers for standard mobile devices. Paralldroid presents a programming model that unifies the different programming models of Android. The user just implements a Java application and introduces a set of Paralldroid annotations in the sections of code to be optimized. The Paralldroid system automatically generates the native C, OpenCL or Renderscript code for the annotated section. The Paralldroid transformation model involves source-to-source transformations and skeletal programming.","A. Acosta and F. Almeida",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Cognitive radio antennas that learn and adapt using Neural Networks","Cognition added to RF/antenna systems has extended software defined radio (SDR) communication systems into cognitive radio systems. Software defined radio has been established as a key enabling technology to realize cognitive radio. Thus a cognitive radio is an SDR that is aware of its environment, and autonomously adjusts its operations to achieve the designated objectives. A cognitive radio system is able to sense, reason, learn and be aware of its environment. A dynamic communication application such as cognitive radio requires antenna researchers to design software controlled reconfigurable antennas. The tuning ability of such antennas and the switching time are important to satisfy the requirements of continuously changing communication channels. Neural Networks (NNs) arose as a perfect candidate to control these antennas through Field Programmable Gate Arrays (FPGAs). NNs represent a perfect solution to add learning and reasoning to the cognitive radio antenna systems. In this work, a NN is applied on a reconfigurable antenna where switches are used to connect and disconnect the different parts of its structure. Reconfigurable antennas are potential candidate for cognitive radio since they are able to change their operating characteristics based on the channel activity. Applying NNs to such antennas result in the association of different antenna configurations with the various frequency responses. This association allows training the NN to be able to configure the antenna and regenerate switch combinations/frequency responses on demand. The NN is built and trained in Matlab Simulink and a Xilinx system generator creates the NN VHDL code to be transferred to the FPGA. The FPGA now controls the switches that are incorporated within the reconfigurable antenna structure. The application of NN on cognitive radio antenna systems allows such systems to react swiftly to any change in their environment. The cognitive radio antennas will regenerate the appropriate switch combinations using NN previous training. This will allow communicating over the unoccupied parts of the spectrum which are called white spaces. The dynamic changes that occur in the spectrum require a robust and fast antenna software control. Thus NN prove to be a valid and necessary technique to employ on CR antennas.","Y. Tawk and J. Costantine and E. Al-Zuraiqi and C. G. Christodoulou",2013,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"MUZZEUM — Augmented Reality and QR codes enabled mobile platform with digital library, used to Guerrilla open the National Museum of Serbia","Serbian National Museum has been closed for past 10 years for renovation. Furthermore, certain inadequate actions of the Government and national cultural institutions have been made. A group of activists decided to make a statement and perform a virtual exhibition in front of the National museum by using QR codes and Augmented Reality. This paper focuses on technical and social implications of consumption and implementation of QR codes and Augmented Reality in terms of museology, cultural heritage, objects and relationship they create with people who are their potential consumers. In the case study, we are tackling here, we investigate the transformation in definitions of space, artifacts, visitor and the power of representation in terms of museology. Since the artifacts which represent the Serbian national heritage of the highest rank are being held in museum depots far from the eyes of the public for such a long time, the project team has produced QR codes which will bring some of the masterpieces back to its audience. The objects from different museums will be represented by special ARQR codes outside of the museum building, reachable by smartphones. This socially engaged project sends direct message to the wider public that usage of IT, mobile technologies, Augmented Reality and QR codes can potentially transform the way we communicate cultural heritage.","V. Jevremovic and S. Petrovski",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Pseudo labels for imbalanced multi-label learning","The classification with instances which can be tagged with any of the 2<sup>L</sup> possible subsets from the predefined L labels is called multi-label classification. Multi-label classification is commonly applied in domains, such as multimedia, text, web and biological data analysis. The main challenge lying in multi-label classification is the dilemma of optimising label correlations over exponentially large label powerset and the ignorance of label correlations using binary relevance strategy (1-vs-all heuristic). The classification with label powerset usually encounters with highly skewed data distribution, called imbalanced problem. While binary relevance strategy reduces the problem from exponential to linear, it totally neglects the label correlations. In this artical, we propose a novel strategy of introducing Balanced Pseudo-Labels (BPL) which build more robust classifiers for imbalanced multi-label classification, which embeds imbalanced data in the problems innately. By incorporating the new balanced labels we aim to increase the average distances among the distinct label vectors. In this way, we also code the label correlation implicitly in the algorithm. Another advantage of the proposed method is that it can combined with any classifier and it is proportional to linear label transformation. In the experiment, we choose five multi-label benchmark data sets and compare our algorithm with the most state-of-art algorithms. Our algorithm outperforms them in standard multi-label evaluation in most scenarios.","Wenrong Zeng and X. Chen and Hong Cheng",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"A Bayesian network approach for compiler auto-tuning for embedded processors","The complexity and diversity of today's architectures require an additional effort from the programmers in porting and tuning the application code across different platforms. The problem is even more complex when considering that also the compiler requires some tuning, since standard optimization options have been customized for specific architectures or designed for the average case. This paper proposes a machine-learning approach for reducing the cost of the compiler auto-tuning phase and to speedup the application performance in embedded architectures. The proposed framework is based on an application characterization done dynamically with microarchitecture independent features and based on the usage of Bayesian Networks. The main characteristic of the Bayesian Network approach consists of not describing the solution as a strict set of compiler transformations to be applied, but as a complex probability distribution function to be sampled. Experimental results, carried out on an ARM platform and GCC transformation space, proved the effectiveness of the proposed methodology for the selected benchmarks. The selected set of solutions (less than 10% of the search space) demonstrated to be very close to the optimal sequence of transformations, showing also an applications performance speedup up to 2.8 (1.5 on average) with respect to -O2 and -O3 for the cBench suite. Additionally, the proposed method demonstrated a 3× speedup in terms of search time with respect to an iterative compilation approach, given the same quality of the solutions1.","A. H. Ashouri and G. Mariani and G. Palermo and C. Silvano",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Active Convolution: Learning the Shape of Convolution for Image Classification","In recent years, deep learning has achieved great success in many computer vision applications. Convolutional neural networks (CNNs) have lately emerged as a major approach to image classification. Most research on CNNs thus far has focused on developing architectures such as the Inception and residual networks. The convolution layer is the core of the CNN, but few studies have addressed the convolution unit itself. In this paper, we introduce a convolution unit called the active convolution unit (ACU). A new convolution has no fixed shape, because of which we can define any form of convolution. Its shape can be learned through backpropagation during training. Our proposed unit has a few advantages. First, the ACU is a generalization of convolution, it can define not only all conventional convolutions, but also convolutions with fractional pixel coordinates. We can freely change the shape of the convolution, which provides greater freedom to form CNN structures. Second, the shape of the convolution is learned while training and there is no need to tune it by hand. Third, the ACU can learn better than a conventional unit, where we obtained the improvement simply by changing the conventional convolution to an ACU. We tested our proposed method on plain and residual networks, and the results showed significant improvement using our method on various datasets and architectures in comparison with the baseline. Code is available at https://github.com/jyh2986/Active-Convolution.","Y. Jeon and J. Kim",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Input selection for fast feature engineering","The application of machine learning to large datasets has become a vital component of many important and sophisticated software systems built today. Such trained systems are often based on supervised learning tasks that require features, signals extracted from the data that distill complicated raw data objects into a small number of salient values. A trained system's success depends substantially on the quality of its features. Unfortunately, feature engineering-the process of writing code that takes raw data objects as input and outputs feature vectors suitable for a machine learning algorithm-is a tedious, time-consuming experience. Because “big data” inputs are so diverse, feature engineering is often a trial-and-error process requiring many small, iterative code changes. Because the inputs are so large, each code change can involve a time-consuming data processing task (over each page in a Web crawl, for example). We introduce Zombie, a data-centric system that accelerates feature engineering through intelligent input selection, optimizing the “inner loop” of the feature engineering process. Our system yields feature evaluation speedups of up to 8× in some cases and reduces engineer wait times from 8 to 5 hours in others.","M. R. Anderson and M. Cafarella",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Mining network traffic for application category recognition on Android platform","Signature-based static mobile malware detection is fragile when facing code obfuscation and transformation attacks. Behavior based malware detection mechanisms have been widely studied and experimented. So far only the application's running behaviors, such as API calls and resource consumption are used, which can also be easily concealed and obfuscated with various coding tricks. Most mobile malware need either cellular or network connection to conduct their malicious activities. We propose to monitor an application's network behavior and interaction to characterize application behaviors. An integrated testbed system has been designed and prototyped for such network behavior collection. Statistical features are derived from application network traffic, which are further fed to a machine-learning based classifier to build one general model for each typical category of mobile applications. Experiments show that applications in each category with identical functionality exhibit similar network behaviors, which makes it possible to use the derived category model of network behaviors to evaluate future unknown application for its trustworthiness.","S. Wei and Gaoxiang Wu and Ziyang Zhou and L. Yang",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Variation support for end users","End-user programming environments provide central repositories where users can execute and store their programs. However, these environments do not provide facilities by which users can keep track of the variations that they create for their programs. In the professional world, software developers use variation management for code reuse, program understanding, change traceability, debugging and maintenance.","S. K. Kuttal",2013,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"A Vision for Interactive Suggested Examples for Novice Programmers","Many systems aim to support programmers within a programming context, whether they recommend API methods, example code, or hints to help novices solve a task. The recommendations may change based on the user's code context, history, or the source of the recommendation content. They are designed to primarily support users in improving their code or working toward a task solution. The recommendations themselves rarely provide support for a user to interact with them directly, especially in ways that benefit the knowledge or understanding of the user. This poster presents a vision and preliminary designs for three ways a user might learn from interactions with suggested examples: describing examples, providing detailed relevance feedback, and selective visualization and tinkering.","M. Ichinco",2018,"[""IEEE"",""Engineering Village""]","Aceito: CA7","Aceito: CA7, CA3"
"2017 NESC(R) Handbook, Eighth Edition","The 2017 NESC Handbook, Eight Edition, a Discussion of the National Electrical Safety Code(R), is an essential companion to the Code. This handbook includes text directly from Code which provides users an easy reference back to the code, ruling-by-ruling. It gives users insight into what lies behind the NESC(R)s rules and how to apply them. The Handbook was developed for use at many levels in the electric and communication industries, including those involved in system design, construction, maintenance, inspection, standards development and worker training. The Handbook also discusses how the NESC Committee has developed the rules in the Code and responded to change proposals during the past 100 years. This allows users to understand how questions they may have were dealt with in the past. These are key points from the 2017 Handbook Edition:- Revising the purpose rule to include only the safeguarding of persons and utility facilities and clarifying the application- Deleting unused definitions and adding definitions for communication and supply space.- Revising the substation impenetrable fence requirements.- Adding an exception to exempt underground cable grounding requirements from the 4 grounds in each mile rule under certain conditions.- Revising and reorganizing the guy insulator placement rules along with eliminating the voltage transfer requirements associated with them.- Requiring a 40 vertical clearance from communication cables in the communication space if a luminaire is not effectively grounded.- Deleting the conductance requirement for underground insulating jacketed grounded neutral supply cables and revising the grounding and bonding rules for supply and communication cables in random separation installations.- Revising and reorganizing the Grades of Construction Table 242-1 that will now include service drops.- Revising the strength rules to require that all conductors be considered for damage due to Aeolian vibration.- Revising the rules in Part 4 to align with changes made to 29 CFR by the Occupational Safety and Health Administration (OSHA).","N. G. Bingel III and C. C. Bleakley and A. L. Clapp and J. H. Dagenhart and M. A. Konz and L. E. Gaunt and M. B. Gunter and M. F. Jurgemeyer",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"2017 NESC(R) Handbook, Premier Edition","The 2017 NESC(R) Handbook, Premier Edition, is an essential companion to the Code. This handbook includes text directly from Code which provides users an easy reference back to the code, ruling-by-ruling. It gives users insight into what lies behind the NESC's rules and how to apply them. The Handbook was developed for use at many levels in the electric and communication industries, including those involved in system design, construction, maintenance, inspection, standards development and worker training. The Handbook also discusses how the NESC Committee has developed the rules in the Code and responded to change proposals during the past 100 years. This allows users to understand how questions they may have were dealt with in the past. The Premier Edition includes: Revising the purpose rule to include only the safeguarding of persons and utility facilities and clarifying the application rules. Deleting unused definitions and adding definitions for communication and supply space. Revising the substation impenetrable fence requirements. Adding an exception to exempt underground cable grounding requirements from the 4 grounds in each mile rule under certain conditions. Revising and reorganizing the guy insulator placement rules along with eliminating the voltage transfer requirements associated with them. Requiring a 40 vertical clearance from communication cables in the communication space if a luminaire is not effectively grounded. Deleting the conductance requirement for underground insulating jacketed grounded neutral supply cables and revising the grounding and bonding rules for supply and communication cables in random separation installations. Revising and reorganizing the Grades of Construction Table 242-1 that will now include service drops. Revising the strength rules to require that all conductors be considered for damage due to Aeolian vibration. Revising the rules in Part 4 to align with changes made to 29 CFR by the Occupational Safety and Health Administration (OSHA).","",2016,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR4"
"Filtering of false positives from IR-based traceability links among software artifacts","Correlation among software artifacts (also known as traceability links) of object oriented software plays a vital role in its maintenance. These traceability links are being commonly identified through Information Retrieval (IR) based techniques. But, it has been found that the resulting links from IR contain many false positives and some complementary approaches have been suggested for the purpose. Still, it usually requires manual verification of links which is neither desirable nor reliable. This paper suggests a new technique which can automatically filter out the false positives links (between requirement and source code) from IR and thus can help in reducing dependence as well as incorrectness of manual verification process. The proposed approach works on the basis of finding correlations among classes using either structural or co-changed dependency or both. A threshold is selected as a cut off on computed dependency values, to accept the presence of structural and co-changed dependency each. Now the traceability links are verified using these dependencies. If atleast one of the structural or co-change information validates the link obtained from IR approach, then that link is selected as candidate link, otherwise removed. Different thresholds have been experimented and comparison of results obtained from IR and the proposed approach is done. The results show that precision increases for all values of thresholds. Further analysis of results indicates that threshold in the range of 0.3 to 0.5 give better results. Hence, the proposed approach can be used as complementary to other Improved IR approaches to filter out false positives.","Jyoti and J. K. Chhabra",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Stealthy software: Next-generation cyber-attacks and defenses","Weaponized software is the latest development in a decades-old battle of virus-antivirus co-evolution. Reactively adaptive malware and automated binary transformation are two recently emerging offensive and defensive (respectively) technologies that may shape future cyberwarfare weapons. The former intelligently learns and adapts to antiviral defenses fully automatically in the wild, while the latter applies code mutation technology to defense, transforming potentially dangerous programs into safe programs. These technologies and their roles within the landscape of malware attack and defense are examined and discussed.","K. W. Hamlen",2013,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Efficient Dynamic Malware Analysis Based on Network Behavior Using Deep Learning","Malware authors or attackers always try to evade detection methods to accomplish their mission. Such detection methods are broadly divided into three types: static feature, host-behavior, and network-behavior based. Static feature-based methods are evaded using packing techniques. Host- behavior-based methods also can be evaded using some code injection methods, such as API hook and dynamic link library hook. This arms race regarding static feature-based and host-behavior- based methods increases the importance of network-behavior-based methods. The necessity of communication between infected hosts and attackers makes it difficult to evade network-behavior- based methods. The effectiveness of such methods depends on how we collect a variety of communications by using malware samples. However, analyzing all new malware samples for a long period is infeasible. Therefore, we propose a method for determining whether dynamic analysis should be suspended based on network behavior to collect malware communications efficiently and exhaustively. The key idea behind our proposed method is focused on two characteristics of malware communication: the change in the communication purpose and the common latent function. These characteristics of malware communications resemble those of natural language from the viewpoint of data structure, and sophisticated analysis methods have been proposed in the field of natural language processing. For this reason, we applied the recursive neural network, which has recently exhibited high classification performance, to our proposed method. In the evaluation with 29,562 malware samples, our proposed method reduced 67.1% of analysis time while keeping the coverage of collected URLs to 97.9% of the method that continues full analyses.","T. Shibahara and T. Yagi and M. Akiyama and D. Chiba and T. Yada",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Selection and Ranking of Optimal Routes through Genetic Algorithm in a Cognitive Routing System for Mobile Ad Hoc Network","Genetic algorithm can be used for proper selection and ranking of all possible variable route addresses in mobile ad-hoc network (MANET). Ranking is based upon priority code of the links. a priority code is calculated by respective routing protocol, which depends on different parameters and metrics. a node can change its position and new nodes may join the MANET, so genetic algorithm can better estimate such kind of variations through its crossover and mutation genetic operators. Genetic algorithm is especially useful in cases of novel cognitive routing for MANET. Cognition in MANET is either based upon learning automata method as in some wireless sensor networks or specialized cognitive neural networks. Ranking of optimal links in MANET after a regular interval through genetic algorithm enhance the performance of cognitive routing. It help in proper selection of desired routing protocol for a given set of network conditions.","M. I. Afridi",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Exploring the educational potential of QR codes","Education through the past years has changed the way teaching and learning is inculcated to the new generation. The rapid growth of technology has accelerated the change in teaching methods and has given birth to new methods like distance learning and online learning, where lectures notes and material are shared via educational platforms. This leaves traditional face to face lecture session to a discussion type session where tutor and student discuss about topics and do more class activity. Technology offers new ways in transferring data and information in large amounts very rapidly and also offers the accessibility not only to large information pools such as the internet but also offers access to online storage remotely accessible. One way education can benefit from this is to make use of Quick Response (QR) Code. With its potential, QR Codes can be exploited to change the ways both tutors and students interact in class. This can be achieved by combining Mobile technology, Quick Response Code technology and Cloud technology.","R. K. Sungkur and V. Neermul and V. Tauckoor",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"The impact of refactoring changes on the SZZ algorithm: An empirical study","SZZ is a widely used algorithm in the software engineering community to identify changes that are likely to introduce bugs (i.e., bug-introducing changes). Despite its wide adoption, SZZ still has room for improvements. For example, current SZZ implementations may still flag refactoring changes as bug-introducing. Refactorings should be disregarded as bug-introducing because they do not change the system behaviour. In this paper, we empirically investigate how refactorings impact both the input (bug-fix changes) and the output (bug-introducing changes) of the SZZ algorithm. We analyse 31,518 issues of ten Apache projects with 20,298 bug-introducing changes. We use an existing tool that automatically detects refactorings in code changes. We observe that 6.5% of lines that are flagged as bug-introducing changes by SZZ are in fact refactoring changes. Regarding bug-fix changes, we observe that 19.9% of lines that are removed during a fix are related to refactorings and, therefore, their respective inducing changes are false positives. We then incorporate the refactoring-detection tool in our Refactoring Aware SZZ Implementation (RA-SZZ). Our results reveal that RA-SZZ reduces 20.8% of the lines that are flagged as bug-introducing changes compared to the state-of-the-art SZZ implementations. Finally, we perform a manual analysis to identify change patterns that are not captured by the refactoring identification tool used in our study. Our results reveal that 47.95% of the analyzed bug-introducing changes contain additional change patterns that RA-SZZ should not flag as bug-introducing.","E. C. Neto and D. A. da Costa and U. Kulesza",2018,"[""IEEE""]","Rejeitado: CR11","Rejeitado: CR11"
"Sensory-motor dynamics of mother-infant-object interactions: Longitudinal changes in micro-behavioral patterns across the first year","Summary form only given. Infants rapidly develop the skills to coordinate attention to objects and people. In particular, the period 9-12 months is thought to bring a new ability to coordinate with and to share the objects of another persons' attention-like following the gaze of the mother or playing a ball tossing game together. This has been termed “triadic” or “you-me-it” attention and is thought to be the basis for future learning, including early language. The development of triadic attention was investigated in a longitudinal sample of 26 mother-infant dyads interacting with objects in a naturalistic setting. Video recordings of sessions when the infants were four, six, nine, and twelve months of age were analyzed frame-by-frame, coding multiple dimensions of mother and infant sensory-motor contact as they attended to one another and to a set of shared objects. Specifically, we coded targets of gaze and manual contact of both participants, noting all changes occurring at 10Hz. Additionally, we coded salient macro level features of the interaction, such as moments of imitation and games. Databases detailing the micro behaviors of the interaction are only beginning to be coded in developmental psychology. Our dataset is the first of its kind to be created in a naturalistic home environment with infants younger than a year. Previous accounts code at the level of the triad (e.g. jointly attending to a toy vs. infant solo toy play). This has led to the conclusion that the development of triadic attention is the result of a sudden, qualitative leap in infant cognitive abilities. By contrast, our analyses of changes in sensory motor dynamics of attending indicate that gradual shifts in sensory motor coordination lead to continuous changes in mother-infant-object interactions over the first year. Our analyses indicate that at four months, infants converged all modalities (gaze and two hands) to a single target. Across individual infants, such sensory-motor coupling decreased gradually over the first year of life. This change fundamentally alters the dyadic social interaction. Specifically, we found that the degree of decoupling was related to the infant's responses to mother's actions on toys. Young infants transitioned all of their modalities to toys manipulated by mom. Over the course of the first year, infants increasingly distributed their attention between toys manipulated by mother and toys in their own possession. This allows the infant to observe manipulations made by mom on one set of objects while simultaneously maintaining a high degree of sensory motor contact on with another object, upon which they can practice performing those same manipulations. We confirmed these relations quantitatively by first creating an index of “total modality time” spent attending to ""already-available"" versus ""maternal bid"" objects. This was a temporal summation of all infant modalities directed towards both kinds of objects, after the mother presented a new object. We modeled session by session changes in the distribution of attending with a beta model, indicating a gradual shift in relative proportions of attending across the first year. Split halves analyses indicated that within a single session, infants who showed higher proportions of sensory-motor coupling showed less mature patterns of responding to the maternal bids, whereas those who decoupled more resembled the profiles of older infants. Overall, our results indicate that triadic attention emerges not as a novel form of social-cognition, but as a continuous product of sensory-motor maturation . In particular, infants changing abilities to distribute attention between multiple targets allows for increasing complexity of mother-infant-object manipulations.","K. de Barbaro and C. M. Johnson and D. Forster and G. Littlewort and G. Deak",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Fault prediction model for software using soft computing techniques","Faulty modules of any software can be problematic in terms of accuracy, hence may encounter more costly redevelopment efforts in later phases. These problems could be addressed by incorporating the ability of accurate prediction of fault prone modules in the development process. Such ability of the software enables developers to reduce the faults in the whole life cycle of software development, at the same time it benefits automation process, and reduces the overall cost and efforts of the software maintenance. In this paper, we propose to design fault prediction model by using a set of code and design metrics; applying various machine learning (ML) classifiers; also used transformation techniques for feature reduction and dealing class imbalance data to improve fault prediction model. The data sets were obtained from publicly available PROMISE repositories. The results of the study revealed that there was no significant impact on the ability to accurately predict the fault-proneness of modules by applying PCA in reducing the dimensions; the results were improved after balancing data by SMOTE, Resample techniques, and by applying PCA with Resample in combination. It has also been seen that Random Forest, Random Tree, Logistic Regression, and Kstar machine learning classifiers have relatively better consistency in prediction accuracy as compared to other techniques.","I. U. Nisa and S. N. Ahsan",2015,"[""IEEE"",""Engineering Village""]","Aceito: CA5, CA6","Aceito: CA5, CA6"
"Analyzing the significance of process metrics for TT amp;amp;C software defect prediction","In the existing studies on software prediction, the most proposed methods are usually assessed over the public datasets like NASA metrics data repository, which include a combination of code metrics merely. Obviously, the process metric is also one of the key factors that affect the defect-proneness of software modules. In this paper, life-cycle based management process metrics set and history change process metrics set have been proposed based on the characteristics of development process. In order to analyze the importance of these different metrics for predicting defects in aerospace tracking telemetry and control (TT&amp;C) software, an improved PSO optimized support vector machine algorithm (PSO-SVM) has been presented and took into application. The experiment results over the actual TT&amp;C projects suggest that the prediction performance can be significance improved if the 2 kinds of process metrics are included in the model.","Y. Xia and G. Yan and H. Zhang",2014,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Individual detection-tracking-recognition using depth activity images","In this paper, a depth camera-based novel approach for human activity recognition is presented using robust depth silhouettes context features and advanced Hidden Markov Models (HMMs). During HAR framework, at first, depth maps are processed to identify human silhouettes from noisy background by considering frame differentiation constraints of human body motion and compute depth silhouette area for each activity to track human movements in a scene. From the depth silhouettes context features, temporal frames information are computed for intensity differentiation measurements, depth history features are used to store gradient orientation change in overall activity sequence and motion difference features are extracted for regional motion identification. Then, these features are processed by Principal component analysis for dimension reduction and k-mean clustering for code generation to make better activity representation. Finally, we proposed a new way to model, train and recognize different activities using advanced HMM. Experimental results show superior recognition rate, resulting up to the mean recognition of 57.69% over the state of the art methods using IM-DailyDepthActivity dataset. In addition, MSRAction3D dataset also showed some promising results.","A. Jalal and S. Kamal and D. Kim",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"The Sparse Polyhedral Framework: Composing Compiler-Generated Inspector-Executor Code","Irregular applications such as big graph analysis, material simulations, molecular dynamics simulations, and finite element analysis have performance problems due to their use of sparse data structures. Inspector-executor strategies improve sparse computation performance through parallelization and data locality optimizations. An inspector reschedules and reorders data at runtime, and an executor is a transformed version of the original computation that uses the newly reorganized schedules and data structures. Inspector-executor transformations are commonly written in a domain-specific or even application-specific fashion. Significant progress has been made in incorporating such inspector-executor transformations into existing compiler transformation frameworks, thus enabling their use with compile-time transformations. However, composing inspector-executor transformations in a general way has only been done in the context of the Sparse Polyhedral Framework (SPF). Though SPF enables the general composition of such transformations, the resulting inspector and executor performance suffers due to missed specialization opportunities. This paper reviews the history and current state of the art for inspector-executor strategies and reviews how the SPF enables the composition of inspector-executor transformations. Further, it describes a research vision to combine this generality in SPF with specialization to achieve composable and high performance inspectors and executors, producing a powerful compiler framework for sparse matrix computations.","M. M. Strout and M. Hall and C. Olschanowsky",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Learning Binary Descriptors for Fingerprint Indexing","Fingerprint indexing is studied widely with the real-valued features, but few works focus on the binary feature descriptors, which are more appropriate to retrieve fingerprints efficiently in the largescale fingerprint database. In this paper, the binary fingerprint descriptor (BFD), which is an effective and discriminative binary feature representation for fingerprint indexing, is proposed based on minutia cylinder code (MCC). Specifically, we first analyze MCC to find that it has characteristics of the high dimensionality, redundancy, and quantization loss. Accordingly, we propose an optimization model to learn a feature-transformation matrix, resulting in dimensionality reduction and diminishing quantization loss. Meanwhile, we also incorporate the balance, independence, and similarity-preservation properties in this learning process. Eventually, a multi-index hashing-based fingerprint indexing scheme further accelerate the exact search in Hamming space. The experiments on numerous public databases show that the BFD is discriminative and compact and that the proposed approach is outstanding for fingerprint indexing.","C. Bai and M. Li and T. Zhao and W. Wang",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"APPification of hospital healthcare and data management using QRcodes","In this work, we describe an integrated system, developed for use by the healthcare personnel within healthcare facilities, adapted to smartphones, tablets and handheld devices. Our key goal is to facilitate doctors, nurses and the involved personnel throughout the facility, regardless of the existence of network connection in the area using a typical smartphone. The proposed application and its backend system support access to patient's history, i.e. previous diagnoses, medication, and specification of allergies. More features include updates on the progress of the patient, sending referrals directly to hematology, microbiology or biochemistry laboratory and instant notification for the retrieval of laboratory results within a hospital or any healthcare institution. Additionally, we integrate Quick Response (QR code) for coding and accessing medical related data of the patient using a smartphone or a tablet, to be used by the facility itself or anyone else certified. The QR code significantly improves interoperability cases for legacy and non interrelated systems based on HL7 and XML transformation of the HL7 patient referrals.","P. Mersini and E. Sakkopoulos and A. Tsakalidis",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Towards indicators of instabilities in software product lines: An empirical evaluation of metrics","A Software Product Line (SPL) is a set of software systems (products) that share common functionalities, so-called features. The success of a SPL design is largely dependent on its stability; otherwise, a single implementation change will cause ripple effects in several products. Therefore, there is a growing concern in identifying means to either indicate or predict design instabilities in the SPL source code. However, existing studies up to now rely on conventional metrics as indicators of SPL instability. These conventional metrics, typically used in standalone systems, are not able to capture the properties of SPL features in the source code, which in turn might neglect frequent causes of SPL instabilities. On the other hand, there is a small set of emerging software metrics that take into account specific properties of SPL features. The problem is that there is a lack of empirical validation of the effectiveness of metrics in indicating quality attributes in the context of SPLs. This paper presents an empirical investigation through two set of metrics regarding their power of indicating instabilities in evolving SPLs. A set of conventional metrics was confronted with a set of metrics we instantiated to capture important properties of SPLs. The software evolution history of two SPLs were analysed in our studies. These SPLs are implemented using two different programming techniques and all together they encompass 30 different versions under analysis. Our analysis confirmed that conventional metrics are not good indicators of instabilities in the context of evolving SPLs. The set of employed feature dependency metrics presented a high correlation with instabilities proving its value as indicator of SPL instabilities.","B. B. P. Cafeo and F. Dantas and E. J. R. Cirilo and A. Garcia",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"A neural words encoding model","This paper proposes a neural network model and learning algorithm that can be applied to encode words. The model realizes the function of words encoding and decoding which can be applied to text encryption/decryption and word-based compression. The model is based on Deep Belief Networks (DBNs) and it differs from traditional DBNs in that it is asymmetric structured and the output of it is a binary vector. With pre-training of multi-layer Restricted Boltzmann Machines (RBMs) and fine-tuning to reconstruct word set, the output of code layer can be used as a kind of representation code of words. We can change the number of neurons of code layer to control the length of representation code for different applications. This paper reports on experiments using English words of American National Corpus to train a neural words encoding model which can be used to encode/decode English words, realizing text encryption and data compression.","Dayiheng Liu and J. Lv and Xiaofeng Qi and Jiangshu Wei",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Prioritization of smelly classes: A two phase approach (Reducing refactoring efforts)","Frequent changes in an object-oriented software system often result into a poor-quality and less maintainable design and the symptoms (known as Code Smells) causing that degradation, need to be corrected for which refactoring is one of the possible solutions. It is not feasible to refactor/ restructure each and every smelly class due to various constraints such as time and cost. Hence it is desirable to make an efficient approach of refactoring. Proposed scheme aims to save time (and cost) of refactoring by carrying out selective refactoring for high priority smelly classes. Prioritization is proposed to be done according to interaction level of each class with other classes. The proposed methodology works in two phases; first phase detects smelly classes using structural information of source code and second phase mines change history to prioritize smelly classes. This prioritization is used to carry out refactoring of more severe classes. This process helps in reducing efforts of refactoring and at the same time may result in avoiding refactoring chains. The proposed technique has been evaluated over a software consisting of 49 classes and results have been validated. The results clearly indicate that the proposed approach performs better and can be very useful for software maintainers in effective and efficient refactoring.","A. Rani and J. K. Chhabra",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12, CR11","Rejeitado: CR12, CR12"
"An empirical investigation of changes in some software properties over time","Software metrics are easy to define, but not so easy to justify. It is hard to prove that a metric is valid, i.e., that measured numerical values imply anything on the vaguely defined, yet crucial software properties such as complexity and maintainability. This paper employs statistical analysis and tests to check some plausible assumptions on the behavior of software and metrics measured for this software in retrospective on its versions evolution history. Among those are the reliability assumption implicit in the application of any code metric, and the assumption that the magnitude of change, i.e., increase or decrease of its size, in a software artifact is correlated with changes to its version number. Putting a suite of 36 metrics to the trial, we confirm most of the assumptions on a large repository of software artifacts. Surprisingly, we show that a substantial portion of the reliability of some metrics can be observed even in random changes to architecture. Another surprising result is that Boolean-valued metrics tend to flip their values more often in minor software version increments than in major increments.","J. Gil and M. Goldstein and D. Moshkovich",2012,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR10, CR8"
"Framework for Detecting Metamorphic Malware Based on Opcode Feature Extraction","Malware is a PC program that is intended to enter and hinder PCs without proprietor's consent. There are different malware types such as key loggers, logic bomb, viruses, rootkits, Trojans, spywares, ransom wares, worms, backdoors, bots, etc. Metamorphic malwares are the malware type which can undergo change at runtime such that they change their code and signature's to bypass the security. Malware's are written in such a way that they get mutated frequently but the original algorithm remains the same. To distinguish a malware, systems like conduct based location, signature based identification and machine learning based procedures are utilized. The mark based discovery framework falls flat in the event that it experiences another obscure malware. In the event of conduct based identification framework, if the antivirus program distinguishes any odd conduct over web then it will produce alert flag, yet there is a high false positive rate as it is inclined to arrange kind records as malware. Machine learning methodology is proposed to detect the malware where feature extraction is based on opcodes in the executable file of the malware. Opcodes of the malware can be extracted into a text file using disassemblers like IDA pro. As success rate of a machine learning algorithm depends on correctness of the features extracted, a robust approach to extract the features is proposed. This model is expected to give success rate of more than 96% in detection of malware","S. Prapulla and S. J. Bhat and G. Shobha",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Preventing erosion of architectural tactics through their strategic implementation, preservation, and visualization","Nowadays, a successful software production is increasingly dependent on how the final deployed system addresses customers' and users' quality concerns such as security, reliability, availability, interoperability, performance and many other types of such requirements. In order to satisfy such quality concerns, software architects are accountable for devising and comparing various alternate solutions, assessing the trade-offs, and finally adopting strategic design decisions which optimize the degree to which each of the quality concerns is satisfied. Although designing and implementing a good architecture is necessary, it is not usually enough. Even a good architecture can deteriorate in subsequent releases and then fail to address those concerns for which it was initially designed. In this work, we present a novel traceability approach for automating the construction of traceabilty links for architectural tactics and utilizing those links to implement a change impact analysis infrastructure to mitigate the problem of architecture degradation. Our approach utilizes machine learning methods to detect tactic-related classes. The detected tactic-related classes are then mapped to a Tactic Traceability Pattern. We train our trace algorithm using code extracted from fifty performance-centric and safety-critical open source software systems and then evaluate it against a real case study.","M. Mirakhorli",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Bridging the gap between ABET outcomes and industry expectations — A case study on software engineering course","The role of ABET accreditation system is quite significant in providing guidance towards program and course design. The program outcomes encompassing through knowledge, skill and attitude play an important role towards competency development of the students. To improve the employability quotient of these fresh graduates, the alignment of respective outcomes along with the base lined expectations of IT industry is needed. In this context Software Engineering (SE) plays an important role in the transformation journey of the graduates to become entry level developers. It also helps to bridge the gap between academia and IT industry so that these new hires are productive as soon as possible. In this paper, the expectations of an IT industry from the new hires is described in conjunction with ABET educational outcomes highlighting pedagogical aspects that enable students develop these abilities. SE course is used as vehicle and an approach is presented integrating software code of ethics (recommended by ACM - IEEE), SE principles, pedagogical aspects and assessment instruments. Subsequently experiences from our corporate learning environment are highlighted.","S. Seshagiri and L. P. Goteti",2014,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Using Topic Models to Support Software Maintenance","Our recent research has shown that the latent information found by commonly used topic models generally relates to the development history of a software system. While it is not always possible to associate these latent topics with human-oriented concepts, it is demonstrable that they identify historical maintenance relationships in source code. Specifically, when a developer makes a change to a software project, it is common for a significant part of that change to relate to a single latent topic. A significant conclusion can be drawn from this: latent topic models identify co-maintenance relationships with no supervision, and therefore topic models can be used to support the maintenance phase of software development.","S. Grant and J. R. Cordy and D. B. Skillicorn",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"A Characterization Study of Repeated Bug Fixes","Programmers always fix bugs when maintaining software. Previous studies showed that developers apply repeated bug fixes-similar or identical code changes-to multiple locations. Based on the observation, researchers built tools to identify code locations in need of similar changes, or to suggest similar bug fixes to multiple code fragments. However, some fundamental research questions, such as what are the characteristics of repeated bug fixes, are still unexplored. In this paper, we present a comprehensive empirical study with 341,856 bug fixes from 3 open source projects to investigate repeated fixes in terms of their frequency, edit locations, and semantic meanings. Specifically, we sampled bug reports and retrieved the corresponding fixing patches in version history. Then we chopped patches into smaller fixes (edit fragments). Among all the fixes related to a bug, we identified repeated fixes using clone detection, and put a fix and its repeated ones into one repeated-fix group. With these groups, we characterized the edit locations, and investigated the common bug patterns as well as common fixes.Our study on Eclipse JDT, Mozilla Firefox, and LibreOffice shows that (1) 15-20% of bugs involved repeated fixes; (2) 73-92% of repeated-fix groups were applied purely to code clones; and (3) 39% of manually examined groups focused on bugs relevant to additions or deletions of whole if-structures. These results deepened our understanding of repeated fixes. They enabled us to assess the effectiveness of existing tools, and will further provide insights for future research directions in automatic software maintenance and program repair.","R. Yue and N. Meng and Q. Wang",2017,"[""IEEE"",""Engineering Village""]","Aceito: CA5, CA3","Aceito: CA5, CA3"
"Parallelization of digital wavelet transformation of ECG signals","The advances in electronics and ICT industry for biomedical use have initiated a lot of new possibilities. However, these IoT solutions face the big data challenge where data comes with a certain velocity and huge quantities. In this paper, we analyze a situation where wearable ECG sensors stream continuous data to the servers. A server needs to receive these streams from a lot of sensors and needs to star various digital signal processing techniques initiating huge processing demands. Our focus in this paper is on optimizing the sequential Wavelet Transform filter. Due to the highly dependent structure of the transformation procedure we propose several optimization techniques for efficient parallelization. We set a hypothesis that optimizing the DWT initialization and processing part can yield a faster code. In this paper, we have provided several experiments to test the validity of this hypothesis by using OpenMP for parallelization. Our analysis shows that proposed techniques can optimize the sequential version of the code.","E. Domazet and M. Gusev",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9, CR9"
"Efficient navigation using slow feature gradients","A model of hierarchical Slow Feature Analysis (SFA) enables a mobile robot to learn a spatial representation of its environment directly from images captured during a random walk. After the unsupervised learning phase a subset of the resulting representations are orientation invariant and code for the position of the robot. Hence, they change monotonically over space even though the variation of the sensory signals received from the environment might change drastically e.g. during rotation on the spot. Furthermore, the property of spatial smoothness allows us to infer a navigation direction by taking the difference between the measurement at the current location and a measurement at a target location. In our work we investigated the use of slow feature representations, learned for a specific environment, for the purpose of navigation. We present a straightforward method for navigation using gradient descent on the difference between two points specified in slow feature space. Due to its slowness objective, the resulting slow feature representations implicitly encode information about static obstacles, allowing a mobile robot to efficiently circumnavigate them by simply following the steepest gradient in slow feature space.","B. Metka and M. Franzius and U. Bauer-Wersing",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Logic controller design system supporting UML activity diagrams","The paper introduces a logic controller design system, called PNAD, supporting UML activity diagrams in version 2.x as a semi-formal specification technique. The system enables transformation of activity diagrams into control Petri nets, their formal verification using model checking technique and the nuXmv tool, generation of synthesizable code in hardware description language VHDL and generation of C code for microcontrollers. The benefits include the support for discrete event system development since the specification till prototype implementation. Additionally, reverse transformation from control Petri nets into UML activity diagrams is also possible. The internal representation of diagrams is based on XML files. The usage of proposed system is illustrated on an example of concrete production process.","M. Grobelny and I. Grobelna",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR9, CR10"
"Understanding the propagation of transient errors in HPC applications","Resiliency of exascale systems has quickly become an important concern for the scientific community. Despite its importance, still much remains to be determined regarding how faults disseminate or at what rate do they impact HPC applications. The understanding of where and how fast faults propagate could lead to more efficient implementation of application-driven error detection and recovery. In this work, we propose a fault propagation framework to analyze how faults propagate in MPI applications and to understand their vulnerability to faults. We employ a combination of compiler-level code transformation and instrumentation, along with a runtime checker. Using the information provided by our framework, we employ machine learning technique to derive application fault propagation models that can be used to estimate the number of corrupted memory locations at runtime.","R. A. Ashraf and R. Gioiosa and G. Kestor and R. F. DeMara and C. Cher and P. Bose",2015,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR9, CR10"
"Dynamically Testing GUIs Using Ant Colony Optimization (T)","In this paper we introduce a dynamic GUI test generator that incorporates ant colony optimization. We created two ant systems for generating tests. Our first ant system implements the normal ant colony optimization algorithm in order to traverse the GUI and find good event sequences. Our second ant system, called AntQ, implements the antq algorithm that incorporates Q-Learning, which is a behavioral reinforcement learning technique. Both systems use the same fitness function in order to determine good paths through the GUI. Our fitness function looks at the amount of change in the GUI state that each event causes. Events that have a larger impact on the GUI state will be favored in future tests. We compared our two ant systems to random selection. We ran experiments on six subject applications and report on the code coverage and fault finding abilities of all three algorithms.","S. Carino and J. H. Andrews",2015,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR8, CR9, CR10"
"Deformable Convolutional Networks","Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets.","J. Dai and H. Qi and Y. Xiong and Y. Li and G. Zhang and H. Hu and Y. Wei",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Histograms of Sparse Codes for Object Detection","Object detection has seen huge progress in recent years, much thanks to the heavily-engineered Histograms of Oriented Gradients (HOG) features. Can we go beyond gradients and do better than HOG? We provide an affirmative answer by proposing and investigating a sparse representation for object detection, Histograms of Sparse Codes (HSC). We compute sparse codes with dictionaries learned from data using K-SVD, and aggregate per-pixel sparse codes to form local histograms. We intentionally keep true to the sliding window framework (with mixtures and parts) and only change the underlying features. To keep training (and testing) efficient, we apply dimension reduction by computing SVD on learned models, and adopt supervised training where latent positions of roots and parts are given externally e.g. from a HOG-based detector. By learning and using local representations that are much more expressive than gradients, we demonstrate large improvements over the state of the art on the PASCAL benchmark for both root-only and part-based models.","X. Ren and D. Ramanan",2013,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Regression Test Selection for Android Applications","Mobile platform pervades human life, and much research in recent years has focused on improving the reliability of mobile applications on this platform, for example by applying automatic testing. However, researchers have primarily considered testing of single version of mobile applications. Although regression testing has been extensively studied for desktop applications, the approaches for desktop applications cannot be directly applied to mobile applications. Our approach leverages the combination of static impact analysis and dynamic code coverage information, and identifies a subset of test cases for re-execution on the modified app version. We implement our approach for Android apps, and illustrate its usefulness based on an Android application.","Q. Do and G. Yang and M. Che and D. Hui and J. Ridgeway",2016,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Learning convolutional action primitives for fine-grained action recognition","Fine-grained action recognition is important for many applications of human-robot interaction, automated skill assessment, and surveillance. The goal is to segment and classify all actions occurring in a time series sequence. While recent recognition methods have shown strong performance in robotics applications, they often require hand-crafted features, use large amounts of domain knowledge, or employ overly simplistic representations of how objects change throughout an action. In this paper we present the Latent Convolutional Skip Chain Conditional Random Field (LC-SC-CRF). This time series model learns a set of interpretable and composable action primitives from sensor data. We apply our model to cooking tasks using accelerometer data from the University of Dundee 50 Salads dataset and to robotic surgery training tasks using robot kinematic data from the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our performance on 50 Salads and JIGSAWS are 18.0% and 5.3% higher than the state of the art, respectively. This model performs well without requiring hand-crafted features or intricate domain knowledge. The code and features have been made public.","C. Lea and R. Vidal and G. D. Hager",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Linux variability anomalies: What causes them and how do they get fixed?","The Linux kernel is one of the largest configurable open source software systems implementing static variability. In Linux, variability is scattered over three different artifacts: source code files, Kconfig files, and Makefiles. Previous work detected inconsistencies between these artifacts that led to anomalies in the intended variability of Linux. We call these variability anomalies. However, there has been no work done to analyze how these variability anomalies are introduced in the first place, and how they get fixed. In this work, we provide an analysis of the causes and fixes of variability anomalies in Linux. We first perform an exploratory case study that uses an existing set of patches which solve variability anomalies to identify patterns for their causes. The observations we make from this dataset allow us to develop four research questions which we then answer in a confirmatory case study on the scope of the whole Linux kernel. We show that variability anomalies exist for several releases in the kernel before they get fixed, and that contrary to our initial suspicion, typos in feature names do not commonly cause these anomalies. Our results show that variability anomalies are often introduced through incomplete patches that change Kconfig definitions without properly propagating these changes to the rest of the system. Anomalies are then commonly fixed through changes to the code rather than to Kconfig files.","S. Nadi and C. Dietrich and R. Tartler and R. C. Holt and D. Lohmann",2013,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10"
"Support of Probabilistic Pointer Analysis in the SSA Form","Probabilistic pointer analysis (PPA) is a compile-time analysis method that estimates the probability that a points-to relationship will hold at a particular program point. The results are useful for optimizing and parallelizing compilers, which need to quantitatively assess the profitability of transformations when performing aggressive optimizations and parallelization. This paper presents a PPA technique using the static single assignment (SSA) form. When computing the probabilistic points-to relationships of a specific pointer, a pointer relation graph (PRG) is first built to represent all of the possible points-to relationships of the pointer. The PRG is transformed by a sequence of reduction operations into a compact graph, from which the probabilistic points-to relationships of the pointer can be determined. In addition, PPA is further extended to interprocedural cases by considering function related statements. We have implemented our proposed scheme including static and profiling versions in the Open64 compiler, and performed experiments to obtain the accuracy and scalability. The static version estimates branch probabilities by assuming that every conditional is equally likely to be true or false, and that every loop executes 10 times before terminating. The profiling version measures branch probabilities dynamically from past program executions using a default workload provided with the benchmark. The average errors for selected benchmarks were 3.80 percent in the profiling version and 9.13 percent in the static version. Finally, SPEC CPU2006 is used to evaluate the scalability, and the result indicates that our scheme is sufficiently efficient in practical use. The average analysis time was 35.59 seconds for an average of 98,696 lines of code.","M. Hung and P. Chen and Y. Hwang and R. D. Ju and J. Lee",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Two-Step Quantization for Low-bit Neural Networks","Every bit matters in the hardware design of quantized neural networks. However, extremely-low-bit representation usually causes large accuracy drop. Thus, how to train extremely-low-bit neural networks with high accuracy is of central importance. Most existing network quantization approaches learn transformations (low-bit weights) as well as encodings (low-bit activations) simultaneously. This tight coupling makes the optimization problem difficult, and thus prevents the network from learning optimal representations. In this paper, we propose a simple yet effective Two-Step Quantization (TSQ) framework, by decomposing the network quantization problem into two steps: code learning and transformation function learning based on the learned codes. For the first step, we propose the sparse quantization method for code learning. The second step can be formulated as a non-linear least square regression problem with low-bit constraints, which can be solved efficiently in an iterative manner. Extensive experiments on CIFAR-10 and ILSVRC-12 datasets demonstrate that the proposed TSQ is effective and outperforms the state-of-the-art by a large margin. Especially, for 2-bit activation and ternary weight quantization of AlexNet, the accuracy of our TSQ drops only about 0.5 points compared with the full-precision counterpart, outperforming current state-of-the-art by more than 5 points.","P. Wang and Q. Hu and Y. Zhang and C. Zhang and Y. Liu and J. Cheng",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR9"
"A learning-based method for detecting defective classes in object-oriented systems","Code or design problems in software classes reduce understandability, flexibility and reusability of the system. Performing maintenance activities on defective components such as adding new features, adapting to the changes, finding bugs, and correcting errors, is hard and consumes a lot of time. Unless the design defects are corrected by a refactoring process these error-prone classes will most likely generate new errors after later modifications. Therefore, these classes will have a high error frequency (EF), which is defined as the ratio between the number of errors and modifications. Early estimate of error-prone classes helps developers to focus on defective modules, thus reduces testing time and maintenance costs. In this paper, we propose a learning-based decision tree model for detecting error-prone classes with structural design defects. The main novelty in our approach is that we consider EFs and change counts (ChC) of classes to construct a proper data set for the training of the model. We built our training set that includes design metrics of classes by analyzing numerous releases of real-world software products and considering EFs of classes to mark them as error-prone or non-error-prone. We evaluated our method using two long-standing software solutions of Ericsson Turkey. We shared and discussed our findings with the development teams. The results show that, our approach succeeds in finding error-prone classes and it can be used to decrease the testing and maintenance costs.","Ç. Biray and F. Buzluca",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7, CR7"
"Iterator-Based Optimization of Imperfectly-Nested Loops","Effective optimization of dense array codes often depends upon the selection of the appropriate execution order for the iterations of nested loops. Tools based on the Polyhedral Model have demonstrated dramatic success in performing such optimizations on many such codes, but others remain an area of active research, leaving programmers to optimize code in other ways. Bertolacci et. al demonstrated that programmer-defined iterators can be used to explore iteration-space reorderings, and that Cray's compiler for the Chapel language can optimize such codes to be competitive with polyhedral tools. This ""iterator-based"" approach allows programmers to explore iteration orderings not identified by automatic optimizers, but was only demonstrated for perfectly-nested loops, and lacked any system for warning about an iterator that would produce an incorrect result. We have now addressed these shortcomings of iterator-based loop optimization, and explored the use of our improved techniques to optimize the imperfectly-nested loops that form the core of Nussinov's algorithm for RNA secondary-structure prediction. Our C++ iterator provides performance that equals the fastest C code, several times faster than was achieved by using the same C compiler on the code with the original iteration ordering, or the code produced by the Pluto loop optimizer. Our Chapel iterators produce run-time that is competitive with the equivalent iterator-free Chapel code, though the Chapel performance does not equal that of the C/C++ code. We have also implemented an iterator that produces an incorrect-but-fast version of Nussinov's algorithm, and used this iterator to illustrate our approaches to error-detection. Manual application of our compile-time error-detection algorithm (which has yet to be integrated into a compiler) identifies this error, as does the run-time approach that we use for codes on which the static test proves inconclusive.","D. Feshbach and M. Glaser and M. Strout and D. G. Wonnacott",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"A Novel Grid Synchronization PLL Method Based on Adaptive Low-Pass Notch Filter for Grid-Connected PCS","The amount of distributed energy resources (DERs) has increased constantly worldwide. The power ratings of DERs have become considerably high, as required by the new grid code requirement. To follow the grid code and optimize the function of grid-connected inverters based on DERs, a phase-locked loop (PLL) is essential for detecting the grid phase angle accurately when the grid voltage is polluted by harmonics and imbalance. This paper proposes a novel low-pass notch filter PLL (LPN-PLL) control strategy to synchronize with the true phase angle of the grid instead of using a conventional synchronous reference frame PLL (SRF-PLL), which requires a d-q-axis transformation of three-phase voltage and a proportional-integral controller. The proposed LPN-PLL is an upgraded version of the PLL method using the fast Fourier transform concept (FFT-PLL) which is robust to the harmonics and imbalance of the grid voltage. The proposed PLL algorithm was compared with conventional SRF-PLL and FFT-PLL and was implemented digitally using a digital signal processor TMS320F28335. A 10-kW three-phase grid-connected inverter was set, and a verification experiment was performed, showing the high performance and robustness of the proposal under low-voltage ride-through operation.","K. Lee and J. Lee and D. Shin and D. Yoo and H. Kim",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"An empirical study of supplementary bug fixes","A recent study finds that errors of omission are harder for programmers to detect than errors of commission. While several change recommendation systems already exist to prevent or reduce omission errors during software development, there have been very few studies on why errors of omission occur in practice and how such errors could be prevented. In order to understand the characteristics of omission errors, this paper investigates a group of bugs that were fixed more than once in open source projects - those bugs whose initial patches were later considered incomplete and to which programmers applied supplementary patches. Our study on Eclipse JDT core, Eclipse SWT, and Mozilla shows that a significant portion of resolved bugs (22% to 33%) involves more than one fix attempt. Our manual inspection shows that the causes of omission errors are diverse, including missed porting changes, incorrect handling of conditional statements, or incomplete refactorings, etc. While many consider that missed updates to code clones often lead to omission errors, only a very small portion of supplementary patches (12% in JDT, 25% in SWT, and 9% in Mozilla) have a content similar to their initial patches. This implies that supplementary change locations cannot be predicted by code clone analysis alone. Furthermore, 14% to 15% of files in supplementary patches are beyond the scope of immediate neighbors of their initial patch locations - they did not overlap with the initial patch locations nor had direct structural dependencies on them (e.g. calls, accesses, subtyping relations, etc.). These results call for new types of omission error prevention approaches that complement existing change recommendation systems.","J. Park and M. Kim and B. Ray and D. Bae",2012,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Secure Software by Design","Based on the calculated cost of a lost record, Yahoo, who “lost” 3 billion records, would be in debt for 450 BILLION DOLLARS. What drives organizations to seek better methods to protect data? The cost of losing data can be high, and it will get higher. Large organizations are able to withstand the malware onslaught, small and mid-size companies have 50-50 chance of remaining in business. To reduce the damage caused by malware, organizations are investing in technology and research. Current research in supervised machine learning is promising. Small and mid-sized companies do not have security professionals to maintain and monitor them. Another area of research is “Honeypots” and “Red Flags”. These techniques may work in espionage, but “white hat testers” demonstrate that these traps are recognized and avoided. Organizations guilty of a data breach, even with clear evidence of negligence are seldom prosecuted. It is very rare that civil or criminal charges are brought against those negligent of reasonable efforts. Can the current environment change? New technologies will eventually be available for small and mid-sized organizations. Laws are changing to make senior management culpable for negligence in protecting sensitive data. Organizations need another way to protect against a data breach. An alternate, and easier strategy for fighting malware is to write software more difficult to hack. This research is identifying how current software practices, lessons learned from malware software, and a novel method to identify critical code, can reduce successful malware attacks. The objective of the research is to search for and identify critical sections in code that should be modified for reducing vulnerabilities. The critical application logic is identified and alternate designs are implemented making it more difficult for the malware author to locate and modify. This research examines easy processes to learn and apply. The work is applicable for all organization, but the existing focus is on helping small and mid-sized organizations. A goal is to reduce the complexity in designing more secure software. The primary considerations are that there are only small additional burdens on software designers and that management sees business value for supporting and requiring more secure software. Because small and mid-sized organizations are more tightly integrated into the supply chain, it the in the interest of large organization, government agencies and the public that these small and mid-sized organizations create more secure software. With an increasing shortage of cyber security professionals, the short-term alternative is to better train software developers for designing more secure software.","S. Rothschild",2018,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Visualizing Software Metrics with Service-Oriented Mining Software Repository for Reviewing Personal Process","We have proposed a framework named SO-MSR: service-oriented mining software repository, which applied service oriented architecture to MSR. Following the SO-MSR, we have developed a web service, named MetricsWebAPI, for metrics calculation from a variety of software repositories and a variety source codes. In this paper, we develop and propose Metrics Viewer, which is client of Metrics Viewer and is a web application to support personal process improvement. Metrics Viewer provides an interactive user interface for repository file exploring. Moreover the Metrics Viewer visualizes change of source code metrics to support overhead view of personal process. End user can improve their development activities based on software repository data without MSR specific knowledge by using Metrics Viewer. We have conducted a pilot study to evaluate the effect of proposed system for personal process improvement.","S. Yasutaka and S. Matsumoto and S. Saiki and M. Nakamura",2013,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Supervised vs Unsupervised Models: A Holistic Look at Effort-Aware Just-in-Time Defect Prediction","Effort-aware just-in-time (JIT) defect prediction aims at finding more defective software changes with limited code inspection cost. Traditionally, supervised models have been used; however, they require sufficient labelled training data, which is difficult to obtain, especially for new projects. Recently, Yang et al. proposed an unsupervised model (LT) and applied it to projects with rich historical bug data. Interestingly, they reported that, under the same inspection cost (i.e., 20 percent of the total lines of code modified by all changes), it could find more defective changes than a state-of-the-art supervised model (i.e., EALR). This is surprising as supervised models that benefit from historical data are expected to perform better than unsupervised ones. Their finding suggests that previous studies on defect prediction had made a simple problem too complex. Considering the potential high impact of Yang et al.'s work, in this paper, we perform a replication study and present the following new findings: (1) Under the same inspection budget, LT requires developers to inspect a large number of changes necessitating many more context switches. (2) Although LT finds more defective changes, many highly ranked changes are false alarms. These initial false alarms may negatively impact practitioners' patience and confidence. (3) LT does not outperform EALR when the harmonic mean of Recall and Precision (i.e., F1-score) is considered. Aside from highlighting the above findings, we propose a simple but improved supervised model called CBS. When compared with EALR, CBS detects about 15% more defective changes and also significantly improves Precision and F1-score. When compared with LT, CBS achieves similar results in terms of Recall, but it significantly reduces context switches and false alarms before first success. Finally, we also discuss the implications of our findings for practitioners and researchers.","Q. Huang and X. Xia and D. Lo",2017,"[""IEEE""]","Rejeitado: CR4","Rejeitado: CR4"
"A prescription and fast code for the long-term evolution of star clusters – II. Unbalanced and core evolution","We introduce version two of the fast star cluster evolution code Evolve Me A Cluster of StarS (EMACSS). The first version (Alexander and Gieles) assumed that cluster evolution is balanced for the majority of the life cycle, meaning that the rate of energy generation in the core of the cluster equals the diffusion rate of energy by two-body relaxation, which makes the code suitable for modelling clusters in weak tidal fields. In this new version, we extend the model to include an unbalanced phase of evolution to describe the pre-collapse evolution and the accompanying escape rate such that clusters in strong tidal fields can also be modelled. We also add a prescription for the evolution of the core radius and density and a related cluster concentration parameter. The model simultaneously solves a series of first-order ordinary differential equations for the rate of change of the core radius, half-mass radius and the number of member stars N. About two thousand integration steps in time are required to solve for the entire evolution of a star cluster and this number is approximately independent of N. We compare the model to the variation of these parameters following from a series of direct N-body calculations of single-mass clusters and find good agreement in the evolution of all parameters. Relevant time-scales, such as the total lifetimes and core collapse times, are reproduced with an accuracy of about 10 per cent for clusters with various initial half-mass radii (relative to their Jacobi radii) and a range of different initial N up to N = 65 536. The current version of EMACSS contains the basic physics that allows us to evolve several cluster properties for single-mass clusters in a simple and fast way. We intend to extend this framework to include more realistic initial conditions, such as a stellar mass spectrum and mass-loss from stars. The EMACSS code can be used in star cluster population studies and in models that consider the co-evolution of (globular) star clusters and large-scale structures.","M. Gieles and P. E. R. Alexander and H. J. G. L. M. Lamers and H. Baumgardt",2014,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Nonlinear Sparse Hashing","To facilitate fast similarity search, this paper proposes to encode the nonlinear similarity and image structure as compact binary codes. Rather than adopting single matrix as projection in the literature, we employ a nonlinear transformation in the form of multilayer neural network to generate binary codes to capture the local structure between data samples. Specifically, we train the network such that the quantization loss is minimized and the variance over all bits is maximized. In addition, we capture the salient structure of image samples at the abstract level with sparsity constraint and inherit the generalization power to unseen samples. Furthermore, we incorporate the supervisory label information into the learning procedure to take advantage of the manual label. To obtain the desired binary codes and the parameterized nonlinear transformation, we optimize the formulated objective problem over each variable with an iterative alternating method. To validate the efficacy of the proposed hashing approach, we conduct experiments on three widely used datasets, namely CIFAR10, MNIST, and SUN397, by comparing with several recent proposed hashing methods.","Z. Chen and J. Lu and J. Feng and J. Zhou",2017,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Connecting software metrics across versions to predict defects","Accurate software defect prediction could help software practitioners allocate test resources to defect-prone modules effectively and efficiently. In the last decades, much effort has been devoted to build accurate defect prediction models, including developing quality defect predictors and modeling techniques. However, current widely used defect predictors such as code metrics and process metrics could not well describe how software modules change over the project evolution, which we believe is important for defect prediction. In order to deal with this problem, in this paper, we propose to use the Historical Version Sequence of Metrics (HVSM) in continuous software versions as defect predictors. Furthermore, we leverage Recurrent Neural Network (RNN), a popular modeling technique, to take HVSM as the input to build software prediction models. The experimental results show that, in most cases, the proposed HVSM-based RNN model has significantly better effort-aware ranking effectiveness than the commonly used baseline models.","Y. Liu and Y. Li and J. Guo and Y. Zhou and B. Xu",2018,"[""IEEE"",""Engineering Village""]","Aceito: CA6","Aceito: CA6"
"Empirical Evaluation of Bug Linking","To collect software bugs found by users, development teams often set up bug trackers using systems such as Bugzilla. Developers would then fix some of the bugs and commit corresponding code changes into version control systems such as svn or git. Unfortunately, the links between bug reports and code changes are missing for many software projects as the bug tracking and version control systems are often maintained separately. Yet, linking bug reports to fix commits is important as it could shed light into the nature of bug fixing processes and expose patterns in software management. Bug linking solutions, such as ReLink, have been proposed. The demonstration of their effectiveness however faces a number of issues, including a reliability issue with their ground truth datasets as well as the extent of their measurements. We propose in this study a benchmark for evaluating bug linking solutions. This benchmark includes a dataset of about 12,000 bug links from 10 programs. These true links between bug reports and their fixes have been provided during bug fixing processes. We designed a number of research questions, to assess both quantitatively and qualitatively the effectiveness of a bug linking tool. Finally, we apply this benchmark on ReLink to report the strengths and limitations of this bug linking tool.","T. F. Bissyandé and F. Thung and S. Wang and D. Lo and L. Jiang and L. Réveillère",2013,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Adversarially Parameterized Optimization for 3D Human Pose Estimation","We propose Adversarially Parameterized Optimization, a framework for learning low-dimensional feasible parameterizations of human poses and inferring 3D poses from 2D input. We train a Generative Adversarial Network to `imagine' feasible poses, and search this imagination space for a solution that is consistent with observations. The framework requires no scene/observation correspondences and enforces known geometric invariances without dataset augmentation. The algorithm can be configured at run time to take advantage of known values such as intrinsic/extrinsic camera parameters or target height when available without additional training. We demonstrate the framework by inferring 3D human poses from projected joint positions for both single frames and sequences. We show competitive results with extremely simple shallow network architectures and make the code publicly available.","D. Jack and F. Maire and A. Eriksson and S. Shirazi",2017,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Using LLVM for Optimized Lightweight Binary Re-Writing at Runtime","Providing new parallel programming models/abstractions as a set of library functions has the huge advantage that it allows for an relatively easy incremental porting path for legacy HPC applications, in contrast to the huge effort needed when novel concepts are only provided in new programming languages or language extensions. However, performance issues are to be expected with fine granular usage of library functions. In previous work, we argued that binary rewriting can bridge the gap by tightly coupling application and library functions at runtime. We showed that runtime specialization at the binary level, starting from a compiled, generic stencil code can help in approaching performance of manually written, statically compiled version. In this paper, we analyze the benefits of post-processing the re-written binary code using standard compiler optimizations as provided by LLVM. To this end, we present our approach for efficiently converting x86-64 binary code to LLVM-IR. Using the mentioned generic code for arbitrary 2d stencils, we present performance numbers with and without LLVM postprocessing. We find that we can now achieve the performance of variants specialized by hand.","A. Engelke and J. Weidendorfer",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Personalized defect prediction","Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction-building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java-the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.","T. Jiang and L. Tan and S. Kim",2013,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Frame-Based Editing: Combining the Best of Blocks and Text Programming","Editing program code as text has several major weaknesses: syntax errors (such as mismatched braces) interrupt programmer flow and make automated tool support harder, boilerplate code templates have to be typed out, and programmers are responsible for layout. These issues have been known about for decades, but early attempts to address these issues, in the form of structured editors, produced unwieldy, hard-to-use tools which failed to catch on. Recently, however, block-based editors in education like Scratch and Snap! have demonstrated that modern graphical structured editors can provide great benefits for programming novices, including very young age groups. These editors become cumbersome for more advanced users, due to their unbending focus on mouse input for block creation and manipulation, and poor scaling of navigation and manipulation facilities to larger programs. Thus, after a few years, learners tend to move from Scratch to text-based editing. In this paper, we present the design and implementation of a novel way to edit programs: frame-based editing. Frame-based editing improves text-based editing by incorporating techniques from block-based editing, and thus provides a suitable follow-on from tools like Scratch. Frame-based editing retains the easy navigation and clearer display of textual code to support manipulation of complex programs, but fuses this with some of the structured editing capabilities that block programming has shown to be viable. The resulting system combines the advantages of text and structured blocks. Preliminary experiments suggest that frame-based editing enables faster program entry than blocks or text, while resulting in fewer syntax errors. We believe it provides an interesting future direction for program editing for learners at all levels of proficiency.","N. C. C. Brown and A. Altadmri and M. Kölling",2016,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Towards a Principled Integration of Multi-camera Re-identification and Tracking Through Optimal Bayes Filters","With the rise of end-to-end learning through deep learning, person detectors and re-identification (ReID) models have recently become very strong. Multi-target multi-camera (MTMC) tracking has not fully gone through this transformation yet. We intend to take another step in this direction by presenting a theoretically principled way of integrating ReID with tracking formulated as an optimal Bayes filter. This conveniently side-steps the need for data-association and opens up a direct path from full images to the core of the tracker. While the results are still sub-par, we believe that this new, tight integration opens many interesting research opportunities and leads the way towards full end-to-end tracking from raw pixels. Code and models for all experiments are publicly available.","L. Beyer and S. Breuers and V. Kurin and B. Leibe",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Towards a refactoring catalogue for knowledge discovery metamodel","Refactorings are a well known technique that assist developers in reformulating the overall structure of applications aiming to improve internal quality attributes while preserving their original behavior. One of the most conventional uses of refactorings are in reengineering processes, whose goal is to change the structure of legacy systems aiming to solve previously identified structural problems. Architecture-Driven Modernization (ADM) is the new generation of reengineering processes; relying just on models, rather than source code, as the main artifacts along the process. However, although ADM provides the general concepts for conducting model-driven modernizations, it does not provide instructions on how to create or apply refactorings in the Knowledge Discovery Metamodel (KDM) metamodel. This leads developers to create their own refactoring solutions, which are very hard to be reused in other contexts. One of the most well known and useful refactoring catalogue is the Fowler's one, but it was primarily proposed for source-code level. In order to fill this gap, in this paper we present a model-oriented version of the Fowler's Catalogue, so that it can be applied to KDM metamodel. In this paper we have focused on four categories of refactorings: (j') renaming, (w) moving features between objects, (iii) organizing data, and (iv) dealing with generalization. We have also developed an environment to support the application of our catalogue. To evaluate our solution we conducted an experiment using eight open source Java application. The results showed that our catalogue can be used to improve the cohesion and coupling of the legacy system.","R. S. Durelli and D. S. M. Santibáñez and M. E. Delamaro and V. V. de Camargo",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Exact Reconstruction From Insertions in Synchronization Codes","This paper studies problems in data reconstruction, an important area with numerous applications. In particular, we examine the reconstruction of binary and nonbinary sequences from synchronization (insertion/deletion-correcting) codes. These sequences have been corrupted by a fixed number of symbol insertions (larger than the minimum edit distance of the code), yielding a number of distinct traces to be used for reconstruction. We wish to know the minimum number of traces needed for exact reconstruction. This is a general version of a problem tackled by Levenshtein for uncoded sequences. We introduce an exact formula for the maximum number of common supersequences shared by sequences at a certain edit distance, yielding an upper bound on the number of distinct traces necessary to guarantee exact reconstruction. Without specific knowledge of the code words, this upper bound is tight. We apply our results to the famous single deletion/insertion-correcting Varshamov-Tenengolts (VT) codes and show that a significant number of VT code word pairs achieve the worst case number of outputs needed for exact reconstruction. We also consider extensions to other channels, such as adversarial deletion and insertion/deletion channels and probabilistic channels.","F. Sala and R. Gabrys and C. Schoeny and L. Dolecek",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Dynamic shift-map coding with side information at the decoder","Shift-map codes have been studied as joint source-channel codes for continuous sources. These codes are useful in delay-limited scenarios and also provide better tolerance to deviations of the signal-to-noise ratio (SNR) from a target SNR, compared to separate source and channel coding. This paper defines a generalized family of shift-map codes that share a strong connection with redundant residue number systems (RRNS), and are henceforth called RRNS-map codes. In the proposed coding scheme, side information about the source allows the decoder to consider only a fraction of the codebook for decoding, with no change in the encoding process. With an appropriately designed RRNS-map code, in this fraction of the codebook, the codewords are much better separated than the original codebook. As a result, RRNS-map codes achieve the same distortion in the mean square error sense as conventional shift-map codes without side information, but significantly outperform shift-map codes when side information is provided to the decoder. This coding scheme is ideally suited to applications where a simple and fixed encoding scheme is desired at the encoder, while the decoder is given access to side information about the source.","Y. Yoo and O. O. Koyluoglu and S. Vishwanath and I. Fiete",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Reliable implementation of linear filters with fixed-point arithmetic","This article deals with the implementation of linear filters or controllers with fixed-point arithmetic. The finite precision of the computations and the roundoff errors induced may have an important impact on the numerical behavior of the implemented system. More-over, the fixed-point transformation is a time consuming and error-prone task, specially with the objective of minimizing the quantization impact. Based on a formalism able to describe every structure of linear filters/controllers, this paper proposes an automatic method to generate fixed-point version of the inputs-to-outputs algorithm and an analysis of the global error added on the output. An example illustrates the approach.","T. Hilaire and B. Lopez",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"A Case Study of the Effects of Architecture Debt on Software Evolution Effort","In large-scale software systems, the majority of defective files are architecturally connected, and the architecture connections usually exhibit design flaws, which are associated with higher change-proneness among files and higher maintenance costs. As software evolves with bug fixes, new features, or improvements, unresolved architecture design flaws can contribute to maintenance difficulties. The impact on effort due to architecture design flaws has been difficult to quantify and justify. In this paper, we conducted a case study where we identified flawed architecture relations and quantified their effects on maintenance activities. Using data from this project's source code and revision history, we identified file groups where files are architecturally connected and participated in flawed architecture designs, quantified the maintenance activities in the detected files, and assessed the penalty related to these files.","W. Snipes and S. Karlekar and R. Mo",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Development of a graphical user interface using Windows 7-C# to download Intel-Hex formatted file into the flash of 89S52 microcontroller","The 89S52 microcontroller (MCU) is very popular among the students, teachers, engineers, scientists and amateurs to build small and cost effective projects for learning purposes and or to incorporate it in a bigger project. In these projects, the chip remains in the system and occasionally need arises to change the program bytes of the code memory (flash) without removing the MCU from the holding instrument. To facilitate entering new codes into the flash without employing expensive commercial ROM Programmer, an interface circuit exists within the 89S52 known as `In-System Programming Interface (ISP)'. ISP Programming requires two components viz., (i) an interactive GUI (Graphical User) Interface (Fig. 7) at the IBMPC side, and (ii) an auxiliary communication controller (ACC) at the target MCU side (Fig. 1). The GUI interface transfers `control information (Chip Erase, chip Blank, Chip Write, Chip Read, Lock Security Bits and etc.)' and `Intel-Hex' formatted program bytes over COM port to the ACC. The ACC decodes the control information, extracts the program bytes and then configures the programming mode of the target MCU. It then activates the ISP interface as per `Serial Programming Instructions [1]' for writing the received program bytes into the flash of the target MCU. This paper has presented the development procedures of the GUI interface written using C# programming language, which is compatible with Windows 7 operating system. The GUI has been tested in an existing 89S52 based `CMCKIT: CISC Microcontroller Learning Kit (Fig. 1)' and found to be working as expected. The contents of this paper will encourage the interested readers to learn C# programming language, physical COM port hardware and programming, virtual COM port concept and finally creating new versions of GUI Interfaces for their own ISP Programmers.","G. Mostafa and Y. Abedin",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Increasing Automation in the Backporting of Linux Drivers Using Coccinelle","Software is continually evolving, to fix bugs and add new features. Industry users, however, often value stability, and thus may not be able to update their code base to the latest versions. This raises the need to selectively backport new features to older software versions. Traditionally, backporting has been done by cluttering the backported code with preprocessor directives, to replace behaviors that are unsupported in an earlier version by appropriate workarounds. This approach however, involves writing a lot of error-prone backporting code, and results in implementations that are hard to read and maintain. We consider this issue in the context of the Linux kernel, for whicholder versions are in wide use. We present a new backporting strategy that relies on the use of a backporting compatability library and on code that is automatically generated using the program transformation tool Coccinelle. This approach reduces the amount of code that must be manually written, and thus can help the Linux kernel backporting effort scale while maintainingthe dependability of the backporting process.","L. R. Rodriguez and J. Lawall",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"ModChecker: Kernel Module Integrity Checking in the Cloud Environment","Kernel modules are an integral part of most operating systems (OS) as they provide flexible ways of adding new functionalities (such as file system or hardware support) to the kernel without the need to recompile or reload the entire kernel. Aside from providing an interface between the user and the hardware, these modules maintain system security and reliability. Malicious kernel level exploits (e.g. code injections) provide a gateway to a system's privileged level where the attacker has access to an entire system. Such attacks may be detected by performing code integrity checks. Several commodity operating systems (such as Linux variants and MS Windows) maintain signatures of different pieces of kernel code in a database for code integrity checking purposes. However, it quickly becomes cumbersome and time consuming to maintain a database of legitimate dynamic changes in the code, such as regular module updates. In this paper we present Mod Checker, which checks in-memory kernel modules' code integrity in real time without maintaining a database of hashes. Our solution applies to virtual environments that have multiple virtual machines (VMs) running the same version of the operating system, an environment commonly found in large cloud servers. Mod Checker compares kernel module among a pool of VMs within a cloud. We thoroughly evaluate the effectiveness and runtime performance of Mod Checker and conclude that Mod Checker is able to detect any change in a kernel module's headers and executable content with minimal or no impact on the guest operating systems' performance.","I. Ahmed and A. Zoranic and S. Javaid and G. G. R. III",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Towards a formal model of the pedagogic discourse and the Zone of Proximal Development (ZPD) of Vygotsky","This article uses conceptual devices from different theories of communication, discourse analysis of Pêcheux and Zone of Proximal Development (ZPD) of Vygotsky, to present a formalization of a new pedagogical discourse based on the pedagogic transformation through a meaning-effect. The formalization of the main components of the pedagogic discourse takes into account the imaginary formations, languages and discursive process of teaching and learning, which allow us to orient ourselves towards producing a more diverse model. Finally technological artifacts created (new technological tools in education, virtual classrooms and new materials) become powerful tools daily in mediating our communication and our activities. Thus new forms of code -according to some communication models- seem to take on increasing significance. Is also described in this paper a proposal incorporating technological resources as elements in imaginary mediation and discursive composition.","L. Reynoso and S. Romero and F. Romero",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Hyperspectral Anomaly Detection via a Sparsity Score Estimation Framework","Anomaly detection has become an important topic in hyperspectral imagery (HSI) analysis over the last 20 years. HSIs usually possess complexly cluttered spectral signals due to the complicated conditions of the land-cover distribution. This in turn makes it difficult to obtain an accurate background estimation to distinguish the anomaly targets. The sparse learning technique provides a way to obtain an implicit background representation with the learned dictionary and corresponding sparse codes. In this paper, we explore the background/anomaly information content for each atom of the learned dictionary, from an analysis based on the frequency of the dictionary atoms for HSI reconstruction. From this perspective, we propose a novel sparsity score estimation framework for hyperspectral anomaly detection. First, an overcomplete dictionary and the corresponding sparse code matrix are obtained from the HSI. The frequency of each dictionary atom for reconstruction, which is also called the atom usage probability, is then estimated from the sparse code matrix. Finally, the estimated frequencies are transformed to the sparsity score for each pixel, which can be seen as the degree of “anomalousness.” In the proposed detection framework, two strategies are proposed to enhance the diversity between the background and anomaly information in the learned dictionary: 1) dictionary-based background feature transformation and 2) dictionary iterative reweighting. A series of real-world HSI data sets is utilized to evaluate the performance of the proposed framework. The experimental results show that the proposed framework achieves a superior performance compared to some of the state-of-the-art anomaly detection methods.","R. Zhao and B. Du and L. Zhang",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"The Knowledge Accumulation and Transfer in Open-Source Software (OSS) Development","We examine the learning curves of individual software developers in Open-Source Software (OSS) Development. We collected the dataset of multi-year code change histories from the repositories for 20 open source software projects involving more than 200 developers. We build and estimate regression models to assess individual developers' learning progress (in reducing the likelihood they make a bug). Our estimation results show that developer's coding and indirect bug-fixing experiences do not decrease bug ratios while bug-fixing experience can lead to the decrease of bug ratio of learning progress. We also find that developer's coding and bug-fixing experiences in other projects do not decrease the developer's bug ratio in a focal project. We empirically confirm the moderating effects of bug types on learning progress. Developers exhibit learning effects for some simple bug types (e.g., Wrong literals) or bug types with many instances (e.g., Wrong if conditionals). The results may have managerial implications and provoke future research on project management about allocating resources on tasks that add new code versus tasks that debug and fix existing code.","Y. Kim and L. Jiang",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR9, CR10"
"Identifying wasted effort in the field via developer interaction data","During software projects, several parts of the source code are usually re-written due to imperfect solutions before the code is released. This wasted effort is of central interest to the project management to assure on-time delivery. Although the amount of thrown-away code can be measured from version control systems, stakeholders are more interested in productivity dynamics that reflect the constant change in a software project. In this paper we present a field study of measuring the productivity of a medium-sized J2EE project. We propose a productivity analysis method where productivity is expressed through dynamic profiles - the so-called Micro-Productivity Profiles (MPPs). They can be used to characterize various constituents of software projects such as components, phases and teams. We collected detailed traces of developers' actions using an Eclipse IDE plug-in for seven months of software development throughout two milestones. We present and evaluate profiles of two important axes of the development process: by milestone and by application layers. MPPs can be an aid to take project control actions and help in planning future projects. Based on the experiments, project stakeholders identified several points to improve the development process. It is also acknowledged, that profiles show additional information compared to a naive diff-based approach.","G. Balogh and G. Antal and Á. Beszédes and L. Vidács and T. Gyimóthy and Á. Z. Végh",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR9, CR10"
"Malware Economics and its Implication to Anti-Malware Situational Awareness","Malware, like any other software, is developed iteratively and improved in incremental versions over a long period of time. Malware economics requires amortizing the cost of malware development over several attacks. Thus, the malware code persists through many incremental versions of the malware-albeit in a transformed and obfuscated state-while the classic indicators of attack, e.g., domain names, file names, and IP addresses, are parameterized and often change with each new version. Recent breakthroughs in automated malware analysis and code debofuscation make it possible to overcome the challenges imposed by code obfuscation and create new anti-malware tools that use the malware code itself as an immutable indicator in anti-malware defense. The resulting technologies can be used to provide situational awareness of the dynamic threat profile of an organization. A persistent adversary that intends to penetrate a particular organization will send morphed variants of the same malware to a large number of people in an organization. Such an attack campaign may be executed over weeks or months. By correlating malware generated from the same code base, one can detect such persistent campaigns against an organization using the malware blocked by an anti-virus. Results from the field demonstrate that this approach has promise in detecting targeted attacks while the attacks are in progress thus giving the defenders' enough time to take preventive actions.","A. Lakhotia and V. Notani and C. LeDoux",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"[Journal First] ChangeLocator: Locate Crash-Inducing Changes Based on Crash Reports","Software crashes are severe manifestations of software bugs. Debugging crashing bugs is tedious and time-consuming. Understanding software changes that induce a crashing bug can provide useful contextual information for bug fixing and is highly demanded by developers. Locating the bug inducing changes is also useful for automatic program repair, since it narrows down the root causes and reduces the search space of bug fix location. However, currently there are no systematic studies on locating the software changes to a source code repository that induce a crashing bug reflected by a bucket of crash reports. To tackle this problem, we first conducted an empirical study on characterizing the bug inducing changes for crashing bugs (denoted as crashinducing changes). We also propose ChangeLocator, a method to automatically locate crash-inducing changes for a given bucket of crash reports. We base our approach on a learning model that uses features originated from our empirical study and train the model using the data from the historical fixed crashes. We evaluated ChangeLocator with six release versions of Netbeans project. The results show that it can locate the crash-inducing changes for 44.7%, 68.5%, and 74.5% of the bugs by examining only top 1, 5 and 10 changes in the recommended list, respectively. It significantly outperforms the existing state-of-the-art approach.","R. Wu and M. Wen and S. Cheung and H. Zhang",2018,"[""IEEE""]","Aceito: CA1, CA5","Aceito: CA1, CA5"
"The Uniqueness of Changes: Characteristics and Applications","Changes in software development come in many forms. Some changes are frequent, idiomatic, or repetitive (e.g. Adding checks for nulls or logging important values) while others are unique. We hypothesize that unique changes are different from the more common similar (or non-unique) changes in important ways, they may require more expertise or represent code that is more complex or prone to mistakes. As such, these unique changes are worthy of study. In this paper, we present a definition of unique changes and provide a method for identifying them in software project history. Based on the results of applying our technique on the Linux kernel and two large projects at Microsoft, we present an empirical study of unique changes. We explore how prevalent unique changes are and investigate where they occur along the architecture of the project. We further investigate developers' contribution towards uniqueness of changes. We also describe potential applications of leveraging the uniqueness of change and implement two of those applications, evaluating the risk of changes based on uniqueness and providing change recommendations for non-unique changes.","B. Ray and M. Nagappan and C. Bird and N. Nagappan and T. Zimmermann",2015,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Application of energy reduction techniques using niched pareto GA of energy analzyer for HPC applications","Energy consumption of High Performance Computing (HPC) architectures, on the path to exa-scale systems, is still a challenging problem among the HPC community owing to the technological issues, such as, power limitations of processor technologies, increased degree of parallelism (both in a node level and in a system level), and a hefty cost of communication which arises while executing applications on such architectures. In addition, the increased electrical billing and the other ensuing ecological hazards, including climate changes, have urged several researchers to focus much on framing solutions that address the energy consumption issues of future HPC systems. Reducing the energy consumption of HPC systems, however, is not an easy task due to its assorted nature of muddled up complicated issues that are tightly dependent on the performance of applications, the energy efficiency of hardware components, and the energy consumption of the compute center infrastructure. This paper presents Niched Pareto Genetic Algorithm (NPGA) based application of energy reduction techniques, namely, code version selection mechanism and compiler optimization switch selection mechanism, for HPC applications using Energy Analyzer tool. The proposed mechanism was tested with HPC applications, such as, MPI-C based HPCC benchmarks, Jacobi, PI, and matrix multiplication applications, on the HPCCLoud Research Laboratory of our premise. This paper could be of an interest to various researchers, namely, HPC application developers, performance analysis tool developers, environmentalist, and energy-aware hardware designers.","S. Benedict",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"On-stack replacement to improve JIT-based obfuscation a preliminary study","As more devices are connecting together, more effective security techniques are needed to protect running software from hackers. One possible security technique is to continuously change the binary code running of given software by recompiling it on the fly. This switching need to be done frequently, quickly, and randomly, not constrained by specific locations in code, to make it difficult for the hacker to track the behavior of the running code or predict its functionality. In our research we are working on a technique that recompiles speculatively and concurrently with current execution, and switches to the new compiled version dynamically, at arbitrary points. This paper presents an early analytical study augmented by experimental analysis on manually applying this technique on simple kernels, to study the concept in comparison with other similar techniques.","M. Yusuf and A. El-Mahdy and E. Rohou",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Automatic Contract Insertion with CCBot","Existing static analysis tools require significant programmer effort. On large code bases, static analysis tools produce thousands of warnings. It is unrealistic to expect users to review such a massive list and to manually make changes for each warning. To address this issue we propose CCBot (short for CodeContracts Bot), a new tool that applies the results of static analysis to existing code through automatic code transformation. Specifically, CCBot instruments the code with method preconditions, postconditions, and object invariants which detect faults at runtime or statically using a static contract checker. The only configuration the programmer needs to perform is to give CCBot the file paths to code she wants instrumented. This allows the programmer to adopt contract-based static analysis with little effort. CCBot's instrumented version of the code is guaranteed to compile if the original code did. This guarantee means the programmer can deploy or test the instrumented code immediately without additional manual effort. The inserted contracts can detect common errors such as null pointer dereferences and out-of-bounds array accesses. CCBot is a robust large-scale tool with an open-source C# implementation. We have tested it on real world projects with tens of thousands of lines of code. We discuss several projects as case studies, highlighting undiscovered bugs found by CCBot, including 22 new contracts that were accepted by the project authors.","S. A. Carr and F. Logozzo and M. Payer",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Keynote 1: LGTM - Software Sensing and Bug Smelling","Summary form only given. Looks Good To Me (LGTM) is a means to stamp an 'ok' on code reviews and then have the code moved to production. One key part of this process is a detailed quality analysis by checking code, looking for potential known bugs, and evaluating the design. As code bases are changed almost every day by many developers, we need to devise means for effective, regular, and focused analysis. In the recent past we have looked into approaches of software sensing and bug smelling. Sensing software is employed by audio-visual and multi-touch interfaces to a code base and its change history. Bug smelling leverages bug prediction on the levels of classes, methods, or change types. Combining both can lead to a more effective quality analysis for reviewing tasks such that the signing off by LGTM is facilitated. In this talk we will present approaches and tools such as SmellTagger or Coco Viz for software sensing and bug smelling. We will also discuss current limitations and potential new horizons for software evolution research.","H. C. Gall",2012,"[""IEEE""]","Rejeitado: CR4","Rejeitado: CR4"
"Seamless GPU Evaluation of Smart Expression Templates","Expression Templates is a technique allowing to write linear algebra code in C++ the same way it would be written on paper. It is also used extensively as a performance optimization technique, especially as the Smart Expression Templates form which allows for even higher performance. It has proved to be very efficient for computation on a Central Processing Unit (CPU). However, due to its design, it is not easily implemented on a Graphics Processing Unit (GPU). In this paper, we devise a set of techniques to allow the seamless evaluation of Smart Expression Templates on the GPU. The execution is transparent for the user of the library which still uses the matrices and vector as if it was on the CPU and profits from the performance and higher multi-processing capabilities of the GPU. We also show that the GPU version is significantly faster than the CPU version, without any change to the code of the user.","B. Wicht and A. Fischer and J. Hennebert",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"A Large Scale Study of License Usage on GitHub","The open source community relies upon licensing in order to govern the distribution, modification, and reuse of existing code. These licenses evolve to better suit the requirements of the development communities and to cope with unaddressed or new legal issues. In this paper, we report the results of a large empirical study conducted over the change history of 16,221 open source Java projects mined from Git Hub. Our study investigates how licensing usage and adoption changes over a period of ten years. We consider both the distribution of license usage within projects of a rapidly growing forge and the extent that new versions of licenses are introduced in these projects.","C. Vendome",2015,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"A hybrid generative/discriminative model based object tracking primary exploration","Based on analysis and discussion of object representation, a hybrid model based tracking by detection algorithm is presented as yet a primary exploration. The whole system is made of a learning-detecting two phase loop. Object model is built on a general Haar-like feature space which is automatically generated and extracted by a special random projection. Our proposed algorithm involves two type of methods for object modeling, one is to learn a transformation matrix by Principal Component Analysis (PCA) as the multi-view appearance model of the target object, and the other is to learn a classifier by Fisher Linear Discriminant Analysis (FLD) as the classification between the foreground and the background. We extend the Fisher criterion to a multi-mode background situation, which is used to formulate features' discriminating power as feature weighting from the online captured positive/negative training data. In additionally, a two-stage detection is involved, in which all input samples firstly are tested by the learned FLD classifier to pick up candidates, then amongst candidates the maximum likelihood to the target template as the final detection result is searched for by PCA code matching. All generative model, discriminative model and target templates should online update due to appearance variation. A number of experiments illustrate that the proposed hybrid model based tracking algorithm does has advantages.","Y. Chen and P. S. Park",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Deep Diffeomorphic Transformer Networks","Spatial Transformer layers allow neural networks, at least in principle, to be invariant to large spatial transformations in image data. The model has, however, seen limited uptake as most practical implementations support only transformations that are too restricted, e.g. affine or homographic maps, and/or destructive maps, such as thin plate splines. We investigate the use of flexible diffeomorphic image transformations within such networks and demonstrate that significant performance gains can be attained over currently-used models. The learned transformations are found to be both simple and intuitive, thereby providing insights into individual problem domains. With the proposed framework, a standard convolutional neural network matches state-of-the-art results on face verification with only two extra lines of simple TensorFlow code.","N. S. Detlefsen and O. Freifeld and S. Hauberg",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Learning Recruits Neurons Representing Previously Established Associations in the Corvid Endbrain","<para>Crows quickly learn arbitrary associations. As a neuronal correlate of this behavior, single neurons in the corvid endbrain area nidopallium caudolaterale (NCL) change their response properties during association learning. In crows performing a delayed association task that required them to map both familiar and novel sample pictures to the same two choice pictures, NCL neurons established a common, prospective code for associations. Here, we report that neuronal tuning changes during learning were not distributed equally in the recorded population of NCL neurons. Instead, such learning-related changes relied almost exclusively on neurons which were already encoding familiar associations. Only in such neurons did behavioral improvements during learning of novel associations coincide with increasing selectivity over the learning process. The size and direction of selectivity for familiar and newly learned associations were highly correlated. These increases in selectivity for novel associations occurred only late in the delay period. Moreover, NCL neurons discriminated correct from erroneous trial outcome based on feedback signals at the end of the trial, particularly in newly learned associations. Our results indicate that task-relevant changes during association learning are not distributed within the population of corvid NCL neurons but rather are restricted to a specific group of association-selective neurons. Such association neurons in the multimodal cognitive integration area NCL likely play an important role during highly flexible behavior in corvids.</para>","L. Veit and G. Pidpruzhnykova and A. Nieder",2017,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"CU2rCU: Towards the complete rCUDA remote GPU virtualization and sharing solution","GPUs are being increasingly embraced by the high performance computing and computational communities as an effective way of considerably reducing execution time by accelerating significant parts of their application codes. However, despite their extraordinary computing capabilities, the adoption of GPUs in current HPC clusters may present certain negative side-effects. In particular, to ease job scheduling in these platforms, a GPU is usually attached to every node of the cluster. In addition to increasing acquisition costs this favors that GPUs may frequently remain idle, as applications usually do not fully utilize them. On the other hand, idle GPUs consume non-negligible amounts of energy, which translates into very poor energy efficiency during idle cycles. rCUDA was recently developed as a software solution to address these concerns. Specifically, it is a middleware that allows transparently sharing a reduced number of GPUs among the nodes in a cluster. rCUDA thus increases the GPU-utilization rate, taking care of job scheduling. While the initial prototype versions of rCUDA demonstrated its functionality, they also revealed several concerns related with usability and performance. With respect to usability, in this paper we present a new component of the rCUDA suite that allows an automatic transformation of any CUDA source code, so that it can be effectively accommodated within this technology. In response to performance, we briefly show some interesting results, which will be deeply analyzed in future publications. The net outcome is a new version of rCUDA that allows, for any CUDA-compatible program, to use remote GPUs in a cluster with minimum overhead.","C. Reaño and A. J. Peña and F. Silla and J. Duato and R. Mayo and E. S. Quintana-Ortí",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A Distributed Classifier for MicroRNA Target Prediction with Validation Through TCGA Expression Data","Background: MicroRNAs (miRNAs) are approximately 22-nucleotide long regulatory RNA that mediate RNA interference by binding to cognate mRNA target regions. Here, we present a distributed kernel SVM-based binary classification scheme to predict miRNA targets. It captures the spatial profile of miRNA-mRNA interactions via smooth B-spline curves. This is accomplished separately for various input features, such as thermodynamic and sequence-based features. Further, we use a principled approach to uniformly model both canonical and non-canonical seed matches, using a novel seed enrichment metric. Finally, we verify our miRNA-mRNA pairings using an Elastic Net-based regression model on TCGA expression data for four cancer types to estimate the miRNAs that together regulate any given mRNA. Results: We present a suite of algorithms for miRNA target prediction, under the banner Avishkar, with superior prediction performance over the competition. Specifically, our final kernel SVM model, with an Apache Spark backend, achieves an average true positive rate (TPR) of more than 75 percent, when keeping the false positive rate of 20 percent, for non-canonical human miRNA target sites. This is an improvement of over 150 percent in the TPR for non-canonical sites, over the best-in-class algorithm. We are able to achieve such superior performance by representing the thermodynamic and sequence profiles of miRNA-mRNA interaction as curves, devising a novel seed enrichment metric, and learning an ensemble of miRNA family-specific kernel SVM classifiers. We provide an easy-to-use system for large-scale interactive analysis and prediction of miRNA targets. All operations in our system, namely candidate set generation, feature generation and transformation, training, prediction, and computing performance metrics are fully distributed and are scalable. Conclusions: We have developed an efficient SVM-based model for miRNA target prediction using recent CLIP-seq data, demonstrating superior performance, evaluated using ROC curves for different species (human or mouse), or different target types (canonical or non-canonical). We analyzed the agreement between the target pairings using CLIP-seq data and using expression data from four cancer types. To the best of our knowledge, we provide the first distributed framework for miRNA target prediction based on Apache Hadoop and Spark. Availability: All source code and sample data are publicly available at https://bitbucket.org/cellsandmachines/avishkar. Our scalable implementation of kernel SVM using Apache Spark, which can be used to solve large-scale non-linear binary classification problems, is available at https://bitbucket.org/cellsandmachines/kernelsvmspark.","A. Ghoshal and J. Zhang and M. A. Roth and K. M. Xia and A. Y. Grama and S. Chaterji",2018,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The impact of requirements on software quality across three product generations","In a previous case study, we presented data demonstrating the impact that a well-written and well-reviewed set of requirements had on software defects and other quality indicators between two generations of an Intel product. The first generation was coded from an unorganized collection of requirements that were reviewed infrequently and informally. In contrast, the second was developed based on a set of requirements stored in a Requirements Management database and formally reviewed at each revision. Quality indicators for the second software product all improved dramatically even with the increased complexity of the newer product. This paper will recap that study and then present data from a subsequent Intel case study revealing that quality enhancements continued on the third generation of the product. The third generation software was designed and coded using the final set of requirements from the second version as a starting point. Key product differentiators included changes to operate with a new Intel processor, the introduction of new hardware platforms and the addition of approximately fifty new features. Software development methodologies were nearly identical, with only the change to a continuous build process for source code check-in added. Despite the enhanced functionality and complexity in the third generation software, requirements defects, software defects, software sightings, feature commit vs. delivery (feature variance), defect closure efficiency rates, and number of days from project commit to customer release all improved from the second to the third generation of the software.","J. Terzakis",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9, CR10, CR8"
"ES-MPICH2: A Message Passing Interface with Enhanced Security","An increasing number of commodity clusters are connected to each other by public networks, which have become a potential threat to security sensitive parallel applications running on the clusters. To address this security issue, we developed a Message Passing Interface (MPI) implementation to preserve confidentiality of messages communicated among nodes of clusters in an unsecured network. We focus on M PI rather than other protocols, because M PI is one of the most popular communication protocols for parallel computing on clusters. Our MPI implementation-called ES-MPICH2-was built based on MPICH2 developed by the Argonne National Laboratory. Like MPICH2, ES-MPICH2 aims at supporting a large variety of computation and communication platforms like commodity clusters and high-speed networks. We integrated encryption and decryption algorithms into the MPICH2 library with the standard MPI interface and; thus, data confidentiality of MPI applications can be readily preserved without a need to change the source codes of the MPI applications. MPI-application programmers can fully configure any confidentiality services in MPICHI2, because a secured configuration file in ES-MPICH2 offers the programmers flexibility in choosing any cryptographic schemes and keys seamlessly incorporated in ES-MPICH2. We used the Sandia Micro Benchmark and Intel MPI Benchmark suites to evaluate and compare the performance of ES-MPICH2 with the original MPICH2 version. Our experiments show that overhead incurred by the confidentiality services in ES-MPICH2 is marginal for small messages. The security overhead in ES-MPICH2 becomes more pronounced with larger messages. Our results also show that security overhead can be significantly reduced in ES-MPICH2 by high-performance clusters. The executable binaries and source code of the ES-MPICH2 implementation are freely available at http:// www.eng.auburn.edu/~xqin/software/es-mpich2/.","X. Ruan and Q. Yang and M. I. Alghamdi and S. Yin and X. Qin",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Recognition of arm activities based on Hidden Markov Models for natural interaction with service robots","This research presents a novel way of representing human motion and recognizing human activities from the skeleton output computed from RGB-D data from vision-based motion capture systems. The method uses a representation of the skeleton which is invariant to rotation and translation, based on Orthogonal Direction Change Chain Codes, as observations for a single Discrete Connected Hidden Markov Model formed by a set of multiple Hidden Markov Models for simple activities, which are merged using a grammar-based structure. The purpose of this research is to provide a service robot with the capability of human activity awareness, which can be used for action planning with implicit and indirect Human-Robot Interaction.","J. I. Figueroa-Angulo and J. Savage-Carmona and E. Bribiesca-Correa and B. Escalante and R. S. Leder and L. E. Sucar",2013,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Improving NASA's Multiscale Modeling Framework for Tropical Cyclone Climate Study","One of the current challenges in tropical cyclone (TC) research is how to improve our understanding of TC interannual variability and the impact of climate change on TCs. Recent advances in global modeling, visualization, and supercomputing technologies at NASA show potential for such studies. In this article, the authors discuss recent scalability improvement to the multiscale modeling framework (MMF) that makes it feasible to perform long-term TC-resolving simulations. The MMF consists of the finite-volume general circulation model (fvGCM), supplemented by a copy of the Goddard cumulus ensemble model (GCE) at each of the fvGCM grid points, giving 13,104 GCE copies. The original fvGCM implementation has a 1D data decomposition; the revised MMF implementation retains the 1D decomposition for most of the code, but uses a 2D decomposition for the massive copies of GCEs. Because the vast majority of computation time in the MMF is spent computing the GCEs, this approach can achieve excellent speedup without incurring the cost of modifying the entire code. Intelligent process mapping allows differing numbers of processes to be assigned to each domain for load balancing. The revised parallel implementation shows highly promising scalability, obtaining a nearly 80-fold speedup by increasing the number of cores from 30 to 3,335.","B. Shen and B. Nelson and S. Cheung and W. Tao",2013,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"A Literature Review of Research in Bug Resolution: Tasks, Challenges and Future Directions","Due to the increasing scale and complexity of software products, software maintenance especially on bug resolution has become a challenging task. Generally in large-scale software programs, developers depend on software artifacts (e.g., bug report, source code and change history) in bug repositories to complete the bug resolution task. However, a mountain of submitted bug reports every day increase the developers' workload. Therefore, ‘How to effectively resolve software defects by utilizing software artifacts?’ becomes a research hotspot in software maintenance. Considerable studies have been done on bug resolution by using multi-techniques, which cover data mining, machine learning and natural language processing. In this paper, we present a literature survey on tasks, challenges and future directions of bug resolution in software maintenance process. Our investigation concerns the most important phases in bug resolution, including bug understanding, bug triage and bug fixing. Moreover, we present the advantages and disadvantages of each study. Finally, based on the investigation and comparison results, we propose the future research directions of bug resolution.","T. Zhang and H. Jiang and X. Luo and A. T. S. Chan",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Cross coupled digital NAND gate comparator based flash ADC","This paper demonstrates the Flash ADC which is constructed using digital 3-input cross coupled NAND gates. These cross coupled configuration of NAND gates form a comparator of flash ADC. It is latch comparator which operates on single phase clock Φ. The output of comparator is thermometer code. An encoder is constructed that encodes the thermometer code to binary as output of comparator. The design is simulated in 180nm technology and implement 4 bit flash ADC version. The Flash ADC is designed in TANNER S-EDIT 13.0.","P. P. Kute and P. Dakhole and P. Palsodkar",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Gabor Feature-Based Collaborative Representation for Hyperspectral Imagery Classification","Sparse-representation-based classification (SRC) assigns a test sample to the class with minimum representation error via a sparse linear combination of all the training samples, which has successfully been applied to several pattern recognition problems. According to compressive sensing theory, the l<sub>1</sub>-norm minimization could yield the same sparse solution as the l<sub>0</sub> norm under certain conditions. However, the computational complexity of the l<sub>1</sub>-norm optimization process is often too high for large-scale high-dimensional data, such as hyperspectral imagery (HSI). To make matter worse, a large number of training data are required to cover the whole sample space, which is difficult to obtain for hyperspectral data in practice. Recent advances have revealed that it is the collaborative representation but not the l<sub>1</sub>-norm sparsity that makes the SRC scheme powerful. Therefore, in this paper, a 3-D Gabor feature-based collaborative representation (3GCR) approach is proposed for HSI classification. When 3-D Gabor transformation could significantly increase the discrimination power of material features, a nonparametric and effective l<sub>2</sub>-norm collaborative representation method is developed to calculate the coefficients. Due to the simplicity of the method, the computational cost has been substantially reduced; thus, all the extracted Gabor features can be directly utilized to code the test sample, which conversely makes the l<sub>2</sub>-norm collaborative representation robust to noise and greatly improves the classification accuracy. The extensive experiments on two real hyperspectral data sets have shown higher performance of the proposed 3GCR over the state-of-the-art methods in the literature, in terms of both the classifier complexity and generalization ability from very small training sets.","S. Jia and L. Shen and Q. Li",2015,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Parallel Primitives for Vendor-Agnostic Implementation of Big Data Mining Algorithms","In the age of Big Data, scalable algorithm implementations as well as powerful computational resources are required. For data mining and data analytics the support of big data platforms is becoming increasingly important, since they provide algorithm implementations with all the resources needed for their execution. However, choosing the best platform might depend on several constraints, including but not limited to computational resources, storage resources, target tasks, service costs. Sometimes it may be necessary to switch from one platform to another depending on the constraints. As a consequence, it is desirable to reuse as much algorithm code as possible, so as to simplify the setup in new target platforms. Unfortunately each big data platform has its own peculiarity, especially to deal with parallelism. This impacts on algorithm implementation, which generally needs to be modified before being executed. This work introduces functional parallel primitives to define the parallelizable parts of algorithms in a uniform way, independent of the target platform. Primitives are then transformed by a compiler into skeletons, which are finally deployed on vendor-dependent frameworks. The procedure proposed aids not only in terms of code reuse but also in terms of parallelization, because programmer's expertise is not demanded. Indeed, it is the compiler that entirely manages and optimizes algorithm parallelization. The experiments performed show that the transformation process does not negatively affect algorithm performance.","C. Bandirali and S. Lodi and G. Moro and A. Pagliarani and C. Sartori",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Automatic Parallel Pattern Detection in the Algorithm Structure Design Space","Parallel design patterns have been developed to help programmers efficiently design and implement parallel applications. However, identifying a suitable parallel pattern for a specific code region in a sequential application is a difficult task. Transforming an application according to support structures applicable to these parallel patterns is also very challenging. In this paper, we present a novel approach to automatically find parallel patterns in the algorithm structure design space of sequential applications. In our approach, we classify code blocks in a region according to the appropriate supportstructure of the detected pattern. This classification eases the transformation of a sequential application into its parallel version. Weevaluated our approach on 17 applications from four different benchmark suites. Our method identified suitable algorithm structure patterns in the sequential applications. We confirmed our results by comparing them with the existing parallel versions of these applications. We also implemented the patterns we detected in cases in which parallel implementations were not available and achieved speedups of up to 14x.","Z. U. Huda and R. Atre and A. Jannesari and F. Wolf",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Filtering noise in mixed-purpose fixing commits to improve defect prediction and localization","In open-source software projects, during fixing software faults, developers sometimes also perform other types of non-fixing code changes such as functionality enhancement, code restructuring/improving, or documentation. They commit non-fixing changes together with the fixing ones in the same transaction. We call them mixed-purpose fixing commits (MFCs). We have conducted an empirical study on MFCs in several popular open-source projects. Our results showed that MFCs are about 11%-39% of total fixing commits. In 3%-41% of MFCs, developers performed other change types without indicating them in the commit logs. Our study also showed that mining software repositories (MSR) approaches that rely on the recovery of the history of fixed/buggy files are affected by the noisy data where non-fixing changes in MFCs are considered as fixing ones. The results of our study motivated us to develop Cardo, a tool to identify MFCs and filter non-fixing changed files in the change sets of the fixing commits. It uses natural language processing to analyze the sentences in commit logs and program analysis to cluster the changes in the change sets to determine if a changed file is for non-fixing. Our empirical evaluation on several open-source projects showed that Cardo achieves on average 93% precision, and existing MSR approaches can be relatively improved up to 32% with data filtered by Cardo.","H. A. Nguyen and A. T. Nguyen and T. N. Nguyen",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"A Portable and Fast Stochastic Volatility Model Calibration Using Multi and Many-Core Processors","Financial markets change precipitously and on-demand pricing and risk models must be constantly recalibrated to reduce risk. However, certain classes of models are computationally intensive to robustly calibrate to intraday pricesstochastic volatility models being an archetypal example due to the non-convexity of the objective function. In order to accelerate this procedure through parallel implementation,nancial application developers are faced with an ever growing plethora of low-level high-performance computing frameworks such as OpenMP, OpenCL, CUDA, or SIMD intrinsics, and forced to make a trade-off between performance versus the portability,exibility and modularity of the code required to facilitate rapid in-house model development and productionization.This paper describes the acceleration of stochastic volatility model calibration on multi-core CPUs and GPUs using the Xcelerit platform. By adopting a simple dataow programming model, the Xcelerit platform enables the application developer to write sequential, high-level C++ code, without concern for low-level high-performance computing frameworks. This platform provides the portability,exibility and modularity required by application developers. Speedups of up to 30x and 293x are respectively achieved on an Intel Xeon CPU and NVIDIA Tesla K40 GPU, compared to a sequential CPU implementation. The Xcelerit platform implementation is further shown to be equivalent in performance to a low-level CUDA version. Overall, we are able to reduce the entire calibration process time of the sequential implementation from 6; 189 seconds to 183:8 and 17:8 seconds on the CPU and GPU respectively without requiring the developer to reimplement in low-level high performance computing frameworks.","M. Dixon and J. Lotze and M. Zubair",2014,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The evolution of data races","Concurrency bugs are notoriously difficult to find and fix. Several prior empirical studies have identified the prevalence and challenges of concurrency bugs in open source projects, and several existing tools can be used to identify concurrency errors such as data races. However, little is known about how concurrency bugs evolve over time. In this paper, we examine the evolution of data races by analyzing samples of the committed code in two open source projects over a multi-year period. Specifically, we identify how the data races in these programs change over time.","C. Sadowski and J. Yi and S. Kim",2012,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Bioacoustic approaches to biodiversity monitoring and conservation in Kenya","Kenya's rich biodiversity faces a number of threats including human encroachment, poaching and climate change. Since Kenya is a developing country, there is need to manage the sometimes competing interests of development, such as infrastructure development, and conservation. To achieve this, tools to effectively monitor the state of Kenya's various ecosystems are essential. In this paper we propose a biodiversity monitoring software tool that integrates acoustic indices of biodiversity, recognition of species of interest based on their vocalizations and acoustic census. This tool can be used by non-experts to determine the current state of their ecosystems by monitoring the state of bird species that serve as indicator taxa and whose abundance is related to the abundance of other terrestrial vertebrates including the “big five”. The tool we propose exploits state-of-the art advances in signal processing and machine learning to perform biodiversity monitoring, bird species detection and census in a joint framework. Using publicly available data we demonstrate how current acoustic indices of biodiversity can be improved by incorporating machine learning based audio segmentation algorithms. We also show how open source toolkits can be used to build bird species recognition systems. Code to reproduce the experiments in this paper is available on Github at https://github.com/ciiram/BirdPy.","C. w. Maina",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Wiretap Polar Codes in Encryption Schemes Based on Learning with Errors Problem","The Learning with Errors (LWE) problem has been extensively studied in cryptography due to its strong hardness guarantees, efficiency and expressiveness in constructing advanced cryptographic primitives. In this work, we show that using polar codes in conjunction with LWE-based encryption yields several advantages. To begin, we demonstrate the obvious improvements in the efficiency or rate of information transmission in the LWE-based scheme by leveraging polar coding (with no change in the cryptographic security guarantee). Next, we integrate wiretap polar coding with LWE-based encryption to ensure provable semantic security over a wiretap channel in addition to cryptographic security based on the hardness of LWE. To the best of our knowledge this is the first wiretap code to have cryptographic security guarantees as well. Finally, we study the security of the private key used in LWE-based encryption with wiretap polar coding, and propose a key refresh method using random bits used in wiretap coding. Under a known-plaintext attack, we show that non-vanishing information-theoretic secrecy can be achieved for the key. We believe our approach is at least as interesting as our final results: our work combines cryptography and coding theory in a novel “non blackbox-way” which may be relevant to other scenarios as well.","A. Rajagopalan and A. Thangaraj and S. Agrawal",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Robust Visual Tracking via Sparse Representation Under Subclass Discriminant Constraint","In this paper, we propose a method for visual tracking based on local sparse representation. Image patches from the object and the background are split into image blocks to construct local representations. Within the subclass discriminant framework, a discriminative subspace is learned to distinguish the object image blocks from the background image blocks while preserving their multimodal structure. A dictionary is constructed using the centers of the object subclasses. With this dictionary, sparse coding is implemented on the projected vectors corresponding to the image blocks, and the sparse coefficients are concatenated to obtain a local sparse code as the feature that represents the image patch. Considering the subclass discriminant constraint and the sparsity constraint imposed on the sparse coding, the subspace learning and sparse representation problems are converted into a joint optimization problem with respect to a transformation matrix and sparse coefficients. To enhance the tracking accuracy, two dictionaries are devised, one to incorporate the original observations of the target and the other to incorporate the latest observations, thereby providing two templates to characterize the appearance of the target. Histogram intersection over the local sparse codes provides an evaluation of the confidence. Finally, the candidate with the maximal confidence is selected as the object image patch. Compared with several state-of-the-art algorithms, our method demonstrates a superior performance when applied to challenging sequences.","C. Qian and Z. Xu",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Automatically Documenting Unit Test Cases","Maintaining unit test cases is important during the maintenance and evolution of a software system. In particular, automatically documenting these unit test cases can ameliorate the burden on developers maintaining them. For instance, by relying on up-to-date documentation, developers can more easily identify test cases that relate to some new or modified functionality of the system. We surveyed 212 developers (both industrial and open-source) to understand their perspective towards writing, maintaining, and documenting unit test cases. In addition, we mined change histories of C# software systems and empirically found that unit test methods seldom had preceding comments and infrequently had inner comments, and both were rarely modified as those methods were modified. In order to support developers in maintaining unit test cases, we propose a novel approach - UnitTestScribe - that combines static analysis, natural language processing, backward slicing, and code summarization techniques to automatically generate natural language documentation of unit test cases. We evaluated UnitTestScribe on four subject systems by means of an online survey with industrial developers and graduate students. In general, participants indicated that UnitTestScribe descriptions are complete, concise, and easy to read.","B. Li and C. Vendome and M. Linares-Vásquez and D. Poshyvanyk and N. A. Kraft",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Performance Analysis and Optimization of the FFTXlib on the Intel Knights Landing Architecture","In this paper, we address the decreasing performance of the FFTXlib, the Fast Fourier Transformation (FFT) kernel of Quantum ESPRESSO, when scaling to a full KNL node. An increased performance in the FFTXlib will likewise increase the performance of the entire Quantum ESPRESSO code one of the most used plane-wave DFT codes in the community of material science. Our approach focuses on, first, overlapping computation and communication and, second, decreasing resource contention for higher compute efficiency. In order to achieve this we use the OmpSs programming model based on task dependencies. We allow overlapping of computation and communication by converting all steps of the FFT into tasks following a flow dependency. In the same way, we decrease resource contention by converting each FFT into an individual task that can be scheduled asynchronously. In both cases, multiple FFTs can be computed in parallel. The task-based optimizations are implemented in the FFTXlib and show up to 10% runtime reduction on the already highly optimized version. Since the task scheduling is done dynamically during execution by the parallel runtime, not statically by the user, it also frees the user from finding the ideal parallel configuration himself.","M. Wagner and V. López and J. Morillo and C. Cavazzoni and F. Affinito and J. Giménez and J. Labarta",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Efficient Non-Binary Hamming Codes for Limited Magnitude Errors in MLC PCMs","Emerging non-volatile main memories (e.g. phase change memories) have been the continuous focus of research currently. These memories provide an attractive alternative to DRAM with their high density and low cost. But the dominant error models in these memories are of limited magnitude caused by resistance drifts. Hamming codes have been used extensively to protect DRAM due to their low decoding latency and low redundancy as well. But with limited magnitude errors, traditional Hamming codes prove to be inefficient. This paper proposes a new systematic limited magnitude error correcting non-binary Hamming code specifically to address limited magnitude errors in multilevel cell memories storing multiple bits per cell. A general construction methodology is presented to correct errors of limited magnitude and is compared to existing schemes addressing limited magnitude errors in phase change memories. A syndrome analysis is done to show the reduction in total number of syndromes for limited magnitude error models. It is shown that the proposed codes provide better latency and complexity compared to existing limited magnitude error correcting non-binary Hamming codes. It is also shown that the proposed codes achieve better redundancy compared to the symbol extended version of binary Hamming codes.","A. Das and N. A. Touba",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"The Lexical and Syntactic Analyzers of the Translator for the EI Language","This paper contains the task of implementation of functionally-imperative programming language EI, the composition and structure of the its translator, a brief description of the lexical and syntactic rules of the language, the algorithms and the basic functions of the lexical and syntactic analyzers. It describes fragments of formal definitions of lexic and syntax of the El-Ianguage that was used for the automated construction of lexical and syntactic analyzers of the compiler in C++ using the client-server package Webtranslab. It describes also the basic algorithms of lexical analyzer (scanner), which in this version of the compiler performs macroprocessing, files including, deleting all the insignificant character sequences and transformation of correct words of language into an internal representation - tokens. We consider the structure and functions of a parser that performs a descending analysis. This parser constructed in the form of a stack automaton with multi-states controlled by the current input token and the current state fields. The composition of the state cell of the automaton and the algorithms of its operation are described, as well as the mechanisms for performing the operations that are embedded in the grammar and intended to implement the functionality of the semantic analyzer and code generator. The current state of realization of two constituent parts of the translator for EI-language - scanner and parser - is described.","A. A. Maliavko",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"An evolutionary study of reusability in Open Source Software","The phenomenon of evolution is closely related to Open Source Software (OSS) as there is a frequent release of versions. Improvements to the software are due to the enormous contributions made by developers. Software reusability is also seen as a necessary characteristic of OSS. In this paper, a conceptual model for reusability assessment is presented and reusability of the software is studied during evolution. The attributes of reusability of different versions are assessed and compared. The relationship between the attributes of reusability is analyzed. The experiment conducted in this paper is one of the potential applications of our proposed reusability assessment model. The study helps to understand the evolution of software from the perspective of reusability. The change in size (e.g. number of lines of code; number of methods/classes), maintainability index complexity, etc. are studied in different versions to analyze their effect on reusability of the software. The results justify the proposed reusability attribute model.","Fazal-e-Amin and A. K. Mahmood and A. Oxley",2012,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR8, CR9, CR10"
"Interactive generalized keyboard driver for Bengali Braille Embosser","Bengali Braille Embosser is a very essential device for learning of visually impaired person in Bangladesh. This paper presents an interactive generalized keyboard driver for Bengali Braille Embosser. This driver is operated by single AVR microcontroller. It can interface any PC keyboard through PS2 interface protocol with the Braille Embosser. The driver is interactive and can communicate with visually impaired person by generating sound of Bengali alphabet after pressing related key of the keyboard. In this paper the Bijoy keyboard layout is used. This driver can stores the written text in memory (micro SD card) and playback the sound of the text also. The designed device supports 16 GB of micro SD storage. This keyboard driver serves the user to edit the written text without help of visual person. So, this device makes a visually impaired person independent of writing and reading. At the same time the device makes Braille code from the written text for printing the Braille text on the Braille paper.","K. Dhar and M. A. Rahman and M. A. Ullah",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Search-based requirements traceability recovery: A multi-objective approach","Software systems nowadays are complex and difficult to maintain due to the necessity of continuous change and adaptation. One of the challenges in software maintenance is keeping requirements traceability up to date automatically. The process of generating requirements traceability is time-consuming and error-prone. Currently, most available tools do not support the automated recovery of traceability links. In some situations, companies accumulate the history of changes from past maintenance experiences. In this paper, we consider requirements traceability recovery as a multi objective search problem in which we seek to assign each requirement to one or many software elements (code elements, API documentation, and comments) by taking into account the recency of change, the frequency of change, and the semantic similarity between the description of the requirement and the software element. We use the Non-dominated Sorting Genetic Algorithm (NSGA-II) to find the best compromise between these three objectives. We report the results of our experiments on three open source projects.","A. Ghannem and M. S. Hamdi and M. Kessentini and H. H. Ammar",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Trace based phase prediction for tightly-coupled heterogeneous cores","Heterogeneous multicore systems are composed of multiple cores with varying energy and performance characteristics. A controller dynamically detects phase changes in applications and migrates execution onto the most efficient core that meets the performance requirements. In this paper, we show that existing techniques that react to performance changes break down at fine-grain intervals, as performance variations between consecutive intervals are high. We propose a predictive trace-based switching controller that predicts an upcoming phase change in a program and preemptively migrates execution onto a more suitable core. This prediction is based on a phase's individual history and the current program context. Our implementation detects re-peatable code sequences to build history, uses these histories to predict an phase change, and preemptively migrates execution to the most appropriate core. We compare our method to phase prediction schemes that track the frequency of code blocks touched during execution as well as traditional reactive controllers, and demonstrate significant increases in prediction accuracy at fine-granularities. For a big-little heterogeneous system that is comprised of a high performing out-of-order core (Big) and an energy-efficient, in-order core (Little), at granularities of 300 instructions, the trace based predictor can spend 28% of execution time on the Little, while targeting a maximum performance degradation of 5%. This translates to an increased energy savings of 15% on average over running only on Big, representing a 60% increase over existing techniques.","S. Padmanabha and A. Lukefahr and R. Das and S. Mahlke",2013,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Exact sequence reconstruction for insertion-correcting codes","We study the problem of perfectly reconstructing sequences from traces. The sequences are codewords from a deletion/insertion-correcting code and the traces are the result of corruption by a fixed number of symbol insertions (larger than the minimum edit distance of the code.) This is the general version of a problem tackled by Levenshtein for uncoded sequences. We introduce an exact formula for the maximum number of common supersequences shared by sequences at a certain edit distance, yielding a tight upper bound on the number of distinct traces necessary to guarantee exact reconstruction. We apply our results to the famous single deletion/insertion-correcting Varshamov-Tenengolts (VT) codes and show that a significant number of VT codeword pairs achieve the worst-case number of outputs needed for exact reconstruction.","F. Sala and R. Gabrys and C. Schoeny and K. Mazooji and L. Dolecek",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"HERCULES: Strong Patterns towards More Intelligent Predictive Modeling","Recent work has shown that program analysis techniques to select meaningful code features of programs are important in the task of deciding the best compiler optimizations. Although, there are many successful state-of-the-art program analysis techniques, they often do not provide a simple method to extract the most expressive information about loops, especially when a target program is computationally intensive with complex loops and data dependencies. In this paper, we introduce a static technique to characterize a program using a pattern-driven system named HERCULES. This characterization technique not only helps a user to understand programs by searching pattern-of-interests, but also can be used for a predictive model that effectively selects the proper compiler optimizations. We formulated 35 loop patterns, then evaluated our characterization technique by comparing the predictive models constructed using HERCULES to three other state-of-the-art characterization methods. We show that our models outperform three state-of-the-art program characterization techniques on two multicore systems in selecting the best optimization combination from a given loop transformation space. We achieved up to 67% of the best possible speedup achievable with the optimization search space we evaluated.","E. Park and C. Kartsaklis and J. Cavazos",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Synchronization algorithm for RFID-reader and its implementation","An algorithm for synchronization of a decoder of a RFID receiver has been proposed, implemented and evaluated, by experimental setup. The solution can be used for UHF RFID (e.g. ISO18000-6 and EPC gen 2) readers using FM0, bi-phase or differential Manchester decoding. Goal of the proposed solution is the improved decoding of the received data in the presence of noise (so by using less power or working at longer distance) by innovative synchronization techniques. Synchronization problem (and challenge) is caused by non-precise timing of the binary signals sent by simple passive RFID-tags. Hardware prototype has been developed for testing and evaluating of the RFID-reader synchronization algorithm for decoding of the binary information. The developed algorithm has been implemented as C/C++ code for DSP and can be evaluated, benchmarked and developed further, with real-world experimental data log files acquired with the developed hardware setup (also on PC). The proposed algorithm is based on the finding of the best correlation of the expected binary preamble (header) waveform of the communication packet with corresponding part of the received noisy signal, as used in other known solutions, with variation of the possibly expected timing parameters, e.g by using matched correlation (FIR) filter-banks. The idea of the proposed solution is to use additionally to maximum correlation of the received signal with the reference preamble waveform also the “quadrature” version of the reference (derived from the ideal preamble) signal, as the “quadrature” correlation has at the best match clear zero value and has change around this “zero” point. So using this additional “Q” correlation channel gives the opportunity to find the exact timing base (“bit duration”) more precisely. The proposed algorithm and the results of the evaluation of the solution with real-world data are described. Also, potential future developments of the solution are discussed.","O. Martens and A. Liimets and A. Kuusik",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An empirical study on the fault-proneness of clone migration in clone genealogies","Copy and paste activities create clone groups in software systems. The evolution of a clone group across the history of a software system is termed as clone genealogy. During the evolution of a clone group, developers may change the location of the code fragments in the clone group. The type of the clone group may also change (e.g., from Type-1 to Type-2). These two phenomena have been referred to as clone migration and clone mutation respectively. Previous studies have found that clone migration occur frequently in software systems, and suggested that clone migration can induce faults in a software system. In this paper, we examine how clone migration phenomena affect the risk for faults in clone segments, clone groups, and clone genealogies from three long-lived software systems JBoss, APACHE-ANT, and ARGOUML. Results show that: (1) migrated clone segments, clone groups, and clone genealogies are not equally fault-prone; (2) when a clone mutation occurs during a clone migration, the risk for faults in the migrated clone is increased; (3) migrating a clone that was not changed for a longer period of time is risky.","S. Xie and F. Khomh and Y. Zou and I. Keivanloo",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Impact of classification of lab assignments and problem solving approach in object oriented programming lab course: A case study","Information systems are growing in both size and complexity. Object orientation has emerged as a dominant paradigm in designing and constructing such large and complex information systems, which is being taught to undergraduate students of Computer Science and Engineering. The use of java technology to develop applications is found preferred one [1]. There exists a knowledge gap between industry expectations and students knowledge. Hence deep learning of java programming is essential attribute to reduce the gap [1]. This paper presents classification of lab assignments as demonstration, Exercises, structured Enquiry type and open ended types with an approach of using conceptual model as intermediate step before writing java code for a given problem statement. This classification of assignments/problem statements move the responsibility from a professor to student where the latter involvement is observed high in achieving the objectives set for each type in classification of assignments, hence helps to meet industry expectations. The conceptual model, drawing a class diagram using standard notations (diagrammatic representations) improves the students understanding of object oriented concepts [7], for a given lab assignment which in-turn eases the student effort in translating the class diagram into program code by following the syntax. The students will be graded during assessment as Grade-S, Grade-A, Grade-B, Grade-C, Grade-D, Grade-E, Grade-F and where Grade-S is highest and Grade-F is lowest. The results of assessment are analyzed for year 2013 and 2014 batches and it is observed that, there is increase in grade A in year 2014 compared to year 2013. The observed change is due to the above said approach. The result analysis of year 2013 and 2012 shows that, the grade F is increased little in results of year 2013, it is due to the current approach where the lab assignments are classified and students are expected to apply the concepts studied effectively and few students unable to follow the approach.","K. M. M. Rajashekharaiah and M. S. Patil and G. H. Joshi",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"When would this bug get reported?","Not all bugs in software would be experienced and reported by end users right away: Some bugs manifest themselves quickly and may be reported by users a few days after they get into the code base; others manifest many months or even years later, and may only be experienced and reported by a small number of users. We refer to the period of time between the time when a bug is introduced into code and the time when it is reported by a user as bug reporting latency. Knowledge of bug reporting latencies has an implication on prioritization of bug fixing activities-bugs with low reporting latencies may be fixed earlier than those with high latencies to shift debugging resources towards bugs highly concerning users. To investigate bug reporting latencies, we analyze bugs from three Java software systems: AspectJ, Rhino, and Lucene. We extract bug reporting data from their version control repositories and bug tracking systems, identify bug locations based on bug fixes, and back-trace bug introducing time based on change histories of the buggy code. Also, we remove non-essential changes, and most importantly, recover root causes of bugs from their treatments/fixes. We then calculate the bug reporting latencies, and find that bugs have diverse reporting latencies. Based on the calculated reporting latencies and features we extract from bugs, we build classification models that can predict whether a bug would be reported early (within 30 days) or later, which may be helpful for prioritizing bug fixing activities. Our evaluation on the three software systems shows that our bug reporting latency prediction models could achieve an AUC (Area Under the Receiving Operating Characteristics Curve) of 70.869%.","F. Thung and D. Lo and L. Jiang and Lucia and F. Rahman and P. T. Devanbu",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"I Know What You Did Last Summer - An Investigation of How Developers Spend Their Time","Developing software is a complex mental activity, requiring extensive technical knowledge and abstraction capabilities. The tangible part of development is the use of tools to read, inspect, edit, and manipulate source code, usually through an IDE (integrated development environment). Common claims about software development include that program comprehension takes up half of the time of a developer, or that certain UI (user interface) paradigms of IDEs offer insufficient support to developers. Such claims are often based on anecdotal evidence, throwing up the question of whether they can be corroborated on more solid grounds. We present an in-depth analysis of how developers spend their time, based on a fine-grained IDE interaction dataset consisting of ca. 740 development sessions by 18 developers, amounting to 200 hours of development time and 5 million of IDE events. We propose an inference model of development activities to precisely measure the time spent in editing, navigating and searching for artifacts, interacting with the UI of the IDE, and performing corollary activities, such as inspection and debugging. We report several interesting findings which in part confirm and reinforce some common claims, but also disconfirm other beliefs about software development.","R. Minelli and A. Mocci and M. Lanza",2015,"[""IEEE""]","Rejeitado: CR10","Rejeitado: CR10"
"Upgrading Your Android, Elevating My Malware: Privilege Escalation through Mobile OS Updating","Android is a fast evolving system, with new updates coming out one after another. These updates often completely overhaul a running system, replacing and adding tens of thousands of files across Android's complex architecture, in the presence of critical user data and applications (apps for short). To avoid accidental damages to such data and existing apps, the upgrade process involves complicated program logic, whose security implications, however, are less known. In this paper, we report the first systematic study on the Android updating mechanism, focusing on its Package Management Service (PMS). Our research brought to light a new type of security-critical vulnerabilities, called Pileup flaws, through which a malicious app can strategically declare a set of privileges and attributes on a low-version operating system (OS) and wait until it is upgraded to escalate its privileges on the new system. Specifically, we found that by exploiting the Pileup vulnerabilities, the app can not only acquire a set of newly added system and signature permissions but also determine their settings (e.g., protection levels), and it can further substitute for new system apps, contaminate their data (e.g., cache, cookies of Android default browser) to steal sensitive user information or change security configurations, and prevent installation of critical system services. We systematically analyzed the source code of PMS using a program verification tool and confirmed the presence of those security flaws on all Android official versions and over 3000 customized versions. Our research also identified hundreds of exploit opportunities the adversary can leverage over thousands of devices across different device manufacturers, carriers and countries. To mitigate this threat without endangering user data and apps during an upgrade, we also developed a new detection service, called SecUP, which deploys a scanner on the user's device to capture the malicious apps designed to exploit Pileup vulnerabilities, based upon the vulnerability-related information automatically collected from newly released Android OS images.","L. Xing and X. Pan and R. Wang and K. Yuan and X. Wang",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR10"
"Ptask: An educational C library for programming real-time systems on Linux","When learning real-time programming, the novice is faced with many technical difficulties due to low-level C libraries that require considerable programming effort even for implementing a simple periodic task. For example, the POSIX Real-Time standard only provides a low level notion of thread, hence programmers usually build higher level code on top of the POSIX API, every time re-inventing the wheel. In this paper we present a simple C library that simplifies realtime programming in Linux by hiding low-level details of task creation, allocation and synchronization, and provides utilities for more high-level functionalities, like support for mode-change and adaptive systems. The library is released as open-source and it is currently being employed to teach real-time programming in university courses in embedded systems.","G. Buttazzo and G. Lipari",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Dot-product based preference preserved hashing for fast collaborative filtering","Recommendation is widely used to deal with information overloading by suggesting items based on historical information of users. One of the most popular recommendation techniques is matrix factorization (MF), in which the preferences of users are estimated by dot products of their real latent factors between users and items. Although MF can achieve high recommendation accuracy, it suffers from efficiency issues when making preferences ranking in real space. Hash retrieval technique can be applied to recommender systems to speed up preferences ranking. Due to the existence of discrete constraints in learning hash codes, it is possible to exploit a two-stage learning procedure according to most existing methods. This two-stage procedure consists of relaxed optimization by discarding discrete constraints and subsequent binary quantization. However, existing methods have not been able to well handle the change of dot product arising from quantization. To this end, we propose a dot-product based preference preserved hashing method, which quantizes both norm and cosine similarity in dot product respectively. We also design an algorithm to optimize the bit length for norm quantization. Based on the evaluation to several datasets, the proposed framework shows consistent superiority to the competing baselines even though only using shorter binary code.","Y. Zhang and G. Yang and L. Hu and H. Wen and J. Wu",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"RegTT: Accelerating Tree Traversals on GPUs by Exploiting Regularities","Tree traversals are widely used irregular applications. Given a tree traversal algorithm, where a single tree is traversed by multiple queries (with truncation), its efficient parallelization on GPUs is hindered by branch divergence, load imbalance and memory-access irregularity, as the nodes and their visitation orders differ greatly under different queries. We leverage a key insight made on several truncation-induced tree traversal regularities to enable as many threads in the same warp as possible to visit the same node simultaneously, thereby enhancing both GPU resource utilization and memory coalescing at the same time. We introduce a new parallelization approach, RegTT, to orchestrate an efficient execution of a tree traversal algorithm on GPUs by starting with BFT (Breadth-First Traversal), then reordering the queries being processed (based on their truncation histories), and finally, switching to DFT (Depth-First Traversal). RegTT is general (without relying on domain-specific knowledge) and automatic (as a source-code transformation). For a set of five representative benchmarks used, RegTT outperforms the state-of-the-art by 1.66x on average.","F. Zhang and P. Di and H. Zhou and X. Liao and J. Xue",2016,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Exploring the Use of Automated API Migrating Techniques in Practice: An Experience Report on Android","In recent years, open source software libraries have allowed developers to build robust applications by consuming freely available application program interfaces (API). However, when these APIs evolve, consumers are left with the difficult task of migration. Studies on API migration often assume that software documentation lacks explicit information for migration guidance and is impractical for API consumers. Past research has shown that it is possible to present migration suggestions based on historical code-change information. On the other hand, research approaches with optimistic views of documentation have also observed positive results. Yet, the assumptions made by prior approaches have not been evaluated on large scale practical systems, leading to a need to affirm their validity. This paper reports our recent practical experience migrating the use of Android APIs in FDroid apps when leveraging approaches based on documentation and historical code changes. Our experiences suggest that migration through historical code-changes presents various challenges and that API documentation is undervalued. In particular, the majority of migrations from removed or deprecated Android APIs to newly added APIs can be suggested by a simple keyword search in the documentation. More importantly, during our practice, we experienced that the challenges of API migration lie beyond migration suggestions, in aspects such as coping with parameter type changes in new API. Future research may aim to design automated approaches to address the challenges that are documented in this experience report.","M. Lamothe and W. Shang",2018,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"ByteWise: A case study in neural network obfuscation identification","Researchers taking advantage of recent advancements in neural networks have made leaps in many fields such as image recognition, natural language processing, and speech recognition. However, little work has been done with neural networks in the field of binary analysis. Recently, researchers have used neural networks to recognize function boundaries in binaries, using only the bytes of the programs as features. In this paper, we extend their work to detect the bytes of bogus basic blocks added in the dead branches of opaque predicates. We perform a case study using the bogus control flow transformation offered by Obfuscator-LLVM. We detect the bytes of bogus basic blocks with a 94% F1 score. This information can be used to prune code for static reverse engineering. We believe this line of research will yield optimized triage, reverse engineering tools, and malware detection based on obfuscation identification using neural networks.","L. Jones and D. Christman and S. Banescu and M. Carlisle",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"A Slice-Based Estimation Approach for Maintenance Effort","Program slicing is used as a basis for an approach to estimate maintenance effort. A case study of the GNU Linux kernel with over 900 versions spanning 17 years of history is presented. For each version a system dictionary is built using a lightweight slicing approach and encodes the forward decomposition static slice profiles for all variables in all the files in the system. Changes to the system are then modeled at the behavioral level using the difference between the system dictionaries of two versions. The three different granularities of slice (i.e., line, function, and file) are analyzed. We use a direct extension of srcML to represent computed change information. The retrieved information reflects the fact that additional knowledge of the differences can be automatically derived to help maintainers understand code changes. We consider the hypotheses: (1) The structured format helps create traceability links between the changes and other software artifacts. (2) This model is predictive of maintenance effort. The results demonstrate that the approach accurately predicts effort in a scalable manner.","H. W. Alomari and M. L. Collard and J. I. Maletic",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Implementing a Knowledge Bases Debugger","Knowledge representation is an important topic in common-sense reasoning and Artificial Intelligence, and one of the earliest techniques to represent it is by means of knowledge bases encoded into logic clauses. Encoding knowledge, however, is prone to typos and other kinds of consistency mistakes, which may yield incorrect results or even internal contradictions with conflicting information from other parts of the same code. In order to overcome such situations, we propose a logic-programming system to debug knowledge bases. The system has a strong theoretical framework on knowledge representation and reasoning, and a suggested on-line prototype where one can test logic programs. Such logic programs may have, of course, conflicting information and the system shall prompt the user where the possible source of conflict is. Besides, the system can be employed to identify conflicts of the knowledge base itself and upcoming new information, it can also be used to locate the source of conflict from a given inherently inconsistent static knowledge base. This paper describes an implementation of a declarative version of the system that has been characterised to debug knowledge bases in a semantical formalism. Some of the key components of such implementation are existing solvers, so this paper focuses on how to use them and why they work, towards an implemented a fully-fledged system.","J. C. A. Guadarrama and J. R. M. Romero and M. Romero and J. H. Camacho",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Combining and Visualizing Time-Oriented Data from the Software Engineering Toolset","The simultaneous use of more than two different data sources from the software engineering toolset is still uncommon in the research areas of software evolution and visualization. In our work, we address this research gap by the design and evaluation of three interactive visualizations which combine the data from the version control (VCS), the issue tracking (ITS), and the continuous integration (CI) system. After analyzing the information needs of a project team and describing the available data, we selected change impact, code ownership, and activity peaks as our visualization topics. Then, we adapted suitable visualization techniques from the literature to meet our design requirements. After implementation, we evaluated our visualizations by conducting a usability test with ten senior software engineers. On the system usability scale (SUS), our visualizations achieved the rating ""good"" from the participants. A scenario success rate of 88% and the qualitative user feedback has provided evidence for the benefits of visualizing combined data from the VCS, ITS, and CI system.","J. Grabner and R. Decker and T. Artner and M. Bernhart and T. Grechenig",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Performance-reliability tradeoff analysis for multithreaded applications","Modern architectures become more susceptible to transient errors with the scale down of circuits. This makes reliability an increasingly critical concern in computer systems. In general, there is a tradeoff between system reliability and performance of multithreaded applications running on multicore architectures. In this paper, we conduct a performance-reliability analysis for different parallel versions of three data-intensive applications including FFT, Jacobi Kernel, and Water Simulation. We measure the performance of these programs by counting execution clock cycles, while the system reliability is measured by Thread Vulnerability Factor (TVF) which is a recently-proposed metric. TVF measures the vulnerability of a thread to hardware faults at a high level. We carry out experiments by executing parallel implementations on multicore architectures and collect data about the performance and vulnerability. Our experimental evaluation indicates that the choice is clear for FFT application and Jacobi Kernel. Transpose algorithm for FFT application results in less than 5% performance loss while the vulnerability increases by 20% compared to binary-exchange algorithm. Unrolled Jacobi code reduces execution time up to 50% with no significant change on vulnerability values. However, the tradeoff is more interesting for Water Simulation where nsquared version reduces the vulnerability values significantly by worsening the performance with similar rates compared to faster but more vulnerable spatial version.","I. Oz and H. R. Topcuoglu and M. Kandemir and O. Tosun",2012,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"To fix or to learn? How production bias affects developers' information foraging during debugging","Developers performing maintenance activities must balance their efforts to learn the code vs. their efforts to actually change it. This balancing act is consistent with the “production bias” that, according to Carroll's minimalist learning theory, generally affects software users during everyday tasks. This suggests that developers' focus on efficiency should have marked effects on how they forage for the information they think they need to fix bugs. To investigate how developers balance fixing versus learning during debugging, we conducted the first empirical investigation of the interplay between production bias and information foraging. Our theory-based study involved 11 participants: half tasked with fixing a bug, and half tasked with learning enough to help someone else fix it. Despite the subtlety of difference between their tasks, participants foraged remarkably differently-making foraging decisions from different types of “patches,” with different types of information, and succeeding with different foraging tactics.","D. Piorkowski and S. D. Fleming and C. Scaffidi and M. Burnett and I. Kwan and A. Z. Henley and J. Macbeth and C. Hill and A. Horvath",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"A novel image encoding and communication technique of B/W images for IOT, robotics and drones using (15, 11) reed solomon scheme","In the modern age of IOT and Robotics, different intelligent entities like robots, drones, IOT nodes or smart vehicles need fast and error-free communication of data, which is predominantly in the form of images. The medium used for communication of this data is mainly wireless and it can be short distance, medium distance, long haul or even satellite links crossing the ionosphere layers. Different wireless mediums incorporate different types of noises in the images being transmitted by Drones, Robots or IOT Nodes. For better analysis and then performing subsequent action on the basis of these received images using artificial intelligence, machine learning or machine vision, it is imperative that the images transmitted are encoded and recovered as fast and as error-free as possible. Normal conventional methods use different image correction algorithms for detection and correction of errors in images. Reed Solomon codes, which are normally used for error detection and correction at data link layer in TCP/IP protocol stack, have a high probability of signal correction and are highly efficient due to their burst error detection and correction capabilities. The RS codes can be implemented where there is a large number of input symbols and noise duration is relatively small as compared to the code word. Sometimes at the receiver end, we get images which are partially corrupted and only half or some part of them is visible. Most of the filters used for image reconstruction insert the approximated bits in place of the corrupted bits by using some algorithms but if only partial part of the image is corrupted, no filter will be able to recover the images properly as it will also change the bits in the non-corrupted part of the image. We have proposed a novel approach of using RS codes for the detection and correction of errors in the images. This novel technique can be used over a variety of applications including robotics, drones, IOT nodes, smart vehicles using wireless and satellite communication, which include the transfer of images and decision making on the basis of the content of the images.","M. Z. Ejaz and K. Khurshid and Z. Abbas and M. A. Aizaz and A. Nawaz",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Mutations: How Close are they to Real Faults?","Mutation analysis is often used to compare the effectiveness of different test suites or testing techniques. One of the main assumptions underlying this technique is the Competent Programmer Hypothesis, which proposes that programs are very close to a correct version, or that the difference between current and correct code for each fault is very small. Researchers have assumed on the basis of the Competent Programmer Hypothesis that the faults produced by mutation analysis are similar to real faults. While there exists some evidence that supports this assumption, these studies are based on analysis of a limited and potentially non-representative set of programs and are hence not conclusive. In this paper, we separately investigate the characteristics of bug-fixes and other changes in a very large set of randomly selected projects using four different programming languages. Our analysis suggests that a typical fault involves about three to four tokens, and is seldom equivalent to any traditional mutation operator. We also find the most frequently occurring syntactical patterns, and identify the factors that affect the real bug-fix change distribution. Our analysis suggests that different languages have different distributions, which in turn suggests that operators optimal in one language may not be optimal for others. Moreover, our results suggest that mutation analysis stands in need of better empirical support of the connection between mutant detection and detection of actual program faults in a larger body of real programs.","R. Gopinath and C. Jensen and A. Groce",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Relationship between features volatility and software architecture design stability in object-oriented software: Preliminary analysis","Software architecture is the core structure of a system. Software architecture describes the functionality and the size of system to be built. Software architecture is illustrated as packages diagram, class diagram or Enterprise Architecture diagram. To make a robust software, it's important to know quality of architecture. Architecture Quality is reflected in its design. There are various topics of research on the quality aspect of the architectural design, from enviroment adaption of architectural design to design stability maintenance. The concept of reuse elements of the system is one of the topics to maintain the stability of the software design. Aversano and Constantinou introduce the method of measuring the stability of the architectural design by taking into account external and internal elements of architecture built. Both just look at the number of packets that undergo additions and deletions to the pair versions. Quantitative research to assess an architectural stability by looking at environmental factors needed to complete measurement. Before implementing this factor, it is necessary to measure the relationship between variables the stability and environmental factors. We introduced a quantitative analysis of the mechanisms related to the extent to which the relationship between features volatility and architecture stability. Architecture design stability is measured by metrics Constantinou, and the calculation of features volatility depend on change of features from consecutive version. We applied this analysis into one project. The source code in the repository extracted to be converted into data according to metrics Constantinou, then the results are validated by experts selected. Datasets that have been validated measured by metrics and measurable correlation with Pearson-Product-Moment analysis.","F. Handani and S. Rochimah",2015,"[""IEEE"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Heterogeneous Face Recognition Using Domain Specific Units","The task of Heterogeneous Face Recognition consists in matching face images that are sensed in different domains, such as sketches to photographs (visual spectra images), thermal images to photographs or near-infrared images to photographs. In this work we suggest that high level features of Deep Convolutional Neural Networks trained on visual spectra images are potentially domain independent and can be used to encode faces sensed in different image domains. A generic framework for Heterogeneous Face Recognition is proposed by adapting Deep Convolutional Neural Networks low level features in, so called, “Domain Specific Units”. The adaptation using Domain Specific Units allow the learning of shallow feature detectors specific for each new image domain. Furthermore, it handles its transformation to a generic face space shared between all image domains. Experiments carried out with four different face databases covering three different image domains show substantial improvements, in terms of recognition rate, surpassing the state-of-the-art for most of them. This work is made reproducible: all the source code, scores and trained models of this approach are made publicly available.","T. d. F. Pereira and A. Anjos and S. Marcel",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Multi-Level Random Walk for Software Test Suite Reduction","Software testing is important and time-consuming. A test suite, i.e., a set of test cases, plays a key role in validating the expected program behavior. In modern test-driven development, a test suite pushes the development progress. Software evolves over time; its test suite is executed to detect whether a new code change adds bugs to the existing code. Executing all test cases after each code change is unnecessary and may be impossible due to the limited development cycle. On the one hand, multiple test cases may focus on an identical piece of code; then several test cases cannot detect extra bugs. On the other hand, even executing a test suite once in a large project takes around one hour [1]; frequent code changes require much time for conducting testing. For instance, in Hadoop, a framework of distributed computing, 2,847 version commits are accepted within one year from September 2014 with a peak of 135 commits in one week [2].","Z. Chi and J. Xuan and Z. Ren and X. Xie and H. Guo",2017,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"CAFe: Coarray Fortran Extensions for Heterogeneous Computing","Emerging hybrid accelerator architectures are often proposed for inclusion as components in an exascale machine, not only for performance reasons but also to reduce total power consumption. Unfortunately, programmers of these architectures face a daunting and steep learning curve that frequently requires learning a new language (e.g., OpenCL) or adopting a new programming model. Furthermore, the distributed (and frequently multi-level) nature of the memory organization of clusters of these machines provides an additional level of complexity. This paper presents preliminary work examining how Fortran coarray syntax can be extended to provide simpler access to accelerator architectures. This programming model integrates the Partitioned Global Address Space (PGAS) features of Fortran with some of the more task-oriented constructs in OpenMP 4.0 and OpenACC. It also includes the potential for compiler-based transformations targeting the Open Community Runtime (OCR) environment. We demonstrate these CoArray Fortran extensions (CAFe) by implementing a multigrid Laplacian solver and transforming this high-level code to a mixture of standard coarray Fortran and OpenCL kernels.","C. Rasmussen and M. Sottile and S. Rasmussen and D. Nagle and W. Dumas",2016,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Abstract Machine Models and Proxy Architectures for Exascale Computing","To achieve exascale computing, fundamental hardware architectures must change. This will significantly impact scientific applications that run on current high performance computing (HPC) systems, many of which codify years of scientific domain knowledge and refinements for contemporary computer systems. To adapt to exascale architectures, developers must be able to reason about new hardware and determine what programming models and algorithms will provide the best blend of performance and energy efficiency in the future. An abstract machine model is designed to expose to the application developers and system software only the aspects of the machine that are important or relevant to performance and code structure. These models are intended as communication aids between application developers and hardware architects during the co-design process. A proxy architecture is a parameterized version of an abstract machine model, with parameters added to elucidate potential speeds and capacities of key hardware components. These more detailed architectural models enable discussion among the developers of analytic models and simulators and computer hardware architects and they allow for application performance analysis, system software development, and hardware optimization opportunities. In this paper, we present a set of abstract machine models and show how they might be used to help software developers prepare for exascale. We then apply parameters to one of these models to demonstrate how a proxy architecture can enable a more concrete exploration of how well application codes map onto future architectures.","J. A. Ang and R. F. Barrett and R. E. Benner and D. Burke and C. Chan and J. Cook and D. Donofrio and S. D. Hammond and K. S. Hemmert and S. M. Kelly and H. Le and V. J. Leung and D. R. Resnick and A. F. Rodrigues and J. Shalf and D. Stark and D. Unat and N. J. Wright",2014,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Current-limiting droop controller with fault-ride-through capability for grid-tied inverters","In this paper, the recently proposed current-limiting droop (CLD) controller for grid-connected inverters is enhanced in order to comply with the Fault-Ride-Through (FRT) requirements set by the Grid Code under grid voltage sags. The proposed version of the CLD extends the operation of the original CLD by fully utilizing the power capacity of the inverter under grid faults. It is analytically proven that during a grid fault, the inverter current increases but never violates a given maximum value. Based on this property, an FRT algorithm is proposed and embedded into the proposed control design to support the voltage of the grid. In contrast to the existing FRT algorithms that change the desired values of both the real and reactive power, the proposed method maximizes only the reactive power to support the grid voltage and the real power automatically drops due to the inherent current-limiting property. Extensive simulations are presented to compare the proposed control approach with the original CLD under a faulty grid.","A. G. Paspatis and G. C. Konstantopoulos and M. Mayfield and V. C. Nikolaidis",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Generalized Independent Component Analysis Over Finite Alphabets","Independent component analysis (ICA) is a statistical method for transforming an observable multi-dimensional random vector into components that are as statistically independent as possible from each other. Usually, the ICA framework assumes a model according to which the observations are generated (such as a linear transformation with additive noise). ICA over finite fields is a special case of ICA in which both the observations and the independent components are over a finite alphabet. In this paper, we consider a generalization of this framework in which an observation vector is decomposed to its independent components (as much as possible) with no prior assumption on the way it was generated. This generalization is also known as Barlow's minimal redundancy representation problem and is considered an open problem. We propose several theorems and show that this hard problem can be accurately solved with a branch and bound search tree algorithm, or tightly approximated with a series of linear problems. Our contribution provides the first efficient set of solutions to Barlow's problem. The minimal redundancy representation (also known as factorial code) has many applications, mainly in the fields of neural networks and deep learning. The binary ICA is also shown to have applications in several domains, including medical diagnosis, multi-cluster assignment, network tomography, and internet resource management. In this paper, we show that this formulation further applies to multiple disciplines in source coding, such as predictive coding, distributed source coding, and coding of large alphabet sources.","A. Painsky and S. Rosset and M. Feder",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Fast End-to-End Trainable Guided Filter","Image processing and pixel-wise dense prediction have been advanced by harnessing the capabilities of deep learning. One central issue of deep learning is the limited capacity to handle joint upsampling. We present a deep learning building block for joint upsampling, namely guided filtering layer. This layer aims at efficiently generating the high-resolution output given the corresponding low-resolution one and a high-resolution guidance map. The proposed layer is composed of a guided filter, which is reformulated as a fully differentiable block. To this end, we show that a guided filter can be expressed as a group of spatial varying linear transformation matrices. This layer could be integrated with the convolutional neural networks (CNNs) and jointly optimized through end-to-end training. To further take advantage of end-to-end training, we plug in a trainable transformation function that generates task-specific guidance maps. By integrating the CNNs and the proposed layer, we form deep guided filtering networks. The proposed networks are evaluated on five advanced image processing tasks. Experiments on MIT-Adobe FiveK Dataset demonstrate that the proposed approach runs 10-100Ã— faster and achieves the state-of-the-art performance. We also show that the proposed guided filtering layer helps to improve the performance of multiple pixel-wise dense prediction tasks. The code is available at https://github.com/wuhuikai/DeepGuidedFilter.","H. Wu and S. Zheng and J. Zhang and K. Huang",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"gCad: A Near-Miss Clone Genealogy Extractor to Support Clone Evolution Analysis","Understanding the evolution of code clones is important for both developers and researchers to understand the maintenance implications of clones and to design robust clone management systems. Generally, a study of clone evolution starts with extracting clone genealogies across multiple versions of a program and classifying them according to their change patterns. Although these tasks are straightforward for exact clones, extracting the history of near-miss clones and classifying their change patterns automatically is challenging due to the potential diverse variety of clone fragments even in the same clone class. In this tool demonstration paper we describe the design and implementation of a near-miss clone genealogy extractor, gCad, that can extract and classify both exact and near-miss clone genealogies. Developers and researchers can compute a wide range of popular metrics regarding clone evolution by simply post processing the gCad results. gCad scales well to large subject systems, works for different granularities of clones, and adapts easily to popular clone detection tools.","R. K. Saha and C. K. Roy and K. A. Schneider",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Applying Transactional Memory for Concurrency-Bug Failure Recovery in Production Runs","Concurrency bugs widely exist and severely threaten system availability. Techniques that help recover from concurrency-bug failures during production runs are highly desired. This paper proposes BugTM, an approach that applies transactional memory techniques for concurrency-bug recovery in production runs. Requiring no knowledge about where are concurrency bugs, BugTM uses static analysis and code transformation to enable BugTM-transformed software to recover from a concurrency-bug failure by rolling back and re-executing the recent history of a failure thread. BugTM is instantiated as three schemes that have different trade-offs in performance and recovery capability: BugTMH uses existing hardware transactional memory (HTM) support, BugTMS leverages software transactional memory techniques, and BugTMHS is a software-hardware hybrid design. BugTM greatly improves the recovery capability of state-of-the-art techniques with low run-time overhead and no changes to OS or hardware, while guarantees not to introduce new bugs.","Y. Chen and S. Wang and S. Lu and K. Sankaralingam",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"V524 Monocerotis: A Marginal Contact Binary with a Cyclic Period Variation","We have secured and analyzed three-color light curves of V524 Monocerotis with the 2003 version of the Wilson–Devinney code. We confirmed that V524 Mon is a shallow W-type contact binary system with a mass ratio of$q$$=$2.099 and a degree of contact factor of$f$$=$7.7%. Based on the new eight times of the light minima and those published by previous investigators, we find that the orbital period of the binary shows a long-term decrease ($dp/dt$$=$$-$1.52$\times$10$^{-10}$), while it undergoes a cyclic oscillation ($T_{3}$$=$23.93 yr,$A_{3}$$=$0.0082 d). The long-term period decrease can be explained by mass transfer from the primary to the secondary. The cyclic change, explained as the light-travel time effect, reveals the presence of a tertiary companion. The marginal contact configuration and the continuous period decrease both suggest that the system may be a newly formed contact binary.","H. Jia-Jia and W. Jing-Jing and Q. Sheng-Bang",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Improved data minimization technique in reducing memory space complexity for DNA local alignment accelerator application","Improved data minimization technique to optimize the length of DNA sequence and alignment result characters representation is presented in this paper. The primary objective is to improve and optimize data representation for DNA sequences alignment and result character. The proposed design change in algorithm and architecture is presented in this paper. Algorithm design based on binary equivalent method is used to obtain the optimal size of characters representation. The code is written, compiled and simulated using Altera Quartus II Version 9.0 EDA tools. Verilog Hardware Description Language (HDL) and Altera Cyclone II EP2C35 FPGA are used as coding language and target device respectively. In addition, the structural modelling technique is used to reduce the design complexity. Simulation result showed that the improved data minimization technique takes 50% more memory compared to previous work, but it covers 6 DNA sequences and alignment result characters.","S. A. M. A. Junid and N. M. Tahir and Z. A. Majid and A. K. Halim and K. K. M. Shariff",2012,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Continuous Deployment and Schema Evolution in SQL Databases","Continuous Deployment is an important enabler of rapid delivery of business value and early end user feedback. While frequent code deployment is well understood, the impact of frequent change on persistent data is less understood and supported. SQL schema evolutions in particular can make it expensive to deploy a new version, and may even lead to downtime if schema changes can only be applied by blocking operations. In this paper we study the problem of continuous deployment in the presence of database schema evolution in more detail. We identify a number of shortcomings to existing solutions and tools, mostly related to avoidable downtime and support for foreign keys. We propose a novel approach to address these problems, and provide an open source implementation. Initial evaluation suggests the approach is effective and sufficiently efficient.","M. d. Jong and A. v. Deursen",2015,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The phase smearing effect in the light curves of contact binaries observed by the Kepler mission and the determination of the parameters of 17 contact systems","The Kepler mission observations, taken in the long cadence mode, have a time resolution of about 30 min. In this paper, we investigate how the long cadence binning influences the shapes of the light curves of eclipsing binaries. A simulated light curve of a contact binary exhibiting a flat-bottom secondary minimum was applied for this purpose. We found that the binning caused a change in the variation amplitude and the shape of the minima. We modelled the simulated light curves corresponding to periods between 0.2 and 2 d using a code that does not account for binning and we derived the parameters. It turned out that only when the binary period is close to or longer than about 1.5 d are the solutions derived with such a code accurate. Rigorous modelling of systems with shorter periods requires the use of codes that do account for phase smearing due to long exposure times. We selected a sample of contact binaries observed by the Kepler mission, exhibiting a flat-bottom secondary minimum and showing no intrinsic activity. We solved the light curves of the sample with the most recent (2015) version of the Wilson–Devinney code and we derived the system parameters. The best models that we derived indicate that most of the systems in our sample have a deep contact configuration and that 13 out of 17 required the addition of a third light for good fits. Our results suggest that 13 systems could have tertiary companions.","S. Zola and A. Baran and B. Debski and D. Jableka",2016,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR8, CR9"
"Vigiles: Fine-Grained Access Control for MapReduce Systems","Security concerns surrounding the rise of Big Data systems have stimulated myriad new Big Data security models and implementations over the past few years. A significant disadvantage shared by most of these implementations is that they customize the underlying system source code to enforce new policies, making the customizations difficult to maintain as these layers evolve over time (e.g., over version updates). This paper demonstrates how a broad class of safety policies, including fine-grained access control policies at the level of key-value data pairs rather than files, can be elegantly enforced on MapReduce clouds with minimal overhead and without any change to the system or OS implementations. The approach realizes policy enforcement as a middleware layer that rewrites the cloud's front-end API with reference monitors. After rewriting, the jobs run on input data authorized by fine-grained access control policies, allowing them to be safely executed without additional system-level controls. Detailed empirical studies show that this more modular approach exhibits just 1% overhead compared to a less modular implementation that customizes MapReduce directly to enforce the same policies.","H. Ulusoy and M. Kantarcioglu and E. Pattuk and K. Hamlen",2014,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Galactic orbital motions of star clusters: static versus semicosmological time-dependent Galactic potentials","In order to understand the orbital history of Galactic halo objects, such as globular clusters, authors usually assume a static potential for our Galaxy with parameters that appear at the present day. According to the standard paradigm of galaxy formation, galaxies grow through a continuous accretion of fresh gas and a hierarchical merging with smaller galaxies from high redshift to the present day. This implies that the mass and size of disc, bulge, and halo change with time. We investigate the effect of assuming a live Galactic potential on the orbital history of halo objects and its consequences on their internal evolution. We numerically integrate backwards the equations of motion of different test objects located in different Galactocentric distances in both static and time-dependent Galactic potentials in order to see if it is possible to discriminate between them. We show that in a live potential, the birth of the objects, 13 Gyr ago, would have occurred at significantly larger Galactocentric distances, compared to the objects orbiting in a static potential. Based on the direct N-body calculations of star clusters carried out with collisional N-body code, nbody6, we also discuss the consequences of the time-dependence of a Galactic potential on the early- and long-term evolution of star clusters in a simple way, by comparing the evolution of two star clusters embedded in galactic models, which represent the galaxy at present and 12 Gyr ago, respectively. We show that assuming a static potential over a Hubble time for our Galaxy as it is often done, leads to an enhancement of mass-loss, an overestimation of the dissolution rates of globular clusters, an underestimation of the final size of star clusters, and a shallower stellar mass function.","H. Haghi and A. H. Zonoozi and S. Taghavi",2015,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Customizable Computing—From Single Chip to Datacenters","Since its establishment in 2009, the Center for Domain-Specific Computing (CDSC) has focused on customizable computing. We believe that future computing systems will be customizable with extensive use of accelerators, as custom-designed accelerators often provide 10–100X performance/energy efficiency over the general-purpose processors. Such an accelerator-rich architecture presents a fundamental departure from the classical von Neumann architecture, which emphasizes efficient sharing of the executions of different instructions on a common pipeline, providing an elegant solution when the computing resource is scarce. In contrast, the accelerator-rich architecture features heterogeneity and customization for energy efficiency; this is better suited for energy-constrained designs where the silicon resource is abundant and spatial computing is favored—which has been the case with the end of Dennard scaling. Currently, customizable computing has garnered great interest; for example, this is evident by Intel’s $17 billion acquisition of Altera in 2015 and Amazon’s introduction of field-programmable gate-arrays (FPGAs) in its AWS public cloud. In this paper, we present an overview of the research programs and accomplishments of CDSC on customizable computing, from single chip to server node and to datacenters, with extensive use of composable accelerators and FPGAs. We highlight our successes in several application domains, such as medical imaging, machine learning, and computational genomics. In addition to architecture innovations, an equally important research dimension enables automation for customized computing. This includes automated compilation for combining source-code-level transformation for high-level synthesis with efficient parameterized architecture template generations, and efficient runtime support for scheduling and transparent resource management for integration of FPGAs for datacenter-scale acceleration with support to the existing programming interfaces, such as MapReduce, Hadoop, and Spark, for large-scale distributed computation. We will present the latest progress in these areas, and also discuss the challenges and opportunities ahead.","J. Cong and Z. Fang and M. Huang and P. Wei and D. Wu and C. H. Yu",2019,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"N-body simulations with a cosmic vector for dark energy","We present the results of a series of cosmological N-body simulations of a vector dark energy (VDE) model, performed using a suitably modified version of the publicly available gadget-2 code. The set-ups of our simulations were calibrated pursuing a twofold aim: (1) to analyse the large-scale distribution of massive objects and (2) to determine the properties of halo structure in this different framework. We observe that structure formation is enhanced in VDE, since the mass function at high redshift is boosted up to a factor of 10 with respect to Λ cold dark matter (ΛCDM), possibly alleviating tensions with the observations of massive clusters at high redshifts and early reionization epoch. Significant differences can also be found for the value of the growth factor, which in VDE shows a completely different behaviour, and in the distribution of voids, which in this cosmology are on average smaller and less abundant. We further studied the structure of dark matter haloes more massive than 5 × 1013h−1M⊙, finding that no substantial difference emerges when comparing spin parameter, shape, triaxiality and profiles of structures evolved under different cosmological pictures. Nevertheless, minor differences can be found in the concentration–mass relation and the two-point correlation function, both showing different amplitudes and steeper slopes. Using an additional series of simulations of a ΛCDM scenario with the same$\Omega _{\rm M}$and σ8used in the VDE cosmology, we have been able to establish whether the modifications induced in the new cosmological picture were due to the particular nature of the dynamical dark energy or a straightforward consequence of the cosmological parameters. On large scales, the dynamical effects of the cosmic vector field can be seen in the peculiar evolution of the cluster number density function with redshift, in the shape of the mass function, in the distribution of voids and on the characteristic form of the growth index γ(z). On smaller scales, internal properties of haloes are almost unaffected by the change of cosmology, since no statistical difference can be observed in the characteristics of halo profiles, spin parameters, shapes and triaxialities. Only halo masses and concentrations show a substantial increase, which can, however, be attributed to the change in the cosmological parameters.","E. Carlesi and A. Knebe and G. Yepes and S. Gottlöber and J. B. Jiménez and A. L. Maroto",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"ART2: coupling Lyα line and multi-wavelength continuum radiative transfer","Narrow-band Lyα line and broad-band continuum have played important roles in the discovery of high-redshift galaxies in recent years. Hence, it is crucial to study the radiative transfer of both Lyα and continuum photons in the context of galaxy formation and evolution in order to understand the nature of distant galaxies. Here, we present a three-dimensional Monte Carlo radiative transfer code, All-wavelength Radiative Transfer with Adaptive Refinement Tree (ART2), which couples Lyα line and multi-wavelength continuum, for the study of panchromatic properties of galaxies and interstellar medium. This code is based on the original version of Li et al., and features three essential modules: continuum emission from X-ray to radio, Lyα emission from both recombination and collisional excitation, and ionization of neutral hydrogen. The coupling of these three modules, together with an adaptive refinement grid, enables a self-consistent and accurate calculation of the Lyα properties, which depend strongly on the UV continuum, ionization structure and dust content of the object. Moreover, it efficiently produces multi-wavelength properties, such as the spectral energy distribution and images, for direct comparison with multi-band observations. As an example, we apply ART2to a cosmological simulation that includes both star formation and black hole growth, and study in detail a sample of massive galaxies at redshifts z = 3.1–10.2. We find that these galaxies are Lyα emitters (LAEs), whose Lyα emission traces the dense gas region, and that their Lyα lines show a shape characteristic of gas inflow. Furthermore, the Lyα properties, including photon escape fraction, emergent luminosity and equivalent width, change with time and environment. Our results suggest that LAEs evolve with redshift, and that early LAEs such as the most distant one detected at z ∼ 8.6 may be dwarf galaxies with a high star formation rate fuelled by infall of cold gas, and a low Lyα escape fraction.","H. Yajima and Y. Li and Q. Zhu and T. Abel",2012,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Production-Run Software Failure Diagnosis via Adaptive Communication Tracking","Software failure diagnosis techniques work either by sampling some events at production-run time or by using some bug detection algorithms. Some of the techniques require the failure to be reproduced multiple times. The ones that do not require such, are not adaptive enough when the execution platform, environment or code changes. We propose ACT, a diagnosis technique for production-run failures, that uses the machine intelligence of neural hardware. ACT learns some invariants (e.g., data communication invariants) on-the-fly using the neural hardware and records any potential violation of them. Since ACT can learn invariants on-the-fly, it can adapt to any change in execution setting or code. Since it records only the potentially violated invariants, the postprocessing phase can pinpoint the root cause fairly accurately without requiring to observe the failure again. ACT works seamlessly for many sequential and concurrency bugs. The paper provides a detailed design and implementation of ACT in a typical multiprocessor system. It uses a three stage pipeline for partially configurable one hidden layer neural networks. We have evaluated ACT on a variety of programs from popular benchmarks as well as open source programs. ACT diagnoses failures caused by 16 bugs from these programs with accurate ranking. Compared to existing learning and sampling based approaches, ACT has better diagnostic ability. For the default configuration, ACT has an average execution overhead of 8.2%.","M. M. U. Alam and A. Muzahid",2016,"[""IEEE"",""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11, CR12"
"CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization","The problem of localization on a geo-referenced aerial/satellite map given a query ground view image remains challenging due to the drastic change in viewpoint that causes traditional image descriptors based matching to fail. We leverage on the recent success of deep learning to propose the CVM-Net for the cross-view image-based ground-to-aerial geo-localization task. Specifically, our network is based on the Siamese architecture to do metric learning for the matching task. We first use the fully convolutional layers to extract local image features, which are then encoded into global image descriptors using the powerful NetVLAD. As part of the training procedure, we also introduce a simple yet effective weighted soft margin ranking loss function that not only speeds up the training convergence but also improves the final matching accuracy. Experimental results show that our proposed network significantly outperforms the state-of-the-art approaches on two existing benchmarking datasets. Our code and models are publicly available on the project website1.","S. Hu and M. Feng and R. M. H. Nguyen and G. H. Lee",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"Architectural Metamodel for Requirements of Images Accessibility in Online Editors","In this study, the authors envision a solution for online editors with accessibility considerations for images. To do this, an architectural metamodel with three levels is proposed: environment, guidelines and information system components. A set of 20 requirements was identified. This set was used to assess the accessibility level of a sample of 12 online platforms. The sample included different types of online platforms: massive open online courses, learning management systems, content management systems and social networking services. After analyzing the results, the authors found that in terms of management of images, the most accessible platforms were Moodle, Sakai, ATutor and MiriadaX. In addition, an insert/edit image interface with accessibility features was prototyped. The validation of this interface showed compliance with the Authoring Tool Accessibility Guidelines (ATAG) 2.0 and the HTML code generated complied with the Web Content Accessibility Guidelines (WCAG) 2.1.","J. Villarroel-Ramos and S. Sanchez-Gordon and S. Luján-Mora",2018,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"A Fast GPU Point-cloud Registration Algorithm","The purpose of point cloud registration is to find a 3D rigid body transformation so that the 3D coordinates of the point cloud at different angles can be correctly matched. With the current advances of high definition (HD) 3D cameras, several applications have emerged utilizing stereoscopic cameras. For real time objects tracking, fast point cloud registration calculations are required. In the current study we have considered two methods: a standard Singular Value Decomposition (SVD) and a truncated SVD (TSVD) point cloud registration algorithm. The registration process can be summarized in the following steps; the centroids of the chosen point datasets are first found, then they both are aligned to the origin, and then the optimal rotation and translation are determined based on the SVD or on the TSVD technique. Our strategy was firstly to identify the major computational bottlenecks of our code, and secondly parallelize them on the GPU accordingly. Performance tests were conducted on three GPU cards in comparison to a serial version of the algorithm executed on a CPU. Performance comparisons are also conducted between the parallel SVD and the parallel TSVD in order to test the computational efficiency of them on GPU cards. The studies indicated that there is no computational benefit from the parallization of the simple SVD on GPU. On the contrary, there is computational advantage of the parallel TSVD, but it varies with the GPU architecture. Speedup factors were recorded for every registration steps for all GPU cards. The step 2 of registration process was the most computational expensive task for the algorithm, and when it was parallelized on K40m card gave a maximum speed up of ~100 for the maximum number of pixels, while for other resolution sizes the performance of K40m decreased dramatically. The GTX1080Ti card achieved the highest speed up of ~150 for block 2 calculations, for 8K resolution. In overall, for the full registration process GTX1080Ti indicated a linear increase of speedup factors versus the number of pixels, fact that renders it is the most suitable GPU card with respect to the other GPU cards used for the specific application.","M. M. Rahman and P. Galanakou and G. Kalantzis",2018,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"RHINO Cluster Control and management system","The objective of this paper is to present the RHINO ARM API Cluster Control System (RAACMS) that will enable a user to access and control networked Reconfigurable Hardware Interface for computing and radio (RHINO) platforms. The framework is designed to run on a reconfigurable platforms consisting of FPGA and an ARM processor connected in a cluster. This system is built around a client-server design, and includes an API on the ARM and control PC, that enables users to execute their code on the control PC and control and change variables on a cluster of RHINO platforms. This paper will present the design and implementation of the RAACMS on the cluster of RHINOs at the University of Cape Town. Tests are performed on a prototype version of the framework. The conclusions discuss uses of the systems, together with further plans for improving the framework.","V. Chiriseri and S. Winberg and S. Rajan",2013,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Real-time implementation of DVFS enhanced LEON3 MPSoC on FPGA","In the field of embedded system there exists a trade off between power/performance optimization, hence many heuristics and techniques were presented at various development levels such as hardware software co-design, schedulers and optimal code compilation. This paper presents an enhanced version of LEON3 architecture which includes support for run-time management of supply voltage and processor operating frequency. This enhancement can be useful to implement various DVFS driving algorithms in LEON3 architecture aiming to leverage power consumption and throughput. Frequency scaling on the fly is based on dynamically reconfigurable clock synthesis feature available in Xilinx FPGA Virtex-4 or higher. The implementation of DVFS in LEON3 architecture is driven and controlled by general-purpose I/O port attached to advanced peripheral bus (APB) to change processor frequency on the fly during the execution of application programs. The work is implemented as a prototype on a Xilinx FPGA platform and incurs very small hardware overheads.","Z. Najam and M. Y. Qadri and S. Najam",2016,"[""IEEE"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Most common fixes students use to improve the correctness of their programs","Teach students how to program is the main goal of most introductory CS courses. In fact, programming is one of the basic skills a professional in CS should have. However, there are many difficulties students face when they are learning how to program and, consequently, it is common introductory programming courses have high dropout rates. The purpose of this paper is to identify and discuss the most common fixes students use to improve the correctness of their programs. The findings can be useful to help students to produce more correct programs and highlight issues about possible difficulties they are having. To do so, we used the BLACKBOX data collection, which stores the actions of the BLUEJ programming environment users. The main idea was to observe the modifications students did in their source codes that made a failed JUNIT test case become succeeded. The results suggest the majority of fixes students use in their source codes are related either to the change of expressions or to the restructuring of code, reflecting difficulties in logic and problem solving among students.","D. M. de Souza and M. Kölling and E. F. Barbosa",2017,"[""IEEE"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Wearable tech for halloween - The gemma MO's embedded python lets you change your code on the fly [Resources_Tools]","Halloween is approaching, and with it a global parade of costumes. So I thought this would be the perfect time to try out a new wearable microcontroller from Adafruit Industries: the Gemma M0. Adafruit has been putting out wearable microcontrollers for several years. These differ from conventional controllers, such as the Arduino Uno, in that the wearables are typically more compact and use pads with large through holes for input and output, instead of pins. These holes make it easy to sew boards to fabric or tie conductive thread to the pads. What makes the Gemma M0 particularly interesting is that it runs CircuitPython, Adafruit's modified version of the Python language designed for embedded devices. (At this point, I should note that Limor Fried, the founder of Adafruit, is a member of IEEE Spectrum's editorial advisory board, but she played no role in the origination of this article.).","S. Cass",2017,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"Table of contents","The following topics are dealt with: power-aware data structure; memory allocation techniques; EPS-based motion recognition systems; secure distance bounding protocol; TH-UWB; cooperative spectrum sensing scheme; femtocell network; video quality; unequal loss protection; Wi-Fi based broadcasting system; example-based retrieval system; human motion data; astronaut virtual training system; layout familiarization training; stereo image correction; 3D optical microscope; UHF RFID; BLE; stereo-based tag association; medical image segmentation; sensitive adaptive thresholding; interactive event recognition; semantic video understanding; Bgslibrary algorithms; traffic surveillance video; CUDA-based acceleration techniques; image filtering; image-based ship detection; AR navigation; augmented reality; vehicle information; head-up display: LiDAR data; classifier performance; people detection; wavelet transform; max-min energy-efficiency optimization; wireless powered communication network; harvest-then-transmit protocol; ARM64bit Server; WeChat text messages service flow traffic classification; machine learning technique; load balancing; WSN; novel Markov decision process based routing algorithm; repulsion-propulsion firefly algorithm; sentence based mathematical problem solving approach; ontology modeling; sentiment analysis; HARN algorithm; real-time road surface condition determination algorithm; automatic weather system; kinematic constraint method; human gesture recognition; weighted dynamic time warping; IT demand governance; business goal structuring notation; software requirement specification; AOP-based approach;decision support system; proactive flood control; context-aware user interface field classification; common vocabulary set; maritime equipment; e-navigation services; genetic algorithm; strategic information systems planning; speech enhancement; ES information; phase-error based filters; holistic service orchestration; distributed micro data center; hierarchical cluster network; wellness sports industry; secure agent based architecture; resource allocation; cloud computing; distributed multi-platform context-aware user interface; parallel prime number labeling; XML data; MapReduce; metadata extension; data presentations; aspect-oriented user interfaces design integration; Angular 2 framework; energy impact; Web user interface technology; mobile devices; JIT compilation-based unified SQL query optimization system; partial materialization; data integration; SQL-on-Hadoop engines; z-transform based encryption algorithm; FARIS; fast and memory-efficient URL filter; domain specific machine; synchronized blind audio watermarking; multilevel DWT; windowed vector modulation; packet length covert channel capacity estimation; flexible authentication protocol; WBAN; SMS-based mobile botnet detection module; full-duplex jamming attack; active eavesdropping; Big Data security analysis; complex security requirements patterns; SSH attacks; SSL/TLS nonintrusive proxy; JSON data; neural stegoclassifier; polymorphic malware detection; linguistic based steganography; lexical substitution; syntactical transformation; holistic-based feature extraction; error correcting code biometric template protection technique; network based IMSI catcher detection; Internet of Things environment; light-weight API-call safety checking; automotive control software; SmartDriver; project management software; model-based testing; exploratory testing; automated ECG beat classification system and convolutional neural networks.","",2016,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"[Title page]","The following topics are dealt with: robust beamforming; tax fraud detection algorithm; multipath effect; intelligent stock trading systems; region based histogram analysis strategy; automatic drug-drug interaction extraction; dynamic feature weighting; bias protein primary sequence; protein structure prediction; edge histogram analysis; sentiment analysis; community question answering systems; user-friendly visual secret sharing; multiple-query image retrieval; singular Lorenz measure method; KNN-scatter search optimization algorithm; VoIP network; quaternion-based salient region detection; wavelet image denoising based spatial noise estimation; subspace-based speech enhancement; high speed vehicle application; INS-GPS navigation system; color image segmentation; virtual machine placement; traffic density estimation; interference binary channel; QoS-aware resource allocation; LTE networks; Petri net based transformation method; objectionable image recognition; convolutional neural nets; semisupervised intrusion detection; online Laplacian twin support vector machine; secure echo steganography; compressed sensing DOA estimation; DV-Hop localization algorithm; wireless sensor networks; salient object detection; global contrast graph; speaker-independent isolated Persian digit recognition; enhanced vector quantization algorithm; dictionary learning; parallel secure turbo code; and traffic sign recognition.","",2015,"[""IEEE""]","Rejeitado: CR12","Rejeitado: CR12"
"[Copyright notice]","Copyright (c) 2013 by The Institute of Electrical and Electronics Engineers, Inc. All rights reserved. Copyright and Reprint Permissions: Abstracting is permitted with credit to the source. Libraries may photocopy beyond the limits of US copyright law, for private use of patrons, those articles in this volume that carry a code at the bottom of the first page, provided that the per-copy fee indicated in the code is paid through the Copyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923. Other copying, reprint, or republication requests should be addressed to: IEEE Copyrights Manager, IEEE Service Center, 445 Hoes Lane, P.O. Box 133, Piscataway, NJ 08855-1331. The papers in this book comprise the proceedings of the meeting mentioned on the cover and title page. They reflect the authors' opinions and, in the interests of timely dissemination, are published as presented and without change. Their inclusion in this publication does not necessarily constitute endorsement by the editors, the IEEE Computer Society, or the Institute of Electrical and Electronics Engineers, Inc. IEEE Computer Society Order Number E5036. BMS Part Number CFP13BAH-ART. ISBN 978-0-7695-5036-7.","",2013,"[""IEEE""]","Rejeitado: CR9","Rejeitado: CR9"
"How Often Does a Source Code Unit Change Within a Release Window?","To form a training set for a source-code change prediction model, e.g., using the association rule mining or machine learning techniques, commits from the source code history are needed. The traceability between releases and commits would facilitate a systematic choice of history in units of the project evolution scale (i.e., commits that constitute a software release). For example, the major release 25.0 in Chrome is mapped to the earliest revision 157687 and latest revision 165096 in the trunk. Using this traceability, an empirical study is reported on the frequency distribution of file changes for different release windows. In Chrome, the majority (50%) of the committed files change only once between a pair of consecutive releases. This trend is reversed after expanding the window size to at least 10. That is, the majority (50%) of the files change multiple times when commits constituting 10 or greater releases are considered. These results suggest that a training set of at least 10 releases is needed to provide a prediction coverage for majority of the files.","Shobe, Joseph F. and Karim, Md Yasser and Kagdi, Huzefa",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Bug Localization by Learning to Rank and Represent Bug Inducing Changes","In software development, bug localization is the process finding portions of source code associated to a submitted bug report. This task has been modeled as an information retrieval task at source code file, where the report is the query. In this work, we propose a model that, instead of working at file level, learns feature representations from source changes extracted from the project history at both syntactic and code change dependency perspectives to support bug localization.

To that end, we structured an end-to-end architecture able to integrate feature learning and ranking between sets of bug reports and source code changes.

We evaluated our model against the state of the art of bug localization on several real world software projects obtaining competitive results in both intra-project and cross-project settings. Besides the positive results in terms of model accuracy, as we are giving the developer not only the location of the bug associated to the report, but also the change that introduced, we believe this could give a broader context for supporting fixing tasks.","Loyola, Pablo and Gajananan, Kugamoorthy and Satoh, Fumiko",2018,"[""ACM"",""Engineering Village""]","Aceito: CA0, CA3","Aceito: CA3"
"Hierarchical Categorization of Edit Operations for Separately Committing Large Refactoring Results","In software configuration management using a version control system, developers have to follow the commit policy of the project. However, preparing changes according to the policy are sometimes cumbersome and time-consuming, in particular when applying large refactoring consisting of multiple primitive refactoring instances. In this paper, we propose a technique for re-organizing changes by recording editing operations of source code. Editing operations including refactoring operations are hierarchically managed based on their types provided by an integrated development environment. Using the obtained hierarchy, developers can easily configure the granularity of changes and obtain the resulting changes based on the configured granularity. We confirmed the feasibility of the technique by applying it to the recorded changes in a large refactoring process.","Matsuda, Jumpei and Hayashi, Shinpei and Saeki, Motoshi",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Modulo-X: A Simple Transformation Language for HPC Programs","While creating a parallel version of a sequential program, some code sections may be duplicated in the translated version, which can hinder the evolution of the newly created program. This can be prevented if parallel sections of a program can be separated from the sequential sections. In this paper, we introduce a transformation language, called Modulo-X, which can make the parallel to sequential conversion task easier without modifying the original source code. A case study is included to show how sequential code can be converted to two leading parallel programming models (i.e., OpenMP and MPI) using Modulo-X.","Jacob, Ferosh and Gray, Jeff and Bangalore, Purushotham",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Learning from Source Code History to Identify Performance Failures","Source code changes may inadvertently introduce performance regressions. Benchmarking each software version is traditionally employed to identify performance regressions. Although effective, this exhaustive approach is hard to carry out in practice. This paper contrasts source code changes against performance variations. By analyzing 1,288 software versions from 17 open source projects, we identified 10 source code changes leading to a performance variation (improvement or regression). We have produced a cost model to infer whether a software commit introduces a performance variation by analyzing the source code and sampling the execution of a few versions. By profiling the execution of only 17% of the versions, our model is able to identify 83% of the performance regressions greater than 5% and 100% of the regressions greater than 50%.","Sandoval Alcocer, Juan Pablo and Bergel, Alexandre and Valente, Marco Tulio",2016,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Impact Analysis of Change Requests on Source Code Based on Interaction and Commit Histories","The paper presents an approach to perform impact analysis (IA) of an incoming change request on source code. The approach is based on a combination of interaction (e.g., Mylyn) and commit (e.g., CVS) histories. The source code entities (i.e., files and methods) that were interacted or changed in the resolution of past change requests (e.g., bug fixes) were used. Information retrieval, machine learning, and lightweight source code analysis techniques were employed to form a corpus from these source code entities. Additionally, the corpus was augmented with the textual descriptions of the previously resolved change requests and their associated commit messages. Given a textual description of a change request, this corpus is queried to obtain a ranked list of relevant source code entities that are most likely change prone. Such an approach that combines information from interactions and commits for IA at the change request level was not previously investigated. Furthermore, the approach requires only the entities that were interacted and/or committed in the past, which differs from the previous solutions that require indexing of a complete snapshot (e.g., a release).

An empirical study on 3272 interactions and 5093 commits from Mylyn, an open source task management tool, was conducted. The results show that the combined approach outperforms an individual approach based on commits. Moreover, it also outperformed an approach based on indexing a single, complete snapshot of a software system.","Zanjani, Motahareh Bahrami and Swartzendruber, George and Kagdi, Huzefa",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Detecting Source Code Changes to Maintain the Consistence of Behavioral Model","It is well-known that as software system evolves, the source code tends to deviate from its design model so that maintaining their consistence is challenging. Our objective is to detect code changes that influence designed program behaviour which are referred as design level changes and update the behavioural model timely and automatically to maintain consistence. We propose an approach that filters out low-level source code changes that do not influence program behaviour, abstracts code changes into updating operations for behavioral model, and automates the integration and update of activity diagrams to maintain consistence. We've recognised that it is not uncommon for developers to introduce quick and dirty implementation that unnecessarily increases program complexity or introduces suboptimal behaviour changes. So while merging code changes into behaviour model, our approach also calculates cyclometric complexity variation before and after the process so that developers can be alerted of significant and/or detrimental changes. Our tool allows the user to approve the change in code before merging and updating the model.","Li, Yuankui and Wang, Linzhang and Li, Xuandong and Cai, Yuanfang",2012,"[""ACM""]","Aceito: CA0, CA3","Aceito: CA3"
"Cookbook: In Situ Code Completion Using Edit Recipes Learned from Examples","Existing code completion engines leverage only pre-defined templates or match a set of user-defined APIs to complete the rest of changes. We propose a new code completion technique, called Cookbook, where developers can define custom edit recipes—a reusable template of complex edit operations—by specifying change examples. It generates an abstract edit recipe that describes the most specific generalization of the demonstrated example program transformations. Given a library of edit recipes, it matches a developer’s edit stream to recommend a suitable recipe that is capable of filling out the rest of change customized to the target. We evaluate Cookbook using 68 systematic changed methods drawn from the version history of Eclipse SWT. Cookbook is able to narrow down to the most suitable recipe in 75% of the cases. It takes 120 milliseconds to find the correct suitable recipe on average, and the edits produced by the selected recipe are on average 82% similar to developer’s hand edit. This shows Cookbook’s potential to speed up manual editing and to minimize developer’s errors. Our demo video is available at https://www.youtube.com/watch?v=y4BNc8FT4RU.","Jacobellis, John and Meng, Na and Kim, Miryung",2014,"[""ACM"",""Engineering Village""]","Aceito: CA3","Aceito: CA3"
"Frapp\'{E}: Querying the Linux Kernel Dependency Graph","Frappé is a developer tool for querying and visualizing the dependencies of large C/C++ software systems to the order of 10s of millions of lines of code in size. It supports developers with a range of code comprehension queries such as Does function X or something it calls write to global variable Y? and How much code could be affected if I change this macro? Results are overlaid on a visualization of the dependency graph data based on a cartographic map metaphor.

In this paper, we give a brief overview of Frappé and describe our experiences implementing it on top of the Neo4j graph database. We detail the graph model used by Frappé and outline its key use cases using representative queries and their runtimes with the dependency graph data of the Unbreakable Enterprise Kernel.

Finally, we discuss some of the open challenges in supporting source code queries across single and multiple versions of an evolving codebase with current property graph database technologies: performance, efficient storage, and the expressivity of the graph querying language given a graph model.","Hawes, Nathan and Barham, Ben and Cifuentes, Cristina",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Is It Dangerous to Use Version Control Histories to Study Source Code Evolution?","Researchers use file-based Version Control System (VCS) as the primary source of code evolution data. VCSs are widely used by developers, thus, researchers get easy access to historical data of many projects. Although it is convenient, research based on VCS data is incomplete and imprecise. Moreover, answering questions that correlate code changes with other activities (e.g., test runs, refactoring) is impossible.

Our tool, CodingTracker, non-intrusively records fine-grained and diverse data during code development. CodingTracker collected data from 24 developers: 1,652 hours of development, 23,002 committed files, and 314,085 testcase runs.

This allows us to answer: How much code evolution data is not stored in VCS? How much do developers intersperse refactorings and edits in the same commit? How frequently do developers fix failing tests by changing the test itself? How many changes are committed to VCS without being tested? What is the temporal and spacial locality of changes?","Negara, Stas and Vakilian, Mohsen and Chen, Nicholas and Johnson, Ralph E. and Dig, Danny",2012,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Learning Programming Languages As Shortcuts to Natural Language Token Replacements","The basic knowledge of computer programming is generally considered a valuable skill for educated citizens outside computer science and engineering professions. However, learning programming can be a challenging task for beginners of all ages especially outside of formal CS education. This paper presents a novel source code editing method that assists novice users understand the logic and syntax of the computer code they type. The method is based on the concept of text replacements that interactively provide the learners with declarative knowledge and help them transform it to procedural knowledge, which has been shown to be more robust against decay. An active tokenization algorithm splits the typed code into tokens as they are typed and replaces them with a pre-aligned translation in a human natural language. The feasibility of the proposed method is demonstrated in seven structurally different natural languages (English, Chinese, German, Greek, Italian, Spanish, and Turkish) using examples of computer code in ECMAScript (JavaScript).","Barmpoutis, Angelos",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Debsources: Live and Historical Views on Macro-level Software Evolution","Context. Software evolution has been an active field of research in recent years, but studies on macro-level software evolution---i.e., on the evolution of large software collections over many years---are scarce, despite the increasing popularity of intermediate vendors as a way to deliver software to final users.

Goal. We want to ease the study of both day-by-day and long-term Free and Open Source Software (FOSS) evolution trends at the macro-level, focusing on the Debian distribution as a proxy of relevant FOSS projects.

Method. We have built Debsources, a software platform to gather, search, and publish on the Web all the source code of Debian and measures about it. We have set up a public Debsources instance at http://sources.debian.net, integrated it into the Debian infrastructure to receive live updates of new package releases, and written plugins to compute popular source code metrics. We have injected all current and historical Debian releases into it.

Results. The obtained dataset and Web portal provide both long term-views over the past 20 years of FOSS evolution and live insights on what is happening at sub-day granularity. By writing simple plugins (~100 lines of Python each) and adding them to our Debsources instance we have been able to easily replicate and extend past empirical analyses on metrics as diverse as lines of code, number of packages, and rate of change---and make them perennial. We have obtained slightly different results than our reference study, but confirmed the general trends and updated them in light of 7 extra years of evolution history.

Conclusions. Debsources is a flexible platform to monitor large FOSS collections over long periods of time. Its main instance and dataset are valuable resources for scholars interested in macro-level software evolution.","Caneill, Matthieu and Zacchiroli, Stefano",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Using Structured Text Source Code Metrics and Artificial Neural Networks to Predict Change Proneness at Code Tab and Program Organization Level","Structured Text (ST) is a high-level text-based programming language which is part of the IEC 61131-3 standard. ST is widely used in the domain of industrial automation engineering to create Programmable Logic Controller (PLC) programs. ST is a Domain Specific Language (DSL) which is specialized to the Automation Engineering (AE) application domain. ST has specialized features and programming constructs which are different than general purpose programming languages. We define, develop a tool and compute 10 source code metrics and their correlation with each-other at the Code Tab (CT) and Program Organization Unit (POU) level for two real-world industrial projects at a leading automation engineering company. We study the correlation between the 10 ST source code metrics and their relationship with change proneness at the CT and POU level by creating experimental dataset consisting of different versions of the system. We build predictive models using Artificial Neural Network (ANN) based techniques to predict change proneness of the software. We conduct a series of experiments using various training algorithms and measure the performance of our approach using accuracy and F-measure metrics. We also apply two feature selection techniques to select optimal features aiming to improve the overall accuracy of the classifier.","Kumar, Lov and Sureka, Ashish",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"A Dictionary to Translate Change Tasks to Source Code","
At the beginning of a change task, software developers spend a substantial amount of their time searching and navigating to locate relevant parts in the source code. Current approaches to support developers in this initial code search predominantly use information retrieval techniques that leverage the similarity between task descriptions and the identifiers of code elements to recommend relevant elements. However, the vocabulary or language used in source code often differs from the one used for describing change tasks, especially since the people developing the code are not the same as the ones reporting bugs or defining new features to be implemented. In our work, we investigate the creation of a dictionary that maps the different vocabularies using information from change sets and interaction histories stored with previously completed tasks. In an empirical analysis on four open source projects, our approach substantially improved upon the results of traditional information retrieval techniques for recommending relevant code elements.","Kevic, Katja and Fritz, Thomas",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Fine-grained and Accurate Source Code Differencing","At the heart of software evolution is a sequence of edit actions, called an edit script, made to a source code file. Since software systems are stored version by version, the edit script has to be computed from these versions, which is known as a complex task. Existing approaches usually compute edit scripts at the text granularity with only add line and delete line actions. However, inferring syntactic changes from such an edit script is hard. Since moving code is a frequent action performed when editing code, it should also be taken into account. In this paper, we tackle these issues by introducing an algorithm computing edit scripts at the abstract syntax tree granularity including move actions. Our objective is to compute edit scripts that are short and close to the original developer intent. Our algorithm is implemented in a freely-available and extensible tool that has been intensively validated.","Falleri, Jean-R{\'e}my and Morandat, Flor{\'e}al and Blanc, Xavier and Martinez, Matias and Monperrus, Martin",2014,"[""ACM"",""Engineering Village""]","Aceito: CA1, CA3","Aceito: CA1, CA3"
"A Methodological Review Based Version Control System with Evolutionary Research for Software Processes","The constant add up of new features and bug fixes often make invariable changes and refinements inevitable in the software development at regular intervals. The software should enable the end-user to decipher the critical version information about the minutest detail in every software artifact and if it does not, no software can be called as complete or nor is it user-friendly. When developers check in the software files they have to put in the version field manually as a part of header information inside the file. It would be difficult to identify vulnerabilities in the software when the files lack embedded version information which makes it harder for the end users to accurately identify the running version of the software. How can version control system (VCS) help in optimizing the bug traceability time and bug fixing potential? Version control system (VCS) is a software application that helps in collaborative software development of software projects. In order to support this instance, we should analyze which version control system is to be considered and whether to use central repository version control system or local repository version control system. Various functional, non functional evaluation criteria and fundamental requirements like installation, configuration, learning curve, performance, commits, branching, merging, and tagging have been considered for a through comparison of many version control systems. In this paper, we review and analyze the currently used VCS that involved in a view of a collaborative software development for software evolution. And an attempt has also been made to identify the version information in every deliverable file or retain it with the source. We demonstrate and perform survey on current versioning systems like Concurrent Version Control System (CVS), Subversion (SVN), Team Foundation Server (TFS), Git, ACME, Visual Source Safe(VSS), Mercurial and Clear Case, etc. that offers opportunities to test produced results and validate.","Rao, N. Rama and Sekharaiah, K. Chandra",2016,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR4"
"A Systematic Literature Review of Traceability Approaches Between Software Architecture and Source Code",,"Javed, Muhammad Atif and Zdun, Uwe",2014,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"Authoring Multi-stage Code Examples with Editable Code Histories","Multi-stage code examples present multiple versions of a program where each stage increases the overall complexity of the code. In order to acquire strategies of program construction using a new language or API, programmers consult multi-stage code examples in books, tutorials and online videos. Authoring multi-stage code examples is currently a tedious process, as it involves keeping several stages of code synchronized in the face of edits and error corrections. We document these difficulties with a formative study examining how programmers author multi-stage code examples. We then present an IDE extension that helps authors create multi-stage code examples by propagating changes (insertions, deletions and modifications) to multiple saved versions of their code. Our system adapts revision control algorithms to the specific task of evolving example code. An informal evaluation finds that taking snapshots of a program as it is being developed and editing these snapshots in hindsight help users in creating multi-stage code examples.","Ginosar, Shiry and De Pombo, Luis Fernando and Agrawala, Maneesh and Hartmann, Bjorn",2013,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Tracking Line Changes in Source Code Repositories","Previous research determined that the analysis of file changes in software repositories is useful for maintenance activities, like defect prediction. Changes rarely modify the entire file contents, but are usually localized in specific code regions.","Fontana, Francesca Arcelli and Zanoni, Marco",2014,"[""ACM""]","Aceito: CA1","Aceito: CA1"
"Chronicler: Interactive Exploration of Source Code History","Exploring source code history is an important task for software maintenance. Traditionally, source code history is navigated on the granularity of individual files. This is not fine-grained enough to support users in exploring the evolution of individual code elements. We suggest to consider the history of individual elements within the tree structure inherent to source code. A history graph created from these trees then enables new ways to explore events of interest defined by structural changes in the source code. We present Tree Flow, a visualization of these structural changes designed to enable users to choose the appropriate level of detail for the task at hand. In a user study, we show that both Chronicler and the history aware timeline, two prototype systems combining history graph navigation with a traditional source code view, outperform the more traditional history navigation on a file basis and users strongly prefer Chronicler for the exploration of source code.","Wittenhagen, Moritz and Cherek, Christian and Borchers, Jan",2016,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Incremental Origin Analysis of Source Code Files","The history of software systems tracked by version control systems is often incomplete because many file movements are not recorded. However, static code analyses that mine the file history, such as change frequency or code churn, produce precise results only if the complete history of a source code file is available. In this paper, we show that up to 38.9% of the files in open source systems have an incomplete history, and we propose an incremental, commit-based approach to reconstruct the history based on clone information and name similarity. With this approach, the history of a file can be reconstructed across repository boundaries and thus provides accurate information for any source code analysis. We evaluate the approach in terms of correctness, completeness, performance, and relevance with a case study among seven open source systems and a developer survey.","Steidl, Daniela and Hummel, Benjamin and Juergens, Elmar",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Clustering Source Code Files to Predict Change Propagation During Software Maintenance","This paper addresses the question that is frequently considered by software developers performing maintenance tasks on large systems: ""If I make a change in this file, are there other files that need to change too?"" If a development tool could automatically answer this question, then time and money could be saved during software maintenance. The proposed solution follows trends from past research results in using data mining techniques and information extracted from the CVS change repository. We define a distance measure using both the revision history of files and text-based information, cluster change sets of files, and then calculate a membership value of each file to each cluster to create groupings of files that are likely to be changed together in the future. Our approach predicts files that may need to be modified based on these clusters, and we evaluate these predictions on portions of the open-source Eclipse project.","Bailey, Megan and Lin, King-Ip and Sherrell, Linda",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Do You Remember This Source Code?","Being familiar with the source code of a program comprises knowledge about its purpose, structure, and details. Consequently, familiarity is an important factor in many contexts of software development, especially for maintenance and program comprehension. As a result, familiarity is considered to some extent in many different approaches, for example, to model costs or to identify experts. Still, all approaches we are aware of require a manual assessment of familiarity and empirical analyses of forgetting in software development are missing. In this paper, we address this issue with an empirical study that we conducted with 60 open-source developers. We used a survey to receive information on the developers' familiarity and analyze the responses based on data we extract from their used version control systems. The results show that forgetting is an important factor when considering familiarity and program comprehension of developers. We find that a forgetting curve is partly applicable for software development, investigate three factors - the number of edits, ratio of owned code, and tracking behavior - that can impact familiarity with code, and derive a general memory strength for our participants. Our findings can be used to scope approaches that have to consider familiarity and they provide insights into forgetting in the context of software development.","Kr\""{u}ger, Jacob and Wiemann, Jens and Fenske, Wolfram and Saake, Gunter and Leich, Thomas",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Empirical Analysis on Effectiveness of Source Code Metrics for Predicting Change-Proneness","Change-prone classes or modules are defined as software components in the source code which are likely to change in the future. Change-proneness prediction are useful to the maintenance team as they can optimize and focus their testing resources on the modules which have a higher likelihood of change. The quality of change-proneness prediction model can be best assessed by the use of software metrics that are considered to design the prediction model. In this work, 62 software metrics with four metrics dimensions, including 7 size metrics, 18 cohesion metrics, 20 coupling metrics, and 17 inheritance metrics are considered to develop a model for predicting change-proneness modules. Since the performance of the change-proneness model depends on the source code metrics, they are used as input of the change-proneness model. We also considered five different types of feature selection techniques to remove irrelevant feature and select best set of features.

The effectiveness of these set of source code metrics are evaluated using eight different machine learning algorithms and two ensemble techniques. Experimental results demonstrates that the model developed by considering selected set of source code metrics by feature selection technique as input achieves better results as compared to considering all source code metrics. The experimental results also ravel that the change-proneness model developed by using coupling metrics achieved better performance as compared other dimension metrics such as size metrics, cohesion metrics, and inheritance metrics.","Kumar, Lov and Rath, Santanu Kumar and Sureka, Ashish",2017,"[""ACM"",""Engineering Village""]","Aceito: CA6, CA5, CA4","Aceito: CA5, CA4, CA6"
"MPAnalyzer: A Tool for Finding Unintended Inconsistencies in Program Source Code","Unintended inconsistencies are caused by missing a modification task that requires code changes on multiple locations in program source code. In order to identify such inconsistencies efficiently, we proposed a new technique. It firstly learns how code fragments were changed in the past modification tasks, and then, it identifies where inconsistencies exist at the latest version. In this paper, we focus on an aspect of the tool that we developed and shows a case study that we conducted with the tool. A video of the tool is available at http://youtu.be/a7_PVVZ4-vo.","Higo, Yoshiki and Kusumoto, Shinji",2014,"[""ACM""]","Aceito: CA0, CA3","Aceito: CA2"
"Transfer-Learning Methods in Programming Course Outcome Prediction","The computing education research literature contains a wide variety of methods that can be used to identify students who are either at risk of failing their studies or who could benefit from additional challenges. Many of these are based on machine-learning models that learn to make predictions based on previously observed data. However, in educational contexts, differences between courses set huge challenges for the generalizability of these methods. For example, traditional machine-learning methods assume identical distribution in all data—in our terms, traditional machine-learning methods assume that all teaching contexts are alike. In practice, data collected from different courses can be very different as a variety of factors may change, including grading, materials, teaching approach, and the students.

Transfer-learning methodologies have been created to address this challenge. They relax the strict assumption of identical distribution for training and test data. Some similarity between the contexts is still needed for efficient learning. In this work, we review the concept of transfer learning especially for the purpose of predicting the outcome of an introductory programming course and contrast the results with those from traditional machine-learning methods. The methods are evaluated using data collected in situ from two separate introductory programming courses.

We empirically show that transfer-learning methods are able to improve the predictions, especially in cases with limited amount of training data, for example, when making early predictions for a new context. The difference in predictive power is, however, rather subtle, and traditional machine-learning models can be sufficiently accurate assuming the contexts are closely related and the features describing the student activity are carefully chosen to be insensitive to the fine differences.","Lagus, Jarkko and Longi, Krista and Klami, Arto and Hellas, Arto",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Identification of Programmers from Typing Patterns","Being able to identify the user of a computer solely based on their typing patterns can lead to improvements in plagiarism detection, provide new opportunities for authentication, and enable novel guidance methods in tutoring systems. However, at the same time, if such identification is possible, new privacy and ethical concerns arise. In our work, we explore methods for identifying individuals from typing data captured by a programming environment as these individuals are learning to program. We compare the identification accuracy of automatically generated user profiles, ranging from the average amount of time that a user needs between keystrokes to the amount of time that it takes for the user to press specific pairs of keys, digraphs. We also explore the effect of data quantity and different acceptance thresholds on the identification accuracy, and analyze how the accuracy changes when identifying individuals across courses. Our results show that, while the identification accuracy varies depending on data quantity and the method, identification of users based on their programming data is possible. These results indicate that there is potential in using this method, for example, in identification of students taking exams, and that such data has privacy concerns that should be addressed.","Longi, Krista and Leinonen, Juho and Nygren, Henrik and Salmi, Joni and Klami, Arto and Vihavainen, Arto",2015,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"TemPerf: Temporal Correlation Between Performance Metrics and Source Code","Today's rapidly evolving software systems continuously introduce new changes that can potentially degrade performance. Large-scale load testing prior to deployment is supposed to avoid performance regressions in production. However, due to the large input space in parameterized load testing, not all performance regressions can be prevented in practice. To support developers in identifying the change sets that had an impact on performance, we present TemPerf, a tool that correlates performance regressions with change sets by exploiting temporal constraints. It is implemented as an Eclipse IDE plugin that allows developers to visualize performance developments over time and display temporally correlated change sets retrieved from version control and continuous integration platforms.","Cito, J\""{u}rgen and Mazlami, Genc and Leitner, Philipp",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Co-evolution of Infrastructure and Source Code: An Empirical Study",,"Jiang, Yujuan and Adams, Bram",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Are Deep Neural Networks the Best Choice for Modeling Source Code?","Current statistical language modeling techniques, including deep-learning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add & remove text, and mix & swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N-gram, as well as RNN, and LSTM deep-learning language models, and release all our source code for public use. Our evaluations suggest that carefully adapting N-gram models for source code can yield performance that surpasses even RNN and LSTM based deep-learning models.","Hellendoorn, Vincent J. and Devanbu, Premkumar",2017,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Automating Repetitive Code Changes Using Examples","While adding features, fixing bugs, or refactoring the code, developers may perform repetitive code edits. Although Integrated Development Environments (IDEs) automate some transformations such as renaming, many repetitive edits are performed manually, which is error-prone and time-consuming. To help developers to apply these edits, we propose a technique to perform repetitive edits using examples. The technique receives as input the source code before and after the developer edits some target locations of the change and produces as output the top-ranked program transformation that can be applied to edit the remaining target locations in the codebase. The technique uses a state-of-the-art program synthesis methodology and has three main components: a) a DSL for describing program transformations; b) synthesis algorithms to learn program transformations in this DSL; c) ranking algorithms to select the program transformation with the higher probability of performing the desired repetitive edit. In our preliminary evaluation, in a dataset of 59 repetitive edit cases taken from real C# source code repositories, the technique performed, in 83% of the cases, the intended transformation using only 2.8 examples.","Rolim, Reudismam",2016,"[""ACM""]","Aceito: CA0, CA3","Aceito: CA3"
"Lazy Stateless Incremental Evaluation Machinery for Attribute Grammars","Many computer programs work with data that changes over time. Computations done over such data usually are repeated completely after a change in the data. For complex computations such repetitive recomputation can become too inefficient. When these recomputations take place on data which has only changed slightly, it often is possible to reformulate the computation to an incremental version which reuses the result of the computation on previous data. Such a situation typically occurs in compilers and editors for structured data (like a program) where program analyses and transformations (for example error checking) are done while editing.

Although rewriting to incremental versions thus offers a solution to this problem, a manual rewrite of an already complex computation to its incremental counterpart is tedious, error prone, and inhibits further development of the original computation. We therefore intend to generate such incremental counterparts (semi)automatically by focusing on computations expressed using Attribute Grammars (AGs).

In this paper we do groundwork for this goal and develop machinery for incremental attribute grammar evaluation based on change propagation and pure functions. We use pretty printing with free variable annotation to explain our techniques. Furthermore, our techniques also expose rules of conduct for a programmer desiring incrementality: the automatic translation of code to an incremental version does not always directly result in efficiency improvements because code often is written in a style unsuitable for automatic incrementalization. We show some common cases in which (small) code changes facilitating incrementality are required. We evaluate the effectiveness of the overall approach using a simple benchmark for the example, and a more extensive benchmark based on constraint-based type inference implemented with AGs.","Bransen, Jeroen and Dijkstra, Atze and Swierstra, S. Doaitse",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Automated Patch Extraction via Syntax- and Semantics-aware Delta Debugging on Source Code Changes","Delta debugging (DD) is an approach to automating the debugging activities based on systematic testing. DD algorithms find the cause of a regression of a program by minimizing the changes applied between a working version and a faulty version of the program. However, it is still an open problem to minimize a huge set of changes while avoiding any invalid subsets that do not result in testable programs, especially in case that no software configuration management system is available. In this paper, we propose a rule-based approach to syntactic and semantic decomposition of changes into independent components to facilitate DD on source code changes, and hence to extract patches automatically. For analyzing changes, we make use of tree differencing on abstract syntax trees instead of common differencing on plain texts. We have developed an experimental implementation for Java programs and applied it to 194 bug fixes from Defects4J and 8 real-life regression bugs from 6 open source Java projects. Compared to a DD tool based on plain text differencing, it extracted patches whose size is reduced by 50% at the cost of 5% more test executions for the former dataset and by 73% at the cost of 40% more test executions for the latter, both on average.","Hashimoto, Masatomo and Mori, Akira and Izumida, Tomonori",2018,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Micro-Versioning Tool to Support Experimentation in Exploratory Programming","Experimentation plays an essential role in exploratory programming, and programmers apply version control operations when switching the part of the source code back to the past state during experimentation. However, these operations, which we refer to as micro-versioning, are not well supported in current programming environments. We first examined previous studies to clarify the requirements for a micro-versioning tool. We then developed a micro-versioning tool that displays visual cues representing possible micro-versioning operations in a textual code editor. Our tool includes a history model that generates meaningful candidates by combining a regional undo model and tree-structured undo model. The history model uses code executions as a delimiter to segment text edit operations into meaning groups. A user study involving programmers indicated that our tool satisfies the above-mentioned requirements and that it is useful for exploratory programming.","Mikami, Hiroaki and Sakamoto, Daisuke and Igarashi, Takeo",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Impact of Consecutive Changes on Later File Versions","By analyzing histories of program versions, many researches have shown that software quality is associated with history-related metrics, such as code-related metrics, commit-related metrics, developer-related metrics, process-related metrics, and organizational metrics etc. It has also been revealed that consecutive changes on commit-level are strongly associated with software defects. In this paper, we introduce two novel concepts of consecutive changes: CFC (chain of consecutive bug-fixing file versions) and CAC (chain of consecutive file versions where each pair of adjacent versions are submitted by different developers). And then several experiments are conducted to explore the correlation between consecutive changes and software quality by using three open-source projects from Github. Our main findings include: 1) CFCs and CACs widely exist in file version histories; 2) Consecutive changes have a negative and strong impact on the later file versions in a short term, especially when the length of consecutive change chain is 4 or 5.","Dai, Meixi and Shen, Beijun and Zhang, Tao and Zhao, Min",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Progress Report on Version Aware LibreOffice","In an earlier paper at DocEng 2013, we reported on our efforts to make LibreOffice Writer documents be ""version aware"". Version aware documents use a namespace protected preamble to include a complete version history within the saved document file, plus unique identifier attributes on the document content elements in order to support efficient differencing and merging of versions. A particular challenge in this effort has been to ensure that the unique identifiers on the elements would be preserved through a complete load-edit-save cycle. This is challenging because content element data passes through three representations in its lifetime. At load time, XML is read to create ImportContext objects, which are then used to generate internal data structures used during editing. At save time, the internal data structures are converted to ExportContext objects, from which XML is generated for the saved file. The internal data structures are drawn from a small forest of inheritance hierarchies, where each hierarchy has a slightly different construction-destruction protocol and thus each one requires a different solution to preserving the unique identifiers.

Working with a particular snapshot of the C++ implementation of LibreOffice Writer, we have reached a point where unique identifiers are preserved on nearly all content elements used in Writer documents. Unfortunately, there is no support for versioning of document style elements at this time. Support for version awareness has added about 3000 lines of code to a code base of slightly more than one million lines. The changes affect 128 files out of a total of 3354, organized in three large modules. We believe that these numbers show that adding full support for version awareness would have only a modest affect on the implementation of an office software suite. However, the fairly large number of files affected shows that version awareness resembles a non-functional requirement, since its support is not isolated in a small set of files.","Pandey, Meenu and Munson, Ethan V. and Thao, Cheng",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"CODECAST: An Innovative Technology to Facilitate Teaching and Learning Computer Programming in a C Language Online Course","This paper introduces the CODECAST tool: an in-browser C language interpreter, paired with an event and voice recorder and player that facilitates teaching and learning to program by synchronizing audio with source code edition, visualization, step by step execution and testing.","Sharrock, R{\'e}mi and Hamonic, Ella and Hiron, Mathias and Carlier, Sebastien",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"Compiling Math to Fast Code","Extracting optimal performance from modern computing platforms has become increasingly difficult over the last few years. The effect is particularly noticeable in computations that are of mathematical nature such as those needed in multimedia processing, communication, control, graphics, and scientific simulations: a straightforward implementation, e.g., in C, is often one or two orders of magnitude slower than the best possible code. The reason is in optimizations that are known to be difficult and often impossible for compilers: parallelization, vectorization, and locality optimizations.

On the other hand, many mathematical applications spend most of their runtime in well-defined mathematical kernels such as matrix computations, Fourier transforms, interpolation, coding, and others. Since these are likely to be needed for decades to come, it makes sense to build program generation systems for their automatic production. The input for the generator would be only the algorithm knowledge in a suitable representation and some information about the computing platform. The output of the generator is highly optimized, platform-tuned code. For new platforms, the code is regenerated; for new types of platforms, the generator is expanded rather than rewriting the actual kernel code.

With Spiral we have built such a system for the domain of linear transforms. In this talk we give a brief survey on the key techniques underlying Spiral: a domain specific mathematical language, rewriting systems for different forms of parallelization and to compute the so-called recursion step closure to improve locality in recursive code, and the use of machine learning to adapt code at installation time. Spiral-generated code has proven to be as good as, and sometimes faster, than any human-written code. As one example, Spiral has been used to enerate part of Intel's commercial libraries IPP and MKL.","P\""{u}schel, Markus",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"Projectional Editing of Variational Software","Editing the source code of variational software is complicated by the presence of variation annotations, such as #ifdef statements, and by code that is only included in some configurations. When editing some configurations and not others, it would be easier to edit a simplified version of the source code that includes only the configurations we currently care about. In this paper, we present a projectional editing model for variational software. Using our approach, a programmer can partially configure a variational program, edit this simplified view of the code, and then automatically update the original, fully variational source code. The model is based on an isolation principle where edits affect only the variants that are visible in the view. We show that this principle has several nice properties that are suggested by related work on bidirectional transformations.","Walkingshaw, Eric and Ostermann, Klaus",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Repairing COTS Router Firmware Without Access to Source Code or Test Suites: A Case Study in Evolutionary Software Repair","The speed with which newly discovered software vulnerabilities are patched is a critical factor in mitigating the harm caused by subsequent exploits. Unfortunately, software vendors are often slow or unwilling to patch vulnerabilities, especially in embedded systems which frequently have no mechanism for updating factory-installed firmware. The situation is particularly dire for commercial off the shelf (COTS) software users, who lack source code and are wholly dependent on patches released by the vendor.

We propose a solution in which the vulnerabilities drive an automated evolutionary computation repair process capable of directly patching embedded systems firmware. Our approach does not require access to source code, regression tests, or any participation from the software vendor. Instead, we present an interactive evolutionary algorithm that searches for patches that resolve target vulnerabilities while relying heavily on post-evolution difference minimization to remove most regressions. Extensions to prior work in evolutionary program repair include: repairing vulnerabilities in COTS router firmware; handling stripped MIPS executables; operating without fault localization information; operating without a regression test suite; and incorporating user interaction into the evolutionary repair process.

We demonstrate this method by repairing two well-known vulnerabilities in version 4 of NETGEAR's WNDR3700 wireless router before NETGEAR released patches publicly for the vulnerabilities. Without fault localization we are able to find repair edits that are not located on execution traces. Without the advantage of regression tests to guide the search, we find that 80% of repairs of the example vulnerabilities retain program functionality after minimization. With minimal user interaction to demonstrate required functionality, 100% of the proposed repairs were able to address the vulnerabilities while retaining required functionality.","Schulte, Eric M. and Weimer, Westley and Forrest, Stephanie",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Mining Fine-grained Code Changes to Detect Unknown Change Patterns","Identifying repetitive code changes benefits developers, tool builders, and researchers. Tool builders can automate the popular code changes, thus improving the productivity of developers. Researchers can better understand the practice of code evolution, advancing existing code assistance tools and benefiting developers even further. Unfortunately, existing research either predominantly uses coarse-grained Version Control System (VCS) snapshots as the primary source of code evolution data or considers only a small subset of program transformations of a single kind - refactorings.

We present the first approach that identifies previously unknown frequent code change patterns from a fine-grained sequence of code changes. Our novel algorithm effectively handles challenges that distinguish continuous code change pattern mining from the existing data mining techniques. We evaluated our algorithm on 1,520 hours of code development collected from 23 developers, and showed that it is effective, useful, and scales to large amounts of data. We analyzed some of the mined code change patterns and discovered ten popular kinds of high-level program transformations. More than half of our 420 survey participants acknowledged that eight out of ten transformations are relevant to their programming activities.","Negara, Stas and Codoban, Mihai and Dig, Danny and Johnson, Ralph E.",2014,"[""ACM"",""Engineering Village""]","Aceito: CA3","Aceito: CA3"
"Using Machine Learning to Improve Automatic Vectorization","Automatic vectorization is critical to enhancing performance of compute-intensive programs on modern processors. However, there is much room for improvement over the auto-vectorization capabilities of current production compilers through careful vector-code synthesis that utilizes a variety of loop transformations (e.g., unroll-and-jam, interchange, etc.).

As the set of transformations considered is increased, the selection of the most effective combination of transformations becomes a significant challenge: Currently used cost models in vectorizing compilers are often unable to identify the best choices. In this paper, we address this problem using machine learning models to predict the performance of SIMD codes. In contrast to existing approaches that have used high-level features of the program, we develop machine learning models based on features extracted from the generated assembly code. The models are trained offline on a number of benchmarks and used at compile-time to discriminate between numerous possible vectorized variants generated from the input code.

We demonstrate the effectiveness of the machine learning model by using it to guide automatic vectorization on a variety of tensor contraction kernels, with improvements ranging from 2× to 8× over Intel ICC's auto-vectorized code. We also evaluate the effectiveness of the model on a number of stencil computations and show good improvement over auto-vectorized code.","Stock, Kevin and Pouchet, Louis-No\""{e}l and Sadayappan, P.",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"More Accurate Recommendations for Method-level Changes","
During the life span of large software projects, developers often apply the same code changes to different code locations in slight variations. Since the application of these changes to all locations is time-consuming and error-prone, tools exist that learn change patterns from input examples, search for possible pattern applications, and generate corresponding recommendations. In many cases, the generated recommendations are syntactically or semantically wrong due to code movements in the input examples. Thus, they are of low accuracy and developers cannot directly copy them into their projects without adjustments.

We present the Accurate REcommendation System (ARES) that achieves a higher accuracy than other tools because its algorithms take care of code movements when creating patterns and recommendations. On average, the recommendations by ARES have an accuracy of 96% with respect to code changes that developers have manually performed in commits of source code archives. At the same time ARES achieves precision and recall values that are on par with other tools.","Dotzler, Georg and Kamp, Marius and Kreutzer, Patrick and Philippsen, Michael",2017,"[""ACM""]","Aceito: CA7, CA3","Aceito: CA3, CA7"
"Change Prediction Through Coding Rules Violations","Static source code analysis is an increasingly important activity to manage software project quality, and is often found as a part of the development process. A widely adopted way of checking code quality is through the detection of violations to specific sets of rules addressing good programming practices. SonarQube is a platform able to detect these violations, called Issues. In this paper we described an empirical study performend on two industrial projects, where we used Issues extracted on different versions of the projects to predict changes in code through a set of machine learning models. We achieved good detection performances, especially when predicting changes in the next version. This result paves the way for future investigations of the interest in an industrial setting towards the prioritization of Issues management according to their impact on change-proneness.","Tollin, Irene and Fontana, Francesca Arcelli and Zanoni, Marco and Roveda, Riccardo",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"ChangeScribe: A Tool for Automatically Generating Commit Messages",,"Linares-V\'{a}squez, Mario and Cort{\'e}s-Coy, Luis Fernando and Aponte, Jairo and Poshyvanyk, Denys",2015,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"GHShot: 3D Design Versioning for Learning and Collaboration in the Web","
During the process of design, copies of files are often stored to track changes and design development or to ensure that previous work will not be lost. In software design field, such process is supported using versioning system, where source code is saved intermittently when features are added or modified for individual or group use. We argue that similar versioning system will also benefit the design community when applied to 3D design files, to see how their designs progress and collaborate. In this paper we outline a implemented web based open ecosystem allows designers to similarly collaborate but with a lower bar for adoption than comparable software versioning system. Our system is to be applied to a classroom setting, where architecture students learn to make structural designs; they are then able to see, modify, and give feedback to each other's work","Cristie, Verina and Joyce, Sam Conrad",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR10"
"Compiler Techniques to Improve Dynamic Branch Prediction for Indirect Jump and Call Instructions","Indirect jump instructions are used to implement multiway branch statements and virtual function calls in object-oriented languages. Branch behavior can have significant impact on program performance, but fortunately hardware predictors can alleviate much of the risk. Modern processors include indirect branch predictors which use part of the target address to update a global history. We present a code generation technique to maximize the branch history information available to the predictor. We implement our optimization as an assembly language transformation, and evaluate it for SPEC benchmarks and interpreters using simulated and real hardware, showing indirect branch misprediction decreases.","Mccandless, Jason and Gregg, David",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Supporting ""What-if"" in Touch-screen Web Applications","Surface computing encourages exploratory interaction, and many applications are designed to work this way. In essence, the fluid interaction causes the user to ask “What if?” We suggest this requires support for recording the history of such explorations and allowing reversion to earlier states. There are currently a variety of related mechanisms, but they are either underpowered for the sort of interaction history management we suggest is needed, or are restricted to very specific domains. We present a prototype implementation of an interaction history manager: Ra is a JavaScript library for supporting this exploration and version tracking in web applications. We illustrate the interface for end users seen in augmenting simple web applications; we describe the underlying technical architecture, which uses ES6 Proxy objects to maintain access to the application’s model; and we present the API, which allows an existing application to include Ra with minimal code change.","Simonyi, Peter and Wilson, Jeff and Brown, Judith M. and Biddle, Robert",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"How to Detect Performance Changes in Software History: Performance Analysis of Software System Versions","Source code changes can affect the performance of software. Structured knowledge about classes of those changes could guide software developers in avoiding negative changes and improving the performance by positive changes. Neither a comprehensive overview nor a mature method for structured detection of those changes exists for this purpose. We address this research challenge by presenting Performance Analysis of Software Systems (PeASS). PeASS builds up a comprehensive knowledge base of changes affecting the performance of a software by analyzing the version history of a repository using its unit tests. It is based on a method for determining the significant performance changes between two unit tests by measurement and statistical analysis. Furthermore, PeASS uses regression test selection for saving measurement time and root cause isolation method for performance changes analysis. We demonstrate our methodology in the context of Java by analyzing the versions of Apache Commons IO.","Reichelt, David Georg and K\""{u}hne, Stefan",2018,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Java Interoperability in Managed X10","The ability to smoothly interoperate with other programming languages is an essential feature to reduce the barriers to adoption for new languages such as X10. Compiler-supported interoperability between Managed X10 and Java was initially previewed in X10 version 2.2.2 and is now fully supported in X10 version 2.3. In this paper we describe and motivate the Java interoperability features of Managed X10. For calling Java from X10, external linkage for Java code is explained. For calling X10 from Java, the current implementation of Java code generation is explained.

An unusual aspect of X10 is that, unlike most other JVM-hosted languages, X10 is also implemented via compilation to C++ (Native X10). The requirement to support multiple execution platforms results in unique challenges to the design of cross-language interoperability. In particular, we discovered that a single top exception type that covers all exception types from source and all target languages is needed as a native type of the source language for portable exception handling. This realization motivated both minor changes in the X10 language specification and an extensive redesign of the X10 core class library for X10 2.3.","Takeuchi, Mikio and Cunningham, David and Grove, David and Saraswat, Vijay",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"Identifying Compression History of Wave Audio and Its Applications","Audio signal is sometimes stored and/or processed in WAV (waveform) format without any knowledge of its previous compression operations. To perform some subsequent processing, such as digital audio forensics, audio enhancement and blind audio quality assessment, it is necessary to identify its compression history. In this article, we will investigate how to identify a decompressed wave audio that went through one of three popular compression schemes, including MP3, WMA (windows media audio) and AAC (advanced audio coding). By analyzing the corresponding frequency coefficients, including modified discrete cosine transform (MDCT) and Mel-frequency cepstral coefficients (MFCCs), of those original audio clips and their decompressed versions with different compression schemes and bit rates, we propose several statistics to identify the compression scheme as well as the corresponding bit rate previously used for a given WAV signal. The experimental results evaluated on 8,800 audio clips with various contents have shown the effectiveness of the proposed method. In addition, some potential applications of the proposed method are discussed.","Luo, Da and Luo, Weiqi and Yang, Rui and Huang, Jiwu",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"Toward Automatic Update from Callbacks to Promises","Javascript is the prevalent scripting language for the web. It lets web pages register callbacks to react to user events. A callback is a function to be invoked later with a result currently unavailable. This pattern also proved to respond efficiently to remote requests. Javascript is currently used to implement complete web applications. However, callbacks are ill-suited to arrange a large asynchronous execution flow. Promises are a more adapted alternative. They provide a unified control over both the synchronous and asynchronous execution flows.

The next version of Javascript proposes to replace callbacks with Promises. This paper brings the first step toward a compiler to help developers prepare this shift. We present an equivalence between callbacks and Dues. The latter are a simpler specification of Promises developed for the purpose of this demonstration. From this equivalence, we implement a compiler to transform an imbrication of callbacks into a chain of Dues. This equivalence is limited to Node.js-style asynchronous callbacks declared in situ. We evaluate our compiler over 64 npm packages, 9 of them present compatible callbacks and compile successfully.

We consider this shift to be a first step toward the merge of concepts from the data-flow programming model into the imperative programming model.","Brodu, Etienne and Fr{\'e}not, St{\'e}phane and Obl{\'e}, Fr{\'e}d{\'e}ric",2015,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Effective Automatic Computation Placement and Data Allocation for Parallelization of Regular Programs","This paper proposes techniques for data allocation and computation mapping when compiling affine loop nest sequences for distributed-memory clusters.Techniques for transformation and detection of parallelism, and generation of communication sets relying on the polyhedral framework already exist. However, these recent approaches used a simple strategy to map computation to nodes -- typically block or block-cyclic. These mappings may lead to excess communication volume for multiple loop nests. In addition, the data allocation strategy used did not permit efficient weak scaling. We address these complementary problems by proposing automatic techniques to determine computation placements for identified parallelism and allocation of data. Our approach for data allocation is driven by tiling of data spaces along with a scheme to allocate and deallocate tiles on demand and reuse them. We show that our approach for computation mapping yields more effective mappings than those that can be developed using vendor-supplied libraries. Experimental results on some sequences of BLAS calls demonstrate a mean speedup of 1.82x over versions written with ScaLAPACK. Besides enabling weak scaling for distributed memory, data tiling also improves locality for shared-memory parallelization. Experimental results on a 32-core shared-memory SMP system shows a mean speedup of 2.67x over code that is not data tiled.","Reddy, Chandan and Bondhugula, Uday",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"An Empirical Study of the Effect of Source-level Loop Transformations on Compiler Stability","Modern compiler optimization is a complex process that offers no guarantees to deliver the fastest, most efficient target code. For this reason, compilers struggle to produce a stable performance from versions of code that carry out the same computation and only differ in the order of operations. This instability makes compilers much less effective program optimization tools and often forces programmers to carry out a brute force search when tuning for performance. In this paper, we analyze the stability of the compilation process and the performance headroom of three widely used general purpose compilers: GCC, ICC, and Clang. For the study, we extracted over 1,000 <pre>for</pre> loop nests from well-known benchmarks, libraries, and real applications; then, we applied sequences of source-level loop transformations to these loop nests to create numerous semantically equivalent mutations; finally, we analyzed the impact of transformations on code quality in terms of locality, dynamic instruction count, and vectorization. Our results show that, by applying source-to-source transformations and searching for the best vectorization setting, the percentage of loops sped up by at least 1.15x is 46.7% for GCC, 35.7% for ICC, and 46.5% for Clang, and on average the potential for performance improvement is estimated to be at least 23.7% for GCC, 18.1% for ICC, and 26.4% for Clang. Our stability analysis shows that, under our experimental setup, the average coefficient of variation of the execution time across all mutations is 18.2% for GCC, 19.5% for ICC, and 16.9% for Clang, and the highest coefficient of variation for a single loop nest reaches 118.9% for GCC, 124.3% for ICC, and 110.5% for Clang. We conclude that the evaluated compilers need further improvements to claim they have stable behavior.","Gong, Zhangxiaowen and Chen, Zhi and Szaday, Justin and Wong, David and Sura, Zehra and Watkinson, Neftali and Maleki, Saeed and Padua, David and Veidenbaum, Alexander and Nicolau, Alexandru and Torrellas, Josep",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Feedback-driven Binary Code Diversification","As described in many blog posts and in the scientific literature, exploits for software vulnerabilities are often engineered on the basis of patches. For example, “Microsoft Patch Tuesday” is often followed by “Exploit Wednesday” during which yet unpatched systems become vulnerable to patch-based exploits. Part of the patch engineering includes the identification of the vulnerable binary code by means of reverse-engineering tools and diffing add-ons. In this article we present a feedback-driven compiler tool flow that iteratively transforms code until diffing tools become ineffective enough to close the “Exploit Wednesday” window of opportunity. We demonstrate the tool's effectiveness on a set of real-world patches and against the latest version of BinDiff.","Coppens, Bart and De Sutter, Bjorn and Maebe, Jonas",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"REPERTOIRE: A Cross-system Porting Analysis Tool for Forked Software Projects","To create a new variant of an existing project, developers often copy an existing codebase and modify it. This process is called software forking. After forking software, developers often port new features or bug fixes from peer projects. Repertoire analyzes repeated work of cross-system porting among forked projects. It takes the version histories as input and identifies ported edits by comparing the content of individual patches. It also shows users the extent of ported edits, where and when the ported edits occurred, which developers ported code from peer projects, and how long it takes for patches to be ported.","Ray, Baishakhi and Wiley, Christopher and Kim, Miryung",2012,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Untangling Development Tasks with Software Developer's Activity",,"Konopka, Martin and Navrat, Pavol",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Towards Compositional and Generative Tensor Optimizations","Many numerical algorithms are naturally expressed as operations on tensors (i.e. multi-dimensional arrays). Hence, tensor expressions occur in a wide range of application domains, e.g. quantum chemistry and physics; big data analysis and machine learning; and computational fluid dynamics. Each domain, typically, has developed its own strategies for efficiently generating optimized code, supported by tools such as domain-specific languages, compilers, and libraries. However, strategies and tools are rarely portable between domains, and generic solutions typically act as ''black boxes'' that offer little control over code generation and optimization. As a consequence, there are application domains without adequate support for easily generating optimized code, e.g. computational fluid dynamics. In this paper we propose a generic and easily extensible intermediate language for expressing tensor computations and code transformations in a modular and generative fashion. Beyond being an intermediate language, our solution also offers meta-programming capabilities for experts in code optimization. While applications from the domain of computational fluid dynamics serve to illustrate our proposed solution, we believe that our general approach can help unify research in tensor optimizations and make solutions more portable between domains.","Susungi, Adilla and Rink, Norman A. and Castrill\'{o}n, Jer\'{o}nimo and Huismann, Immo and Cohen, Albert and Tadonki, Claude and Stiller, J\""{o}rg and Fr\""{o}hlich, Jochen",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"RefDistiller: A Refactoring Aware Code Review Tool for Inspecting Manual Refactoring Edits","Manual refactoring edits are error prone, as refactoring requires developers to coordinate related transformations and understand the complex inter-relationship between affected types, methods, and variables. We present RefDistiller, a refactoring-aware code review tool that can help developers detect potential behavioral changes in manual refactoring edits. It first detects the types and locations of refactoring edits by comparing two program versions. Based on the reconstructed refactoring information, it then detects potential anomalies in refactoring edits using two techniques: (1) a template-based checker for detecting missing edits and (2) a refactoring separator for detecting extra edits that may change a program's behavior. By helping developers be aware of deviations from pure refactoring edits, RefDistiller can help developers have high confidence about the correctness of manual refactoring edits. RefDistiller is available as an Eclipse plug-in at https://sites.google.com/site/refdistiller/ and its demonstration video is available at http://youtu.be/0Iseoc5HRpU.","Alves, Everton L. G. and Song, Myoungkyu and Kim, Miryung",2014,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"NeedFeed: Taming Change Notifications by Modeling Code Relevance","Most software development tools allow developers to subscribe to notifications about code checked-in by their team members in order to review changes to artifacts that they are responsible for. However, past user studies have indicated that this mechanism is counter-productive, as developers spend a significant amount of effort sifting through such feeds looking for items that are relevant to them. We present NeedFeed, a system that models code relevance by mining a project's software repository and highlights changes that a developer may need to review. We evaluate several techniques to model code relevance, from a naive TOUCH-based approach to generic HISTORY-based classifiers using temporal code metrics at file and method-level granularities, which are then improved by building developer-specific models using TEXT-based features from commit messages. NeedFeed reduces notification clutter by more than 90%, on average, with the best strategy giving an average precision and recall of more than 75%.","Padhye, Rohan and Mani, Senthil and Sinha, Vibha Singhal",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Partitioning Composite Code Changes to Facilitate Code Review",,"Tao, Yida and Kim, Sunghun",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Fast Deployment and Scoring of Support Vector Machine Models in CPU and GPU",,"Castro-Lopez, Oscar and Vega-Lopez, Ines F.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Recommender System for Model Driven Software Development","Models are key artifacts in model driven software engineering, similar to source code in traditional software engineering. Integrated development environments help users while writing source code, e.g. with typed auto completions, quick fixes, or automatic refactorings. Similar integrated features are rare for modeling IDEs. The above source code IDE features can be seen as a recommender system.

A recommender system for model driven software engineering can combine data from different sources in order to infer a list of relevant and actionable model changes in real time. These recommendations can speed up working on models by automating repetitive tasks and preventing errors when the changes are atypical for the changed models.

Recommendations can be based on common model transformations that are taken from the literature or learned from models in version control systems. Further information can be taken from instance- to meta-model relationships, modeling related artifacts (e.g. correctness constraints), and versions histories of models under version control.

We created a prototype recommender that analyses the change history of a single model. We computed its accuracy via cross-validation and found that it was between 0.43 and 0.82 for models from an open source project.

In order to have a bigger data set for the evaluation and the learning of model transformation, we also mined repositories from Eclipse projects for Ecore meta models and their versions. We found 4374 meta models with 17249 versions. 244 of these meta models were changed at least ten times and are candidates for learning common model transformations.

We plan to evaluate our recommender system in two ways: (1) In off-line evaluations with data sets of models from the literature, created by us, or taken from industry partners. (2) In on-line user studies with participants from academia and industry, performed as case studies and controlled experiments.","K\""{o}gel, Stefan",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Meta-programming for Cross-domain Tensor Optimizations","
Many modern application domains crucially rely on tensor operations. The optimization of programs that operate on tensors poses difficulties that are not adequately addressed by existing languages and tools. Frameworks such as TensorFlow offer good abstractions for tensor operations, but target a specific domain, i.e. machine learning, and their optimization strategies cannot easily be adjusted to other domains. General-purpose optimization tools such as Pluto and existing meta-languages offer more flexibility in applying optimizations but lack abstractions for tensors. This work closes the gap between domain-specific tensor languages and general-purpose optimization tools by proposing the Tensor optimizations Meta-Language (TeML). TeML offers high-level abstractions for both tensor operations and loop transformations, and enables flexible composition of transformations into effective optimization paths. This compositionality is built into TeML's design, as our formal language specification will reveal. We also show that TeML can express tensor computations as comfortably as TensorFlow and that it can reproduce Pluto's optimization paths. Thus, optimized programs generated by TeML execute at least as fast as the corresponding Pluto programs. In addition, TeML enables optimization paths that often allow outperforming Pluto.","Susungi, Adilla and Rink, Norman A. and Cohen, Albert and Castrillon, Jeronimo and Tadonki, Claude",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"On Mapping Releases to Commits in Open Source Systems","The paper presents an empirical study on the release naming and structure in three open source projects: Google Chrome, GNU gcc, and Subversion. Their commonality and variability are discussed. An approach is developed that establishes the mapping from a particular release (major or minor) to the specific earliest and latest revisions, i.e., a commit window of a release, in the source control repository. For example, the major release 25.0 in Chrome is mapped to the earliest revision 157687 and latest revision 165096 in the trunk. This mapping between releases and commits would facilitate a systematic choice of history in units of the project evolution scale (i.e., commits that constitute a software release). A projected application is in forming a training set for a source-code change prediction model, e.g., using the association rule mining or machine learning techniques, commits from the source code history are needed.","Shobe, Joe F. and Karim, Md Yasser and Zanjani, Motahareh Bahrami and Kagdi, Huzefa",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"POSTER: Hybrid Data Dependence Analysis for Loop Transformations","Loop optimizations span from vectorization, scalar promotion, loop invariant code motion, software pipelining to loop fusion, skewing, tiling and loop parallelization. These transformations are essential in the quest for automated high-performance code generation.

Determining the validity of loop transformations at compile time requires analyzing all possible data dependences that may exist at run-time, i.e., all may-dependences. One fundamental issue faced by loop optimizers relates to the precision of alias and dependence information. Static alias and dependence analysis has been extensively studied, but many factors make the problem extremely tough.

We provide theoretical and practical foundations to safely apply aggressive loop transformations on programs with polynomial data access relations. We do so by inverting how the may-dependence problem is attacked. Instead of providing alias information to the optimizer such that it can filter invalid transformations, we request that it performs a transformation that we believe will reduce the execution time and we generate a fast and precise test to validate, at run-time, if the optimization can be taken.","Sampaio, Diogo Nunes and Ketterlin, Alain and Pouchet, Louis-Noel and Rastello, Fabrice",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Annotation Support for Generic Patches",,"Dotzler, Georg and Veldema, Ronald and Philippsen, Michael",2012,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Change Impact Using Dynamic History Analysis: (Abstract Only)",,"Pugh, Sydney and Binkley, David",2018,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR1"
"Edit Distance for Timed Automata","The edit distance between two (untimed) traces is the minimum cost of a sequence of edit operations (insertion, deletion, or substitution) needed to transform one trace to the other. Edit distances have been extensively studied in the untimed setting, and form the basis for approximate matching of sequences in different domains such as coding theory, parsing, and speech recognition.

In this paper, we lift the study of edit distances from untimed languages to the timed setting. We define an edit distance between timed words which incorporates both the edit distance between the untimed words and the absolute difference in time stamps. Our edit distance between two timed words is computable in polynomial time. Further, we show that the edit distance between a timed word and a timed language generated by a timed automaton, defined as the edit distance between the word and the closest word in the language, is PSPACE-complete. While computing the edit distance between two timed automata is undecidable, we show that the approximate version, where we decide if the edit distance between two timed automata is either less than a given parameter or more than δ away from the parameter, for δ < 0, can be solved in exponential space and is EXPSPACE-hard. Our definitions and techniques can be generalized to the setting of hybrid systems, and analogous decidability results hold for rectangular automata.","Chatterjee, Krishnendu and Ibsen-Jensen, Rasmus and Majumdar, Rupak",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"FinPar: A Parallel Financial Benchmark","Commodity many-core hardware is now mainstream, but parallel programming models are still lagging behind in efficiently utilizing the application parallelism. There are (at least) two principal reasons for this. First, real-world programs often take the form of a deeply nested composition of parallel operators, but mapping the available parallelism to the hardware requires a set of transformations that are tedious to do by hand and beyond the capability of the common user. Second, the best optimization strategy, such as what to parallelize and what to efficiently sequentialize, is often sensitive to the input dataset and therefore requires multiple code versions that are optimized differently, which also raises maintainability problems.

This article presents three array-based applications from the financial domain that are suitable for gpgpu execution. Common benchmark-design practice has been to provide the same code for the sequential and parallel versions that are optimized for only one class of datasets. In comparison, we document (1) all available parallelism via nested map-reduce functional combinators, in a simple Haskell implementation that closely resembles the original code structure, (2) the invariants and code transformations that govern the main trade-offs of a data-sensitive optimization space, and (3) report target cpu and multiversion gpgpu code together with an evaluation that demonstrates optimization trade-offs and other difficulties. We believe that this work provides useful insight into the language constructs and compiler infrastructure capable of expressing and optimizing such applications, and we report in-progress work in this direction.","Andreetta, Christian and B{\'e}got, Vivien and Berthold, Jost and Elsman, Martin and Henglein, Fritz and Henriksen, Troels and Nordfang, Maj-Britt and Oancea, Cosmin E.",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"A Declarative Foundation for Comprehensive History Querying",,"Stevens, Reinout",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Learning to Rank Relevant Files for Bug Reports Using Domain Knowledge","When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files of a project with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and potentially could lead to a substantial increase in productivity. This paper introduces an adaptive ranking approach that leverages domain knowledge through functional decompositions of source code files into methods, API descriptions of library components used in the code, the bug-fixing history, and the code change history. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features encoding domain knowledge, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluated our system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the newly introduced learning-to-rank approach significantly outperforms two recent state-of-the-art methods in recommending relevant files for bug reports. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70% of the bug reports in the Eclipse Platform and Tomcat projects.","Ye, Xin and Bunescu, Razvan and Liu, Chang",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Can We Predict Types of Code Changes?: An Empirical Analysis",,"Giger, Emanuel and Pinzger, Martin and Gall, Harald C.",2012,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Algorithmic Differentiation of Code with Multiple Context-Specific Activities","Algorithmic differentiation (AD) by source-transformation is an established method for computing derivatives of computational algorithms. Static dataflow analysis is commonly used by AD tools to determine the set of active variables, that is, variables that are influenced by the program input in a differentiable way and have a differentiable influence on the program output. In this work, a context-sensitive static analysis combined with procedure cloning is used to generate specialised versions of differentiated procedures for each call site. This enables better detection and elimination of unused computations and memory storage, resulting in performance improvements of the generated code, in both forward- and reverse-mode AD. The implications of this multi-activity AD approach on the static analysis of an AD tool is shown using dataflow equations. The worst-case cost of multi-activity AD on the differentiation process is analysed and practical remedies to avoid running into this worst case are presented. The method was implemented in the AD tool Tapenade, and we present its application to a 3D unstructured compressible flow solver, for which we generate an adjoint solver that performs significantly faster when multi-activity AD is used.","H\""{u}ckelheim, Jan Christian and Hasco\""{e}t, Laurent and M\""{u}ller, Jens-Dominik",2017,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Forge: Generating a High Performance DSL Implementation from a Declarative Specification","Domain-specific languages provide a promising path to automatically compile high-level code to parallel, heterogeneous, and distributed hardware. However, in practice high performance DSLs still require considerable software expertise to develop and force users into tool-chains that hinder prototyping and debugging. To address these problems, we present Forge, a new meta DSL for declaratively specifying high performance embedded DSLs. Forge provides DSL authors with high-level abstractions (e.g., data structures, parallel patterns, effects) for specifying their DSL in a way that permits high performance. From this high-level specification, Forge automatically generates both a naïve Scala library implementation of the DSL and a high performance version using the Delite DSL framework. Users of a Forge-generated DSL can prototype their application using the library version, and then switch to the Delite version to run on multicore CPUs, GPUs, and clusters without changing the application code. Forge-generated Delite DSLs perform within 2x of hand-optimized C++ and up to 40x better than Spark, an alternative high-level distributed programming environment. Compared to a manually implemented Delite DSL, Forge provides a factor of 3-6x reduction in lines of code and does not sacrifice any performance. Furthermore, Forge specifications can be generated from existing Scala libraries, are easy to maintain, shield DSL developers from changes in the Delite framework, and enable DSLs to be retargeted to other frameworks transparently.","Sujeeth, Arvind K. and Gibbons, Austin and Brown, Kevin J. and Lee, HyoukJoong and Rompf, Tiark and Odersky, Martin and Olukotun, Kunle",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Recognizing Relevant Code Elements During Change Task Navigation","Developers spend a significant amount of their time exploring source code. Yet, little is known about the way developers break down their code exploration or the fine-grained navigation for change tasks within methods. The objective of our research is to address this gap and learn more about developers' code navigation for change tasks to devise better tool support. For our research, we perform exploratory studies also taking advantage of eye-tracking technology and interaction monitoring to gather detailed data on developers' code navigation down to the line-level. Based on the findings, we devise a model and approach that capture developers' code navigation and allow to automatically determine code elements that are relevant in the near future as well as to determine and summarize the elements that are currently relevant and might, for instance, be helpful for task resumption after interruptions. We plan to evaluate our model and approach in a multiple cases study on navigation recommendation and work resumption.","Kevic, Katja",2016,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Visualizing Code and Coverage Changes for Code Review","One of the tasks of reviewers is to verify that code modifications are well tested. However, current tools offer little support in understanding precisely how changes to the code relate to changes to the tests. In particular, it is hard to see whether (modified) test code covers the changed code. To mitigate this problem, we developed Operias, a tool that provides a combined visualization of fine-grained source code differences and coverage impact. Operias works both as a stand-alone tool on specific project versions and as a service hooked to GitHub. In the latter case, it provides automated reports for each new pull request, which reviewers can use to assess the code contribution. Operias works for any Java project that works with maven and its standard Cobertura coverage plugin. We present how Operias could be used to identify test-related problems in real-world pull requests. Operias is open source and available on GitHub with a demo video: https://github.com/SERG-Delft/operias","Oosterwaal, Sebastiaan and Deursen, Arie van and Coelho, Roberta and Sawant, Anand Ashok and Bacchelli, Alberto",2016,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Investigating Code Review Practices in Defective Files: An Empirical Study of the Qt System",,"Thongtanunam, Patanamon and McIntosh, Shane and Hassan, Ahmed E. and Iida, Hajimu",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"The Storyteller Version Control System: Tackling Version Control, Code Comments, and Team Learning","This demonstration shows the Storyteller version control system. The tool aims to change the way software developers learn by opening up for examination how they do their work. The tool has traditional version control functionality (branching and merging) but in addition it records how development work is done, organizes it, and allows it to be played back for others. Most importantly, the tool allows developers to tell stories about what they did and why. It captures and organizes institutional knowledge that would otherwise be lost.","Mahoney, Mark",2012,"[""ACM""]","Rejeitado: CR12, CR11","Rejeitado: CR11, CR12"
"Versioning for End-to-End Machine Learning Pipelines","End-to-end machine learning pipelines that run in shared environments are challenging to implement. Production pipelines typically consist of multiple interdependent processing stages. Between stages, the intermediate results are persisted to reduce redundant computation and to improve robustness. Those results might come in the form of datasets for data processing pipelines or in the form of model coefficients in case of model training pipelines. Reusing persisted results improves efficiency but at the same time creates complicated dependencies. Every time one of the processing stages is changed, either due to code change or due to parameters change, it becomes difficult to find which datasets can be reused and which should be recomputed.

In this paper we build upon previous work to produce derivations of datasets to ensure that multiple versions of a pipeline can run in parallel while minimizing the amount of redundant computations. Our extensions include partial derivations to simplify navigation and reuse, explicit support for schema changes of pipelines, and a central registry of running pipelines to coordinate upgrading pipelines between teams.","van der Weide, Tom and Papadopoulos, Dimitris and Smirnov, Oleg and Zielinski, Michal and van Kasteren, Tim",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Learning Natural Coding Conventions","Every programmer has a characteristic style, ranging from preferences about identifier naming to preferences about object relationships and design patterns. Coding conventions define a consistent syntactic style, fostering readability and hence maintainability. When collaborating, programmers strive to obey a project’s coding conventions. However, one third of reviews of changes contain feedback about coding conventions, indicating that programmers do not always follow them and that project members care deeply about adherence. Unfortunately, programmers are often unaware of coding conventions because inferring them requires a global view, one that aggregates the many local decisions programmers make and identifies emergent consensus on style. We present NATURALIZE, a framework that learns the style of a codebase, and suggests revisions to improve stylistic consistency. NATURALIZE builds on recent work in applying statistical natural language processing to source code. We apply NATURALIZE to suggest natural identifier names and formatting conventions. We present four tools focused on ensuring natural code during development and release management, including code review. NATURALIZE achieves 94 % accuracy in its top suggestions for identifier names. We used NATURALIZE to generate 18 patches for 5 open source projects: 14 were accepted.","Allamanis, Miltiadis and Barr, Earl T. and Bird, Christian and Sutton, Charles",2014,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Program Generation for Performance","It has become extraordinarily difficult to write software that performs close to optimally on complex modern microarchitectures. Particularly plagued are domains that require complex mathematical computations such as multimedia processing, communication, control, graphics, and machine learning. In these domains, performance-critical components are usually written in C (with possible extensions) and often even in assembly, carefully ""tuned"" to the platform's architecture and microarchitecture. The result is usually long, rather unreadable code that needs to be re-written or re-tuned with every platform upgrade. On the other hand, the performance penalty for relying on straightforward, non-tuned, ""more elegant"" implementations can be often a factor of 10, 100, or even more. The overall problem is one of productivity, maintainability, and quality (namely performance), i.e., software engineering. However, even though a large set of sophisticated software engineering theory and tools exist, it appears that to date this community has not focused much on mathematical computations nor performance in the detailed, close-to-optimal sense above. The reason for the latter may be that performance, unlike various aspects of correctness, is not syntactic in nature (and in reality is often even unpredictable and, well, messy). The aim of this talk is to draw attention to the performance/productivity problem for mathematical applications and to make the case for a more interdisciplinary attack. As a set of thoughts in this direction we offer some of the lessons we have learned in the last decade in our own research on Spiral (www.spiral.net), a program generation framework for numerical kernels. Key techniques used in Spiral include staged declarative domain-specific languages to express algorithm knowledge and algorithm transformations, the use of platform-cognizant rewriting systems for parallelism and locality optimizations, and the use of search and machine learning techniques to navigate possible spaces of choices. Experimental results show that the codegenerated by Spiral competes with, and sometimes outperforms, the best available human-written code. Spiral has been used to generate part of Intel's commercial libraries IPP and MKL.","P\""{u}schel, Markus",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"Review Code Evolution History in OSS Universe","Software evolves all the time because of the changing requirements, in particular, in the diverse Internet environment. Evolution history recorded in software repositories, e.g., Version Control Systems, reflects people's software development practice. Exploring this history could help practitioners to reuse the best practices therefore improve productivity and software quality. Because of the difficulty of collecting and standardizing data, most existing work could only utilize small project set. In this study, we target the open source software universe to build a universal code evolution model for large-scale data. We consider code evolution from two aspects: code version changing history in a single project and code reuse history in the whole universe. In the model, files/modules are built as nodes, and relations (version change or reuse) between files/modules are built as connections. Based on the model, we design and implement a code evolution review framework, i.e., Code Evolution Reviewer (CER), which provides a series of data interfaces to review code evolution history, in particular, code version changing in single project and code reuse among projects. Further, CER could be utilized to explore best practices across large-scale project set.","Zhu, Jiaxin and Lin, Hongwu and Zhou, Minghui and Mei, Hong",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Post Release Versions Based Code Change Quality Metrics","Software Metric is a quantitative measure of the degree to which a system, component or process possesses a given attribute. Bug fixing, new features (NFs) introduction and feature improvements (IMPs) are the key factors in deciding the next version of software. For fixing an issue (bug/new feature/feature improvement), a lot of changes have to be incorporated into the source code of the software. These code changes need to be understood by software engineers and managers when performing their daily development and maintenance tasks. In this paper, we have proposed four new metrics namely code change quality, code change density, file change quality and file change density to understand the quality of code changes across the different versions of five open source software products, namely Avro, Pig, Hive, jUDDI and Whirr of Apache project. Results show that all the products get better code change quality over a period of time. We have also observed that all the five products follow the similar code change trend.","Sharma, Meera and Kumari, Madhu and Singh, V. B.",2015,"[""ACM"",""Engineering Village""]","Aceito: CA6","Aceito: CA6"
"Mining API Expertise Profiles with Partial Program Analysis","A developer's API usage expertise can be estimated by analyzing source code that they have checked-in to a software repository. In prior work we proposed a system for creating a social network of developers centered around the APIs they use in order to recommend people and projects they might be interested in. The implementation of such a system requires analyzing code from repositories of large numbers of projects that use different build systems. Hence, one challenge is to determine the APIs referenced in code in these repositories without relying on the ability to resolve every project's external dependencies. In this paper, we consider a technique called Partial Program Analysis for resolving type bindings in Java source code in the absence of third-party library binaries. Another important design decision concerns the approach of associating such API references with the developers who authored them such as walking entire change history or use blame information. We evaluate these different design options on 4 open-source Java projects and found that both Partial Program Analysis and blame-based approach provide precision greater than 80%. However, use of blame as opposed to complete program history leads to significant recall loss, in most cases greater than 40%.","Mani, Senthil and Padhye, Rohan and Sinha, Vibha Singhal",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Hydra: Automatic Algorithm Exploration from Linear Algebra Equations","Hydra accepts an equation written in terms of operations on matrices and automatically produces highly efficient code to solve these equations. Processing of the equation starts by tiling the matrices. This transforms the equation into either a single new equation containing terms involving tiles or into multiple equations some of which can be solved in parallel with each other. Hydra continues transforming the equations using tiling and seeking terms that Hydra knows how to compute or equations it knows how to solve. The end result is that by transforming the equations Hydra can produce multiple solvers with different locality behavior and/or different parallel execution profiles. Next, Hydra applies empirical search over this space of possible solvers to identify the most efficient version. In this way, Hydra enables the automatic production of efficient solvers requiring very little or no coding at all and delivering performance approximating that of the highly tuned library routines such as Intel's MKL.","Padua, David and Barthou, Denis and Duchateau, Alexandre X.",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Risky Files: An Approach to Focus Quality Improvement Effort","Hydra accepts an equation written in terms of operations on matrices and automatically produces highly efficient code to solve these equations. Processing of the equation starts by tiling the matrices. This transforms the equation into either a single new equation containing terms involving tiles or into multiple equations some of which can be solved in parallel with each other. Hydra continues transforming the equations using tiling and seeking terms that Hydra knows how to compute or equations it knows how to solve. The end result is that by transforming the equations Hydra can produce multiple solvers with different locality behavior and/or different parallel execution profiles. Next, Hydra applies empirical search over this space of possible solvers to identify the most efficient version. In this way, Hydra enables the automatic production of efficient solvers requiring very little or no coding at all and delivering performance approximating that of the highly tuned library routines such as Intel's MKL.","Mockus, Audris and Hackbarth, Randy and Palframan, John",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"The Effect of IMPORT Change in Software Change History","Source code change analysis in the project history is one of main issues in mining software repositories, which incorporates change prediction, API evolution and refactoring, etc. In the previous studies, fine-grained source code changes, that is, code level changes, were used to find source code change patterns. In this paper, we closely investigate IMPORT change type, which has not been unnoticed. At first, we performed modifying the existing change extraction tool, change distiller [3], to extract IMPORT change history. And then, we extracted commit history data from project repository of eclipse CDT and IDT. Change types for each change in commit history data have been determined using change types in [3] and IMPORT change types defined in this work. Finally, we analyzed the effect of IMPORT change using the frequency of each change type occurred in the commit history. Experimental result shows that the IMPORT change meaningfully affects other changes and it would be better to consider IMPORT change types in change analysis work.","Kim, Jungil and Lee, Eunjoo",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"A Case Study of Cross-system Porting in Forked Projects",,"Ray, Baishakhi and Kim, Miryung",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"How Do Centralized and Distributed Version Control Systems Impact Software Changes?","Distributed Version Control Systems (DVCS) have seen an increase in popularity relative to traditional Centralized Version Control Systems (CVCS). Yet we know little on whether developers are benefitting from the extra power of DVCS. Without such knowledge, researchers, developers, tool builders, and team managers are in the danger of making wrong assumptions.

In this paper we present the first in-depth, large scale empirical study that looks at the influence of DVCS on the practice of splitting, grouping, and committing changes. We recruited 820 participants for a survey that sheds light into the practice of using DVCS. We also analyzed 409M lines of code changed by 358300 commits, made by 5890 developers, in 132 repositories containing a total of 73M LOC. Using this data, we uncovered some interesting facts. For example, (i) commits made in distributed repositories were 32% smaller than the centralized ones, (ii) developers split commits more often in DVCS, and (iii) DVCS commits are more likely to have references to issue tracking labels.","Brindescu, Caius and Codoban, Mihai and Shmarkatiuk, Sergii and Dig, Danny",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"DT: An Upgraded Detection Tool to Automatically Detect Two Kinds of Code Smell: Duplicated Code and Feature Envy","Code smell is unreasonable programming, and is produced when software developers don't have good habits of development and experience of development and other reasons. Code becomes more and more chaotic, the code structure become bloated. Code smell can make degradation of code quality. It also can make some difficulties for software developers to understand and maintain the source code of projects, and then cause unnecessary maintenance costs.

This study presents an evolutionary version of detection tool DT. DT can support detection of two kinds of code smell -Duplicated code and Feature envy. At the same time, two types of code smell are mainly detected by two detection thoughts: dynamic programming algorithm (DP) and abstract grammar tree (AST). DP can be applied in code smell - Duplicated code. DP uses the similarity between comparative lines to determine whether there is duplicated code; AST is used to detect code smell- feature envy, AST use tree structure to represent the source code, the grammatical structure of the source code transforms to each tree node. Through statistical analysis of existence of these nodes, we can determine whether is a kind of code smell or not.

In experiment, detection tool DT compares with four famous detection tools Checkstyle, PMD, iPlasma and JDeodorant. Detection accuracy is higher than above detection tools. In addition, these four well-known tools can't support detection of large industrial projects, while DT can support detection of industrial projects. In future work, we want to detect more kinds of code smell, meanwhile we want update detection precision of detection tool.","Liu, Xinghua and Zhang, Cheng",2018,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Automated Analysis of Student Programmer Coding Behavior Patterns (Abstract Only)",,"Ford, Corey and Staley, Clinton",2016,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"DSL-based Support for Semi-automated Architectural Component Model Abstraction Throughout the Software Lifecycle",,"Haitzer, Thomas and Zdun, Uwe",2012,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Using Computational Methods to Discover Student Science Conceptions in Interview Data",,"Sherin, Bruce",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Free Rider: A Source-Level Transformation Tool for Retargeting Platform-Specific Intrinsic Functions",,"Manilov, Stanislav and Franke, Bj\""{o}rn and Magrath, Anthony and Andrieu, Cedric",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Interactive Churn Metrics: Socio-technical Variants of Code Churn","A central part of software quality is finding bugs. One method of finding bugs is by measuring important aspects of the software product and the development process. In recent history, researchers have discovered evidence of a ""code churn"" effect whereby the degree to which a given source code file has changed over time is correlated with faults and vulnerabilities. Computing the code churn metric comes from counting source code differences in version control repositories. However, code churn does not take into account a critical factor of any software development team: the human factor, specifically who is making the changes. In this paper, we introduce a new class of human-centered metrics, ""interactive churn metrics"" as variants of code churn. Using the git blame tool, we identify the most recent developer who changed a given line of code in a file prior to a given revision. Then, for each line changed in a given revision, determined if the revision author was changing his or her own code (""self churn""), or the author was changing code last modified by somebody else (""interactive churn""). We derive and present several metrics from this concept. Finally, we conducted an empirical analysis of these metrics on the PHP programming language and its post-release vulnerabilities. We found that our interactive churn metrics are statistically correlated with post-release vulnerabilities and only weakly correlated with code churn metrics and source lines of code. The results indicate that interactive churn metrics are associated with software quality and are different from the code churn and source lines of code.","Meneely, Andrew and Williams, Oluyinka",2012,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"CIAhelper: Towards Change Impact Analysis in Delta-oriented Software Product Lines","Change is inevitable for software systems to deal with the evolving environment surrounding them, and applying changes requires careful design and implementation not to break existing functionalities. Evolution in software product lines (SPLs) is more complex compared to evolution for individual products: a change applied to a single feature might affect all the products in the whole product family. In this paper we present an approach for change impact analysis in delta-oriented programming (DOP), an existing language aimed at supporting SPLs. We propose the CIAHelper tool to identify dependencies within a DOP program, by analyzing the semantics of both the code artifacts and variability models to construct a directed dependency graph. We also consider how the source code history could be used to enhance the recall of detecting the affected artifacts given a change proposal. We evaluate our approach by means of five case studies on two different DOP SPLs.","Hamza, Mostafa and Walker, Robert J. and Elaasar, Maged",2018,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Using Graph-based Program Characterization for Predictive Modeling","Using machine learning has proven effective at choosing the right set of optimizations for a particular program. For machine learning techniques to be most effective, compiler writers have to develop expressive means of characterizing the program being optimized. The current state-of-the-art techniques for characterizing programs include using a fixed-length feature vector of either source code features extracted during compile time or performance counters collected when running the program. For the problem of identifying optimizations to apply, models constructed using performance counter characterizations of a program have been shown to outperform models constructed using source code features. However, collecting performance counters requires running the program multiple times, and this ""dynamic"" method of characterizing programs can be specific to inputs of the program. It would be preferable to have a method of characterizing programs that is as expressive as performance counter features, but that is ""static"" like source code features and therefore does not require running the program.

In this paper, we introduce a novel way of characterizing programs using a graph-based characterization, which uses the program's intermediate representation and an adapted learning algorithm to predict good optimization sequences. To evaluate different characterization techniques, we focus on loop-intensive programs and construct prediction models that drive polyhedral optimizations, such as auto-parallelism and various loop transformation.

We show that our graph-based characterization technique outperforms three current state-of-the-art characterization techniques found in the literature. By using the sequences predicted to be the best by our graph-based model, we achieved up to 73% of the speedup achievable in our search space for a particular platform, whereas we could only achieve up to 59% by other state-of-the-art techniques we evaluated.","Park, Eunjung and Cavazos, John and Alvarez, Marco A.",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Perspectives on Task Ownership in Mobile Operating System Development (Invited Talk)",,"Datta, Subhajit",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"FLUCCS: Using Code and Change Metrics to Improve Fault Localization","Fault localization aims to support the debugging activities of human developers by highlighting the program elements that are suspected to be responsible for the observed failure. Spectrum Based Fault Localization (SBFL), an existing localization technique that only relies on the coverage and pass/fail results of executed test cases, has been widely studied but also criticized for the lack of precision and limited effort reduction. To overcome restrictions of techniques based purely on coverage, we extend SBFL with code and change metrics that have been studied in the context of defect prediction, such as size, age and code churn. Using suspiciousness values from existing SBFL formulas and these source code metrics as features, we apply two learn-to-rank techniques, Genetic Programming (GP) and linear rank Support Vector Machines (SVMs). We evaluate our approach with a ten-fold cross validation of method level fault localization, using 210 real world faults from the Defects4J repository. GP with additional source code metrics ranks the faulty method at the top for 106 faults, and within the top five for 173 faults. This is a significant improvement over the state-of-the-art SBFL formulas, the best of which can rank 49 and 127 faults at the top and within the top five, respectively.","Sohn, Jeongju and Yoo, Shin",2017,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"CoExist: Overcoming Aversion to Change",,"Steinert, Bastian and Cassou, Damien and Hirschfeld, Robert",2012,"[""ACM""]","Rejeitado: CR0","Rejeitado: CR0"
"An Industrial Study on the Risk of Software Changes",,"Shihab, Emad and Hassan, Ahmed E. and Adams, Bram and Jiang, Zhen Ming",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Traceability in the Wild: Automatically Augmenting Incomplete Trace Links",,"Rath, Michael and Rendall, Jacob and Guo, Jin L. C. and Cleland-Huang, Jane and M\""{a}der, Patrick",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"A Case Study on the Relationship Between Code Ownership and Refactoring Activities in a Java Software System","Refactoring, the activity of changing source code design without affecting its external behavior, is a widely used practice among developers, since it is considered to positively affect the quality of software systems. However, there are some ""human factors"" to be considered while performing refactoring, including developers knowledge of systems architecture. Recent studies showed how much ""people"" metrics, such as code ownership, might affect software quality as well. In this preliminary study we investigated the relationship between code ownership and refactoring activity performed by developers. This study can provide useful insights on who performs refactoring and help team leaders to properly manage human resources during software development.","Orr\'{u}, Matteo and Marchesi, Michele",2016,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Transfer Learning for Cross-Project Change-Proneness Prediction in Object-Oriented Software Systems: A Feasibility Analysis","Change-prone classes or modules are defined as regions of the source code which are more likely to change as a result of a software development of maintenance activity. Automatic identification of change-prone classes are useful for the software development team as they can focus their testing efforts on areas within the source code which are more likely to change. Several machine learning techniques have been proposed for predicting change-prone classes based on the application of source code metrics as indicators. However, most of the work has focused on within-project training and model building. There are several real word scenario in which sufficient training dataset is not available for model building such as in the case of a new project. Cross-project prediction is an approach which consists of training a model from dataset belonging to one project and testing it on dataset belonging to a different project. Cross-project change-proneness prediction is relatively unexplored.

We propose a machine learning based approach for cross-project change-proneness prediction. We conduct experiments on 10 open-source Eclipse plug-ins and demonstrate the effectiveness of our approach. We frame several research questions comparing the performance of within project and cross project prediction and also propose a Genetic Algorithm (GA) based approach for identifying the best set of source code metrics. We conclude that for within project experimental setting, Random Forest (RF) technique results in the best precision. In case of cross-project change-proneness prediction, our analysis reveals that the NDTF ensemble method performs higher than other individual classifiers (such as decision tree and logistic regression) and ensemble methods in the experimental dataset. We conduct a comparison of within-project, cross-project without GA and cross-project with GA and our analysis reveals that cross-project with GA performs best followed by within-project and then cross-project without GA.","Kumar, Lov and Behera, Ranjan Kumar and Rath, Santanu and Sureka, Ashish",2017,"[""ACM""]","Aceito: CA7","Aceito: CA7"
"An Empirical Investigation into Learning Bug-fixing Patches in the Wild via Neural Machine Translation",,"Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"A Supervised Machine Learning Approach to Classify Host Roles on Line Using sFlow",,"Li, Bingdong and Gunes, Mehmet Hadi and Bebis, George and Springer, Jeff",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"API Code Recommendation Using Statistical Learning from Fine-grained Changes","Learning and remembering how to use APIs is difficult. While code-completion tools can recommend API methods, browsing a long list of API method names and their documentation is tedious. Moreover, users can easily be overwhelmed with too much information. We present a novel API recommendation approach that taps into the predictive power of repetitive code changes to provide relevant API recommendations for developers. Our approach and tool, APIREC, is based on statistical learning from fine-grained code changes and from the context in which those changes were made. Our empirical evaluation shows that APIREC correctly recommends an API call in the first position 59% of the time, and it recommends the correct API call in the top five positions 77% of the time. This is a significant improvement over the state-of-the-art approaches by 30-160% for top-1 accuracy, and 10-30% for top-5 accuracy, respectively. Our result shows that APIREC performs well even with a one-time, minimal training dataset of 50 publicly available projects.","Nguyen, Anh Tuan and Hilton, Michael and Codoban, Mihai and Nguyen, Hoan Anh and Mast, Lily and Rademacher, Eli and Nguyen, Tien N. and Dig, Danny",2016,"[""ACM""]","Aceito: CA7, CA3","Aceito: CA3, CA7"
"Type-directed Diffing of Structured Data",,"Miraldo, Victor Cacciari and Dagand, Pierre-\'{E}variste and Swierstra, Wouter",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"GitcProc: A Tool for Processing and Classifying GitHub Commits",,"Casalnuovo, Casey and Suchak, Yagnik and Ray, Baishakhi and Rubio-Gonz\'{a}lez, Cindy",2017,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"Automatic Performance Testing Using Input-sensitive Profiling",,"Luo, Qi",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR0"
"Performance Search Engine Driven by Prior Knowledge of Optimization",,"Kim, Youngsung and \v{C}ern\'{y}, Pavol and Dennis, John",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Enhancing Visualizations in Pedagogical Debuggers by Leveraging on Code Analysis",,"Santos, Andr{\'e} L.",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Version Aware LibreOffice Documents",,"Pandey, Meenu and Munson, Ethan V.",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Linking User Requests, Developer Responses and Code Changes: Android OS Case Study",,"Licorish, Sherlock A. and Zolduoarrati, Elijah and Stanger, Nigel",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"The Use of Development History in Software Refactoring Using a Multi-objective Evolutionary Algorithm","One of the widely used techniques for evolving software systems is refactoring, a maintenance activity that improves design structure while preserving the external behavior. Exploring past maintenance and development history can be an effective way of finding refactoring opportunities. Code elements which undergo changes in the past, at approximately the same time, bear a good probability for being semantically related. Moreover, these elements that experienced a huge number of refactoring in the past have a good chance for refactoring in the future. In addition, the development history can be used to propose new refactoring solutions in similar contexts. In this paper, we propose a multi-objective optimization-based approach to find the best sequence of refactorings that minimizes the number of bad-smells, and maximizes the use of development history and semantic coherence. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-off between these three objectives. We report the results of our experiments using different large open source projects.","Ouni, Ali and Kessentini, Marouane and Sahraoui, Houari and Hamdi, Mohamed Salah",2013,"[""ACM""]","Aceito: CA0, CA3","Aceito: CA3"
"Panel: Future Directions of Block-based Programming",,"Brown, Neil C.C. and M\""{o}nig, Jens and Bau, Anthony and Weintrop, David",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Recommending a Starting Point for a Programming Task: An Initial Investigation",,"Thompson, C. Albert and Murphy, Gail C.",2014,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"A Visual MapReduce Program Development Environment for Heterogeneous Computing on Clouds",,"Liang, Tyng-Yeu and Yeh, Li-Wei and Wu, Chi-Hong",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Empirical Analysis of Performance Problems on Code Level","Performance problems are well known on architecture level. On code level their occurrences have not been systematically researched so far. Since a lot of everyday work of software developers is done on code level, methods and tools with focus on frequent performance problems are relevant. In the presented thesis, a method for systematically evaluating the occurrence and the frequency of performance problems on code level is presented and applied to repositories. The results of this empirical research will be a classification of performance problems and a quantification of their frequency. This will raise the awareness on certain problem classes for developers and will provide a basis for the development of new performance tools for preventing performance problems.","Reichelt, David Georg and K\""{u}hne, Stefan",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"ChangeLocator: Locate Crash-inducing Changes Based on Crash Reports",,"Wu, Rongxin and Wen, Ming and Cheung, Shing-Chi and Zhang, Hongyu",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Analyzing Conflict Predictors in Open-source Java Projects","In collaborative development environments integration conflicts occur frequently. To alleviate this problem, different awareness tools have been proposed to alert developers about potential conflicts before they become too complex. However, there is not much empirical evidence supporting the strategies used by these tools. Learning about what types of changes most likely lead to conflicts might help to derive more appropriate requirements for early conflict detection, and suggest improvements to existing conflict detection tools. To bring such evidence, in this paper we analyze the effectiveness of two types of code changes as conflict predictors. Namely, editions to the same method, and editions to directly dependent methods. We conduct an empirical study analyzing part of the development history of 45 Java projects from GitHub and Travis CI, including 5,647 merge scenarios, to compute the precision and recall for the conflict predictors aforementioned. Our results indicate that the predictors combined have a precision of 57.99% and a recall of 82.67%. Moreover, we conduct a manual analysis which provides insights about strategies that could further increase the precision and the recall.","Accioly, Paola and Borba, Paulo and Silva, L{\'e}uson and Cavalcanti, Guilherme",2018,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Identifying Bug-inducing Changes for Code Additions","Background. SZZ algorithm has been popularly used to identify bug-inducing changes in version history. It is still limited to link a fixing change to an inducing one, when the fix constitutes of code additions only. Goal. We improve the original SZZ by proposing a way to link the code additions in a fixing change to a list of candidate inducing changes. Method. The improved version, A-SZZ, finds the code block encapsulating the new code added in a fixing change, and traces back to the historical changes of the code block. We mined the GitHub repositories of two projects, Angular.js and Vue, and ran A-SZZ to identify bug-inducing changes of code additions. We evaluated the effectiveness of A-SZZ in terms of inducing and fixing ratios, and time span between the two changes. Results. The approach works well for linking code additions with previous changes, although it still produces many false positives. Conclusions. Nearly a quarter of the files in fixing changes contain code additions only, and hence, new heuristics should be implemented to link those with inducing changes in a more efficient way.","Sahal, Emre and Tosun, Ayse",2018,"[""ACM""]","Aceito: CA3, CA0","Aceito: CA3"
"History-sensitive Heuristics for Recovery of Features in Code of Evolving Program Families","Background. SZZ algorithm has been popularly used to identify bug-inducing changes in version history. It is still limited to link a fixing change to an inducing one, when the fix constitutes of code additions only. Goal. We improve the original SZZ by proposing a way to link the code additions in a fixing change to a list of candidate inducing changes. Method. The improved version, A-SZZ, finds the code block encapsulating the new code added in a fixing change, and traces back to the historical changes of the code block. We mined the GitHub repositories of two projects, Angular.js and Vue, and ran A-SZZ to identify bug-inducing changes of code additions. We evaluated the effectiveness of A-SZZ in terms of inducing and fixing ratios, and time span between the two changes. Results. The approach works well for linking code additions with previous changes, although it still produces many false positives. Conclusions. Nearly a quarter of the files in fixing changes contain code additions only, and hence, new heuristics should be implemented to link those with inducing changes in a more efficient way.","Nunes, Camila and Garcia, Alessandro and Lucena, Carlos and Lee, Jaejoon",2012,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Using Code Change Types in an Analogy-based Classifier for Short-term Defect Prediction","Current approaches for defect prediction usually analyze files (or modules) and their development as work is done on a given release, to predict post-release defects. What is missing is an approach for predicting bugs to be detected in a more short-term interval, even within the development of a particular version. In this paper, we propose a defect predictor that looks into change bursts in a given file, analyzing the number of changes and their types, and then predict whether the file is likely to have a bug found within the next 3 months after that change burst. An analogy-based classifier is used for this task: the prediction is made based on comparisons with similar change bursts that occurred in other files. New metrics are described to capture the change type of a file (e.g., small local change, massive change all in one place, multiple changes scattered throughout the file).","Tass{\'e}, Jos{\'e}e",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"From Problem Landscapes to Language Landscapes: Questions in Genetic Improvement","Managing and curating software is a time consuming process particularly as programming languages, libraries, and execution environments change. To support the engineering of software, we propose applying a GP-based continuous learning system to all ""useful"" software. We take the position that search-based itemization and analysis of all commonly used software is feasible, in large part, because the requirements that people place on software can be used to bound the search space to software which is of high practical use. By repeatedly reusing the information generated during the search process we hope to attain a higher-level, but also more rigorous, understanding of our engineering material - source code.","Cody-Kenny, Brendan and Fenton, Michael and O'Neill, Michael",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"What Makes a Code Change Easier to Review: An Empirical Investigation on Code Change Reviewability",,"Ram, Achyudh and Sawant, Anand Ashok and Castelluccio, Marco and Bacchelli, Alberto",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"A Supervised Learning Model for Identifying Inactive VMs in Private Cloud Data Centers",,"Kim, In Kee and Zeng, Sai and Young, Christopher and Hwang, Jinho and Humphrey, Marty",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Investigating Static Analysis Errors in Student Java Programs","Research on students learning to program has produced studies on both compile-time errors (syntax errors) and run-time errors (exceptions). Both of these types of errors are natural targets, since detection is built into the programming language. In this paper, we present an empirical investigation of static analysis errors present in syntactically correct code. Static analysis errors can be revealed by tools that examine a program's source code, but this error detection is typically not built into common programming languages and instead requires separate tools. Static analysis can be used to check formatting or commenting expectations, but it also can be used to identify problematic code or to find some kinds of conceptual or logic errors. We study nearly 10 million static analysis errors found in over 500 thousand program submissions made by students over a five-semester period. The study includes data from four separate courses, including a non-majors introductory course as well as the CS1/CS2/CS3 sequence for CS majors. We examine the differences between the error rates of CS major and non-major beginners, and also examine how these patterns change over time as students progress through the CS major course sequence. Our investigation shows that while formatting and Javadoc issues are the most common, static checks that identify coding flaws that are likely to be errors are strongly correlated with producing correct programs, even when students eventually fix the problems. With experience, students produce fewer errors, but the errors that are most frequent are consistent between both computer science majors and non-majors, and across experience levels. These results can highlight student struggles or misunderstandings that have escaped past analyses focused on syntax or run-time errors.","Edwards, Stephen H. and Kandru, Nischel and Rajagopal, Mukund B.M.",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Fast Transform Mode Decision for HEVC Screen Content Coding",,"Lee, Dokyung and Jeong, Jecheng",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"How Do Developers Resolve Merge Conflicts? An Investigation into the Processes, Tools, and Improvements",,"Brindescu, Caius",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Addressing Scalability in API Method Call Analytics",,"Cergani, Ervina and Proksch, Sebastian and Nadi, Sarah and Mezini, Mira",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Transform-data-by-example (TDE): An Extensible Search Engine for Data Transformations","Today, business analysts and data scientists increasingly need to clean, standardize and transform diverse data sets, such as name, address, date time, and phone number, before they can perform analysis. This process of data transformation is an important part of data preparation, and is known to be difficult and time-consuming for end-users.

Traditionally, developers have dealt with these longstanding transformation problems using custom code libraries. They have built vast varieties of custom logic for name parsing and address standardization, etc., and shared their source code in places like GitHub. Data transformation would be a lot easier for end-users if they can discover and reuse such existing transformation logic.

We developed Transform-Data-by-Example (TDE), which works like a search engine for data transformations. TDE ""indexes"" vast varieties of transformation logic in source code, DLLs, web services and mapping tables, so that users only need to provide a few input/output examples to demonstrate a desired transformation, and TDE can interactively find relevant functions to synthesize new programs consistent with all examples. Using an index of 50K functions crawled from GitHub and Stackoverflow, TDE can already handle many common transformations not currently supported by existing systems. On a benchmark with over 200 transformation tasks, TDE generates correct transformations for 72% tasks, which is considerably better than other systems evaluated. A beta version of TDE for Microsoft Excel is available via Office store1. Part of the TDE technology also ships in Microsoft Power BI.","He, Yeye and Chu, Xu and Ganjam, Kris and Zheng, Yudian and Narasayya, Vivek and Chaudhuri, Surajit",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"How Preprocessor Annotations (Do Not) Affect Maintainability: A Case Study on Change-proneness","Preprocessor annotations (e.g., #ifdef in C) enable the development of similar, but distinct software variants from a common code base. One particularly popular preprocessor is the C preprocessor, cpp. But the cpp is also widely criticized for impeding software maintenance by making code hard to understand and change. Yet, evidence to support this criticism is scarce. In this paper, we investigate the relation between cpp usage and maintenance effort, which we approximate with the frequency and extent of source code changes. To this end, we mined the version control repositories of eight open- source systems written in C. For each system, we measured if and how individual functions use cpp annotations and how they were changed. We found that functions containing cpp annotations are generally changed more frequently and more profoundly than other functions. However, when accounting for function size, the differences disappear or are greatly diminished. In summary, with respect to the frequency and extent of changes, our findings do not support the criticism of the cpp regarding maintainability.","Fenske, Wolfram and Schulze, Sandro and Saake, Gunter",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Clone-Slicer: Detecting Domain Specific Binary Code Clones Through Program Slicing",,"Xue, Hongfa and Venkataramani, Guru and Lan, Tian",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"A Real-time Framework for Detecting Efficiency Regressions in a Globally Distributed Codebase",,"Valdez-Vivas, Martin and Gocmen, Caner and Korotkov, Andrii and Fang, Ethan and Goenka, Kapil and Chen, Sherry",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Prediction and Ranking of Co-change Candidates for Clones",,"Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Automatic Improvement of Apache Spark Queries Using Semantics-preserving Program Reduction","Apache Spark is a popular framework for large-scale data analytics. Unfortunately, Spark's performance can be difficult to optimise, since queries freely expressed in source code are not amenable to traditional optimisation techniques. This article describes Hylas, a tool for automatically optimising Spark queries embedded in source code via the application of semantics-preserving transformations. The transformation method is inspired by functional programming techniques of ""deforestation"", which eliminate intermediate data structures from a computation. This contrasts with approaches defined entirely within structured query formats such as Spark SQL. Hylas can identify certain computationally expensive operations and ensure that performing them creates no superfluous data structures. This optimisation leads to significant improvements in execution time, with over 10,000 times improvement observed in some cases.","Kocsis, Zoltan A. and Drake, John H. and Carson, Douglas and Swan, Jerry",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Coding Dojo As a Transforming Practice in Collaborative Learning of Programming: An Experience Report",,"da R. Rodrigues, Peterson Luiz and Franz, Luiz Paulo and Cheiran, Jean Felipe P. and da Silva, Jo\~{a}o Pablo S. and Bordin, Andr{\'e}a S.",2017,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"A Method for Assessing Class Change Proneness","Change proneness is a quality characteristic of software artifacts that represents their probability to change in the future due to: (a) evolving requirements, (b) bug fixing, or (c) ripple effects. In the literature, change proneness has been associated with many negative consequences along software evolution. For example, artifacts that are change-prone tend to produce more defects, and accumulate more technical debt. Therefore, identifying and monitoring modules of the system that are change-prone is of paramount importance. Assessing change proneness requires information from two sources: (a) the history of changes in the artifact as a proxy of how frequently the artifact itself is changing, and (b) the source code structure that affects the probability of a change being propagated among artifacts. In this paper, we propose a method for assessing the change proneness of classes based on the two aforementioned information sources. To validate the proposed approach, we performed a case study on five open-source projects. Specifically, we compared the accuracy of the proposed approach to the use of other software metrics and change history to assess change proneness, based on the 1061-1998 IEEE Standard on Software Measurement. The results of the case study suggest that the proposed method is the most accurate and reliable assessor of change proneness. The high accuracy of the method suggests that the method and accompanying tool can effectively aid practitioners during software maintenance and evolution.","Arvanitou, Elvira-Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Avgeriou, Paris",2017,"[""ACM"",""Engineering Village""]","Aceito: CA0, CA1","Aceito: CA1"
"Collaborative Creation and Versioning of Modeling Languages with MetaEdit+",,"Kelly, Steven and Tolvanen, Juha-Pekka",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10"
"Inferring Crypto API Rules from Code Changes",,"Paletov, Rumen and Tsankov, Petar and Raychev, Veselin and Vechev, Martin",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Discovering Maintainability Changes in Large Software Systems","In this paper we propose an approach to automatically discover meaningful changes to maintainability of applications developed using object oriented programming languages. Our approach consists of an algorithm that employs the values of several class-level software metrics that can be easily obtained using open source software. Based on these values, a score that illustrates the maintainability change between two versions of the system is calculated. We present relevant related work, together with the state of research regarding the link between software metrics and maintainability for object oriented systems. In order to validate the approach, we undertake a case study that covers the entire development history of the jEdit open source text editor. We consider 41 version pairs that are assessed for changes to maintainability. First, a manual tool assisted examination of the source code was performed, followed by calculating the Maintainability Index for each application version. In the last step, we apply the proposed approach and compare the findings with those of the manual examination as well as those obtained using the Maintainability Index. In the final section, we present the identified issues and propose future work to further fine tune the approach.","Molnar, Arthur and Motogna, Simona",2017,"[""ACM""]","Aceito: CA6","Aceito: CA3"
"Improving Understanding of Dynamically Typed Software Developed by Agile Practitioners",,"Garc\'{\i}a, Jair and Garc{\'e}s, Kelly",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Preliminary Product Line Support in BitKeeper",,"McVoy, Larry",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Identifying HPC Codes via Performance Logs and Machine Learning",,"DeMasi, Orianna and Samak, Taghrid and Bailey, David H.",2013,"[""ACM""]","Rejeitado: CR7","Rejeitado: CR0"
"Process Mining Multiple Repositories for Software Defect Resolution from Control and Organizational Perspective","Issue reporting and resolution is a software engineering process supported by tools such as Issue Tracking System (ITS), Peer Code Review (PCR) system and Version Control System (VCS). Several open source software projects such as Google Chromium and Android follow process in which a defect or feature enhancement request is reported to an issue tracker followed by source-code change or patch review and patch commit using a version control system. We present an application of process mining three software repositories (ITS, PCR and VCS) from control flow and organizational perspective for effective process management. ITS, PCR and VCS are not explicitly linked so we implement regular expression based heuristics to integrate data from three repositories for Google Chromium project. We define activities such as bug reporting, bug fixing, bug verification, patch submission, patch review, and source code commit and create an event log of the bug resolution process. The extracted event log contains audit trail data such as caseID, timestamp, activity name and performer. We discover runtime process model for bug resolution process spanning three repositories using process mining tool, Disco, and conduct process performance and efficiency analysis. We identify bottlenecks, define and detect basic and composite anti-patterns. In addition to control flow analysis, we mine event log to perform organizational analysis and discover metrics such as handover of work, subcontracting, joint cases and joint activities.","Gupta, Monika and Sureka, Ashish and Padmanabhuni, Srinivas",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"In-Situ Visualisation of Fractional Code Ownership over Time","The term code ownership is used in software engineering to describe who authored a certain piece of software. Code ownership is commonly determined by investigating the data from version control systems, defining the owner as the person who contributed the most lines to a file, module, etc. Existing visualisation for ownership usually relies on per-line annotations from the version control system, thus only conveying the information who changed a line the last time, potentially adding some visual cue about how old the respective change is. This, however, can be misleading, because any change of even a single character changes ownership. In this paper, we propose a visualisation that accounts for fractional ownership changes over time on a per-character basis. Our technique incorporates visual cues to convey that typical definitions of ownership have an inherent uncertainty and provides details on the cause of this uncertainty on demand. For our definition of code ownership being a low-level one, we propose implementing the visualisation as an in-situ visualisation in the code editor of modern development environments. We will show examples of the efficacy of our approach and discuss its advantages and disadvantages compared to conventional line-based ownership.","M\""{u}ller, Christoph and Reina, Guido and Ertl, Thomas",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Graph-based Approach for Detecting Impure Refactoring from Version Commits",,"Tsutsumi, Shogo and Choi, Eunjong and Yoshida, Norihiro and Inoue, Katsuro",2016,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR0"
"Reinforcement Learning for Automatic Test Case Prioritization and Selection in Continuous Integration",,"Spieker, Helge and Gotlieb, Arnaud and Marijan, Dusica and Mossige, Morten",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Transform-Data-by-Example (TDE): Extensible Data Transformation in Excel","Business analysts and data scientists today increasingly need to clean, standardize and transform diverse data sets, such as name, address, date time, phone number, etc., before they can perform analysis. These ad-hoc transformation problems are typically solved by one-off scripts, which is both difficult and time-consuming.

Our observation is that these domain-specific transformation problems have long been solved by developers with code libraries, which are often shared in places like GitHub. We thus develop an extensible data transformation system called Transform-Data-by-Example (TDE) that can leverage rich transformation logic in source code, DLLs, web services and mapping tables, so that end-users only need to provide a few (typically 3) input/output examples, and TDE can synthesize desired programs using relevant transformation logic from these sources. The beta version of TDE was released in Office Store for Excel.","He, Yeye and Ganjam, Kris and Lee, Kukjin and Wang, Yue and Narasayya, Vivek and Chaudhuri, Surajit and Chu, Xu and Zheng, Yudian",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Optimistic Loop Optimization",,"Doerfert, Johannes and Grosser, Tobias and Hack, Sebastian",2017,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Codemotion: Expanding the Design Space of Learner Interactions with Computer Programming Tutorial Videos",,"Khandwala, Kandarp and Guo, Philip J.",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Modern Version Control: Creating an Efficient Development Ecosystem","In 2011, Santa Clara University School of Law Technology and Academic Computing (LTAC) identified that its version control system could greatly benefit from the use of modern source control management software. Source code for high value projects, such as the Santa Clara Law website, were previously held in a Subversion (SVN) repository in a client-server model, providing version control and redundancy. Because of the resource footprint associated with SVN, only projects with high importance could be setup with version control. As more web-based applications were introduced, the need for a more efficient revision control system arose. Git, a highly efficient decentralized version control system (DVCS), was selected after evaluating similar technologies. This change transformed the entire development process, making the development cycle more streamlined and with greater flexibility. In the early use of Git, LTAC also discovered its use as a deployment tool, increasing redundancy on servers and reducing overhead usually associated with revision control. It also serves as the vital link between LTAC's issue tracking system, Redmine, and the development team. The introduction of Redmine has helped LTAC monitor website issues, manage projects, and continually review changes to the code base. LTAC has created a development ecosystem that provides redundancy and accountability using open source products that carry no cost. Git has significant performance gains over SVN, making its integration and use less frustrating and distracting for developers. Redmine gives developers and customers the opportunity to organize, track, and resolve issues. The flexibility of the technology used means that any project, from a content management system to a one-off script, can benefit from source control without large costs or long deployment times.","Bertino, Nic",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Multimodal Analysis of Client Behavioral Change Coding in Motivational Interviewing",,"Aswamenakul, Chanuwas and Liu, Lixing and Carey, Kate B. and Woolley, Joshua and Scherer, Stefan and Borsari, Brian",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Assessing the Threat of Untracked Changes in Software Evolution","While refactoring is extensively performed by practitioners, many Mining Software Repositories (MSR) approaches do not detect nor keep track of refactorings when performing source code evolution analysis. In the best case, keeping track of refactorings could be unnecessary work; in the worst case, these untracked changes could significantly affect the performance of MSR approaches. Since the extent of the threat is unknown, the goal of this paper is to assess whether it is significant. Based on an extensive empirical study, we answer positively: we found that between 10 and 21% of changes at the method level in 15 large Java systems are untracked. This results in a large proportion (25%) of entities that may have their histories split by these changes, and a measurable effect on at least two MSR approaches. We conclude that handling untracked changes should be systematically considered by MSR studies.","Hora, Andre and Silva, Danilo and Valente, Marco Tulio and Robbes, Romain",2018,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"A Scalable and Efficient Approach for Compiling and Analyzing Commit History",,"Behnamghader, Pooyan and Meemeng, Patavee and Fostiropoulos, Iordanis and Huang, Di and Srisopha, Kamonphop and Boehm, Barry",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Integration of Existing Software Artifacts into a View- and Change-Driven Development Approach","For the description of complex software systems, a variety of different languages can be used, e.g., architecture diagrams, behavior models, source code, etc. Maintaining these artifacts is a complex task, if the contained information is not completely disjoint. There are approaches that keep such redundancies consistent by propagating changes. This is, however, only possible if change descriptions are recorded or provided. Therefore, these change-driven approaches cannot be used to maintain existing software projects, which did not record all changes during development. This results in the need of developing a possibility for the integration of said projects. We therefore propose two different strategies for integrating existing artifacts into a change-driven approach: Our first strategy creates change histories for models, while the second strategy builds correspondences from given trace links and their attached artifacts. Furthermore, we suggest a number of criteria, which determine which strategy is best used, and when. In this paper, we applied our strategies to architectural models and object-oriented source code and used a reverse engineering tool to create the necessary trace links. We evaluated the applicability of our approach with several case studies, containing five real world architectural models and five medium-sized Java projects.","Leonhardt, Sven and Hettwer, Benjamin and Hoor, Johannes and Langhammer, Michael",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"How Does Regression Test Prioritization Perform in Real-world Software Evolution?",,"Lu, Yafeng and Lou, Yiling and Cheng, Shiyang and Zhang, Lingming and Hao, Dan and Zhou, Yangfan and Zhang, Lu",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Why Did This Code Change?",,"Rastkar, Sarah and Murphy, Gail C.",2013,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Supporting Continuous Integration by Code-churn Based Test Selection",,"Knauss, Eric and Staron, Miroslaw and Meding, Wilhelm and S\""{o}der, Ola and Nilsson, Agneta and Castell, Magnus",2015,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"A Game-based Learning Approach to Road Safety: The Code of Everand",,"Dunwell, Ian and de Freitas, Sara and Petridis, Panagiotis and Hendrix, Maurice and Arnab, Sylvester and Lameras, Petros and Stewart, Craig",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Revisiting Context-based Code Smells Prioritization: On Supporting Referred Context",,"Sae-Lim, Natthawute and Hayashi, Shinpei and Saeki, Motoshi",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR0"
"Demo",,"Holtz, Jarrett and Guha, Arjun and Biswas, Joydeep",2018,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"How the Evolution of Emerging Collaborations Relates to Code Changes: An Empirical Study",,"Panichella, Sebastiano and Canfora, Gerardo and Di Penta, Massimiliano and Oliveto, Rocco",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR0"
"Structural and Visual Comparisons for Web Page Archiving",,"Law, Marc Teva and Thome, Nicolas and Gan\c{c}arski, St{\'e}phane and Cord, Matthieu",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Language Model Rest Costs and Space-efficient Storage",,"Heafield, Kenneth and Koehn, Philipp and Lavie, Alon",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"A Code Morphing Methodology to Automate Power Analysis Countermeasures","We introduce a general framework to automate the application of countermeasures against Differential Power Attacks aimed at software implementations of cryptographic primitives. The approach enables the generation of multiple versions of the code, to prevent an attacker from recognizing the exact point in time where the observed operation is executed and how such operation is performed. The strategy increases the effort needed to retrieve the secret key through hindering the formulation of a correct hypothetical consumption to be correlated with the power measurements. The experimental evaluation shows how a DPA attack against OpenSSL AES implementation on an industrial grade ARM-based SoC is hindered with limited performance overhead.","Agosta, Giovanni and Barenghi, Alessandro and Pelosi, Gerardo",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Modular Capture Avoidance for Program Transformations",,"Ritschel, Nico and Erdweg, Sebastian",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Version Control Graphical Interface for Open OnDemand",,"Chen, Huan and Fietkiewicz, Chris",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Querying Sequential Software Engineering Data",,"Sun, Chengnian and Zhang, Haidong and Lou, Jian-Guang and Zhang, Hongyu and Wang, Qiang and Zhang, Dongmei and Khoo, Siau-Cheng",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Barriers Faced by Coding Bootcamp Students",,"Thayer, Kyle and Ko, Andrew J.",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Variable Provenance in Software Systems","Data Provenance is defined as lineage or history of the given dataitem. Knowing the source of the data or the transformation of the data-source to compute the given data is critical for analyzing the quality of the data. Many transformations of data are done in software (source code). We introduce and define the concept of Variable Provenance for source code. We argue that determining the origin(s) of the data held by a variable and the history of modifications of the variable can provide critical information along many dimensions about what happens in the source code. We use understanding of source code and creating business rules from source code as use-cases to illustrate our view-point. To compute the variable provenance, we combine program slicing techniques and operational rules associated with mathematical operators in computations to propagate the annotations. We predict that the solution to the problem of variable provenance can lead to many use-cases in the software engineering community, effective discovery of processes and business rules from existing systems, and powerful development, debugging and evolution techniques for the software industry.","Chittimalli, Pavan Kumar and Naik, Ravindra",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Transforming Programs and Tests in Tandem for Fault Localization",,"Li, Xia and Zhang, Lingming",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Search-based Refactoring Detection","We propose an approach to automate the detection of source code refactoring using structural information. Our approach takes as input a list of possible refactorings, a set of structural metrics and the initial and revised versions of the source code. It generates as output a sequence of detected changes in terms of refactorings. In this case, a solution is defined as the sequence of refactoring operations that minimizes the metrics variation between the revised version of the software and the version yielded by the application of the refactoring sequence to the initial version of the software. We use and adapt global and local heuristic search algorithms to explore the space of possible solutions.","Mahouachi, Rim and Kessentini, Marouane and \'{O} Cinn{\'e}ide, Mel",2013,"[""ACM""]","Aceito: CA4, CA6","Aceito: CA1, CA2, CA3, CA7"
"SARATHI: Characterization Study on Regression Bugs and Identification of Regression Bug Inducing Changes: A Case-Study on Google Chromium Project",,"Khattar, Manisha and Lamba, Yash and Sureka, Ashish",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"InTime: A Machine Learning Approach for Efficient Selection of FPGA CAD Tool Parameters",,"Kapre, Nachiket and Ng, Harnhua and Teo, Kirvy and Naude, Jaco",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Is It Possible to Automatically Port Kernel Modules?",,"Zhen, Yanjie and Zhang, Wei and Dai, Zhenyang and Mao, Junjie and Chen, Yu",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Automating Object Transformations for Dynamic Software Updating","Dynamic software updating (DSU) systems eliminate costly downtime by dynamically fixing bugs and adding features to executing programs. Given a static code patch, most DSU systems construct runtime code changes automatically. However, a dynamic update must also specify how to change the running program's execution state, e.g., the stack and heap, to make it compatible with the new code. Constructing such state transformations correctly and automatically remains an open problem. This paper presents a solution called Targeted Object Synthesis (TOS). TOS first executes the same tests on the old and new program versions separately, observing the program heap state at a few corresponding points. Given two corresponding heap states, TOS matches objects in the two versions using key fields that uniquely identify objects and correlate old and new-version objects. Given example object pairs, TOS then synthesizes the simplest-possible function that transforms an old-version object to its new-version counterpart. We show that TOS is effective on updates to four open-source server programs for which it generates non-trivial transformation functions that use conditionals, operate on collections, and fix memory leaks. These transformations help programmers understand their changes and apply dynamic software updates.","Magill, Stephen and Hicks, Michael and Subramanian, Suriya and McKinley, Kathryn S.",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"ADiJaC -- Automatic Differentiation of Java Classfiles","This work presents the current design and implementation of ADiJaC, an automatic differentiation tool for Java classfiles. ADiJaC uses source transformation to generate derivative codes in both the forward and the reverse modes of automatic differentiation. We describe the overall architecture of the tool and present various details and examples for each of the two modes of differentiation. We emphasize the enhancements that have been made over previous versions of ADiJaC and illustrate their influence on the generality of the tool and on the performance of the generated derivative codes. The ADiJaC tool has been used to generate derivatives for a variety of problems, including real-world applications. We evaluate the performance of such codes and compare it to derivatives generated by Tapenade, a well-established automatic differentiation tool for Fortran and C/C++. Additionally, we present a more detailed performance analysis of a real-world application. Apart from being the only general-purpose automatic differentiation tool for Java bytecode, we argue that ADiJaC’s features and performance are comparable to those of similar mature tools for other programming languages such as C/C++ or Fortran.","Slu\c{s}anschi, Emil I. and Dumitrel, Vlad",2016,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Evaluating Software Merge Quality",,"Mehdi, Ahmed-Nacer and Urso, Pascal and Charoy, Fran\c{c}ois",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"SOTorrent: Reconstructing and Analyzing the Evolution of Stack Overflow Posts",,"Baltes, Sebastian and Dumani, Lorik and Treude, Christoph and Diehl, Stephan",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"An Unsupervised Distance Learning Framework for Multimedia Retrieval",,"Valem, Lucas Pascotti and Pedronette, Daniel Carlos Guimar\~{a}es",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"The Past, Present and Future of Technical Debt: Learning from the Past to Prepare for the Future",,"Woods, Eoin",2018,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR0"
"Locality-aware Mapping and Scheduling for Multicores",,"Ding, Wei and Kandemir, Mahmut and Yedlapalli, Praveen and Zhang, Yuanrui and Srinivas, Jithendra",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Embrace Your Issues: Compassing the Software Engineering Landscape Using Bug Reports",,"Borg, Markus",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Seeking the Ground Truth: A Retroactive Study on the Evolution and Migration of Software Libraries",,"Cossette, Bradley E. and Walker, Robert J.",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Verification Modulo Versions: Towards Usable Verification",,"Logozzo, Francesco and Lahiri, Shuvendu K. and F\""{a}hndrich, Manuel and Blackshear, Sam",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Virtual Textual Model Composition for Supporting Versioning and Aspect-orientation","The maintenance of modern systems often requires developers to perform complex and error-prone cognitive tasks, which are caused by the obscurity, redundancy, and irrelevancy of code, distracting from essential maintenance tasks. Typical maintenance scenarios include multiple branches of code in repositories, which involves dealing with branch-interdependent changes, and aspects in aspect-oriented development, which requires in-depth knowledge of behavior-interdependent changes. Thus, merging branched files as well as validating the behavior of statically composed code requires developers to conduct exhaustive individual introspection.

In this work we present VirtualEdit for associative, commutative, and invertible model composition. It allows simultaneous editing of multiple model versions or variants through dynamically derived virtual models. We implemented the approach in terms of an open-source framework that enables multi-version editing and aspect-orientation by selectively focusing on specific parts of code, which are significant for a particular engineering task.

The VirtualEdit framework is evaluated based on its application to the most popular publicly available Xtext-based languages. Our results indicate that VirtualEdit can be applied to existing languages with reasonably low effort.","Bill, Robert and Neubauer, Patrick and Wimmer, Manuel",2017,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Minimizing the Cost of Iterative Compilation with Active Learning",,"Ogilvie, William F. and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh",2017,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Multiversioned Decoupled Access-execute: The Key to Energy-efficient Compilation of General-purpose Programs",,"Koukos, Konstantinos and Ekemark, Per and Zacharopoulos, Georgios and Spiliopoulos, Vasileios and Kaxiras, Stefanos and Jimborean, Alexandra",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Identifying Hotspots in a Program for Data Parallel Architecture: An Early Experience",,"Sarkar, Santonu and Maltouf, Mageri Filali",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR0"
"Portable Mapping of Data Parallel Programs to OpenCL for Heterogeneous Systems",,"O'Boyle, Michael F.  P. and Wang, Zheng and Grewe, Dominik",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Living in Parallel Realities: Co-Existing Schema Versions with a Bidirectional Database Evolution Language",,"Herrmann, Kai and Voigt, Hannes and Behrend, Andreas and Rausch, Jonas and Lehner, Wolfgang",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"How Much Really Changes?: A Case Study of Firefox Version Evolution Using a Clone Detector",,"Lavoie, Thierry and Merlo, Ettore",2013,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR0"
"What Can Changes Tell About Software Processes?",,"Russo, Barbara and Steff, Maximilian",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"The Code Time Machine","Exploring and analyzing the history of changes is an intrinsic part of software evolution comprehension. Existing tools that exploit the data residing in version control repositories provide only limited support for the intuitive navigation of code changes from a historical perspective. We present the Code Time Machine, a lightweight IDE plugin which uses visualization techniques to depict the history of any chosen file augmented with information mined from the underlying versioning system. Inspired by Apple's Time Machine, our tool allows both developers and the system itself to seamlessly move through time. A video of the Code Time Machine can be found at https://youtu.be/meblwFO95oA.","Aghajani, Emad and Mocci, Andrea and Bavota, Gabriele and Lanza, Michele",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR12"
"Industrial Experience with the Migration of Legacy Models Using a DSL",,"Schuts, Mathijs and Hooman, Jozef and Tielemans, Paul",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Ant Build Maintenance with Formiga",,"Hardt, Ryan and Munson, Ethan V.",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Patch Verification via Multiversion Interprocedural Control Flow Graphs",,"Le, Wei and Pattison, Shannon D.",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Development Context Driven Change Awareness and Analysis Framework","Recent work on workspace monitoring allows conflict pre- diction early in the development process, however, these approaches mostly use syntactic differencing techniques to compare different program versions. In contrast, traditional change-impact analysis techniques analyze related versions of the program only after the code has been checked into the master repository. We propose a novel approach, DeCAF (Development Context Analysis Framework), that leverages the development context to scope a change impact analysis technique. The goal is to characterize the impact of each developer on other developers in the team. There are various client applications such as task prioritization, early conflict detection, and providing advice on testing that can benefit from such a characterization. The DeCAF frame- work leverages information from the development context to bound the iDiSE change impact analysis technique to analyze only the parts of the code base that are of interest. Bounding the analysis can enable DeCAF to efficiently com- pute the impact of changes using a combination of program dependence and symbolic execution based approaches.","Sarma, Anita and Branchaud, Josh and Dwyer, Matthew B. and Person, Suzette and Rungta, Neha",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR11"
"What Change History Tells Us About Thread Synchronization",,"Gu, Rui and Jin, Guoliang and Song, Linhai and Zhu, Linjie and Lu, Shan",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Hacking and Making at Time-Bounded Events: Current Trends and Next Steps in Research and Event Design",,"Filippova, Anna and Chapman, Brad and Geiger, R. Stuart and Herbsleb, James D. and Kalyanasundaram, Arun and Trainer, Erik and Moser, Aurelia and Stoltzfus, Arlin",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"The 2Nd Workshop on Hacking and Making at Time-Bounded Events: Current Trends and Next Steps in Research and Event Design",,"Pe Than, Ei Pa Pa and Herbsleb, James and Nolte, Alexander and Gerber, Elizabeth and Fiore-Gartland, Brittany and Chapman, Brad and Moser, Aurelia and Wilkins-Diehr, Nancy",2018,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"On the Diffuseness and the Impact on Maintainability of Code Smells: A Large Scale Empirical Investigation",,"Palomba, Fabio and Bavota, Gabriele and Di Penta, Massimiliano and Fasano, Fausto and Oliveto, Rocco and De Lucia, Andrea",2018,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR0"
"DeepBugs: A Learning Approach to Name-based Bug Detection","Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents DeepBugs, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 JavaScript files yields bug detectors that have a high accuracy (between 89% and 95%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68% true positive rate) in real-world code.","Pradel, Michael and Sen, Koushik",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR7"
"Learning API Usages from Bytecode: A Statistical Approach",,"Nguyen, Tam The and Pham, Hung Viet and Vu, Phong Minh and Nguyen, Tung Thanh",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"An Exploratory Study of Contribution Barriers Experienced by Newcomers to Open Source Software Projects",,"Hannebauer, Christoph and Book, Matthias and Gruhn, Volker",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Injecting Mechanical Faults to Localize Developer Faults for Evolving Software","This paper presents a novel methodology for localizing faults in code as it evolves. Our insight is that the essence of failure-inducing edits made by the developer can be captured using mechanical program transformations (e.g., mutation changes). Based on the insight, we present the FIFL framework, which uses both the spectrum information of edits (obtained using the existing FaultTracer approach) as well as the potential impacts of edits (simulated by mutation changes) to achieve more accurate fault localization. We evaluate FIFL on real-world repositories of nine Java projects ranging from 5.7KLoC to 88.8KLoC. The experimental results show that FIFL is able to outperform the state-of-the-art FaultTracer technique for localizing failure-inducing program edits significantly. For example, all 19 FIFL strategies that use both the spectrum information and simulated impact information for each edit outperform the existing FaultTracer approach statistically at the significance level of 0.01. In addition, FIFL with its default settings outperforms FaultTracer by 2.33% to 86.26% on 16 of the 26 studied version pairs, and is only inferior than FaultTracer on one version pair.","Zhang, Lingming and Zhang, Lu and Khurshid, Sarfraz",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Regressing Towards Simpler Prediction Systems","This talk will focus on our experience in managing the complexity of Sibyl, a large scale machine learning system that is widely used within Google. We believe that a large fraction of the challenges faced by Sibyl are inherent to large scale production machine learning and that other production systems are likely to encounter them as well [1]. Thus, these challenges present interesting opportunities for future research. The Sibyl system is complex for a number of reasons. We have learnt that a complete end-to-end machine learning solution has to have subsystems to address a variety of different needs: data ingestion, data analysis, data verification, experimentation, model analysis, model serving, configuration, data transformations, support for different kinds of loss functions and modeling, machine learning algorithm implementations, etc. Machine learning algorithms themselves constitute a relatively small fraction of the overall system.

Each subsystem consists of a number of distinct components to support the variety of product needs. For example, Sibyl supports more than 5 different model serving systems, each with its own idiosyncrasies and challenges. In addition, Sibyl configuration contains more lines of code than the core Sibyl learner itself. Finally existing solutions for some of the challenges don't feel adequate and we believe these challenges present opportunities for future research.

Though the overall system is complex, our users need to be able to deploy solutions quickly. This is because a machine learning deployment is typically an iterative process of model improvements. At each iteration, our users experiment with new features, find those that improve the model's prediction capability, and then ""launch"" a new model with those improved features. A user may go through 10 or more such productive launches. Not only is speed of iteration crucial to our users, but they are often willing to sacrifice the improved prediction quality of a high quality but cumbersome system for the speed of iteration of a lower quality but nimble system.

In this talk I will give an example of how simplification drives systems design and sometimes the design of novel algorithms.","Chandra, Tushar",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Coding of Non-stationary Sources As a Foundation for Detecting Change Points and Outliers in Binary Time-series",,"Sunehag, Peter and Shao, Wen and Hutter, Marcus",2012,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Coaster: Teaching Computer Graphics Incrementally (Abstract Only)",,"Lewis, Robert R.",2015,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Avoiding Useless Mutants","Mutation testing is a program-transformation technique that injects artificial bugs to check whether the existing test suite can detect them. However, the costs of using mutation testing are usually high, hindering its use in industry. Useless mutants (equivalent and duplicated) contribute to increase costs. Previous research has focused mainly on detecting useless mutants only after they are generated and compiled. In this paper, we introduce a strategy to help developers with deriving rules to avoid the generation of useless mutants. To use our strategy, we pass as input a set of programs. For each program, we also need a passing test suite and a set of mutants. As output, our strategy yields a set of useless mutants candidates. After manually confirming that the mutants classified by our strategy as ""useless"" are indeed useless, we derive rules that can avoid their generation and thus decrease costs. To the best of our knowledge, we introduce 37 new rules that can avoid useless mutants right before their generation. We then implement a subset of these rules in the MUJAVA mutation testing tool. Since our rules have been derived based on artificial and small Java programs, we take our MUJAVA version embedded with our rules and execute it in industrial-scale projects. Our rules reduced the number of mutants by almost 13% on average. Our results are promising because (i) we avoid useless mutants generation; (ii) our strategy can help with identifying more rules in case we set it to use more complex Java programs; and (iii) our MUJAVA version has only a subset of the rules we derived.","Fernandes, Leonardo and Ribeiro, M\'{a}rcio and Carvalho, Luiz and Gheyi, Rohit and Mongiovi, Melina and Santos, Andr{\'e} and Cavalcanti, Ana and Ferrari, Fabiano and Maldonado, Jos{\'e} Carlos",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Automatic and Portable Mapping of Data Parallel Programs to OpenCL for GPU-Based Heterogeneous Systems",,"Wang, Zheng and Grewe, Dominik and O'boyle, Michael F. P.",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"MJ No More: Using Concurrent Wikipedia Edit Spikes with Social Network Plausibility Checks for Breaking News Detection",,"Steiner, Thomas and van Hooland, Seth and Summers, Ed",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"HireBuild: An Automatic Approach to History-driven Repair of Build Scripts",,"Hassan, Foyzul and Wang, Xiaoyin",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"QR Code Based Image Steganography via Variable Step Size Firefly Algorithm and Lifting Wavelet Transform",,"Raja, P. M. Siva and Baburaj, E.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Flexible Access Control for Javascript","Providing security guarantees for systems built out of untrusted components requires the ability to define and enforce access control policies over untrusted code. In Web 2.0 applications, JavaScript code from different origins is often combined on a single page, leading to well-known vulnerabilities. We present a security infrastructure which allows users and content providers to specify access control policies over subsets of a JavaScript program by leveraging the concept of delimited histories with revocation. We implement our proposal in WebKit and evaluate it with three policies on 50 widely used websites with no changes to their JavaScript code and report performance overheads and violations.","Richards, Gregor and Hammer, Christian and Zappa Nardelli, Francesco and Jagannathan, Suresh and Vitek, Jan",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Code Transformations Based on Speculative SDC Scheduling",,"Lattuada, Marco and Ferrandi, Fabrizio",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"App Making for Pro-Social and Environmental Change at an Equity-Oriented Makeathon","The purpose of this paper is to analyze the effectiveness of an equity-oriented Makeathon designed to foster maker mindsets and maker identities in high school girls by engaging them with diverse tools, materials, mentorship, collaborative prototyping, and creative computing. The central theme of the University of British Columbia (UBC) Girls' Makeathon emphasized Tech for Change. Teams were challenged to make a mobile app related to an issue that teen girls face in local or global communities throughout the world. Participants learned how to identify a complex problem, design effective solutions, and change the world for the better by communicating their ideas (via apps and a pitch to a panel of experts). The results from the girls' design experiences, team presentations, surveys, and interviews show gains in their understandings of greening making and pro-social change making. Findings underscore the matter concerning how, why, and where do girls learn to become makers, coders, and inventors of media and technology (thereby overturning traditional gender and generational stereotypes)? The lessons learned in empowering girls to tackle inequality and create the futures they want to be part of will be useful for teachers and researchers who are interested in working with youth to design apps and innovative maker activities for environmental and sustainable education, research, and/or outreach.","MacDowell, Paula and Ralph, Rachel and Ng, David",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Feature-based Object Identification for Web Automation",,"Herzog, Christoph and Kordomatis, Iraklis and Holzinger, Wolfgang and Fayzrakhmanov, Ruslan R. and Kr\""{u}pl-Sypien, Bernhard",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Understanding Sequence Conservation With Deep Learning",,"Li, Yi and Quang, Daniel and Xie, Xiaohui",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Software Developers Are Humans, Too!",,"Vasilescu, Bogdan",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Maximizing Correctness with Minimal User Effort to Learn Data Transformations","Data transformation often requires users to write many trivial and task-dependent programs to transform thousands of records. Recently, programming-by-example (PBE) approaches enable users to transform data without coding. A key challenge of these PBE approaches is to deliver correctly transformed results on large datasets, since these transformation programs are likely to be generated by non-expert users. To address this challenge, existing approaches aim to identify a small set of potentially incorrect records and ask users to examine these records instead of the entire dataset. However, because the transformation scenarios are highly task-dependent, existing approaches cannot capture the incorrect records for various scenarios. We present a approach that learns from past transformation scenarios to generate a meta-classifier to identify the incorrect records. Our approach color-codes these transformed records and then presents them for users to examine. The method allows users to either enter an example for a record transformed incorrectly or confirm the correctness of a transformed record. And our approach can learn from the users' labels to refine the meta-classifier to accurately identify the incorrect records. Simulation results and a user study show that our method can identify the incorrectly transformed records and reduce the user efforts in examining the results.","Wu, Bo and Knoblock, Craig A.",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"On-chip Sparse Learning with Resistive Cross-point Array Architecture",,"Yu, Shimeng and Cao, Yu",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Parameterized BLOSUM Matrices for Protein Alignment",,"Song, Dandan and Chen, Jiaxing and Chen, Guang and Li, Ning and Li, Jin and Fan, Jun and Bu, Dongbo and Li, Shuai Cheng",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"An Empirical Comparison of the Development History of Cloudstack and Eucalyptus","Open source cloud computing solutions, such as CloudStack and Eucalyptus, have become increasingly popular in recent years. Despite this popularity, a better understanding of the factors influencing user adoption is still under active research. For example, increased project agility may lead to solutions that remain competitive in a rapidly evolving market, while keeping the software quality under control. Like any software system that is subject to frequent evolution, cloud computing solutions are subject to errors and quality problems, which may affect user experience and require frequent bug fixes. While prior comparisons of cloud platforms have focused most often on their provided services and functionalities, the current paper provides an empirical comparison of CloudStack and Eucalyptus, focusing on quality-related software development aspects. More specifically, we study the change history of the source code and its unit tests, as well as the history of bugs in the Jira issue tracker. We found that CloudStack has a high and more rapidly increasing test coverage than Eucalyptus. CloudStack contributors are more likely to participate in development and testing. We also observed differences between both projects pertaining to the bug life cycle and bug fixing time.","Zerouali, Ahmed and Mens, Tom",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Attributing Authorship of Revisioned Content",,"de Alfaro, Luca and Shavlovsky, Michael",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An Evaluation of Policy Frameworks for Addressing Ethical Considerations in Learning Analytics","Higher education institutions have collected and analysed student data for years, with their focus largely on reporting and management needs. A range of institutional policies exist which broadly set out the purposes for which data will be used and how data will be protected. The growing advent of learning analytics has seen the uses to which student data is put expanding rapidly. Generally though the policies setting out institutional use of student data have not kept pace with this change.

Institutional policy frameworks should provide not only an enabling environment for the optimal and ethical harvesting and use of data, but also clarify: who benefits and under what conditions, establish conditions for consent and the de-identification of data, and address issues of vulnerability and harm. A directed content analysis of the policy frameworks of two large distance education institutions shows that current policy frameworks do not facilitate the provision of an enabling environment for learning analytics to fulfil its promise.","Prinsloo, Paul and Slade, Sharon",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"DeepSim: Deep Learning Code Functional Similarity","Measuring code similarity is fundamental for many software engineering tasks, e.g., code search, refactoring and reuse. However, most existing techniques focus on code syntactical similarity only, while measuring code functional similarity remains a challenging problem. In this paper, we propose a novel approach that encodes code control flow and data flow into a semantic matrix in which each element is a high dimensional sparse binary feature vector, and we design a new deep learning model that measures code functional similarity based on this representation. By concatenating hidden representations learned from a code pair, this new model transforms the problem of detecting functionally similar code to binary classification, which can effectively learn patterns between functionally similar code with very different syntactics.

We have implemented our approach, DeepSim, for Java programs and evaluated its recall, precision and time performance on two large datasets of functionally similar code. The experimental results show that DeepSim significantly outperforms existing state-of-the-art techniques, such as DECKARD, RtvNN, CDLH, and two baseline deep neural networks models.","Zhao, Gang and Huang, Jeff",2018,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Coarse Grain Parallelization of Deep Neural Networks",,"Tallada, Marc Gonzalez",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Learning compact hashing codes for efficient tag completion and prediction",,"Wang, Qifan and Ruan, Lingyun and Zhang, Zhiwei and Si, Luo",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A High-level Framework for Parallelizing Legacy Applications for Multiple Platforms",,"Arora, Ritu and Capetillo, Ejenio and Bangalore, Purushotham and Mernik, Marjan",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Project Centralization Based on Graph Coloring",,"Ma, Lei and Artho, Cyrille and Sato, Hiroyuki",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Safely Managing Data Variety in Big Data Software Development",,"Cerqueus, Thomas and de Almeida, Eduardo Cunha and Scherzinger, Stefanie",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Learning Agile Software Engineering Practices Using Coding Dojo",,"Heinonen, Kenny and Hirvikoski, Kasper and Luukkainen, Matti and Vihavainen, Arto",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Refining Interprocedural Change-impact Analysis Using Equivalence Relations","Change-impact analysis (CIA) is the task of determining the set of program elements impacted by a program change. Precise CIA has great potential to avoid expensive testing and code reviews for (parts of) changes that are refactorings (semantics-preserving). However most statement-level CIA techniques suffer from imprecision as they do not incorporate the semantics of the change.

We formalize change impact in terms of the trace semantics of two program versions. We show how to leverage equivalence relations to make dataflow-based CIA aware of the change semantics, thereby improving precision in the presence of semantics-preserving changes. We propose an anytime algorithm that applies costly equivalence-relation inference incrementally to refine the set of impacted statements. We implemented a prototype and evaluated it on 322 real-world changes from open-source projects and benchmark programs used by prior research. The evaluation results show an average 35% improvement in the number of impacted statements compared to prior dataflow-based techniques.
We have implemented our approach, DeepSim, for Java programs and evaluated its recall, precision and time performance on two large datasets of functionally similar code. The experimental results show that DeepSim significantly outperforms existing state-of-the-art techniques, such as DECKARD, RtvNN, CDLH, and two baseline deep neural networks models.","Gyori, Alex and Lahiri, Shuvendu K. and Partush, Nimrod",2017,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"The Learning Curves in Open-Source Software (OSS) Development Network",,"Kim, Youngsoo and Jiang, Lingxiao",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Dynamically Adaptive Parsons Problems",,"Ericson, Barbara J.",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Ariadne: Analysis for Machine Learning Programs","The purpose of this paper is to analyze the effectiveness of an equity-oriented Makeathon designed to foster maker mindsets and maker identities in high school girls by engaging them with diverse tools, materials, mentorship, collaborative prototyping, and creative computing. The central theme of the University of British Columbia (UBC) Girls' Makeathon emphasized Tech for Change. Teams were challenged to make a mobile app related to an issue that teen girls face in local or global communities throughout the world. Participants learned how to identify a complex problem, design effective solutions, and change the world for the better by communicating their ideas (via apps and a pitch to a panel of experts). The results from the girls' design experiences, team presentations, surveys, and interviews show gains in their understandings of greening making and pro-social change making. Findings underscore the matter concerning how, why, and where do girls learn to become makers, coders, and inventors of media and technology (thereby overturning traditional gender and generational stereotypes)? The lessons learned in empowering girls to tackle inequality and create the futures they want to be part of will be useful for teachers and researchers who are interested in working with youth to design apps and innovative maker activities for environmental and sustainable education, research, and/or outreach.","Dolby, Julian and Shinnar, Avraham and Allain, Allison and Reinen, Jenna",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"How to Study Programming on Mobile Touch Devices: Interactive Python Code Exercises",,"Ihantola, Petri and Helminen, Juha and Karavirta, Ville",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Iridescent: A Tool for Rapid Semantic Annotation of Web Service Descriptions",,"Stavropoulos, Thanos G. and Vrakas, Dimitris and Vlahavas, Ioannis",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Multi-layered Approach for Recovering Links Between Bug Reports and Fixes","The links between the bug reports in an issue-tracking system and the corresponding fixing changes in a version repository are not often recorded by developers. Such linking information is crucial for research in mining software repositories in measuring software defects and maintenance efforts. However, the state-of-the-art bug-to-fix link recovery approaches still rely much on textual matching between bug reports and commit/change logs and cannot handle well the cases where their contents are not textually similar.

This paper introduces MLink, a multi-layered approach that takes into account not only textual features but also source code features of the changed code corresponding to the commit logs. It is also capable of learning the association relations between the terms in bug reports and the names of entities/components in the changed source code of the commits from the established bug-to-fix links, and uses them for link recovery between the reports and commits that do not share much similar texts. Our empirical evaluation on real-world projects shows that MLink can improve the state-of-the-art bug-to-fix link recovery methods by 11--18%, 13--17%, and 8--17% in F-score, recall, and precision, respectively.","Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Hoan Anh and Nguyen, Tien N.",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Techniques for Testing Scientific Programs Without an Oracle",,"Kanewala, Upulee and Bieman, James M.",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Does Refactoring Improve Software Structural Quality? A Longitudinal Study of 25 Projects","
Code smells in a program represent indications of structural quality problems, which can be addressed by software refactoring. Refactoring is widely practiced by developers, and considerable development effort has been invested in refactoring tooling support. There is an explicit assumption that software refactoring improves the structural quality of a program by reducing its density of code smells. However, little has been reported about whether and to what extent developers successfully remove code smells through refactoring. This paper reports a first longitudinal study intended to address this gap. We analyze how often the commonly-used refactoring types affect the density of 5 types of code smells along the version histories of 25 projects. Our findings are based on the analysis of 2,635 refactorings distributed in 11 different types. Surprisingly, 2,506 refactorings (95.1%) did not reduce or introduce code smells. Thus, these findings suggest that refactorings lead to smell reduction less often than what has been reported. According to our data, only 2.24% of refactoring changes removed code smells and 2.66% introduced new ones. Moreover, several smells induced by refactoring tended to live long, i.e., 146 days on average. These smells were only eventually removed when smelly elements started to exhibit poor structural quality and, as a consequence, started to be more costly to get rid of.","Cedrim, Diego and Sousa, Leonardo and Garcia, Alessandro and Gheyi, Rohit",2016,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Software Change Contracts","Software errors often originate from incorrect changes, including incorrect program fixes, incorrect feature updates, and so on. Capturing the intended program behavior explicitly via contracts is thus an attractive proposition. In our recent work, we had espoused the notion of “change contracts” to express the intended program behavior changes across program versions. Change contracts differ from program contracts in that they do not require the programmer to describe the intended behavior of those program features which are unchanged across program versions. In this work, we present the formal semantics of our change contract language built on top of the Java modeling language (JML). Our change contract language can describe behavioral as well as structural changes. We evaluate the expressivity of the change contract language via a survey given to final-year undergraduate students. The survey results enable to understand the usability of our change contract language for purposes of writing contracts, comprehending written contracts, and modifying programs according to given change contracts.

Finally, we develop both dynamic and static checkers for change contracts, and show how they can be used in maintaining software changes. We use our dynamic checker to automatically suggest tests that manifest violations of change contracts. Meanwhile, we use our static checker to verify that a program is changed as specified in its change contract. Apart from verification, our static checker also performs various other software engineering tasks, such as localizing the buggy method, detecting/debugging regression errors, and classifying the cause for a test failure as either error in production code or error in test code.","Yi, Jooyong and Qi, Dawei and Tan, Shin Hwei and Roychoudhury, Abhik",2015,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"How Developers Document Pull Requests with External References","Online resources of formal and informal documentation-such as reference manuals, forum discussions and tutorials-have become an asset to software developers, as they allow them to tackle problems and to learn about new tools, libraries, and technologies. This study investigates to what extent and for which purpose developers refer to external online resources when they contribute changes to a repository by raising a pull request. Our study involved (i) a quantitative analysis of over 150k URLs occurring in pull requests posted in GitHub, (ii) a manual coding of the kinds of software evolution activities performed in commits related to a statistically significant sample of 2,130 pull requests referencing external documentation resources, (iii) a survey with 69 participants, who provided feedback on how they use online resources and how they refer to them when filing a pull request. Results of the study indicate that, on the one hand, developers find external resources useful to learn something new or to solve specific problems, and they perceive useful referring such resources to better document changes. On the other hand, both interviews and repository mining suggest that external resources are still rarely referred in document changes.","Zampetti, Fiorella and Ponzanelli, Luca and Bavota, Gabriele and Mocci, Andrea and Di Penta, Massimiliano and Lanza, Michele",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"If Memory Serves: Towards Designing and Evaluating a Game for Teaching Pointers to Undergraduate Students",,"McGill, Monica M. and Johnson, Chris and Atlas, James and Bouchard, Durell and Messom, Chris and Pollock, Ian and Scott, Michael James",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Active Files As a Measure of Software Maintainability","In this paper, we explore the set of source files which are changed unusually often. We define these files as active files. Although discovery of active files relies only on version history and defect classification, the simple concept of active files can deliver key insights into software development activities. Active files can help focus code reviews, implement targeted testing, show areas for potential merge conflicts and identify areas that are central for program comprehension.

In an empirical study of six large software systems within Microsoft ranging from products to services, we found that active files constitute only between 2-8% of the total system size, contribute 20-40% of system file changes, and are responsible for 60-90% of all defects. Not only this, but we establish that the majority, 65-95%, of the active files are architectural hub files which change due to feature addition as opposed to fixing defects.","Schulte, Lukas and Sajnani, Hitesh and Czerwonka, Jacek",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"New Technology, New Learning?","
There has been a lot of excitement recently about how new technologies can transform learning. MOOCs, the internet of education and flipped classrooms are the latest hotly debated ways of changing how students learn in the modern world. At the same time, a diversity of innovative learning apps has been developed for tabletops, tablets and phones, supporting new forms of learning -- mobile, collaborative and situated. New electronic toolkits and programming environments are also emerging intended to introduce new generations to coding and computation in creative and engaging ways -- that go way beyond Logo. Never before has there been so much opportunity and buzz to make learning accessible, immersive, interactive, exciting, provocative and enjoyable. To realize the true potential of these latest technological developments, however, requires designing interfaces and apps to not only match learner's needs but also to encourage collaboration, mindful engagement, conversational skills and the art of reflection.","Rogers, Yvonne",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Study of the Paired Change Points in Bacterial Genes",,"Suvorova, Yulia M. and Korotkova, Maria A. and Korotkov, Eugene V.",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"A Transactional Memory with Automatic Performance Tuning","A significant obstacle to the acceptance of transactional memory (TM) in real-world parallel programs is the abundance of substantially different TM algorithms. Each TM algorithm appears well-suited to certain workload characteristics, but the best choice of algorithm is sensitive to program inputs, available cores, and program phases. Furthermore, operating system and hardware characteristics can affect which algorithm is best, with tradeoffs changing across iterations of a single ISA.

This paper introduces methods for constructing policies to dynamically select the most appropriate TM algorithm based on static and dynamic information. We leverage intraprocedural static analysis to create a static profile of the application. We also introduce a low-overhead framework for dynamic profiling of a running transactional application. Armed with these complementary descriptions of a program's behavior, we present novel expert adaptivity policies as well as machine learning policies that are trained off-line using simple microbenchmarks. In our evaluation, we find that both the expert and learned policies provide better performance than any single TM algorithm across the entire STAMP benchmark suite. In addition, policies that combine expert and learned policies offer the best combination of performance, maintainability, and flexibility.","Wang, Qingping and Kulkarni, Sameer and Cavazos, John and Spear, Michael",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"Impact of Feature Selection Techniques on Bug Prediction Models",,"Muthukumaran, K. and Rallapalli, Akhila and Murthy, N. L. Bhanu",2015,"[""ACM""]","Rejeitado: CR0","Rejeitado: CR0"
"BDCI: Behavioral Driven Conflict Identification","Source Code Management (SCM) systems support software evolution by providing features, such as version control, branching, and conflict detection. Despite the presence of these features, support to parallel software development is often limited. SCM systems can only address a subset of the conflicts that might be introduced by developers when concurrently working on multiple parallel branches. In fact, SCM systems can detect textual conflicts, which are generated by the concurrent modification of the same program locations, but they are unable to detect higher-order conflicts, which are generated by the concurrent modification of different program locations that generate program misbehaviors once merged. Higher-order conflicts are painful to detect and expensive to fix because they might be originated by the interference of apparently unrelated changes.

In this paper we present Behavioral Driven Conflict Identification (BDCI), a novel approach to conflict detection. BDCI moves the analysis of conflicts from the source code level to the level of program behavior by generating and comparing behavioral models. The analysis based on behavioral models can reveal interfering changes as soon as they are introduced in the SCM system, even if they do not introduce any textual conflict.

To evaluate the effectiveness and the cost of the proposed approach, we developed BDCIf, a specific instance of BDCI dedicated to the detection of higher-order conflicts related to the functional behavior of a program. The evidence collected by analyzing multiple versions of Git and Redis suggests that BDCIf can effectively detect higher-order conflicts and report how changes might interfere.","Pastore, Fabrizio and Mariani, Leonardo and Micucci, Daniela",2017,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Text Steganography in Font Color of MS Excel Sheet",,"Alsaadi, Husam Ibrahiem and Al-Anni, Maad Kamal and Almuttairi, Rafah M. and Bayat, Oguz and Ucan, Osman Nuri",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Storyteller: A New Medium for Guiding Students Through Code Examples (Abstract Only)",,"Mahoney, Mark",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Automatic Generation of Release Notes",,"Moreno, Laura and Bavota, Gabriele and Di Penta, Massimiliano and Oliveto, Rocco and Marcus, Andrian and Canfora, Gerardo",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Visualizations of Evolving Graphical Models in the Context of Model Review",,"Zoubek, Florian and Langer, Philip and Mayerhofer, Tanja",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"An Ensemble SVM Model for the Accurate Prediction of Non-canonical MicroRNA Targets",,"Ghoshal, Asish and Grama, Ananth and Bagchi, Saurabh and Chaterji, Somali",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Advantages and Disadvantages of Using Shared Code from the Developers Perspective: A Qualitative Study",,"Ribeiro, Danilo Monteiro and da Silva, Fabio Q. B. and Valen\c{c}a, Diana and Freitas, Elyda L. S. X. and Fran\c{c}a, C{\'e}sar",2016,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR0"
"Rethinking Compilers in the Rise of Machine Learning and AI (Keynote)",,"Shen, Xipeng",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Structural Health Monitoring by Using a Sparse Coding-based Deep Learning Algorithm with Wireless Sensor Networks",,"Guo, Junqi and Xie, Xiaobo and Bie, Rongfang and Sun, Limin",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Foundation for Refactoring C with Macros",,"Overbey, Jeffrey L. and Behrang, Farnaz and Hafiz, Munawar",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Analysis of Hotspot Methods in JVM for Best-effort Run-time Parallelization",,"Ain, Qurrat Ul and Ahmed, Saqib and Zafar, Abdullah and Mehmood, Muhammad Amir and Waheed, Abdul",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Evaluator-executor Transformation for Efficient Pipelining of Loops with Conditionals",,"Jeong, Yeonghun and Seo, Seongseok and Lee, Jongeun",2013,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"How Confident Are You to Counter Uncertainty?","Accuracy is an essential factor to determine how confident a biometric system makes a decision. However, to achieve high accuracy, any biometric systems need to deal with intra-class variability and inter-class similarity problems. The intra-class variability makes exact biometric matching difficult and increases the level of uncertainty, since errors are permitted in matching. Reducing uncertainty in biometric matching is important because it indicates the matcher can reliably decide whether a query biometric template belongs to a registered biometric template or not. In biometric systems where image processing techniques are mainly used, it is impossible to achieve exact biometric matching due to estimation problem, that is exactly estimating the registered image given the query image. However, we approach the estimation problem from a post-processing perspective where derivative information of the registered template is used to transform the query template into another template that can be used to perform an exact match with the registered template. Moreover, large inter-class similarity due to mainly random matches increase false acceptance rate (FAR). To tackle such similarity, the longest common substring (LCS) between the two templates is obtained since it counts for the most significant contiguous matches. We extensively tested our proposed method using iris images from the public and commercial Bath dataset and found that the success rate of the transformation to produce exact matches are 97.5% and 94.34% respectively without falsely accepting an imposter (FAR=0), losing biometric information and multiple scanning.","Sukarno, Parman and Bhattacharjee, Nandita and Srinivasan, Bala",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Improving MiRNA Prediction Accuracy by Deep Learning Strategies",,"Xue, Bin",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"The Evolution of a Digital Ecosystem",,"Um, SungyYong",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Predicting Defectiveness of Software Patches",,"Soltanifar, Behjat and Erdem, Atakan and Bener, Ayse",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features",,"Wan, Jun and Ruan, Qiuqi and Li, Wei and Deng, Shuang",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Native-2-native: Automated Cross-platform Code Synthesis from Web-based Programming Resources",,"Byalik, Antuan and Chadha, Sanchit and Tilevich, Eli",2015,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"Speeding Up Iterative Polyhedral Schedule Optimization with Surrogate Performance Models",,"Ganser, Stefan and Gr\""{o}\sslinger, Armin and Siegmund, Norbert and Apel, Sven and Lengauer, Christian",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Rethinking User Interfaces for Feature Location",,"Beck, Fabian and Dit, Bogdan and Velasco-Madden, Jaleo and Weiskopf, Daniel and Poshyvanyk, Denys",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"A Methodology for Managing Database and Code Changes in a Regression Testing Framework",,"Salama, Roberto and McGuire, James and Rosenberg, Michael K.",2012,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"Users Beware: Preference Inconsistencies Ahead","The structure of preferences for modern highly-configurable software systems has become extremely complex, usually consisting of multiple layers of access that go from the user interface down to the lowest levels of the source code. This complexity can lead to inconsistencies between layers, especially during software evolution. For example, there may be preferences that users can change through the GUI, but that have no effect on the actual behavior of the system because the related source code is not present or has been removed going from one version to the next. These inconsistencies may result in unexpected program behaviors, which range in severity from mild annoyances to more critical security or performance problems. To address this problem, we present SCIC (Software Configuration Inconsistency Checker), a static analysis technique that can automatically detect these kinds of inconsistencies. Unlike other configuration analysis tools, SCIC can handle software that (1) is written in multiple programming languages and (2) has a complex preference structure. In an empirical evaluation that we performed on 10 years worth of versions of both the widely used Mozilla Core and Firefox, SCIC was able to find 40 real inconsistencies (some determined as severe), whose lifetime spanned multiple versions, and whose detection required the analysis of code written in multiple languages.","Behrang, Farnaz and Cohen, Myra B. and Orso, Alessandro",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"An Automation Framework for Configuration Management to Reduce Manual Intervention","To support the increasing expectations of clients in getting quick bug resolutions and feature enhancements, many IT industries today use agile software development methodology. Under this methodology changes are made in the software on daily basis. In order to cope-up with these changes, Software Configuration Management (SCM) plays an important role. Configuration manager is responsible for maintaining and monitoring changes which are made in software over a period of time. Version control tools are used by the configuration manager to maintain the software. It allows developers to keep source code in repository and take copy from repository, whenever needed. This makes it necessary to integrate complete code in repository, compile it and create deliverable package. Configuration manager performs these activities, such as analyzing logs of daily compilation and package creation software that is to be delivered. In current scenario these tasks are performed manually. This paper proposes a framework to reduce manual intervention and automate above mentioned tasks of the configuration manager.","Karale, Supriya V. and Kaushal, Vishal",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Small Code Changes Can Make Many Single-threaded CPU Programs Run Hundreds of Times Faster in Parallel on a GPU Using C++ AMP",,"Wynters, Erik L.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"On the Effectiveness of Random Testing for Android: Or How I Learned to Stop Worrying and Love the Monkey",,"Patel, Priyam and Srinivasan, Gokul and Rahaman, Sydur and Neamtiu, Iulian",2018,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"Asynchronous Gaze Sharing: Towards a Dynamic Help System to Support Learners During Program Comprehension",,"Deitelhoff, Fabian",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Towards a Catalog of Variability Evolution Patterns: The Linux Kernel Case","A complete understanding of evolution of variability requires analysis over all project spaces that contain it: source code, build system and the variability model. Aiming at better understanding of how complex variant-rich software evolve, we set to study one, the Linux kernel, in detail. We qualitatively analyze a number of evolution steps in the kernel history and present our findings as a preliminary sample of a catalog of evolution patterns. Our patterns focus on how the variability evolves when features are removed from the variability model, but are kept as part of the software. The identified patterns relate changes to the variability model, the build system, and implementation code. Despite preliminary, they already indicate evolution steps that have not been captured by prior studies, both empirical and theoretical.","Passos, Leonardo and Czarnecki, Krzysztof and W\k{a}sowski, Andrzej",2012,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Comprehension First: Evaluating a Novel Pedagogy and Tutoring System for Program Tracing in CS1",,"Nelson, Greg L. and Xie, Benjamin and Ko, Andrew J.",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Needle: Detecting Code Plagiarism on Student Submissions",,"Jiang, Yanyan and Xu, Chang",2018,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR0"
"The Plastic Surgery Hypothesis",,"Barr, Earl T. and Brun, Yuriy and Devanbu, Premkumar and Harman, Mark and Sarro, Federica",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Adaptive Power Management in Solar Energy Harvesting Sensor Node Using Reinforcement Learning",,"Shresthamali, Shaswot and Kondo, Masaaki and Nakamura, Hiroshi",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Going Deeper Than Deep Learning for Massive Data Analytics Under Physical Constraints","Deep Neural Networks (DNNs) are a set of powerful yet computationally complex learning mechanisms that are projected to dominate various artificial intelligence and massive data analytic domains. Physical viability, such as timing, memory, or energy efficiency, are standing challenges in realizing the true potential of DNNs. We propose DeLight, a set of novel methodologies which aim to bring physical constraints as design parameters in the training and execution of DNN architectures. We use physical profiling to bound the network size in accordance to the pertinent platform's characteristics. An automated customization methodology is proposed to adaptively conform the DNN configurations to meet the characterization of the underlying hardware while minimally affecting the inference accuracy. The key to our approach is a new content- and resource-aware transformation of data to a lower-dimensional embedding by which learning the correlation between data samples requires significantly smaller number of neurons. We leverage the performance gain achieved as a result of the data transformation to enable the training of multiple DNN architectures that can be aggregated to further boost the inference accuracy. An accompanying API is also developed, which can be used for rapid prototyping of an arbitrary DNN application customized to the platform. Proof-of concept evaluations for deployment of different imaging, audio, and smart-sensing applications demonstrate up to 100-fold performance improvement compared to the state-of-the-art DNN solutions.","Rouhani, Bita Darvish and Mirhoseini, Azalia and Koushanfar, Farinaz",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10, CR9"
"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis","In large introductory programming classes, teacher feedback on individual incorrect student submissions is often infeasible. Program synthesis techniques are capable of fixing student bugs and generating hints automatically, but they lack the deep domain knowledge of a teacher and can generate functionally correct but stylistically poor fixes. We introduce a mixed-initiative approach which combines teacher expertise with data-driven program synthesis techniques. We demonstrate our novel approach in two systems that use different interaction mechanisms. Our systems use program synthesis to learn bug-fixing code transformations and then cluster incorrect submissions by the transformations that correct them. The MistakeBrowser system learns transformations from examples of students fixing bugs in their own submissions. The FixPropagator system learns transformations from teachers fixing bugs in incorrect student submissions. Teachers can write feedback about a single submission or a cluster of submissions and propagate the feedback to all other submissions that can be fixed by the same transformation. Two studies suggest this approach helps teachers better understand student bugs and write reusable feedback that scales to a massive introductory programming classroom.","Head, Andrew and Glassman, Elena and Soares, Gustavo and Suzuki, Ryo and Figueredo, Lucas and D'Antoni, Loris and Hartmann, Bj\""{o}rn",2017,"[""ACM""]","Aceito: CA3","Aceito: CA1, CA3, CA7"
"Do Developers Update Third-party Libraries in Mobile Apps?",,"Salza, Pasquale and Palomba, Fabio and Di Nucci, Dario and D'Uva, Cosmo and De Lucia, Andrea and Ferrucci, Filomena",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Unix Literacy for First-Year Computer Science Students",,"Kendon, Tyson and Stephenson, Ben",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Reconfiguration Algorithm for Power-Aware Parallel Applications",,"De Sensi, Daniele and Torquati, Massimo and Danelutto, Marco",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Improving the Memory Access Locality of Hybrid MPI Applications",,"Diener, Matthias and White, Sam and Kale, Laxmikant V. and Campbell, Michael and Bodony, Daniel J. and Freund, Jonathan B.",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Using POGIL to Teach Students to Be Better Problem Solvers (Abstract Only)",,"Hu, Helen H.",2012,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Question Independent Grading Using Machine Learning: The Case of Computer Program Grading",,"Singh, Gursimran and Srikant, Shashank and Aggarwal, Varun",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Author Retrospective for Optimizing for Parallelism and Data Locality",,"McKinley, Kathryn S.",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Model-driven Test Case Design for Model-to-model Semantics Preservation",,"Gerking, Christopher and Ladleif, Jan and Sch\""{a}fer, Wilhelm",2015,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"Incremental Symbolic Execution for Automated Test Suite Maintenance",,"Makhdoom, Sarmad and Khan, Muhammad Adeel and Siddiqui, Junaid Haroon",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Cyber Education: Towards a Pedagogical and Heuristic Learning",,"Alvarez, Isabel Borges and Silva, Nuno S. Alves and Correia, Luisa Sampaio",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Implicit Theories of Programming Aptitude As a Barrier to Learning to Code: Are They Distinct from Intelligence?",,"Scott, Michael James and Ghinea, Gheorghita",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Mining Control Flow Graph As API Call-grams to Detect Portable Executable Malware",,"Faruki, Parvez and Laxmi, Vijay and Gaur, M. S. and Vinod, P.",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Exploring the Impact of Inter-smell Relations on Software Maintainability: An Empirical Study",,"Yamashita, Aiko and Moonen, Leon",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Code Parallelization Through Sequential Code Search",,"Cai, Bowen",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"vsInk: Integrating Digital Ink with Program Code in Visual Studio",,"Sutherland, Craig J. and Plimmer, Beryl",2013,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Speeding Up Database Applications with Pyxis",,"Cheung, Alvin and Arden, Owen and Madden, Samuel and Myers, Andrew C.",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Acquiring Human-Robot Interaction Skills with Transfer Learning Techniques",,"Mohammed, Omar and Bailly, Gerard and Pellier, Damien",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Transforming US Education with Computer Science",,"Partovi, Hadi",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Assessing the Value of Branches with What-if Analysis","Branches within source code management systems (SCMs) allow a software project to divide work among its teams for concurrent development by isolating changes. However, this benefit comes with several costs: increased time required for changes to move through the system and pain and error potential when integrating changes across branches. In this paper, we present the results of a survey to characterize how developers use branches in a large industrial project and common problems that they face. One of the major problems mentioned was the long delay that it takes changes to move from one team to another, which is often caused by having too many branches (branchmania). To monitor branch health, we introduce a novel what-if analysis to assess alternative branch structures with respect to two properties, isolation and liveness. We demonstrate with several scenarios how our what-if analysis can support branch decisions. By removing high-cost-low-benefit branches in Windows based on our what-if analysis, changes would each have saved 8.9 days of delay and only introduced 0.04 additional conflicts on average.","Bird, Christian and Zimmermann, Thomas",2012,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR8, CR10"
"Trading Robustness for Maintainability: An Empirical Study of Evolving C\# Programs",,"Cacho, N{\'e}lio and C{\'e}sar, Thiago and Filipe, Thomas and Soares, Eliezio and Cassio, Arthur and Souza, Rafael and Garcia, Israel and Barbosa, Eiji Adachi and Garcia, Alessandro",2014,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR0"
"SINk: A Middleware for Synchronization of Heterogeneous Software Interfaces",,"Hosseini, Mohammad and Jiang, Yu and Wu, Poliang and Berlin,Jr., Richard B. and Sha, Lui",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"LDA Revisited: Entropy, Prior and Convergence","Inference algorithms of latent Dirichlet allocation (LDA), either for small or big data, can be broadly categorized into expectation-maximization (EM), variational Bayes (VB) and collapsed Gibbs sampling (GS). Looking for a unified understanding of these different inference algorithms is currently an important open problem. In this paper, we revisit these three algorithms from the entropy perspective, and show that EM can achieve the best predictive perplexity (a standard performance metric for LDA accuracy) by minimizing directly the cross entropy between the observed word distribution and LDA's predictive distribution. Moreover, EM can change the entropy of LDA's predictive distribution through tuning priors of LDA, such as the Dirichlet hyperparameters and the number of topics, to minimize the cross entropy with the observed word distribution. Finally, we propose the adaptive EM (AEM) algorithm that converges faster and more accurate than the current state-of-the-art SparseLDA [20] and AliasLDA [12] from small to big data and LDA models. The core idea is that the number of active topics, measured by the residuals between E-steps at successive iterations, decreases significantly, leading to the amortized σ(1) time complexity in terms of the number of topics. The open source code of AEM is available at GitHub.","Zhang, Jianwei and Zeng, Jia and Yuan, Mingxuan and Rao, Weixiong and Yan, Jianfeng",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"LEGO-based Active Learning Exercises for Teaching Software Development: (Abstract Only)",,"Kurkovsky, Stan and Ludi, Stephanie",2018,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"DeFlaker: Automatically Detecting Flaky Tests","Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle.

We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96 Java projects on TravisCI, and found 87 previously unknown flaky tests in 10 of these projects. We also ran experiments on project histories, where DeFlaker detected 1, 874 flaky tests from 4, 846 failures, with a low false alarm rate (1.5%). DeFlaker had a higher recall (95.5% vs. 23%) of confirmed flaky tests than Maven's default flaky test detector.","Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko",2018,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"Unravel: Rapid Web Application Reverse Engineering via Interaction Recording, Source Tracing, and Library Detection","Professional websites with complex UI features provide real world examples for developers to learn from. Yet despite the availability of source code, it is still difficult to understand how these features are implemented. Existing tools such as the Chrome Developer Tools and Firebug offer debugging and inspection, but reverse engineering is still a time consuming task. We thus present Unravel, an extension of the Chrome Developer Tools for quickly tracking and visualizing HTML changes, JavaScript method calls, and JavaScript libraries. Unravel injects an observation agent into websites to monitor DOM interactions in real-time without functional interference or external dependencies. To manage potentially large observations of events, the Unravel UI provides affordances to reduce, sort, and scope observations. Testing Unravel with 13 web developers on 5 large-scale websites, we found a 53% decrease in time to discovering the first key source behind a UI feature and a 32% decrease in time to understanding how to fully recreate a feature.","Hibschman, Joshua and Zhang, Haoqi",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Live Versioning of Web Applications Through Refactoring","Client-Side Web Refactorings (CSWRs) allow improving interaction aspects of web applications by applying changes to the user interface without altering the code base, even in production settings. However, developers are not always willing, or even allowed to apply external adaptations to their applications’ user interface. Besides, CSWRs do not guarantee improvements in all contexts, so it may be unwise to install them in a production version. We propose a tool that allows creating private versions of a running web application almost automatically. Using this tool, developers or usability experts can easily combine CSWRs to create alternative versions of web applications, without the need of creating a cloned sandbox environment for each version. This yields many uses, such as quickly setting up user tests, showing live alternatives to Product Owners, and even performing A/B testing. The tool is built on top of Kobold, a service that allows applying CSWRs to fix usability smells. Kobold with versioning is available at: https://autorefactoring.lifia.info.unlp.edu.ar. A screencast of the tool is available at https://youtu.be/LVc3BOtVP3I.","Grigera, Juli\'{a}n and Gardey, Juan Cruz and Garrido, Alejandra and Rossi, Gustavo",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Selective Regression Testing for Web Applications Created with Google Web Toolkit",,"Hirzel, Matthias",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"A Structured Survey on the Usage of the Issue Tracking System Provided by the GitHub Platform",,"Neto, Casimiro Conde Marco and de O. Barros, M\'{a}rcio",2017,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"GuideAutomator: Continuous Delivery of End User Documentation",,"Souza, Rodrigo and Oliveira, Allan",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Learning a Shared Shape Space for Multimodal Garment Design",,"Wang, Tuanfeng Y. and Ceylan, Duygu and Popovi\'{c}, Jovan and Mitra, Niloy J.",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Responses to Adaptive Feedback for Software Testing",,"Buffardi, Kevin and Edwards, Stephen H.",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Teaching Operating Systems Using Code Review",,"Dall, Christoffer and Nieh, Jason",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"A Search-based Training Algorithm for Cost-aware Defect Prediction","Research has yielded approaches to predict future defects in software artifacts based on historical information, thus assisting companies in effectively allocating limited development resources and developers in reviewing each others' code changes. Developers are unlikely to devote the same effort to inspect each software artifact predicted to contain defects, since the effort varies with the artifacts' size (cost) and the number of defects it exhibits (effectiveness). We propose to use Genetic Algorithms (GAs) for training prediction models to maximize their cost-effectiveness. We evaluate the approach on two well-known models, Regression Tree and Generalized Linear Model, and predict defects between multiple releases of six open source projects. Our results show that regression models trained by GAs significantly outperform their traditional counterparts, improving the cost-effectiveness by up to 240%. Often the top 10% of predicted lines of code contain up to twice as many defects.","Panichella, Annibale and Alexandru, Carol V. and Panichella, Sebastiano and Bacchelli, Alberto and Gall, Harald C.",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Halftone QR Codes",,"Chu, Hung-Kuo and Chang, Chia-Sheng and Lee, Ruen-Rone and Mitra, Niloy J.",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Sound Recycling from Public Databases: Another BigData Approach to Sound Collections",,"Ordiales, Hern\'{a}n and Bruno, Mat\'{\i}as Lennie",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Codeopticon: Real-Time, One-To-Many Human Tutoring for Computer Programming",,"Guo, Philip J.",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"I Know What You Did Last Summer: An Investigation of How Developers Spend Their Time",,"Minelli, Roberto and and, Andrea Mocci and Lanza, Michele",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Tackling Androids Native Library Malware with Robust, Efficient and Accurate Similarity Measures",,"Kalysch, Anatoli and Milisterfer, Oskar and Protsenko, Mykolai and M\""{u}ller, Tilo",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Using Different Characteristics of Machine Learners to Identify Different Defect Families",,"Petri\'{c}, Jean",2016,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR0"
"Who is to Change?: Nudging and Provocative Communication Discussed Through L\OGstrup's Ontological Ethics","This paper discusses nudging and provocative communication as possible approaches to designing behavioural change concerning minimisation of waste within the framework of Løgstrup's ontological ethics. Waste management companies are confronted with ethical concerns as their course of action consequently affects their relationship with the citizens whose waste they manage. Waste management companies might be experts within their field, but they are challenged when entering new contexts and must therefore redefine or reframe their role in society. This became evident during an action research project as an ethical challenge was identified through a strategic workshop facilitated for AVV in relation to the Nulskrald project. The main focus of Nulskrald is citizen empowerment as well as organisational learning and responsibility. Through Løgstrup's ontological ethics the ethical demand, as it is posed by 'the other person' towards the 'I', will show concerns and possibilities for engaging citizens, while at the same time resulting in organisational development. Therefore, the research question is: what ethical issues and organisational implications exist concerning the use of nudging and provocative communication, respectively?","Winkel, Thomas Dyrmann and Jensen, Thessa and Poulsen, S{\o}ren Bolvig",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Integrating Security, Analytics and Application Management into the Mobile Development Lifecycle",,"Pistoia, Marco and Tripp, Omer",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Making Offline Analyses Continuous","Developers use analysis tools to help write, debug, and understand software systems under development. A developer's change to the system source code may affect analysis results. Typically, to learn those effects, the developer must explicitly initiate the analysis. This may interrupt the developer's workflow and/or the delay until the developer learns the implications of the change. The situation is even worse for impure analyses — ones that modify the code on which it runs — because such analyses block the developer from working on the code.

This paper presents Codebase Replication, a novel approach to easily convert an offline analysis — even an impure one — into a continuous analysis that informs the developer of the implications of recent changes as quickly as possible after the change is made. Codebase Replication copies the developer's codebase, incrementally keeps this copy codebase in sync with the developer's codebase, makes that copy codebase available for offline analyses to run without disturbing the developer and without the developer's changes disturbing the analyses, and makes analysis results available to be presented to the developer.

We have implemented Codebase Replication in Solstice, an open-source, publicly-available Eclipse plug-in. We have used Solstice to convert three offline analyses — FindBugs, PMD, and unit testing — into continuous ones. Each conversion required on average 436 NCSL and took, on average, 18 hours. Solstice-based analyses experience no more than 2.5 milliseconds of runtime overhead per developer action.","Mu\c{s}lu, K\ivan\c{c} and Brun, Yuriy and Ernst, Michael D. and Notkin, David",2013,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Towards a Framework for Stochastic Performance Optimizations in Compilers and Interpreters: An Architecture Overview",,"Krauss, Oliver",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"SPLEMMA: A Generic Framework for Controlled-evolution of Software Product Lines","Managing in a generic way the evolution process of feature-oriented Software Product Lines (spls) is complex due to the number of elements that are impacted and the heterogeneity of the spls regarding artifacts used to define them. Existing work presents specific approaches to manage the evolution of spls in terms of such artifacts, i.e., assets, feature models and relation definitions. Moreover stakeholders do not necessarily master all the knowledge of the spl making its evolution difficult and error-prone without a proper tool support. In order to deal with these issues, we introduce SPLEmma, a generic framework that follows a Model Driven Engineering approach to capture the evolution of a spl independently of the kind of assets, technologies or feature models used for the product derivation. Authorized changes are described by the spl maintainer and captured in a model used to generate tools that guide the evolution process and preserve the consistency of the whole spl. We report on the application of our approach on two spls: YourCast for digital signage systems, and SALOON, which enables generation of configurations for cloud providers.","Romero, Daniel and Urli, Simon and Quinton, Cl{\'e}ment and Blay-Fornarino, Mireille and Collet, Philippe and Duchien, Laurence and Mosser, S{\'e}bastien",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Accurate and Efficient Refactoring Detection in Commit History","Refactoring detection algorithms have been crucial to a variety of applications: (i) empirical studies about the evolution of code, tests, and faults, (ii) tools for library API migration, (iii) improving the comprehension of changes and code reviews, etc. However, recent research has questioned the accuracy of the state-of-the-art refactoring detection tools, which poses threats to the reliability of their application. Moreover, previous refactoring detection tools are very sensitive to user-provided similarity thresholds, which further reduces their practical accuracy. In addition, their requirement to build the project versions/revisions under analysis makes them inapplicable in many real-world scenarios.

To reinvigorate a previously fruitful line of research that has stifled, we designed, implemented, and evaluated RMiner, a technique that overcomes the above limitations. At the heart of RMiner is an AST-based statement matching algorithm that determines refactoring candidates without requiring user-defined thresholds. To empirically evaluate RMiner, we created the most comprehensive oracle to date that uses triangulation to create a dataset with considerably reduced bias, representing 3,188 refactorings from 185 open-source projects. Using this oracle, we found that RMiner has a precision of 98% and recall of 87%, which is a significant improvement over the previous state-of-the-art.","Tsantalis, Nikolaos and Mansouri, Matin and Eshkevari, Laleh M. and Mazinanian, Davood and Dig, Danny",2018,"[""ACM""]","Aceito: CA0, CA1, CA3","Aceito: CA1, CA3"
"Drag-and-drop Refactoring: Intuitive and Efficient Program Transformation",,"Lee, Yun Young and Chen, Nicholas and Johnson, Ralph E.",2013,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Pegasus: Automatic Barrier Inference for Stable Multithreaded Systems","Deterministic multithreaded systems (DMTs) are designed to ensure reproducibility of program behavior for a given input. In these systems, even minor changes to the code (or input) can perturb the schedule. This increases the number of feasible schedules making reasoning about these programs harder. Stable multithreaded systems (StableMTs) address the problem such that a schedule is unaffected by minor changes. Unfortunately, determinism in these systems can potentially serialize the execution imposing a significant penalty on the performance. Programmer hints in the form of soft barriers attempt to eliminate the performance bottlenecks. However, the process is arduous, error-prone and requires manual intervention to reconsider the location of the barrier for every modification to the source code. In this paper, we propose an effective approach to automate the task of adding soft barriers in the source code. Our approach analyzes the deterministic program executions to extract the program and semantic order of executions and builds a weighted constraint graph. Using this graph, a schedule is synthesized which is used to identify bottlenecks and insert soft barriers in the program source. We validate our implementation, named PEGASUS, by applying it on multiple benchmarks. Our experimental results demonstrate that we are able to reduce the overall execution time of programs by upto 34% when compared to the execution time where barriers are inserted manually. Moreover, we observe a performance improvement ranging from 38% to 88% as compared to programs without barriers. Our experimental results show that adapting PEGASUS to infer barriers for multiple versions of a source program is seamless. The memory and time overheads associated with the usage of PEGASUS is negligible making it a vital cog in building scalable StableMTs.","Dhok, Monika and Mudduluru, Rashmi and Ramanathan, Murali Krishna",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Measurement of Exception-handling Code: An Exploratory Study",,"D\'{u}laigh, Keith \'{O} and Power, James F. and Clarke, Peter J.",2012,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Diagnosing Machine Learning Pipelines with Fine-grained Lineage","We present the Hippo system to enable the diagnosis of distributed machine learning (ML) pipelines by leveraging fine-grained data lineage. Hippo exposes a concise yet powerful API, derived from primitive lineage types, to capture fine-grained data lineage for each data transformation. It records the input datasets, the output datasets and the cell-level mapping between them. It also collects sufficient information that is needed to reproduce the computation. Hippo efficiently enables common ML diagnosis operations such as code debugging, result analysis, data anomaly removal, and computation replay. By exploiting the metadata separation and high-order function encoding strategies, we observe an O(10^3)x total improvement in lineage storage efficiency vs. the baseline of cell-wise mapping recording while maintaining the lineage integrity. Hippo can answer the real use case lineage queries within a few seconds, which is low enough to enable interactive diagnosis of ML pipelines.","Zhang, Zhao and Sparks, Evan R. and Franklin, Michael J.",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Defect, Defect, Defect: Defect Prediction 2.0","Defect prediction has been a very active research area in software engineering [6--8, 11, 13, 16, 19, 20].

In 1971, Akiyama proposed one of the earliest defect prediction models using Lines of Code (LOC) [1]: ""Defect = 4.86 + 0.018LOC.""

Since then, many effective new defect prediction models and metrics have been proposed. For the prediction models, typical machine learners and regression algorithms such as Naive Bayes, Decision Tree, and Linear Regression are widely used. On the other hand, Kim et al. proposed a cache-based prediction model using bug occurrence properties [9]. Hassan proposed a change entropy model to effectively predict defects [6]. Recently, Bettenburg et al. proposed Multivariate Adaptive Regression Splines to improve defect prediction models by learning from local and global properties together [4].

Besides LOC, many new effective metrics for defect prediction have been proposed. Among them, source code metrics and change history metrics are widely used and yield reasonable defect prediction accuracy. For example, Basili et al. [3] used Chidamber and Kemerer metrics, and Ohlsson et al. [14] used McCabe's cyclomatic complexity for defect prediction. Moser et al. [12] used the number of revisions, authors, and past fixes, and age of a file as defect predictors. Recently, micro interaction metrics (MIMs) [10] and source code quality measures [15] for effective defect prediction are proposed.

However, there is much room to improve for defect prediction 2.0. First of all, understanding the actual causes of defects is necessary. Without understanding them, we may reach to nonsensical conclusions from defect prediction results [18]. Many effective prediction models have been proposed, but successful application cases in practice are scarcely reported. To be more attractive for developers in practice, it is desirable to predict defects in finer granularity levels such as the code line or even keyword level. Note that static bug finders such as FindBugs [2] can identify potential bugs in the line level, and many developers find them useful in practice. Dealing with noise in defect data has become an important issue. Bird et al. identified there is non-neglectable noise in defect data [5]. This noise may yield poor and/or meaningless defect prediction results. Cross-prediction is highly desirable: for new projects or projects with limited training data, it is necessary to learn a prediction model using sufficient training data from other projects, and to apply the model to those projects. However, Zimmermann et al. [21] identified cross-project prediction is a challenging problem. Turhan et al. [17] analyzed Cross-Company (CC) and Within-Company (WC) data for defect prediction, and confirmed that it is challenging to reuse CC data directly to predict defects in other companies' software.

Overall, defect prediction is a very interesting and promising research area. However, there are still many research challenges and problems to be addressed. Hopefully, this discussion calls new solutions and ideas to address these challenges.","Kim, Sunghun",2012,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Older Adults Learning Computer Programming: Motivations, Frustrations, and Design Opportunities",,"Guo, Philip J.",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"It's Alive! Continuous Feedback in UI Programming",,"Burckhardt, Sebastian and Fahndrich, Manuel and de Halleux, Peli and McDirmid, Sean and Moskal, Michal and Tillmann, Nikolai and Kato, Jun",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR0"
"Lighthouse: An Automated Solver Selection Tool","Linear algebra provides the building blocks for a wide variety of scientific and engineering simulation codes. Users of these codes face a world of continuously changing algorithms and high-performance implementations. In this paper, we describe new capabilities of our Lighthouse framework, whose goal is to match specific problems in the area of high-performance numerical computing with the best available solutions. Lighthouse's innovative strategy eliminates intensive reading of documents and automates the process for developing linear algebra software. Lighthouse provides a searchable taxonomy of popular but difficult to use numerical software for dense and sparse linear algebra while providing the user with the best algorithms for a given problem based on machine learning methods. We introduce the design of Lighthouse and show examples of its interface. We also present algorithm classification results for the preconditioned iterative linear solvers in the Parallel Extensible Toolkit for Scientific Computation (PETSc) and the Trilinos library.","Motter, Pate and Sood, Kanika and Jessup, Elizabeth and Norris, Boyana",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"An Efficient Path Based Equivalence Checking for Petri Net Based Models of Programs","A user written program goes through significant optimizing and parallelizing transformations, both (compiler) automated and human guided, before being mapped to an architecture. Formally verifying these transformations is crucial to ensure that they preserve the original behavioural specification. The PRES+ model (Petri net based Representation of Embedded Systems) encompassing data processing is used to model parallel behaviours more succinctly. Being value based with a natural capability of capturing parallelism, PRES+ models depict such data dependencies more vividly; accordingly, they are likely to be more convenient as the intermediate representations (IRs) of both source and transformed codes for translation validation than strictly sequential, variable-based IRs such as Finite State Machines with Data path (FSMDs) which are essentially sequential control and data flow graphs (CDFGs). This paper presents a scheme for verifying equivalence between two given PRES+ models for translation validation of optimizing and parallelizing code transformations; one of the two models represents the source code and the other represents its optimized and (or) parallelized version.","Bandyopadhyay, Soumyadip and Sarkar, Dipankar and Mandal, Chittaranjan",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR7"
"Differentiable Programming for Image Processing and Deep Learning in Halide",,"Li, Tzu-Mao and Gharbi, Micha\""{e}l and Adams, Andrew and Durand, Fr{\'e}do and Ragan-Kelley, Jonathan",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Real-time Evolutionary Learning of Cooperative Predator-prey Strategies",,"Wittkamp, Mark and Barone, Luigi and Hingston, Phil and While, Lyndon",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"An Agile Software Engineering Course with Product Hand-off",,"Shepherd, Jason B.",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Bots vs. Wikipedians, Anons vs. Logged-Ins (Redux): A Global Study of Edit Activity on Wikipedia and Wikidata",,"Steiner, Thomas",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"A First Year Common Course on Computational Problem Solving and Programming (Abstract Only)",,"Char, Bruce W. and Hewett, Thomas T.",2014,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Changing Perceptions of Discrete Mathematics Through Scrum-based Course Management Practices",,"Duvall, Shannon and Hutchings, Duke and Kleckner, Michele",2017,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Code Phonology: An Exploration into the Vocalization of Code",,"Hermans, Felienne and Swidan, Alaaeddin and Aivaloglou, Efthimia",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Preventing Erosion of Architectural Tactics Through Their Strategic Implementation, Preservation and Visualization",,"Mirakhorli, Mehdi",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Are Software Engineering Textbooks a Thing of the Past?","Computer science is a rapidly changing field with new tools and products introduced almost daily. The software developer, whether beginning student or seasoned professional, must not only have a good background on the fundamentals, but must also be able to keep up with the latest trends and techniques through research, education, and practice. This paper addresses this issue in terms of student learning, and discusses the issue of currency as it relates to student learning materials such as textbooks.","Alkadi, Ghassan and Beaubouef, Theresa",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Compressed Video Coding with Multi-Kernel Gaussian Process Regression",,"Tian, Jieyu and Li, Yong and Xiong, Hongkai",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"MD5 Secured Cryptographic Hash Value",,"de Guzman, Larry B. and Sison, Ariel M. and Medina, Ruji P.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Online Handwritten Character Recognition for Malayalam",,"Sampath, Amritha and Tripti, C. and Govindaru, V.",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Modeling Work-ethics Spread in Software Organizations","Most organizations – whether software or otherwise – have a statement of mission, a vision for the future, and values that help them get there. People, on the other hand, have a wide range of ideas, beliefs and objectives. How does one reconcile the two? Otherwise put: how do work practices and ethics that are important from an organizational standpoint get inculcated in employees? We propose a Human Behavior Change model and apply it to the context of ethics spread in software organizations. The model is founded upon an internal cost-reward function of individuals and transmission effect between people that contributes to the decision to change and comply with certain ethics and practices. We use Agent Based Modeling and Simulation (ABMS) technique to represent our model. We experiment with virtual teams and create various scenarios to understand ethic spread. Although our larger aim is to model behavior change, in this early stage of our work, we build models to know how ethics spread through change in compliance behavior. We base our study on existing literature, a limited survey, some assumptions and simulation. Through simulation, we also seek to ask associated questions related to team size and compliance, spread of counter-ethics, message mechanism and speed of spread and the ‘mean time to change’ for different types of individuals.","Athavale, Sandeep and Singh, Meghendra",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10"
"Towards Content-driven Reputation for Collaborative Code Repositories","As evidenced by SourceForge and GitHub, code repositories now integrate Web 2.0 functionality that enables global participation with minimal barriers-to-entry. To prevent detrimental contributions enabled by crowdsourcing, reputation is one proposed solution. Fortunately this is an issue that has been addressed in analogous version control systems such as the wiki for natural language content. The WikiTrust algorithm (""content-driven reputation""), while developed and evaluated in wiki environments operates under a possibly shared collaborative assumption: actions that ""survive"" subsequent edits are reflective of good authorship.

In this paper we examine WikiTrust's ability to measure author quality in collaborative code development. We first define a mapping from repositories to wiki environments and use it to evaluate a production SVN repository with 92,000 updates. Analysis is particularly attentive to reputation loss events and attempts to establish ground truth using commit comments and bug tracking. A proof-of-concept evaluation suggests the technique is promising (about two-thirds of reputation loss is justified) with false positives identifying areas for future refinement. Equally as important, these false positives exemplify differences in content evolution and the cooperative process between wikis and code repositories.","West, Andrew G. and Lee, Insup",2012,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR8, CR10, CR9"
"Are Fix-inducing Changes a Moving Target?: A Longitudinal Case Study of Just-in-time Defect Prediction","Change-level defect prediction [5], a.k.a., Just-In-Time (JIT) defect prediction [1], is an alternative to module-level defect prediction that offers several advantages. First, since code changes are often smaller than modules (e.g., classes), JIT predictions are made at a finer granularity, which localizes the inspection process. Second, while modules have a group of authors, changes have only one, which makes triaging JIT predictions easier. Finally, unlike module level prediction, JIT models can scan changes as they are being produced, which means that problems can be investigated while design decisions are still fresh in the developers' minds.

Despite the advantages of JIT defect prediction, like all prediction models, they assume that the properties of past events (fix-inducing changes) are similar to the properties of future ones. This assumption may not hold---the properties of fix-inducing changes in one time period may be different from those of another period. In our paper [4], we set out to address the following central question:

Do the important properties of fix-inducing changes remain consistent as systems evolve?

To address our central question, we train JIT models using six families of code change properties, which are primarily derived from prior studies [1-3, 5]. These properties measure: (a) the magnitude of the change (Size); (b) the dispersion of the changes across modules (Diffusion); (c) the defect proneness of prior changes to the modified modules (History); (d) the experience of the author (Auth. Exp.) and (e) code reviewer(s) (Rev. Exp.); and (f) the amount of participation in the review of the code change (Review).

Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that the answer to our central question is no:

• JIT models lose a large proportion of their discriminatory power (AUC) and calibration (Brier) scores one year after being trained.

• The magnitude of the importance scores of code change properties fluctuate as systems evolve (e.g., Figure 1 shows fluctuations across six-month periods of OpenStack).

• These fluctuations can lead to consistent overestimates (and underestimates) of the future impact of the studied families of code change properties.

To mitigate the impact on model performance, researchers and practitioners should add recently accumulated data to the training set and retrain JIT models to contain fresh data from within the last three months. To better calibrate quality improvement plans (which are based on interpretation of the importance scores of code change properties), researchers and practitioners should put a greater emphasis on larger caches of data, which contain at least six months worth of data, to smooth the effect of spikes and troughs in the importance of properties of fix-inducing changes.","McIntosh, Shane and Kamei, Yasutaka",2018,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Predictive Modeling Methodology for Compiler Phase-ordering","Today's compilers offer a huge number of transformation options to choose among and this choice can significantly impact on the performance of the code being optimized. Not only the selection of compiler options represents a hard problem to be solved, but also the ordering of the phases is adding further complexity, making it a long standing problem in compilation research. This paper presents an innovative approach for tackling the compiler phase-ordering problem by using predictive modeling. The proposed methodology enables i) to efficiently explore compiler exploration space including optimization permutations and repetitions and ii) to extract the application dynamic features to predict the next-best optimization to be applied to maximize the performance given the current status. Experimental results are done by assessing the proposed methodology with utilizing two different search heuristics on the compiler optimization space and it demonstrates the effectiveness of the methodology on the selected set of applications. Using the proposed methodology on average we observed up to 4% execution speedup with respect to LLVM standard baseline.","Ashouri, Amir Hossein and Bignoli, Andrea and Palermo, Gianluca and Silvano, Cristina",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Sparse Representation for Recognizing Object-to-object Actions Under Occlusions","In this paper, we describe the formatting guidelines for ACM SIG Proceedings. This paper proposes a novel event classification scheme to analyze various interaction actions between persons using sparse representation. The occlusion problem and the high complexity to model complicated interactions are two major challenges in person-to-person action analysis. To address the occlusion problem, the proposed scheme represents an action sample in an over-complete dictionary whose base elements are the training samples themselves. This representation is naturally sparse and makes errors (caused by different environmental changes like lighting or occlusions) sparsely appear in the training library. Because of the sparsity, it is robust to occlusions and lighting changes. In addition, a novel Hamming distance classification (HDC) scheme is proposed to classify action events to detailed types. Because the nature of Hamming code is highly tolerant to noise, the HDC scheme is also robust to occlusions. The high complexity of complicated action modeling can be tackled by adding more examples to the over-complete dictionary. Thus, even though the interaction relations are complicated, the proposed method still works successfully to recognize them and can be easily extended to analyze action events among multiple persons. More importantly, the HDC scheme is very efficient and suitable for real-time applications because no optimization process is involved to calculate the reconstruction error.","Hsieh, Jun-We and Chuang, Kai-Ting and Yan, Yilin and Chen, Li-Chih",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Was Self-admitted Technical Debt Removal a Real Removal?: An In-depth Perspective",,"Zampetti, Fiorella and Serebrenik, Alexander and Di Penta, Massimiliano",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Droplet, a Blocks-based Editor for Text Code",,"Bau, David",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Studying Volatility Predictors in Open Source Software","Volatile software modules, for the purposes of this work, are defined as those that are significantly more change-prone than other modules in the same system or subsystem. There is significant literature investigating models for predicting which modules in a system will become volatile, and/or are defect-prone. Much of this work focuses on using source code-related characteristics (e.g., complexity metrics) and simple change metrics (e.g., number of past changes) as inputs to the predictive models. Our work attempts to broaden the array of factors considered in such prediction approaches. To this end, we collected data directly from development personnel about the factors they rely on to foresee what parts of a system are going to become volatile. In this paper, we describe a focus group study conducted with the development team of a small but active open source project, in which we asked this very question. The results of the focus group indicate, among other things, that a period of volatility in a particular area of the system is often predicted by a pattern characterized by inactivity in a certain area (resulting in that area becoming less mature than others), increased communication between developers regarding opportunities for improvement in that area, and then the emergence of a champion who takes the initiative to start working on those improvements. The initial changes lead to more changes (both to extend the improvements already made and to fix problems introduced), thus leading to volatility.","Braunschweig, Brandt and Dhage, Neha and Viera, Maria Jose and Seaman, Carolyn and Sampath, Sreedevi and Koru, Gunes A.",2012,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR12"
"An Exploratory Study on Change Suggestions for Methods Using Clone Detection",,"Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.",2016,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Kahawai: High-Quality Mobile Gaming Using GPU Offload",,"Cuervo, Eduardo and Wolman, Alec and Cox, Landon P. and Lebeck, Kiron and Razeen, Ali and Saroiu, Stefan and Musuvathi, Madanlal",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Conversion: Multi-version Concurrency Control for Main Memory Segments",,"Merrifield, Timothy and Eriksson, Jakob",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Characteristics of Defective Infrastructure As Code Scripts in DevOps","Defects in infrastructure as code (IaC) scripts can have serious consequences for organizations who adopt DevOps. By identifying which characteristics of IaC scripts correlate with defects, we can identify anti-patterns, and help software practitioners make informed decisions on better development and maintenance of IaC scripts, and increase quality of IaC scripts. The goal of this paper is to help practitioners increase the quality of IaC scripts by identifying characteristics of IaC scripts and IaC development process that correlate with defects, and violate security and privacy objectives. We focus on characteristics of IaC scripts and IaC development that (i) correlate with IaC defects, and (ii) violate security and privacy-related objectives namely, confidentiality, availability, and integrity. For our initial studies, we mined open source version control systems from three organizations: Mozilla, Openstack, and Wikimedia, to identify the defect-related characteristics and conduct our case studies. From our empirical analysis, we identify (i) 14 IaC code and four churn characteristics that correlate with defects; and (ii) 12 process characteristics such as, frequency of changes, and ownership of IaC scripts that correlate with defects. We propose the following studies: (i) identify structural characteristics that correlate with defects; (ii) with respect to prediction performance, compare which characteristics of IaC scripts are more correlated with defects; and (iii) identify characteristics that violate security and privacy objectives.","Rahman, Akond",2018,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Coding Tutorials for Any Programming Language or Interactive Tutorials for C and Arduino: (Abstract Only)",,"Sharrock, Remi and Gaultier, Baptiste and Taylor, Petra and Goudzwaard, Michael and Hiron, Mathias and Hamonic, Ella",2018,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Polyhedral AST Generation Is More Than Scanning Polyhedra",,"Grosser, Tobias and Verdoolaege, Sven and Cohen, Albert",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Teaching Cyber Security Using Competitive Software Obfuscation and Reverse Engineering Activities",,"Asghar, Muhammad Rizwan and Luxton-Reilly, Andrew",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Coding Varied Behavior Types Using the Crowd","Social science researchers spend signiï¬cant time annotating behavioral events in video data in order to quantitatively assess interactions [2]. These behavioral events may be instantaneous changes, continuous actions that span unbounded periods of time, or behaviors that would be best described by severity or other scalar ratings. The complexity of these judgments, coupled with the time and eï¬ort required to meticulously assess video, results in a training and evaluation process that can take days or weeks. Computational analysis of video data is still limited due to the challenges introduced by objective interpretation and varied contexts. Glance [4] introduced a means of leveraging human intelligence by recruiting crowds of paid online workers to accurately analyze hours of video data in a matter of minutes. This approach has been shown to expedite work in human-centered ï¬elds, as well as generate training data for automated recognition systems. In this paper, we describe an interactive demonstration of an improved, more expressive version of Glance that expands the initial set of supported annotation formats (e.g. time range, classiï¬cation, etc.) from one to nine. Worker interfaces for each of these options are dynamically generated, along with tutorials, based on the analystâs question. These new features allow analysts to acquire more speciï¬c information about events in video datasets.","Yim, Jinyeong and Jasani, Jeel and Henderson, Aubrey and Koutra, Danai and Dow, Steven P. and Leung, Winnie and Lim, Ellen and Gordon, Mitchell and Bigham, Jeffrey P. and Lasecki, Walter S.",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"A Basic Model for Proactive Event-driven Computing","During the movie ""Source Code"" there is a shift in the plot; from (initially) reacting to a train explosion that already occurred and trying to eliminate further explosions, to (later) changing the reality to avoid the original train explosion. Whereas changing the history after events have happened is still within the science fiction domain, changing the reality to avoid events that have not happened yet is, in many cases, feasible, and may yield significant benefits. We use the term proactive behavior to designate the change of what will be reality in the future. In particular, we focus on proactive event-driven computing: the use of event-driven systems to predict future events and react to them before they occur. In this paper we start our investigation of this large area by constructing a model and end-to-end implementation of a restricted subset of basic proactive applications that is trying to eliminate a single forecasted event, selecting between a finite and relatively small set of feasible actions, known at design time, based on quantified cost functions over time. After laying out the model, we describe the extensions required of the conceptual architecture of event processing to support such applications: supporting proactive agents as part of the model, supporting the derivation of forecasted events, and supporting various aspects of uncertainty; next, we show a decision algorithm that selects among the alternatives. We demonstrate the approach by implementing an example of a basic proactive application in the area of condition based maintenance, and showing experimental results.","Engel, Yagil and Etzion, Opher and Feldman, Zohar",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Taking a Long Look at QUIC: An Approach for Rigorous Evaluation of Rapidly Evolving Transport Protocols",,"Kakhki, Arash Molavi and Jero, Samuel and Choffnes, David and Nita-Rotaru, Cristina and Mislove, Alan",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Generating Reusable Web Components from Mockups","The transformation of a user interface mockup designed by a graphic designer to web components in the final app built by a web developer is often laborious, involving manual and time consuming steps. We propose an approach to automate this aspect of web development by generating reusable web components from a mockup. Our approach employs visual analysis of the mockup, and unsupervised learning of visual cues to create reusable web components (e.g., React components). We evaluated our approach, implemented in a tool called VizMod, on five real-world web mockups, and assessed the transformations and generated components through comparison with web development experts. The results show that VizMod achieves on average 94% precision and 75% recall in terms of agreement with the developers' assessment. Furthermore, the refactorings yielded 22% code reusability, on average.","Bajammal, Mohammad and Mazinanian, Davood and Mesbah, Ali",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"History-based Security for JavaScript",,"Vitek, Jan",2013,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR0"
"Crowdsourcing Code and Process via Code Hunt",,"Xie, Tao and Bishop, Judith and Horspool, R. Nigel and Tillmann, Nikolai and de Halleux, Jonathan",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"An Adaptive Window Length Strategy for Eukaryotic CDS Prediction",,"Shakya, Devendra K. and Saxena, Rajiv and Sharma, Sanjeev N.",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Broadcasting in Noisy Radio Networks",,"Censor-Hillel, Keren and Haeupler, Bernhard and Hershkowitz, D. Ellis and Zuzic, Goran",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Lightweight Approach to Detect Drive-by Download Attacks Based on File Type Transition",,"Shindo, Yasutaka and Satoh, Akihiro and Nakamura, Yutaka and Iida, Katsuyoshi",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Runestone Interactive: Tools for Creating Interactive Course Materials","This demonstration will showcase a work in progress that implements a new and unique vision for electronic computer science textbooks. It incorporates a number of active components such as video, code editing and execution, and code visualization as a way to enhance the typical static electronic book format. In addition, the textbook is created with an open source authoring system that has been developed to allow the instructor to customize the content of the active and passive parts of the text.","Miller, Brad and Ranum, David",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Using Feature Feeds to Improve Developer Awareness in Software Ecosystem Evolution","In many domains organizations need to serve a mass market while at the same time customers request highly individual solutions. Companies thus form software ecosystems (SECOs) comprising various related hardware and software product lines (SPLs). Technology changes, internal enhancements, and customer requests drive the evolution of such SECOs. Multiple projects are conducted in parallel to deliver customized solutions to customers. Developers often adhere to a staged configuration process: first, required software components are selected to derive an initial product, which is then evolved by refining features and adapting source code to meet customer requirements. These customer-specific solutions are often created using a clone-and-own approach and typically contain features potentially reusable in other solutions. However, the awareness of developers about such platform extensions is typically low and feedback from products to SPLs is often lacking. In this research-in-progress paper we thus present a publish-subscribe approach fostering the awareness about feature implementations in SECOs. The approach is based on feature feeds and SECO awareness models.","Lettner, Daniela and Gr\""{u}nbacher, Paul",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Optimizing Irregular Applications for Energy and Performance on the Tilera Many-core Architecture",,"Chavarr\'{\i}a-Miranda, Daniel and Panyala, Ajay and Halappanavar, Mahantesh and Manzano, Joseph B. and Tumeo, Antonino",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Beyond Precision and Recall: Understanding Uses (and Misuses) of Similarity Hashes in Binary Analysis",,"Pagani, Fabio and Dell'Amico, Matteo and Balzarotti, Davide",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Directed Incremental Symbolic Execution",,"Yang, Guowei and Person, Suzette and Rungta, Neha and Khurshid, Sarfraz",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Iterative Schedule Optimization for Parallelization in the Polyhedron Model",,"Ganser, Stefan and Gr\""{o}sslinger, Armin and Siegmund, Norbert and Apel, Sven and Lengauer, Christian",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Impact of Gamification on Code Review Process: An Experimental Study",,"Khandelwal, Shivam and Sripada, Sai Krishna and Reddy, Y. Raghu",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Multimodal Learning Analytics: Description of Math Data Corpus for ICMI Grand Challenge Workshop",,"Oviatt, Sharon and Cohen, Adrienne and Weibel, Nadir",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Vector Abstraction and Concretization for Scalable Detection of Refactorings",,"Milea, Narcisa Andreea and Jiang, Lingxiao and Khoo, Siau-Cheng",2014,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR0"
"Improv: Live Coding for Robot Motion Design",,"Nilles, Alexandra Q. and Beckman, Mattox and Gladish, Chase and LaViers, Amy",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Learning a Discriminative Dictionary for Single-channel Speech Separation",,"Bao, Guangzhao and Xu, Yangfei and Ye, Zhongfu",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Application Runtime Variability and Power Optimization for Exascale Computers",,"Porterfield, Allan and Fowler, Rob and Bhalachandra, Sridutt and Rountree, Barry and Deb, Diptorup and Lewis, Rob",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Refactoring HUBzero for Linked Data","The HUBzero cyberinfrastructure provides a virtual research environment that includes a set of tools for web-based, scientific collaboration and a platform for publishing and using resources such as executable software, source code, images, learning modules, videos, documents, and datasets. Released as open source in 2010, HUBzero has been implemented on a typical LAMP stack (Linux, Apache, MySQL, and PHP) and utilizes the Joomla! content management system. This paper describes the subsequent refactoring of HUBzero to produce and expose Linked Data from its backend, relational database, altering the external expression of the data without changing its internal structure. The Open Archives Initiative Object Reuse and Exchange (OAI-ORE) specification is applied to model the basic structural semantics of HUBzero resources as Nested Aggregations, and data and metadata are mapped to vocabularies such as Dublin Core and published within the web representations of the resources using RDFa. Resource Maps can be harvested using an RDF crawler or an OAI-PMH data provider that were bundled for demonstration purposes. A visualization was produced to browse and navigate the relations among data and metadata from an example hub.","Witt, Michael and Yu, Yongyang",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"An Extensive Analysis of Efficient Bug Prediction Configurations","Background: Bug prediction helps developers steer maintenance activities towards the buggy parts of a software. There are many design aspects to a bug predictor, each of which has several options, i.e., software metrics, machine learning model, and response variable.

Aims: These design decisions should be judiciously made because an improper choice in any of them might lead to wrong, misleading, or even useless results. We argue that bug prediction configurations are intertwined and thus need to be evaluated in their entirety, in contrast to the common practice in the field where each aspect is investigated in isolation.

Method: We use a cost-aware evaluation scheme to evaluate 60 different bug prediction configuration combinations on five open source Java projects.

Results: We find out that the best choices for building a cost-effective bug predictor are change metrics mixed with source code metrics as independent variables, Random Forest as the machine learning model, and the number of bugs as the response variable. Combining these configuration options results in the most efficient bug predictor across all subject systems.

Conclusions: We demonstrate a strong evidence for the interplay among bug prediction configurations and provide concrete guidelines for researchers and practitioners on how to build and evaluate efficient bug predictors.","Osman, Haidar and Ghafari, Mohammad and Nierstrasz, Oscar and Lungu, Mircea",2017,"[""ACM"",""Engineering Village""]","Aceito: CA6","Aceito: CA3, CA2"
"Sensor Network Provenance Compression Using Dynamic Bayesian Networks",,"Wang, Changda and Bertino, Elisa",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Pattern-driven Parallel I/O Tuning",,"Behzad, Babak and Byna, Surendra and Prabhat and Snir, Marc",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Model-driven Approach to Teaching Concurrency",,"Carro, Manuel and Herranz, \'{A}ngel and Mari\~{n}o, Julio",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Generating Custom Code for Efficient Query Execution on Heterogeneous Processors",,"Bre{\$\beta\$}, Sebastian and K\""{o}cher, Bastian and Funke, Henning and Zeuch, Steffen and Rabl, Tilmann and Markl, Volker",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Challenging Stereotypes and Changing Attitudes: The Effect of a Brief Programming Encounter on Adults' Attitudes Toward Programming",,"Charters, Polina and Lee, Michael J. and Ko, Andrew J. and Loksa, Dastyni",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Optimal DNN Primitive Selection with Partitioned Boolean Quadratic Programming",,"Anderson, Andrew and Gregg, David",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Measuring the Quality of Issue Tracking Data",,"Tu, Feifei and Zhang, Feixue",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"TFX: A TensorFlow-Based Production-Scale Machine Learning Platform",,"Baylor, Denis and Breck, Eric and Cheng, Heng-Tze and Fiedel, Noah and Foo, Chuan Yu and Haque, Zakaria and Haykal, Salem and Ispir, Mustafa and Jain, Vihan and Koc, Levent and Koo, Chiu Yuen and Lew, Lukasz and Mewald, Clemens and Modi, Akshay Naresh and Polyzotis, Neoklis and Ramesh, Sukriti and Roy, Sudip and Whang, Steven Euijong and Wicke, Martin and Wilkiewicz, Jarek and Zhang, Xin and Zinkevich, Martin",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Interpretable Representation Learning for Healthcare via Capturing Disease Progression Through Time",,"Bai, Tian and Zhang, Shanshan and Egleston, Brian L. and Vucetic, Slobodan",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Segregating Feature Interfaces to Support Software Product Line Maintenance",,"Cafeo, Bruno B. P. and Hunsen, Claus and Garcia, Alessandro and Apel, Sven and Lee, Jaejoon",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval",,"Mor\`{e}re, Olivier and Lin, Jie and Veillard, Antoine and Duan, Ling-Yu and Chandrasekhar, Vijay and Poggio, Tomaso",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Learning Curve Analysis for Programming: Which Concepts Do Students Struggle With?",,"Rivers, Kelly and Harpstead, Erik and Koedinger, Ken",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Automatic Query Reformulations for Text Retrieval in Software Engineering",,"Haiduc, Sonia and Bavota, Gabriele and Marcus, Andrian and Oliveto, Rocco and De Lucia, Andrea and Menzies, Tim",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"LSOD: Local Sparse Orthogonal Descriptor for Image Matching",,"Zhao, Yiru and Li, Yaoyi and Shao, Zhiwen and Lu, Hongtao",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"From Drawing to Programming: Attracting Middle-school Students to Programming Through Self-disclosing Code (Abstract Only)",,"Hall, Pelle and Hirakawa, Andrew and Nystrom, Jennelle and Rebelsky, Samuel A.",2012,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Employing Retention of Flow to Improve Online Tutorials",,"Basawapatna, Ashok and Repenning, Alexander",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Predicting Protein Function Using Multiple Kernels",,"Yu, Guoxian and Rangwala, Huzefa and Domeniconi, Carlotta and Zhang, Guoji and Zhang, Zili",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Transforming Blocking MPI Collectives to Non-blocking and Persistent Operations",,"Ahmed, Hadia and Skjellumh, Anthony and Bangalore, Purushotham and Pirkelbauer, Peter",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Automated Testing of GUI Applications: Models, Tools, and Controlling Flakiness",,"Memon, Atif M. and Cohen, Myra B.",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Improving Scenario Selection for Simulations by Run-time Control-flow Analysis",,"Berger, Christian",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Client-oriented Web Alteration Detection System Using Link Change State of a Web Page Based on Past and Current Page Content",,"Mochizuki, Shouta and Takada, Tetsuji",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Learn Java in N Games (Abstract Only)",,"Drake, Peter and Goadrich, Mark",2014,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"A Field Study of Refactoring Challenges and Benefits","It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring benefits or investigate developers' perception towards these benefits. This paper presents a field study of refactoring benefits and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey finds that the refactoring definition in practice is not confined to a rigorous definition of semantics-preserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized effort on refactoring Windows. The quantitative analysis of Windows 7 version history finds that the binary modules refactored by this team experienced significant reduction in the number of inter-module dependencies and post-release defects, indicating a visible benefit of refactoring.","Kim, Miryung and Zimmermann, Thomas and Nagappan, Nachiappan",2012,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR11"
"Data Mining the Memory Access Stream to Detect Anomalous Application Behavior",,"Moreira, Francis B. and Diener, Matthias and Navaux, Philippe O. A. and Koren, Israel",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Code Vectors: Understanding Programs Through Embedded Abstracted Symbolic Traces",,"Henkel, Jordan and Lahiri, Shuvendu K. and Liblit, Ben and Reps, Thomas",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Algebraic Foundations for Effect-dependent Optimisations",,"Kammar, Ohad and Plotkin, Gordon D.",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Can I Clone This Piece of Code Here?","While code cloning is a convenient way for developers to reuse existing code, it may potentially lead to negative impacts, such as degrading code quality or increasing maintenance costs. Actually, some cloned code pieces are viewed as harmless since they evolve independently, while some other cloned code pieces are viewed as harmful since they need to be changed consistently, thus incurring extra maintenance costs. Recent studies demonstrate that neither the percentage of harmful code clones nor that of harmless code clones is negligible. To assist developers in leveraging the benefits of harmless code cloning and/or in avoiding the negative impacts of harmful code cloning, we propose a novel approach that automatically predicts the harmfulness of a code cloning operation at the point of performing copy-and-paste. Our insight is that the potential harmfulness of a code cloning operation may relate to some characteristics of the code to be cloned and the characteristics of its context. Based on a number of features extracted from the cloned code and the context of the code cloning operation, we use Bayesian Networks, a machine-learning technique, to predict the harmfulness of an intended code cloning operation. We evaluated our approach on two large-scale industrial software projects under two usage scenarios: 1) approving only cloning operations predicted to be very likely of no harm, and 2) blocking only cloning operations predicted to be very likely of harm. In the first scenario, our approach is able to approve more than 50% cloning operations with a precision higher than 94.9% in both subjects. In the second scenario, our approach is able to avoid more than 48% of the harmful cloning operations by blocking only 15% of the cloning operations for the first subject, and avoid more than 67% of the cloning operations by blocking only 34% of the cloning operations for the second subject.","Wang, Xiaoyin and Dang, Yingnong and Zhang, Lu and Zhang, Dongmei and Lan, Erica and Mei, Hong",2012,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Pix2Code: Generating Code from a Graphical User Interface Screenshot",,"Beltramelli, Tony",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Prototyping Real-time Tracking Systems on Mobile Devices",,"Lee, Kyunghun and Ben Salem, Haifa and Damarla, Thyagaraju and Stechele, Walter and Bhattacharyya, Shuvra S.",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR10"
"Subtype Polymorphism \`{A} La Carte via Machine Learning on Dependent Types",,"Swan, Jerry and Johnson, Colin G. and Brady, Edwin C.",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Exploring Experienced Professionals\&Rsquo; Reflections on Computing Education",,"Exter, Marisa and Turnage, Nichole",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Binary Coding by Matrix Classifier for Efficient Subspace Retrieval",,"Zhou, Lei and Bai, Xiao and Liu, Xianglong and Zhou, Jun",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Social Coding in GitHub: Transparency and Collaboration in an Open Software Repository",,"Dabbish, Laura and Stuart, Colleen and Tsay, Jason and Herbsleb, Jim",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR0"
"Obfuscation Resilient Search Through Executable Classification",,"Su, Fang-Hsiang and Bell, Jonathan and Kaiser, Gail and Ray, Baishakhi",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"From Fine Grained Code Diversity to JIT-ROP to Execute-Only Memory: The Cat and Mouse Game Between Attackers and Defenders Continues",,"Franz, Michael",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Batch Model for Batched Timestamps Data Analysis with Application to the SSA Disability Program",,"Yue, Qingqi and Yuan, Ao and Che, Xuan and Huynh, Minh and Zhou, Chunxiao",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"LASER: A Scalable Response Prediction Platform for Online Advertising",,"Agarwal, Deepak and Long, Bo and Traupman, Jonathan and Xin, Doris and Zhang, Liang",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"AuDroid: Preventing Attacks on Audio Channels in Mobile Devices",,"Petracca, Giuseppe and Sun, Yuqiong and Jaeger, Trent and Atamli, Ahmad",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Binary Similarity Detection Using Machine Learning","Finding similar procedures in stripped binaries has various use cases in the domains of cyber security and intellectual property. Previous works have attended this problem and came up with approaches that either trade throughput for accuracy or address a more relaxed problem.

In this paper, we present a cross-compiler-and-architecture approach for detecting similarity between binary procedures, which achieves both high accuracy and peerless throughput. For this purpose, we employ machine learning alongside similarity by composition: we decompose the code into smaller comparable fragments, transform these fragments to vectors, and build machine learning-based predictors for detecting similarity between vectors that originate from similar procedures.

We implement our approach in a tool called Zeek and evaluate it by searching similarities in open source projects that we crawl from the world-wide-web. Our results show that we perform 250X faster than state-of-the-art tools without harming accuracy.","Shalev, Noam and Partush, Nimrod",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Process Approach in Realizing Common Software Platform: A Dream Come True",,"Rv, Prasanth and Parakal, Elizabeth",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"An Abstract Interpretation Framework for Refactoring with Application to Extract Methods with Contracts",,"Cousot, Patrick M. and Cousot, Radhia and Logozzo, Francesco and Barnett, Michael",2012,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR0"
"A Multi-objective Auto-tuning Framework for Parallel Codes",,"Jordan, Herbert and Thoman, Peter and Durillo, Juan J. and Pellegrini, Simone and Gschwandtner, Philipp and Fahringer, Thomas and Moritsch, Hans",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Co-designing a Physical to Digital Experience for an Onboarding and Blended Learning Platform",,"Lochrie, Mark and Matthys, Glenn and Gradinar, Adrian and Dickinson, Andy and Baudouin, Onno and Egglestone, Paul",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"On Binary Embedding Using Circulant Matrices",,"Yu, Felix X. and Bhaskara, Aditya and Kumar, Sanjiv and Gong, Yunchao and Chang, Shih-Fu",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Comparison of Expressive Speech Synthesis Approaches Based on Neural Network",,"Xue, Liumeng and Zhu, Xiaolian and An, Xiaochun and Xie, Lei",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Smart Arse: Posture Classification with Textile Sensors in Trousers",,"Skach, Sophie and Stewart, Rebecca and Healey, Patrick G. T.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Predicting Program Properties from ""Big Code""","We present a new approach for predicting program properties from massive codebases (aka ""Big Code""). Our approach first learns a probabilistic model from existing data and then uses this model to predict properties of new, unseen programs.

The key idea of our work is to transform the input program into a representation which allows us to phrase the problem of inferring program properties as structured prediction in machine learning. This formulation enables us to leverage powerful probabilistic graphical models such as conditional random fields (CRFs) in order to perform joint prediction of program properties.

As an example of our approach, we built a scalable prediction engine called JSNice for solving two kinds of problems in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNice predicts correct names for 63% of name identifiers and its type annotation predictions are correct in 81% of the cases. In the first week since its release, JSNice was used by more than 30,000 developers and in only few months has become a popular tool in the JavaScript developer community.

By formulating the problem of inferring program properties as structured prediction and showing how to perform both learning and inference in this context, our work opens up new possibilities for attacking a wide range of difficult problems in the context of ""Big Code"" including invariant generation, decompilation, synthesis and others.","Raychev, Veselin and Vechev, Martin and Krause, Andreas",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR10"
"3D Nanosystems Enable Embedded Abundant-data Computing: Special Session Paper",,"Hwang, William and Aly, Mohamed M. Sabry and Malviya, Yash H. and Gao, Mingyu and Wu, Tony F. and Kozyrakis, Christos and Wong, H.-S. Philip and Mitra, Subhasish",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9"
"SIFT's Scale-space Extrema Detection on GPU for Real-time Applications (WIP)",,"Kumar, R. P. and Muknahallipatna, S. S. and McInroy, J. E.",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"ArIA: Arduino Code Generation Based on the CAPS",,"Sharaf, Mohammad and Muccini, Henry and Abughazala, Moamin",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Recovering Traceability Links Between an API and Its Learning Resources",,"Dagenais, Barth{\'e}l{\'e}my and Robillard, Martin P.",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Short Mobile Game Development Projects for CS1/2",,"Kurkovsky, Stan and Defoe, Delvin",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Towards a (Semi)-automatic Reference Process to Support the Reverse Engineering and Reconstruction of Software Architectures",,"Guam\'{a}n, Daniel and P{\'e}rez, Jennifer and D\'{\i}az, Jessica",2018,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR0"
"Temporal Frequency Probing for 5D Transient Analysis of Global Light Transport",,"O'Toole, Matthew and Heide, Felix and Xiao, Lei and Hullin, Matthias B. and Heidrich, Wolfgang and Kutulakos, Kiriakos N.",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"OpenMP-style Parallelism in Data-centered Multicore Computing with R",,"Jiang, Lei and Patel, Pragneshkumar B. and Ostrouchov, George and Jamitzky, Ferdinand",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Privacy Amplification with Asymptotically Optimal Entropy Loss",,"Chandran, Nishanth and Kanukurthi, Bhavana and Ostrovsky, Rafail and Reyzin, Leonid",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Synthesizing Programs That Expose Performance Bottlenecks",,"Toffola, Luca Della and Pradel, Michael and Gross, Thomas R.",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Accelerating an MPI Lattice Boltzmann Code Using OpenACC",,"Blair, Stu and Albing, Carl and Grund, Alexander and Jocksch, Andreas",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Supporting Developers' Coordination in the IDE","Teamwork in software engineering is time-consuming and problematic. In this paper, we explore how to better support developers' collaboration in teamwork, focusing on the software implementation phase happening in the integrated development environment (IDE). Conducting a qualitative investigation, we learn that developers' teamwork needs mostly regard coordination, rather than concurrent work on the same (sub)task, and that developers successfully deal with scenarios considered problematic in literature, but they have problems dealing with breaking changes made by peers on the same project. We derive implications and recommendations. Based on one of the latter, we analyze the current IDE support for receiving code changes, finding that historical information is neither visible nor easily accessible. Consequently, we devise and qualitatively evaluate Bellevue, the design of an IDE extension to make received changes always visible and code history accessible in the editor.","Guzzi, Anja and Bacchelli, Alberto and Riche, Yann and van Deursen, Arie",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Sigma*: Symbolic Learning of Input-output Specifications","We present Sigma*, a novel technique for learning symbolic models of software behavior. Sigma* addresses the challenge of synthesizing models of software by using symbolic conjectures and abstraction. By combining dynamic symbolic execution to discover symbolic input-output steps of the programs and counterexample guided abstraction refinement to over-approximate program behavior, Sigma* transforms arbitrary source representation of programs into faithful input-output models. We define a class of stream filters---programs that process streams of data items---for which Sigma* converges to a complete model if abstraction refinement eventually builds up a sufficiently strong abstraction. In other words, Sigma* is complete relative to abstraction. To represent inferred symbolic models, we use a variant of symbolic transducers that can be effectively composed and equivalence checked. Thus, Sigma* enables fully automatic analysis of behavioral properties such as commutativity, reversibility and idempotence, which is useful for web sanitizer verification and stream programs compiler optimizations, as we show experimentally. We also show how models inferred by Sigma* can boost performance of stream programs by parallelized code generation.","Botin\v{c}an, Matko and Babi\'{c}, Domagoj",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Mutual Assessment in the Social Programmer Ecosystem: An Empirical Investigation of Developer Profile Aggregators","The multitude of social media channels that programmers can use to participate in software development has given rise to online developer profiles that aggregate activity across many services. Studying members of such developer profile aggregators, we found an ecosystem that revolves around the social programmer. Developers are assessing each other to evaluate whether other developers are interesting, worth following, or worth collaborating with. They are self-conscious about being assessed, and thus manage their public images. They value passion for software development, new technologies, and learning. Some recruiters participate in the ecosystem and use it to find candidates for hiring; other recruiters struggle with the interpretation of signals and issues of trust. This mutual assessment is changing how software engineers collaborate and how they advance their skills.","Singer, Leif and Figueira Filho, Fernando and Cleary, Brendan and Treude, Christoph and Storey, Margaret-Anne and Schneider, Kurt",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Employing Neural Networks for the Detection of SQL Injection Attack",,"Sheykhkanloo, Naghmeh Moradpoor",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Language, Thinking, Code: Interactive Essays with Twine (Abstract Only)",,"Tirto, Darren and Hamme, Alexander and O'Hara, Keith J. and Anderson, Sven",2018,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"A Survey of Appearance Models in Visual Object Tracking",,"Li, Xi and Hu, Weiming and Shen, Chunhua and Zhang, Zhongfei and Dick, Anthony and Hengel, Anton Van Den",2013,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"An Abstract Stack Based Approach to Verified Compositional Compilation to Machine Code",,"Wang, Yuting and Wilke, Pierre and Shao, Zhong",2019,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Proposed Sizing Model for Managing 3rd Party Code Technical Debt",,"Snipes, Will and Ramaswamy, Srini",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"""Come Code with Codester"": An Educational App That Teaches Computer Science (Abstract Only)",,"Rusak, Gili and Lim, Darren",2014,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Types of Computational Self-awareness and How We Might Implement Them","Computing systems increasingly comprise large numbers of heterogeneous sub-systems, each with their own local perspective and goals, connected in dynamic networks, and interacting with each other and humans in ways which are difficult to predict. Nevertheless, users engaging with different parts of the system still expect high performance, reliability, security and other qualities, provided in a way that is robust or adaptive in the presence of unforeseen changes (including to users, the network, physical environment or the system itself). Examples of systems which are facing this challenge are wide-ranging and include robot swarms, personal devices, web services and sensor networks. In all these cases, advanced levels of autonomous behaviour can enable the system to adapt itself at run time, by learning behaviours in real time, appropriate to changing conditions.","Lewis, Peter R.",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Using Loops For Malware Classification Resilient to Feature-unaware Perturbations",,"Machiry, Aravind and Redini, Nilo and Gustafson, Eric and Fratantonio, Yanick and Choe, Yung Ryn and Kruegel, Christopher and Vigna, Giovanni",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Tf-idf Based Topic Model for Identifying lncRNAs from Genomic Background",,"Madhavan, Manu and G, Gopakumar",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"SPAIN: Security Patch Analysis for Binaries Towards Understanding the Pain and Pills",,"Xu, Zhengzi and Chen, Bihuan and Chandramohan, Mahinthan and Liu, Yang and Song, Fu",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Multi-Target C Code Generation from MATLAB",,"Bispo, Jo\~{a}o and Reis, Lu\'{\i}s and Cardoso, Jo\~{a}o M. P.",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Automated Compiler Optimization of Multiple Vector Loads/Stores",,"Aleen, Farhana and Zakharin, Vyacheslav P. and Krishaniyer, Rakesh and Gupta, Garima and Kreitzer, David and Lin,Jr, Chang-Sun",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"AODP: Refactoring Code to Provide Advanced Aspect-oriented Modularization of Design Patterns",,"Giunta, Rosario and Pappalardo, Giuseppe and Tramontana, Emiliano",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR0"
"Efficient Binary Coding for Subspace-based Query-by-Image Video Retrieval",,"Xu, Ruicong and Yang, Yang and Shen, Fumin and Xie, Ning and Shen, Heng Tao",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Analyzing PHP Frameworks for Use in a Project-based Software Engineering Course",,"Lancor, Lisa and Katha, Samyukta",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"CSTutor: A Pen-based Tutor for Data Structure Visualization",,"Buchanan, Sarah and Ochs, Brandon and LaViola Jr., Joseph J.",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Time- and Space-efficient Flow-sensitive Points-to Analysis",,"Nasre, Rupesh",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Exploring the Utility of Developer Exhaust","Using machine learning to analyze data often results in developer exhaust -- code, logs, or metadata that do not define the learning algorithm but are byproducts of the data analytics pipeline. We study how the rich information present in developer exhaust can be used to approximately solve otherwise complex tasks. Specifically, we focus on using log data associated with training deep learning models to perform model search by predicting performance metrics for untrained models. Instead of designing a different model for each performance metric, we present two preliminary methods that rely only on information present in logs to predict these characteristics for different architectures. We introduce (i) a nearest neighbor approach with a hand-crafted edit distance metric to compare model architectures and (ii) a more generalizable, end-to-end approach that trains an LSTM using model architectures and associated logs to predict performance metrics of interest. We perform model search optimizing for best validation accuracy, degree of overfitting, and best validation accuracy given a constraint on training time. Our approaches can predict validation accuracy within 1.37% error on average, while the baseline achieves 4.13% by using the performance of a trained model with the closest number of layers. When choosing the best performing model given constraints on training time, our approaches select the top-3 models that overlap with the true top-3 models 82% of the time, while the baseline only achieves this 54% of the time. Our preliminary experiments hold promise for how developer exhaust can help learn models that can approximate various complex tasks efficiently.","Zhang, Jian and Lam, Max and Wang, Stephanie and Varma, Paroma and Nardi, Luigi and Olukotun, Kunle and Re, Christopher",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Fraud Detection via Coding Nominal Attributes",,"Jianyu, Wang and Chunming, Wu and Shouling, Ji and Qinchen, Gu and Zhao, Li",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Testing Cyber-Physical Systems Through Bayesian Optimization",,"Deshmukh, Jyotirmoy and Horvat, Marko and Jin, Xiaoqing and Majumdar, Rupak and Prabhu, Vinayak S.",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Aggregated Machine Learning on Indicators of Compromise in Android Devices",,"San Miguel, John M. and Kline, Megan E. M. and Hallman, Roger A. and Slayback, Scott M. and Rogers, Alexis and Chang, Stefanie S. F.",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"A Transactional Correctness Tool for Abstract Data Types",,"Peterson, Christina and Dechev, Damian",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Learning to Aggregate: An Automated Aggregation Method for Software Quality Model","Quality models are regarded as a well-accepted approach for assessing high-level abstract quality characteristics (e.g., maintainability) by aggregation from low-level metrics. However, most of the existing quality models adopt the weighted linear aggregation method which suffers from a lack of consensus in how to decide the correct weights. To address this issue, we present an automated aggregation method which adopts a kind of probabilistic weight instead of the subjective weight in previous aggregation methods. In particular, we utilize a topic modeling technique to estimate the probabilistic weight by learning from a software benchmark. In this manner, our approach can enable automated quality assessment by using the learned knowledge without manual effort. In addition, we conduct an application on the maintainability assessment of the systems in our benchmark. The result shows that our approach can reveal the maintainability well through a correlation analysis with the changed lines of code.","Yan, Meng and Zhang, Xiaohong and Liu, Chao and Zou, Jie and Xu, Ling and Xia, Xin",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Refactoring Dynamic Languages",,"Reia, Rafael and Menezes Leit\~{a}o, Ant\'{o}nio",2016,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Automated Verification Code Generation in HLS Using Software Execution Traces (Abstract Only)",,"Yang, Liwei and Gurumani, Swathi and Fahmy, Suhaib A. and Chen, Deming and Rupnow, Kyle",2016,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"A Methodology for Learning, Analyzing, and Mitigating Social Influence Bias in Recommender Systems","The seminal 2003 paper by Cosley, Lab, Albert, Konstan, and Reidl, demonstrated the susceptibility of recommender systems to rating biases. To facilitate browsing and selection, almost all recommender systems display average ratings before accepting ratings from users which has been shown to bias ratings. This effect is called Social Inuence Bias (SIB); the tendency to conform to the perceived \norm"" in a community. We propose a methodology to 1) learn, 2) analyze, and 3) mitigate the effect of SIB in recommender systems. In the Learning phase, we build a baseline dataset by allowing users to rate twice: before and after seeing the average rating. In the Analysis phase, we apply a new non-parametric significance test based on the Wilcoxon statistic to test whether the data is consistent with SIB. If significant, we propose a Mitigation phase using polynomial regression and the Bayesian Information Criterion (BIC) to predict unbiased ratings. We evaluate our approach on a dataset of 9390 ratings from the California Report Card (CRC), a rating-based system designed to encourage political engagement. We found statistically significant evidence of SIB. Mitigating models were able to predict changed ratings with a normalized RMSE of 12.8% and reduce bias by 76.3%. The CRC, our data, and experimental code are available at: http://californiareportcard.org/data/","Krishnan, Sanjay and Patel, Jay and Franklin, Michael J. and Goldberg, Ken",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Learning Multi-view Deep Features for Small Object Retrieval in Surveillance Scenarios",,"Guo, Haiyun and Wang, Jinqiao and Xu, Min and Zha, Zheng-Jun and Lu, Hanqing",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Deep Learning Pulsed-based Convolutional Neuroprocessor Architecture on FPGAs",,"Carballo-Hern\'{a}ndez, Walther and Arias-Estrada, Miguel",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Safe and Flexible Adaptation via Alternate Data Structure Representations",,"Kusum, Amlan and Neamtiu, Iulian and Gupta, Rajiv",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Bringing Real-Time Collaboration to Visual Programming (Abstract Only)",,"Broll, Brian and Ledeczi, Akos",2017,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Kitsune: Efficient, General-Purpose Dynamic Software Updating for C",,"Hayden, Christopher M. and Saur, Karla and Smith, Edward K. and Hicks, Michael and Foster, Jeffrey S.",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Lines of Malicious Code: Insights into the Malicious Software Industry",,"Lindorfer, Martina and Di Federico, Alessandro and Maggi, Federico and Comparetti, Paolo Milani and Zanero, Stefano",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Run-time Resource Management Based on Design Space Exploration",,"Ykman-Couvreur, Chantal and Hartmann, Philipp A. and Palermo, Gianluca and Colas-Bigey, Fabien and San, Laurent",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Restricted Isometry of Fourier Matrices and List Decodability of Random Linear Codes",,"Cheraghchi, Mahdi and Guruswami, Venkatesan and Velingker, Ameya",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"CACTI 7: New Tools for Interconnect Exploration in Innovative Off-Chip Memories",,"Balasubramonian, Rajeev and Kahng, Andrew B. and Muralimanohar, Naveen and Shafiee, Ali and Srinivas, Vaishnav",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Linking Language \&\#38; Thinking with Code: Computing Within a Writing-Intensive Introduction to the Liberal Arts",,"O'Hara, Keith J. and Burke, Kathleen and Ruggiero, Diana and Anderson, Sven",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Android Apps and User Feedback: A Dataset for Software Evolution and Quality Improvement","Nowadays, Android represents the most popular mobile platform with a market share of around 80%. Previous research showed that data contained in user reviews and code change history of mobile apps represent a rich source of information for reducing software maintenance and development effort, increasing customers' satisfaction. Stemming from this observation, we present in this paper a large dataset of Android applications belonging to 23 different apps categories, which provides an overview of the types of feedback users report on the apps and documents the evolution of the related code metrics. The dataset contains about 395 applications of the F-Droid repository, including around 600 versions, 280,000 user reviews and more than 450,000 user feedback (extracted with specific text mining approaches). Furthermore, for each app version in our dataset, we employed the Paprika tool and developed several Python scripts to detect 8 different code smells and compute 22 code quality indicators. The paper discusses the potential usefulness of the dataset for future research in the field.","Grano, Giovanni and Di Sorbo, Andrea and Mercaldo, Francesco and Visaggio, Corrado A. and Canfora, Gerardo and Panichella, Sebastiano",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Infants' Brains Are Wired to Learn from Culture: Implications for Social Robots",,"Meltzoff, Andrew N.",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"POSTER: A Framework for Phylogenetic Analysis in Mobile Environment",,"Martinelli, Fabio and Mercaldo, Francesco and Ssaracino, Andrea",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Assessment of Introducing Algorithms with Video Lectures and Pseudocode Rhymed to a Melody",,"Schreiber, Benjamin J. and Dougherty, John P.",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Looking Glass","Looking Glass is the successor to Storytelling Alice designed for middle and high school students. By dragging and dropping, users can construct programs that direct the behavior of characters in a 3D scene. The system consists of a downloadable application and an online community.

Looking Glass is intended to support creative, open-ended programming. To help spark ideas, the Looking Glass community hosts a set of community-created templates and remixes, animations that can be imported into any new world. These templates and remixes can help to inspire a story line. In more formal settings, templates and remixes allow an instructor to provide shared resources for an assignment and for students to collaborate with each other.

Exploring the behavior of existing code examples can be a powerful way to learn new skills. In addition to providing story inspiration, community remixes serve as code examples that students can explore. To support users in learning new skills based on code shared through remixes, Looking Glass includes a play and explore mode in which the environment records an execution history and changes to the 3D scene as a program runs. Users can scroll back in time, see which actions were executing, and replay individual statements.

A free download is available at lookingglass.wustl.edu for Windows, Mac, and Linux.","Kelleher, Caitlin",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Prototyping a Functional Language Using Higher-order Logic Programming: A Functional Pearl on Learning the Ways of {\$\lambda\$}Prolog/Makam",,"Stampoulis, Antonis and Chlipala, Adam",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Multidisciplinary Research and Education with Open Tools: Metagenomic Analysis of 16S rRNA Using Arduino, Android, Mothur and XSEDE",,"Muterspaw, Kristin and Urner, Tara and Lewis, Ruth and Babic, Ivan and Srinath, Deeksha and Peck, Charles and Cerda-Granados, David and Lemiszki, Peter and S\'{a}nchez-Miranda, Mar\'{\i}a and Mayorga-M{\'e}ndez, Mercedes and Petursson, Olafur and Smith, Ben",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"DS.Js: Turn Any Webpage into an Example-Centric Live Programming Environment for Learning Data Science",,"Zhang, Xiong and Guo, Philip J.",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Using A Visual Programming Environment and Custom Robots to Learn C Programming and K-12 STEM Concepts",,"Krishnamoorthy, Sai Prasanth and Kapila, Vikram",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Introduction to Modern OpenGL Programming",,"Angel, Edward and Shreiner, Dave",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Clairvoyant Approach to Evaluating Software (In)Security",,"Jain, Bhushan and Tsai, Chia-Che and Porter, Donald E.",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"A Study on Inappropriately Partitioned Commits: How Much and What Kinds of IP Commits in Java Projects?","When we use code repositories, each commit should include code changes for only a single task and code changes for a single task should not be scattered over multiple commits. There are many studies on the former violation-often referred to as tangled commits- but the latter violation has been out of scope for MSR research. In this paper, we firstly investigate how much and what kinds of inappropriately partitioned commits in Java projects. Then, we propose a simple technique to detect such commits automatically. We also report evaluation results of the proposed technique.","Arima, Ryo and Higo, Yoshiki and Kusumoto, Shinji",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Refactoring with Synthesis","Refactoring has become an integral part of modern software development, with wide support in popular integrated development environments (IDEs). Modern IDEs provide a fixed set of supported refactorings, listed in a refactoring menu. But with IDEs supporting more and more refactorings, it is becoming increasingly difficult for programmers to discover and memorize all their names and meanings. Also, since the set of refactorings is hard-coded, if a programmer wants to achieve a slightly different code transformation, she has to either apply a (possibly non-obvious) sequence of several built-in refactorings, or just perform the transformation by hand.

We propose a novel approach to refactoring, based on synthesis from examples, which addresses these limitations. With our system, the programmer need not worry how to invoke individual refactorings or the order in which to apply them. Instead, a transformation is achieved via three simple steps: the programmer first indicates the start of a code refactoring phase; then she performs some of the desired code changes manually; and finally, she asks the tool to complete the refactoring.

Our system completes the refactoring by first extracting the difference between the starting program and the modified version, and then synthesizing a sequence of refactorings that achieves (at least) the desired changes. To enable scalable synthesis, we introduce local refactorings, which allow for first discovering a refactoring sequence on small program fragments and then extrapolating it to a full refactoring sequence.

We implemented our approach as an Eclipse plug-in, with an architecture that is easily extendable with new refactorings. The experimental results are encouraging: with only minimal user input, the synthesizer was able to quickly discover complex refactoring sequences for several challenging realistic examples.","Raychev, Veselin and Sch\""{a}fer, Max and Sridharan, Manu and Vechev, Martin",2013,"[""ACM"",""Engineering Village""]","Aceito: CA2","Aceito: CA2"
"On-stack Replacement, Distilled",,"D\&\#39;Elia, Daniele Cono and Demetrescu, Camil",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"IBM Worklight Hands-on Workshop",,"Fung, Jane and Moghal, Salman and Sheikh, Ozair and Ivanov, Vess and Marini, John",2012,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Parallelization Hints via Code Skeletonization",,"Aguston, Cfir and Ben Asher, Yosi and Haber, Gadi",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Bots vs. Wikipedians, Anons vs. Logged-ins",,"Steiner, Thomas",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"How I Learned to Stop Worrying and Love In Situ Analytics: Leveraging Latent Synchronization in MPI Collective Algorithms",,"Levy, Scott and Ferreira, Kurt B. and Widener, Patrick and Bridges, Patrick G. and Mondragon, Oscar H.",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"The Backbone Project (Abstract Only)",,"Shah, Dharmin",2015,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"A Replication Case Study to Measure the Architectural Quality of a Commercial System",,"Reimanis, Derek and Izurieta, Clemente and Luhr, Rachael and Xiao, Lu and Cai, Yuanfang and Rudy, Gabe",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Test Framework for Jenkins Shared Libraries",,"Antunes, Renan Vieira and Navarro, Gabriela Matias and Hanazumi, Simone",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Computing in the Physical World Engages Students: Impact on Their Attitudes and Self-efficacy Towards Computer Science Through Robotic Activities",,"Theodoropoulos, Anastasios and Leon, Prokopis and Antoniou, Angeliki and Lepouras, George",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"From Scratch to Patch: Easing the Blocks-Text Transition",,"Robinson, William",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Automation for Instruction Enhancing Feedback: (Abstract Only)",,"Gusukuma, Luke",2018,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Managing Forked Product Variants",,"Rubin, Julia and Kirshin, Andrei and Botterweck, Goetz and Chechik, Marsha",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Mobile Scaffolding Application to Support Novice Learners of Computer Programming",,"Mbogo, Chao and Blake, Edwin and Suleman, Hussein",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Student Projects Are Not Throwaways: Teaching Practical Software Maintenance in a Software Engineering Course",,"Szabo, Claudia",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Business Process Management (BPM) in a Day",,"Bist, Gary and Cheung, Kenneth K.",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"GitHub in the Classroom: Not Just for Group Projects",,"Griffin, Terry and Seals, Shawn",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Pex4Fun: A Web-based Environment for Educational Gaming via Automated Test Generation",,"Tillmann, Nikolai and De Halleux, Jonathan and Xie, Tao and Bishop, Judith",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Judging a Commit by Its Cover: Correlating Commit Message Entropy with Build Status on travis-CI",,"Santos, Eddie Antonio and Hindle, Abram",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Constraints and Modularity (Keynote)",,"Borning, Alan",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Recommending Relevant Classes for Bug Reports Using Multi-objective Search",,"Almhana, Rafi and Mkaouer, Wiem and Kessentini, Marouane and Ouni, Ali",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"A ""Grand Tour"" of Computer Science: Re-Designing CS1 for Breadth and Retention (Abstract Only)",,"Linnell, Natalie and Tran, Nicholas",2016,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"Holistic Development of Underrepresented Students Through Academic: Industry Partnerships",,"Burge, Legand and Mejias, Marlon and Galloway, KaMar and Gosha, Kinnis and Muhammad, Jean",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Fast and Accurate Maximum Inner Product Recommendations on Map-Reduce",,"Hall, Rob and Attenberg, Josh",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Digest of ACM Educational Activities",,"McGettrick, Andrew and Timanovsky, Yan",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Codespells: How to Design Quests to Teach Java Concepts",,"Esper, Sarah and Wood, Samantha R. and Foster, Stephen R. and Lerner, Sorin and Griswold, William G.",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Destination-passing Style for Efficient Memory Management",,"Shaikhha, Amir and Fitzgibbon, Andrew and Peyton Jones, Simon and Vytiniotis, Dimitrios",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Indexing the Real World: Sensing, Big Data and Mobility",,"Tirri, Henry",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0, CR0"
"Exploring API: Client Co-evolution","Software libraries evolve over time, as do their APIs and the clients that use them. Studying this co-evolution of APIs and API clients can give useful insights into both how to manage the co-evolution, and how to design software so that it is more resilient against API changes.

In this paper, we discuss problems and challenges of API and client code co-evolution, and the tools and methods we will need to resolve them.","Eilertsen, Anna Maria and Bagge, Anya Helene",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"On Testing the Source Compatibility in Java",,"H\'{y}bl, Jan and Tron\'{\i}\v{c}ek, Zden\v{e}k",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Google's Go and Dart: Parallelism and Structured Web Development for Better Analytics and Applications",,"Dhiman, Karan and Quach, Benson",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Knowledge Capture in the Wild: A Perspective from Semantic Wiki Communities",,"Gil, Yolanda and Ratnakar, Varun",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"CCDWN '16: Proceedings of the 1st Workshop on Content Caching and Delivery in Wireless Networks",,,2016,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"Impact of Tool Support in Patch Construction","In this work, we investigate the practice of patch construction in the Linux kernel development, focusing on the differences between three patching processes: (1) patches crafted entirely manually to fix bugs, (2) those that are derived from warnings of bug detection tools, and (3) those that are automatically generated based on fix patterns. With this study, we provide to the research community concrete insights on the practice of patching as well as how the development community is currently embracing research and commercial patching tools to improve productivity in repair. The result of our study shows that tool-supported patches are increasingly adopted by the developer community while manually-written patches are accepted more quickly. Patch application tools enable developers to remain committed to contributing patches to the code base. Our findings also include that, in actual development processes, patches generally implement several change operations spread over the code, even for patches fixing warnings by bug detection tools. Finally, this study has shown that there is an opportunity to directly leverage the output of bug detection tools to readily generate patches that are appropriate for fixing the problem, and that are consistent with manually-written patches.","Koyuncu, Anil and Bissyand{\'e}, Tegawend{\'e} F. and Kim, Dongsun and Klein, Jacques and Monperrus, Martin and Le Traon, Yves",2017,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"Toward Realistic Hands Gesture Interface: Keeping It Simple for Developers and Machines",,"Krupka, Eyal and Karmon, Kfir and Bloom, Noam and Freedman, Daniel and Gurvich, Ilya and Hurvitz, Aviv and Leichter, Ido and Smolin, Yoni and Tzairi, Yuval and Vinnikov, Alon and Bar-Hillel, Aharon",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Online Technical Education in Advanced Technical Education Funded Programs: (Abstract Only)",,"Calhoun, Cheryl",2018,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"A Framework for Evaluating Continuous Microservice Delivery Strategies",,"Lehmann, Martin and Sandnes, Frode Eika",2017,"[""ACM""]","Rejeitado: CR10, CR9","Rejeitado: CR0"
"Getting Deep Recommenders Fit: Bloom Embeddings for Sparse Binary Input/Output Networks",,"Serr\`{a}, Joan and Karatzoglou, Alexandros",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Identification of Great Apes Using Gabor Features and Locality Preserving Projections","In the ongoing biodiversity crisis many species, particularly primates like chimpanzees for instance are threatened and need to be protected. Often, autonomous monitoring techniques using remote camera devices are used to estimate the remaining population sizes. Unfortunately, the manual analysis of the resulting video material is very tedious and time consuming. To reduce the burden of time consuming routine work, researches have recently started to use computer vision algorithms to identify individuals. In this paper we present an approach for automatic face identification for primates, especially chimpanzees. We successfully combine Gabor features with Locality Preserving Projections (LPP). As classifier we use a new method called Sparse Representation Classification (SRC). In two experiments we show that our approach outperforms a recently published algorithm for face recognition of Great Apes. We also compare our algorithm to other state-of-the-art face recognition algorithms using three methods for feature-space transformation and two different classification approaches, namely SRC and an enhanced version called Robust Sparse Coding (RSC). Our approach not only outperforms the other algorithms for full-frontal faces but is also more invariant to pose changes. For our experiments we use two publicly available, real-world databases of captive and free-living chimpanzees from the zoo of Leipzig, Germany and the Tai National Park, Africa, respectively. Even though both datasets are very challenging due to difficult lighting conditions, non-cooperative subjects, various pose changes and even partial occlusion, the achieved recognition rates are very promising and therefore our approach has the potential to open up new ways in effective biodiversity conservation management.","Loos, Alexander",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Under-sampling Based on Sparse Data/Parity Patterns in STBC-OFDM Environment",,"Petrellis, Nikos",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Building Lean Continuous Integration and Delivery Pipelines by Applying DevOps Principles: A Case Study at Varidesk",,"Debroy, Vidroha and Miller, Senecca and Brimble, Lance",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"AutoManner: An Automated Interface for Making Public Speakers Aware of Their Mannerisms",,"Tanveer, M. Iftekhar and Zhao, Ru and Chen, Kezhen and Tiet, Zoe and Hoque, Mohammed Ehsan",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Programming with Everybody: Tightening the Copy-modify-publish Feedback Loop","At the beginning of every research effort, researchers in empirical software engineering have to go through the processes of extracting data from raw data sources and transforming them to what their tools expect as inputs. This step is time consuming and error prone, while the produced artifacts (code, intermediate datasets) are usually not of scientific value. In the recent years, Apache Spark has emerged as a solid foundation for data science and has taken the big data analytics domain by storm. We believe that the primitives exposed by Apache Spark can help software engineering researchers create and share reproducible, high-performance data analysis pipelines.

In our technical briefing, we discuss how researchers can profit from Apache Spark, through a hands-on case study.","Lieber, Thomas and Miller, Rob",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"A Multi-level Dataset of Linux Kernel Patchwork",,"Xu, Yulin and Zhou, Minghui",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Adapting Proof Automation to Adapt Proofs",,"Ringer, Talia and Yazdani, Nathaniel and Leo, John and Grossman, Dan",2018,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Outstanding Doctoral Dissertation Award",,,2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Gesture Coder: A Tool for Programming Multi-touch Gestures by Demonstration",,"L\""{u}, Hao and Li, Yang",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"On the Usefulness of Object Tracking Techniques in Performance Analysis",,"Llort, Germ\'{a}n and Servat, Harald and Gonz\'{a}lez, Juan and Gim{\'e}nez, Judit and Labarta, Jes\'{u}s",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Stochastic Program Optimization",,"Schkufza, Eric and Sharma, Rahul and Aiken, Alex",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Recording and Analyzing In-browser Programming Sessions",,"Helminen, Juha and Ihantola, Petri and Karavirta, Ville",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Tracking Performance Failures with Rizel",,"Alcocer, Juan Pablo Sandoval and Bergel, Alexandre",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Big Data Software Analytics with Apache Spark","At the beginning of every research effort, researchers in empirical software engineering have to go through the processes of extracting data from raw data sources and transforming them to what their tools expect as inputs. This step is time consuming and error prone, while the produced artifacts (code, intermediate datasets) are usually not of scientific value. In the recent years, Apache Spark has emerged as a solid foundation for data science and has taken the big data analytics domain by storm. We believe that the primitives exposed by Apache Spark can help software engineering researchers create and share reproducible, high-performance data analysis pipelines.

In our technical briefing, we discuss how researchers can profit from Apache Spark, through a hands-on case study.","Gousios, Georgios",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Identifying Security Issues in Software Development: Are Keywords Enough?",,"Morrison, Patrick and Oyetoyan, Tosin Daniel and Williams, Laurie",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Socio-spatial Self-organizing Maps: Using Social Media to Assess Relevant Geographies for Exposure to Social Processes",,"Relia, Kunal and Akbari, Mohammad and Duncan, Dustin and Chunara, Rumi",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Facilitating Collaboration and Interaction Across the Enterprise with OSLC",,"Kennedy, Sean and Jiu, Lin",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"IPSN '18: Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks",,,2018,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"Binary Optimized Hashing",,"Dai, Qi and Li, Jianguo and Wang, Jingdong and Jiang, Yu-Gang",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Leveraging DiaGrid Hub for Interactively Generating and Running Parallel Programs",,"Arora, Ritu and Chen, Kevin and Gupta, Madhav and Clark, Steven and Song, Carol",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Impact of Vectorization Over 16-bit Data-Types on GPUs",,"Reis, Lu\'{\i}s and Nobre, Ricardo and Cardoso, Jo\~{a}o M. P.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"What's Making That Sound?","In this paper, we investigate techniques to localize the sound source in video made using one microphone. The visual object whose motion generates the sound is located and segmented based on the synchronization analysis of object motion and audio energy. We first apply an effective region tracking algorithm to segment the video into a number of spatial-temporal region tracks, each representing the temporal evolution of an appearance-coherent image structure (i.e., object). We then extract the motion features of each object as its average acceleration in each frame. Meanwhile, Short-term Fourier Transform is applied to the audio signal to extract audio energy feature as the audio descriptor. We further impose a nonlinear transformation on both audio and visual descriptors to obtain the audio and visual codes in a common rank correlation space. Finally, the correlation between an object and the audio signal is simply evaluated by computing the Hamming distance between the audio and visual codes generated in previous steps. We evaluate the proposed method both qualitatively and quantitatively using a number of challenging test videos. In particular, the proposed method is compared with a state-of-the-art audiovisual source localization algorithm. The results demonstrate the superior performance of the proposed algorithm in spatial-temporal localization and segmentation of audio sources in the visual domain.correlation space. Finally, the correlation between an object and the audio signal is simply evaluated by computing the Hamming distance between the audio and visual codes generated in previous steps. We evaluate the proposed method both qualitatively and quantitatively using a number of challenging test videos. In particular, the proposed method is compared with a state-of-the-art audiovisual source localization algorithm. The results demonstrate the superior performance of the proposed algorithm in spatial-temporal localization and segmentation of audio sources in the visual domain.","Li, Kai and Ye, Jun and Hua, Kien A.",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Fraglight: Shedding Light on Broken Pointcuts in Evolving Aspect-oriented Software",,"Khatchadourian, Raffi and Rashid, Awais and Masuhara, Hidehiko and Watanabe, Takuya",2015,"[""ACM"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR0"
"Deep Unsupervised Multi-view Detection of Video Game Stream Highlights",,"Ringer, Charles and Nicolaou, Mihalis A.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"DeltaImpactFinder: Assessing Semantic Merge Conflicts with Dependency Analysis",,"Dias, Mart\'{\i}n and Polito, Guillermo and Cassou, Damien and Ducasse, St{\'e}phane",2015,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR0"
"A Non-negative Symmetric Encoder-Decoder Approach for Community Detection",,"Sun, Bing-Jie and Shen, Huawei and Gao, Jinhua and Ouyang, Wentao and Cheng, Xueqi",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Evolution-aware Monitoring-oriented Programming",,"Legunsen, Owolabi and Marinov, Darko and Ro\c{s}u, Grigore",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Statistical Migration of API Usages",,"Phan, Hung Dang and Nguyen, Anh Tuan and Nguyen, Trong Duc and Nguyen, Tien N.",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Initiating a Moving Target Network Defense with a Real-time Neuro-evolutionary Detector",,"Smith, Robert J. and Zincir-Heywood, Ayse Nur and Heywood, Malcolm I. and Jacobs, John T.",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Software Engineering for 'Social Good': Integrating Action Research, Participatory Design, and Agile Development",,"Ferrario, Maria Angela and Simm, Will and Newman, Peter and Forshaw, Stephen and Whittle, Jon",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Space Savings and Design Considerations in Variable Length Deduplication","Explosion of data growth and duplication of data in enterprises has led to the deployment of a variety of deduplication technologies. However not all deduplication technologies serve the needs of every workload. Most prior research in deduplication concentrates on fixed block size (or variable block size at a fixed block boundary) deduplication which provides sub-optimal space efficiency in workloads where the duplicate data is not block aligned. Workloads also differ in the nature of operations and their priorities thereby affecting the choice of the right flavor of deduplication. Object workloads for instance, hold multiple versions of archived documents that have a high degree of duplicate data. They are also write-once read-many in nature and follow a whole object GET, PUT and DELETE model and would be better served by a deduplication strategy that takes care of nonblock aligned changes to data.

In this paper, we describe and evaluate a hybrid of a variable length and block based deduplication that is hierarchical in nature. We are motivated by the following insights from real world data: (a) object workload applications do not do in-place modification of data and hence new versions of objects are written again as a whole (b) significant amount of data among different versions of the same object is shareable but the changes are usually not block aligned. While the second point is the basis for variable length technique, both the above insights motivate our hierarchical deduplication strategy.

We show through experiments with production data-sets from enterprise environments that this provides up to twice the space savings compared to a fixed block deduplication.","Appaji Nag Yasa, Giridhar and Nagesh, P. C.",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"An Analysis of a Media-Based Approach to Teach Programming to Middle School Students",,"Araujo, Luis Gustavo J. and Bittencourt, Roberto A. and Santos, David M.B.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Context-based Refactoring Recommendation Approach Using Simulated Annealing: Two Industrial Case Studies","Refactoring is a highly valuable solution to reduce and manage the growing complexity of software systems. However, programmers are ""opportunistic"" when they apply refactorings since most of them are interested in improving the quality of the code fragments that they frequently update or those related to the planned activities for the next release (fixing bugs, adding new functionalities, etc.). In this paper, we describe a search based approach to recommend refactorings based on the analysis of the history of changes to maximize the recommended refactorings for recently modified classes, classes containing incomplete refactorings detected in previous releases, and buggy classes identified in the history of previous bug reports. The obtained results on two industrial projects show significant improvements of the relevance of recommended refactorings, as evaluated by the original developers of the systems.","Kessentini, Marouane and Dea, Troh Josselin and Ouni, Ali",2017,"[""ACM""]","Aceito: CA0","Aceito: CA1"
"Detecting Depression Using Vocal, Facial and Semantic Communication Cues",,"Williamson, James R. and Godoy, Elizabeth and Cha, Miriam and Schwarzentruber, Adrianne and Khorrami, Pooya and Gwon, Youngjune and Kung, Hsiang-Tsung and Dagli, Charlie and Quatieri, Thomas F.",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Empirical Resilience Evaluation of an Architecture-based Self-adaptive Software System",,"C\'{a}mara, Javier and Correia, Pedro and de Lemos, Rog{\'e}rio and Vieira, Marco",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Understanding and Detecting Evolution-induced Compatibility Issues in Android Apps",,"He, Dongjie and Li, Lian and Wang, Lei and Zheng, Hengjie and Li, Guangwei and Xue, Jingling",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Supporting Expressive Procedural Art Creation Through Direct Manipulation",,"Jacobs, Jennifer and Gogia, Sumit and M\u{e}ch, Radom\'{\i}r and Brandt, Joel R.",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"An Exploratory Study of the Pull-based Software Development Model",,"Gousios, Georgios and Pinzger, Martin and Deursen, Arie van",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"How Do We Teach Graphics with OpenGL?",,"Wolff, David",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Data for Good: Abstract","I use the tagline ""Data for Good"" to state paronomastically how we as a community should be promoting data science, especially in training future generations of data scientists. First, we should use data science for the good of humanity and society. Data science should be used to better people's lives. Data science should be used to improve relationships among people, organizations, and institutions. Data science, in collaboration with other disciplines, should be used to help tackle societal grand challenges such as climate change, education, energy, environment, healthcare, inequality, and social justice. Second, we should use data in a good manner. The acronym FATES suggests what ""good"" means. Fairness means that the models we build are used to make unbiased decisions or predictions. Accountability means to determine and assign responsibility-to someone or to something-for a judgment made by a machine. Transparency means being open and clear to the end user about how an outcome, e.g., a classification, a decision, or a prediction, is made. Ethics for data science means paying attention to both the ethical and privacy-preserving collection and use of data as well as the ethical decisions that the automated systems we build will make. Safety and security (yes, two words for one ""S"") means ensuring that the systems we build are safe (do no harm) and secure (guard against malicious behavior).","Wing, Jeannette M.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"TestEvol: A Tool for Analyzing Test-suite Evolution",,"Pinto, Leandro Sales and Sinha, Saurabh and Orso, Alessandro",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Improving Communication in PGAS Environments: Static and Dynamic Coalescing in UPC",,"Alvanos, Michail and Farreras, Montse and Tiotto, Ettore and Amaral, Jos{\'e} Nelson and Martorell, Xavier",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Using Document-oriented GUIs in Dynamic Software Product Lines",,"Kramer, Dean and Oussena, Samia and Komisarczuk, Peter and Clark, Tony",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Map-based Algorithm Visualization with METAL Highway Data",,"Teresco, James D. and Fathi, Razieh and Ziarek, Lukasz and Bamundo, MariaRose and Pengu, Arjol and Tarbay, Clarice F.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Portable VM-based Implementation Platform for Non-strict Functional Programming Languages",,"Jansen, Jan Martin and van Groningen, John",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"POSTER: Towards Compiler-Assisted Taint Tracking on the Android Runtime (ART)",,"Backes, Michael and Schranz, Oliver and von Styp-Rekowsky, Philipp",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"How, and Why, Process Metrics Are Better",,"Rahman, Foyzur and Devanbu, Premkumar",2013,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Detecting Obfuscated JavaScript Malware Using Sequences of Internal Function Calls",,"Gorji, Alireza and Abadi, Mahdi",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Evaluating the Efficiency and Effectiveness of Adaptive Parsons Problems","
Practice is essential for learning. There is evidence that solving Parsons problems (putting mixed up code blocks in order) is a more efficient, but just as effective, form of practice than writing code from scratch. However, not all students successfully solve every Parsons problem. Making the problems adaptive, so that the difficulty changes based on the learner's performance, should keep the learner in Vygotsky's zone of proximal development and maximize learning gains. This paper reports on a study comparing the efficiency and effectiveness of learning from solving adaptive Parsons problems vs non-adaptive Parsons problem vs writing the equivalent code. The adaptive Parsons problems used both intra-problem and inter-problem adaptation. Intra-problem adaptation means that if the learner is struggling to solve the current problem, the problem can dynamically be made easier. Inter-problem adaptation means that the difficulty of the next problem is modified based on the learner's performance on the previous problem. This study provides evidence that solving intra-problem and inter-problem adaptive Parsons problems is a more efficient, but just as effective, form of practice as writing the equivalent code.","Ericson, Barbara J. and Foley, James D. and Rick, Jochen",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Dual Ecological Measures of Focus in Software Development",,"Posnett, Daryl and D\&\#039;Souza, Raissa and Devanbu, Premkumar and Filkov, Vladimir",2013,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Conc-iSE: Incremental Symbolic Execution of Concurrent Software",,"Guo, Shengjian and Kusano, Markus and Wang, Chao",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Evolution of Software Development Strategies",,"Falkner, Katrina and Szabo, Claudia and Vivian, Rebecca and Falkner, Nickolas",2015,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"A New Hierarchical Clustering Technique for Restructuring Software at the Function Level","Ill-structured code is difficult to understand and thereby is costly to maintain and reuse. Software restructuring techniques based on hierarchical agglomerative clustering (HAC) algorithms have been widely used to restructure large modules with low cohesion into smaller modules with high cohesion, without changing the overall behaviour of the software. These techniques generate clustering trees, of modules, that are sliced at different cut-points to obtain desired restructurings. Choosing appropriate cut-points has always been a difficult problem in clustering. Previous HAC techniques generate clustering trees that have large number of cut-points. Moreover, many of those cut-points return clusters of which only a few lead to a meaningful restructuring of the software. In this paper, we give a new hierarchical clustering technique, the (k, w)-Core Clustering ((k, w)-CC) technique, for restructuring software at the function level that generates clustering trees with lower number of cut-points, which yield a lower number of redundant clusters. (k, w)-CC gives good restructurings. To establish this, we provide an experimental comparison of (k, w)-CC with four previous HAC techniques: single linkage algorithm (SLINK), complete linkage algorithm (CLINK), weighted pair group method of arithmetic averages (WPGMA), and adaptive k-nearest neighbour algorithm (A-KNN). In the experiments, the techniques were implemented on Java functions extracted from real-life industrial programs.","Hussain, Aftab and Rahman, Md. Saidur",2013,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"What Design Topics Do Developers Discuss?",,"Viviani, Giovanni and Janik-Jones, Calahan and Famelis, Michalis and Xia, Xin and Murphy, Gail C.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"WATERFALL: An Incremental Approach for Repairing Record-replay Tests of Web Applications",,"Hammoudi, Mouna and Rothermel, Gregg and Stocco, Andrea",2016,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR0"
"In-vivo and Offline Optimisation of Energy Use in the Presence of Small Energy Signals: A Case Study on a Popular Android Library",,"Bokhari, Mahmoud A. and Alexander, Brad and Wagner, Markus",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Compiling to Avoid Communication","
Future computing system designs will be constrained by power density and total system energy, and will require new programming models and implementation strategies. Data movement in the memory system and interconnect will dominate running time and energy costs, making communication cost reduction the primary optimization criteria for compilers. Communication cost can be divided into latency costs, which are per communication event, and bandwidth costs, which grow with total communication volume.

Latency can be reduced by a number of techniques, including message aggregation, reducing software overhead of messaging, and overlapping communication with computation or with other communication events. While these techniques have been studied extensively, there are still many open challenges in automating these techniques in the context of explicitly parallel programs. I will describe some of the history of this work, and the program analysis challenges related to keeping a simple semantic model for programmers.

Bandwidth reduction requires more substantial algorithmic transformations, although some techniques, such as loop tiling, are well known. These can be applied as hand-optimizations, through code generation strategies in autotuned libraries, or as fully automatic compiler transformations. Less obvious techniques for communication avoidance have arisen in developing algorithms that are provably communication-optimal, the so-called ""2.5D"" algorithms in dense linear algebra. I will describe how these ideas generalize to other loop nests and some initial thoughts on automating such transformations.","Yelick, Kathy",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"Growth Analysis of Eclipse and NetBeans Using Gini Coefficient","Software are developed according to the consumer need. However, the requirements changed time to time due to the advancement of the era, which make software more complex over the time. Therefore, software needs to be maintained properly to reduce complexity and increase productivity. This required a comprehensive growth analysis to be performed on a timely basis to understand the changes occurred in the software over the long time. In this paper, we have performed a growth analysis of two existing software Netbeans and Eclipse applications. Both of these software's are used for programming Integrated Development Environment (IDE) to develop applications. This paper highlights the different versions of both software's, their features and functions, features evolution, change log audit, growth and change analysis in context of Gini coefficient and the Lorenz curve. Both software's have been analysed based on evolution metrics, and results are derived using Count Lines of Code (CLOC) an open source application.","Tabassum, Mujahid and Sijore, Jeffery Jeselee",2017,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR11"
"WhoseFault: Automatic Developer-to-fault Assignment Through Fault Localization",,"Servant, Francisco and Jones, James A.",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Beyond Parallel Programming with Domain Specific Languages",,"Olukotun, Kunle",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Open Source-style Collaborative Development Practices in Commercial Projects Using GitHub",,"Kalliamvakou, Eirini and Damian, Daniela and Blincoe, Kelly and Singer, Leif and German, Daniel M.",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Mutable Checkpoint-restart: Automating Live Update for Generic Server Programs",,"Giuffrida, Cristiano and Iorgulescu, C\u{a}lin and Tanenbaum, Andrew S.",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Exploring Socio-Technical Dependencies in Open Source Software Projects: Towards an Automated Data-driven Approach",,"Syeed, M. M. Mahbubul and Hammouda, Imed and Berko, Csaba",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"BlackHorse: Creating Smart Test Cases from Brittle Recorded Tests",,"Carino, Santo and Andrews, James H. and Goulding, Sheldon and Arunthavarajah, Pradeepan and Florio, Tony and Hertyk, Jakub",2012,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"Exploiting Vector Instructions with Generalized Stream Fusio",,"Mainland, Geoffrey and Leshchinskiy, Roman and Peyton Jones, Simon",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Composite Correlation Quantization for Efficient Multimodal Retrieval",,"Long, Mingsheng and Cao, Yue and Wang, Jianmin and Yu, Philip S.",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"20 Years of Teaching Parallel Processing to Computer Science Seniors",,"Liu, Jie",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"An Empirical Study of Iterative Improvement in Programming Assignments","As automated tools for grading programming assignments become more widely used, it is imperative that we better understand how students are utilizing them. Other researchers have provided helpful data on the role automated assessment tools (AATs) have played in the classroom. In order to investigate improved practices in using AATs for student learning, we sought to better understand how students iteratively modify their programs toward a solution by analyzing more than 45,000 student submissions over 7 semesters in an introductory (CS1) programming course. The resulting metrics allowed us to study what steps students took toward solutions for programming assignments. This paper considers the incremental changes students make and the correlating score between sequential submissions, measured by metrics including source lines of code, cyclomatic (McCabe) complexity, state space, and the 6 Halstead measures of complexity of the program. We demonstrate the value of throttling and show that generating software metrics for analysis can serve to help instructors better guide student learning.","Pettit, Raymond and Homer, John and Gee, Roger and Mengel, Susan and Starbuck, Adam",2015,"[""ACM""]","Aceito: CA4, CA3","Aceito: CA6"
"Energy-efficient and Robust Middleware Prototyping for Smart Mobile Computing",,"Tiku, Saideep and Pasricha, Sudeep",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Lessons from Our Elders: Identifying Obstacles to Digital Literacy Through Direct Engagement",,"Kumar, Shreya and Ureel,II, Leo C. and King, Harriet and Wallace, Charles",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Statistical Deobfuscation of Android Applications",,"Bichsel, Benjamin and Raychev, Veselin and Tsankov, Petar and Vechev, Martin",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"An Auditory Inspired Amplitude Modulation Filter Bank for Robust Feature Extraction in Automatic Speech Recognition",,"Moritz, Niko and Anem\""{u}ller, J\""{o}rn and Kollmeier, Birger",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Compiler-agnostic Translation Validation",,"Banerjee, Kunal and Karfa, Chandan",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Automatic Input Rectification",,"Long, Fan and Ganesh, Vijay and Carbin, Michael and Sidiroglou, Stelios and Rinard, Martin",2012,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Linear Cross-modal Hashing for Efficient Multimedia Search",,"Zhu, Xiaofeng and Huang, Zi and Shen, Heng Tao and Zhao, Xin",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Plenary: Kimberly Bryant \&\#38; Sarah Guthals in Conversation",,"Bryant, Kimberly and Guthals, Sarah",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Toward Predicting Architectural Significance of Implementation Issues",,"Shahbazian, Arman and Nam, Daye and Medvidovic, Nenad",2018,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR0"
"Empirically Detecting False Test Alarms Using Association Rules",,"Herzig, Kim and Nagappan, Nachiappan",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Free Rider: A Tool for Retargeting Platform-Specific Intrinsic Functions",,"Manilov, Stanislav and Franke, Bj\""{o}rn and Magrath, Anthony and Andrieu, Cedric",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"An Archive of Their Own: A Case Study of Feminist HCI and Values in Design",,"Fiesler, Casey and Morrison, Shannon and Bruckman, Amy S.",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Semantics-Aware Android Malware Classification Using Weighted Contextual API Dependency Graphs",,"Zhang, Mu and Duan, Yue and Yin, Heng and Zhao, Zhiruo",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Runtime Pointer Disambiguation",,"Alves, P{\'e}ricles and Gruber, Fabian and Doerfert, Johannes and Lamprineas, Alexandros and Grosser, Tobias and Rastello, Fabrice and Pereira, Fernando Magno Quint\~{a}o",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Safe Evolution Patterns for Software Product Lines",,"Dintzner, Nicolas",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Mid-career Review of Teaching Computer Science I",,"Kumar, Amruth N.",2013,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"Automated Recommendation of Dynamic Software Update Points: An Exploratory Study",,"Zhao, Zelin and Ma, Xiaoxing and Xu, Chang and Yang, Wenhua",2014,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"Student Perceptions of Success in Computer Science Senior Capstone Projects (Abstract Only)",,"Parker, Rick",2016,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Runtime Verification of Scientific Computing: Towards an Extreme Scale",,"Dinh, Minh Ngoc and Jin, Chao and Abramson, David and Jeffery, Clinton L.",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Live Programming in the LogicBlox System: A MetaLogiQL Approach",,"Green, Todd J. and Olteanu, Dan and Washburn, Geoffrey",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Flow Logic for Process Calculi",,"Nielson, Hanne Riis and Nielson, Flemming and Pilegaard, Henrik",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Embrace, Defend, Extend: A Methodology for Embedding Preexisting DSLs",,"Kulkarni, Abhishek and Newton, Ryan R.",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"GraphX: A Resilient Distributed Graph System on Spark",,"Xin, Reynold S. and Gonzalez, Joseph E. and Franklin, Michael J. and Stoica, Ion",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Q\#: Enabling Scalable Quantum Computing and Development with a High-level DSL",,"Svore, Krysta and Geller, Alan and Troyer, Matthias and Azariah, John and Granade, Christopher and Heim, Bettina and Kliuchnikov, Vadym and Mykhailova, Mariia and Paz, Andres and Roetteler, Martin",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"The Firefox Temporal Defect Dataset",,"Habayeb, Mayy and Miranskyy, Andriy and Murtaza, Syed Shariyar and Buchanan, Leotis and Bener, Ayse",2015,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Poster: RTDroid: A Real-Time Solution with Android",,"Yan, Yin and Dantu, Karthik and Ko, Steven Y. and Ziarek, Lukasz",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"EARec: Leveraging Expertise and Authority for Pull-request Reviewer Recommendation in GitHub",,"Ying, Haochao and Chen, Liang and Liang, Tingting and Wu, Jian",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Faults in Linux 2.6",,"Palix, Nicolas and Thomas, Ga\={e}l and Saha, Suman and Calv\`{e}s, Christophe and Muller, Gilles and Lawall, Julia",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Supporting Runtime Adaptation of Context-Aware Services",,"Boudaa, Boudjemaa and Hammoudi, Slimane and Bouguessa, Abdelkader and Chikh, Mohammed Amine",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Design of Smart Shopping Wall Using Hand Gesture and Facial Image Recognition",,"Lee, Jia-Hong and Wu, Mei-Yi and Liu, Che-Yu and Chuang, Yun-Hao",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Application of Domain-aware Binary Fuzzing to Aid Android Virtual Machine Testing",,"Kyle, Stephen and Leather, Hugh and Franke, Bj\""{o}rn and Butcher, Dave and Monteith, Stuart",2015,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"Automating and Optimizing Data Transfers for Many-core Coprocessors",,"Ren, Bin and Ravi, Nishkam and Yang, Yi and Feng, Min and Agrawal, Gagan and Chakradhar, Srimat",2014,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Ghostbuster: A Tool for Simplifying and Converting GADTs",,"McDonell, Trevor L. and Zakian, Timothy A. K. and Cimini, Matteo and Newton, Ryan R.",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Making the Common Case the Only Case with Anticipatory Memory Allocation",,"Sundararaman, Swaminathan and Zhang, Yupu and Subramanian, Sriram and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Study to Measure the Effect of Framing a Robot As a Social Agent or As a Machine on Children's Social Behavior",,"Kory Westlund, Jacqueline M. and Martinez, Marayna and Archie, Maryam and Das, Madhurima and Breazeal, Cynthia",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Automated Mobile UI Test Fragility: An Exploratory Assessment Study on Android",,"Coppola, Riccardo and Raffero, Emanuele and Torchiano, Marco",2016,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"SecureKeeper: Confidential ZooKeeper Using Intel SGX",,"Brenner, Stefan and Wulf, Colin and Goltzsche, David and Weichbrodt, Nico and Lorenz, Matthias and Fetzer, Christof and Pietzuch, Peter and Kapitza, R\""{u}diger",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Identifying Linux Bug Fixing Patches",,"Tian, Yuan and Lawall, Julia and Lo, David",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"EDDIE: EM-Based Detection of Deviations in Program Execution","This paper describes EM-Based Detection of Deviations in Program Execution (EDDIE), a new method for detecting anomalies in program execution, such as malware and other code injections, without introducing any overheads, adding any hardware support, changing any software, or using any resources on the monitored system itself. Monitoring with EDDIE involves receiving electromagnetic (EM) emanations that are emitted as a side effect of execution on the monitored system, and it relies on spikes in the EM spectrum that are produced as a result of periodic (e.g. loop) activity in the monitored execution. During training, EDDIE characterizes normal execution behavior in terms of peaks in the EM spectrum that are observed at various points in the program execution, but it does not need any characterization of the malware or other code that might later be injected. During monitoring, EDDIE identifies peaks in the observed EM spectrum, and compares these peaks to those learned during training. Since EDDIE requires no resources on the monitored machine and no changes to the monitored software, it is especially well suited for security monitoring of embedded and IoT devices. We evaluate EDDIE on a real IoT system and in a cycle-accurate simulator, and find that even relatively brief injected bursts of activity (a few milliseconds) are detected by EDDIE with high accuracy, and that it also accurately detects when even a few instructions are injected into an existing loop within the application.","Nazari, Alireza and Sehatbakhsh, Nader and Alam, Monjur and Zajic, Alenka and Prvulovic, Milos",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Programming for the Humanities: A Whirlwind Tour of Assignments (Abstract Only)",,"Kokensparger, Brian and Peyou, Wade",2018,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Discovering Bug Patterns in JavaScript","JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug finding tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using language-construct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript.","Hanam, Quinn and Brito, Fernando S. de M. and Mesbah, Ali",2016,"[""ACM"",""Engineering Village""]","Aceito: CA5, CA3","Aceito: CA5, CA3"
"Framework for Evaluation of Text Captchas",,"Thomas, Achint and Punera, Kunal and Kennedy, Lyndon and Tseng, Belle and Chang, Yi",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"miniKanren, Live and Untagged: Quine Generation via Relational Interpreters (Programming Pearl)",,"Byrd, William E. and Holk, Eric and Friedman, Daniel P.",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Bridging the Gap: Towards Optimization Across Linear and Relational Algebra",,"Kunft, Andreas and Alexandrov, Alexander and Katsifodimos, Asterios and Markl, Volker",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Synergistic Analysis Method for Explaining Failed Regression Tests",,"Yi, Qiuping and Yang, Zijiang and Liu, Jian and Zhao, Chen and Wang, Chao",2015,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"Leveraging Speculative Architectures for Runtime Program Validation",,"Santos, Juan Carlos Martinez and Fei, Yunsi",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Formal Validation of Fault Management Design Solutions",,"Gibson, Corrina and Karban, Robert and Andolfato, Luigi and Day, John",2014,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR0"
"Studying the Effectiveness of Application Performance Management (APM) Tools for Detecting Performance Regressions for Web Applications: An Experience Report",,"Ahmed, Tarek M. and Bezemer, Cor-Paul and Chen, Tse-Hsun and Hassan, Ahmed E. and Shang, Weiyi",2016,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"Automating Property-based Testing of Evolving Web Services",,"Li, Huiqing and Thompson, Simon and Lamela Seijas, Pablo and Francisco, Miguel Angel",2014,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"A Crafts-Oriented Approach to Computing in High School: Introducing Computational Concepts, Practices, and Perspectives with Electronic Textiles",,"Kafai, Yasmin B. and Lee, Eunkyoung and Searle, Kristin and Fields, Deborah and Kaplan, Eliot and Lui, Debora",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"How Not to Structure Your Database-backed Web Applications: A Study of Performance Bugs in the Wild",,"Yang, Junwen and Subramaniam, Pranav and Lu, Shan and Yan, Cong and Cheung, Alvin",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Testing and Debugging UML Models Based on fUML",,"Mayerhofer, Tanja",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Exploiting Vector Instructions with Generalized Stream Fusion",,"Mainland, Geoffrey and Leshchinskiy, Roman and Jones, Simon Peyton",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"I3QL: Language-integrated Live Data Views",,"Mitschke, Ralf and Erdweg, Sebastian and K\""{o}hler, Mirko and Mezini, Mira and Salvaneschi, Guido",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"REEF: Retainable Evaluator Execution Framework",,"Chun, Byung-Gon and Condie, Tyson and Curino, Carlo and Douglas, Chris and Matusevych, Sergiy and Myers, Brandon and Narayanamurthy, Shravan and Ramakrishnan, Raghu and Rao, Sriram and Rosen, Josh and Sears, Russell and Weimer, Markus",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Towards Transparent Hardening of Distributed Systems",,"Behrens, Diogo and Fetzer, Christof and Junqueira, Flavio P. and Serafini, Marco",2013,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Tardis: Affordable Time-travel Debugging in Managed Runtimes",,"Barr, Earl T. and Marron, Mark",2014,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR0"
"An Introduction to Docker for Reproducible Research",,"Boettiger, Carl",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Static Cut-off for Task Parallel Programs",,"Iwasaki, Shintaro and Taura, Kenjiro",2016,"[""ACM""]","Rejeitado: CR8","Rejeitado: CR0"
"Representing Documents Through Their Readers","From Twitter to Facebook to Reddit, users have become accustomed to sharing the articles they read with friends or followers on their social networks. While previous work has modeled what these shared stories say about the user who shares them, the converse question remains unexplored: what can we learn about an article from the identities of its likely readers? To address this question, we model the content of news articles and blog posts by attributes of the people who are likely to share them. For example, many Twitter users describe themselves in a short profile, labeling themselves with phrases such as ""vegetarian"" or ""liberal."" By assuming that a user's labels correspond to topics in the articles he shares, we can learn a labeled dictionary from a training corpus of articles shared on Twitter. Thereafter, we can code any new document as a sparse non-negative linear combination of user labels, where we encourage correlated labels to appear together in the output via a structured sparsity penalty.

Finally, we show that our approach yields a novel document representation that can be effectively used in many problem settings, from recommendation to modeling news dynamics. For example, while the top politics stories will change drastically from one month to the next, the ""politics"" label will still be there to describe them. We evaluate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012, demonstrating that our approach is effective.","El-Arini, Khalid and Xu, Min and Fox, Emily B. and Guestrin, Carlos",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"A Model-driven Approach for Promoting Cloud PaaS Portability",,"da Silva, Elias Adriano Nogueira and Fortes, Renata P. M. and Lucr{\'e}dio, Daniel",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"On the Effect of Object Redundancy Elimination in Randomly Testing Collection Classes",,"Ponzio, Pablo and Bengolea, Valeria and Brida, Sim\'{o}n Guti{\'e}rrez and Scilingo, Gast\'{o}n and Aguirre, Nazareno and Frias, Marcelo",2018,"[""ACM""]","Rejeitado: CR11","Rejeitado: CR0"
"Using UML Modeling to Facilitate Three-Tier Architecture Projects in Software Engineering Courses",,"Mitra, Sandeep",2014,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Declarative Web Application Development: Encapsulating Dynamic JavaScript Widgets (Abstract Only)",,"Bolton, Robert and Ing, David and Rebert, Christopher and Thai, Kristina Lam",2012,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Is Simplicity the Key to Engagement for Children on the Autism Spectrum?",,"Keay-Bright, Wendy and Howarth, Imogen",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Android Repository Mining for Detecting Publicly Accessible Functions Missing Permission Checks",,"Nguyen, Hoang H. and Jiang, Lingxiao and Quan, Tho",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Cache Timing Attacks from The SoCFPGA Coherency Port (Abstract Only)",,"Chaudhuri, Sumanta",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Runtime Metric Meets Developer: Building Better Cloud Applications Using Feedback","A unifying theme of many ongoing trends in software engineering is a blurring of the boundaries between building and operating software products. In this paper, we explore what we consider to be the logical next step in this succession: integrating runtime monitoring data from production deployments of the software into the tools developers utilize in their daily workflows (i.e., IDEs) to enable tighter feedback loops. We refer to this notion as feedback-driven development (FDD). This more abstract FDD concept can be instantiated in various ways, ranging from IDE plugins that implement feedback-driven refactoring and code optimization to plugins that predict performance and cost implications of code changes prior to even deploying the new version of the soft- ware. We demonstrate existing proof-of-concept realizations of these ideas and illustrate our vision of the future of FDD and cloud-based software development in general. Further, we discuss the major challenges that need to be solved be- fore FDD can achieve mainstream adoption.","Cito, J\""{u}rgen and Leitner, Philipp and Gall, Harald C. and Dadashi, Aryan and Keller, Anne and Roth, Andreas",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"On-device objective-C Application Optimization Framework for High-performance Mobile Processors",,"Bournoutian, Garo and Orailoglu, Alex",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Enhancing Control Flow Graph Based Binary Function Identification",,"Jonischkeit, Clemens and Kirsch, Julian",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Debugging Transactions and Tracking Their Provenance with Reenactment",,"Niu, Xing and Arab, Bahareh Sadat and Lee, Seokki and Feng, Su and Zou, Xun and Gawlick, Dieter and Krishnaswamy, Vasudha and Liu, Zhen Hua and Glavic, Boris",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Programmatic and Direct Manipulation, Together at Last","Direct manipulation interfaces and programmatic systems have distinct and complementary strengths. The former provide intuitive, immediate visual feedback and enable rapid prototyping, whereas the latter enable complex, reusable abstractions. Unfortunately, existing systems typically force users into just one of these two interaction modes. We present a system called Sketch-n-Sketch that integrates programmatic and direct manipulation for the particular domain of Scalable Vector Graphics (SVG). In Sketch-n-Sketch, the user writes a program to generate an output SVG canvas. Then the user may directly manipulate the canvas while the system immediately infers a program update in order to match the changes to the output, a workflow we call live synchronization. To achieve this, we propose (i) a technique called trace-based program synthesis that takes program execution history into account in order to constrain the search space and (ii) heuristics for dealing with ambiguities. Based on our experience with examples spanning 2,000 lines of code and from the results of a preliminary user study, we believe that Sketch-n-Sketch provides a novel workflow that can augment traditional programming systems. Our approach may serve as the basis for live synchronization in other application domains, as well as a starting point for yet more ambitious ways of combining programmatic and direct manipulation.","Chugh, Ravi and Hempel, Brian and Spradlin, Mitchell and Albers, Jacob",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Client-aware Checking and Information Hiding in Interface Specifications with JML/Ajmlc",,"Reb\^{e}lo, Henrique and Leavens, Gary T. and Lima, Ricardo Massa",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Flare: Optimizing Apache Spark with Native Compilation for Scale-up Architectures and Medium-size Data",,"Essertel, Gr{\'e}gory M. and Tahboub, Ruby Y. and Decker, James M. and Brown, Kevin J. and Olukotun, Kunle and Rompf, Tiark",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Detection Mechanisms for Unauthorized Wireless Transmissions",,"Chang, Doohwang and Bhat, Ganapati and Ogras, Umit and Bakkaloglu, Bertan and Ozev, Sule",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"KATCH: High-coverage Testing of Software Patches",,"Marinescu, Paul Dan and Cadar, Cristian",2013,"[""ACM""]","Rejeitado: CR12","Rejeitado: CR0"
"Confidentiality Breach Through Acoustic Side-Channel in Cyber-Physical Additive Manufacturing Systems",,"Chhetri, Sujit Rokka and Canedo, Arquimedes and Faruque, Mohammad Abdullah Al",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"mSwitch: A Highly-scalable, Modular Software Switch","In recent years software network switches have regained eminence as a result of a number of growing trends, including the prominence of software-defined networks, as well as their use as back-ends to virtualization technologies, to name a few. Consequently, a number of high performance switches have been recently proposed in the literature, though none of these simultaneously provide (1) high packet rates, (2) high throughput, (3) low CPU usage, (4) high port density and (5) a flexible data plane. This is not by chance: these features conflict, and while achieving one or a few of them is (now) a solved problem, addressing the combination requires significant new design effort.

In this paper we fill the gap by presenting mSwitch. To prove the flexibility and performance of our approach, we use mSwitch to build four distinct modules: a learning bridge consisting of 45 lines of code that outperforms FreeBSD's bridge by up to 8 times; an accelerated Open vSwitch module requiring small changes to the code and boosting performance by 2.6--3 times; a protocol demultiplexer for userspace protocol stacks; and a filtering module that can direct packets to virtualized middleboxes.","Honda, Michio and Huici, Felipe and Lettieri, Giuseppe and Rizzo, Luigi",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Enabling Public Cameras to Talk to the Public",,"Cao, Siyuan and Wang, He",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Multimodal Detection of Salient Behaviors of Approach-avoidance in Dyadic Interactions",,"Xiao, Bo and Georgiou, Panayiotis and Baucom, Brian and Narayanan, Shrikanth",2012,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Scalable and Incremental Software Bug Detection","An important, but often neglected, goal of static analysis for detecting bugs is the ability to show defects to the programmer quickly. Unfortunately, existing static analysis tools scale very poorly, or are shallow and cannot find complex interprocedural defects. Previous attempts at reducing the analysis time by adding more resources (CPU, memory) or by splitting the analysis into multiple sub-analyses based on defect detection capabilities resulted in limited/negligible improvements.

We present a technique for parallel and incremental static analysis using top-down, bottom-up and global specification inference based around the concept of a work unit, a self-contained atom of analysis input, that deterministically maps to its output. A work unit contains both abstract and concrete syntax to analyze, a supporting fragment of the class hierarchy, summarized interprocedural behavior, and defect reporting information, factored to ensure a high level of reuse when analyzing successive versions incrementally. Work units are created and consumed by an analysis master process that coordinates the multiple analysis passes, the flow of information among them, and incrementalizes the computation. Meanwhile, multiple analysis worker processes use abstract interpretation to compute work unit results. Process management and interprocess communication is done by a general-purpose computation distributor layer.

We have implemented our approach and our experimental results show that using eight processor cores, we can perform complete analysis of code bases with millions of lines of code in a few hours, and even faster after incremental changes to that code. The analysis is thorough and accurate: it usually reports about one crash-causing defect per thousand lines of code, with a false positive rate of 10--20%","McPeak, Scott and Gros, Charles-Henri and Ramanathan, Murali Krishna",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Towards Dynamically Monitoring Android Applications on Non-rooted Devices in the Wild",,"Tang, Xiaoxiao and Lin, Yan and Wu, Daoyuan and Gao, Debin",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Inter-media Hashing for Large-scale Retrieval from Heterogeneous Data Sources",,"Song, Jingkuan and Yang, Yang and Yang, Yi and Huang, Zi and Shen, Heng Tao",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Navigating the Maze of Graph Analytics Frameworks Using Massive Graph Datasets",,"Satish, Nadathur and Sundaram, Narayanan and Patwary, Md. Mostofa Ali and Seo, Jiwon and Park, Jongsoo and Hassaan, M. Amber and Sengupta, Shubho and Yin, Zhaoming and Dubey, Pradeep",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Promise Checked is a Promise Kept: Inspection Testing",,"Breitner, Joachim",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Coupled POS Tagging on Heterogeneous Annotations",,"Li, Zhenghua and Chao, Jiayuan and Zhang, Min and Chen, Wenliang and Zhang, Meishan and Fu, Guohong and Zhenghua Li and Jiayuan Chao and Min Zhang and Wenliang Chen and Meishan Zhang and Guohong Fu",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"A Characteristic Study on Failures of Production Distributed Data-parallel Programs",,"Li, Sihan and Zhou, Hucheng and Lin, Haoxiang and Xiao, Tian and Lin, Haibo and Lin, Wei and Xie, Tao",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"FPGA Implementation of Non-Uniform DFT for Accelerating Wireless Channel Simulations (Abstract Only)",,"Siripurapu, Srinivas and Gayasen, Aman and Gopalakrishnan, Padmini and Chandrachoodan, Nitin",2017,"[""ACM""]","Rejeitado: CR1","Rejeitado: CR0"
"Identifying Software Process Management Challenges: Survey of Practitioners in a Large Global IT Company",,"Gupta, Monika and Sureka, Ashish and Padmanabhuni, Srinivas and Asadullah, Allahbaksh Mohammedali",2015,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"Adaptive Input-aware Compilation for Graphics Engines",,"Samadi, Mehrzad and Hormati, Amir and Mehrara, Mojtaba and Lee, Janghaeng and Mahlke, Scott",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Reducing Coordination Overhead in SPLs: Peering in on Peers",,"Montalvillo, Leticia and D\'{\i}az, Oscar and Fogdal, Thomas",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Evospace-js: Asynchronous Pool-based Execution of Heterogeneous Metaheuristics","This paper is part of a continuing effort in the field of EC to develop algorithms that follow an opportunistic approach to computing, allowing the exploitation of freely available services over the Internet by using free tiers of cloud services or volunteer computing resources; the EvoSpace model is able to tap from both kind of resources, using asynchronous evolutionary algorithms. We present its design, which follows an an event-driven architecture and asynchronous I/O model, and its implementation, with a server-side tier programmed in Node.js that uses Redis as an in-memory and high performance data store for the population. This population store is exposed to clients running population-based and nature-inspired metaheuristics through a REST API. Additional capabilities where implemented in this version to allow the logging of experiments where heterogeneous algorithms are executed in parallel. These logs can then be transformed to other formats. As a case study an hybrid global optimization algorithm has been implemented mixing two algorithms: a PSO algorithm from the EvoloPy library and a GA using the DEAP framework. The result was transformed to files compatible to the Comparing Continuous Optimizer platform in order to use their post-processing code. Clients in this case have been developed in the Python language, the language used to implement both libraries. The results from this case study suggest, first, that EvoSpace can be used as a paradigm- and language-agnostic platform for population-based optimization algorithms, and also that this software can yield performance improvements and a viable platform to execute and compare asynchronous pool-based metaheuristics.","Garc\'{\i}a-Valdez, Mario and Merelo, JJ",2017,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Design Patterns As First-class Connectors",,"Hasso, Sargon and Carlson, Carl",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"A Comparison of Two Iterations of a Software Studio Course Based on Continuous Integration",,"Billingsley, William and Steel, Jim",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"FPGA HPC Using OpenCL: Case Study in 3D FFT",,"Sanaullah, Ahmed and Herbordt, Martin C.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Resource-Aware Throughput Optimization for High-Level Synthesis","With the emergence of robust high-level synthesis tools to automatically transform codes written in high-level languages into RTL implementations, the programming productivity when synthesising accelerators improves significantly. However, although the state-of-the-art high-level synthesis tools can offer high-quality designs for simple nested loop kernels, there is still a significant performance gap between the synthesized and the optimal design for real world complex applications with multiple loops.

In this work we first demonstrate that maximizing the throughput of each individual loop is not always the most efficient approach to achieving the maximum system-level throughput. More area efficient non-fully pipelined design variants may outperform the fully-pipelined version by enabling larger degrees of parallelism. We develop an algorithm to determine the optimal resource usage and initiation intervals for each loop in the applications to achieve maximum throughput within a given area budget. We report experimental results on eight applications, showing an average of 31% performance speedup over state-of-the-art HLS solutions.","Li, Peng and Zhang, Peng and Pouchet, Louis-Noel and Cong, Jason",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Understanding Myths and Realities of Test-suite Evolution",,"Pinto, Leandro Sales and Sinha, Saurabh and Orso, Alessandro",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Transparently Consistent Asynchronous Shared Memory",,"Akkan, Hakan and Ionkov, Latchesar and Lang, Michael",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Verifying Increasingly Expressive Temporal Logics for Infinite-State Systems",,"Cook, Byron and Khlaaf, Heidy and Piterman, Nir",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"ALCHEMY: A Language and Compiler for Homomorphic Encryption Made easY",,"Crockett, Eric and Peikert, Chris and Sharp, Chad",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Concept of Web-based Energy Data Quality Assurance and Control System",,"Tyukov, Anton and Brebels, Adriaan and Shcherbakov, Maxim and Kamaev, Valeriy",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Duet: Exploring Joint Interactions on a Smart Phone and a Smart Watch",,"Chen, Xiang 'Anthony' and Grossman, Tovi and Wigdor, Daniel J. and Fitzmaurice, George",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Manifestations of Preoperational Reasoning on Similar Programming Tasks",,"Teague, Donna and Lister, Raymond",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Deep Location-Specific Tracking",,"Yang, Lingxiao and Liu, Risheng and Zhang, David and Zhang, Lei",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"User Interface: Legal Protection",,"Buchalska, Joanna",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Graph-Grammar-Based IP-Integration (GRIP)\&\#x02014;An EDA Tool for Software-Defined SoCs",,"Jassi, Munish and Hu, Yong and Mueller-Gritschneder, Daniel and Schlichtmann, Ulf",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Our Troubles with Linux Kernel Upgrades and Why You Should Care",,"Harji, Ashif S. and Buhr, Peter A. and Brecht, Tim",2013,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Evaluating Test-suite Reduction in Real Software Evolution",,"Shi, August and Gyori, Alex and Mahmood, Suleman and Zhao, Peiyuan and Marinov, Darko",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Morphosyntactic Parser and Textual Corpora: Processing Uncommon Phenomena of Tibetan Language",,"Dobrov, Aleksei and Dobrova, Anastasia and Grokhovskiy, Pavel and Soms, Nikolay",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"DPRMA '13: Proceedings of the 1st International Workshop on Digital Preservation of Research Methods and Artefacts",,,2013,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"ICER '16: Proceedings of the 2016 ACM Conference on International Computing Education Research",,,2016,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"Networks Beat Pipelines: The Design of FG 2.0",,"Johnson, Peter C. and Cormen, Thomas H.",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Evaluating Mobile Apps with A/B and Quasi A/B Tests",,"Xu, Ya and Chen, Nanyu",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Analysing Timed Rebeca Using McErlang",,"Kristinsson, Haukur and Jafari, Ali and Khamespanah, Ehsan and Magnusson, Brynjar and Sirjani, Marjan",2013,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Competition: Reliability Through Timeslotted Channel Hopping and Flooding-based Routing",,"Gomes, Pedro Henrique and Watteyne, Thomas and Gosh, Pradipta and Krishnamachari, Bhaskar",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Dependent Types and Multi-monadic Effects in F*",,"Swamy, Nikhil and Hri\c{t}cu, C\u{a}t\u{a}lin and Keller, Chantal and Rastogi, Aseem and Delignat-Lavaud, Antoine and Forest, Simon and Bhargavan, Karthikeyan and Fournet, C{\'e}dric and Strub, Pierre-Yves and Kohlweiss, Markulf and Zinzindohoue, Jean-Karim and Zanella-B{\'e}guelin, Santiago",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"An Empirical Study of Inadequate and Adequate Test Suite Reduction Approaches",,"Coviello, Carmen and Romano, Simone and Scanniello, Giuseppe",2018,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Graph-Based Software Design for Managing Complexity and Enabling Concurrency in Multiphysics PDE Software",,"Notz, Patrick K. and Pawlowski, Roger P. and Sutherland, James C.",2012,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"SWAT: A Programmable, In-Memory, Distributed, High-Performance Computing Platform",,"Grossman, Max and Sarkar, Vivek",2016,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"Ligero: Lightweight Sublinear Arguments Without a Trusted Setup",,"Ames, Scott and Hazay, Carmit and Ishai, Yuval and Venkitasubramaniam, Muthuramakrishnan",2017,"[""ACM""]","Rejeitado: CR0","Rejeitado: CR0"
"Active Refinement of Clone Anomaly Reports",,"Lucia and Lo, David and Jiang, Lingxiao and Budi, Aditya",2012,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Computing with Near Data","One cost that plays a significant role in shaping the overall performance of both single-threaded and multi-thread applications in modern computing systems is the cost of moving data between compute elements and storage elements. Traditional approaches to address this cost are code and data layout reorganizations and various hardware enhancements. More recently, an alternative paradigm, called Near Data Computing (NDC) or Near Data Processing (NDP), has been shown to be effective in reducing the data movements costs, by moving computation to data, instead of the traditional approach of moving data to computation. Unfortunately, the existing Near Data Computing proposals require significant modifications to hardware and are yet to be widely adopted.

In this paper, we present a software-only (compiler-driven) approach to reducing data movement costs in both single-threaded and multi-threaded applications. Our approach, referred to as Computing with Near Data (CND), is built upon a concept called ""recomputation,"" in which a costly data access is replaced by a few less costly data accesses plus some extra computation, if the cumulative cost of the latter is less than that of the costly data access. If implemented carefully, CND can successfully trade off data access with computation, and considering the continuously increasing latency gap between the two, doing so can significantly reduce the execution latencies of both sequential and parallel application programs.

We i) quantify the intrinsic recomputability of a set of single-threaded and multi-threaded applications, ii) propose a practical, compiler-driven approach that automatically transforms a given application code fragment to a version that employs recomputation, iii) discuss an optimization strategy that increases recomputability; and iv) compare CND, both qualitatively and quantitatively, against NDC. Our experimental analysis of CND reveals that i) the average recomputability across our benchmarks is 51.1%, ii) our compiler-driven strategy is able to exploit 79.3% of the recomputation opportunities presented by our workloads, and iii) our enhancements increase the value of the recomputability metric significantly. As a result, our compiler-driven approach with the proposed enhancements brings an average execution time improvement of 40.1%.","Tang, Xulong and Kandemir, Mahmut Taylan and Zhao, Hui and Jung, Myoungsoo and Karakoy, Mustafa",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"MAdFraud: Investigating Ad Fraud in Android Applications",,"Crussell, Jonathan and Stevens, Ryan and Chen, Hao",2014,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Adopting Autonomic Computing Capabilities in Existing Large-scale Systems: An Industrial Experience Report",,"Li, Heng and Chen, Tse-Hsun (Peter) and Hassan, Ahmed E. and Nasser, Mohamed and Flora, Parminder",2018,"[""ACM""]","Rejeitado: CR4","Rejeitado: CR0"
"GPU Concurrency: Weak Behaviours and Programming Assumptions",,"Alglave, Jade and Batty, Mark and Donaldson, Alastair F. and Gopalakrishnan, Ganesh and Ketema, Jeroen and Poetzl, Daniel and Sorensen, Tyler and Wickerson, John",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Algorithmic Regularity for Polynomials and Applications",,"Bhattacharyya, Arnab and Hatami, Pooya and Tulsiani, Madhur",2015,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Philosophically Speaking...","I like nothing so much as a good philosophical discussion. For one thing, it just feels so delightfully scientific to be bandying about high minded terms like ""epistemology,"" ""ontology,"" ""realism,"" ""relativism,"" and ""paradigm"" with great abandon. For another thing, it is simply essential in my view of the world of science to which I enjoy belonging that one should be fully engaged in the ethical and normative underpinnings of scientific practice. That, to me, operationally characterizes philosophy of science. Our scientific philosophies facilitate the debating, codifying, and explicating of our ""agreed-to rules of the road"" for doing our work. They are our scientific code of conduct, so to speak. In my way of thinking, the rules of the road are important for all good researchers to understand and share if we are to hew faithfully to our paradigms and do good normal science research.

The problem for me is that I see the emerging debate over whether our field is a science or not through two distinct lenses, and I'm getting some parallax out of it. I've earned two doctorates at two different stages of my career; one in marketing when I was much younger, and one more recently in information systems. That does not make me any smarter than the average bear, and it could certainly be argued that it's a sure sign of not being smarter, since one could say I had to do it twice to get it right. Yet, it has afforded me a very unique perspective on the emerging ontological debate about our discipline because, quite literally, I Have Been There Before! I've studied the Philosophy of Science two fulsome times in my career, and I've gotten some interesting perspectives each time as each was in the midst of an existential debate over status, and both of these experience bear upon my agenda-setting here, as an editor.

While learning my scientific craft as a first-time doctoral student in marketing, I reveled in the debate that much of academic marketing was enmeshed in during the 80s on whether marketing was a science or a technology and if a science, how best to be practiced. We future scientists all read Kuhn (1970) in our philosophy of science seminar, as well as Dubin (1978) and a flock of excellent philosophical essays collected in a tome edited by the redoubtable Jagdish Sheth (Sheth & Garrett, 1986) - a philosophy of science volume that many in marketing and elsewhere still use, aged though it might be. We marketers also saw a collection of philosophical essays by thought leaders of the field under such titles as ""Marketing, Scientific Progress, and the Scientific Method"" (Anderson, 1983), ""On Making Marketing Science more Scientific"" (Arndt, 1985), ""Paradigms Lost"" (Deshpande, 1983), and ""Metatheory and Metamethodology in Marketing"" (Leong, 1985). The debate in marketing continued well past that point and has extended into recent years (e.g., Brown, 1996; Easton, 2002), indicating that the debate we have now in information systems has years yet to go. It is not a simple question we deal with, and the answers take time and fulsome consideration.","Stafford, Thomas F.",2018,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Dynamic Partitioning-based JPEG Decompression on Heterogeneous Multicore Architectures",,"Sodsong, Wasuwee and Hong, Jingun and Chung, Seongwook and Lim, Yeongkyu and Kim, Shin-Dug and Burgstaller, Bernd",2007,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"ZenIDS: Introspective Intrusion Detection for PHP Applications",,"Hawkins, Byron and Demsky, Brian",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Using Storage Class Memory Efficiently for an In-memory Database",,"Gottesman, Yonatan and Nider, Joel and Kat, Ronen and Weinsberg, Yaron and Factor, Michael",2016,"[""ACM""]","Rejeitado: CR10","Rejeitado: CR0"
"Differential Community Detection in Paired Biological Networks",,"Mall, Raghvendra and Ullah, Ehsan and Kunji, Khalid and D'Angelo, Fulvio and Bensmail, Halima and Ceccarelli, MIchele",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"Weak Compositions and Their Applications to Polynomial Lower Bounds for Kernelization",,"Hermelin, Danny and Wu, Xi",2012,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR0"
"A Volatile-by-default JVM for Server Applications",,"Liu, Lun and Millstein, Todd and Musuvathi, Madanlal",2017,"[""ACM"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR0"
"MODULARITY '14: Proceedings of the Companion Publication of the 13th International Conference on Modularity",,,2014,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"MODULARITY '14: Proceedings of the 13th International Conference on Modularity",,,2014,"[""ACM""]","Rejeitado: CR5","Rejeitado: CR0"
"Cognitive and Contextual Enterprise Mobile Computing: Invited Keynote Talk",,"Zodik, Gabi",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"On the Morality of Teaching Students IT Crime Skills: Extended Abstract",,"Olivier, Martin S.",2016,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Technology, Education and Access: A 'Fair Go' for People with Disabilities",,"Hollier, Scott",2017,"[""ACM""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Source Transformation of C++ Codes for Compatibility with Operator Overloading","In C++, new features and semantics can be added to an existing software package without sweeping code changes by introducing a user-defined type using operator overloading. This approach is used, for example, to add capabilities such as algorithmic differentiation. However, the introduction of operator overloading can cause a multitude of compilation errors. In a previous paper, we identified code constructs that cause a violation of the C++ language standard after a type change, and a tool called OO-Lint based on the Clang compiler that identifies these code constructs with lint-like messages. In this paper, we present an extension of this work that automatically transforms such problematic code constructs in order to make an existing code base compatible with a semantic augmentation through operator overloading. We applied our tool to the CFD software OpenFOAM and detected and transformed 23 instances of problematic code constructs in 160,000 lines of code. A significant amount of these root causes are included up to 425 times in other files causing a tremendous compiler error amplification. In addition, we show the significance of our work with a case study of the evolution of the ice flow modeling software ISSM, comparing a recent version which was manually type changed with a legacy version. The recent version shows no signs of problematic code constructs. In contrast, our tool detected and transformed a remarkable amount of issues in the legacy version that previously had to be manually located and fixed.","Alexander Hück and Jean Utke and Christian Bischof",2016,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"J3 Model: A novel framework for improved Modified Condition/Decision Coverage analysis","In the real-time systems and safety critical domains, software quality assurance adheres to protocols such as DO-178C standard. Regarding these issues, concolic testing generates test cases that can attain high coverage using an augmented approach based on Modified Condition/Decision Coverage (MC/DC). In this paper, we propose a framework to compute MC/DC percentage for test case generation. To achieve an increase in MC/DC, we transform the input Java program, J, into its transformed version, J′, using Java Program Code Transformer (JPCT). Then, we use JCUTE tool to generate test cases. At last, we use Java Coverage Analyzer (JCA) to compute MC/DC percentage. The Java program code transformer adds additional empty nested if-else conditional statements for each decision that causes variation in MC/DC percentage. In later step, these extra conditional statements get stripped-off. This approach resolves some of the bottleneck issues associated with traditional concolic testers. In our experimental study, we have experimented with forty Java programs. We have computed the difference of MC/DC%, for both the scenarios (i.e. with code transformation and without code transformation). Our approach (i.e. with code transformation achieves) 24.09% average increase in MC/DC% over the traditional approach (i.e. without code transformation).","Sangharatna Godboley and Arpita Dutta and Durga Prasad Mohapatra and Rajib Mall",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"GRASP2018—A Fortran 95 version of the General Relativistic Atomic Structure Package","The present Grasp2018 is an updated Fortran 95 version of the recommended block versions of programs from Grasp2K Version 1_1 for large-scale calculations Jönsson et al. (2013). MPI programs are included so that all major tasks can be executed using parallel computers. Tools have been added that simplify the generation of configuration state function expansions for the multireference single- and double computational model. Names of programs have been changed to accurately reflect the task performed by the code. Modifications to the relativistic self-consistent field program have been made that, in some instances, greatly reduce the number of iterations needed for determining the requested eigenvalues and the memory required. Changes have been made to the relativistic configuration interaction program to substantially cut down on the time for constructing the Hamiltonian matrix for configuration state function expansions based on large orbital sets. In the case of a finite nucleus the grid points have been changed so that the first non-zero point is Z-dependent as for the point nucleus. A number of tools have been developed to generate LaTeX tables of eigenvalue composition, energies, transition data and lifetimes. Tools for plotting and analyzing computed properties along an iso-electronic sequence have also been added. A number of minor errors have been corrected. A detailed manual is included that describes different aspects of the package as well as the steps needed in order to produce reliable results.
Program summary
Program Title:Grasp2018 Program Files doi:http://dx.doi.org/10.17632/x574wpp2vg.1 Licensing provisions: MIT license Programming language: Fortran 95. Nature of problem: Prediction of atomic properties – atomic energy levels, isotope shifts, oscillator strengths, radiative decay rates, hyperfine structure parameters, specific mass shift parameters, Zeeman effects – using a multiconfiguration Dirac–Hartree–Fock approach. Solution method: The computational method is the same as in the previous Grasp2K [1,2] version except that only the latest recommended versions of certain routines are included. Restrictions: All calculations are for bound state solutions. Instead of relying on packing algorithms for specifying arguments of arrays of integrals, orbitals are designated by a “short integer” requiring one byte of memory for a maximum of 127 orbitals. The tables of reduced coefficients of fractional parentage used in this version are limited to sub-shells with j≤9∕2 [3]; occupied sub-shells with j>9∕2 are, therefore, restricted to a maximum of two electrons. Some other parameters, such as the maximum number of orbitals are determined in a parameter_def_M.f90 file that can be modified prior to compile time. Unusual features: Parallel versions are available for several applications. References•[[1]] P. Jönsson, X. He, C. Froese Fischer, and I. P. Grant, Comput. Phys. Commun. 176, 597 (2007).•[[2]] P. Jönsson, G. Gaigalas, J. Bieroń, C. Froese Fischer, and I. P. Grant, Comput. Phys. Commun. 184, 2197 (2013).•[[3]] G. Gaigalas, S. Fritzsche, Z. Rudzikas, Atomic Data and Nuclear Data Tables 76, 235 (2000).","C. Froese Fischer and G. Gaigalas and P. Jönsson and J. Bieroń",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Sparse representation over discriminative dictionary for stereo matching","We propose a novel data-driven matching cost for dense correspondence based on sparse theory. The ability of sparse coding to selectively express the sources of influence on stereo images allows us to learn a discriminative dictionary. The dictionary learning process is incorporated with discriminative learning and weighted sparse coding to enhance the discrimination of sparse coefficients and weaken the influence of radiometric changes. Then, the sparse representations over the learned discriminative dictionary are utilized to measure the dissimilarity between image patches. Semi-global cost aggregation and postprocessings are finally enforced to further improve the matching accuracy. Extensive experimental comparisons demonstrate that: the proposed matching cost outperforms traditional matching costs, the discriminative dictionary learning model is more suitable than previous dictionary learning models for stereo matching, and the proposed stereo method ranks the third place on the Middlebury benchmark v3 in quarter resolution up to the submitting, and achieves the best accuracy on 30 classic stereo images.","Jihao Yin and Hongmei Zhu and Ding Yuan and Tianfan Xue",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Transformation of a PID Controller for Numerical Accuracy","Numerical programs performing floating-point computations are very sensitive to the way formulas are written. Several techniques have been proposed concerning the transformation of expressions in order to improve their accuracy and now we aim at going a step further by automatically transforming larger pieces of code containing several assignments and control structures. This article presents a case study in this direction. We consider a PID controller and we transform its code in order to improve its accuracy. The experimental data obtained when we compare the different versions of the code (which are mathematically equivalent) show that those transformations have a significant impact on the accuracy of the computations.","N. Damouche and M. Martel and A. Chapoutot",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Multi-step ahead time series forecasting via sparse coding and dictionary based techniques","Sparse coding is based on the concept of having a large dictionary of candidate basis vectors. Any given vector is expressed as a sparse linear combination of the dictionary vectors. It has been developed in the signal processing field, and has many applications in data compression and image processing. In this paper we propose applying sparse coding to the time series forecasting field. Specifically, the paper investigates different dictionary based local learning techniques for building predictive models for the time series forecasting problem. The proposed methodology is based on a local learning framework whereby the query point is embedded and coded in terms of a sparse combination of the training dictionary atoms (vectors). Then this embedding is used for estimating the target value of the query point, by applying the same embedding to the target vectors of the dictionary training atoms. We present an experimental study of several sparse coding algorithms. Experiments are performed on the large monthly time series benchmark from the M3 competition, and these experiments showed that the sparse methods Lasso and Elastic-Net presented the best results among the sparse coding algorithms. Moreover, they outperformed the K-nearest neighbor (KNN) regression and most of the compared machine learning and statistical forecasting techniques, especially for higher horizons.","Ahmed Helmi and Mohamed W. Fakhr and Amir F. Atiya",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Generative Predictive Codes by Multiplexed Hippocampal Neuronal Tuplets","Summary
Rapid internal representations are continuously formed based on single experiential episodes in space and time, but the neuronal ensemble mechanisms enabling rapid encoding without constraining the capacity for multiple distinct representations are unknown. We developed a probabilistic statistical model of hippocampal spontaneous sequential activity and revealed existence of an internal model of generative predictive codes for the regularities of multiple future novel spatial sequences. During navigation, the inferred difference between external stimuli and the internal model was encoded by emergence of intrinsic-unlikely, novel functional connections, which updated the model by preferentially potentiating post-experience. This internal model and these predictive codes depended on neuronal organization into inferred modules of short, high-repeat sequential neuronal “tuplets” operating as “neuro-codons.” We propose that flexible multiplexing of neuronal tuplets into repertoires of extended sequences vastly expands the capacity of hippocampal predictive codes, which could initiate top-down hierarchical cortical loops for spatial and mental navigation and rapid learning.","Kefei Liu and Jeremie Sibille and George Dragoi",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Use of ontology structure and Bayesian models to aid the crowdsourcing of ICD-11 sanctioning rules","The International Classification of Diseases (ICD) is the de facto standard international classification for mortality reporting and for many epidemiological, clinical, and financial use cases. The next version of ICD, ICD-11, will be submitted for approval by the World Health Assembly in 2018. Unlike previous versions of ICD, where coders mostly select single codes from pre-enumerated disease and disorder codes, ICD-11 coding will allow extensive use of multiple codes to give more detailed disease descriptions. For example, “severe malignant neoplasms of left breast” may be coded using the combination of a “stem code” (e.g., code for malignant neoplasms of breast) with a variety of “extension codes” (e.g., codes for laterality and severity). The use of multiple codes (a process called post-coordination), while avoiding the pitfall of having to pre-enumerate vast number of possible disease and qualifier combinations, risks the creation of meaningless expressions that combine stem codes with inappropriate qualifiers. To prevent that from happening, “sanctioning rules” that define legal combinations are necessary. In this work, we developed a crowdsourcing method for obtaining sanctioning rules for the post-coordination of concepts in ICD-11. Our method utilized the hierarchical structures in the domain to improve the accuracy of the sanctioning rules and to lower the crowdsourcing cost. We used Bayesian networks to model crowd workers’ skills, the accuracy of their responses, and our confidence in the acquired sanctioning rules. We applied reinforcement learning to develop an agent that constantly adjusted the confidence cutoffs during the crowdsourcing process to maximize the overall quality of sanctioning rules under a fixed budget. Finally, we performed formative evaluations using a skin-disease branch of the draft ICD-11 and demonstrated that the crowd-sourced sanctioning rules replicated those defined by an expert dermatologist with high precision and recall. This work demonstrated that a crowdsourcing approach could offer a reasonably efficient method for generating a first draft of sanctioning rules that subject matter experts could verify and edit, thus relieving them of the tedium and cost of formulating the initial set of rules.","Yun Lou and Samson W. Tu and Csongor Nyulas and Tania Tudorache and Robert J.G. Chalmers and Mark A. Musen",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Co-changing code volume prediction through association rule mining and linear regression model","Code smells are symptoms in the source code that indicate possible deeper problems and may serve as drivers for code refactoring. Although effort has been made on identifying divergent changes and shotgun surgeries, little emphasis has been put on predicting the volume of co-changing code that appears in the code smells. More specifically, when a software developer intends to perform a particular modification task on a method, a predicted volume of code that will potentially be co-changed with the method could be considered as significant information for estimating the modification effort. In this paper, we propose an approach to predicting volume of co-changing code affected by a method to be modified. The approach has the following key features: co-changing methods can be identified for detecting divergent changes and shotgun surgeries based on association rules mined from change histories; and volume of co-changing code affected by a method to be modified can be predicted through a derived fitted regression line with t-test based on the co-changing methods identification results. The experimental results show that the success rate of co-changing methods identification is 82% with a suggested threshold, and the numbers of correct identifications would not be influenced by the increasing number of commits as a project continuously evolves. Additionally, the mean absolute error of co-changing code volume predictions is 133 lines of code which is 95.3% less than the one of a naive approach.","Shin-Jie Lee and Li Hsiang Lo and Yu-Cheng Chen and Shi-Min Shen",2016,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR9, CR10"
"CLUMPY v3: γ-ray and ν signals from dark matter at all scales","We present the third release of the CLUMPY code for calculating γ-ray and ν signals from annihilations or decays in dark matter structures. This version includes the mean extragalactic signal with several pre-defined options and keywords related to cosmological parameters, mass functions for the dark matter structures, and γ-ray absorption up to high redshift. For more flexibility and consistency, dark matter halo masses and concentrations are now defined with respect to a user-defined overdensity Δ. We have also made changes for the user’s benefit: distribution and versioning of the code via git, less dependencies and a simplified installation, better handling of options in run command lines, consistent naming of parameters, and a new Sphinx documentation at http://lpsc.in2p3.fr/clumpy/.
Program summary
Program Title:CLUMPY Program Files doi:http://dx.doi.org/10.17632/4n33mbh9bc.1 Licensing provisions: GPLv2 Programming language: C/C++ External routines/libraries:GSL (http://www.gnu.org/software/gsl), cfitsio ( http://heasarc.gsfc.nasa.gov/fitsio/fitsio.html), CERN ROOT (http://root.cern.ch; optional, for interactive figures and stochastic simulation of halo substructures), GreAT (http://lpsc.in2p3.fr/great; optional, for MCMC Jeans analyses) Nature of problem: Calculation of the γ-ray and ν signals from dark matter annihilation/decay at any redshift z. Solution method: New in this release: Numerical integration of moments (in redshift and mass) of the mass function, absorption, and intensity multiplier (related to the DM density along the line of sight). Restrictions: Secondary radiation from dark matter leptons, which depends on astrophysical ingredients (radiation fields in the Universe) is the last missing piece to provide a full description of the expected signal.","Moritz Hütten and Céline Combet and David Maurin",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR10, CR9"
"Review of heavy charged particle transport in MCNP6.2","The release of version 6.2 of the MCNP6 radiation transport code is imminent. To complement the newest release, a summary of the heavy charged particle physics models used in the 1 MeV to 1 GeV energy regime is presented. Several changes have been introduced into the charged particle physics models since the merger of the MCNP5 and MCNPX codes into MCNP6. This paper discusses the default models used in MCNP6 for continuous energy loss, energy straggling, and angular scattering of heavy charged particles. Explanations of the physics models’ theories are included as well.","K. Zieb and H.G. Hughes and M.R. James and X.G. Xu",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Neural plasticity is modified over the human menstrual cycle: Combined insight from sensory evoked potential LTP and repetition suppression","In healthy women, fluctuations in hormones including progesterone and oestradiol lead to functional changes in the brain over the course of each menstrual cycle. Though considerable attention has been directed towards understanding changes in human cognition over the menstrual cycle, changes in underlying processes such as neural plasticity have largely only been studied in animals. In this study we explored predictive coding and repetition suppression via the roving mismatch negativity paradigm as a model of short-term plasticity (Garrido, Kilner, Kiebel, et al., 2009), and Hebbian learning via visual sensory long-term potentiation (LTP) as a model of long-term plasticity (Teyler et al., 2005). Electroencephalography (EEG) was recorded in 20 females during their early follicular and mid-luteal phases. Event-related potential (ERP) analyses were complemented with dynamic causal modelling (DCM) to characterise changes in the underlying neural architecture. More sustained variability in the ERP response to a change in tone during the luteal phase are interpreted as a delayed habituation of the P3a component in the luteal relative to the follicular phase. The additional increased forward connection strength over tone repetitions compared to the follicular phase suggests that, in this phase, females may be less efficient when processing deviations from predicted sensory input (error). In contrast, there appears to be no reliable change in sensory LTP. This suggests that predictive coding, but not Hebbian plasticity is modified in the mid-luteal compared to the follicular phase, at least at the days of the menstrual cycle tested. This finding implicates the human menstrual cycle in complex changes in neural plasticity and provides further evidence for the importance of considering the menstrual cycle when including females in electrophysiological research.","R.L. Sumner and M.J. Spriggs and R.L. McMillan and F. Sundram and I.J. Kirk and S.D. Muthukumaraswamy",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Hyperactive piggyBac transposase improves transformation efficiency in diverse insect species","Even in times of advanced site-specific genome editing tools, the improvement of DNA transposases is still on high demand in the field of transgenesis: especially in emerging model systems where evaluated integrase landing sites have not yet been created and more importantly in non-model organisms such as agricultural pests and disease vectors, in which reliable sequence information and genome annotations are still pending. In fact, random insertional mutagenesis is essential to identify new genomic locations that are not influenced by position effects and thus can serve as future stable transgene integration sites. In this respect, a hyperactive version of the most widely used piggyBac transposase (PBase) has been engineered. The hyperactive version (hyPBase) is currently available with the original insect codon-based coding sequence (ihyPBase) as well as in a mammalian codon-optimized (mhyPBase) version. Both facilitate significantly higher rates of transposition when expressed in mammalian in vitro and in vivo systems compared to the classical PBase at similar protein levels. Here we demonstrate that the usage of helper plasmids encoding the hyPBase - irrespective of the codon-usage - also strikingly increases the rate of successful germline transformation in the Mediterranean fruit fly (Medfly) Ceratitis capitata, the red flour beetle Tribolium castaneum, and the vinegar fly Drosophila melanogaster. hyPBase-encoding helpers are therefore highly suitable for the generation of transgenic strains of diverse insect orders. Depending on the species, we achieved up to 15-fold higher germline transformation rates compared to PBase and generated hard to obtain transgenic T. castaneum strains that express constructs affecting fitness and viability. Moreover, previously reported high sterility rates supposedly caused by hyPBase (iPB7), encoded by ihyPBase, could not be confirmed by our study. Therefore, we value hyPBase as an effective genetic engineering tool that we highly recommend for insect transgenesis.","Kolja N. Eckermann and Hassan M.M. Ahmed and Mohammad KaramiNejadRanjbar and Stefan Dippel and Christian E. Ogaugwu and Peter Kitzmann and Musa D. Isah and Ernst A. Wimmer",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"C2x: A tool for visualisation and input preparation for Castep and other electronic structure codes","The c2x code fills two distinct roles. Its first role is in acting as a converter between the binary format .check files from the widely-used Castep [1] electronic structure code and various visualisation programs. Its second role is to manipulate and analyse the input and output files from a variety of electronic structure codes, including Castep, Onetep and Vasp, as well as the widely-used ‘Gaussian cube’ file format. Analysis includes symmetry analysis, and manipulation arbitrary cell transformations. It continues to be under development, with growing functionality, and is written in a form which would make it easy to extend it to working directly with files from other electronic structure codes. Data which c2x is capable of extracting from Castep’s binary checkpoint files include charge densities, spin densities, wavefunctions, relaxed atomic positions, forces, the Fermi level, the total energy, and symmetry operations. It can recreate .cell input files from checkpoint files. Volumetric data can be output in formats useable by many common visualisation programs, and c2x will itself calculate integrals, expand data into supercells, and interpolate data via combinations of Fourier and trilinear interpolation. It can extract data along arbitrary lines (such as lines between atoms) as 1D output. C2x is able to convert between several common formats for describing molecules and crystals, including the .cell format of Castep. It can construct supercells, reduce cells to their primitive form, and add specified k-point meshes. It uses the spglib library [2] to report symmetry information, which it can add to .cell files. C2x is a command-line utility, so is readily included in scripts. It is available under the GPL and can be obtained from http://www.c2x.org.uk. It is believed to be the only open-source code which can read Castep’s .check files, so it will have utility in other projects.
Program summary
Program Title: c2x Program Files doi:http://dx.doi.org/10.17632/wj5hcj7x39.1 Licensing provisions: GPLv3 Programming language: C Nature of problem: C2x is able to extract a large variety of data from Castep’s [1] large, binary-format, output files to facilitate further processing or visualisation. These binary files are optimised for rapid input and output by Castep, but are impossible to read without detailed knowledge of their internal structure, which varies between different versions of Castep. In them wavefunctions are stored as plane wave coefficients, so need to be Fourier transformed before they can be visualised in real space. C2x can manipulate the input files of Castep, and other common crystallographic formats, performing functions useful for setting up calculations, such as supercell construction, format conversions, and symmetry analysis. Different electronic structure codes use different file formats for outputting densities. C2x is able to convert between some of the major formats, allowing visualisation and post-processing tools targeted at one electronic structure code to be used with others. Solution method: C2x is a command-line utility written in standard C. It is able to process large (multi-GB) binary files efficiently, extracting much smaller datasets. C2x has considerable knowledge of the structure of the checkpoint files as written by various versions of Castep. Wavefunctions are optionally converted to densities, and then transformed to real space using its own FFT routine. Weighted sums of densities from multiple bands can be accumulated. C2x interpolates volumetric data by using user-specified combinations of trilinear and Fourier interpolation. For symmetry analysis it relies on the existing spglib [2] library, which is also used by Castep. It supports cell transformations with the axes of the new cell expressed in absolute terms, or in terms of the original axes. It has its own internal representation of the unit cell and its contents, and has several routines for converting this to and from common crystallographic file formats. C2x is able to read densities from Castep’s binary formats, and both read and write Castep’s formatted density files, Vasp’s formatted density files, and the .cube files used by Gaussian and Onetep. Additional comments: Primary site for distribution and documentation: www.c2x.org.uk [1] S. J. Clark, M. D. Segall, C. J. Pickard, P. J. Hasnip, M. J. Probert, K. Refson, M. Payne, First principles methods using CASTEP, Z. Kristall. 220 (2005) 567–570. [2] A. Togo, Spglib, a library for finding and handling crystal symmetries [cited 2017-02-23]. https://atztogo.github.io/spglib/","M.J. Rutter",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Semi-automated architectural abstraction specifications for supporting software evolution","In this paper we present an approach for supporting the semi-automated architectural abstraction of architectural models throughout the software life-cycle. It addresses the problem that the design and implementation of a software system often drift apart as software systems evolve, leading to architectural knowledge evaporation. Our approach provides concepts and tool support for the semi-automatic abstraction of architecture component and connector views from implemented systems and keeping the abstracted architecture models up-to-date during software evolution. In particular, we propose architecture abstraction concepts that are supported through a domain-specific language (DSL). Our main focus is on providing architectural abstraction specifications in the DSL that only need to be changed, if the architecture changes, but can tolerate non-architectural changes in the underlying source code. Once the software architect has defined an architectural abstraction in the DSL, we can automatically generate architectural component views from the source code using model-driven development (MDD) techniques and check whether architectural design constraints are fulfilled by these models. Our approach supports the automatic generation of traceability links between source code elements and architectural abstractions using MDD techniques to enable software architects to easily link between components and the source code elements that realize them. It enables software architects to compare different versions of the generated architectural component view with each other. We evaluate our research results by studying the evolution of architectural abstractions in different consecutive versions of five open source systems and by analyzing the performance of our approach in these cases.","Thomas Haitzer and Uwe Zdun",2014,"[""Science Direct""]","Rejeitado: CR11","Rejeitado: CR11"
"Dynamic background estimation and complementary learning for pixel-wise foreground/background segmentation","Change and motion detection plays a basic and guiding role in surveillance video analysis. Since most outdoor surveillance videos are taken in native and complex environments, these “static” backgrounds change in some unknown patterns, which make perfect foreground extraction very difficult. This paper presents two universal modifications for pixel-wise foreground/background segmentation: dynamic background estimation and complementary learning. These two modifications are embedded in three classic background subtraction algorithms: probability based background subtraction (Gaussian mixture model, GMM), sample based background subtraction (visual background extractor, ViBe) and code words based background subtraction (code book, CB). Experiments on several popular public datasets prove the effectiveness and real-time performance of the proposed method. Both GMM and CB with the proposed modifications have better performance than the original versions. Especially, ViBe with the modifications outperforms some state-of-art algorithms presented on the CHANGEDETECTION website.","Weifeng Ge and Zhenhua Guo and Yuhan Dong and Youbin Chen",2016,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"TLEL: A two-layer ensemble learning approach for just-in-time defect prediction","Context
Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time [1].
Objective
Ensemble learning becomes a hot topic in recent years. There have been several studies about applying ensemble learning to defect prediction [2–5]. Traditional ensemble learning approaches only have one layer, i.e., they use ensemble learning once. There are few studies that leverages ensemble learning twice or more. To bridge this research gap, we try to hybridize various ensemble learning methods to see if it will improve the performance of just-in-time defect prediction. In particular, we focus on one way to do this by hybridizing bagging and stacking together and leave other possibly hybridization strategies for future work.
Method
In this paper, we propose a two-layer ensemble learning approach TLEL which leverages decision tree and ensemble learning to improve the performance of just-in-time defect prediction. In the inner layer, we combine decision tree and bagging to build a Random Forest model. In the outer layer, we use random under-sampling to train many different Random Forest models and use stacking to ensemble them once more.
Results
To evaluate the performance of TLEL, we use two metrics, i.e., cost effectiveness and F1-score. We perform experiments on the datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. Also, we compare our approach with three baselines, i.e., Deeper, the approach proposed by us [6], DNC, the approach proposed by Wang et al. [2], and MKEL, the approach proposed by Wang et al. [3]. The experimental results show that on average across the six datasets, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code, as compared with about 50% for the baselines. In addition, the F1-scores TLEL can achieve are substantially and statistically significantly higher than those of three baselines across the six datasets.
Conclusion
TLEL can achieve a substantial and statistically significant improvement over the state-of-the-art methods, i.e., Deeper, DNC and MKEL. Moreover, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code.","Xinli Yang and David Lo and Xin Xia and Jianling Sun",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Sensitivity analysis of plasma edge code parameters through algorithmic differentiation","Anomalous radial transport coefficients, boundary conditions and reaction rates are among the main sources of uncertainty within plasma edge modeling. In principle, an analysis to determine the sensitivity of code results to changes in uncertain model parameters can be easily implemented through finite differences. However, this incurs in error accumulations and allows scanning only one parameter at a time, requiring a huge computational effort. Algorithmic Differentiation (AD) is a possible alternative to finite differences already applied to several transport codes in different research domains but not yet in plasma edge modeling. AD tools preprocess the source code, identifying elementary operations for which the differential form is well known, and producing a new version of the code that contains the additional derivative information. In this paper, the feasibility of applying AD to plasma edge codes is demonstrated using the TAPENADE tool on the SOLPS-ITER code. As a first preliminary step, the AD tool is applied in the so-called “forward” mode on the B2.5 plasma solver, adopting a fluid neutral approximation to obtain the sensitivities of the calculated quantities of interest on selected code parameters. The proof of principle is carried out by comparing the AD results with those evaluated with finite differences on an ITER H-only case. The sensitivities of the target peak heat load and maximum electron temperature with respect to anomalous radial transport coefficients and core input power are assessed. The comparison with finite differences results in a relative error lower than 10−6. This proves that, in a next step, AD can be exploited for an efficient and accurate sensitivity analysis in the framework of plasma edge simulations.","Stefano Carli and Maarten Blommaert and Wouter Dekeyser and Martine Baelmans",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Estimating residential energy consumption in metropolitan areas: A microsimulation approach","Prior research has shown that land use patterns and the spatial configurations of cities have a significant impact on residential energy demand. Given the pressing issues surrounding energy security and climate change, there is renewed interest in developing and retrofitting cities to make them more energy efficient. Yet deriving micro-scale residential energy footprints of metropolitan areas is challenging because high resolution data from energy providers is generally unavailable. In this study, a bottom-up model is proposed to estimate residential energy demand using datasets that are commonly available in the United States. The model applies novel machine learning methods to match records in the Residential Energy Consumption Survey with Public Use Microdata samples. This matching and machine learning produce a synthetic household energy distribution at a neighborhood scale. The model was tested and validated with data from the Atlanta metropolitan region to demonstrate its application and promise.","Wenwen Zhang and Caleb Robinson and Subhrajit Guhathakurta and Venu M. Garikapati and Bistra Dilkina and Marilyn A. Brown and Ram M. Pendyala",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"A historical, textual analysis approach to feature location","Context
Feature location is the task of finding the source code that implements specific functionality in software systems. A common approach is to leverage textual information in source code against a query, using Information Retrieval (IR) techniques. To address the paucity of meaningful terms in source code, alternative, relevant source-code descriptions, like change-sets could be leveraged for these IR techniques. However, the extent to which these descriptions are useful has not been thoroughly studied.
Objective
This work rigorously characterizes the efficacy of source-code lexical annotation by change-sets (ACIR), in terms of its best-performing configuration.
Method
A tool, implementing ACIR, was used to study different configurations of the approach and to compare them to a baseline approach (thus allowing comparison against other techniques going forward). This large-scale evaluation employs eight subject systems and 600 features.
Results
It was found that, for ACIR: (1) method level granularity demands less search effort; (2) using more recent change-sets improves effectiveness; (3) aggregation of recent change-sets by change request, decreases effectiveness; (4) naive, text-classification-based filtering of “management” change-sets also decreases the effectiveness. In addition, a strongly pronounced dichotomy of subject systems emerged, where one set recorded better feature location using ACIR and the other recorded better feature location using the baseline approach. Finally, merging ACIR and the baseline approach significantly improved performance over both standalone approaches for all systems.
Conclusion
The most fundamental finding is the importance of rigorously characterizing proposed feature location techniques, to identify their optimal configurations. The results also suggest it is important to characterize the software systems under study when selecting the appropriate feature location technique. In the past, configuration of the techniques and characterization of subject systems have not been considered first-class entities in research papers, whereas the results presented here suggests these factors can have a big impact.","Muslim Chochlov and Michael English and Jim Buckley",2017,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Examining studies on learning management systems in SSCI database: A content analysis study","Distance education systems have become more prevalent over time and their use continue to increase. Learning Management Systems (LMS) is the main component for distance education to be carried out effectively and distantly. In this stuy, it is aimed to examine current studies on LMS published between 2010 and 2014 and provide an instructive source for researchers. The obtained data were analyzed and interpreted based on certain criteria. According to the results, LMS studies are mostly published in Computer & Science Journal, however there is a decline in the number of studies in 2014. Results also showed that researchers mostly conducted studies with university students and they examined the systems either developed by themselves or by an institution. It was revealed that Moodle is mostly used as open source code systems and WebCT (Blackboard) is mostly used as commercial. It can be indicated that seeking for new LMS by researchers would change based on the requirements and technological advances in the further research.","Fatih Soykan and Burak Şimşek",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Studying software evolution using topic models","Topic models are generative probabilistic models which have been applied to information retrieval to automatically organize and provide structure to a text corpus. Topic models discover topics in the corpus, which represent real world concepts by frequently co-occurring words. Recently, researchers found topics to be effective tools for structuring various software artifacts, such as source code, requirements documents, and bug reports. This research also hypothesized that using topics to describe the evolution of software repositories could be useful for maintenance and understanding tasks. However, research has yet to determine whether these automatically discovered topic evolutions describe the evolution of source code in a way that is relevant or meaningful to project stakeholders, and thus it is not clear whether topic models are a suitable tool for this task. In this paper, we take a first step towards evaluating topic models in the analysis of software evolution by performing a detailed manual analysis on the source code histories of two well-known and well-documented systems, JHotDraw and jEdit. We define and compute various metrics on the discovered topic evolutions and manually investigate how and why the metrics evolve over time. We find that the large majority (87%–89%) of topic evolutions correspond well with actual code change activities by developers. We are thus encouraged to use topic models as tools for studying the evolution of a software system.","Stephen W. Thomas and Bram Adams and Ahmed E. Hassan and Dorothea Blostein",2014,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Structural damage detection via adaptive dictionary learning and sparse representation of measured acceleration responses","Extracting damage-sensitive features from measured acceleration responses is still a big challenge in structural damage detection (SDD). In order to obtain the sparse representation of acceleration responses for damage identification, two dictionary learning methods and a damage indicator based on the change in mean sparsity (CMS) have been proposed in this paper. The first dictionary is empirical mode decomposition (EMD)-based dictionary generated by collecting a series of intrinsic mode functions (IMFs) while the second one is sparse coding (SC)-based dictionary learned by adaptively iterative optimization. The CMS is based on variation of a sparse features set. Numerical simulations on a simply-supported beam and on a 31-bar planar truss under different damage severities illustrate the effectiveness of the proposed methods for sparse representation and the capabilities of the defined CMS for damage indication. Comparative studies show that the SC-based dictionary outperforms the EMD-based dictionary in sparse representation. Moreover, a series of SDD experimental verifications on a simply-supported beam with a rectangular section in laboratory provide a further support to the proposed methods potentially using in-site acceleration measurements.","Zepeng Chen and Chudong Pan and Ling Yu",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Automatic detection of system-specific conventions unknown to developers","In Apache Ant, a convention to improve maintenance was introduced in 2004 stating a new way to close files instead of the Java generic InputStream.close(). Yet, six years after its introduction, this convention was still not generally known to the developers. Two existing solutions could help in these cases. First, one can deprecate entities, but, in our example, one can hardly deprecate Java’s method. Second, one can create a system-specific rule to be automatically enforced. In a preceding publication, we showed that system-specific rules are more likely to be noticed by developers than generic ones. However, in practice, developers rarely create specific rules. We therefore propose to free the developers from the need to create rules by automatically detecting such conventions from source code repositories. This is done by mining the change history of the system to discover similar changes being applied over several revisions. The proposed approach is applied to a real-world system, and the extracted rules are validated with the help of experts. The results show that many rules are in fact relevant for the experts.","André Hora and Nicolas Anquetil and Anne Etien and Stéphane Ducasse and Marco Túlio Valente",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"PRISM revisited: Declarative implementation of a probabilistic programming language using multi-prompt delimited control","PRISM is a probabilistic programming language based on Prolog, augmented with primitives to represent probabilistic choice. It is implemented using a combination of low level support from a modified version of B-Prolog, source level program transformation, and libraries for inference and learning implemented in C. More recently, developers working with functional programming languages have taken the approach of embedding probabilistic primitives into an existing language, with little or no modification to the host language, often by using delimited continuations. Captured continuations represent pieces of the probabilistic program which can be manipulated to achieve a great variety of computational effects useful for inference. In this paper, I will describe an approach based on delimited control operators recently introduced into SWI Prolog. These are used to create a system of nested effect handlers which together implement a core functionality of PRISM—the building of explanation graphs—entirely in Prolog and using an order of magnitude less code. Other declarative programming tools, such as constraint logic programming, are used to implement tools for inference, such as the inside-outside and EM algorithms, lazy best-first explanation search, and MCMC samplers. By embedding the functionality of PRISM into SWI Prolog, users gain access to its rich libraries and development environment. By expressing the functionality of PRISM in a small amount of pure, high-level Prolog, this implementation facilitates further experimentation with the mechanisms of probabilistic logic programming, including new probabilistic modelling features and inference algorithms, such as variational inference in models with real-valued variables.","Samer Abdallah",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Preventing Pollution Attacks in Cloud Storages","Cloud storage is a cloud-computing model in which data is stored on remote servers accessed from the internet. It has significantly changed the way users and administrators manage and access their data. Using remote storages to store data has many advantages in terms of availability and operational costs, but the security of such data is still one of the major concerns for the users. Pollution attack, where an adversary modifies some of the stored data is one of the many potent risks that affect the cloud data. In this paper, we show how disastrous pollution attack can be in coding based block level cloud storages, and how our algorithm using LRC, a version of Raptor codes, can identify an attack even before decoding all of the received packets.","Aswin Viswas V and Philip Samuel",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Numerical simulation of single bubble condensation in subcooled flow using OpenFOAM","The single condensing bubble behavior in subcooled flow has been numerical investigated using the open source code OpenFOAM. A coupled Level Set (LS) and Volume of Fluid (VOF) method (CLSVOF) model with a phase change model for condensation was developed and implemented in the code. The simulated results were firstly compared with the experimental results, they were in great agreements, and thus the simulation model was validated. The validated numerical model was then used to analyze the condensing bubble deformation, bubble lifetime, bubble size history, condensate Nusselt number and other interesting parameters with different variables in subcooled flow. The numerical results indicated that the initial bubble size, subcooling of liquid and system pressure play an important role to influence the condensing bubble behaviors significantly and bubble will be pierced when the subcooling and initial diameter reach a certain value at the later condensing stage. The bubble diameter history and condensate Nusselt number were found in good agreement with the empirical correlation. The drag force coefficient was predicted well by introducing a reduced drag coefficient.","Qingyun Zeng and Jiejin Cai and Huaqiang Yin and Xingtuan Yang and Tadashi Watanabe",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Improved bug localization based on code change histories and bug reports","Context
Several issues or defects in released software during the maintenance phase are reported to the development team. It is costly and time-consuming for developers to precisely localize bugs. Bug reports and the code change history are frequently used and provide information for identifying fault locations during the software maintenance phase.
Objective
It is difficult to standardize the style of bug reports written in natural languages to improve the accuracy of bug localization. The objective of this paper is to propose an effective information retrieval-based bug localization method to find suspicious files and methods for resolving bugs.
Method
In this paper, we propose a novel information retrieval-based bug localization approach, termed Bug Localization using Integrated Analysis (BLIA). Our proposed BLIA integrates analyzed data by utilizing texts, stack traces and comments in bug reports, structured information of source files, and the source code change history. We improved the granularity of bug localization from the file level to the method level by extending previous bug repository data.
Results
We evaluated the effectiveness of our approach based on experiments using three open-source projects, namely AspectJ, SWT, and ZXing. In terms of the mean average precision, on average our approach improves the metric of BugLocator, BLUiR, BRTracer, AmaLgam and the preliminary version of BLIA by 54%, 42%, 30%, 25% and 15%, respectively, at the file level of bug localization.
Conclusion
Compared with prior tools, the results showed that BLIA outperforms these other methods. We analyzed the influence of each score of BLIA from various combinations based on the analyzed information. Our proposed enhancement significantly improved the accuracy. To improve the granularity level of bug localization, a new approach at the method level is proposed and its potential is evaluated.","Klaus Changsun Youm and June Ahn and Eunseok Lee",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Dynamic and Speculative Polyhedral Parallelization of Loop Nests Using Binary Code Patterns","Speculative parallelization is a classic strategy for automatically parallelizing codes that cannot be handled at compile-time due to the use of dynamic data and control structures. Another motivation of being speculative is to adapt the code to the current execution context, by selecting at run-time an efficient parallel schedule. However, since this parallelization scheme requires on-the-fly semantics verification, it is in general difficult to perform advanced transformations for optimization and parallelism extraction. We propose a framework dedicated to speculative parallelization of scientific nested loop kernels, able to transform the code at runtime by re-scheduling the iterations to exhibit parallelism and data locality. The run-time process includes a transformation selection guided by profiling phases on short samples, using an instrumented version of the code. During this phase, the accessed memory addresses are interpolated to build a predictor of the forthcoming accesses. The collected addresses are also used to compute on-the-fly dependence distance vectors by tracking accesses to common addresses. Interpolating functions and distance vectors are then employed in dynamic dependence analysis and in selecting a parallelizing transformation that, if the prediction is correct, does not induce any rollback during execution. In order to ensure that the rollback time overhead stays low, the code is executed in successive slices of the outermost original loop of the nest. Each slice can be either a parallelized version, a sequential original version, or an instrumented version. Moreover, such slicing of the execution provides the opportunity of transforming differently the code to adapt to the observed execution phases. Parallel code generation is achieved almost at no cost by using binary code patterns that are generated at compile-time and that are simply patched at run-time to result in the transformed code. The framework has been implemented with extensions of the LLVM compiler and an x86-64 runtime system. Significant speed-ups are shown on a set of benchmarks that could not have been handled efficiently by a compiler.","Alexandra Jimborean and Philippe Clauss and Jean-François Dollinger and Vincent Loechner and Juan Manuel Martinez Caamaño",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Log-Data Clustering Analysis for Dropout Prediction in Beginner Programming Classes","Educational data mining (EDM) involves the application of data mining, machine learning, and statistics to information generated from an educational setting. In most school education, one teacher teaches many students. A periodic examination is used as a method to confirm that students have acquired skills. However, it is difficult to grasp the status of the student from each lesson, since examinations cannot be carried out easily. On the other hand, in programming classes, the students’ history of UNIX commands and source-code editing can be easily and automatically stored as log-data. Therefore, attempts have been made to estimate the student’s performance from this log-data, although their estimation accuracy is not high. In this research, we aim to extract those students who cannot keep up with programming lessons, rather than estimating the student’s performance from the log-data. Specifically, we propose a method for predicting dropouts using outlier detection to cluster data with unsupervised learning.","Shinichi Oeda and Genki Hashimoto",2017,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR8, CR9, CR10"
"Automated refactoring to the Null Object design pattern","Context
Null-checking conditionals are a straightforward solution against null dereferences. However, their frequent repetition is considered a sign of poor program design, since they introduce source code duplication and complexity that impacts code comprehension and maintenance. The Null Object design pattern enables the replacement of null-checking conditionals with polymorphic method invocations that are bound, at runtime, to either a real object or a Null Object.
Objective
This work proposes a novel method for automated refactoring to Null Object that eliminates null-checking conditionals associated with optional class fields, i.e., fields that are not initialized in all class instantiations and, thus, their usage needs to be guarded in order to avoid null dereferences.
Method
We introduce an algorithm for automated discovery of refactoring opportunities to Null Object. Moreover, we specify the source code transformation procedure and an extensive set of refactoring preconditions for safely refactoring an optional field and its associated null-checking conditionals to the Null Object design pattern. The method is implemented as an Eclipse plug-in and is evaluated on a set of open source Java projects.
Results
Several refactoring candidates are discovered in the projects used in the evaluation and their refactoring lead to improvement of the cyclomatic complexity of the affected classes. The successful execution of the projects’ test suites, on their refactored versions, provides empirical evidence on the soundness of the proposed source code transformation. Runtime performance results highlight the potential for applying our method to a wide range of project sizes.
Conclusion
Our method automates the elimination of null-checking conditionals through refactoring to the Null Object design pattern. It contributes to improvement of the cyclomatic complexity of classes with optional fields. The runtime processing overhead of applying our method is limited and allows its integration to the programmer’s routine code analysis activities.","Maria Anna G. Gaitani and Vassilis E. Zafeiris and N.A. Diamantidis and E.A. Giakoumakis",2015,"[""Science Direct""]","Rejeitado: CR11","Rejeitado: CR11"
"Inference rules for generic code migration of aspect-oriented programs","Several changes occurred in the AspectJ language to provide support for parametric polymorphism. Such changes aim to improve the source code type safety and to prepare the language to support generic code migration. Current approaches for this kind of migration focus only on object-oriented code. Therefore, they do not consider the use of aspects to encapsulate crosscutting concerns. We propose a collection of type constraint rules for the polymorphic version of AspectJ. These rules are used together with an existing constraint based algorithm to enable the conversion of non-generic code to add actual type parameters in both Java and AspectJ languages.","Fernando Barden Rubbo and Eduardo Kessler Piveta and Daltro José Nunes",2013,"[""Science Direct""]","Rejeitado: CR12","Rejeitado: CR12"
"Uncooperative gait recognition: Re-ranking based on sparse coding and multi-view hypergraph learning","Gait is an important biometric which can operate from a distance without subject cooperation. However, it is easily affected by changes in covariate conditions (carrying, clothing, view angle, walking speed, random noise etc.). It is hard for training set to cover all conditions. Bipartite ranking model has achieved success in gait recognition without assumption of subject cooperation. We propose a multi-view hypergraph learning re-ranking (MHLRR) method by integrating multi-view hypergraph learning (MHL) with hypergraph-based re-ranking framework. Sparse coding re-ranking (SCRR) and MHLRR are integrated under the graph-based framework to get a model. We define it as the sparse coding multi-view hypergraph learning re-ranking (SCMHLRR) method, which makes our approach achieve higher recognition accuracy under a genuine uncooperative setting. Extensive experiments demonstrate that our approach drastically outperforms existing ranking based methods, achieving good increase in recognition rate under the most difficult uncooperative settings.","Xin Chen and Jiaming Xu",2016,"[""Science Direct""]","Rejeitado: CR12","Rejeitado: CR12"
"Enhanced geometric capabilities for the transient analysis code T-ReX and its application to simulating TREAT experiments","Advances in computational architecture have prompted a resurgence in the simulation of reactor transients from first principles. Most codes are unable to simulate transient events with complex models, and require numerous approximations. The code T-ReX (Transient-Reactor eXperiment simulator), an extensive update to TDKENO, has been developed as a transient analysis tool with few geometric limitations, and minimal theoretical approximations. T-ReX achieves this by employing the Improved Quasi-Static (IQS) method to solve the time-dependent Boltzmann transport equation with explicit representation of delayed neutrons. The primary change in T-ReX relative to TDKENO is the incorporation of a modified version of the Monte Carlo code KENO-VI to calculate the flux shape and model the geometry of a problem. Using KENO-VI to model systems allows exact representation of the geometry. The changes to T-ReX are verified by comparison of solutions to computational benchmark problems found with a previous version of TDKENO that made use of KENO V.a, and several other codes with time-dependent capabilities. In addition, a three-dimensional KENO-VI model of the Transient Reactor Test Facility (TREAT) core is used in simulations of several temperature-limited transient experiments from the M8 Calibration series. T-ReX produces results that agree with benchmark problems and are in better agreement with TREAT experimental data than TDKENO.","Zander Mausolff and Mark DeHart and Sedat Goluoglu",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Facilitating the development of cross-platform software via automated code synthesis from web-based programming resources","When a mobile application is supported on multiple major platforms, its market penetration is maximized. Such cross-platform native applications essentially deliver the same core functionality, albeit within the conventions of each supported platform. Maintaining and evolving a cross-platform native application is tedious and error-prone, as each modification requires replicating the changes for each of the application׳s platform-specific variants. Syntax-directed source-to-source translation proves inadequate to alleviate the problem, as native API access is always domain-specific. In this paper, we present a novel approach—Native-2-Native—that uses program transformations performed on one platform to automatically synthesize equivalent code blocks to be used on another platform. When a programmer modifies the source version of an application, the changes are captured. Based on the changes, Native-2-Native identifies the semantic content of the source code block and formulates an appropriate query to search for the equivalent target code block using popular web-based programming resources. The discovered target code block is then presented to the programmer as an automatically synthesized target language source file for further fine-tuning and subsequent integration into the mobile application׳s target version. We evaluate the proposed method using common native resources, such as sensors, network access, and canonical data structures. We show that our approach can correctly synthesize more than 74% of iOS code from the provided Android source code and 91% of Android code from the provided iOS source code. The presented approach effectively automates the process of extracting the source code block׳s semantics and discovering existing target examples with the equivalent functionality, thus alleviating some of the most laborious and intellectually tiresome programming tasks in modern mobile development.","Sanchit Chadha and Antuan Byalik and Eli Tilevich and Alla Rozovskaya",2017,"[""Science Direct""]","Rejeitado: CR12","Rejeitado: CR12"
"Ring: A unifying meta-model and infrastructure for Smalltalk source code analysis tools","Source code management systems record different versions of code. Tool support can then compute deltas between versions. To ease version history analysis we need adequate models to represent source code entities. Now naturally the questions of their definition, the abstractions they use, and the APIs of such models are raised, especially in the context of a reflective system which already offers a model of its own structure. We believe that this problem is due to the lack of a powerful code meta-model as well as an infrastructure. In Smalltalk, often several source code meta-models coexist: the Smalltalk reflective API coexists with the one of the Refactoring engine or distributed versioning system such as Monticello or Store. While having specific meta-models is an adequate engineered solution, it multiplies meta-models and it requires more maintenance efforts (e.g., duplication of tests, transformation between models), and more importantly hinders navigation tool reuse when meta-models do not offer polymorphic APIs. As a first step to provide an infrastructure to support history analysis, this article presents Ring, a unifying source code meta-model that can be used to support several activities and proposes a unified and layered approach to be the foundation for building an infrastructure for version and stream of change analyses. We re-implemented three tools based on Ring to show that it can be used as the underlying meta-model for remote and off-image browsing, scoping refactoring, and visualizing and analyzing changes. As a future work and based on Ring we will build a new generation of history analysis tools.","Verónica Uquillas Gómez and Stéphane Ducasse and Theo D'Hondt",2012,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"A Support System for Generating SCORM Compliant Open Source Software Usage Manuals","Open Source Software (OSS) is software whose source code that is open to the public through the Internet. Currently, OSS is widely used in many aspects of IT society. Because OSS development is community based, unlike commercial software, the lack of good documentation or the maintenance of manuals is one of the main problems of using OSS. Due to its rapid development, OSS manuals become easily obsolete. Moreover, the installation or the usage varies depending on the operating system. To solve the documentation problems, Murakami et al. proposed a method of automatically generating a web manual for installing an OSS by editing the log information recorded during the installation process. Unfortunately, the web manual generated by this system was not suitable for wide use in learning management systems. Therefore, this paper extends the sys- tem by Murakami et al. to one with the ability to deliver an automatically generated Web manual on an e-learning management system, modify the content of the manual, and skip unnecessary information in the learning process.","Akhmad Syaikhul Hadi and Takashi Yukawa and Yukikazu Murakami",2013,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"RNA editing and modifications of RNAs might have favoured the evolution of the triplet genetic code from an ennuplet code","Here we suggest that the origin of the genetic code, that is to say, the birth of first mRNAs has been triggered by means of a widespread modification of all RNAs (proto-mRNAs and proto-tRNAs), as today observed in the RNA editing and in post-transcriptional modifications of RNAs, which are considered as fossils of this evolutionary stage of the genetic code origin. We consider also that other mechanisms, such as the trans-translation and ribosome frameshifting, could have favoured the transition from an ennuplet code to a triplet code. Therefore, according to our hypothesis all these mechanisms would be reflexive of this period of the evolutionary history of the genetic code.","Massimo Di Giulio and Marco Moracci and Beatrice Cobucci-Ponzano",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Supporting streams of changes during branch integration","When developing large applications, integrators face the problem of integrating changes between branches or forks. While version control systems provide support for merging changes, this support is mostly text-based, and does not take the program entities into account. Furthermore, there exists no support for assessing which other changes a particular change depends on have to be integrated. Consequently, integrators are left to perform a manual and tedious comparison of the changes within the sequence of their branch and to successfully integrate them. In this paper, we present an approach that analyzes changes within a sequence of changes (stream of changes): such analysis identifies and characterizes dependencies between the changes. The approach identifies changes as autonomous, only used by others, only using other changes, or both. Such a characterization aims at easing the integrator's work. In addition, the approach supports important queries that an integrator otherwise has to perform manually. We applied the approach to a stream of changes representing 5 years of development work on an open-source project and report our experiences.","Verónica Uquillas Gómez and Stéphane Ducasse and Andy Kellens",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Scaling modified condition/decision coverage using distributed concolic testing for Java programs","Object-Oriented languages such as Java language introduce advantageous features which overcome the demerits of procedural languages to some extent. Therefore, Java language is now going to be used by the industries to develop their critical safety system software products. In this paper, we propose some code transformation methodologies, which are implemented in Java language to test Java written code. We apply Java Distributed Concolic testing technique to improve the code coverage, which is more powerful than non-distributed concolic testing in terms of speed of test case generation. We develop a Java coverage analyzer according to the test cases produced by Java distributed concolic testers. This version of the MC/DC analyzer is more powerful than that of procedural languages. Our core idea is to integrate the existing and developed modules to produce a single tool for measuring MC/DC score. This novel idea automates the flow of testing 100%. Our experimental results present different scenarios, and suggest the stronger one. On an average, for forty-five Java programs using three nodes in the client–server architecture, we achieved higher MC/DC score.","Sangharatna Godboley and Arpita Dutta and Durga Prasad Mohapatra and Rajib Mall",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The TDHF code Sky3D version 1.1","The nuclear mean-field model based on Skyrme forces or related density functionals has found widespread application to the description of nuclear ground states, collective vibrational excitations, and heavy-ion collisions. The code Sky3D solves the static or dynamic equations on a three-dimensional Cartesian mesh with isolated or periodic boundary conditions and no further symmetry assumptions. Pairing can be included in the BCS approximation for the static case. The code is implemented with a view to allow easy modifications for including additional physics or special analysis of the results.
New version program summary
Program title: Sky3D Program Files doi:http://dx.doi.org/10.17632/vzbrzvyrn4.1 Licensing provisions: GPLv3 Programming language: Fortran 90. The OpenMP version requires a relatively recent compiler; it was found to work using gfortran 4.6.2 or later and the Intel compiler version 12 or later. Journal reference of previous version: J. A. Maruhn, P.-G. Reinhard, P. D. Stevenson, and A. S. Umar, “The TDHF Code Sky3D”, Comp. Phys. Comm. 185, 2195 (2014). Does the new version supersede the previous version?: Yes. Nature of problem: The time-dependent Hartree–Fock equations can be used to simulate nuclear vibrations and collisions between nuclei for low energies. This code implements the equations based on a Skyrme energy functional and also allows the determination of the ground-state structure of nuclei through the static version of the equations. For the case of vibrations the principal aim is to calculate the excitation spectra by Fourier-analyzing the time dependence of suitable observables. In collisions, the formation of a neck between nuclei, the dissipation of energy from collective motion, processes like charge transfer and the approach to fusion are of principal interest. Solution method: The nucleonic wave function spinors are represented on a three-dimensional Cartesian mesh with no further symmetry restrictions. The boundary conditions are always periodic for the wave functions, while the Coulomb potential can also be calculated for an isolated charge distribution. All spatial derivatives are evaluated using the finite Fourier transform method. The code solves the static Hartree–Fock equations with a damped gradient iteration method and the time-dependent Hartree–Fock equations with an expansion of the time-development operator. Any number of initial nuclei can be placed into the mesh with arbitrary positions and initial velocities. Reasons for the new version: A few bugs were fixed and a number of enhancements added concerning faster convergence, better stability, and more sophisticated analysis of some results. Summary of revisions: The following is a brief summary. A more complete documentation can be found as update.pdf in the Documentation subdirectory. New documentation: It was decided to switch the documentation to using the Doxygen system (available from www.doxygen.org), which can generate the documentation in a variety of formats. To generate the documentation, go into the Doc-doxygen subdirectory and execute make html, make latex, or make all to produce the corresponding version or both of them. The documentation inserted into the source files accounts for most of the formal changes in them. The general documentation is also updated and present as “Documentation.pdf”. Bug fixes: 1.In the force database forces.data two digits were interchanged in the definition of SLy4d, leading to wrong results for that force.2.If a restart is done for a two-body collision, the code changed the number of fragments to nof =1. The restart is then initialized like a single-nucleus case with nof =1. But two-body analysis was activated only for nof =1 such that it was absent after restart.3.In the time-dependent mode, the wave functions were only save at intervals of mprint and mrest, respectively. If a calculation stops because of reaching the final distance or fulfilling the convergence criterion, this may lead to a loss of information, so that now both are done also in this event before the job finishes.4.The external field parameters were calculated directly from the input in getin_external. Since this is called before the fragment initialization is done, coefficients depending on proton or neutron number will not be calculated correctly. For this reason, the calculation of these coefficients is separated into a new routine init_external, which is called directly before the dynamic calculation starts. Array allocation: It turned out that having the working arrays as automatic variables could cause problems, as they are allocated on the stack and the proper stack size must be calculated. Therefore in all cases where a larger array is concerned, it is now changed to ALLOCATABLE and allocated and deallocated as necessary. Elimination of “guru” mode of FFTW3 While the guru mode as defined in the FFTW3 package (see fftw.org) offers an elegant formulation of complicated multidimensional transforms, it is not implemented in some support libraries like the Intel® MKL. There is not much loss in speed when this is replaced by standard transforms with some explicit loops added where necessary. This affects the wave function transforms in the y and z direction. Enhancement of the makefile In the previous version there were several versions of the makefile, which had to be edited by hand to use different compilers. This was reformulated using a more flexible file with various targets predefined. Thus to generate the executable code, it is sufficient to execute “maketarget” in the Code subdirectory, where target is one of the following: •seq : simple sequential version with the gfortran compiler.•ifort, ifort_seq : sequential version using the Intel compiler.•omp and ifort_omp produce the OpenMP version for the gfortran and Intel compiler, respectively.•mpi : MPI version, which uses the compiler mpif90.•mpi-omp : MPI version also using OpenMP on each node.•debug, seq_debug, omp_debug, mpi_debug : enable debugging mode for these cases. The first three use the gfortran compiler.•clean : removes the generated object and module files.•clean-exec : same as clean but removes the executable files as well.The generated executable files are called sky3d.seq, sky3d.ifort.seq, sky3d.mpi, sky3d.omp, sky3d.ifort.omp, and sky3d.mpi-omp, which should be self-explanatory. Thus several versions may be kept in the code directory, but a make clean should be done before producing a new version to make sure the object and module files are correct. Skyrme-force compatibility for static restart: the code normally checks that the Skyrme forces for all the input wave functions agree. It may be useful, however, to initialize a static calculation from results for a different Skyrme force. Therefore the consistency check was eliminated for the static case. Acceleration of the static calculations: The basic parameters for the static iterations are (see Eq. 12 of the original paper) x0 (variable x0dmp), which determines the size of the gradient step, and E0 (variable e0dmp) for the energy damping. These were read in and never changed throughout the calculation, except possibly through a restart. This can cause slow convergence, so that a method was developed to change x0dmp during the iterations. The value from the input is now regarded as the minimum allowed one and saved in x0dmpmin. At the start of the iterations, however, x0dmp is multiplied by 3 to attempt a faster convergence. The change in x0dmp is then implemented by comparing the HF energy ehf and the fluctuations efluct1 and efluct2 to the previous values saved in the variables ehfprev, efluct1prev, and efluct2prev. If the energy decreases or one of the fluctuations decreases by a factor of less than 1−10−5, x0dmp is increased by a factor 1.005 to further speed up convergence. If none of these conditions holds, it is assumed that the step was too large and x0dmp is reduced by a factor 0.8, but is never allowed to fall below x0dmpmin. This whole process is turned on only if the input variable tvaryx_0 in the namelist “static” is .TRUE. The default value is .FALSE. A speedup up to a factor of 3 has been observed. External field expectation value This value, which is printed in the file external.res, was calculated from the spatial field including the (time-independent) amplitude amplq0. The temporal Fourier transform then becomes quadratic in the amplitude, as the fluctuations in the density also grow linearly in amplq0 (provided the perturbation is not strong enough to take it into the nonlinear regime). This may be confusing and we therefore divided the expectation value by this factor. Note that if the external field is composed of a mixture of different multipoles (not coded presently), an overall scale factor should instead be used. Enhanced two-body analysis: the analysis of the final two-body quantities after breakup included directly in the code was very simplified and actually it was superfluous to do this so frequently. This is replaced by a much more thorough analysis, including determination of the internal angular momenta of the fragments and of a quite accurate Coulomb energy. It is done only when the final separation is reached, while a simple determination of whether the fragments have separated and, if so, what their distance is, is performed every time step. Diagonalization In the original program the diagonalization of the Hamiltonian in the subroutine diagstep was carried out employing an eigenvalue decomposition using the LAPACK routine ZHBEVD which is optimized for banded matrices. This routine is replaced in the update by the routine ZHEEVD which is optimized for general hermitian matrices. This change should result in a moderate speed up for very large calculations. Furthermore the array unitary, previously a nstmax×nstmax array has been reduced to a nlin×nlin array, where nlin is the number of wave functions of either neutrons or protons. This array is now used as input and output for ZHEEVD. New formulation of the spin–orbit term: The action of the spin–orbit term has been corrected to comply with a strictly variational form. Starting from the spin–orbit energy (1)Els=tls∫d3rJ→⋅∇ρ,we obtain by variation with respect to the s.p. wavefunction ψ∗ the spin–orbit term in the mean field in the symmetrized form (2)hˆlsψ=i2W→⋅(σ→×∇)ψ+σ→⋅(∇×(W→ψ))where W→=tls∇ρ. In the previous version of the code, this term was simplified by applying the product rule for the ∇ operator yielding (3)i2W→⋅(σ→×∇)ψ+σ→⋅(∇×(W→ψ))=iW→⋅(σ→×∇)ψ.Closer inspection reveals that the product rule is not perfectly fulfilled if the ∇ operator is evaluated with finite Fourier transformation as inevitably done in the grid representation of the code. It turned out that this slight mismatch can accumulate to instabilities in TDHF runs over long times. Thus the variationally correct form (2) has been implemented now, although it leads to slightly longer running times. Supplementary material: Extensive documentation and a number of utility programs to analyze the results and prepare them for graphics output using the Silo library (http://wci.llnl.gov/simulation/computer-codes/silo) for use in VisIT [1] or Paraview (https://www.paraview.org). The code can serve as a template for interfacing to other database or graphics systems. External routines/libraries: LAPACK, FFTW3. Restrictions: The reliability of the mean-field approximation is limited by the absence of hard nucleon–nucleon collisions. This limits the scope of applications to collision energies about a few MeV per nucleon above the Coulomb barrier and to relatively short interaction times. Similarly, some of the missing time-odd terms in the implementation of the Skyrme interaction may restrict the applications to even–even nuclei. Unusual features: The possibility of periodic boundary conditions and the highly flexible initialization make the code also suitable for astrophysical nuclear-matter applications. Acknowledgments This work was supported by DOE under contract numbers DE-SC0013847, DE-NA0002847, DE-SC0013365, and DE-SC0008511, by BMBF under contract number 05P15RDFN1, and by UK STFC under grant number ST/P005314/1. References[1]H. Childs, E. Brugger, B. Whitlock, J. Meredith, S. Ahern, D. Pugmire, K. Biagas, M. Miller, C. Harrison, G. H. Weber, H. Krishnan, T. Fogal, A. Sanderson, C. Garth, E. W. Bethel, D. Camp, O. Rübel, M. Durant, J. M. Favre, P. Navrátil, VisIt: An End-User Tool For Visualizing and Analyzing Very Large Data, in: High Performance Visualization–Enabling Extreme-Scale Scientific Insight, 2012, pp. 357–372.","B. Schuetrumpf and P.-G. Reinhard and P.D. Stevenson and A.S. Umar and J.A. Maruhn",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"TIM, a ray-tracing program for METATOY research and its dissemination","TIM (The Interactive METATOY) is a ray-tracing program specifically tailored towards our research in METATOYs, which are optical components that appear to be able to create wave-optically forbidden light-ray fields. For this reason, TIM possesses features not found in other ray-tracing programs. TIM can either be used interactively or by modifying the openly available source code; in both cases, it can easily be run as an applet embedded in a web page. Here we describe the basic structure of TIMʼs source code and how to extend it, and we give examples of how we have used TIM in our own research.
Program summary
Program title: TIM Catalogue identifier: AEKY_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEKY_v1_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: GNU General Public License No. of lines in distributed program, including test data, etc.: 124 478 No. of bytes in distributed program, including test data, etc.: 4 120 052 Distribution format: tar.gz Programming language: Java Computer: Any computer capable of running the Java Virtual Machine (JVM) 1.6 Operating system: Any; developed under Mac OS X Version 10.6 RAM: Typically 145 MB (interactive version running under Mac OS X Version 10.6) Classification: 14, 18 External routines: JAMA [1] (source code included) Nature of problem: Visualisation of scenes that include scene objects that create wave-optically forbidden light-ray fields. Solution method: Ray tracing. Unusual features: Specifically designed to visualise wave-optically forbidden light-ray fields; can visualise ray trajectories; can visualise geometric optic transformations; can create anaglyphs (for viewing with coloured “3D glasses”) and random-dot autostereograms of the scene; integrable into web pages. Running time: Problem-dependent; typically seconds for a simple scene. References:[1]JAMA: A Java matrix package, http://math.nist.gov/javanumerics/jama/.","Dean Lambert and Alasdair C. Hamilton and George Constable and Harsh Snehanshu and Sharvil Talati and Johannes Courtial",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Optimizing generated aspect-oriented assertion checking code for JML using program transformations: An empirical study","The AspectJ JML compiler (ajmlc) explores aspect-oriented programming (AOP) mechanisms to implement JML specifications, such as pre- and postconditions, and enforce them during runtime. This compiler was created to improve source-code modularity. Some experiments were conducted to evaluate the performance of the code generated through ajmlc. Results demonstrated that the strategy of adopting AOP to implement JML specifications is very promising. However, there is still a need for optimization of the generated code’s bytecode size and running time. This paper presents a catalog of transformations which represent the optimizations implemented in the new optimized version of the ajmlc compiler. We employ such transformations to reduce the bytecode size and running time of the code generated through the ajmlc compiler. Aiming at demonstrating the impact of such transformation on the code quality, we conduct an empirical study using four applications in optimized and non-optimized versions generated by ajmlc. We show that our AOP transformations provide a significant improvement, regarding bytecode size and running time.","Henrique Rebêlo and Ricardo Lima and Gary T. Leavens and Márcio Cornélio and Alexandre Mota and César Oliveira",2013,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Collaborative multiview hashing","In this paper, we propose a collaborative multiview hashing (CMH) approach to incorporate multiview representations into the binary code learning for scalable visual retrieval. Unlike most existing multiview hashing methods which learn linear projections to preserve the fused similarity relationship across different views in unsupervised manner, we employ the nonlinear hashing functions as the projection in each view and exploit the diverse information of multiview representations by utilizing the collaboration between view representations and the correlation between the view representations and the semantic labels. Specifically, the binary codes in each view are constrained to be predictive to each other to exploit the collaboration between the descriptors in different views that describe the same sample. Furthermore, the binary codes in all views are enforced to preserve the semantic relationship between data samples. The hashing functions are implemented in the form of multi-layer neural network with nonlinear transformations at each layer and trained with both the view collaboration and semantic preserving constraints on the outputs. Experimental results on two datasets validate the superiority of the proposed approach in comparison with several state-of-the-art hashing methods.","Zhixiang Chen and Jie Zhou",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Conversion of STOPP/START version 2 into coded algorithms for software implementation: a multidisciplinary consensus procedure","Background
The rapid digitalization of medical practice has attracted growing interest in developing software applications for clinical guidelines and explicit screening tools to detect potentially inappropriate prescribing, such as STOPP/START criteria. The aim of the current study was to develop and provide logically unambiguous algorithms of STOPP/START criteria version 2, encoded with international disease and medication classification codes, to facilitate the development of software applications for multiple purposes.
Methods
A four round multidisciplinary consensus and validation procedure was conducted to develop implementable coded algorithms for software applications of STOPP/START criteria version 2, based on ICD, ICPC, LOINC and ATC classification databases.
Results
Consensus was reached for all 34 START criteria and 76 out of 80 STOPP criteria. The resulting 110 algorithms, modeled as inference rules in decision tables, are provided as supplementary data.
Conclusion
This is the first study providing implementable algorithms for software applications based on STOPP/START version 2, validated in a computer decision support system. These algorithms could serve as a template for applying STOPP/START criteria version 2 to any software application, allowing for adaptations of the included ICD, ICPC and ATC codes and changing the cut-off levels for laboratory measurements to match local guidelines or clinical expertise.","Corlina J.A. (Lianne) Huibers and Bastiaan T.G.M. Sallevelt and Dominique A. de Groot and Maarten J. Boer and Jos P.C.M. van Campen and Cathelijn J. Davids and Jacqueline G. Hugtenburg and Annemieke M.A. Vermeulen Windsant-Van den Tweel and Hein P.J. van Hout and Rob J. van Marum and Michiel C. Meulendijk",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Towards an understanding of change types in bug fixing code","Context
As developing high quality software becomes increasingly challenging because of the explosive growth of scale and complexity, bugs become inevitable in software systems. The knowledge of bugs will naturally guide software development and hence improve software quality. As changes in bug fixing code provide essential insights into the original bugs, analyzing change types is an intuitive and effective way to understand the characteristics of bugs.
Objective
In this work, we conduct a thorough empirical study to investigate the characteristics of change types in bug fixing code.
Method
We first propose a new change classification scheme with 5 change types and 9 change subtypes. We then develop an automatic classification tool CTforC to categorize changes. To gain deeper insights into change types, we perform our empirical study based on three questions from three perspectives, i.e. across project, across domain and across version.
Results
Based on 17 versions of 11 systems with thousands of faulty functions, we find that: (1) across project: the frequencies of change subtypes are significantly similar across most studied projects; interface related code changes are the most frequent bug-fixing changes (74.6% on average); most of faulty functions (65.2% on average) in studied projects are finally fixed by only one or two change subtypes; function call statements are likely to be changed together with assignment statements or branch statements; (2) across domain: the frequencies of change subtypes share similar trends across studied domains; changes on function call, assignment, and branch statements are often the three most frequent changes in studied domains; and (3) across version: change subtypes occur with similar frequencies across studied versions, and the most common subtype pairs tend to be same.
Conclusion
Our experimental results improve the understanding of changes in bug fixing code and hence the understanding of the characteristics of bugs.","Yangyang Zhao and Hareton Leung and Yibiao Yang and Yuming Zhou and Baowen Xu",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"The History and Impact of Molecular Coding Changes on Coverage and Reimbursement of Molecular Diagnostic Tests: Transition from Stacking Codes to the Current Molecular Code Set Including Genomic Sequencing Procedures","Changes in coding and coverage generate an uncertain reimbursement environment for molecular pathology laboratories. We analyzed our experience with two representative molecular oncology tests: a T-cell receptor (TCR) β rearrangement test and a large (467-gene) cancer next-generation sequencing panel, the Columbia Combined Cancer Panel (CCCP). Before 2013, the TCR β test was coded using stacked current procedural terminology codes and subsequently transitioned to a tier 1 code. CCCP was coded using a combination of tier 1 and 2 codes until 2015, when a new Genomic Sequencing Procedure code was adopted. A decrease in reimbursement of 61% was observed for the TCR β test on moving from stacking to tier 1 codes. No initial increase in total rejection rate was observed, but a subsequent increase in rejection rates in 2015 and 2016 was noted. The CCCP test showed a similar decrease (48%) in reimbursement after adoption of the new Genomic Sequencing Procedure code and was accompanied by a sharp increase in rejection rates both on implementation of the new code and over time. Changes in coding can result in substantial decreases in reimbursement. This may be a barrier to patient access because of the high cost of molecular diagnostics. Revisions to the molecular code set will continue. These findings help laboratories and manufacturers prepare for the financial impact and advocate appropriately.","Susan J. Hsiao and Mahesh M. Mansukhani and Melissa C. Carter and Anthony N. Sireci",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Source Code Editing Evaluator for Learning Programming","Each semester, School of Electrical Engineering and Informatics (SEEI) ITB manages ±400 students taking programming courses with many examinations and exercises but can only provides limited feedback and facilities. The number of PCs are much less than the students. Although most students have personal computing devices, those devices have various specifications that create difficulties in evaluation. To tackle these issues, we provide an integrated system for learning programming that covers source code editing, compiling, execution, submission and evaluation of the process and the result. In this paper, we emphasize on source code editing environment named Doppel and Ganger (D&G). D&G is a web based application aimed for monitoring coding process by recording typing activities. It also supports online compilation and execution using an autograder. We have experimented programming practice using this environment and it has shown that D&G can significantly support the learning process of programming.","Timotius Nugroho Chandra and Inggriani Liem",2013,"[""Science Direct""]","Rejeitado: CR12","Rejeitado: CR12"
"Empirical analysis of change metrics for software fault prediction","A quality assurance activity, known as software fault prediction, can reduce development costs and improve software quality. The objective of this study is to investigate change metrics in conjunction with code metrics to improve the performance of fault prediction models. Experimental studies are performed on different versions of Eclipse projects and change metrics are extracted from the GIT repositories. In addition to the existing change metrics, several new change metrics are defined and collected from the Eclipse project repository. Machine learning algorithms are applied in conjunction with the change and source code metrics to build fault prediction models. The classification model with new change metrics performs better than the models using existing change metrics. In this work, the experimental results demonstrate that change metrics have a positive impact on the performance of fault prediction models, and high-performance models can be built with several change metrics.","Garvit Rajesh Choudhary and Sandeep Kumar and Kuldeep Kumar and Alok Mishra and Cagatay Catal",2018,"[""Science Direct"",""Engineering Village""]","Aceito: CA6","Aceito: CA6"
"Improving regression test efficiency with an awareness of refactoring changes","Context. Developers often improve software quality through refactorings—the practice of behavior-preserving changes to existing code. Recent studies showed that, despite their awareness of tool support for automated refactorings, developers prefer manual refactorings. This practice can be often error-prone and increase testing cost. Objective. To address the problem, we present the Refactorings Investigation and Testing technique, called Rit. Rit improves the testing efficiency for validating refactoring changes and providing confidence that changed parts behave as intended. As testing is expensive for developers of high-assurance software, Rit reduces a considerable amount of its costs by only identifying dependent statements on a failure in each test and by detecting specific refactoring edits responsible for testing failures. Method. Our approach identifies refactorings by analyzing original and edited versions of a program. It then uses the semantic impact of a set of identified refactoring changes to detect tests whose behavior may have been affected and modified by refactoring edits. Given each failed asserts after running regression test suites, Rit helps developers focus their attention on logically related program statements by applying program slicing for minimizing each test. For debugging purposes, Rit determines specific failure-inducing refactoring edits, separating from other changes that only affect other asserts or tests. Results. We evaluated Rit on three open source projects, and found that Rit detected tests affected by refactorings with 80.9% accuracy on average. Furthermore, it identified and formed partitions relating program statements only dependent on failed asserts with 97.2% accuracy on average. Conclusion. Rit, which combines a refactoring reconstruction technique with change impact analysis to localize failure-inducing program edits, helps developers localize fault causes by focusing on refactoring changes as opposed to all the code fragments in the new version.","Zhiyuan Chen and Hai-Feng Guo and Myoungkyu Song",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"R2PCAH: Hashing with two-fold randomness on principal projections","Hashing based strategies have recently been widely used in fast similarity search on large scale datasets. Data-independent methods such as Locality Sensitive Hashing (LSH) usually adopt random projections as hash functions, with theoretical guarantees that the performance improves with the increasing code length. Thus they require relatively long codes, making them less effective than data-dependent methods. On the other hand, in many data-dependent hashing methods, Principal Component Analysis (PCA) is widely used to generate compact hash codes. However, PCA based methods tend not to be effective for generating long codes because projections with small variances may induce certain redundancy and noise. In order to address these deficiencies, we present a R2PCAH framework that conducts two-fold random transformations based on principal projections for hash code learning. Specifically, only the top PCA projections of the training data are extracted and two-fold random transformations, i.e. random rotations and random shifts are performed on the projected data to generate several pieces of component short codes. The multiple component short codes are then concatenated into one piece of long code. We observe that our method shares the advantages of both LSH and PCA based hashing methods. Extensive experiments demonstrate the effectiveness of the proposed method.","Peng Li and Peng Ren",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Coding interactions in Motivational Interviewing with computer-software: What are the advantages for process researchers?","Motivational Interviewing (MI) is an evidence-based behavior change intervention. The interactional change processes that make MI effective have been increasingly studied using observational coding schemes. We introduce an implementation of a software-supported MI coding scheme—the Motivational Interviewing Treatment Integrity code (MITI)—and discuss advantages for process researchers. Furthermore, we compared reliability of the software version with prior results of the paper version. A sample of 14 double-coded dyadic interactions showed good to excellent interrater reliabilities. We selected a second sample of 22 sessions to obtain convergent validity results of the software version: substantial correlations were obtained between the software instrument and the Rating Scales for the Assessment of Empathic Communication. Finally, we demonstrate how the software version can be used to test whether single code frequencies obtained by using intervals shorter than 20min (i.e., 5 or 10min) are accurate estimates of the respective code frequencies for the entire session (i.e., behavior slicing). Our results revealed that coding only a 10-min interval provides accurate estimates of the entire session. Our study demonstrates that the software implementation of the MITI is a reliable and valid instrument. We discuss advantages of the software version for process research in MI.","Florian E. Klonek and Vicenç Quera and Simone Kauffeld",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Adaptive Prediction Error Coding in the Human Midbrain and Striatum Facilitates Behavioral Adaptation and Learning Efficiency","Summary
Effective error-driven learning benefits from scaling of prediction errors to reward variability. Such behavioral adaptation may be facilitated by neurons coding prediction errors relative to the standard deviation (SD) of reward distributions. To investigate this hypothesis, we required participants to predict the magnitude of upcoming reward drawn from distributions with different SDs. After each prediction, participants received a reward, yielding trial-by-trial prediction errors. In line with the notion of adaptive coding, BOLD response slopes in the Substantia Nigra/Ventral Tegmental Area (SN/VTA) and ventral striatum were steeper for prediction errors occurring in distributions with smaller SDs. SN/VTA adaptation was not instantaneous but developed across trials. Adaptive prediction error coding was paralleled by behavioral adaptation, as reflected by SD-dependent changes in learning rate. Crucially, increased SN/VTA and ventral striatal adaptation was related to improved task performance. These results suggest that adaptive coding facilitates behavioral adaptation and supports efficient learning.","Kelly M.J. Diederen and Tom Spencer and Martin D. Vestergaard and Paul C. Fletcher and Wolfram Schultz",2016,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Domain-specific acceleration and auto-parallelization of legacy scientific code in FORTRAN 77 using source-to-source compilation","Massively parallel accelerators such as GPGPUs, manycores and FPGAs represent a powerful and affordable tool for scientists who look to speed up simulations of complex systems. However, porting code to such devices requires a detailed understanding of heterogeneous programming tools and effective strategies for parallelization. In this paper we present a source to source compilation approach with whole-program analysis to automatically transform single-threaded FORTRAN 77 legacy code into OpenCL-accelerated programs with parallelized kernels. The main contributions of our work are: (1) whole-source refactoring to allow any subroutine in the code to be offloaded to an accelerator. (2) Minimization of the data transfer between the host and the accelerator by eliminating redundant transfers. (3) Pragmatic auto-parallelization of the code to be offloaded to the accelerator by identification of parallelizable maps and reductions. We have validated the code transformation performance of the compiler on the NIST FORTRAN 78 test suite and several real-world codes: the Large Eddy Simulator for Urban Flows, a high-resolution turbulent flow model; the shallow water component of the ocean model Gmodel; the Linear Baroclinic Model, an atmospheric climate model and Flexpart-WRF, a particle dispersion simulator. The automatic parallelization component has been tested on as 2-D Shallow Water model (2DSW) and on the Large Eddy Simulator for Urban Flows (UFLES) and produces a complete OpenCL-enabled code base. The fully OpenCL-accelerated versions of the 2DSW and the UFLES are resp. 9x and 20x faster on GPU than the original code on CPU, in both cases this is the same performance as manually ported code.","Wim Vanderbauwhede and Gavin Davidson",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Open source FreeRTOS as a case study in real-time operating system evolution","This paper studies the evolution of a real-time operating system, the open source FreeRTOS. We focus on the changes in real-time performance and behaviour over the last ten years. Six major release versions are benchmarked, presenting quantitative and qualitative development trends. We also use the available source code to discover the reasons for the changes. By analysing the results, we draw some conclusions related to this RTOS’s evolution which can be useful for the FreeRTOS group, other RTOS developments, and also RTOS users.","Fei Guan and Long Peng and Luc Perneel and Martin Timmerman",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"How deep learning extracts and learns leaf features for plant classification","Plant identification systems developed by computer vision researchers have helped botanists to recognize and identify unknown plant species more rapidly. Hitherto, numerous studies have focused on procedures or algorithms that maximize the use of leaf databases for plant predictive modeling, but this results in leaf features which are liable to change with different leaf data and feature extraction techniques. In this paper, we learn useful leaf features directly from the raw representations of input data using Convolutional Neural Networks (CNN), and gain intuition of the chosen features based on a Deconvolutional Network (DN) approach. We report somewhat unexpected results: (1) different orders of venation are the best representative features compared to those of outline shape, and (2) we observe multi-level representation in leaf data, demonstrating the hierarchical transformation of features from lower-level to higher-level abstraction, corresponding to species classes. We show that these findings fit with the hierarchical botanical definitions of leaf characters. Through these findings, we gained insights into the design of new hybrid feature extraction models which are able to further improve the discriminative power of plant classification systems. The source code and models are available at: https://github.com/cs-chan/Deep-Plant.","Sue Han Lee and Chee Seng Chan and Simon Joseph Mayo and Paolo Remagnino",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Spiking signal processing: Principle and applications in control system","This paper introduces an innovative interpretation of spiking signal processing (SSP) and proposes applications in control system. Taking the firing rate as the coding principle employed by biological neurons, we have transformed continuous signals in geometric spiking series evolving over time. New fundamental SSP equations are presented and compared to classical digital signal processing (DSP). Application to the linear system analysis and control drive using spiking transformation is finally presented. This paper launches the theoretical SSP basis to investigate more deeply the geometric neural networks with learning ability.","L.M. Grzesiak and V. Meganck",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Affine-transformation and 2D-projection invariant k-NN classification of handwritten characters via a new matching measure","Pattern recognition based on matching remains important because it is a fundamental technique, it does not require a learning process, and the result of matching provides intuitive and geometrical information. Wakahara et al. proposed global affine transformation (GAT) correlation matching, which can compensate for affine transformations imposed on a pattern. GAT correlation matching with an acceleration method and a new matching measure, called the nearest-neighbor distance of equi-gradient direction (NNDEGD), achieved high performance in experiments using the MNIST database. The GAT matching measure was extended to a global projection transformation (GPT) matching measure to allow deformation by 2D projection transformations. The purpose of this paper is threefold. First, we develop an acceleration method for GPT correlation matching. Second, in order to improve recognition performance, we apply the curvature of edges in strokes to the matching measure. Curvature is often used as a feature of characters. However, in this paper, we use it as a weight in the NNDEGD. Third, to investigate the performance of the proposed methods, we apply them to image matching and recognition from the MNIST and the IPTP databases for k-nearest neighbors (k-NN). In the experiment with the MNIST database, the GPT correlation matching with the curvature-weighted NNDEGD matching measure achieves the lowest error rate of 0.30% among k-NN based methods.","Yukihiko Yamashita and Toru Wakahara",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A detection framework for semantic code clones and obfuscated code","Code obfuscation is a staple tool in malware creation where code fragments are altered substantially to make them appear different from the original, while keeping the semantics unaffected. A majority of the obfuscated code detection methods use program structure as a signature for detection of unknown codes. They usually ignore the most important feature, which is the semantics of the code, to match two code fragments or programs for obfuscation. Obfuscated code detection is a special case of the semantic code clone detection task. We propose a detection framework for detecting both code obfuscation and clone using machine learning. We use features extracted from Java bytecode dependency graphs (BDG), program dependency graphs (PDG) and abstract syntax trees (AST). BDGs and PDGs are two representations of the semantics or meaning of a Java program. ASTs capture the structural aspects of a program. We use several publicly available code clone and obfuscated code datasets to validate the effectiveness of our framework. We use different assessment parameters to evaluate the detection quality of our proposed model. Experimental results are excellent when compared with contemporary obfuscated code and code clone detectors. Interestingly, we achieve 100% success in detecting obfuscated code based on recall, precision, and F1-Score. When we compare our method with other methods for all of obfuscations types, viz, contraction, expansion, loop transformation and renaming, our model appears to be the winner. In case of clone detection our model achieve very high detection accuracy in comparison to other similar detectors.","Abdullah Sheneamer and Swarup Roy and Jugal Kalita",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Enhancing developer recommendation with supplementary information via mining historical commits","Given a software issue request, one important activity is to recommend suitable developers to resolve it. A number of approaches have been proposed on developer recommendation. These developer recommendation techniques tend to recommend experienced developers, i.e., the more experienced a developer is, the more possible he/she is recommended. However, if the experienced developers are hectic, the junior developers may be employed to finish the incoming issue. But they may have difficulty in these tasks for lack of development experience. In this article, we propose an approach, EDR_SI, to enhance developer recommendation by considering their expertise and developing habits. Furthermore, EDR_SI also provides the personalized supplementary information for developers to use, such as personalized source code files, developer network and source-code change history. An empirical study on five open source subjects is conducted to evaluate the effectiveness of EDR_SI. In our study, EDR_SI is also compared with the state-of-art developer recommendation techniques, iMacPro, Location and ABA-Time-tf-idf, to evaluate the effectiveness of developer recommendation, and the results show that EDR_SI can not only improve the accuracy of developer recommendation, but also effectively provide useful supplementary information for them to use when they implement the incoming issue requests.","Xiaobing Sun and Hui Yang and Xin Xia and Bin Li",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Towards a Severity and Activity based Assessment of Code Smells","Code smells are the structural weaknesses which reside in a software system. They evolve negatively over time reducing the system quality i.e., maintainability, understandability etc. Therefore, they should be detected and prioritized based on criticality in order to be refactored. Most of the existing approaches are based on severity score, but little works have been done to include the information from changes history. Thus, we introduce a Harmfulness Model that integrates both information: severity and changes history (i.e., code smells activity). This study characterizes a god class activity based on its severity and change frequency of the JHotDraw open source system. The result indicates that there are two main activities of god class that can be assessed as active and passive smells. In fact, an active god class can be differentiated as strong, stable, and ameliorate smells while a passive god class has one type called dormant. Besides that, from severity and activity information, the model can compute the harmfulness score and also indicate the degree of harmfulness level. The harmfulness level may be useful to improve change likelihood estimation and refactoring candidates prioritization.","Harris Kristanto Husien and Muhammad Firdaus Harun and Horst Lichter",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"First Results with HIJING++ in High-Energy Heavy-Ion Collisions","First calculated results with the new HIJING++ are presented for identified hadron production in high-energy heavy ion collisions. The recently developed HIJING++ version is based on the latest version of PYTHIA8 and contains all the nuclear effects has been included in the HIJING2.552, which will be improved by a new version of the shadowing parametrization and jet quenching module. Here, we summarize the major changes of the new program code beside the comparison between experimental data for some specific high-energy nucleus-nucleus collisions.","Gergely Gábor Barnaföldi and Gábor Bíró and Miklos Gyulassy and Szilveszter Miklós Haranozó and Péter Lévai and Guoyang Ma and Gábor Papp and Xin-Nian Wang and Ben-Wei Zhang",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"DNAD, a simple tool for automatic differentiation of Fortran codes using dual numbers","DNAD (dual number automatic differentiation) is a simple, general-purpose tool to automatically differentiate Fortran codes written in modern Fortran (F90/ 95/2003) or legacy codes written in previous version of the Fortran language. It implements the forward mode of automatic differentiation using the arithmetic of dual numbers and the operator overloading feature of F90/ 95/2003. Very minimum changes of the source codes are needed to compute the first derivatives of Fortran programs. The advantages of DNAD in comparison to other existing similar computer codes are its programming simplicity, extensibility, and computational efficiency. Specifically, DNAD is more accurate and efficient than the popular complex-step approximation. Several examples are used to demonstrate its applications and advantages.
Program summary
Program title: DNAD Catalogue identifier: AEOS_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEOS_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 3922 No. of bytes in distributed program, including test data, etc.: 18275 Distribution format: tar.gz Programming language: Fortran 90/95/2003. Computer: All computers with a modern FORTRAN compiler. Operating system: All platforms with a modern FORTRAN compiler. Classification: 4.12, 6.2. Nature of problem: Derivatives of outputs with respect to inputs of a Fortran code are often needed in physics, chemistry, and engineering. The author of the analysis code may no longer be available and the user may not have a deep knowledge of the code. Thus a simple tool is necessary to automatically differentiate the code with very minimum change to the source codes. This can be achieved using dual number arithmetic and operator overloading. Solution method: A new data type is defined with the first scalar component holding the function value and the second array component holding the first derivatives. All the basic operations and functions are overloaded with the new definitions according to dual number arithmetic. To differentiate an existing code, all real numbers should be replaced with this new data type and the input/output of the code should also be modified accordingly. Running time: For each additional independent variable, DNAD takes less time than the running time of the original analysis code. However, the actual running time depends on the compiler, the computer, and the operations involved in the code to be differentiated.","Wenbin Yu and Maxwell Blair",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Psychometric properties of the Motivational Interviewing Treatment Integrity coding system 4.2 with jail inmates","Motivational Interviewing (MI) is an evidence-based approach shown to be helpful for a variety of behaviors across many populations. Treatment fidelity is an important tool for understanding how and with whom MI may be most helpful. The Motivational Interviewing Treatment Integrity coding system was recently updated to incorporate new developments in the research and theory of MI, including the relational and technical hypotheses of MI (MITI 4.2). To date, no studies have examined the MITI 4.2 with forensic populations. In this project, twenty-two brief MI interventions with jail inmates were evaluated to test the reliability of the MITI 4.2. Validity of the instrument was explored using regression models to examine the associations between global scores (Empathy, Partnership, Cultivating Change Talk and Softening Sustain Talk) and outcomes. Reliability of this coding system with these data was strong. We found that therapists had lower ratings of Empathy with participants who had more extensive criminal histories. Both Relational and Technical global scores were associated with criminal histories as well as post-intervention ratings of motivation to decrease drug use. Findings indicate that the MITI 4.2 was reliable for coding sessions with jail inmates. Additionally, results provided information related to the relational and technical hypotheses of MI. Future studies can use the MITI 4.2 to better understand the mechanisms behind how MI works with this high-risk group.","Mandy D. Owens and Lauren N. Rowell and Theresa Moyers",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The MESSAGEix Integrated Assessment Model and the ix modeling platform (ixmp): An open framework for integrated and cross-cutting analysis of energy, climate, the environment, and sustainable development","The MESSAGE Integrated Assessment Model (IAM) developed by IIASA has been a central tool of energy-environment-economy systems analysis in the global scientific and policy arena. It played a major role in the Assessment Reports of the Intergovernmental Panel on Climate Change (IPCC); it provided marker scenarios of the Representative Concentration Pathways (RCPs) and the Shared Socio-Economic Pathways (SSPs); and it underpinned the analysis of the Global Energy Assessment (GEA). Alas, to provide relevant analysis for current and future challenges, numerical models of human and earth systems need to support higher spatial and temporal resolution, facilitate integration of data sources and methodologies across disciplines, and become open and transparent regarding the underlying data, methods, and the scientific workflow. In this manuscript, we present the building blocks of a new framework for an integrated assessment modeling platform; the “ecosystem” comprises: i) an open-source GAMS implementation of the MESSAGE energy++ system model integrated with the MACRO economic model; ii) a Java/database back-end for version-controlled data management, iii) interfaces for the scientific programming languages Python & R for efficient input data and results processing workflows; and iv) a web-browser-based user interface for model/scenario management and intuitive “drag-and-drop” visualization of results. The framework aims to facilitate the highest level of openness for scientific analysis, bridging the need for transparency with efficient data processing and powerful numerical solvers. The platform is geared towards easy integration of data sources and models across disciplines, spatial scales and temporal disaggregation levels. All tools apply best-practice in collaborative software development, and comprehensive documentation of all building blocks and scripts is generated directly from the GAMS equations and the Java/Python/R source code.","Daniel Huppmann and Matthew Gidden and Oliver Fricko and Peter Kolp and Clara Orthofer and Michael Pimmer and Nikolay Kushin and Adriano Vinca and Alessio Mastrucci and Keywan Riahi and Volker Krey",2019,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Version 4.0 of code Java for 3D simulation of the CCA model","This paper presents a new version Java code for the three-dimensional simulation of Cluster–Cluster Aggregation (CCA) model to replace the previous version. Many redundant traverses of clusters-list in the program were totally avoided, so that the consumed simulation time is significantly reduced. In order to show the aggregation process in a more intuitive way, we have labeled different clusters with varied colors. Besides, a new function is added for outputting the particle’s coordinates of aggregates in file to benefit coupling our model with other models.
New version program summary
Program Title: CCA v04 Program Files doi:http://dx.doi.org10.17632/zwz37tvjny.1 Licensing provisions: Apache-2.0 Programming language: Java Journal reference of previous version: Computer Physics Communications 207 (2016) 547–548 Does the new version supersede the previous version?: Yes Nature of problem: The previous program for CCA model can be optimized for yielding better operation efficiency and visualization effects. Besides, it can be extended for coupling with other models by adding some new functions. Solution method: Some redundant traverses of the clusters-list for updating an important variable were removed technically. Furthermore, displaying different clusters with distinct colors can benefit to the effectiveness of visualization. We have also introduced a new function for outputting the aggregated clusters, so that this model can be easily coupled with other models. Reasons for the new version: 1. In the previous versions [1–3], the global variable of Dmax, which means the maximum diffusion coefficient of clusters, is supposed to be calculated when aggregation occurs. In fact, this variable is critical to determine whether the currently selected cluster carries out diffusion or not. In the previous version, we had to traverse the clusters-list to update Dmax, even though it may keep unchanged after a round of aggregation. But, we have explored that it is not necessary to traverse all clusters, if we compare the previous variable of Dmax with the diffusion coefficient of the new cluster, which is formed by aggregating two clusters. In the case of the new one is larger, Dmax will be updated to it; otherwise, Dmax keep unchanged. Obviously, removing redundant traverses can enhance algorithm efficiency, and the time complexity of the algorithm is closely related to the loop in it. As a result, the new version of code can greatly cut down the simulation time, in contrast to the previous codes. 2. The old versions paint all particles with same color, which does not help to identify different clusters. In order to show the visualization of the 3D aggregate process in different states intuitively, we introduce labeling different clusters with different colors. Consequently, the particles belong to one cluster are painted with the same color. 3. A new function is added to record intermediate or final state of clusters so that it can be used for further research when using other relevant models. Summary of revisions: 1. Reduce unnecessary traverses of the clusters-list for speeding up simulation. 2. Paint clusters with different colors to pursuit better visualization effects. 3. Add a new function of exporting the coordinates of clusters to a text file for benefiting other models. Additional comments: To test the optimization effects offered by the new version of code, we have carried out a comparison experimentto measure the simulation time with different initial particles, by using the new and the previous version 3.0 of code. In the experiments, we set concentration C to 0.01, diffusion exponent γ to 0, sticking probability exponent σ to 1, the sticking probability P1 to 0.1, absolute temperature T to 298 K, and the side length L of cube was respectively set to 30, 40, 50, 60, 70 and 80 to yield different initial particles, as the number of particles N can be calculated by the formula N=C∗L3 [1]. To avoid the influences of hardware and software, we use Monte Carlo steps (MCS) as our simulation time [4–6]. As shown in Fig. 1, the slope coefficient caused by this new version of code is about 1.3813, it is close to line complexity, as for the previous version, it is 1.5703, larger than the new one. It is obvious the new one has a faster running speed than the previous one. More importantly, the reduction of simulation time becomes more obvious when the number of simulated particles is getting larger. The changed visualization of our program is shownin Fig. 2. We can clearly see how much clusters in simulation system, and how they distribute. And the button “save” marked red in Fig. 2 can help realize our new function by exporting the particle coordinates to a text file, which also store the related initial conditions. Acknowledgments This work was partially supported by “National Natural Science Foundation of China (Nos. 41271292, 61303038)”. References[1]C. Li, H. Xiong, 3D simulation of the Cluster-Cluster aggregation model, Computer Physics Communications 185 (12) (2014) 3424–3429.[2]K. Zhang, H. Xiong, C. Li, A new version of code Java for 3D simulation of the CCA model, Computer Physics Communications 204 (2016) 214–215.[3]K. Zhang, J. Zuo, Y. Dou, C. Li, H. Xiong, Version 3.0 of code Java for 3D simulation of the CCA model, Computer Physics Communications 207 (2016) 547–548.[4]T. Vicsek, Dynamic scaling for aggregation of clusters, Physical Review Letters 52 (19) (1984) 1669–1672.[5]P. A. Netz, D. Samios, The study of network formation as a cluster-cluster diffusion-limited aggregation process: Modelling of the curing of an epoxy-resin using the Monte Carlo method, Macromolecular Theory Simulations 3 (3) (1994) 607–621.[6]H. Xiong, H. Li, W. Chen, L. Wu, Data structure for on-lattice cluster-cluster aggregation model performance optimization, Computer Physics Communications 185 (3) (2014) 836–840.","Linyu Fan and Jianwei Liao and Junsen Zuo and Kebo Zhang and Chao Li and Hailing Xiong",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Deconstructing the symbol digit modalities test in multiple sclerosis: The role of memory","Background
The Symbol Digit Modalities Test (SDMT) is a sensitive measure of impaired cognition in people with MS. While the SDMT is primarily considered a test of information processing speed, other components such as visual scanning and oral-motor ability have also been linked to performance. The objective of this study was to determine the role of memory in the performance of the SDMT.
Methods
Two version of a modified computerized SDMT (c-SDMT) were employed, a fixed and a variable. For each group 50 MS and 33 healthy control (HC) participants were recruited. In the fixed c-SDMT, the symbol-digit code is kept constant for the entire test whereas in the variable version, it changes eight times. Unlike the traditional SDMT which records the correct number of responses, the c-SDMT presented here measures the mean response time (in seconds) for the eight trials.
Results
MS participants were slower than HC on the fixed (p < 0.001) and variable (p = 0.005) c-SDMT. Trend analysis showed performance improvement on the fixed, but not on the variable c-SDMT in both MS and HC groups. Furthermore, immediate visual memory recall was associated with the fixed (β = −0.299, p = 0.017), but not variable (B = −0.057, p = 0.260) c-SDMT. Immediate verbal memory was not associated with either versions of the c-SDMT.
Conclusions
Given that the fixed and variable c-SDMTs are identical in every way apart from the fixity of the code, the ability of participants to speed up responses over the course of the fixed version only points to the contribution of incidental visual memory in test performance.","Viral P. Patel and Lisa A.S. Walker and Anthony Feinstein",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Infographic restitution of the historic centre of the Spanish town of Oviedo","Our historic town centres possess an evocative quality that has recently led to considering these town centres a subject of scientific research, overtaking, if not contradicting, the romantic vision by which they obtained importance in the Western world from the late 18th century. Thus, the present work is framed in the existing and ultimately consolidated method of European universities, which analyse, with an increasing scientific rigour from many different perspectives, the knowledge of the abundant and rich historic town centres that geographically define our living environment. This study focuses on the reconstruction of the urban shape of the town of Oviedo in a series of stages that are marked by the cartographic data collected during the time of the study and that involve substantial changes to the urban fabric of the town. The study attempts to be a historical analysis of urban transformation. This research established a systematic study of earlier times when the urban form offered space patterns different from existing patterns. Using the current structure of the historic centre of Oviedo and the analysis of its pathologies throughout history as a starting point, this study considers the possibility of a methodical study of the past. This project presents the coexistence of parallel textual and visual accounts, which can be read together, as a new methodology to understand the spatial formation and transformation of urban landscapes.","Marta Alonso-Rodriguez and Antonio Alvaro-Tordesillas and Eduardo Carazo-Lefort",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"A coding tool for examining the substance of teacher professional learning and change with example cases from middle school science lesson study","Although lesson study is increasingly adopted in the United States (U.S.), the impact of lesson study on teacher learning is uncertain. This study presents a theoretically grounded set of codes to systematically document the various aspects of teacher learning and change (knowledge and beliefs, professional learning community, resources) in lesson study across contexts. To present examples of the codes in use, a subset of codes related to change in teacher knowledge and beliefs were applied to analyze teachers' professional discourse in three middle school science lesson study teams.","Christine Lee Bae and Kathryn N. Hayes and Jeffery Seitz and Dawn O'Connor and Rachelle DiStefano",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"A modular foreign function interface","Foreign function interfaces are typically organised monolithically, tying together the specification of each foreign function with the mechanism used to make the function available in the host language. This leads to inflexible systems, where switching from one binding mechanism to another (say from dynamic binding to static code generation) often requires changing tools and rewriting large portions of code. We show that ML-style module systems support exactly the kind of abstraction needed to separate these two aspects of a foreign function binding, leading to declarative foreign function bindings that support switching between a wide variety of binding mechanisms — static and dynamic, synchronous and asynchronous, etc. — with no changes to the function specifications. Note. This is a revised and expanded version of an earlier paper, Declarative Foreign Function Binding Through Generic Programming[19]. This paper brings a greater focus on modularity, and adds new sections on error handling, and on the practicality of the approach we describe.","Jeremy Yallop and David Sheets and Anil Madhavapeddy",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Multiple feature fusion for unconstrained palm print authentication","Over the last decade, palm print recognition has emerged as the strongest technology for human authentication in many aspects. To carry out an effective recognition, this paper presents a feature level fusion of block-wise scale invariant feature transform and texture code co-occurrence matrix based features. Initially, an attempt to access the quality of extracted region of interest image is made. This is followed by application of fractional differential mask resulting in improvement of textural detail. In order to select the most discriminate palm features, a feature transformation algorithm inspired by subspace learning is employed. It led to reduction in computation time and feature dimensions, along with higher level of performance. A trained support vector machine utilizes the selected features to determine whether image belongs to genuine or imposter class. Comparative experimental analysis described in this paper indicates customarily outperforming results than competing methods and validate efficacy of proposed approach.","Gaurav Jaswal and Amit Kaul and Ravinder Nath",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"TFmix: A high-precision implementation of the finite-temperature Thomas–Fermi model for a mixture of atoms","In this work we present a TFmix code intended for numerical calculation of the thermal part of electronic thermodynamic properties of a mixture of elements by the finite-temperature Thomas–Fermi model. The code is based on analytical models for both first and second derivatives of Helmholtz thermodynamic potential. All numerical calculations are made within a controlled high accuracy: tests for thermodynamic consistency give at least 11 coinciding decimal digits. The code calculates thermodynamic functions on a regular grid of isotherms and isochores; at each grid point some extensive parameters and the number of free electrons are output both for the whole mixture and for each component. Other extensive or intensive thermodynamic properties, including pressure, entropy, isochoric and isobaric heat capacities, isothermal and adiabatic sound velocities can be easily calculated from the information available at each grid point. Several unit systems are provided for convenience. A cross-platform graphical user interface is developed to simplify the use of the code.
Program summary
Program Title: TFmix, version 1.0 Program Files doi:http://dx.doi.org/10.17632/mc3vj77jfn.1 Licensing provisions: GPLv3 Programming language: C, Python Nature of problem: Any substance consists of elements so its equation of state contains a contribution of electronic gas. Thermodynamics of the electronic gas in a mixture of ions and electrons has been studied in many approaches. Thermodynamic properties of a uniform ideal electron gas can be calculated using the well-known analytical model of Fermi-gas. On the other hand, models of electron gas which take into account interaction effects are quite complicated and require sophisticated computational techniques. Even a simplified semiclassical Thomas–Fermi model is based upon the numerical solution of a non-linear boundary problem. Two main issues of the Thomas–Fermi model restrict its usage: uncontrolled accuracy of calculated thermodynamic functions (especially second derivatives of a thermodynamic potential), and unphysical behavior of the model at relatively low temperatures. Solution method: Each atom in the mixture is surrounded by a spherical cell. The radii of the cells are fitted to equalize the chemical potentials of all atoms. A guaranteed accuracy of first derivatives of the thermodynamic potential is provided by a transformation of integrals over the Thomas–Fermi potential to a system of differential equations. One of equations in the system is the Thomas–Fermi equation. Second derivatives of the thermodynamic potential are calculated similarly with the only difference that a corresponding derivative of the Thomas–Fermi equation is used in the system of differential equation. To avoid the unphysical behavior of the Thomas–Fermi model at low temperatures we extract a thermal contribution to thermodynamic properties which vanishes at zero temperature. To eliminate the error which appears from the subtraction of the cold part at low temperatures we use asymptotic expressions for thermodynamic functions and the Thomas–Fermi equation. The code calculates regular tables of thermodynamic functions on a grid of isotherms and isochores including second derivatives of a thermodynamic potential. This information is necessary for astrophysical applications, for continuum mechanics simulation of processes in plasma and for the creation of wide-range equations of state. A graphical user interface is provided with the code and allows to specify input parameters, to perform calculations and to plot the results. Additional comments including restrictions and unusual features: GSL library version 1.16 or 2.x is required for compilation; matplotlib Python library is required to run the graphical user interface.","O.P. Shemyakin and P.R. Levashov and P.A. Krasnova",2019,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Do android developers neglect error handling? a maintenance-Centric study on the relationship between android abstractions and uncaught exceptions","All the mainstream programming languages in widespread use for mobile app development provide error handling mechanisms to support the implementation of robust apps. Android apps, in particular, are usually written in the Java programming language. Java includes an exception handling mechanism that allows programs to signal the occurrence of errors by throwing exceptions and to handle these exceptions by catching them. All the Android-specific abstractions, such as activities and asynctasks, can throw exceptions when errors occur. When an app catches the exceptions that it or the libraries upon which it depends throw, it can resume its activity or, at least, fail in a graceful way. On the other hand, uncaught exceptions can lead an app to crash, particularly if they occur within the main thread. Previous work has shown that, in real Android apps available at the Play Store, uncaught exceptions thrown by Android-specific abstractions often cause these apps to fail. This paper presents an empirical study on the relationship between the usage of Android abstractions and uncaught exceptions. Our approach is quantitative and maintenance-centric. We analyzed changes to both normal and exception handling code in 112 versions extracted from 16 software projects covering a number of domains, amounting to more than 3 million LOC. Change impact analysis and exception flow analysis were performed on those versions of the projects. The main finding of this study is that, during the evolution of the analyzed apps, an increase in the use of Android abstractions exhibits a positive and statistically significant correlation with the number of uncaught exception flows. Since uncaught exceptions cause apps to crash, this result suggests that these apps are becoming potentially less robust as a consequence of exception handling misuse. Analysis of multiple versions of these apps revealed that Android developers usually employ abstractions that may throw exceptions without adding the appropriate handlers for these exceptions. This study highlights the need for better testing and verification tools with a focus on exception handling code and for a change of culture in Android development or, at least, in the design of its APIs.","Juliana Oliveira and Deise Borges and Thaisa Silva and Nelio Cacho and Fernando Castor",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Self-learning-based post-processing for image/video deblocking via sparse representation","Blocking artifact, characterized by visually noticeable changes in pixel values along block boundaries, is a common problem in block-based image/video compression, especially at low bitrate coding. Various post-processing techniques have been proposed to reduce blocking artifacts, but they usually introduce excessive blurring or ringing effects. This paper proposes a self-learning-based post-processing framework for image/video deblocking by properly formulating deblocking as an MCA (morphological component analysis)-based image decomposition problem via sparse representation. Without the need of any prior knowledge (e.g., the positions where blocking artifacts occur, the algorithm used for compression, or the characteristics of image to be processed) about the blocking artifacts to be removed, the proposed framework can automatically learn two dictionaries for decomposing an input decoded image into its “blocking component” and “non-blocking component.” More specifically, the proposed method first decomposes a frame into the low-frequency and high-frequency parts by applying BM3D (block-matching and 3D filtering) algorithm. The high-frequency part is then decomposed into a blocking component and a non-blocking component by performing dictionary learning and sparse coding based on MCA. As a result, the blocking component can be removed from the image/video frame successfully while preserving most original visual details. Experimental results demonstrate the efficacy of the proposed algorithm.","Chia-Hung Yeh and Li-Wei Kang and Yi-Wen Chiou and Chia-Wen Lin and Shu-Jhen Fan Jiang",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Linear and kernel methods for multivariate change detection","The iteratively reweighted multivariate alteration detection (IR-MAD) algorithm may be used both for unsupervised change detection in multi- and hyperspectral remote sensing imagery and for automatic radiometric normalization of multitemporal image sequences. Principal components analysis (PCA), as well as maximum autocorrelation factor (MAF) and minimum noise fraction (MNF) analyses of IR-MAD images, both linear and kernel-based (nonlinear), may further enhance change signals relative to no-change background. IDL (Interactive Data Language) implementations of IR-MAD, automatic radiometric normalization, and kernel PCA/MAF/MNF transformations are presented that function as transparent and fully integrated extensions of the ENVI remote sensing image analysis environment. The train/test approach to kernel PCA is evaluated against a Hebbian learning procedure. Matlab code is also available that allows fast data exploration and experimentation with smaller datasets. New, multiresolution versions of IR-MAD that accelerate convergence and that further reduce no-change background noise are introduced. Computationally expensive matrix diagonalization and kernel image projections are programmed to run on massively parallel CUDA-enabled graphics processors, when available, giving an order of magnitude enhancement in computational speed. The software is available from the authors' Web sites.","Morton J. Canty and Allan A. Nielsen",2012,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Auditory predictions shape the neural responses to stimulus repetition and sensory change","Perception is a highly active process relying on the continuous formulation of predictive inferences using short-term sensory memory templates, which are recursively adjusted based on new input. According to this idea, earlier studies have shown that novel stimuli preceded by a higher number of repetitions yield greater novelty responses, indexed by larger mismatch negativity (MMN). However, it is not clear whether this MMN memory trace effect is driven by more adapted responses to prior stimulation or rather by a heightened processing of the unexpected deviant, and only few studies have so far attempted to characterize the functional neuroanatomy of these effects. Here we implemented a modified version of the auditory frequency oddball paradigm that enables modeling the responses to both repeated standard and deviant stimuli. Fifteen subjects underwent functional magnetic resonance imaging (fMRI) while their attention was diverted from auditory stimulation. We found that deviants with longer stimulus history of standard repetitions yielded a more robust and widespread activation in the bilateral auditory cortex. Standard tones repetition yielded a pattern of response entangling both suppression and enhancement effects depending on the predictability of upcoming stimuli. We also observed that regularity encoding and deviance detection mapped onto spatially segregated cortical subfields. Our data provide a better understanding of the neural representations underlying auditory repetition and deviance detection effects, and further support that perception operates through the principles of Bayesian predictive coding.","Raffaele Cacciaglia and Jordi Costa-Faidella and Katarzyna Zarnowiec and Sabine Grimm and Carles Escera",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A new Fortran 90 program to compute regular and irregular associated Legendre functions (new version announcement)","This is a revised and updated version of a modern Fortran 90 code to compute the regular Plm(x) and irregular Qlm(x) associated Legendre functions for all x∈(−1,+1) (on the cut) and |x|>1 and integer degree (l) and order (m). The necessity to revise the code comes as a consequence of some comments of Prof. James Bremer of the UC//Davis Mathematics Department, who discovered that there were errors in the code for large integer degree and order for the normalized regular Legendre functions on the cut.
New version program summary
Program Title: Associated Legendre Functions Program Files doi:http://dx.doi.org/10.17632/29j2mvwgwt.1 Licensing provisions: GPLv2 Programming language: Fortran 90 Does the new version supersede the previous version?: Yes Journal reference of previous version: Comput. Phys. Commun. 181 (2010) 2091 Reason for new version: The necessity to revise the code comes as a consequence of some comments of Prof. James Bremer of the UC//Davis Mathematics Department, who discovered that there were errors in the code for large integer degree and order for the normalized regular Legendre functions on the cut. This occurred because renormalization of the un-normalized functions involved the manipulation of factorials of large integer value. The problem was solved by a direct computation of the normalized functions from a revised recursion formula. This completely avoids the manipulation of the factorials that were present in the first version. In re-examining the code we also noticed that there were cases for large degree, order and argument that led to overflows causing the results to display infinities or NaN when printed out, without crashing. There does not appear to be any good approach, other than resorting to extended precision arithmetic, to cure this pathology. Since it was not our original intention to provide such a code, all we can do is to point out that this can happen and that the user should take care not to blindly accept the results of the computation for large degree and order. We have not found a general approach to predict in advance when this will occur in this three dimensional parameter space. Summary of revisions: None of the basic algorithms has been changed. We have eliminated the computation of factorials for large integer values and now calculate the normalized regular Legendre functions on the cut (-1,+1) directly from the recursion relation for those functions rather than computing the unnormalized functions and then normalizing after the computation. This required some modifications of the original Fortran subroutines as well as the addition of a new module. Nature of problem: Compute the regular and irregular associated Legendre functions for integer values of the degree and order and for all real arguments. The computation of the interaction of two electrons, 1∕|r1−r2|, in prolate spheroidal coordinates is used as one example where these functions are required for all values of the argument and we are able to easily compare the series expansion in associated Legendre functions and the exact value. Solution method: The code evaluates the regular and irregular associated Legendre functions using forward recursion when |x|<1 starting the recursion with the analytically known values of the first two members of the sequence. For values of the argument |x|<1, the upward recursion over the degree for the regular functions is numerically stable. For the irregular functions, backward recursion must be applied and a suitable method of starting the recursion is required. The program has two options; a modified version of Miller’s algorithm and the use of the wronskian relation between the regular and irregular functions, which was the method considered in [1]. Both approaches require the computation of a continued fraction to begin the recursion. The wronskian method (which can also be described as a modified Miller’s method) is a convenient method of computations when both the regular and irregular functions are needed. References[1]A. Gil, J. Segura, A code to evaluate prolate and oblate spheroidal harmonics, Comput. Phys. Commun. 108 (1998) 267–278.","Barry I. Schneider and Javier Segura and Amparo Gil and Xiaoxu Guan and Klaus Bartschat",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Extensions to hybrid code networks for FAIR dialog dataset","Goal-oriented dialog systems require a different approach from chit-chat conversational systems in that they should perform various subtasks as well as continue the conversation itself. Since these systems typically interact with an external knowledge base that changes over time, it is desirable to incorporate domain knowledge to deal with such changes, yet with minimum human effort. This paper presents an extended version of the Hybrid Code Network (HCN) developed for the Facebook AI research (FAIR) dialog dataset used in the Sixth Dialog System Technology Challenge (DSTC6). Compared to the original HCN, the system was more adaptable to changes in the knowledge base due to the modules that are extended to be learned from data. Using the proposed learning scheme with fairly elementary domain-specific rules, the proposed model achieved 100% accuracy in all test datasets.","Jiyeon Ham and Soohyun Lim and Kyeng-Hun Lee and Kee-Eung Kim",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"An aggregated coupling measure for the analysis of object-oriented software systems","Coupling is a fundamental property of software systems which is strongly connected with the quality of software design and has high impact on program understanding. The coupling between software components influences software maintenance and evolution as well. In order to ease the maintenance and evolution processes it is essential to estimate the impact of changes made in the software system, coupling indicating such a possible impact. This paper introduces a new aggregated coupling measurement which captures both the structural and the conceptual characteristics of coupling between the software components. The proposed measure combines the textual information contained in the source code with the structural relationships between software components. We conduct several experiments which underline that the proposed aggregated coupling measure reveals new characteristics of coupling and is also effective for change impact analysis.","Istvan Gergely Czibula and Gabriela Czibula and Diana-Lucia Miholca and Zsuzsanna Onet-Marian",2019,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Securing templates in a face recognition system using Error-Correcting Output Code and chaos theory","In biometric cryptosystems, biometric data is combined with cryptography algorithms to generate secure templates. In these systems, creating protected templates with both high discriminability and high security is a challenging issue. To address this issue, this paper proposes a new face cryptosystem based on binarization transformation, chaos feature permutation and fuzzy commitment scheme. To enhance discriminability, real-valued templates are converted into their binary versions using a new discriminant binarization transformation based on Error-Correcting Output Code. Then, the chaos feature permutation is used to increase the security and privacy of binary templates, and also to protect the fuzzy commitment scheme against cross-matching attacks. The proposed scheme is evaluated on three well-known face databases, i.e. CMU PIE, FEI, and Extended Yale B. Experimental results show that the proposed method improves discriminability, as well as privacy and security of the system, compared to the existing face template protection algorithms.","Sara Nazari and Mohammad-Shahram Moin and Hamidreza Rashidy Kanan",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Quantitative structural determination of active sites from in situ and operando XANES spectra: From standard ab initio simulations to chemometric and machine learning approaches","In the last decade the appearance of progressively more sophisticated codes, together with the increased computational capabilities, has made XANES a spectroscopic technique able to quantitatively confirm (or discard) a structural model, thus becoming a new fundamental diagnostic tool in catalysis, where the active species are often diluted metal centers supported on a matrix. After providing a brief historical introduction and the basic insights on the technique, in this review article, we provide a selection of four examples where operando XANES technique has been able to provide capital information on the structure of the active site in catalysts of industrial relevance: (i) Phillips catalyst for ethylene polymerization reaction; (ii) TS-1 catalyst for selective hydrogenation reactions; (iii) carbon supported Pd nanoparticles for hydrogenation reactions; (iv) Cu-CHA zeolite for NH3-assisted selective reduction of NOx and for partial oxidation of methane to methanol. The last example testifies how the multivariate curve resolution supported by the alternating least-squares algorithm applied to a high number of XANES spectra collected under operando conditions allows to quantitatively determine different species in mutual transformation. This approach is particularly powerful in the analysis of experiments where a large number of spectra has been collected, typical of time- or space-resolved experiments. Finally, machine learning approaches (both indirect and direct) have been applied to determine, from the XANES spectra, the structure of CO, CO2 and NO adsorbed on Ni2+ sites of activated CPO-27-Ni metal-organic framework.","Alexander A. Guda and Sergey A. Guda and Kirill A. Lomachenko and Mikhail A. Soldatov and Ilia A. Pankin and Alexander V. Soldatov and Luca Braglia and Aram L. Bugaev and Andrea Martini and Matteo Signorile and Elena Groppo and Alessandro Piovano and Elisa Borfecchia and Carlo Lamberti",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"PLUMED 2: New feathers for an old bird","Enhancing sampling and analyzing simulations are central issues in molecular simulation. Recently, we introduced PLUMED, an open-source plug-in that provides some of the most popular molecular dynamics (MD) codes with implementations of a variety of different enhanced sampling algorithms and collective variables (CVs). The rapid changes in this field, in particular new directions in enhanced sampling and dimensionality reduction together with new hardware, require a code that is more flexible and more efficient. We therefore present PLUMED 2 here—a complete rewrite of the code in an object-oriented programming language (C++). This new version introduces greater flexibility and greater modularity, which both extends its core capabilities and makes it far easier to add new methods and CVs. It also has a simpler interface with the MD engines and provides a single software library containing both tools and core facilities. Ultimately, the new code better serves the ever-growing community of users and contributors in coping with the new challenges arising in the field.
Program summary
Program title: PLUMED 2 Catalogue identifier: AEEE_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEEE_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Yes No. of lines in distributed program, including test data, etc.: 700646 No. of bytes in distributed program, including test data, etc.: 6618136 Distribution format: tar.gz Programming language: ANSI-C++. Computer: Any computer capable of running an executable produced by a C++ compiler. Operating system: Linux operating system, Unix OSs. Has the code been vectorized or parallelized?: Yes, parallelized using MPI. RAM: Depends on the number of atoms, the method chosen and the collective variables used. Classification: 3, 7.7, 23. Catalogue identifier of previous version: AEEE_v1_0. Journal reference of previous version: Comput. Phys. Comm. 180 (2009) 1961. External routines: GNU libmatheval, Lapack, Blas, MPI. Does the new version supersede the previous version?: This version supersedes the previous version for the most part. There are a small number of very specific situations where the previous version is better, due to performance or to non-ported features. We are actively working on porting these last few features into the new code. Nature of problem: Calculation of free-energy surfaces for molecular systems of interest in biology, chemistry and materials science, on the fly and a posteriori analysis of molecular dynamics trajectories using advanced collective variables. Solution method: Implementations of various collective variables and enhanced sampling techniques. Reasons for new version: The old version was difficult to maintain and its design was not as flexible as this new version. This lack of flexibility made it difficult to implement a number of novel methods that have emerged since the release of the original code. Summary of revisions: The new version of the code has a completely redesigned architecture, which allows for several important enhancements. This allows for a much simpler and robust input syntax and for improved performance. In addition, it provides several, more-complex collective variables which could not have been written using the previous implementation. Furthermore, the entire code is fully documented so it is easier to extend. Finally, the code is designed so that users can implement new variables directly in the input files and thus develop bespoke applications of these powerful algorithms. Unusual features: PLUMED 2 can be used either as a standalone program, e.g. for a posteriori analysis of trajectories, or as a library embedded in a molecular dynamics code (such as GROMACS, NAMD, Quantum ESPRESSO, and LAMMPS). Interfaces with these particular codes are provided in patches, which a simple script will insert into the underlying molecular dynamics codes source code files. For other molecular dynamics codes there is extensive documentation on how to add PLUMED in our manual. Additional comments: The distribution file contains a test suite, user and developer documentation and a collection of patches and utilities. Running time: Depends on the number of atoms, the method chosen and the collective variables used. The regression test suite provided takes approximately 1 min to run.","Gareth A. Tribello and Massimiliano Bonomi and Davide Branduardi and Carlo Camilloni and Giovanni Bussi",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A revised dosimetric characterization of 60Co BEBIG source: From single-source data to clinical dose distribution","Purpose
Although the dosimetric characterization of 60Co BEBIG source can be found in several literature studies, the data sets show major discrepancies and the lack of uncertainty analyses. This study tried to determine an accurate dosimetric data set for this source using Monte Carlo (MC) simulations along with detailed uncertainty analysis. To explore how different dosimetric data sets can make changes in practical situations, clinical dose distributions based on our results were compared with the dose distributions derived from Granero et al. and consensus data sets.
Methods and Materials
The MC simulations were performed with Monte Carlo N-Particle eXtended code (MCNPX) version 2.6.0 and the TG-43 parameters were estimated adhering to the American Association of Physicists in Medicine (AAPM) and European SocieTy for Radiotherapy and Oncology (ESTRO) 229 report. The dose rate distributions for single-source and two typical clinical cases, including one intracavitary and one interstitial, were calculated using an in-house code on the basis of the TG-43 formalism.
Results
The total uncertainties for water dose rate on source transverse axis at 1 cm and 5 cm, air kerma strength, and dose rate constant were evaluated to be 0.10%, 0.09%, 0.04%, and 0.11%, respectively. Meaningful differences were found for the interstitial case in which 22% of clinical target volume (CTV) showed differences from ±1% to ±10% or even larger.
Conclusions
The MC uncertainty was derived about 16 times smaller than the typical MC component stated in TG-138, partly because of large number of histories and partly because the spectra of 60Co and also its photons' attenuation coefficients are adequately accurate. The results showed that in the clinical situations, the applicator geometry and the superposition of single-source dose distributions can reduce the differences observed between several data sets.","Sara Abdollahi and Mahdieh Dayyani and Elie Hoseinian-Azghadi and Hashem Miri-Hakimabad and Laleh Rafat-Motavalli",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"PArthENoPE reloaded","We describe the main features of a new and updated version of the program PArthENoPE, which computes the abundances of light elements produced during Big Bang Nucleosynthesis. As the previous first release in 2008, the new one, PArthENoPE2.0, is publicly available and distributed from the code site, http://parthenope.na.infn.it. Apart from minor changes, which will be also detailed, the main improvements are as follows. The powerful, but not freely accessible, NAG routines have been substituted by ODEPACK libraries, without any significant loss in precision. Moreover, we have developed a Graphical User Interface (GUI) which allows a friendly use of the code and a simpler implementation of running for grids of input parameters.
New Version program summary
Program Title:PArthENoPE2.0 Program Files doi:http://dx.doi.org/10.17632/wvgr7d8yt9.1 Licensing provisions: GPLv3 Programming language: Fortran 77 and Python Supplementary material: User Manual available on the web page http://parthenope.na.infn.it Journal reference of previous version: Comput. Phys. Commun. 178 (2008) 956-971 Does the new version supersede the previous version?: Yes Reasons for the new version: Make the code more versatile and user friendly Summary of revisions: (1) Publicly available libraries (2) GUI for configuration Nature of problem: Computation of yields of light elements synthesized in the primordial universe Solution method: Livermore Solver for Ordinary Differential Equations (LSODE) for stiff and nonstiff systems","R. Consiglio and P.F. de Salas and G. Mangano and G. Miele and S. Pastor and O. Pisanti",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"TouchTerrain: A simple web-tool for creating 3D-printable topographic models","An open-source web-application, TouchTerrain, was developed to simplify the production of 3D-printable terrain models. Direct Digital Manufacturing (DDM) using 3D Printers can change how geoscientists, students, and stakeholders interact with 3D data, with the potential to improve geoscience communication and environmental literacy. No other manufacturing technology can convert digital data into tangible objects quickly at relatively low cost; however, the expertise necessary to produce a 3D-printed terrain model can be a substantial burden: knowledge of geographical information systems, computer aided design (CAD) software, and 3D printers may all be required. Furthermore, printing models larger than the build volume of a 3D printer can pose further technical hurdles. The TouchTerrain web-application simplifies DDM for elevation data by generating digital 3D models customized for a specific 3D printer's capabilities. The only required user input is the selection of a region-of-interest using the provided web-application with a Google Maps-style interface. Publically available digital elevation data is processed via the Google Earth Engine API. To allow the manufacture of 3D terrain models larger than a 3D printer's build volume the selected area can be split into multiple tiles without third-party software. This application significantly reduces the time and effort required for a non-expert like an educator to obtain 3D terrain models for use in class. The web application is deployed at http://touchterrain.geol.iastate.edu, while source code and installation instructions for a server and a stand-alone version are available at Github: https://github.com/ChHarding/TouchTerrain_for_CAGEO.","Franciszek J. Hasiuk and Chris Harding and Alex Raymond Renner and Eliot Winer",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Deep Learning-based Multimodal Control Interface for Human-Robot Collaboration","In human-robot collaborative manufacturing, industrial robot is required to dynamically change its pre-programmed tasks and collaborate with human operators at the same workstation. However, traditional industrial robot is controlled by pre-programmed control codes, which cannot support the emerging needs of human-robot collaboration. In response to the request, this research explored a deep learning-based multimodal robot control interface for human-robot collaboration. Three methods were integrated into the multimodal interface, including voice recognition, hand motion recognition, and body posture recognition. Deep learning was adopted as the algorithm for classification and recognition. Human-robot collaboration specific datasets were collected to support the deep learning algorithm. The result presented at the end of the paper shows the potential to adopt deep learning in human-robot collaboration systems.","Hongyi Liu and Tongtong Fang and Tianyu Zhou and Yuquan Wang and Lihui Wang",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Answering software evolution questions: An empirical evaluation","Context
Developers often need to find answers to questions regarding the evolution of a system when working on its code base. While their information needs require data analysis pertaining to different repository types, the source code repository has a pivotal role for program comprehension tasks. However, the coarse-grained nature of the data stored by commit-based software configuration management systems often makes it challenging for a developer to search for an answer.
Objective
We present Replay, an Eclipse plug-in that allows developers to explore the change history of a system by capturing the changes at a finer granularity level than commits, and by replaying the past changes chronologically inside the integrated development environment, with the source code at hand.
Method
We conducted a controlled experiment to empirically assess whether Replay outperforms a baseline (SVN client in Eclipse) on helping developers to answer common questions related to software evolution.
Results
The experiment shows that Replay leads to a decrease in completion time with respect to a set of software evolution comprehension tasks.
Conclusion
We conclude that there are benefits in using Replay over the state of the practice tools for answering questions that require fine-grained change information and those related to recent changes.","Lile Hattori and Marco D’Ambros and Michele Lanza and Mircea Lungu",2013,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR10, CR12","Rejeitado: CR10, CR12"
"Software bug prediction using weighted majority voting techniques","Mining software repositories is a growing research field where rich data available in the different development software repositories, are analyzed and cross-linked to uncover useful information. Bug prediction is one of the potential benefits that can be gained through mining software repositories. Predicting potential defects early as they are introduced to the version control system would definitely help in saving time and effort during testing or maintenance phases. In this paper, defect prediction models that uses ensemble classification techniques have been proposed. The proposed models have been applied using different sets of software metrics as attributes of the classification techniques and tested on datasets of different sizes. The results show that change metrics outperform static code metrics and the combined model of change and static code metrics. Ensembles tend to be more accurate than their base classifiers. Defect prediction models using change metrics and ensemble classifiers have revealed the best performance, especially when the datasets used have imbalanced class distribution.","Sammar Moustafa and Mustafa Y. ElNainay and Nagwa El Makky and Mohamed S. Abougabal",2018,"[""Science Direct""]","Aceito: CA6, CA5","Aceito: CA5, CA6"
"Medial Prefrontal Cortex Reduces Memory Interference by Modifying Hippocampal Encoding","Summary
The prefrontal cortex (PFC) is crucial for accurate memory performance when prior knowledge interferes with new learning, but the mechanisms that minimize proactive interference are unknown. To investigate these, we assessed the influence of medial PFC (mPFC) activity on spatial learning and hippocampal coding in a plus maze task that requires both structures. mPFC inactivation did not impair spatial learning or retrieval per se, but impaired the ability to follow changing spatial rules. mPFC and CA1 ensembles recorded simultaneously predicted goal choices and tracked changing rules; inactivating mPFC attenuated CA1 prospective coding. mPFC activity modified CA1 codes during learning, which in turn predicted how quickly rats adapted to subsequent rule changes. The results suggest that task rules signaled by the mPFC become incorporated into hippocampal representations and support prospective coding. By this mechanism, mPFC activity prevents interference by “teaching” the hippocampus to retrieve distinct representations of similar circumstances.","Kevin G. Guise and Matthew L. Shapiro",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical simulation of buffeting longitudinal wind forces on buildings","A developed numerical simulation technique for buffeting longitudinal wind forces on rectangular buildings is presented in this paper. This technique mainly aims to generate time histories with prescribed probabilistic characteristics for the longitudinal wind load turbulent component which is considered as a zero mean stationary multivariate one-dimensional stochastic process, then the instantaneous wind load time histories are obtained by adding each mean wind load component to the corresponding generated time history of the turbulent component. A simplified procedure, which takes the advantage of aerodynamic admittance function, is proposed to estimate the longitudinal wind load on buildings in the frequency domain. A very efficient simulation algorithm based on spectral representation method which depends on superposition of trigonometric functions with random phase angles is used to perform the transformation from the frequency domain to the time domain. A MATLAB function is coded to implement the proposed simulation technique. By testing the proposed technique statistically, it has been noted that there are good agreements between the temporal and target auto/cross-correlation functions of the simulated wind forces, and by testing it structurally, the generated instantaneous wind load time histories show a good ability in giving a reasonable dynamic response for the studied building.","Mohamed M. El-Heweity and Mohamed H. Abdelnaby and Elsayed M. Eshra",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Teaching concurrent and parallel programming by patterns: An interactive ICT approach","The use of programming patterns is considered to be a conceptual aid for programmers for developing understandable and testable concurrent and parallel code which is not only well built but also safe. By using programming patterns and their implementations as computer programs, difficult new concepts can be smoothly taught in lectures to students who before trying this teaching approach would have been reluctant to enroll on Parallel and Concurrent Programming courses. The approach presented in this paper consists in changing the traditional programming teaching and learning model to one where students are first introduced to syntactical constructs through selected introductory program code-patterns. In the theory lessons that follow, through the use of laptops with multi-core processors and access to the Virtual Campus services of our university, the students are easily able to implement and master the new concepts as they are taught. This teaching experiment was implemented to teach a concurrent and real-time programming course which is part of the computer engineering (CE) degree and taught during the third semester of the CE curriculum. Evaluation of the students’ academic performance when they had been taught with this approach revealed a 20.6% improvement in the students’ end-of-course grades.","Manuel I. Capel and Antonio J. Tomeu and Alberto G. Salguero",2017,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR10"
"Version 3.0 of code Java for 3D simulation of the CCA model","In this paper we provide a new version of program for replacing the previous version. The frequency of traversing the clusters-list was reduced, and some code blocks were optimized properly; in addition, we appended and revised the comments of the source code for some methods or attributes. The compared experimental results show that new version has better time efficiency than the previous version.
New version program summary
Program title: CCA v03 Program Files doi:10.17632/gzncs28f95.1 Licensing provisions: Apache-2.0 License Programming language: Java Journal Reference of previous version: Computer Physics Communications 204 (2016) 214–215. Does the new version supersede the previous version?: Yes Nature of problem: There is poor running efficiency in the previous version. In addition, some methods in Java entity classes are short of necessary optimization. Solution method: A number of redundant steps, and running frequency of loop statements, are optimized into a more reasonable range. Object oriented solution, easy to reuse, extend and customize, in any development environment which supports Java JDK. Reasons for the new version: 1. In the previous version [2], for a random selected cluster moving a unit length, it must firstly obtain the maximum diffusion coefficient Dmax [1] which is searched from the clusters-list. In fact, it is not necessary that the program traverse the clusters-list for every diffusion step. If there is no aggregation between two clusters in the process of diffusion, the program does not have to traverse the clusters-list to get the Dmax in this step, because of no change for every cluster’s property around the system. Dmax just be saved as a global variable which is used to store the latest maximum value. When aggregation takes place, we just update the current maximum value of Dmax, so greatly reduce loop steps. Although contrary to the principles of object-oriented programming to some extent, it is worthy to make such sacrifice. 2. Some methods in Java entity classes are short of necessary optimization, containing redundant codes which consume extra hardware resources. 3. The source code of old program lacks necessary explanations for some methods or attributes. Moreover, some code comments, which are incorrect or inexplicable, need revise. Summary of revisions: 1. Cut down the frequency of traversing the clusters-list. 2. Optimize some code blocks. 3. Append and revise the comments of the source code for some methods or attributes, so as to easily for users to understand, debug and maintain. Additional comments:Fig. 1Log–log plot of simulation time (measurement by millisecond) vs. number of particles (N). For constant concentration (C=0.01), N varies in 270, 640, 1250, 2160 and 3430. We provide a referential log–log plot of simulation time versus number of particles as shown in Fig. 1 by a general PC. In this simulation, constant concentration C is set to 0.01, diffusion exponent is set to 0.5, sticking probability exponent is set to 0, and absolute temperature is set to 298K, but the side length L of cube varies in 30, 40, 50, 60 and 70 (these parameters come from literature [1]). The number of particles N varies with the side length L(N=C∗L3), so we just adjust the side length to get different particle numbers. As we can see from Fig. 1, the coefficient of x is 2.2413 of the new version, which is near to quadratic complexity, the coefficient of x is 2.9367 of the previous version, which is very close to cubic complexity. Obviously, the new version has a better running efficiency than the previous version. The new program brings a high running speed, but cause another problem that one user cannot promptly click the “Suspend” button to see a snapshot what he or she wanted to see. For this fault, we append a “Slow down” checkbox which can reduce the running speed for users’ choice, so it improves user-experience. Running time: Determined by the initial parameters. References:[1]C. Li, H. Xiong, Computer Physics Communications 185 (2014) 3424–3429.[2]K. Zhang, H. Xiong, C. Li, Computer Physics Communications 204 (2016) 214–215.","Kebo Zhang and Junsen Zuo and Yifeng Dou and Chao Li and Hailing Xiong",2016,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Socioemotional wealth in family firms: A longitudinal content analysis of corporate disclosures","Family business literature has noted the nature and presence of socioemotional wealth (SEW) in family firms. One method of observing SEW is by a five-dimension approach, collectively termed FIBER. While the dimensions are well defined, they have been critiqued, as have the theoretical foundations of SEW. Regardless, given the concept of SEW is about a decade old and the FIBER dimensions less so, it is reasonable to argue more research is needed. One potentially useful research approach is an historical one, which we will here term SEW history – the use of historical research to support (or question) the development of SEW as a concept. We undertake a content analysis of corporate disclosures through the Chairman’s Statement of two Irish family breweries over a period of about two decades. To conduct the analysis, we develop a coding scheme based on the FIBER dimensions and offer some research propositions around these dimensions of SEW being stable (or not) over time. Our findings reveal that the Chairman’s Statement does include FIBER dimensions in both breweries and they do change over time. Subsequent statistical analysis reveals significant differences in the FIBER dimensions between the two breweries and context is revealed as a key issue in the assessment of SEW, something prior research has noted. The study also raises some questions on the nature of some FIBER dimensions, in particular the “I” dimension.","Peter Cleary and Martin Quinn and Alonso Moreno",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Consistent merging of model versions","While many engineering tasks can, and should be, manageable independently, it does place a great burden on explicit collaboration needs—including the need for frequent and incremental merging of artifacts that software engineers manipulate using these tools. State-of-the-art merging techniques are often limited to textual artifacts (e.g., source code) and they are unable to discover and resolve complex merging issues beyond simple conflicts. This work focuses on the merging of models where we consider not only conflicts but also arbitrary syntactic and semantic consistency issues. Consistent artifacts are merged fully automatically and only inconsistent/conflicting artifacts are brought to the users’ attention, together with a systematic proposal of how to resolve them. Our approach is neutral with regard to who made the changes and hence reduces the bias caused by any individual engineer’s limited point of view. Our approach also applies to arbitrary design or models, provided that they follow a well-defined metamodel with explicit constraints—the norm nowadays. The extensive empirical evaluation suggests that our approach scales to practical settings.","Hoa Khanh Dam and Alexander Egyed and Michael Winikoff and Alexander Reder and Roberto E. Lopez-Herrejon",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Just in time? Using QR codes for multi-professional learning in clinical practice","Clinical guidelines and policies are widely available on the hospital intranet or from the internet, but can be difficult to access at the required time and place. Clinical staff with smartphones could use Quick Response (QR) codes for contemporaneous access to relevant information to support the Just in Time Learning (JIT-L) paradigm. There are several studies that advocate the use of smartphones to enhance learning amongst medical students and junior doctors in UK. However, these participants are already technologically orientated. There are limited studies that explore the use of smartphones in nursing practice. QR Codes were generated for each topic and positioned at relevant locations on a medical ward. Support and training were provided for staff. Website analytics and semi-structured interviews were performed to evaluate the efficacy, acceptability and feasibility of using QR codes to facilitate Just in Time learning. Use was intermittently high but not sustained. Thematic analysis of interviews revealed a positive assessment of the Just in Time learning paradigm and context-sensitive clinical information. However, there were notable barriers to acceptance, including usability of QR codes and appropriateness of smartphone use in a clinical environment. The use of Just in Time learning for education and reference may be beneficial to healthcare professionals. However, alternative methods of access for less technologically literate users and a change in culture of mobile device use in clinical areas may be needed.","Joseph Tawanda Jamu and Hannah Lowi-Jones and Colin Mitchell",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Present status of theoretical understanding of charge changing processes at low beam energies","A model for the evaluation of charge-state distributions of fast heavy ions in solid targets is being developed since late eighties in terms of ETACHA code. Time to time it is being updated to deal with more number of electrons and non-perturbative processes. The calculation approach of the recent one, which is formulated for handling the non-perturbative processes better, is different from the earlier ones. However, the experimental results for the projectiles up to 28 electrons can be compared with the predictions from any versions of ETACHA code. Though earlier versions are not meant for the non-perturbative cases, but the detail comparison suggests that predictions from an earlier version is somewhat superior to that of the recent version. However, certain difference up to 4 units of charge found between the earlier version and experimental results on the mean charge states and it is attributed to nonradiative electron capture taking place at the exit surface in the influence of wake and dynamic screening effects. This mechanism can play its role in multiply charge formation in the electrospray ionization of big molecules.","D.K. Swami and T. Nandi",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Cognitive complexity as a quantifier of version to version Java-based source code change: An empirical probe","Context
It has been often argued that it is challenging to modify code fragments from existing software that contains files that are difficult to comprehend. Since systematic software maintenance includes an extensive human activity, cognitive complexity is one of the intrinsic factors that could potentially contribute to or impede an efficient software maintenance practice, the empirical validation of which remains vastly unaddressed.
Objective
This study conducts an experimental analysis in which the software developer's level of difficulty in comprehending the software: the cognitive complexity, is theoretically computed and empirically evaluated for estimating its relevance to actual software change.
Method
For multiple successive releases of two Java-based software projects, where the source code of a previous release has been substantively used in a novel release, we calculate the change results and the values of the cognitive complexity for each of the version's source code Java files. We construct eight datasets and build predictive models using statistical analysis and machine learning techniques.
Results
The pragmatic comparative examination of the estimated cognitive complexity against prevailing metrics of software change and software complexity clearly validates the cognitive complexity metric as a noteworthy measure of version to version source code change.","Loveleen Kaur and Ashutosh Mishra",2019,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Bayesian denoising hashing for robust image retrieval","Learning to hash is one of the most popular techniques in image retrieval, but few work investigates its robustness to noise corrupted images in which the unknown pattern of noise would heavily deteriorate the performance. To deal with this issue, we present in this paper a Bayesian denoising hashing algorithm whose output can be regarded a denoised version of the input hash code. We show that our method essentially seeks to reconstruct a new but more robust hash code by preserving the original input information while imposing extra constraints so as to correct the corrupted bits. We optimized this model in variational Bayes framework which has a closed-form update in each iteration that is more efficient than numerical optimization. Furthermore, our method can be added at the top of any original hashing layer, serving as a post-processing denoising layer with no change to previous training procedure. Experiments on three popular datasets demonstrate that the proposed method yields robust and meaningful hash code, which significantly improves the performance of state-of-the-art hash learning methods on challenging tasks such as large-scale natural image retrieval and retrieval with corrupted images.","Dong Wang and Ge Song and Xiaoyang Tan",2019,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Detection of seagrass scars using sparse coding and morphological filter","The proximity of seagrass meadows to centers of human activity makes them vulnerable to a variety of habitat degrading insults. Physical scarring has long been recognized as an important but difficult-to-quantify source of habitat fragmentation and seagrass loss. We present a pixel-based algorithm to detect seafloor propeller seagrass scars in shallow water that promises to automate the detection and measurement of scars across the submarine landscape.11A preliminary version of the paper was presented at the SPIE Remote Sensing Conference, Amsterdam, Netherlands, 2014 (Oguslu et al., 2014). We applied the algorithm to multispectral and panchromatic images captured at the Deckle Beach, Florida using the WorldView-2 commercial satellite. The algorithm involves four steps using spectral and spatial information from radiometrically calibrated multispectral and panchromatic images. First, we fused multispectral and panchromatic images using a principal component analysis (PCA)-based pan-sharpening method to obtain multispectral pan-sharpened bands. In the second step, we enhanced the image contrast of the pan-sharpened bands for better scar detection. In the third step, we classified the contrast enhanced image pixels into scar and non-scar categories based on a sparse coding algorithm that produced an initial scar map in which false positive scar pixels were also present. In the fourth step, we applied post-processing techniques including a morphological filter and local orientation to reduce false positives. Our results show that the proposed method may be implemented on a regular basis to monitor changes in habitat characteristics of coastal waters.","E. Oguslu and Kazi Islam and Daniel Perez and V.J. Hill and W.P. Bissett and R.C. Zimmerman and J. Li",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Balancing the Robustness and Efficiency of Odor Representations during Learning","Summary
For reliable stimulus identification, sensory codes have to be robust by including redundancy to combat noise, but redundancy sacrifices coding efficiency. To address how experience affects the balance between the robustness and efficiency of sensory codes, we probed odor representations in the mouse olfactory bulb during learning over a week, using longitudinal two-photon calcium imaging. When mice learned to discriminate between two dissimilar odorants, responses of mitral cell ensembles to the two odorants gradually became less discrete, increasing the efficiency. In contrast, when mice learned to discriminate between two very similar odorants, the initially overlapping representations of the two odorants became progressively decorrelated, enhancing the robustness. Qualitatively similar changes were observed when the same odorants were experienced passively, a condition that would induce implicit perceptual learning. These results suggest that experience adjusts odor representations to balance the robustness and efficiency depending on the similarity of the experienced odorants.","Monica W. Chu and Wankun L. Li and Takaki Komiyama",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"How are e-leadership practices in implementing a school virtual learning environment enhanced? A grounded model study","E-leadership is defined as a social influence process mediated by information and communication technology to produce change in behavior and performance with individuals and groups in an organization. This study investigates e-leadership practices among users of a school virtual learning environment. It was performed in two stages. First, semi-structured interviews with school administrators, teachers, students, parents and school software experts were conducted. The qualitative data collected from the interviews were coded and analyzed using open and axial coding procedures. As a result, an e-leadership model emerged from the data that consisted of eight themes: e-leadership quality with seven core factors, namely, readiness, practices, strategies, support, culture, needs and obstacles. Second, the validity and reliability of the model were further ascertained with a quantitative survey study involving 320 school administrators. The findings of this study established a grounded model for e-leadership practices in schools.","Yan Piaw Chua and Yee Pei Chua",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A meta-model for dataflow-based rules in smart environments: Evaluating user comprehension and performance","A considerable part of the behavior in smart environments relies on event-driven and rule specification. Rules are the mechanism most often used to enable user customization of the environment. However, the expressiveness of the rules available to users in editing and other tools is usually either limited or the available rule editing interfaces are not designed for end-users with low skills in programming. This means we have to look for interaction techniques and new ways to define user customization rules. This paper describes a generic and flexible meta-model to support expressive rules enhanced with data flow expressions that will graphically support the definition of rules without writing code. An empirical study was conducted on the ease of understanding of the visual data flow expressions, which are the key elements in our rule proposal. The visual dataflow language was compared to its corresponding textual version in terms of comprehension and ease of learning by teenagers in exercises involving calculations, modifications, writing and detecting equivalences in expressions in both languages. Although the subjects had some previous experience in editing mathematical expressions on spreadsheets, the study found their performance with visual dataflows to be significantly better in calculation and modification exercises. This makes our dataflow approach a promising mechanism for expressing user-customized reactive behavior in Ambient Intelligence (AmI) environments. The performance of the rule matching processor was validated by means of two stress tests to ensure that the meta-model approach adopted would be able to scale up with the number of types and instances in the space.","Alejandro Catala and Patricia Pons and Javier Jaen and Jose A. Mocholi and Elena Navarro",2013,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"TORBEAM 2.0, a paraxial beam tracing code for electron-cyclotron beams in fusion plasmas for extended physics applications","The paraxial WKB code TORBEAM (Poli, 2001) is widely used for the description of electron-cyclotron waves in fusion plasmas, retaining diffraction effects through the solution of a set of ordinary differential equations. With respect to its original form, the code has undergone significant transformations and extensions, in terms of both the physical model and the spectrum of applications. The code has been rewritten in Fortran 90 and transformed into a library, which can be called from within different (not necessarily Fortran-based) workflows. The models for both absorption and current drive have been extended, including e.g. fully-relativistic calculation of the absorption coefficient, momentum conservation in electron–electron collisions and the contribution of more than one harmonic to current drive. The code can be run also for reflectometry applications, with relativistic corrections for the electron mass. Formulas that provide the coupling between the reflected beam and the receiver have been developed. Accelerated versions of the code are available, with the reduced physics goal of inferring the location of maximum absorption (including or not the total driven current) for a given setting of the launcher mirrors. Optionally, plasma volumes within given flux surfaces and corresponding values of minimum and maximum magnetic field can be provided externally to speed up the calculation of full driven-current profiles. These can be employed in real-time control algorithms or for fast data analysis.","E. Poli and A. Bock and M. Lochbrunner and O. Maj and M. Reich and A. Snicker and A. Stegmeir and F. Volpe and N. Bertelli and R. Bilato and G.D. Conway and D. Farina and F. Felici and L. Figini and R. Fischer and C. Galperti and T. Happel and Y.R. Lin-Liu and N.B. Marushchenko and U. Mszanowski and F.M. Poli and J. Stober and E. Westerhof and R. Zille and A.G. Peeters and G.V. Pereverzev",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9, CR8, CR10"
"A fast code for channel limb radiances with gas absorption and scattering in a spherical atmosphere","We present a radiative transfer code capable of accurately and rapidly computing channel limb radiances in the presence of gaseous absorption and scattering in a spherical atmosphere. The code has been prototyped for the Mars Climate Sounder measuring limb radiances in the thermal part of the spectrum (200–900cm−1) where absorption by carbon dioxide and water vapor and absorption and scattering by dust and water ice particles are important. The code relies on three main components: 1) The Gauss Seidel Spherical Radiative Transfer Model (GSSRTM) for scattering, 2) The Planetary Line-By-Line Radiative Transfer Model (P-LBLRTM) for gas opacity, and 3) The Optimal Spectral Sampling (OSS) for selecting a limited number of spectral points to simulate channel radiances and thus achieving a substantial increase in speed. The accuracy of the code has been evaluated against brute-force line-by-line calculations performed on the NASA Pleiades supercomputer, with satisfactory results. Additional improvements in both accuracy and speed are attainable through incremental changes to the basic approach presented in this paper, which would further support the use of this code for real-time retrievals and data assimilation. Both newly developed codes, GSSRTM/OSS for MCS and P-LBLRTM, are available for additional testing and user feedback.","Janusz Eluszkiewicz and Gennady Uymin and David Flittner and Karen Cady-Pereira and Eli Mlawer and John Henderson and Jean-Luc Moncet and Thomas Nehrkorn and Michael Wolff",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Reflection on Teaching: A Way to Learn from Practice","Developing students’ reflection on their learning is currently one of the major learning goals in higher education. Today's students need to be prepared to function in the rapidly changing world of professional practice. In line with the above, reflection is currently a key concept in teacher education. The purpose of the present study is to support student teachers’ reflection. More specifically, to find out what kind of problematic situations students face in their practical teaching and which levels of activity they report in reflection when using a reduced version of the guided reflection procedure. The analysis is based on 34 written individual reports of the student teachers from a university in Estonia. Data was analysed using qualitative content analysis method, the employed coding scheme was developed based on Korthagen & Vasalos (2005) model of core reflection. The majority of problems were brought out in connection with the students themselves. Finding solutions to the problematic situations showed that reflections were made on all levels of the onion model (Korthagen & Vasalos, 2005). The most frequent level of reflection was the level of beliefs, followed by environment, behaviour, competencies, identity and mission.","Anu Sööt and Ele Viskus",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Local electron tomography using angular variations of surface tangents: Stomo version 2","In a recent publication, we investigated the prospect of measuring the outer three-dimensional (3D) shapes of nano-scale atom probe specimens from tilt-series of images collected in the transmission electron microscope. For this purpose alone, an algorithm and simplified reconstruction theory were developed to circumvent issues that arise in commercial “back-projection” computations in this context. In our approach, we give up the difficult task of computing the complete 3D continuum structure and instead seek only the 3D morphology of internal and external scattering interfaces. These interfaces can be described as embedded 2D surfaces projected onto each image in a tilt series. Curves and other features in the images are interpreted as inscribed sets of tangent lines, which intersect the scattering interfaces at unknown locations along the direction of the incident electron beam. Smooth angular variations of the tangent line abscissa are used to compute the surface tangent intersections and hence the 3D morphology as a “point cloud”. We have published the explicit details of our alternative algorithm along with the source code entitled “stomo_version_1”. For this work, we have further modified the code to efficiently handle rectangular image sets, perform much faster tangent-line “edge detection” and smoother tilt-axis image alignment using simple bi-linear interpolation. We have also adapted the algorithm to detect tangent lines as “ridges”, based upon 2nd order partial derivatives of the image intensity; the magnitude and orientation of which is described by a Hessian matrix. Ridges are more appropriate descriptors for tangent-line curves in phase contrast images outlined by Fresnel fringes or absorption contrast data from fine-scale objects. Improved accuracy, efficiency and speed for “stomo_version_2” is demonstrated in this paper using both high resolution electron tomography data of a nano-sized atom probe tip and simulated absorption-contrast images.
Program summary
Program title: STOMO version 2 Catalogue identifier: AEFS_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEFS_v2_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 2854 No. of bytes in distributed program, including test data, etc.: 23 559 Distribution format: tar.gz Programming language: C/C++ Computer: PC Operating system: Windows XP RAM: Scales as the product of experimental image dimensions multiplied by the number of points chosen by the user in polynomial fitting. Typical runs require between 50 Mb and 100 Mb of RAM. Supplementary material: Sample output files, for the test run provided, are available. Classification: 7.4, 14 Catalogue identifier of previous version: AEFS_v1_0 Journal reference of previous version: Comput. Phys. Comm. 181 (2010) 676 Does the new version supersede the previous version?: Yes Nature of problem: A local electron tomography algorithm of specimens for which conventional back projection may fail and or data for which there is a limited angular range (which would otherwise cause significant ‘missing-wedge’ artefacts). The algorithm does not solve the tomography back projection problem but rather locally reconstructs the 3D morphology of surfaces defined by varied scattering densities. Solution method: Local reconstruction is effected using image-analysis edge and ridge detection computations on experimental tilt series to measure smooth angular variations of surface tangent-line intersections, which generate point clouds decorating the embedded and or external scattering surfaces of a specimen. Reasons for new version: The new version was coded to cater for rectangular images in experimental tilt-series, ensure smoother image rotations, provide ridge detection (suitable for sensing phase-contrast Fresnel fringes and other fine-scale structures), faster/larger kernel edge detection and also greatly reduce RAM usage. Specimen surface normals are also explicitly computed from tangent-line and edge intersections, providing new information for potential use in point cloud rendering. Hysteresis thresholding implemented in the version 1 edge-detection algorithm provided only sparse edge-linking. Version 2 now implements edge tracking using recursion to fully link the edges during hysteresis thresholding. Furthermore in version 1 the minimum number of fitted polynomial points (specified in the input file) was not correctly imposed, which has been fixed for version 2. Most of these changes increase the accuracy of 3d morphology surface-tomography reconstructions by facilitating the use of more/finer tilt angles and experimental images of increased spatial-resolution. The ridge detection was incorporated to specifically improve the reconstruction of internal specimen morphology. Summary of revisions:•Included Hessian() function to compute 2nd order spatial derivatives of image intensities (operates in the same fashion as the previous and existing Sobel() function).•Changed convolve_Gaussian() function to alternatively use successive 1D convolutions (rather than cumbersome 2D summations implemented in version 1), resulting in a large increase in computational speed without any loss in accuracy. The convolution kernel size was hence widened to three times the full width half maximum of the Gaussian filter to improve scale-space selection accuracy.•A ridge detection option was included to compute edge maps sensitive to ridges, rather than edges, using elements from a Hessian matrix; the eigenvalues of which were used to define ridge direction for Canny-type hysteresis thresholding. Function edge_detect_Canny() was also altered to pass the gradient-direction maps (from either Hessian or Sobel based operators) in and out of scope for computation of surface normals; thereby enabling the output of both point-cloud and corresponding unstructured vector-field surface descriptors.•Function rotate_imgs() was changed to incorporate basic bi-linear interpolation for improved tilt-axis alignment of the entire tilt series in exp_data.dat. Smoother and more accurate edge maps are thereby produced.•Algorithm convert_point_cloud_to_tomogram() was created to output the tomogram 3d_imgs.dat in a more memory efficient manner. The function shell_sort(), adapted from numerical recipes in C, was also coded for this purpose.•The new function compute_xyz() was coded to calculate point-clouds and tomogram surface normals using information from single tilt images, as opposed to the entire stack. This function is hence used iteratively throughout the reconstruction as each tilt image is analysed in succession.•The new function reconstruct_local() is the heart of stomo_version_2.cpp. the main() source code in stomo_version_1.cpp has been rewritten here to process experimental images and edge maps one at a time, using a buffered 3d array of dimensions dictated solely by the number of tilt images required for the local SVD fit of the angular variations. These changes (along with similar iterative file writing) have been made to vastly reduce memory usage and hence allow higher spatial and angular resolution data sets to be analysed without recourse to high performance computing resources.•The input file has been simplified by removing the ‘slices’ and ‘channels’ settings (used in version 1 for crude image binning), which are now equal to the respective numbers of image rows and columns.•Every summation over image rows and columns has been checked to enable the analysis of rectangular images without error. For images of specimens with high aspect-ratios, such as narrow tips, these fixes allow significant reductions in computation time and memory usage.•Some arrays in the source code were not appropriately zeroed in version 1, causing reconstruction artefacts in some cases. These problems have now been fixed.•Fixed an if-statement to correctly impose the minimum number of fitted polynomial points, thereby reducing noise in the reconstructed data.•Implemented proper edge linking in the hysteresis thresholding code for Canny edge detection. Restrictions: The input experimental tilt-series of images must be registered with respect to a common single tilt axis with known orientation and position. Running time: For high quality reconstruction, 2–5 min.","T.C. Petersen and S.P. Ringer",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Gradient-enhanced model and its micromorphic regularization for simulation of Lüders-like bands in shape memory alloys","Shape memory alloys, notably NiTi, often exhibit softening pseudoelastic response that results in formation and propagation of Lüders-like bands upon loading, for instance, in uniaxial tension. A common approach to modelling softening and strain localization is to resort to gradient-enhanced formulations that are capable of restoring well-posedness of the boundary-value problem. This approach is also followed in the present paper by introducing a gradient-enhancement into a simple one-dimensional model of pseudoelasticity. In order to facilitate computational treatment, a micromorphic-type regularization of the gradient-enhanced model is subsequently performed. The formulation employs the incremental energy minimization framework that is combined with the augmented Lagrangian treatment of the resulting non-smooth minimization problem. A thermomechanically coupled model is also formulated and implemented in a finite-element code. The effect of the loading rate on the localization pattern in a NiTi wire under tension is studied, and the features predicted by the model show a good agreement with the experimental observations. Additionally, an analytical solution is provided for a propagating interface (macroscopic transformation front) both for the gradient-enhanced model and for its micromorphic version.","Mohsen Rezaee Hajidehi and Stanisław Stupkiewicz",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A novel prediction method for down-hole working conditions of the beam pumping unit based on 8-directions chain codes and online sequential extreme learning machine","In the oilfield operation, the beam pumping unit is a very important artificial lift method. As the down-hole parts work at hundred and thousand meters underground, they are hard to be found immediately when failures come out. If we can predict down-hole working conditions and master its continuous operation states in time, great improvement of the oil well production will be developed. In this paper, a novel down-hole working conditions prediction method for the beam pumping unit based on the chaos time series prediction is proposed. First, curve contour of the dynamometer card is redrawn by 8-directions chain codes, and then eight feature vectors are extracted to construct eight feature vector time series; then, the online sequential extreme learning machine (OS-ELM) method is used to build the prediction model, which can realize fast updating with dynamic work condition changes; finally, the grey interval relational degree between the predicted feature vectors and feature vectors of each fault type is calculated to determine the predicted fault type. Actual production data of an oil well are used for example verification, and both online diagnosis and offline diagnosis illustrate the effectiveness of the method.","Kun Li and Ying Han and Tong Wang",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A neural mechanism of dynamic gating of task-relevant information by top-down influence in primary visual cortex","Visual recognition involves bidirectional information flow, which consists of bottom-up information coding from retina and top-down information coding from higher visual areas. Recent studies have demonstrated the involvement of early visual areas such as primary visual area (V1) in recognition and memory formation. V1 neurons are not passive transformers of sensory inputs but work as adaptive processor, changing their function according to behavioral context. Top-down signals affect tuning property of V1 neurons and contribute to the gating of sensory information relevant to behavior. However, little is known about the neuronal mechanism underlying the gating of task-relevant information in V1. To address this issue, we focus on task-dependent tuning modulations of V1 neurons in two tasks of perceptual learning. We develop a model of the V1, which receives feedforward input from lateral geniculate nucleus and top-down input from a higher visual area. We show here that the change in a balance between excitation and inhibition in V1 connectivity is necessary for gating task-relevant information in V1. The balance change well accounts for the modulations of tuning characteristic and temporal properties of V1 neuronal responses. We also show that the balance change of V1 connectivity is shaped by top-down signals with temporal correlations reflecting the perceptual strategies of the two tasks. We propose a learning mechanism by which synaptic balance is modulated. To conclude, top-down signal changes the synaptic balance between excitation and inhibition in V1 connectivity, enabling early visual area such as V1 to gate context-dependent information under multiple task performances.","Akikazu Kamiyama and Kazuhisa Fujita and Yoshiki Kashimori",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Improving multi-objective code-smells correction using development history","One of the widely used techniques to improve the quality of software systems is refactoring. Software refactoring improves the internal structure of the system while preserving its external behavior. These two concerns drive the existing approaches to refactoring automation. However, recent studies demonstrated that these concerns are not enough to produce correct and consistent refactoring solutions. In addition to quality improvement and behavior preservation, studies consider, among others, construct semantics preservation and minimization of changes. From another perspective, development history was proven as a powerful source of knowledge in many maintenance tasks. Still, development history is not widely explored in the context of automated software refactoring. In this paper, we use the development history collected from existing software projects to propose new refactoring solutions taking into account context similarity with situations seen in the past. We propose a multi-objective optimization-based approach to find good refactoring sequences that (1) minimize the number of code-smells, and (2) maximize the use of development history while (3) preserving the construct semantics. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between these three objectives. We evaluate our approach using a benchmark composed of five medium and large-size open-source systems and four types of code-smells (Blob, spaghetti code, functional decomposition, and data class). Our experimental results show the effectiveness of our approach, compared to three different state-of-the-art approaches, with more than 85% of code-smells fixed and 86% of suggested refactorings semantically coherent when the change history is used.","Ali Ouni and Marouane Kessentini and Houari Sahraoui and Katsuro Inoue and Mohamed Salah Hamdi",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Initial Uncertainty Impacts Statistical Learning in Sound Sequence Processing","This paper features two studies confirming a lasting impact of first learning on how subsequent experience is weighted in early relevance-filtering processes. In both studies participants were exposed to sequences of sound that contained a regular pattern on two different timescales. Regular patterning in sound is readily detected by the auditory system and used to form “prediction models” that define the most likely properties of sound to be encountered in a given context. The presence and strength of these prediction models is inferred from changes in automatically elicited components of auditory evoked potentials. Both studies employed sound sequences that contained both a local and longer-term pattern. The local pattern was defined by a regular repeating pure tone occasionally interrupted by a rare deviating tone (p=0.125) that was physically different (a 30msvs. 60ms duration difference in one condition and a 1000Hz vs. 1500Hz frequency difference in the other). The longer-term pattern was defined by the rate at which the two tones alternated probabilities (i.e., the tone that was first rare became common and the tone that was first common became rare). There was no task related to the tones and participants were asked to ignore them while focussing attention on a movie with subtitles. Auditory-evoked potentials revealed long lasting modulatory influences based on whether the tone was initially encountered as rare and unpredictable or common and predictable. The results are interpreted as evidence that probability (or indeed predictability) assigns a differential information-value to the two tones that in turn affects the extent to which prediction models are updated and imposed. These effects are exposed for both common and rare occurrences of the tones. The studies contribute to a body of work that reveals that probabilistic information is not faithfully represented in these early evoked potentials and instead exposes that predictability (or conversely uncertainty) may trigger value-based learning modulations even in task-irrelevant incidental learning.","Juanita Todd and Alexander Provost and Lisa Whitson and Daniel Mullens",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Analyzing software evolution and quality by extracting Asynchrony change patterns","Change patterns describe two or more files were often changed together during the development or the maintenance of software systems. Several studies have been presented to detect change patterns and to analyze their types and their impact on software quality. In this context, we introduced the Asynchrony change pattern to describes a set of files that always change together in the same change periods, regardless developers who maintained them. In this paper, we investigate the impact of Asynchrony change pattern on design and code smells such as anti-patterns and code clones. Concretely, we conduct an empirical study by detecting Asynchrony change patterns, anti-patterns and code clones occurrences on 22 versions of four software systems and analyzing their fault-proneness. Results show that cloned files that follow the same Asynchrony change patterns have significantly increased fault-proneness with respect to other clones, and that anti-patterns following the same Asynchrony change pattern can be up to five times more risky in terms of fault-proneness as compared to other anti-patterns. Asynchrony change patterns thus seem to be strong indicators of fault-proneness for clones and anti-patterns.","Fehmi Jaafar and Angela Lozano and Yann-Gaël Guéhéneuc and Kim Mens",2017,"[""Science Direct""]","Aceito: CA5","Aceito: CA5"
"Semi-supervised subspace learning with L2graph","Subspace learning aims to learn a projection matrix from a given training set so that a transformation of raw data to a low-dimensional representation can be obtained. In practice, the labels of some training samples are available, which can be used to improve the discrimination of low-dimensional representation. In this paper, we propose a semi-supervised learning method which is inspired by the biological observation of similar inputs having similar codes (SISC), i.e., the same collection of cortical columns of the mammal׳s visual cortex is always activated by the similar stimuli. More specifically, we propose a mathematical formulation of SISC which minimizes the distance among the data points with the same label while maximizing the separability between different subjects in the projection space. The proposed method, namely, semi-supervised L2graph (SeL2graph) has two advantages: (1) unlike the classical dimension reduction methods such as principle component analysis, SeL2graph can automatically determine the dimension of feature space. This remarkably reduces the effort to find an optimal feature dimension for a good performance; and (2) it fully exploits the prior knowledge carried by the labeled samples and thus the obtained features are with higher discrimination and compactness. Extensive experiments show that the proposed method outperforms 7 subspace learning algorithms on 15 data sets with respect to classification accuracy, computational efficiency, and robustness to noises and disguises.","Xi Peng and Miaolong Yuan and Zhiding Yu and Wei Yun Yau and Lei Zhang",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Modeling polyp activity of Paragorgia arborea using supervised learning","While the distribution patterns of cold-water corals, such as Paragorgia arborea, have received increasing attention in recent studies, little is known about their in situ activity patterns. In this paper, we examine polyp activity in P. arborea using machine learning techniques to analyze high-resolution time series data and photographs obtained from an autonomous lander cluster deployed in the Stjernsund, Norway. An interactive illustration of the models derived in this paper is provided online as supplementary material. We find that the best predictor of the degree of extension of the coral polyps is current direction with a lag of three hours. Other variables that are not directly associated with water currents, such as temperature and salinity, offer much less information concerning polyp activity. Interestingly, the degree of polyp extension can be predicted more reliably by sampling the laminar flows in the water column above the measurement site than by sampling the more turbulent flows in the direct vicinity of the corals. Our results show that the activity patterns of the P. arborea polyps are governed by the strong tidal current regime of the Stjernsund. It appears that P. arborea does not react to shorter changes in the ambient current regime but instead adjusts its behavior in accordance with the large-scale pattern of the tidal cycle itself in order to optimize nutrient uptake.","Arne N. Johanson and Sascha Flögel and Wolf-Christian Dullo and Peter Linke and Wilhelm Hasselbring",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Case-Based Learning Facilitates Critical Thinking in Undergraduate Nutrition Education: Students Describe the Big Picture","Background
The vision of dietetics professions is based on interdependent education, credentialing, and practice. Case-based learning is a method of problem-based learning that is designed to heighten higher-order thinking. Case-based learning can assist students to connect education and specialized practice while developing professional skills for entry-level practice in nutrition and dietetics.
Objective
This study examined student perspectives of their learning after immersion into case-based learning in nutrition courses.
Design
The theoretical frameworks of phenomenology and Bloom’s Taxonomy of Educational Objectives triangulated the design of this qualitative study.
Participants/setting
Data were drawn from 426 written responses and three focus group discussions among 85 students from three upper-level undergraduate nutrition courses.
Main outcome measures
Coding served to deconstruct the essence of respondent meaning given to case-based learning as a learning method. The analysis of the coding was the constructive stage that led to configuration of themes and theoretical practice pathways about student learning.
Results
Four leading themes emerged. Story or Scenario represents the ways that students described case-based learning, changes in student thought processes to accommodate case-based learning are illustrated in Method of Learning, higher cognitive learning that was achieved from case-based learning is represented in Problem Solving, and Future Practice details how students explained perceived professional competency gains from case-based learning.
Conclusions
The skills that students acquired are consistent with those identified as essential to professional practice. In addition, the common concept of Big Picture was iterated throughout the themes and demonstrated that case-based learning prepares students for multifaceted problems that they are likely to encounter in professional practice.","Tara Harman and Brenda Bertrand and Annette Greer and Arianna Pettus and Jill Jennings and Elizabeth Wall-Bassett and Oyinlola Toyin Babatunde",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"SIGNUM: A Matlab, TIN-based landscape evolution model","Several numerical landscape evolution models (LEMs) have been developed to date, and many are available as open source codes. Most are written in efficient programming languages such as Fortran or C, but often require additional code efforts to plug in to more user-friendly data analysis and/or visualization tools to ease interpretation and scientific insight. In this paper, we present an effort to port a common core of accepted physical principles governing landscape evolution directly into a high-level language and data analysis environment such as Matlab. SIGNUM (acronym for Simple Integrated Geomorphological Numerical Model) is an independent and self-contained Matlab, TIN-based landscape evolution model, built to simulate topography development at various space and time scales. SIGNUM is presently capable of simulating hillslope processes such as linear and nonlinear diffusion, fluvial incision into bedrock, spatially varying surface uplift which can be used to simulate changes in base level, thrust and faulting, as well as effects of climate changes. Although based on accepted and well-known processes and algorithms in its present version, it is built with a modular structure, which allows to easily modify and upgrade the simulated physical processes to suite virtually any user needs. The code is conceived as an open-source project, and is thus an ideal tool for both research and didactic purposes, thanks to the high-level nature of the Matlab environment and its popularity among the scientific community. In this paper the simulation code is presented together with some simple examples of surface evolution, and guidelines for development of new modules and algorithms are proposed.","A. Refice and E. Giachetta and D. Capolongo",2012,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Associative Learning Enhances Population Coding by Inverting Interneuronal Correlation Patterns","Summary
Learning-dependent cortical encoding has been well described in single neurons. But behaviorally relevant sensory signals drive the coordinated activity of millions of cortical neurons; whether learning produces stimulus-specific changes in population codes is unknown. Because the pattern of firing rate correlations between neurons—an emergent property of neural populations—can significantly impact encoding fidelity, we hypothesize that it is a target for learning. Using an associative learning procedure, we manipulated the behavioral relevance of natural acoustic signals and examined the evoked spiking activity in auditory cortical neurons in songbirds. We show that learning produces stimulus-specific changes in the pattern of interneuronal correlations that enhance the ability of neural populations to recognize signals relevant for behavior. This learning-dependent enhancement increases with population size. The results identify the pattern of interneuronal correlation in neural populations as a target of learning that can selectively enhance the representations of specific sensory signals.","James M. Jeanne and Tatyana O. Sharpee and Timothy Q. Gentner",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Detecting broken pointcuts using structural commonality and degree of interest","Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base-code can lead to join points incorrectly falling in or out of the scope of pointcuts. Deciding which pointcuts have broken due to base-code changes is a daunting venture, especially in large and complex systems. We present an automated approach that recommends pointcuts that are likely to require modification due to a particular base-code change, as well as ones that do not. Our hypothesis is that join points selected by a pointcut exhibit common structural characteristics. Patterns describing such commonality are used to recommend pointcuts that have potentially broken with a degree of confidence as the developer is typing. The approach is implemented as an extension to the popular Mylyn Eclipse IDE plug-in, which maintains focused contexts of entities relevant to the task at hand using a Degree of Interest (DOI) model. We show that it is accurate in revealing broken pointcuts by applying it to multiple versions of several open source projects and evaluating the quality of the recommendations produced against actual modifications. We found that our tool made broken pointcuts 2.14 times more interesting in the DOI model than unbroken ones, with a p-value under 0.1, indicating a significant difference in final DOI value between the two kinds of pointcuts (i.e., broken and unbroken).","Raffi Khatchadourian and Awais Rashid and Hidehiko Masuhara and Takuya Watanabe",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Trimming and gluing Gray codes","We consider the algorithmic problem of generating each subset of [n]:={1,2,…,n} whose size is in some interval [k,l], 0≤k≤l≤n, exactly once (cyclically) by repeatedly adding or removing a single element, or by exchanging a single element. For k=0 and l=n this is the classical problem of generating all 2n subsets of [n] by element additions/removals, and for k=l this is the classical problem of generating all (nk) subsets of [n] by element exchanges. We prove the existence of such cyclic minimum-change enumerations for a large range of values n, k, and l, improving upon and generalizing several previous results. For all these existential results we provide optimal algorithms to compute the corresponding Gray codes in constant O(1) time per generated set and O(n) space. Rephrased in terms of graph theory, our results establish the existence of (almost) Hamilton cycles in the subgraph of the n-dimensional cube Qn induced by all levels [k,l]. We reduce all remaining open cases to a generalized version of the middle levels conjecture, which asserts that the subgraph of Q2k+1 induced by all levels [k−c,k+1+c], c∈{0,1,…,k}, has a Hamilton cycle. We also prove an approximate version of this generalized conjecture, showing that this graph has a cycle that visits a (1−o(1))-fraction of all vertices.","Petr Gregor and Torsten Mütze",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Changes in interlimb coordination during walking and grasping task in older adult fallers and non-fallers","The aim of this study was to investigate interlimb coordination in young and older adults with and without a history of falls during the combined task of walking and prehension with different levels of manual task difficulty. Participants walked on a pathway and grasped a dowel. A vector coding technique evaluated coordination patterns. The coordination pattern was not affected by the difficulty level of the manual task. Older adults seemed to prioritize the movement of the right shoulder to grasp the dowel and then ‘froze’ the movement of the other joint (left shoulder) not directly involved in the grasping task. The preference to pick up the dowel in the double support phase and the increase in right shoulder phase made by older adults with a history of falls suggests an even greater decoupling between walking and prehension.","Natalia Madalena Rinaldi and Richard van Emmerik and Renato Moraes",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Evaluating Surgical Coaching: A Mixed Methods Approach Reveals More Than Surveys Alone","Objective
Traditionally, surgical educators have relied upon participant survey data for the evaluation of educational interventions. However, the ability of such subjective data to completely evaluate an intervention is limited. Our objective was to compare resident and attending surgeons’ self-assessments of coaching sessions from surveys with independent observations from analysis of intraoperative and postoperative coaching transcripts.
Design
Senior residents were video-recorded operating. Each was then coached by the operative attending in a 1:1 video review session. Teaching points made in the operating room (OR) and in post-OR coaching sessions were coded by independent observers using dialogue analysis then compared using t-tests. Participants were surveyed regarding the degree of teaching dedicated to specific topics and perceived changes in teaching level, resident comfort, educational assessments, and feedback provision between the OR and the post-OR coaching sessions.
Setting
A single, large, urban, tertiary-care academic institution.
Participants
Ten PGY4 to 5 general surgery residents and 10 attending surgeons.
Results
Although the reported experiences of teaching and coaching sessions by residents and faculty were similar (Pearson correlation coefficient = 0.88), these differed significantly from independent observations. Observers found that residents initiated a greater proportion of teaching points and had more educational needs assessments during coaching, compared to the OR. However, neither residents nor attendings reported a change between the 2 environments with regard to needs assessments nor comfort with asking questions or making suggestions. The only metric on which residents, attendings, and observers agreed was the provision of feedback.
Conclusions
Participants’ perspectives, although considered highly reliable by traditional metrics, rarely aligned with analysis of the associated transcripts from independent observers. Independent observation showed a distinct benefit of coaching in terms of frequency and type of learning points. These findings highlight the importance of seeking different perspectives, data sources, and methodologies when evaluating clinical education interventions. Surgical education can benefit from increased use of dialogue analyses performed by independent observers, which may represent a viewpoint distinct from that obtained by survey methodology.","Laura M. Mazer and Yue-Yung Hu and Alexander F. Arriaga and Caprice C. Greenberg and Stuart R. Lipsitz and Atul A. Gawande and Douglas S. Smink and Steven J. Yule",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Extending Paralldroid with object oriented annotations","The popularity of the handheld systems ( smartphones, tablets , ...) and their computational capability open new challenges in terms of the efficient use of such devices. The heterogeneity of these SoCs and MPSoCs demands very specific knowledge of the devices, involving a very high learning curve for the programmers. To ease the development task we build Paralldroid, a framework oriented to general purpose programmers for mobile devices. Paralldroid unifies the Android programming models and allows for the automatic generation of parallel code. Sections of code to be optimized in a Java program can be annotated using Paralldroid annotations. Paralldroid automatically generates the native C or Renderscript code required to take advantage of the underlying parallel platform (GPU included). The code generated by Paralldroid offers a good performance with a very low cost of development, contributing to increased productivity when developing efficient code.","Alejandro Acosta and Sergio Afonso and Francisco Almeida",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Spectral history model in DYN3D: Verification against coupled Monte-Carlo thermal-hydraulic code BGCore","This research focuses on the verification of a recently developed methodology accounting for spectral history effects in 3D full core nodal simulations. The traditional deterministic core simulation procedure includes two stages: (1) generation of homogenized macroscopic cross section sets and (2) application of these sets to obtain a full 3D core solution with nodal codes. The standard approach adopts the branch methodology in which the branches represent all expected combinations of operational conditions as a function of burnup (main branch). The main branch is produced for constant, usually averaged, operating conditions (e.g. coolant density). As a result, the spectral history effects that associated with coolant density variation are not taken into account properly. Number of methods to solve this problem (such as micro-depletion and spectral indexes) were developed and implemented in modern nodal codes. Recently, we proposed a new and robust method to account for history effects. The methodology was implemented in DYN3D and involves modification of the few-group cross section sets. The method utilizes the local Pu-239 concentration as an indicator of spectral history. The method was verified for PWR and VVER applications. However, the spectrum variation in BWR core is more pronounced due to the stronger coolant density change. The purpose of the current work is investigating the applicability of the method to BWR analysis. The proposed methodology was verified against recently developed BGCore system, which couples Monte Carlo neutron transport with depletion and thermal-hydraulic solvers and thus capable of providing a reference solution for 3D simulations. The results clearly show that neglecting the spectral history effects leads to a very large deviation (e.g. 1700pcm in multiplication factor) from the reference solution. Application of the Pu-correction method results in a very good agreement between DYN3D and BGCore on the order of 200pcm in kinf.","Y. Bilodid and D. Kotlyar and M. Margulis and E. Fridman and E. Shwageraus",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Does a Written Tool to Guide Structured Debriefing Improve Discourse? Implications for Interprofessional Team Simulation","PURPOSE
Timely debriefing following a simulated event supports learners in critically reflecting on their performance and areas for improvement. Content of debriefing has been shown to affect learner skill acquisition and retention. The use of good judgment statements from debriefing facilitators is considered superior to judgmental or nonjudgmental statements. Ideally, the majority of the conversation will consist of learner self-reflection and focused facilitation rather than directive performance feedback. We hypothesized that the introduction of a written tool to help facilitate high-quality debriefing techniques could improve the ratio of judgmental, nonjudgmental, and good judgment statements from facilitators, as well as shift the percentage of talk in the debrief away from directive performance feedback and toward self-assessment and focused facilitation.
METHODS
The University of Wisconsin Joint Trauma Simulation Program is an interdisciplinary project to improve quality of trauma care through simulation. Simulations use teams of five trauma trainees: two surgery residents, an emergency medicine resident, and two nurses. Three faculty members conducted the scenarios and debriefings. Debriefings were video recorded. Videos were transcribed and dialogue analyzed according to the teaching/learning strategy used in each turn of talk. Discourse was coded into three categories: (1) learner self-assessment; (2) focused facilitation; and (3) directive performance feedback. Each facilitation statement was coded as either (1) judgmental; (2) nonjudgmental, or (3) good judgment. The TEAM Debrief Tool is a written guide designed to help facilitators adhere to best practices, with example structure and phrasing, similar to the Promoting Excellence and Reflective Learning in Simulation tool. Pre- and post-implementation analysis was completed to assess for efficacy of the tool.
RESULTS
Seven videos before the implementation of the tool and seven videos after implementation were analyzed. The percentage of learner self-assessment increased significantly with tool use (7.23% vs 24.99%, p = 0.00004), and directive performance feedback decreased significantly (56.13% vs 32.75%, p = 0.0042). There was no significant change in the percentage of talk using focused facilitation. After implementation of the tool, there was a significant decrease in use of the nonjudgmental debriefing style (60.63% vs 37.31%, p = 0.00017), and a significant increase in the use of good judgment debriefing (38.77% vs 59.82%, p = 0.00038). There was also a slight increase in judgmental debriefing (0.60% vs 2.87%, p = 0.0027).
CONCLUSIONS
The discourse in our interprofessional trauma simulation debriefings unaided by a written debriefing tool skewed heavily toward direct performance feedback, with a preponderance of nonjudgmental statements. After introduction of the tool, dialogue shifted significantly toward learner self-assessment, and there was a large increase in utilization of debriefing with good judgment. This shift toward higher quality debriefing styles demonstrates the utility of such a tool in the debriefing of interprofessional simulations.","Ryan Thompson and Sarah Sullivan and Krystle Campbell and Ingie Osman and Brianna Statz and Hee Soo Jung",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Using simulation with interprofessional team training to improve RRT/code performance","Early recognition of and response to changes in patients’ conditions are a National Patient Safety Goal. Rapid Response Teams (RRTs) are one safety strategy aimed at early recognition of signs and symptoms of clinical deterioration and reduction in rates of cardiopulmonary arrest and death in hospitalized patients. Mock codes and RRTs are another strategy for improving outcomes.The Corporal Michael J. Crescenz VA Medical Center(CMCVAMC) used data from the American Heart Association National Registry of Cardio-Pulmonary Resuscitation to create an interprofessional, collaborative program using simulation. The program included: review of emergency responses and hands-on sessions with crash cart equipment, airway management, and BLS skills, followed by a mock RRT and Code with debriefing. Participants in this quality improvement initiative were nurses, physicians, anesthetists, pharmacists, and respiratory therapists. They evaluated the simulation as a positive learning experience. Staff and patient outcomes were improved after the program. The program engaged staff and promoted interprofessional collaboration that may ultimately improve the quality of patient care.","Patricia Dillon and Helene Moriarty and Gregg Lipschik",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Maximal granularity structure and generalized multi-view discriminant analysis for person re-identification","This paper proposes a novel descriptor called Maximal Granularity Structure Descriptor (MGSD) for feature representation and an effective metric learning method called Generalized Multi-view Discriminant Analysis based on representation consistency (GMDA-RC) for person re-identification (Re-ID). The proposed descriptor of MGSD captures rich local structural information from overlapping macro-pixels in an image, analyzes the horizontal occurrence of multi-granularity and maximizes the occurrence to extract a robust representation for viewpoint changes. As a result, the proposed descriptor of MGSD can obtain rich person appearance whilst being robust against different condition changes. Besides, considering multi-view information, we present a new GMDA-RC for different views, inspired by the observation that different views share similar data structures. The proposed metric learning method of GMDA-RC seeks multiple discriminant common spaces for multiple views by jointly learning multiple view-specific linear transforms. Finally, we evaluate the proposed method of (MGSD+GMDA-RC) on three publicly available person Re-ID datasets: VIPeR, CUHK-01 and Wide Area Re-ID dataset (WARD). For the VIPeR and CUHK-01, the experimental results show that our method significantly outperforms the state-of-the-art methods, achieving the rank-1 matching rates of 67.09%, 70.61%, and the improvements of 17.41%, 5.34%, respectively. For the WARD, we consider different pairwise camera views (camera 1–2, camera 1–3, camera 2–3) and our method can achieve the rank-1 matching rates of 64.33%, 59.42%, 70.32%, increasing of 5.68%, 11.04%, 9.06% compared with the state-of-the-art methods, respectively.","Cairong Zhao and Xuekuan Wang and Duoqian Miao and Hanli Wang and Weishi Zheng and Yong Xu and David Zhang",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Temporal ordering of substitutions in RNA evolution: Uncovering the structural evolution of the Human Accelerated Region 1","The Human Accelerated Region 1 (HAR1) is the most rapidly evolving region in the human genome. It is part of two overlapping long non-coding RNAs, has a length of only 118 nucleotides and features 18 human specific changes compared to an ancestral sequence that is extremely well conserved across non-human primates. The human HAR1 forms a stable secondary structure that is strikingly different from the one in chimpanzee as well as other closely related species, again emphasizing its human-specific evolutionary history. This suggests that positive selection has acted to stabilize human-specific features in the ensemble of HAR1 secondary structures. To investigate the evolutionary history of the human HAR1 structure, we developed a computational model that evaluates the relative likelihood of evolutionary trajectories as a probabilistic version of a Hamiltonian path problem. The model predicts that the most likely last step in turning the ancestral primate HAR1 into the human HAR1 was exactly the substitution that distinguishes the modern human HAR1 sequence from that of Denisovan, an archaic human, providing independent support for our model. The MutationOrder software is available for download and can be applied to other instances of RNA structure evolution.","Maria Beatriz Walter Costa and Christian Höner zu Siederdissen and Dan Tulpan and Peter F. Stadler and Katja Nowick",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Dedicated Educational Nursing Unit: Clinical Instructors Role Perceptions and Learning Needs","Over the past decade, health care leaders have called for a radical transformation in health care and nursing education. Patient care has become complex, demanding succinct interprofessional communication and collaboration to optimize the care of the patient, and the nurse at the bedside is the optimal leader at the point of care. Assistance with the clinical reasoning and critical thinking with nursing students is pivotal for successful patient outcomes. The expert clinical nurse at the bedside is the premier faculty to guide the young practitioner in the care of the patient. A dedicated educational unit (DEU) is an example of an academic–practice partnership designed to provide students with a positive clinical learning environment. The purpose of this qualitative research study was to identify the role perceptions of staff nurse's participating as clinical instructors on a DEU and the perceived educational learning needs of the experienced staff nurses. After Veterans Affairs Boston Healthcare System Institutional Review Board approval, a total of 8 nurses serving in the role of clinical instructor on a DEU participated in the study. Content analyses were used to code and synthesize common theses from the interviews. The themes that emerged related to role perception were mentoring, ensuring competency with basic skills and tasks, and development of critical thinking in nursing clinical education. The themes related to perceived learning needs of staff nurses related to the role of clinical instructor were the need for clear objectives from the academic affiliate, more coordination and acknowledgement by the academic affiliate, and addition education in dealing with students with diverse learning needs and accommodations.","Donna M. Glynn and Cecilia McVey and Judith Wendt and Bonnie Russell",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Validation of the MCU-PTR computational model of beryllium poisoning using selected experiments at the IRT-T research reactor","This paper presents the results of the validation of the MCU-PTR code computational model of beryllium poisoning caused by 3He and 6Li buildup against selected experiments at the IRT-T research reactor in the National Research Tomsk Polytechnic University. Calculations including the reflector irradiation history modeling for the entire reactor lifetime were performed using the continuous energy Monte Carlo code MCU–PTR. The measured reactivity loss (∼3.2%Δk/k) after the reactor shutdown period of 672 days and the reactivity increase (0.9%Δk/k) after replacement of the part of the reflector beryllium blocks by fresh blocks were compared with the calculated results. The impact of the parameters of beryllium poisoning model on the calculated results was investigated. The calculation underestimates the reactivity loss caused by 3He buildup during the 672-day shutdown by approximately 0.5%Δk/k.","M.V. Shchurovskaya and V.P. Alferov and N.I. Geraskin and A.I. Radaev and A.G. Naymushin and Yu.B. Chertkov and M.N. Anikin and I.I. Lebedev",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mitochondrial genotype modulates mtDNA copy number and organismal phenotype in Drosophila","We evaluated the role of natural mitochondrial DNA (mtDNA) variation on mtDNA copy number, biochemical features and life history traits in Drosophila cybrid strains. We demonstrate the effects of both coding region and non-coding A+T region variation on mtDNA copy number, and demonstrate that copy number correlates with mitochondrial biochemistry and metabolically important traits such as development time. For example, high mtDNA copy number correlates with longer development times. Our findings support the hypothesis that mtDNA copy number is modulated by mtDNA genome variation and suggest that it affects OXPHOS efficiency through changes in the organization of the respiratory membrane complexes to influence organismal phenotype.","Tiina S. Salminen and Marcos T. Oliveira and Giuseppe Cannino and Päivi Lillsunde and Howard T. Jacobs and Laurie S. Kaguni",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The hazard prediction test: A comparison of free-response and multiple-choice formats","Hazard perception skill is often related to lower crash risk, and the hazard perception test has been widely employed to measure this ability in drivers. An increasingly popular test-variant is the hazard prediction test: driving videos are occluded immediately prior to a hazard and participants are asked to predict how the situation will develop. Early versions of this test asked participants to provide a free-response answer which was subsequently coded. Later versions, however, have used a multiple-choice format where participants are provided with four options presented on screen. While the benefits of a multiple-choice format are obvious in terms of providing immediate feedback without relying on subjective coding, it is unclear whether this change in format affects the discriminative validity of the test. For the current study, a free-response test and a multiple-choice test were created using the same video clips. The free-response test (experiment 1) was found to successfully discriminate between novice and experienced drivers, with the latter predicting more hazards correctly. The answers provided by participants in Experiment 1 were then used to generate the options for a multiple-choice test (experiment 2). This second test was also found to discriminate between novice and experienced drivers, and a comparison between the two tests failed to reveal an advantage for one over the other. Despite this, correlations between prediction accuracy and both years of post-license driving, and annual mileage, were only significant for the multiple-choice test. The results suggest that the multiple-choice format is not only time- and cost-efficient, but is ostensibly as good as the free-response test in discriminating between driver groups.","Petya Ventsislavova and David Crundall",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"F-TRIDYN: A Binary Collision Approximation code for simulating ion interactions with rough surfaces","Fractal TRIDYN (F-TRIDYN) is a modified version of the widely used Monte Carlo, Binary Collision Approximation code TRIDYN that includes an explicit model of surface roughness and additional output modes for coupling to plasma edge and material codes. Surface roughness plays an important role in ion irradiation processes such as sputtering; roughness can significantly increase the angle of maximum sputtering and change the maximum observed sputtering yield by a factor of 2 or more. The complete effect of surface roughness on sputtering and other ion irradiation phenomena is not completely understood. Many rough surfaces can be consistently and realistically modeled by fractals, using the fractal dimension and fractal length scale as the sole input parameters. F-TRIDYN includes a robust fractal surface algorithm that is more computationally efficient than those in previous fractal codes and which reproduces available experimental sputtering data from rough surfaces. Fractals provide a compelling path toward a complete and concise understanding of the effect that surface geometry plays on the behavior of plasma-facing materials. F-TRIDYN is a flexible code for simulating ion-solid interactions and coupling to plasma and material codes for multiscale modeling.","Jon Drobny and Alyssa Hayes and Davide Curreli and David N. Ruzic",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Solution of the Skyrme–HF+BCS equation on a 3D mesh, II: A new version of the Ev8 code","We describe a new version of the Ev8 code that solves the nuclear Skyrme–Hartree–Fock+BCS problem using a 3-dimensional cartesian mesh. Several new features have been implemented with respect to the earlier version published in 2005. In particular, the numerical accuracy has been improved for a given mesh size by (i) implementing a new solver to determine the Coulomb potential for protons, and (ii) implementing a more precise method to calculate the derivatives on a mesh that had already been implemented earlier in our beyond-mean-field codes. The code has been made very flexible to enable the use of a large variety of Skyrme energy density functionals that have been introduced in the last years. Finally, the treatment of the constraints that can be introduced in the mean-field equations has been improved. The code Ev8 is today the tool of choice to study the variation of the energy of a nucleus from its ground state to very elongated or triaxial deformations with a well-controlled accuracy.
Program summary
Program title: Ev8 Catalogue identifier: ADWA_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ADWA_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 29956 No. of bytes in distributed program, including test data, etc.: 235072 Distribution format: tar.gz Programming language: FORTRAN-90. Computer: AMD Opteron 6274, AMD Opteron 6134, AMD Opteron 2378, Intel Core i7-4700HQ. Operating system: Unix, Linux, OS X. RAM: On the order of 64 megabytes for the examples provided. Classification: 17.22. Catalogue identifier of previous version: ADWA_v1_0 Journal reference of previous version: Comput. Phys. Comm. 171(2005)49 Does the new version supersede the previous version?: Yes, but when used in the same conditions both codes give the same result. Nature of problem: By means of the Hartree–Fock+BCS method for Skyrme-type energy density functionals, Ev8 allows the study of the evolution of the binding energy of even–even atomic nuclei for various shapes determined by the most general quadrupole and monopole constraints. Solution method: The program expands the single-particle wave-functions on a 3D Cartesian mesh. The nonlinear mean-field equations are solved by the imaginary time step method. A quadratic constraint is used to obtain states corresponding to given values of the monopole and quadrupole operators. Reasons for new version: The code has been generalized in several directions. The main changes concern the energy density functional that is more general than previously (including now tensor terms) and the accuracy of the final result that has been significantly improved by a new algorithm to determine the Coulomb potential. Several other changes should make the code more user friendly than it was before. Summary of revisions: 1. Skyrme energy functionals with tensor terms; 2. Improved accuracy for calculating derivatives; 3. Improved accuracy for solving Coulomb problem; 4. Improvement of the numerics of constraints; Restrictions: Ev8 assumes time-reversal invariance and nuclear shapes exhibiting three plane-reflection symmetries. Pairing correlations are treated at the BCS level of approximation. Running time: A few minutes for the examples provided, which concern rather heavy nuclei in modest boxes with an initial guess of Nilsson wavefunctions.","W. Ryssens and V. Hellemans and M. Bender and P.-H. Heenen",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Simulation of boil-off losses during transfer at a LH2 based hydrogen refueling station","Losses along the LH2 pathway are intrinsic to the utilization of a cryogenic fluid. They occur when the fluid is transferred between 2 vessels (liquefaction plant to trailer, trailer to station storage, station storage to pump or compressor, then possibly onto fuel cell electric vehicles …) and when it is warmed up due to heat transfer with the environment. Those losses can be estimated with good accuracy using thermodynamic models based on the conservation of mass and energy, provided that the thermodynamic states are correctly described. Indeed, the fluid undergoes various changes as it moves along the entire pathway (2 phase transition, sub-cooled liquid phase, super-heated warming, non-uniform temperature distributions across the saturation film) and accurate equations of state and 2 phase behavior implementations are essential. The balances of mass and energy during the various dynamics processes then enable to quantify the boil-off losses. In this work, a MATLAB code previously developed by NASA to simulate rocket loading is used as the basis for a LH2 transfer model. This code implements complex physical phenomena such as the competition between condensation and evaporation and the convection vs. conduction heat transfer as a function of the relative temperatures on both sides of the saturated film. The original code was modified to consider real gas equations of state, and some semi-empirical relationships, such as between the heat of vaporization and the critical temperature, were replaced by a REFPROP equivalent expression, assumed to be more accurate. Non-constant liquid temperature equations were added to simulate sub-cooled conditions. The model shows that under environmental heat transfer only the liquid phase of a LH2 vessel would experience cooling, while the boil-off is mainly a result of evaporation from the saturation film onto the vapor phase. Under the conditions assumed for this work, it was also concluded that the actual LH2 density was lower than the corresponding saturation density given by the working pressure of the vessel. During a bottom fill transfer, for example from a LH2 trailer to an on-site stationary vessel, it is shown that the boil-off losses are due to the compression of the vapor phase (“pdV” force). The model indicates that the magnitude of those losses is not dependent on the regulated pressure in the receiving vessel but is rather a function of the initial pressure in the vessel, amounting to more than 12% of losses for a vessel initially at 100 psia. At last, the model is used to estimate the amount of vapor H2 vented when depressurizing a LH2 trailer following a LH2 delivery.","Guillaume Petitpas",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Information transmission and the oral tradition: Evidence of a late-life service niche for Tsimane Amerindians","Storytelling can affect wellbeing and fitness by transmitting information and reinforcing cultural codes of conduct. Despite their potential importance, the development and timing of storytelling skills, and the transmission of story knowledge have received minimal attention in studies of subsistence societies that more often focus on food production skills. Here we examine how storytelling and patterns of information transmission among Tsimane forager-horticulturalists are predicted by the changing age profiles of storytellers’ abilities and accumulated experience. We find that storytelling skills are most developed among older adults who demonstrate superior knowledge of traditional stories and who report telling stories most. We find that the important information transmitted via storytelling typically flows from older to younger generations, and stories are primarily learned from older same-sex relatives, especially grandparents. Our findings suggest that the oral tradition provides a specialized late-life service niche for Tsimane adults who have accumulated important experience and knowledge relevant to foraging and sociality, but have lost comparative advantage in other productive domains. These findings may help extend our understanding of the evolved human life history by illustrating how changes in embodied capital predict the development of information transmission services in a forager-horticulturalist economy.","Eric Schniter and Nathaniel T. Wilcox and Bret A. Beheim and Hillard S. Kaplan and Michael Gurven",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Visualizing Time-based Weighted Coupling Using Particle Swarm Optimization to Aid Program Comprehension","By knowing software coupling, developers can get better view of the software quality and improve their productivity in development and maintenance. This paper presents a method to visualize coupling network that are often very complex, using heuristic approach based on particle swarming optimization. Each node is placed randomly and assigned with initial speed. Node that are coupled together will be attracted each other and trying to get closer until they reach a particular distance. This distance is determined from the coupling value of two nodes. A closely related nodes will move closer until reaching a short distance. On each iteration, node position is dynamically updated based on attraction and repulsive force around them. Thus, gradually forming a near best solution of logical coupling graph. The coupling values are measured by mining the association rule from changes history. A software development project sometimes can be very active, updates happen within minutes. Sometimes it becomes slow with weekly or even monthly updates. Time-based weighted analysis method was used to accommodate these time sensitive situations. A co-change in a short duration will be weighted more than co-changes that happen in longer duration.","Rully Agus Hendrawan and Katsuhisa Maruyama",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Food Insecurity Experience: Building Empathy in Future Food and Nutrition Professionals","Objective
To assess changes in empathy in students completing a food insecurity experience.
Design
Mixed methods; quantitative data from survey in years 1 and 2; qualitative data extracted from students' workbooks in years 2–5. This study was conducted over 10 weeks annually for 5 years.
Setting
Northwest US land-grant university.
Participants
Students enrolled in a community nutrition course who chose to complete the food insecurity exercise. Total included 58 students in quantitative analysis in years 1 and 2 and 119 in qualitative analysis, years 2–5.
Intervention(s)
The intervention was a food insecurity experience in which participants spent no more than $3/d on food for 5 days ($15 total) while striving for a nutritious diet and reflecting on their experience.
Main Outcome Measures
Empathy scores measured by Likert scales; participant responses and reflections recorded in workbook journals.
Analysis
Comparison of means across time using paired t tests (P < .05); coding and sorting themes from workbook journals.
Results
Quantitative findings indicated that both classroom content and experiential exercises were important for enhancing empathy about food insecurity. Empathy scores increased from time I to time II and from time I to time III. Qualitative reflections among participants included terms such as guilt, empathy, compassion, and raised consciousness about food insecurity.
Conclusions and Implications
Experiential and transformational learning to develop empathy can take place in a 5-day food insecurity experience during a typical university-level community nutrition course. This intervention can be tested for applications in other contexts.","Alison Harmon and Kara Landolfi and Carmen Byker Shanks and Leanna Hansen and Laura Iverson and Melody Anacker",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A coupled approach for vehicle brake cooling performance simulations","Advances of CFD methods together with the constant growth of computer capacity enables simulations of complex coupled fluid and thermal problems. One such problem is the evaluation of brake cooling performance. The brake system is a critical component for passenger vehicles and ensuring correct brake operation under all possible load scenarios is a safety issue. An accurate prediction of convection, conduction and radiation heat fluxes for such a complicated system is challenging from modelling as well as numerical efficiency perspectives. This study describes a simulation procedure developed to numerically predict brake system component temperatures during a downhill brake performance test. Such tests have stages of forced and natural convection, and therefore, the airflow is influenced by the temperature changes within the system. For the numerical simulation, a coupled approach is utilized by combining aerodynamic and thermal codes. The aerodynamic code computes the convective heat transfer using a fully-detailed vehicle model in the virtual wind tunnel. The thermal code then uses this data and combines it with conduction and radiation calculations to give an accurate prediction of the component temperatures, which are subsequently used for airflow recalculation. The procedure is described in considerable detail for most parts of the setup. The calculated temperature history results are validated against experimental data and show good agreement. The method allows detailed investigations of distribution and direction of the heat fluxes inside the system, and of how these fluxes are affected by changes in material properties as well as changes in parts within or outside the brake system. For instance, it is shown that convection and especially convection from the inner vanes is the main contributor for the heat dissipation from the brake disc. Finally, some examples of how changing the vehicle design affects the brake cooling performance are also discussed.","Alexey Vdovin and Mats Gustafsson and Simone Sebben",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Energy use implications of different design strategies for multi-storey residential buildings under future climates","The effects of climate change on the final and primary energy use of versions of a multi-storey residential building have been analysed. The building versions are designed to the Swedish building code (BBR 2015) and passive house criteria (Passive 2012) with different design and overheating control strategies under different climate scenarios. Future climate datasets are based on Representative Concentration Pathway scenarios for 2050–2059 and 2090–2099. The analysis showed that strategies giving the lowest space heating and cooling demands for the Passive 2012 building version remained the same under all climate scenarios. In contrast, strategies giving the lowest space heating and cooling demands for the BBR 2015 version varied, as cooling demand became more significant under future climate scenarios. Cooling demand was more dominant than heating for the Passive 2012 building version under future climate scenarios. Household equipment and technical installations based on best available technology gave the biggest reduction in total primary energy use among considered strategies. Overall, annual total operation primary energy decreased by 37–54% for the building versions when all strategies are implemented under the considered climate scenarios. This study shows that appropriate design strategies could result in significant primary energy savings for low-energy buildings under changing climates.","Uniben Yao Ayikoe Tettey and Ambrose Dodoo and Leif Gustavsson",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Centrality bias measure for high density QR code module recognition","High density bar codes are very popular today due to large storage capacity and small code area. But unfortunately, high density versions of QR codes are not being used due to reading problems with most smartphones and flatbed scanner QR code applications. Due to frequent changes and small sizes of the black and white modules, there are reading problems in the QR code binarization and tilt correction processes. The binarization method sets the global or local threshold, and binarizes each pixel separately, that is why they are sensitive to print-and-scan distortion and luminosity. In this paper, we focus on the recognition of high density QR codes. We propose to use the centrality bias of each module to improve the module recognition results. This measure has been used for proposed classification methods and for standard QR code recognition methods. In both cases, the recognition rate was improved, as confirmed by the experimental results.","Iuliia Tkachenko and William Puech and Olivier Strauss and Jean-Marc Gaudin and Christophe Destruel and Christian Guichard",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Electronic stopping in oxides beyond Bragg additivity","We present stopping cross sections calculated by our PASS code for several ions in metal oxides and SiO2 over a wide energy range. Input takes into account changes in the valence structure by assigning two additional electrons to the 2p shell of oxygen and removing the appropriate number of electrons from the outer shells of the metal atom. Results are compared with tabulated experimental values and with two versions of Bragg’s additivity rule. Calculated stopping cross sections are applied in testing a recently-proposed scaling rule, which relates the stopping cross section to the number of oxygen atoms per molecule.","P. Sigmund and A. Schinner",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mapping Abbreviated Injury Scale data from 1990 to 1998 versions: A stepping-stone in the contemporary evaluation of trauma","Introduction
Many trauma registries have used the 1990 revision of the Abbreviated Injury Scale (AIS; AIS90) to code injuries sustained by trauma patients. Due to changes made to the AIS codeset since its release, AIS90-coded data lacks currency in the assessment of injury severity. The ability to map between the 1998 revision of AIS (AIS98) and the current (2008) AIS version (AIS08) already exists. The development of a map for transforming AIS90-coded data into AIS98 would therefore enable contemporary injury severity estimates to be derived from AIS90-coded data.
Methods
Differences between the AIS90 and AIS98 codesets were identified, and AIS98 maps were generated for AIS90 codes which changed or were not present in AIS98. The effectiveness of this map in describing the severity of trauma using AIS90 and AIS98 was evaluated using a large state registry dataset, which coded injury data using AIS90 over several years. Changes in Injury Severity Scores (ISS) calculated using AIS90 and mapped AIS98 codesets were assessed using three distinct methods.
Results
Forty-nine codes (out of 1312) from the AIS90 codeset changed or were not present in AIS98. Twenty-four codes required the assignment of maps to AIS98 equivalents. AIS90-coded data from 78,075 trauma cases were used to evaluate the map. Agreement in calculated ISS between coded AIS90 data and mapped AIS98 data was very high (kappa=0.971). The ISS changed in 1902 cases (2.4%), and the mean difference in ISS across all cases was 0.006 points. The number of cases classified as major trauma using AIS98 decreased by 0.8% compared with AIS90. A total of 3102 cases (4.0%) sustained at least one AIS90 injury which required mapping to AIS98.
Conclusions
This study identified the differences between the AIS90 and AIS98 codesets, and generated maps for the conversion process. In practice, the differences between AIS90- and AIS98-coded data were very small. As a result, AIS90-coded data can be mapped to the current AIS version (AIS08) via AIS98, with little apparent impact on the functional accuracy of the mapped dataset produced.","Cameron S. Palmer and Jacelle Lang and Glen Russell and Natalie Dallow and Kathy Harvey and Belinda Gabbe and Peter Cameron",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Investigation on the potential use of thorium as fuel for the Sodium-cooled Fast Reactor","Generation IV reactors are planned to be an evolutionary step in the history of nuclear power plants. Although they have many advantageous properties, five of the six concepts are designed for the utilization of uranium–plutonium fuel. In this paper, the Sodium-cooled Fast Reactor is investigated regarding the potential application of thorium. The basis of the investigation is the European concept of SFR, which is studied with thorium-containing fuel compositions. Two different approaches are presented with the results of several full-core burnup calculations performed by the Monte Carlo code Serpent 2. The presented results are the multiplication factor changes, delayed neutron fractions, fuel temperature and void coefficients and fissile isotope vectors as well. The results can help to determine how 233U could be produced in this type of reactors and how it could be used as alternative fuel in SFR.","H. György and Sz. Czifrus",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Exploring the relations between epistemic beliefs, emotions, and learning from texts","Conflicting claims about important socio-scientific debates are proliferating in contemporary society. It is therefore important to understand the individual characteristics that predict learning from conflicting claims. We explored individuals’ beliefs about the nature of knowledge and knowing (i.e., epistemic beliefs) and their emotions as potentially interrelated sets of learner characteristics that predict learning in such contexts. Undergraduate university students (N=282) self-reported their topic-specific epistemic beliefs and were given three conflicting texts about climate change to read. Immediately after each of the three texts, participants self-reported the emotions they experienced. Following reading and self-report, participants wrote summaries of the conflicting texts. Text-mining and human coding were applied to summaries to construct two indices of learning from conflicting texts that reflected which source’s information is privileged in memory. Results from structural equation modeling revealed that epistemic beliefs were consistent in their predictions of emotions, which in turn variously predicted different learning outcomes. In particular, a belief that knowledge is justified by inquiry predicted surprise and curiosity, which at times facilitated learning. In contrast, confusion, predicted by passive reliance on external sources, related to impaired memory of conflicting content. Theoretical and methodological implications are discussed for research on the relations between epistemic beliefs, emotions, and learning about controversial topics.","Gregory J. Trevors and Krista R. Muis and Reinhard Pekrun and Gale M. Sinatra and Marloes M.L. Muijselaar",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Adaptively post-encoding multiple description video coding","In video coding and transmission, the network conditions are crucial, i.e., bandwidth, delay and other factors. However, how to effectively take these factors into account is still a challenge, as heterogeneous networks are dynamic, and therefore it is difficult to predict their changes. This paper first reports a post-encoding of Scalable Multiple Description Coding (SMDC), which is towards self-adaptive video delivery under different conditions of the network. The proposed scheme contains three major steps: (1) spatiotemporal wavelet transformation of input video sequence; (2) context-based adaptive binary arithmetic coding; and (3) rate allocation under the conditions of the network and analysis of the principle relationship of the rate–distortion slope ratio with packet-loss probability in network links. The performance of the SMDC is compared with that of scalable video coding and scalable H.264, and SMDC is demonstrated to effectively optimize the video delivery to adapt to the dynamics of heterogeneous networks. Part of this work has been published on Data Compression Conference 2009 as a short abstract version [37].","Xuguang Lan and Meng Yang and Yuan Yuan and Songlin Zhao and Nanning Zheng",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Face recognition algorithm based on discriminative dictionary learning and sparse representation","In order to overcome the defect that the face recognition (FR) rate is greatly reduced in the existing uncontrolled environments such as the change of illumination, occlusion, and posture, etc, Face recognition algorithm based on discriminative dictionary learning and regularized robust coding was proposed. In this proposed algorithm, the Gabor amplitude images of a face image are obtained via using Gabor filter at first, then we extract the uniform local binary histogram and use Fisher criterion to gain a new dictionary, finally the test image is classified as the existing class via sparse representation Coding. The experimental results obtained from Extended Yale B databases and AR databases show that the proposed algorithm has higher face recognition rate in the existing uncontrolled environments in comparison with K-SVD, LC-K-SVD, FDDL and so on.","Zhenyu Lu and  Linghua Zhang",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Associative Fear Learning Enhances Sparse Network Coding in Primary Sensory Cortex","Summary
Several models of associative learning predict that stimulus processing changes during association formation. How associative learning reconfigures neural circuits in primary sensory cortex to “learn” associative attributes of a stimulus remains unknown. Using 2-photon in vivo calcium imaging to measure responses of networks of neurons in primary somatosensory cortex, we discovered that associative fear learning, in which whisker stimulation is paired with foot shock, enhances sparse population coding and robustness of the conditional stimulus, yet decreases total network activity. Fewer cortical neurons responded to stimulation of the trained whisker than in controls, yet their response strength was enhanced. These responses were not observed in mice exposed to a nonassociative learning procedure. Our results define how the cortical representation of a sensory stimulus is shaped by associative fear learning. These changes are proposed to enhance efficient sensory processing after associative learning.","Amos Gdalyahu and Elaine Tring and Pierre-Olivier Polack and Robin Gruver and Peyman Golshani and Michael S. Fanselow and Alcino J. Silva and Joshua T. Trachtenberg",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Shifting attitudes towards research and evidence-based medicine within the naturopathic medical community: The power of people, money and acceptance","Background
Naturopathic medicine (NM) is a distinct system of primary health care often considered a form of complementary and alternative medicine. Evidence-based medicine (EBM) is an approach to medicine which has gained increasing prominence over the past 2 decades. Like other health professions, as the influence of EBM grew in the global medical community, NM had to discover, explore and take stock of it through the lenses of its own unique culture, history and values. We conducted a phenomenological qualitative research study to explore attitudes toward EBM, probe for evidence of cultural change, and to investigate the drivers of said change within the naturopathic medical community.
Methods
Participants were selected by purposive sampling and interviews were based on semi-structured questionnaires focusing on the participants’ perceptions of research, EBM, and their relationship to the field of naturopathic medicine. All interviews were transcribed and then coded independently and in duplicate by two investigators who assigned thematic codes to relevant excerpts. Themes and a concept map were identified, reviewed, and analyzed by investigators. Atlas.ti (version 6.2) software was used for coding and concept mapping.
Results
Seventeen interviews were conducted of which 15 were available for transcription and ranged in length from 17 to 55 minutes. A total of 34 codes were identified, which we aggregated into three themes: (1) a spectrum of EBM definitions, (2) attitudes towards research and EBM, and (3) drivers of change. Interviewees used a spectrum of definitions for EBM which informed their reported attitudes toward it. While current attitudes toward research and EBM were generally described as favorable, “spectrums,” “subgroups,” or even “factions” were described representing a continuum of attitudes within the naturopathic community. Overall, the interviewees described a rapid cultural shift in attitudes from hesitancy to the cautious embrace of research and EBM. Numerous promoters of this cultural change were described with the majority of interviewees emphasizing the importance of influential people within the profession, research and EBM funding, and the desire for acceptance from the larger medical community.
Discussion
As a profession which developed from vitalism on the margins of the larger medical community, naturopathic medicine has grown rapidly in size and influence and, as it has entered new non-vitalistic (mainstream) practice environments, it has incorporated new peers and new role models. Research and EBM acculturation may represent a flashpoint example of a professional adolescence for naturopathic medicine. In which case a relevant question becomes, will the profession be able to adequately integrate the values of its youth/roots with the values of the EBM-driven medical community?","Joshua Z. Goldenberg and Bonnie S. Burlingham and Jane Guiltinan and Erica B. Oberg",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Transformation of Arden Syntax's medical logic modules into ArdenML for a business rules management system","Introduction
Arden Syntax is a Health Level Seven International (HL7) standard language that is used for representing medical knowledge as logic statements. Arden Syntax Markup Language (ArdenML) is a new representation of Arden Syntax based on XML. Compilers are required to execute medical logic modules (MLMs) in the hospital environment. However, ArdenML may also replace the compiler. The purpose of this study is to demonstrate that MLMs, encoded in ArdenML, can be transformed into a commercial rule engine format through an XSLT stylesheet and made executable in a target system.
Methods
The target rule engine selected was Blaze Advisor. We developed an XSLT stylesheet to transform MLMs in ArdenML into Structured Rules Language (SRL) in Blaze Advisor, through a comparison of syntax between the two languages. The stylesheet was then refined recursively, by building and applying rules collected from the billing and coding guidelines of the Korean health insurance service. Two nurse coders collected and verified the rules and two information technology (IT) specialists encoded the MLMs and built the XSLT stylesheet. Finally, the stylesheet was validated by importing the MLMs into Blaze Advisor and applying them to claims data.
Results
The language comparison revealed that Blaze Advisor requires the declaration of variables with explicit types. We used both integer and real numbers for numeric types in ArdenML. “IF∼THEN” statements and assignment statements in ArdenML become rules in Blaze Advisor. We designed an XSLT stylesheet to solve this issue. In addition, we maintained the order of rule execution in the transformed rules, and added two small programs to support variable declarations and action statements. A total of 1489 rules were reviewed during this study, of which 324 rules were collected. We removed duplicate rules and encoded 241 unique MLMs in ArdenML, which were successfully transformed into SRL and imported to Blaze Advisor via the XSLT stylesheet. When applied to 73,841 outpatients’ insurance claims data, the review result was the same as that of the legacy system.
Conclusion
We have demonstrated that ArdenML can replace a compiler for transforming MLMs into commercial rule engine format. While the proposed XSLT stylesheet requires refinement for general use, we anticipate that the development of further XSLT stylesheets will support various rule engines.","Chai Young Jung and Jong-Ye Choi and Seong Jik Jeong and Kyunghee Cho and Yong Duk Koo and Jin Hee Bae and Sukil Kim",2018,"[""Science Direct""]","Rejeitado: CR8","Rejeitado: CR8"
"JaSTA-2: Second version of the Java Superposition T-matrix Application","In this article, we announce the development of a new version of the Java Superposition T-matrix App (JaSTA-2), to study the light scattering properties of porous aggregate particles. It has been developed using Netbeans 7.1.2, which is a java integrated development environment (IDE). The JaSTA uses double precision superposition T-matrix codes for multi-sphere clusters in random orientation, developed by Mackowski and Mischenko (1996). The new version consists of two options as part of the input parameters: (i) single wavelength and (ii) multiple wavelengths. The first option (which retains the applicability of older version of JaSTA) calculates the light scattering properties of aggregates of spheres for a single wavelength at a given instant of time whereas the second option can execute the code for a multiple numbers of wavelengths in a single run. JaSTA-2 provides convenient and quicker data analysis which can be used in diverse fields like Planetary Science, Atmospheric Physics, Nanoscience, etc. This version of the software is developed for Linux platform only, and it can be operated over all the cores of a processor using the multi-threading option.
New version program summary
Program Title: Java superposition T-matrix Application: version - 2.0 Program Files doi:http://dx.doi.org/10.17632/bbtjj8kd74.1 Licensing provisions: GPLv3 Programming language: Fortran, Java. External routines/libraries: jfreechart-1.0.14 [1] (free plotting library for java), j3d-jre-1.5.2 [2] StdDraw3D (3D visualization) Subprograms used: spline.f90, splinek.f90, wavesort.f90 Journal reference of previous version: Comput. Phys. Commun., 2014, 185, 2369 Does the new version supersede the previous version?: No. Nature of problem: Light scattering properties of cosmic dust aggregates Solution method: Java application based on Mackowski and Mishchenko’s Superposition T-Matrix code (1996). Reasons for the new version: The earlier version was mainly developed to calculate the optical properties of cosmic dust aggregates for a single wavelength in vacuum. The user had to calculate multiple times for different wavelengths to analyze the variation of different scattering parameters (e.g., phase function, polarization, the extinction efficiency, the absorption efficiency, the scattering efficiency, etc.) of aggregate particles for a range of wavelengths which was quite time-consuming. Therefore we have developed the new version of JaSTA with an ability to calculate the scattering parameters for a wide range of wavelength. In this new version, we have introduced the multi-threading option to distribute the multi-wavelength calculations to the maximum number of processing cores present in a computer. Hence the calculation time decreases considerably. Summary of revisions: Java superposition T-matrix App (JaSTA) [3] is a Java swing application developed to study the light scattering properties of cosmic dust aggregates based on the Mackowski & Mischenko’s Superposition T-matrix (STM) code [4]. This application calculates the polarization and other scattering matrix elements along with extinction, absorption, and scattering efficiencies for a single wavelength. JaSTA was solely devoted to the light scattering properties of cosmic dust aggregates. Cosmic dust includes comet dust, interplanetary dust, and interstellar dust. Many investigators studied the light scattering properties of comet dust [5]–[10], interplanetary dust [11], interstellar dust [12] using the superposition T-matrix code. JaSTA provided a much better platform for the STM code as a packaged software which can calculate light scattering properties of cosmic dust aggregates for a single wavelength in the vacuum with a click of a button and saves the results in a database so that the saved data can be re-utilized. Recently JaSTA has been used to compute the orientation-averaged scattering matrix elements for fractal aggregates of black carbon aerosols [13]. The interesting feature of the new version of JaSTA is that it can calculate the extinction efficiency at different wavelengths for a given size in a single run which is applicable in the study of interstellar extinction by aggregates at different wavelengths. To analyze the wavelength dependence of extinction one had to execute the calculation for several wavelengths by changing the wavelength and refractive indices in the input for each run in older version of JaSTA. This was very much time consuming and cumbersome. Hence this major drawback led us to rethink and re-establish JaSTA with the multi-wavelength option. JaSTA-2 is the second version of JaSTA aimed to provide the multi-wavelength facility along with the default single wavelength option. JaSTA-2 comes with some other two major updates: cubic spline interpolation of refractive index with wavelengths and multi-threading option for faster calculation. It is developed using Netbeans IDE and is available only for Linux OS. It uses jFreechart-1.0.14 java library to plot various graphs. The multi-wavelength feature will help us to extract the dependence of extinction efficiency on wavelength. Further information on the new features and applicability of JaSTA-2 are provided in the documentation file ‘ JaSTA-2_doc.pdf’, and ‘ readme.txt’ file which are available in the software package. [1]http://www.jfree.org/index.html[2]https://java3d.java.net/[3]P. Halder, A. Chakraborty, P. Deb Roy & H.S. Das, Computer Physics Communications, 185 (2014) 2369-2379.[4]D.W. Mackowski & M.I. Mischenko, J. Opt. Soc. Am. Am., 13 (1996) 2266-2278.[5]H. Kimura, L. Kolokolova & I. Mann, A&A, 407 (2003) L5-L8.[6]H. Kimura, L. Kolokolova & I. Mann, A&A, 449 (2006) 1243-1254.[7]H.S. Das, S.R. Das, T. Paul, A. Suklabaidya & A.K. Sen, MNRAS, 389 (2008) 787-791.[8]H.S. Das, S.R. Das & A.K Sen, MNRAS, 390 (2008) 1195-1199.[9]H.S. Das, A. Suklabaidya, S. Datta Majumder & A.K. Sen, Research in A&A, 10 (2010) 355-362[10]H.S. Das, D. Paul, A. Suklabaidya & A.K. Sen, MNRAS, 416 (2011) 94-100.[11]J. Lasue, A.C. Levasseur-Regourd, N. Fray & H. Cottin, A&A, 473 (2007) 641-649.[12]M. A. Iati, A. Giusto, R. Saija, F. Borghese, P. Denti, C. Cecchi-Pestellini & S. Aielo, ApJ, 615 (2004), 286.[13]A. Pandey & R. K. Chakrabarty, Optics Letters, 41 (2016) 3351-3354.","Prithish Halder and Himadri Sekhar Das",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Learning to know, be, do, and live together with in the cross-cultural experiences of immigrant teacher educators","This study examined three Afro-Caribbean immigrant teacher educators whose learning based on reflections about their experiences with teachers in the United States revealed how they developed knowledge beyond practice in their learning to know, do, be and live together with others. The educators' learning reflected the processes of observation, reflection, awareness, requesting student feedback in the moment, and the passing of time that resulted in adjustment to their body language, changes in their expectations of students, a modification in their communication, code-switching and sensitivity. Implications based on the study for the new kind of teacher educator are subsequently addressed.","Patriann Smith",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Learning-based approach for layered adaptive video streaming over SDN","Software-defined networking is a recently emerging paradigm that decouples the control and data planes of computer networks. It allows for the implementation of application-specific routing algorithms, and the advantages that the SDN architecture enables can be used to enhance the performance of multimedia communication applications. In this paper, we propose an adaptive video streaming system with a learning-based approach, running over SDN. In the proposed video streaming system, we use a novel learning model to determine the optimal time to re-route the traffic flows and to change the bitrate of the video. The learning model aims to minimize the packet loss rate, quality changes and controller cost while adapting the flow routes and video quality. We have tested the performance of the learning-based approach by comparing it to traditional Internet routing and the greedy approach. The results show that the proposed system significantly outperforms the traditional Internet routing approach and the greedy approach in terms of quality of experience (QoE) and network cost under different network scenarios.","Tuba Uzakgider and Cihat Cetinkaya and Muge Sayit",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"MC21 v.6.0 – A continuous-energy Monte Carlo particle transport code with integrated reactor feedback capabilities","MC21 is a continuous-energy Monte Carlo radiation transport code for the calculation of the steady-state spatial distributions of reaction rates in three-dimensional models. The code supports neutron and photon transport in fixed source problems, as well as iterated-fission-source (eigenvalue) neutron transport problems. MC21 has been designed and optimized to support large-scale problems in reactor physics, shielding, and criticality analysis applications. The code also supports many in-line reactor feedback effects, including depletion, thermal feedback, xenon feedback, eigenvalue search, and neutron and photon heating. MC21 uses continuous-energy neutron/nucleus interaction physics over the range from 10−5eV to 20MeV. The code treats all common neutron scattering mechanisms, including fast-range elastic and non-elastic scattering, and thermal- and epithermal-range scattering from molecules and crystalline materials. For photon transport, MC21 uses continuous-energy interaction physics over the energy range from 1keV to 100GeV. The code treats all common photon interaction mechanisms, including Compton scattering, pair production, and photoelectric interactions. All of the nuclear data required by MC21 is provided by the NDEX system of codes, which extracts and processes data from EPDL-, ENDF-, and ACE-formatted source files. For geometry representation, MC21 employs a flexible constructive solid geometry system that allows users to create spatial cells from first- and second-order surfaces. The system also allows models to be built up as hierarchical collections of previously defined spatial cells, with interior detail provided by grids and template overlays. Results are collected by a generalized tally capability which allows users to edit integral flux and reaction rate information. Results can be collected over the entire problem or within specific regions of interest through the use of phase filters that control which particles are allowed to score each tally. The tally system has been optimized to maintain a high level of efficiency, even as the number of edit regions becomes very large.","D.P. Griesheimer and D.F. Gill and B.R. Nease and T.M. Sutton and M.H. Stedry and P.S. Dobreff and D.C. Carpenter and T.H. Trumbull and E. Caro and H. Joo and D.L. Millman",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A linear approach for sparse coding by a two-layer neural network","Many approaches to transform classification problems from non-linear to linear by feature transformation have been recently presented in the literature. These notably include sparse coding methods and deep neural networks. However, many of these approaches require the repeated application of a learning process upon the presentation of unseen data input vectors, or else involve the use of large numbers of parameters and hyper-parameters, which must be chosen through cross-validation, thus increasing running time dramatically. In this paper, we propose and experimentally investigate a new approach for the purpose of overcoming limitations of both kinds. The proposed approach makes use of a linear auto-associative network (called SCNN) with just one hidden layer. The combination of this architecture with a specific error function to be minimized enables one to learn a linear encoder computing a sparse code which turns out to be as similar as possible to the sparse coding that one obtains by re-training the neural network. Importantly, the linearity of SCNN and the choice of the error function allow one to achieve reduced running time in the learning phase. The proposed architecture is evaluated on the basis of two standard machine learning tasks. Its performances are compared with those of recently proposed non-linear auto-associative neural networks. The overall results suggest that linear encoders can be profitably used to obtain sparse data representations in the context of machine learning problems, provided that an appropriate error function is used during the learning phase.","Alessandro Montalto and Giovanni Tessitore and Roberto Prevete",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Full 1-loop calculation of BR(Bs,d0→ℓℓ̄) in models beyond the MSSM with SARAH  and SPheno","We present the possibility of calculating the quark flavor changing neutral current decays Bs0→ℓℓ̄ and Bd0→ℓℓ̄ for a large variety of supersymmetric models. For this purpose, the complete one-loop calculation has been implemented in a generic form in the Mathematica package SARAH. This information is used by SARAH  to generate Fortran source code for SPheno  for a numerical evaluation of these processes in a given model. We comment also on the possibility to use this setup for non-supersymmetric models.
Program summary
Program title: SARAH Catalogue identifier: AEIB_v2_1 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIB_v2_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 467777 No. of bytes in distributed program, including test data, etc.: 3927691 Distribution format: tar.gz Programming language: Mathematica, Fortran. Computer: All computers which can run Mathematica and SPheno. Operating system: Linux, MacOS. Classification: 11.1, 11.6. Catalogue identifier of previous version: AEIB_v2_0 Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 1792 Does the new version supercede the previous version?: Yes, the new version includes all known features of previous versions but provides also the new features mentioned below. Nature of problem: Models beyond the SM can have new contributions to the decays of neutral B-mesons. For a precise prediction of the corresponding branching ratios a full 1-loop calculation including all possible wave, penguin and box diagrams is necessary. This usually requires a big effort and public codes for these calculations so far only support a few selected models. Solution method: The implementation of a new model in SARAH is easy and straightforward. In addition, SARAH is already delivered with many different supersymmetric and a few non-supersymmetric models. As a first step, SARAH derives the analytical expressions for masses, interactions and renormalization group equations for the given model. Furthermore, SARAH checks for all possible tree- and 1-loop diagrams which can contribute to the B-meson decays into two leptons. This information is exported to Fortran source code which can afterwards be compiled with SPheno. This generates a fully functional spectrum generator: besides the mass spectrum, sparticle and Higgs decays the new SPheno modules also calculate precision observables like the B-meson decays based on the parameters chosen by the user. Reasons for new version: The possible decays of neutral B-mesons into two leptons are constraining models beyond the Standard Model. SARAH allows, in the new version, the production of SPheno source code to calculate those decays at full 1-loop for a large variety of models. Summary of revisions: Full 1-loop calculation of Bs,d0→ℓℓ̄ for any model which can be implemented in SARAH. Restrictions: SARAH can only calculate the renormalization group equations for a supersymmetric model. Hence, for a non-supersymmetric model it is not possible to calculate the running parameters which enter the calculation. These have to be provided by the user as input. In addition, the effects of chiral resummation as well as next-to-leading order QCD corrections known for the MSSM are not included. Unusual features: This is the first public tool which allows a full 1-loop calculation of B-meson decays in more complicated models than the minimal or next-to-minimal supersymmetric standard model. Any new contribution in a renormalizable model stemming from an extended matter content or gauge sector is taken into account. Running time: Measured CPU time for the evaluation of the MSSM using a Lenovo Thinkpad X220 with i7 processor (2.53 GHz). Calculating the complete Lagrangian: 9 s. Calculating all vertices: 51 s. Output of the UFO model files: 49 s.","H. Dreiner and K. Nickel and W. Porod and F. Staub",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Generative software product line development using variability-aware design patterns","Software Product Lines (SPLs) are an approach to reuse in-the-large that models a set of closely related software systems in terms of commonalities and variabilities. Design patterns are best practices for addressing recurring design problems in object-oriented source code. In the practice of implementing SPL, instances of certain design patterns are employed to handle variability, which makes these “variability-aware design patterns” a best practice for SPL design. However, currently there is no dedicated method for proactively developing SPLs using design patterns suitable for realizing variable functionality. In this paper, we present a method to perform generative SPL development with design patterns. We use role models to capture design patterns and their relation to a variability model. We further allow mapping of individual design pattern roles to (parts of) implementation elements to be generated (e.g., classes, methods) and check the conformance of the realization with the specification of the pattern. We provide definitions for the variability-aware versions of the design patterns Observer, Strategy, Template Method and Composite. Furthermore, we support generation of realizations in Java, C++ and UML class diagrams utilizing annotative, compositional and transformational variability realization mechanisms. Hence, we support proactive development of SPLs using design patterns to apply best practices for the realization of variability. We realize our concepts within the Eclipse IDE and demonstrate them within a case study.","Christoph Seidl and Sven Schuster and Ina Schaefer",2017,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR9"
"Dynamic changes of resting state connectivity related to the acquisition of a lexico-semantic skill","The brain undergoes adaptive changes during learning. Spontaneous neural activity has been proposed to play an important role in acquiring new information and/or improve the interaction of task related brain regions. A promising approach is the investigation of resting state functional connectivity (rs-fc) and resting state networks, which rely on the detection of interregional correlations of spontaneous BOLD fluctuations. Using Morse Code (MC) as a model to investigate neural correlates of lexico-semantic learning we sought to identify patterns in rs-fc that predict learning success and/or undergo dynamic changes during a 10-day training period. Thirty-five participants were trained to decode twelve letters of MC. Rs-fMRI data were collected before and after the training period and rs-fc analyses were performed using a group independent component analysis. Baseline connectivity between the language-network (LANG) and the anterior-salience-network (ASN) predicted learning success and learning was associated with an increase in LANG – ASN connectivity. Furthermore, a disconnection between the default mode network (DMN) and the ASN as well as the left fusiform gyrus, which is critically involved in MC deciphering, was observed. Our findings demonstrate that rs-fc can undergo behaviorally relevant changes within 10 training days, reflecting a learning dependent modulation of interference between task specific networks.","L. Schlaffke and L. Schweizer and N.N. Rüther and R. Luerding and M. Tegenthoff and C. Bellebaum and T. Schmidt-Wilcke",2017,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"A brief history of parameterized matching problems","Parameterized pattern matching is a string searching variant that was initially defined to detect duplicate code but later proved to support several other applications. In particular, two equal-length strings X andY are a parameterized-match if there exists a bijective function g for which every text symbol in X is equal to g(Y). Baker was the first researcher to have addressed this problem (Baker, 1993) and, since then, many others have followed her work. She did, indeed, open up a wide field of extensive research. Over the years, many variants and extensions that have been pursued include: parameterized matching under edit and Hamming distances, parameterized multi-pattern matching, two dimensional parameterized matching, structural matching, function matching, and the very recent developments in succinct and streaming models. This accelerated research could only be justified by the usefulness of its practical applications such as in software maintenance, image processing and bioinformatics to name some. Even though the problem was posed about 25 years ago, research on parameterized matching is still very active. Its extensive study over the years and its current relevance motivate us to review the most notorious contributions as road map for current and future research.","Juan Mendivelso and Sharma V. Thankachan and Yoan Pinzón",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Metaphors of code—Structuring and broadening the discussion on teaching children to code","Digital technology has become embedded into our daily lives. Code is at the heart of this technology. The way code is perceived influences the way our everyday interaction with digital technologies is perceived: is it an objective exchange of ones and zeros, or a value- laden power struggle between white male programmers and those who think they are users, when they are, in fact, the product being sold. Understanding the nature of code thus enables the imagination and exploration of the present state and alternative future developments of digital technologies. A wider imagination is especially important for developing basic education so that it provides the capabilities for coping with these developments. Currently, the discussion has been mainly on the technical details of code. We study how to broaden this narrow view in order to support the design of more comprehensive and future-proof education around code and coding. We approach the concept of code through nine different metaphors from the existing literature on systems thinking and organisational studies. The metaphors we use are machine, organism, brain, flux and transformation, culture, political system, psychic prison, instrument of domination and carnival. We describe their epistemological backgrounds and give examples of how code is perceived through each of them. We then use the metaphors in order to suggest different complementary ways that ICT could be taught in schools. The metaphors illustrate different contexts and help to interpret the discussions related to developments in digital technologies such as free software movement, democratization of information and internet of things. They also help to identify the dominant views and the tensions between the views. We propose that the systematic use of metaphors described in this paper would be a useful tool for broadening and structuring the dialogue about teaching children to code.","Tomi Dufva and Mikko Dufva",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Non-accidental properties, metric invariance, and encoding by neurons in a model of ventral stream visual object recognition, VisNet","When objects transform into different views, some properties are maintained, such as whether the edges are convex or concave, and these non-accidental properties are likely to be important in view-invariant object recognition. The metric properties, such as the degree of curvature, may change with different views, and are less likely to be useful in object recognition. It is shown that in a model of invariant visual object recognition in the ventral visual stream, VisNet, non-accidental properties are encoded much more than metric properties by neurons. Moreover, it is shown how with the temporal trace rule training in VisNet, non-accidental properties of objects become encoded by neurons, and how metric properties are treated invariantly. We also show how VisNet can generalize between different objects if they have the same non-accidental property, because the metric properties are likely to overlap. VisNet is a 4-layer unsupervised model of visual object recognition trained by competitive learning that utilizes a temporal trace learning rule to implement the learning of invariance using views that occur close together in time. A second crucial property of this model of object recognition is, when neurons in the level corresponding to the inferior temporal visual cortex respond selectively to objects, whether neurons in the intermediate layers can respond to combinations of features that may be parts of two or more objects. In an investigation using the four sides of a square presented in every possible combination, it was shown that even though different layer 4 neurons are tuned to encode each feature or feature combination orthogonally, neurons in the intermediate layers can respond to features or feature combinations present is several objects. This property is an important part of the way in which high capacity can be achieved in the four-layer ventral visual cortical pathway. These findings concerning non-accidental properties and the use of neurons in intermediate layers of the hierarchy help to emphasise fundamental underlying principles of the computations that may be implemented in the ventral cortical visual stream used in object recognition.","Edmund T. Rolls and W. Patrick C. Mills",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Assessment of ASSERT-PV for prediction of critical heat flux in CANDU bundles","Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The recently released ASSERT-PV 3.2 provides enhanced models for improved predictions of flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against five full-scale CANDU bundle experiments conducted in 1990s and in 2009 by Stern Laboratories (SL), using 28-, 37- and 43-element (CANFLEX) bundles. A total of 15 CHF test series with varying pressure-tube creep and/or bearing-pad height were analyzed. The SL experiments encompassed the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for CANDU reactors. Code predictions of channel dryout power and axial and radial CHF locations were compared against measurements from the SL CHF tests to quantify the code prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. Furthermore, the sensitivity studies evaluated the contribution of each CHF model change or enhancement to the improvement in CHF prediction. Overall, the assessment demonstrated significant improvement in prediction of channel dryout power and axial and radial CHF locations in horizontal fuel channels containing CANDU bundles.","Y.F. Rao and Z. Cheng and G.M. Waddington",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Letters don’t matter: No effect of orthography on the perception of conversational speech","It has been claimed that learning to read changes the way we perceive speech, with detrimental effects for words with sound–spelling inconsistencies. Because conversational speech is peppered with segment deletions and alterations that lead to sound–spelling inconsistencies, such an influence would seriously hinder the perception of conversational speech. We hence tested whether the orthographic coding of a segment influences its deletion costs in perception. German glottal stop, a segment that is canonically present but not orthographically coded, allows such a test. The effects of glottal-stop deletion in German were compared to deletion of /h/ in German (grapheme: h) and deletion of glottal stop in Maltese (grapheme: q) in an implicit task with conversational speech and explicit task with careful speech. All segment deletions led to similar reduction costs in the implicit task, while an orthographic effect, with larger effects for orthographically coded segments, emerged in the explicit task. These results suggest that learning to read does not influence how we process speech but mainly how we think about it.","Holger Mitterer and Eva Reinisch",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Study of the influence of atmospheric air climatic parameters on the air kerma measurements in low energy X reference radiation fields","The control of climatic conditions in a radiation metrology laboratory is very important. Air pressure, temperature and humidity affect the value of the air density, and, consequently, alter the absorption of the photon radiation. Then, the values of the air kerma Ka, conversion coefficients from Ka to the dose equivalent quantities (hp,K(10,α) and h*K(10)) and their product (Hp(10) and H*(10)) are affected by climatic changes. For low energy X radiation fields, changes in climatic conditions are more critical. The International Organization for Standardization (ISO), via the ISO 4037-4, specifies corrections for air density for all quantities defined in 10 mm depth in tissue for nominal tube potentials varying from 10 kV to 30 kV (inclusive). In this work, we utilized monte carlo method to evaluate the influence of atmospheric air climate parameters on the air kerma measurements, for the ISO low energies, series N and L. Simulations were performed using the MCNPX code version 2.7.d, running under MPI (Message Passing Interface) on a computational cluster. We simulated the air with different humidity levels, and consequently, different densities and elemental compositions. The ISO 4037 reference beams of the Dosimeters Calibration Laboratory of the Nuclear Technology Development Center (LCD/CDTN) were utilized to validate the Monte Carlo simulations.","M.T.T. Figueiredo and M.A.S. Lacerda and T.A. da Silva",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Automatic parallelization of recursive functions with rewriting rules","Functional programming languages, since their early days, have been regarded as the holy grail of parallelism. And, in fact, the absence of race conditions, coupled with algorithmic skeletons such as map and reduce, have given developers the opportunity to write many different techniques aimed at the automatic parallelization of programs. However, there are many functional programs that are still difficult to parallelize. This difficulty stems from many factors, including the complex syntax of recursive functions. This paper provides new equipment to deal with this problem. Such instrument consists of an insight, plus a code transformation that is enabled by this insight. Concerning the first contribution, we demonstrate that many recursive functions can be rewritten as a combination of associative operations. We group such functions into two categories, which involve monoid and semiring operations. Each of these categories admits a parallel implementation. To demonstrate the effectiveness of this idea, we have implemented an automatic code rewriting tool for Haskell, and have used it to convert six well-known recursive functions to algorithms that run in parallel. Our tool is totally automatic, and it is able to deliver non-trivial speedups upon the sequential version of the programs that it receives. In particular, the automatically generated parallel code delivers good scalability when varying the number of threads or the input size.","Rodrigo C.O. Rocha and Luís F.W. Góes and Fernando M.Q. Pereira",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR9"
"Student Reflection Papers on a Global Clinical Experience: A Qualitative Study","Background
Many of the 70,000 graduating US medical students [per year] have reported participating in a global health activity at some stage of medical school. This case study design provided a method for understanding the student's experience that included student’s learning about culture, health disparities, exposure and reaction to a range of diseases actually encountered. The broad diversity of themes among students indicated that the GCE provided a flexible, personalized experience. We need to understand the student’s experience in order to help design appropriate curricular experiences [and valid student assessment].
Objective
Our research aim was to analyze medical student reflection papers to understand how they viewed their Global Clinical Experience (GCE).
Methods
A qualitative case study design was used to analyze student reflection papers. All 28 students who participated in a GCE from 2008-2010 and in 2014-2015 and submitted a reflection paper on completion of the GCE were eligible to participate in the study. One student did not submit a reflection paper and was not included in the study.
Findings
All 27 papers were coded by paragraph for reflection and for themes. System of Care/Range of Care was mentioned most often, Aids to Adjustment Process was mentioned least. The theme, “Diseases,” referred to any mention of a disease in the reflection papers, and 44 diseases were mentioned in the papers. The analysis for depth of reflection yielded the following data: Observation, 81/248 paragraphs; Observation and Interpretation, 130/248 paragraphs; and Observation, Interpretation, and Suggestions for change, 36/248 paragraphs; 9 reflection papers contained 27 separate accounts of a transformational experience.
Conclusions
This study provided a method for understanding the student's experience that included student’s learning about culture, health disparities, and exposure and reaction to a range of diseases actually encountered. The broad diversity of themes among students indicated that the GCE provided a flexible, personalized experience. How we might design a curriculum to facilitate transformational learning experiences needs further research.","Carmi Z. Margolis and Robert M. Rohrbaugh and Luisa Tsang and Jennifer Fleischer and Mark J. Graham and Anne Kellett and Janet P. Hafler",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Thermal hydraulic model of the molten salt reactor experiment with the NEAMS system analysis module","System analysis codes have a long history of providing best-estimate and conservative safety analysis for both light water and advanced reactor technologies, including molten salt reactors. As interest continues to expand with advanced reactor concepts, system analysis codes will need revisions to accommodate the behavior of these technologies. Legacy system analysis codes will need to be updated to the latest numerical techniques to shorten execution time and increase the accuracy of results. One example of a modern system analysis code that already encompasses these characteristics is the System Analysis Module (SAM). One key objective of this paper was to review available information for system code modeling of the Molten Salt Reactor Experiment (MSRE) from sources in the open literature and collect the information from these open sources in one place for the first time. This supports the potential objective of developing an open specification for system code analysis for MSRE steady state and transients with and without reactor kinetics. Data from actual MSRE tests will serve as the basis for code-to-code comparison exercises, including the MSRE zero power physics tests, the fuel pump start-up and coast down tests, and the natural circulation transient. The objective is to produce a code-to-code benchmark with a standardized set of comparison problems, recognizing the limitations of the original data. To demonstrate an initial application of this objective and the usefulness of compiling this open data, two Molten Salt Reactor Experiment (MSRE)-related models were developed to evaluate SAM for liquid fueled molten salt reactors. One model was the SAM MSRE hydraulic mockup, which provided experimental data for pressure drop measurements. The second model was the complete MSRE primary loop. The MSRE primary loop model incorporated a fluoride salt fuel/coolant with heat transfer in both the core and heat exchanger. For both the hydraulic mockup and MSRE primary loop models, a holistic 1-D system description was built using open documentation, an open description that can be readily modified and applied for any system analysis code. SAM results for the pressure drop of the hydraulic mockup model were within 6% with measurements. Coolant temperatures for the primary loop model matched the expected axial change in temperature from historical calculations. Using alternative coolant properties obtained from the literature, corresponding to salts with different actinide contents, returned similar trends in core temperature profiles. A thermal hydraulic demonstration of a loss-of-flow transient showed the importance of coupling SAM thermal hydraulic analysis to neutronics. This coupling is essential for simulating MSR transients with system analysis codes.","Adrian M. Leandro and Florent Heidet and Rui Hu and Nicholas R. Brown",2019,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"DFMDEF18: A C-code for the double folding interaction potential of a spherical nucleus with deformed nucleus","This is a new version of the DFMDEF code published earlier. The new version is designed to obtain the nucleus–nucleus potential between two nuclei (one of which may be deformed) by using the double folding model (DFM). In particular the code enables to find the Coulomb barrier. The new version allows the user to employ his (her) own charge, proton, and neutron density distributions. The main functionalities of the original code (the nucleus–nucleus potential as a function of the incident angle and the distance between the centers of mass of colliding nuclei; the Coulomb barrier characteristics) have not been modified.
New version program summary
Program title: DFMDEF18 Program Files doi:http://dx.doi.org/10.17632/y9pwd2p24z.1 Licensing provisions: CC0 1.0 Programming language: C Journal reference of previous version: Comp. Phys. Comm. 184 (2013) 172 Does the new version supersede the previous version? Yes Nature of problem: The code calculates in a semimicroscopic way the bare interaction potential between two colliding nuclei one of which can be deformed and axially symmetric. The potential is evaluated as a function of the center of mass distance and the angle between the axis of symmetry and the beam direction. The heights and the positions of the Coulomb barriers are found. Dependence of the barrier parameters upon the characteristics of the effective NN forces (like, e.g. the range of the exchange part of the nuclear term) as well as upon the parameters of the density distributions can be investigated. Method of solution: The nucleus–nucleus potential is calculated using the double folding model with the Coulomb and the effective M3Y NN interactions. For the direct parts of the Coulomb and the nuclear terms, the Fourier transform method is used. In order to calculate the exchange parts, the density matrix expansion method is applied. Reason for new version: Many users asked us how to implement their own density distributions in the code. Now this option has been added. Summary of revisions: 1. Additional features of DFMDEF18:Projectile and target densities as input files In the DFMDEF [1, 2], only the Woods–Saxon profile for the charge, proton, and neutron density distributions was used. In the new version, DFMDEF18, the user can use the same profile, but there is an option to provide six input files <inp_rhoP_Z.c>, <inp_rhoP_N.c>, <inp_rhoP_q.c>, <inp_rhoT_Z.c>, <inp_rhoT_N.c>, and <inp_rhoT_q.c>similar to what was done in [3]. In these files the proton (_Z), neutron (_N), and charge (_q) density distributions are defined for the projectile (P) and target (T) nuclei as functions of the distance from the nucleus center (r) and the zenith angle (the_deg). We will refer to this set of six files as to “rho-input files”. The technical explanation of the rho-input files might be found in file <Program_changes.txt>(subsection 2.11). The values of the densities required for producing the interaction potential are found by means of the following interpolating polynomial. In order to avoid the overlap in notations for the zenith angle, instead of the traditional θ, we use here Θ. (1)ρr,Θ=ρr0,Θ0∗r−r1r0−r1∗r−r2r0−r2∗Θ−Θ1Θ0−Θ1∗Θ−Θ2Θ0−Θ2+ρr1,Θ0∗r−r0r1−r0∗r−r2r1−r2∗Θ−Θ1Θ0−Θ1∗Θ−Θ2Θ0−Θ2+ρr2,Θ0∗r−r1r2−r1∗r−r0r2−r0∗Θ−Θ1Θ0−Θ1∗Θ−Θ2Θ0−Θ2+ρr0,Θ1∗r−r1r0−r1∗r−r2r0−r2∗Θ−Θ0Θ1−Θ0∗Θ−Θ2Θ1−Θ2+ρr0,Θ2∗r−r1r0−r1∗r−r2r0−r2∗Θ−Θ1Θ2−Θ1∗Θ−Θ0Θ2−Θ0+ρr1,Θ1∗r−r0r1−r0∗r−r2r1−r2∗Θ−Θ0Θ1−Θ0∗Θ−Θ2Θ1−Θ2+ρr1,Θ2∗r−r0r1−r0∗r−r2r1−r2∗Θ−Θ1Θ2−Θ1∗Θ−Θ0Θ2−Θ0+ρr2,Θ1∗r−r1r2−r1∗r−r0r2−r0∗Θ−Θ0Θ1−Θ0∗Θ−Θ2Θ1−Θ2+ρr2,Θ2∗r−r1r2−r1∗r−r0r2−r0∗Θ−Θ1Θ2−Θ1∗Θ−Θ0Θ2−Θ0.Thus the density at the point with arbitrary coordinates r,Θ is determined by the nine values of the density ρri,Θi from the tables provided by user. The values r0,r1,r2 (Θ0,Θ1,Θ2) are the closest ones to the necessary r (Θ), and the interval r0÷r2 (Θ0÷Θ2) always contains the value ofr (Θ). This interpolation works rather accurately. However, due to the inevitable numerical errors in the derivatives entering the kF (see Eq. (26) in Ref. [1]) the value of kF2 sometimes becomes negative. Therefore in the present code we keep only zero-order term calculating kF as follows (2)kF=1.5π2ρAr→1∕3.The quality of this approximation is illustrated by Fig. 1. In Fig. 1 a the Coulomb barrier heights obtained using the earlier published code [1] (line with symbols, Vb1) and the present code (line without symbols, Vbp) are shown. One sees that two codes produce very close results. To quantify this statement we present in Fig. 1 b the fractional difference between the barrier heights (3)ε=Vb1Vbp−1.Since the fractional difference does not exceed 0.5% we believe the approximation (2) can beaccepted. The original version of the code [1] allowed performing the calculations of the strong (nuclear) term of the nucleus–nucleus interaction potential in two different ways. Namely, the phenomenological Woods–Saxon parametrization [4, 5] and semimicroscopic double folding calculations could be used. In order to use the Woods–Saxon parametrization it is necessary to know the deformation parameters of the target nucleus. If the code uses the rho-input files for the densities provided by the user, such parameters can be absent. Therefore the Woods–Saxon parametrization for the potential is removed from the present version of the code. The Woods–Saxon approximation of the calculated DFM nuclear term of the potential is also removed for the same reason. Thus in the present code only the options concerning the double folding calculations remain. 2. The program The code consists now of 8 files and one header file. It reads the data from 8 input files and prints the results into two output files. For specific details regarding the changes in each source file see the file <Program_changes.txt>. The main input file has been split into two files: <inp_dfpdef.c> and <inp_dens.c>. Their description as well as the description of the output file might be also found in the file <Program_changes.txt>. References[1]I. I. Gontchar, M. V. Chushnyakova, Comput. Phys. Commun. 184 (2013) 172.[2]I. I. Gontchar, D. J. Hinde, M. Dasgupta, C. R. Morton, J. O. Newton, Phys. Rev. C 73 (2006) 034610.[3]I. I. Gontchar, M. V. Chushnyakova, Comput. Phys. Commun. 206 (2016) 97.[4]I.I. Gontchar, M. Dasgupta, D. J. Hinde, R. D. Butt, A. Mukherjee, Phys. Rev. C 65 (2002) 034610.[5]I.I. Gontchar, M. Dasgupta, D. J. Hinde, J. O. Newton, Phys. At. Nucl. 69 (2006) 1428Appendix TEST RUN OUTPUT Input file <inp_dfpdef.c>  Output file <out_dfmsph.c> ","I.I. Gontchar and M.V. Chushnyakova",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"PFMCal : Photonic force microscopy calibration extended for its application in high-frequency microrheology","The present document is an update of the previously published MatLab code for the calibration of optical tweezers in the high-resolution detection of the Brownian motion of non-spherical probes [1]. In this instance, an alternative version of the original code, based on the same physical theory [2], but focused on the automation of the calibration of measurements using spherical probes, is outlined. The new added code is useful for high-frequency microrheology studies, where the probe radius is known but the viscosity of the surrounding fluid maybe not. This extended calibration methodology is automatic, without the need of a user’s interface. A code for calibration by means of thermal noise analysis [3] is also included; this is a method that can be applied when using viscoelastic fluids if the trap stiffness is previously estimated [4]. The new code can be executed in MatLab and using GNU Octave.
New version program summary
Program Title: PFMCal Program Files doi:http://dx.doi.org/10.17632/s59f3gz729.1 Licensing provisions: GPLv3 Programming language: MatLab 2016a (MathWorks Inc.) and GNU Octave 4.0 Operating system: Linux and Windows. Supplementary material: A new document README.pdf includes basic running instructions for the new code. Journal reference of previous version: Computer Physics Communications, 196 (2015) 599 Does the new version supersede the previous version?: No. It adds alternative but compatible code while providing similar calibration factors. Nature of problem (approx. 50–250 words): The original code uses a MatLab-provided user’s interface, which is not available in GNU Octave, and cannot be used outside of a proprietary software as MatLab. Besides, the process of calibration when using spherical probes needs an automatic method when calibrating big amounts of different data focused to microrheology. Solution method (approx. 50–250 words): The new code can be executed in the latest version of MatLab and using GNU Octave, a free and open-source alternative to MatLab. This code generates an automatic calibration process which requires only to write the input data in the main script. Additionally, we include a calibration method based on thermal noise statistics, which can be used with viscoelastic fluids if the trap stiffness is previously estimated. Reasons for the new version: This version extends the functionality of PFMCal for the particular case of spherical probes and unknown fluid viscosities. The extended code is automatic, works in different operating systems and it is compatible with GNU Octave. Summary of revisions: The original MatLab program in the previous version, which is executed by PFMCal.m, is not changed. Here, we have added two additional main archives named PFMCal_auto.m and PFMCal_histo.m, which implement automatic calculations of the calibration process and calibration through Boltzmann statistics, respectively. The process of calibration using this code for spherical beads is described in the README.pdf file provided in the new code submission. Here, we obtain different calibration factors, β (given in μm/V), according to [2], related to two statistical quantities: the mean-squared displacement (MSD), βMSD, and the velocity autocorrelation function (VAF), βVAF. Using that methodology, the trap stiffness, k, and the zero-shear viscosity of the fluid, η, can be calculated if the value of the particle’s radius, a, is previously known. For comparison, we include in the extended code the method of calibration using the corner frequency of the power-spectral density (PSD) [5], providing a calibration factor βPSD. Besides, with the prior estimation of the trap stiffness, along with the known value of the particle’s radius, we can use thermal noise statistics to obtain calibration factors, β, according to the quadratic form of the optical potential, βE, and related to the Gaussian distribution of the bead’s positions, βσ2. This method has been demonstrated to be applicable to the calibration of optical tweezers when using non-Newtonian viscoelastic polymeric liquids [4]. An example of the results using this calibration process is summarized in Table 1. Using the data provided in the new code submission, for water and acetone fluids, we calculate all the calibration factors by using the original PFMCal.m and by the new non-GUI code PFMCal_auto.m and PFMCal_histo.m. Regarding the new code, PFMCal_auto.m returns η, k, βMSD, βVAF and βPSD, while PFMCal_histo.m provides βσ2 and βE. Table 1 shows how we obtain the expected viscosity of the two fluids at this temperature and how the different methods provide good agreement between trap stiffnesses and calibration factors. Additional comments including Restrictions and Unusual features (approx. 50–250 words): The original code, PFMCal.m, runs under MatLab using the Statistics Toolbox. The extended code, PFMCal_auto.m and PFMCal_histo.m, can be executed without modification using MatLab or GNU Octave. The code has been tested in Linux and Windows operating systems. References[1]A. Butykai, F. Mor, R. Gaál, P. Domínguez-García, L. Forró, J. S., Calibration of optical tweezers with non-spherical probes via high-resolution detection of brownian motion, Comput. Phys. Commun. 196 (2015) 599–610.[2]M. Grimm, T. Franosch, S. Jeney, High-resolution detection of brownian motion for quantitative optical tweezers experiments, Phys. Rev. E 86 (2012) 021912.[3]E.-L. Florin, A. Pralle, E. H. K. Stelzer, J. K. H. Hörber, Photonic force microscope calibration by thermal noise analysis, Appl. Phys. A 66 (1998) S75–S78.[4]P. Domínguez-García, L. Forró, S. Jeney, Interplay between optical, viscous, and elastic forces on an optically trapped brownian particle immersed in a viscoelastic fluid, Appl. Phys. Lett. 109 (14) (2016) 143702.[5]K. Berg-Sørensen, H. Flyvbjerg, Power spectrum analysis for optical tweezers, Rev. Sci. Instrum. 75 (3) (2004) 594–612.","A. Butykai and P. Domínguez-García and F.M. Mor and R. Gaál and L. Forró and S. Jeney",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A Comparison of Natural Language Processing Methods for Automated Coding of Motivational Interviewing","Motivational interviewing (MI) is an efficacious treatment for substance use disorders and other problem behaviors. Studies on MI fidelity and mechanisms of change typically use human raters to code therapy sessions, which requires considerable time, training, and financial costs. Natural language processing techniques have recently been utilized for coding MI sessions using machine learning techniques, rather than human coders, and preliminary results have suggested these methods hold promise. The current study extends this previous work by introducing two natural language processing models for automatically coding MI sessions via computer. The two models differ in the way they semantically represent session content, utilizing either 1) simple discrete sentence features (DSF model) and 2) more complex recursive neural networks (RNN model). Utterance- and session-level predictions from these models were compared to ratings provided by human coders using a large sample of MI sessions (N=341 sessions; 78,977 clinician and client talk turns) from 6 MI studies. Results show that the DSF model generally had slightly better performance compared to the RNN model. The DSF model had “good” or higher utterance-level agreement with human coders (Cohen's kappa>0.60) for open and closed questions, affirm, giving information, and follow/neutral (all therapist codes); considerably higher agreement was obtained for session-level indices, and many estimates were competitive with human-to-human agreement. However, there was poor agreement for client change talk, client sustain talk, and therapist MI-inconsistent behaviors. Natural language processing methods provide accurate representations of human derived behavioral codes and could offer substantial improvements to the efficiency and scale in which MI mechanisms of change research and fidelity monitoring are conducted.","Michael Tanana and Kevin A. Hallgren and Zac E. Imel and David C. Atkins and Vivek Srikumar",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"How can a recurrent neurodynamic predictive coding model cope with fluctuation in temporal patterns? Robotic experiments on imitative interaction","The current paper examines how a recurrent neural network (RNN) model using a dynamic predictive coding scheme can cope with fluctuations in temporal patterns through generalization in learning. The conjecture driving this present inquiry is that a RNN model with multiple timescales (MTRNN) learns by extracting patterns of change from observed temporal patterns, developing an internal dynamic structure such that variance in initial internal states account for modulations in corresponding observed patterns. We trained a MTRNN with low-dimensional temporal patterns, and assessed performance on an imitation task employing these patterns. Analysis reveals that imitating fluctuated patterns consists in inferring optimal internal states by error regression. The model was then tested through humanoid robotic experiments requiring imitative interaction with human subjects. Results show that spontaneous and lively interaction can be achieved as the model successfully copes with fluctuations naturally occurring in human movement patterns.","Ahmadreza Ahmadi and Jun Tani",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Experimental fatigue crack growth behavior and predictions under multiaxial variable amplitude service loading histories","This study presents experimental crack growth data for naturally initiated fatigue cracks in tubular specimens of 2024-T3 aluminum alloy subjected to both uniaxial and multiaxial variable amplitude flight loading spectra. Experimental crack growth behavior is compared to predictions based on two state-of-the-art analysis codes: UniGrow and FASTRAN. UniGrow is based on the idea that residual stress distributions surrounding the crack tip are responsible for causing load sequence effects in variable amplitude crack growth, while FASTRAN attributes these effects to varying degrees of plasticity induced closure in the crack wake. For variable amplitude fatigue tests performed under pure axial nominal loading conditions, both UniGrow and FASTRAN analyses were found to produce conservative growth life predictions despite good agreement with constant amplitude crack growth data. For variable amplitude torsion and combined axial-torsion crack growth analyses, however, the conservatism in growth life predictions was found to reduce. This is attributed to multiaxial nominal stress state effects, such as T-stress and mixed-mode crack growth, which are not accounted for in either UniGrow or FASTRAN, but were observed in constant amplitude crack growth tests to increase experimental crack growth rates. Additionally, by comparing experimental crack growth lives between tests performed using full and edited versions of the same loading history, it was found that a 94% reduction in loading history length resulted in differences in experimental crack growth life of less than a factor of 5. Since cracks in this study were initiated naturally, the effect of initial crack geometry assumptions on crack growth predictions was also investigated.","Nicholas R. Gates and Ali Fatemi",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"GenASiS   Basics: Object-oriented utilitarian functionality for large-scale physics simulations (Version 2)","GenASiSBasics provides Fortran 2003 classes furnishing extensible object-oriented utilitarian functionality for large-scale physics simulations on distributed memory supercomputers. This functionality includes physical units and constants; display to the screen or standard output device; message passing; I/O to disk; and runtime parameter management and usage statistics. This revision–Version 2 of Basics–makes mostly minor additions to functionality and includes some simplifying name changes.
New version program summary
Program Title: SineWaveAdvection, SawtoothWaveAdvection, and RiemannProblem (fluid dynamics example problems illustrating GenASiSBasics); ArgonEquilibrium and ClusterFormation (molecular dynamics example problems illustrating GenASiSBasics) Program Files doi:http://dx.doi.org/10.17632/6w9ygpygmc.1 Licensing provisions: GPLv3 Programming language: Fortran 2003 (tested with gfortran 6.1.0, Intel Fortran 16.0.3, Cray Compiler 8.5.3) Journal reference of previous version: Computer Physics Communications, 196 (2015) 506 Does the new version supersede the previous version?: Yes Reasons for the new version: This version makes mostly minor additions to functionality and includes some simplifying name changes. Summary of revisions: Several additions to functionality are minor. Two new singleton objects are KIND_SMALL and KIND_TINY, for smaller sized numbers than those specified by the previously available KIND_DEFAULT. The class MeasuredValueForm can now handle some more complicated cases of unit string processing. The numerical values in the CONSTANT singleton have been updated to 2016 values [3], and CONSTANT and UNIT contain a few additional members. A new class TimerForm can be used to track the wall time occupied by various segments of code. The PROGRAM_HEADER singleton now contains an array member of this new class. With calls like the user can initialize their own timers; on return iMyTimer contains the index of the newly initialized timer. The calls and should surround the block of code to be timed. The information displayed by calling the ShowStatistics method of PROGRAM_HEADER includes data from all initialized timers, including one for overall execution time which is present by default. The code now expects to be compiled with OpenMP, typically by applying compiler flags. Strictly speaking this is only required for the PROGRAM_HEADER singleton, which queries the number of threads via a library call. In GenASiSBasics, OpenMP directives (which appear as comments as far as the Fortran 2003 standard is concerned) are only used in the Clear and Copy commands. There have been a number of name changes, mostly for simplification and consistency. These include the classes in ArrayArrays, where for example ArrayInteger_1D_Form is now simply Integer_1D_Form. Similar streamlining changes have been made to MessagePassing classes: IncomingMessageArrayRealForm is now MessageIncoming_1D_R_Form, for instance. The class VariableGroupArrayMetadata is now VariableGroup_1D_Form. The name ParametersStreamForm has been changed by one character (deletion of an s) to ParameterStreamForm. The member Selected of VariableGroupForm has been changed to iaSelected, where the prefix ia is a conventional prefix we use for an array of array indices. The interface and functionality of the SetGrid member of StructuredGridImageForm have been modified so as not to include boundary cells exterior to the computational domain, which prevented display of the computational domain in 3D plots with VisIt [4] unless a Box operator was applied. See the fluid dynamics examples for the modified usage. Finally, version 4.10 of the Silo library [2] introduced an include file named silo_f9x.inc, which the FileSystem classes of GenASiSBasics now expect to be available instead of silo.inc. Nature of problem: By way of illustrating GenASiSBasics functionality, solve example fluid dynamics and molecular dynamics problems. Solution method: For fluid dynamics examples, finite-volume. For molecular dynamics examples, leapfrog and velocity-Verlet integration. External routines/libraries: MPI [1] and Silo [2] Additional comments including Restrictions and Unusual features: The example problems named above are not ends in themselves, but serve to illustrate our object-oriented approach and the functionality available though GenASiSBasics. In addition to these more substantial examples, we provide individual unit test programs for each of the classes comprised by GenASiSBasics. GenASiSBasics is available in the CPC Program Library and also at https://github.com/GenASiS. [1]http://www.mcs.anl.gov/mpi/[2]https://wci.llnl.gov/simulation/computer-codes/silo[3]C. Patrignani et al. (Particle Data Group), Chin. Phys. C 40 (2016) 100001[4]https://wci.llnl.gov/simulation/computer-codes/visit","Christian Y. Cardall and Reuben D. Budiardja",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Assessment of ASSERT-PV for prediction of post-dryout heat transfer in CANDU bundles","Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The recently released ASSERT-PV 3.2 provides enhanced models for improved predictions of subchannel flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against PDO tests performed during five full-size CANDU bundle experiments conducted between 1992 and 2009 by Stern Laboratories (SL), using 28-, 37- and 43-element bundles. A total of 10 PDO test series with varying pressure-tube creep and/or bearing-pad height were analyzed. The SL experiments encompassed the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for existing CANDU reactors. Code predictions of maximum PDO fuel-sheath temperature were compared against measurements from the SL PDO tests to quantify the code's prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. Furthermore, separate-effects sensitivity studies quantified the contribution of each PDO model change or enhancement to the improvement in PDO heat transfer prediction. Overall, the assessment demonstrated significant improvement in prediction of PDO sheath temperature in horizontal fuel channels containing CANDU bundles.","Z. Cheng and Y.F. Rao and G.M. Waddington",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"HOTB: High precision parallel code for calculation of four-particle harmonic oscillator transformation brackets","This new version of the HOTB program for calculation of the three and four particle harmonic oscillator transformation brackets provides some enhancements and corrections to the earlier version (Germanas et al., 2010) [1]. In particular, new version allows calculations of harmonic oscillator transformation brackets be performed in parallel using MPI parallel communication standard. Moreover, higher precision of intermediate calculations using GNU Quadruple Precision and arbitrary precision library FMLib [2] is done. A package of Fortran code is presented. Calculation time of large matrices can be significantly reduced using effective parallel code. Use of Higher Precision methods in intermediate calculations increases the stability of algorithms and extends the validity of used algorithms for larger input values.
New version program summary
Title of program: HOTB_MPI Catalogue identifier: AEFQ_v4_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEFQ_v4_0.html Program obtainable from: CPC Program Library, Queen’s University of Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 Number of lines in programs, including test data, etc.: 1711 Number of bytes in distributed programs, including test data, etc.: 11667 Distribution format: tar.gz Program language used: FORTRAN 90 with MPI extensions for parallelism Computer: Any computer with FORTRAN 90 compiler Operating system: Windows, Linux, FreeBSD, True64 Unix Has the code been vectorized of parallelized?: Yes, parallelism using MPI extensions. Number of CPUs used: up to 999 RAM(per CPU core): Depending on allocated binomial and trinomial matrices and use of precision; at least 500 MB Catalogue identifier of previous version: AEFQ_v1_0 Journal reference of previous version: Comput. Phys. Comm. 181, Issue 2, (2010) 420–425 Does the new version supersede the previous version? Yes Nature of problem: Calculation of matrices of three-particle harmonic oscillator brackets (3HOB) and four-particle harmonic oscillator brackets (4HOB) in a more effective way, which allows us to calculate matrix of the brackets up to a few hundred times more rapidly and more accurate than in a previous version. Solution method: Using external parallelization libraries and mutable precision we created a pack of numerical codes based on the methods of compact expressions of the three and four-particle harmonics oscillator brackets 3HOB, 4HOB, presented in [3]. Restrictions: For double precision version calculations can be done up to harmonic oscillator (HO) energy quanta e=28. For quadruple precision mantissa is equal to approximately 34 decimal digits, therefore calculations can be done up to HO energy quanta to e=52. Running time: The running time depends on the harmonic oscillator energy quanta, cluster size and the precision of intermediate calculations. More information on Table 1 for 3HOB and Table 2 for 4HOB. Reasons for a new version: The new program version expands the limits of harmonic oscillator energy quanta and gives shorter calculation time. Summary of revisions: 1.Additional features of new code HOTB_MPI: (a)Extend the limits of calculation of HOB First version was able to produce harmonic oscillator transformation brackets for three and four particles if E≤HO energy quanta. With this version of our program, if quadruple or arbitrary precision functions are being used, it is possible to calculate three and four particle harmonic oscillator transformation brackets for greater values of energy and momenta, while sustaining tolerable margin of error.(b)Calculation time As the code of previous version of program was redone using parallelism paradigma, it is now possible to reduce the calculation time of transformation matrices significantly, depending on the size of computing cluster, as the dimensions of matrices are growing very rapidly according to the energy and momenta values.2.Modifications or corrections to HOTB:New program HOTB_MPI is written in the FORTRAN 90 language, according to formulas described in [3]. In total there are six files: HOTB_mpi.f90, data_module.f90, matrix_tools.f90, dp_module.f90, qd_module.f90, and fm_module.f90. File HOTB_mpi.f90 contains main program for executing calculations. File data_module.f90 contains different kind of precision matrices for storage of binomial and trinomial values and also additional parameters.Detailed descriptions of parameters used by functions and subroutines are located in file README.txt.File matrix_tools.f90 contains functions needed to determine the dimensions of matrix, creation of state array and parallel calculation of desired matrix. (a)matrix_tools.f90i.subroutinematrix_4HOB_dimensionCalculates the dimension of 4HOB matrix.ii.subroutinematrix_3HOB_dimensionCalculates the dimension of 3HOB matrix,iii.subroutinematrix_3HOBCalculates the global state array which is used in parallel calculation of 3HOB matrix.iv.subroutinematrix_4HOBCalculates the global state array which is used in parallel calculation of 4HOB matrix.v.subroutinestate_array_3HOBCreates state array for 3HOB matrix output.vi.subroutinestate_array_4HOBCreates state array for 4HOB matrix output.vii.subroutinecalculate_3HOBPerforms parallel calculations of 3HOB matrix.viii.subroutinecalculate_4HOBPerforms parallel calculations of 4HOB matrix.*_module.f90 files contain modules for calculations respectively to the precision that is going to be used. We describe only functions of module dp_module.f90 as other modules are created replicating the structure of dp_module, except arbitrary precision module, which has several functions which will be described below.The naming convention for qd_module and fm_module is the same except that respectively qd_ and fm_ prefixes are added to function names. (a)dp_module.f90i.double precision functiondp_4HOBCalculates matrix element for 4HOB.ii.subroutinedp_binomFills the array of binomial coefficients.iii.subroutinedp_trinomFills the array of trinomial coefficients.iv.integer functiontriFunction for triangle condition testing.v.double precision functiondp_c6jFunction for 6-j coefficient calculation.vi.double precision functiondp_c9jFunction for 9-j coefficient calculationvii.double precision functiondp_kl0Function for Clebsch–Gordan coefficient with zero projection calculation.viii.double precision functiondp_gFunction for gamma element calculation.ix.double precision functiondp_3HOBCalculates three particle harmonic oscillator transformation bracket.Additional functions located in fm_module.f90, which are required for arbitrary precision calculation: (a)fm_module.f90i.type(fm) functionbinomasFunction for calculation of binomial value using FMLIB function Binomial.ii.type(fm) functionaccess_binomFunction for accessing triangular binomial matrix fm_bin.iii.type(fm) functioncheck_binFunction for checking if required binomial value is located in matrix fm_bin. If not, the value is calculated using FMLIB function Binomial.iv.subroutinewrite_binomFunction for writing calculated binomial value to triangular matrix fm_bin.","A. Stepšys and S. Mickevicius and D. Germanas and R.K. Kalinauskas",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Developmental metaplasticity in neural circuit codes of firing and structure","Firing-rate dynamics have been hypothesized to mediate inter-neural information transfer in the brain. While the Hebbian paradigm, relating learning and memory to firing activity, has put synaptic efficacy variation at the center of cortical plasticity, we suggest that the external expression of plasticity by changes in the firing-rate dynamics represents a more general notion of plasticity. Hypothesizing that time constants of plasticity and firing dynamics increase with age, and employing the filtering property of the neuron, we obtain the elementary code of global attractors associated with the firing-rate dynamics in each developmental stage. We define a neural circuit connectivity code as an indivisible set of circuit structures generated by membrane and synapse activation and silencing. Synchronous firing patterns under parameter uniformity, and asynchronous circuit firing are shown to be driven, respectively, by membrane and synapse silencing and reactivation, and maintained by the neuronal filtering property. Analytic, graphical and simulation representation of the discrete iteration maps and of the global attractor codes of neural firing rate are found to be consistent with previous empirical neurobiological findings, which have lacked, however, a specific correspondence between firing modes, time constants, circuit connectivity and cortical developmental stages.","Yoram Baram",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A method to localize faults in concurrent C programs","We describe a new approach to localize faults in concurrent programs, which is based on bounded model checking and sequentialization techniques. The main novelty is the idea of reproducing a faulty behavior, in a sequential version of a concurrent program. In order to pinpoint faulty lines, we analyze counterexamples generated by a model checker, to the new instrumented sequential program, and search for a diagnostic value, which corresponds to actual lines in a program. This approach is useful to improve debugging processes for concurrent programs, since it tells which line should be corrected and what values lead to a successful execution. We implemented this approach as a code-to-code transformation from concurrent into non-deterministic sequential programs, which are used as inputs to existing verification tools. Experimental results show that our approach is effective and capable of identifying faults in our benchmark set, which was extracted from the SV-COMP 2016 suite.","Erickson H. da S. Alves and Lucas C. Cordeiro and Eddie B. de L. Filho",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Dr TIM: Ray-tracer TIM, with additional specialist scientific capabilities","We describe several extensions to TIM, a raytracing program for ray-optics research. These include relativistic raytracing; simulation of the external appearance of Eaton lenses, Luneburg lenses and generalised focusing gradient-index lens (GGRIN) lenses, which are types of perfect imaging devices; raytracing through interfaces between spaces with different optical metrics; and refraction with generalised confocal lenslet arrays, which are particularly versatile METATOYs.
Program summary
Program title: TIM Catalogue identifier: AEKY_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEKY_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licencing provisions: GNU General Public License No. of lines in distributed program, including test data, etc.: 106905 No. of bytes in distributed program, including test data, etc.: 6327715 Distribution format: tar.gz Programming language: Java. Computer: Any computer capable of running the Java Virtual Machine (JVM) 1.6. Operating system: Any, developed under Mac OS X Version 10.6 and 10.8.3. RAM: Typically 130 MB (interactive version running under Mac OS X Version 10.8.3) Classification: 14, 18. Catalogue identifier of previous version: AEKY_v1_0 Journal reference of previous version: Comput. Phys. Comm. 183(2012)711 External routines: JAMA [1] (source code included) Does the new version supersede the previous version?: Yes Nature of problem: Visualisation of scenes that include scene objects that create wave-optically forbidden light-ray fields. Solution method: Ray tracing. Reasons for new version: Significant extension of the capabilities (see Summary of revisions), as demanded by our research. Summary of revisions: Added capabilities include the simulation of different types of camera moving at relativistic speeds relative to the scene; visualisation of the external appearance of generalised focusing gradient-index (GGRIN) lenses, including Maxwell fisheye, Eaton and Luneburg lenses; calculation of refraction at the interface between spaces with different optical metrics; and handling of generalised confocal lenslet arrays (gCLAs), a new type of METATOY. Unusual features: Specifically designed to visualise wave-optically forbidden light-ray fields; can visualise ray trajectories and geometric optic transformations; can simulate photos taken with different types of camera moving at relativistic speeds, interfaces between spaces with different optical metrics, the view through METATOYs and generalised focusing gradient-index lenses; can create anaglyphs (for viewing with coloured “3D glasses”), HDMI-1.4a standard 3D images, and random-dot autostereograms of the scene; integrable into web pages. Running time: Problem-dependent; typically seconds for a simple scene. References: [1] JAMA: A Java Matrix Package, http://math.nist.gov/javanumerics/jama/","Stephen Oxburgh and Tomáš Tyc and Johannes Courtial",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Inquiry into sustainability issues by preservice teachers: A pedagogy to enhance sustainability consciousness","Education for sustainable development (ESD) is transformative education aiming at developing participants’ understanding of sustainability issues and transforming their attitudes and behaviours regarding environment, society and economy. Sustainability consciousness, an expected outcome of ESD, is a complex of cognitive and affective learning. Development of sustainability consciousness requires transformative learning experiences. The study presented in this paper employed action research to enhance sustainability consciousness of the preservice teachers through inquiry based learning. The study was done in the Institute of Education, Lahore College for Women University, Pakistan. The study integrated sustainability education in an existing course entitled ‘Research Methods in Education’. The course is included in the final year of B.Ed. (Honours) programme. Outcome of the ESD-integration was measured in terms of change in the sustainability consciousness of the preservice teachers. The Action Research project engaged 27 preservice teachers in inquiry-based learning (empirical investigations and research based discussions) for a period of 11 weeks. The participants investigated sustainability issues collaboratively. To investigate the change in participants’ sustainability consciousness through inquiry based learning, researchers collected through pre- and post-tests, interviews and observation. Quantitative data were analyzed through paired t-test while qualitative data through thematic coding. The data indicate that empirical investigations into sustainability issues by the presevice teachers and research-based discussions enhanced preservice teachers’ sustainability consciousness. This highlights the transformative potential of inquiry based learning. Moreover, it indicates that sustainability education can be successfully integrated in the course of ‘research methods in education’. The findings suggest that teacher education programmes and other university programmes may employ inquiry based learning as a vehicle to enhance sustainability consciousness of the undergraduate students.","Qudsia Kalsoom and Afifa Khanam",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The Role of Education, Educational Processes, and Education Culture on the Development of Virtual Learning in Iran","The aim of this study was to investigate the effect of education and education culture on the development of virtual learning in Iran. The qualitative method with a grounded theory approach was developed. Virtual students as well as their teachers teaching in Tehran, Iran were chosen as the population. The research instrument for the data collection was a thorough unstructured interview. Based on the systematic grounded approach, the data were encoded through three different coding systems: open coding (in two levels), axial coding, and selective coding and finally the grounded theory is explained. The results showed attention to the influence of technology as well as its entrance to the world of education; such a thing that leads to changes in current traditional teaching and finally the combination of virtual and traditional education.","Mojdeh Naderzadeh Gavareshki and F. Haddadian and Mc. HassanzadehKalleh",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A novel algorithm for longitudinal track-bridge interactions considering loading history and using a verified mechanical model of fasteners","The mechanical behavior of fasteners has considerable influence on track–bridge interaction (TBI) analysis, and a consensus has been gradually reached that their loading history and nonlinear characteristics should be taken into account. In this study, we present a new variation pattern of the longitudinal resistance of the fasteners considering loading history, which was experimentally verified based on the Dahl friction model. The model’s mechanical behavior was more consistent with measured results compared with previously proposed Ruge’s model and the double-spring model. According to the new model, an algorithm for TBI analysis considering loading history was derived based on the Ritz method and the principle of minimum potential energy, which was applied in a case study of an N-span, simply-supported girder bridge in a high-speed railway with a typical loading sequence: (1) the seasonal temperature change of the bridge; (2) the bending of the bridge structure under a vertical train load; (3) the braking of the train. Additionally, some significant conclusions were obtained by comparing the numerical results and adopting the mechanical parameters of the fasteners specified in various codes, according to both the linear superposition method (LSM) and loading history method (LHM).","Jun Luo and Zhiping Zeng",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A binary differential evolution algorithm learning from explored solutions","Although real-coded differential evolution (DE) algorithms can perform well on continuous optimization problems (CoOPs), designing an efficient binary-coded DE algorithm is still a challenging task. Inspired by the learning mechanism in particle swarm optimization (PSO) algorithms, we propose a binary learning differential evolution (BLDE) algorithm that can efficiently locate the global optimal solutions by learning from the last population. Then, we theoretically prove the global convergence of BLDE, and compare it with some existing binary-coded evolutionary algorithms (EAs) via numerical experiments. Numerical results show that BLDE is competitive with the compared EAs. Further study is performed via the change curves of a renewal metric and a refinement metric to investigate why BLDE cannot outperform some compared EAs for several selected benchmark problems. Finally, we employ BLDE in solving the unit commitment problem (UCP) in power systems to show its applicability to practical problems.","Yu Chen and Weicheng Xie and Xiufen Zou",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"MT: A Mathematica package to compute convolutions","We introduce the Mathematica package MT which can be used to compute, both analytically and numerically, convolutions involving harmonic polylogarithms, polynomials or generalized functions. As applications contributions to next-to-next-to-next-to leading order Higgs boson production and the Drell–Yan process are discussed.
Program summary
Title of program:MT Available from:http://www-ttp.physik.uni-karlsruhe.de/Progdata/ttp13/ttp13-27/ Computer for which the program is designed and others on which it is operable: Any computer where Mathematica version 6 or higher is running. Operating system or monitor under which the program has been tested: Linux No. of bytes in distributed program including test data etc.: approximately 50000 bytes, and tables of approximately 60 megabytes Distribution format: source code Keywords: Convolution of partonic cross sections and splitting functions, Mellin transformation, harmonic sums, harmonic polylogarithms, Higgs boson production, Drell–Yan process Nature of physical problem: For the treatment of collinear divergences connected to initial-state radiation it is necessary to consider convolutions of partonic cross sections with splitting functions. MT can be used to compute such convolutions. Method of solution: MT is implemented in Mathematica and we provide several functions in order to perform transformations to Mellin space, manipulations of the expressions, and inverse Mellin transformations. Restrictions on the complexity of the problem: In case the weight of the input quantities is too high the tables for the (inverse) Mellin transforms have to be extended. In the current implementation the tables contain expressions up to weight eight, code for the generation of tables of even higher weight is provided, too. MT can only handle convolutions of expressions involving harmonic polylogarithms, plus distributions and polynomials in the partonic variable x. Typical running time: In general the run time for the individual operations is at most of the order of a few minutes (depending on the speed and memory of the computer).","Maik Höschele and Jens Hoff and Alexey Pak and Matthias Steinhauser and Takahiro Ueda",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The relationship between counselors' technical skills, clients' in-session verbal responses, and outcome in smoking cessation treatment","Background
The technical component of Motivational Interviewing (MI) posits that client language mediates the relationship between counselor techniques and subsequent client behavioral outcomes. The purpose of this study was to examine this hypothesized technical component of MI in smoking cessation treatment in more depth.
Method
Secondary analysis of 106 first treatment sessions, derived from the Swedish National Tobacco Quitline, and previously rated using the Motivational Interviewing Sequential Code for Observing Process Exchanges (MI-SCOPE) Coder's Manual and the Motivational Interviewing Treatment Integrity code (MITI) Manual, version 3.1. The outcome measure was self-reported 6-month continuous abstinence at 12-month follow-up.
Results
Sequential analyses indicated that clients were significantly more likely than expected by chance to argue for change (change talk) following MI-consistent behaviors and questions and reflections favoring change. Conversely, clients were more likely to argue against change (sustain talk) following questions and reflections favoring status-quo. Parallel mediation analysis revealed that a counselor technique (reflections of client sustain talk) had an indirect effect on smoking outcome at follow-up through client language mediators.
Conclusions
The study makes a significant contribution to our understanding of how MI works in smoking cessation treatment and adds further empirical support for the hypothesized technical component in MI. The results emphasize the importance of counselors avoiding unintentional reinforcement of sustain talk and underline the need for a greater emphasis on the direction of questions and reflections in MI trainings and fidelity measures.","Helena Lindqvist and Lars Forsberg and Pia Enebrink and Gerhard Andersson and Ingvar Rosendahl",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Machine Learning Methods to Extract Documentation of Breast Cancer Symptoms From Electronic Health Records","Context
Clinicians document cancer patients' symptoms in free-text format within electronic health record visit notes. Although symptoms are critically important to quality of life and often herald clinical status changes, computational methods to assess the trajectory of symptoms over time are woefully underdeveloped.
Objectives
To create machine learning algorithms capable of extracting patient-reported symptoms from free-text electronic health record notes.
Methods
The data set included 103,564 sentences obtained from the electronic clinical notes of 2695 breast cancer patients receiving paclitaxel-containing chemotherapy at two academic cancer centers between May 1996 and May 2015. We manually annotated 10,000 sentences and trained a conditional random field model to predict words indicating an active symptom (positive label), absence of a symptom (negative label), or no symptom at all (neutral label). Sentences labeled by human coder were divided into training, validation, and test data sets. Final model performance was determined on 20% test data unused in model development or tuning.
Results
The final model achieved precision of 0.82, 0.86, and 0.99 and recall of 0.56, 0.69, and 1.00 for positive, negative, and neutral symptom labels, respectively. The most common positive symptoms were pain, fatigue, and nausea. Machine-based labeling of 103,564 sentences took two minutes.
Conclusion
We demonstrate the potential of machine learning to gather, track, and analyze symptoms experienced by cancer patients during chemotherapy. Although our initial model requires further optimization to improve the performance, further model building may yield machine learning methods suitable to be deployed in routine clinical care, quality improvement, and research applications.","Alexander W. Forsyth and Regina Barzilay and Kevin S. Hughes and Dickson Lui and Karl A. Lorenz and Andrea Enzinger and James A. Tulsky and Charlotta Lindvall",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Implementation of an efficient logarithmic-Hamiltonian three-body code","The numerical integration of the gravitational few-body problem using the logarithmic-Hamiltonian leapfrog algorithm has been found to produce highly accurate results, especially when combined with the extrapolation method. We describe in detail an implementation of the algorithm to the three-body problem. The considered algorithm has proved itself in numerous simulations and is believed to have led to one of the simplest well working codes for the problem. The advantage of the present code is its relative simplicity since no coordinate transformations are required and therefore adaptations to special purposes are straightforward.","Seppo Mikkola and Kiyotaka Tanikawa",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Non-linearity issues and multiple ionization satellites in the PIXE portion of spectra from the Mars alpha particle X-ray spectrometer","Spectra from the laboratory and flight versions of the Curiosity rover’s alpha particle X-ray spectrometer were fitted with an in-house version of GUPIX, revealing departures from linear behavior of the energy-channel relationships in the low X-ray energy region where alpha particle PIXE is the dominant excitation mechanism. The apparent energy shifts for the lightest elements present were attributed in part to multiple ionization satellites and in part to issues within the detector and/or the pulse processing chain. No specific issue was identified, but the second of these options was considered to be the more probable. Approximate corrections were derived and then applied within the GUAPX code which is designed specifically for quantitative evaluation of APXS spectra. The quality of fit was significantly improved. The peak areas of the light elements Na, Mg, Al and Si were changed by only a few percent in most spectra. The changes for elements with higher atomic number were generally smaller, with a few exceptions. Overall, the percentage peak area changes are much smaller than the overall uncertainties in derived concentrations, which are largely attributable to the effects of rock heterogeneity. The magnitude of the satellite contributions suggests the need to incorporate these routinely in accelerator-based PIXE using helium beams.","John L. Campbell and Christopher M. Heirwegh and Brianna Ganly",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An empirical evaluation of supervised learning approaches in assigning diagnosis codes to electronic medical records","Background
Diagnosis codes are assigned to medical records in healthcare facilities by trained coders by reviewing all physician authored documents associated with a patient's visit. This is a necessary and complex task involving coders adhering to coding guidelines and coding all assignable codes. With the popularity of electronic medical records (EMRs), computational approaches to code assignment have been proposed in the recent years. However, most efforts have focused on single and often short clinical narratives, while realistic scenarios warrant full EMR level analysis for code assignment.
Objective
We evaluate supervised learning approaches to automatically assign international classification of diseases (ninth revision) – clinical modification (ICD-9-CM) codes to EMRs by experimenting with a large realistic EMR dataset. The overall goal is to identify methods that offer superior performance in this task when considering such datasets.
Methods
We use a dataset of 71,463 EMRs corresponding to in-patient visits with discharge date falling in a two year period (2011–2012) from the University of Kentucky (UKY) Medical Center. We curate a smaller subset of this dataset and also use a third gold standard dataset of radiology reports. We conduct experiments using different problem transformation approaches with feature and data selection components and employing suitable label calibration and ranking methods with novel features involving code co-occurrence frequencies and latent code associations.
Results
Over all codes with at least 50 training examples we obtain a micro F-score of 0.48. On the set of codes that occur at least in 1% of the two year dataset, we achieve a micro F-score of 0.54. For the smaller radiology report dataset, the classifier chaining approach yields best results. For the smaller subset of the UKY dataset, feature selection, data selection, and label calibration offer best performance.
Conclusions
We show that datasets at different scale (size of the EMRs, number of distinct codes) and with different characteristics warrant different learning approaches. For shorter narratives pertaining to a particular medical subdomain (e.g., radiology, pathology), classifier chaining is ideal given the codes are highly related with each other. For realistic in-patient full EMRs, feature and data selection methods offer high performance for smaller datasets. However, for large EMR datasets, we observe that the binary relevance approach with learning-to-rank based code reranking offers the best performance. Regardless of the training dataset size, for general EMRs, label calibration to select the optimal number of labels is an indispensable final step.","Ramakanth Kavuluru and Anthony Rios and Yuan Lu",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"History and genomic sequence analysis of the herpes simplex virus 1 KOS and KOS1.1 sub-strains","A collection of genomic DNA sequences of herpes simplex virus (HSV) strains has been defined and analyzed, and some information is available about genomic stability upon limited passage of viruses in culture. The nature of genomic change upon extensive laboratory passage remains to be determined. In this report we review the history of the HSV-1 KOS laboratory strain and the related KOS1.1 laboratory sub-strain, also called KOS (M), and determine the complete genomic sequence of an early passage stock of the KOS laboratory sub-strain and a laboratory stock of the KOS1.1 sub-strain. The genomes of the two sub-strains are highly similar with only five coding changes, 20 non-coding changes, and about twenty non-ORF sequence changes. The coding changes could potentially explain the KOS1.1 phenotypic properties of increased replication at high temperature and reduced neuroinvasiveness. The study also provides sequence markers to define the provenance of specific laboratory KOS virus stocks.","Robert C. Colgrove and Xueqiao Liu and Anthony Griffiths and Priya Raja and Neal A. Deluca and Ruchi M. Newman and Donald M. Coen and David M. Knipe",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"F-TRIDYN simulations of tungsten self-sputtering and applications to coupling plasma and material codes","Fractal-TRIDYN (F-TRIDYN) is an upgraded version of the Monte Carlo, Binary Collision Approximation code TRIDYN for simulating ion-surface interactions. F-TRIDYN adds an explicit model of surface roughness and additional output modes for coupling to both plasma and material codes. Code-coupling represents a compelling path toward whole-device modeling, especially for future fusion reactors. Whole device models need to span length and time scales of many orders of magnitude. Atomic processes in materials that occur on the order of picoseconds, such as changes to surface morphology, will have an effect on fusion plasma performance over many hours of operational time. Conversely, interactions with the plasma will drive chemical, thermal, and morphological processes in the material. Simulating this complex interaction between the plasma and the plasma-facing material demands fast interfaces between material and plasma codes. F-TRIDYN is a flexible code for simulating atomic-scale ion-surface interactions, which are responsible for interactions between plasma and surface such as sputtering and implantation. F-TRIDYNs surface roughness model allows the effect of surface roughness on ion-surface interactions to be simulated. Surface roughness can significantly alter sputtering yields and other ion-surface interaction quantities. Understanding the role surface roughness plays in Plasma-Material Interactions will be crucial to modeling the performance of future fusion reactors such as ITER. F-TRIDYN is also suited for the simulation of a wider range of plasma-surface interactions where surface morphology may play a role, including those utilized for sputter-coating and plasma treating applications.","Jon Drobny and Davide Curreli",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Spatial imagery of novel places based on visual scene transformation","The hippocampus is known to maintain memories of object-place associations that can produce a scene expectation at a novel viewpoint. To implement such capabilities, the memorized distances and directions of an object from the viewer at a fixed location should be integrated with the imaginary displacement to the new viewpoint. However, neural dynamics of such scene expectation at the novel viewpoint have not been discussed. In this study, we propose a method of coding novel places based on visual scene transformation as a component of the object-place memory in the hippocampus. In this coding, a novel place is represented by a transformed version of a viewer’s scene with imaginary displacement. When the places of individual objects are stored with the coding in the hippocampus, the object’s displacement at the imaginary viewpoint can be evaluated through the comparison of a transformed viewer’s scene with the stored scene. Results of computer experiments demonstrated that the coding successfully produced scene expectation of a three object arrangement at a novel viewpoint. Such the scene expectation was retained even without similarities between the imaginary scene and the real scene at the location, where the imaginary scenes only functioned as indices to denote the topographical relationship between object locations. The results suggest that the hippocampus uses the place coding based on scene transformation and implements the spatial imagery of object-place associations from the novel viewpoint.","Naoyuki Sato",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"mRPL: Boosting mobility in the Internet of Things","The 6loWPAN (the light version of IPv6) and RPL (routing protocol for low-power and lossy links) protocols have become de facto standards for the Internet of Things (IoT). In this paper, we show that the two native algorithms that handle changes in network topology – the Trickle and Neighbor Discovery algorithms – behave in a reactive fashion and thus are not prepared for the dynamics inherent to nodes mobility. Many emerging and upcoming IoT application scenarios are expected to impose real-time and reliable mobile data collection, which are not compatible with the long message latency, high packet loss and high overhead exhibited by the native RPL/6loWPAN protocols. To solve this problem, we integrate a proactive hand-off mechanism (dubbed smart-HOP) within RPL, which is very simple, effective and backward compatible with the standard protocol. We show that this add-on halves the packet loss and reduces the hand-off delay dramatically to one tenth of a second, upon nodes’ mobility, with a sub-percent overhead. The smart-HOP algorithm has been implemented and integrated in the Contiki 6LoWPAN/RPL stack (source-code available on-line mrpl: smart-hop within rpl, 2014) and validated through extensive simulation and experimentation.","Hossein Fotouhi and Daniel Moreira and Mário Alves",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"PSCAD modeling of a two-level space vector pulse width modulation algorithm for power electronics education","This paper presents the design details of a two-level space vector pulse width modulation algorithm in PSCAD that is able to generate pulses for three-phase two-level DC/AC converters with two different switching patterns. The presented FORTRAN code is generic and can be easily modified to meet many other kinds of space vector modulation strategies. The code is also editable for hardware programming. The new component is tested and verified by comparing its output as six gating signals with those of a similar component in MATLAB library. Moreover the component is used to generate digital signals for closed-loop control of STATCOM for reactive power compensation in PSCAD. This add-on can be an effective tool to give students better understanding of the space vector modulation algorithm for different control tasks in power electronics area, and can motivate them for learning.","Ahmet Mete Vural",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Semi-supervised manifold-embedded hashing with joint feature representation and classifier learning","Recently, learning-based hashing methods which are designed to preserve the semantic information, have shown promising results for approximate nearest neighbor (ANN) search problems. However, most of these methods require a large number of labeled data which are difficult to access in many real applications. With very limited labeled data available, in this paper we propose a semi-supervised hashing method by integrating manifold embedding, feature representation and classifier learning into a joint framework. Specifically, a semi-supervised manifold embedding is explored to simultaneously optimize feature representation and classifier learning to make the learned binary codes optimal for classification. A two-stage hashing strategy is proposed to effectively address the corresponding optimization problem. At the first stage, an iterative algorithm is designed to obtain a relaxed solution. At the second stage, the hashing function is refined by introducing an orthogonal transformation to reduce the quantization error. Extensive experiments on three benchmark databases demonstrate the effectiveness of the proposed method in comparison with several state-of-the-art hashing methods.","Tiecheng Song and Jianfei Cai and Tianqi Zhang and Chenqiang Gao and Fanman Meng and Qingbo Wu",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Impact of introduction of the 9-valent human papillomavirus vaccine on vaccination coverage of youth in North Carolina","Objectives
The objective of this study was to evaluate the impact of introduction of 9vHPV vaccine on HPV vaccination uptake (doses per capita) and initiation (≥1 doses), completion (≥3 doses) and compliance (≥3 doses within 12 months) by adolescents.
Methods
We used a retrospective cohort analysis using North Carolina Immunization Registry (NCIR) data from January 2008 through October 2016. The sample included Vaccines for Children eligible adolescents aged 9 to 17 years in 2016, for whom the NCIR contains complete vaccination history. We applied an interrupted time series design to measure associations between ZIP Code Tabulation Area (ZCTA)-level HPV vaccination outcomes over time with the introduction of 9vHPV in North Carolina (NC) in July 2015.
Results
Each outcome displayed a linear upward trend over time with large seasonal spikes near August of each year, corresponding to the time when adolescents often receive other vaccines required for school entry. After accounting for these underlying trends, introduction of 9vHPV was not associated with a change in publicly funded HPV vaccination rates in NC.
Conclusions
Our results indicate that 9vHPV substituted for 4vHPV in the first year after release in NC, but the release of 9vHPV was not associated with an overall change in HPV vaccination.","Justin G. Trogdon and Paul Shafer and Brianna Lindsay and Tamera Coyne-Beasley",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Evaluation and utilization of CloudSat and CALIPSO data to analyze the impact of dust aerosol on the microphysical properties of cirrus over the Tibetan Plateau","The present study elucidates on the evaluation of two versions (V3 and V4.10) of vertical feature mask (VFM) and aerosol sub-types data derived from the Cloud-Aerosol LiDAR and Infrared Pathfinder Satellite Observations (CALIPSO), and its utilization to analyze the impact of dust aerosol on the microphysical properties of cirrus over the Tibetan Plateau (TP). In conjunction to the CALIPSO, we have also used the CloudSat data to study the same during the summer season for the years 2007–2010 over the study area 25–40°N and 75–100°E. Compared to V3 of CALIPSO, V4.10 was found to have undergone substantial changes in the code, algorithm, and data products. Intercomparison of both versions of data products in the selected grid between 30–31°N and 83–84°E within the study area during 2007–2017 revealed that the VFM and aerosol sub-types are in good agreement of ∼95.27% and ∼82.80%, respectively. Dusty cirrus is defined as the clouds mixed with dust aerosols or existing in dust aerosol conditions, while the pure cirrus is that in a dust-free environment. The obtained results illustrated that the various microphysical properties of cirrus, namely ice water content (IWC), ice water path (IWP), ice distribution width (IDW), ice effective radius (IER), and ice number concentration (INC) noticed a decrease of 17%, 18%, 4%, 19%, and 10%, respectively due to the existence of dust aerosol, consistent with the classical “Twomey effect” for liquid clouds. Moreover, the aerosol optical depth (AOD) showed moderate negative correlations between −0.4 and −0.6 with the microphysical characteristics of cirrus. As our future studies, in addition to the present work undertaken, we planned to gain knowledge and interested to explore the impact of a variety of aerosols apart from the dust aerosol on the microphysical properties of cirrus in different regions of China.","Baiwan Pan and Zhendong Yao and Minzhong Wang and Honglin Pan and Lingbing Bu and K. Raghavendra Kumar and Haiyang Gao and Xingyou Huang",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"On a pursuit for perfecting an undergraduate requirements engineering course","Requirements Engineering (RE) is an essential component of any software development cycle. Understanding and satisfying stakeholder needs and wants is the difference between the success and failure of a product. However, RE is often perceived as a “soft” skill by students and is often ignored by students who prioritize the learning of coding, testing, and algorithmic thinking. This view contrasts with the industry, where “soft” skills are instead valued equal to any other engineering ability. A key challenge in teaching RE is that students who are accustomed to technical work have a hard time relating to something that is non-technical. Furthermore, students are rarely afforded the opportunity to practice requirements elicitation and management skills in a meaningful way while learning the RE concepts as an adjunct to other content. At Rose-Hulman, several project-based approaches have been experimented with in teaching RE, and these have evolved over time. In this paper, the progress of teaching methodologies is documented to capture the pros and cons of these varied approaches, and to reflect on what worked and what did not in teaching RE to undergraduate engineering students.","Chandan R. Rupakheti and Mark Hays and Sriram Mohan and Stephen Chenoweth and Amanda Stouder",2018,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR11"
"Real Time Strategy Games: A Reinforcement Learning Approach","In this paper we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method we use Q-learning1 and SARSA1 algorithms with generalised reward function to train the reinforcement learning agent. We evaluated the performance of our proposed algorithms on Real Time Strategy (RTS) game called BattleCity. There are two main advantages of having such an approach as compared to other works in RTS. (1) We can ignore the concept of a simulator which is often game specific and is usually hard coded in any type of RTS games (2) our system can learn from interaction with any opponents and quickly change the strategy according to the opponents and do not need any human traces as used in previous works.","Harshit Sethy and Amit Patel and Vineet Padmanabhan",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"ASSERT-PV 3.2: Advanced subchannel thermalhydraulics code for CANDU fuel bundles","Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The most recent release version, ASSERT-PV 3.2 has enhanced phenomenon models for improved predictions of flow distribution, dryout power and CHF location, and post-dryout (PDO) sheath temperature in horizontal CANDU fuel bundles. The focus of the improvements is mainly on modeling considerations for the unique features of CANDU bundles such as horizontal flows, small pitch to diameter ratios, high mass fluxes, and mixed and irregular subchannel geometries, compared to PWR/BWR fuel assemblies. This paper provides a general introduction to ASSERT-PV 3.2, and describes the model changes or additions in the new version to improve predictions of flow distribution, dryout power and CHF location, and PDO sheath temperatures in CANDU fuel bundles.","Y.F. Rao and Z. Cheng and G.M. Waddington and A. Nava-Dominguez",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"Continuous validation of ASTEC containment models and regression testing","The focus of the ASTEC (Accident Source Term Evaluation Code) development at GRS is primarily on the containment module CPA (Containment Part of ASTEC), whose modelling is to a large extent based on the GRS containment code COCOSYS (COntainment COde SYStem). Validation is usually understood as the approval of the modelling capabilities by calculations of appropriate experiments done by external users different from the code developers. During the development process of ASTEC CPA, bugs and unintended side effects may occur, which leads to changes in the results of the initially conducted validation. Due to the involvement of a considerable number of developers in the coding of ASTEC modules, validation of the code alone, even if executed repeatedly, is not sufficient. Therefore, a regression testing procedure has been implemented in order to ensure that the initially obtained validation results are still valid with succeeding code versions. Within the regression testing procedure, calculations of experiments and plant sequences are performed with the same input deck but applying two different code versions. For every test-case the up-to-date code version is compared to the preceding one on the basis of physical parameters deemed to be characteristic for the test-case under consideration. In the case of post-calculations of experiments also a comparison to experimental data is carried out. Three validation cases from the regression testing procedure are presented within this paper. The very good post-calculation of the HDR E11.1 experiment shows the high quality modelling of thermal-hydraulics in ASTEC CPA. Aerosol behaviour is validated on the BMC VANAM M3 experiment, and the results show also a very good agreement with experimental data. Finally, iodine behaviour is checked in the validation test-case of the THAI IOD-11 experiment. Within this test-case, the comparison of the ASTEC versions V2.0r1 and V2.0r2 shows how an error was detected by the regression testing procedure and why the regression testing is an important part of the validation process. The corrected version V2.0r2 delivers a very good validation result for the iodine behaviour in the post-calculation of the THAI IOD-11 experiment.","Holger Nowack and Nils Reinke and Martin Sonnenkalb",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Pulsed laser annealing of carbon black","Laser heating was used to study the rates and trajectories of carbon black during the earliest stages of annealing. A commercial carbon black, Regal 250 (R250 Cabot Corporation) was heated with a Q-switched Nd:YAG laser and a continuous wave CO2 laser. Structural transformations were observed with transmission electron microscopy. Micrographs were processed with in-house codes for the purpose of extracting distributions of fringe length, tortuosity (curvature), and number of lamellae per stack. Time-temperature-histories with nanosecond temporal resolution and temperature reproducibility within tens of degrees Celsius were determined by spectrally resolving the laser induced incandescence signal and applying multi-wavelength pyrometry. The Nd:YAG laser fluences include: 25, 50, 100, 200, 300, and 550 mJ/cm2. The maximum observed temperature ranged from 2400 °C to the C2 sublimation temperature of 4180 °C. The CO2 laser was used to collect a series of isothermal (2600 °C) heat treatments versus time (100 ms–20 s). Laser heated samples are compared against R250 annealed in a furnace at 2600 °C. The material transformation trajectory of Nd:YAG laser heated R250 was different than the traditional furnace heating. The traditional furnace annealing pathway is followed for CO2 laser heating as based upon equivalent end structures.","Joseph P. Abrahamson and Madhu Singh and Jonathan P. Mathews and Randy L. Vander Wal",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Performance of Direct Displacement Based Design on Regular Concrete Building Against Indonesian Response Spectrum","The renewal of Indonesian seismic code from SNI 1726-2002 into SNI 1726-2012 brings significant change in the design spectrum. Focused on several regular plan concrete building which have been design using displacement based design method, the aim of this study is to verify their performance using nonlinear time history analysis based on parameters: drift, damage indices, and plastic mechanism determined by FEMA 356. The excitation is spectrum consistent accelerogram based on El-Centro 1940 N-S, to match with the new Indonesian response spectrum for soft soil in low- and high intensity area. It is found that the code-designed buildings are not suitable for the targeted design of level-2 with maximum drift of 2.5% due to major. This is caused by improper selection of SNI spectrum as the design major earthquake. In fact, it is only equivalent to small earthquake. Although buildings survive up to a very rare earthquake without collapse but they suffer excessive damage and rotation due to small- to major-earthquake. The capacity design procedure is able to maintain ductile mechanism, but some columns experience yielding at prohibited locations.","Ima Muljati and Benjamin Lumantarna and Reynaldo P. Intan and Arygianny Valentino",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Quality improvement of International Classification of Diseases, 9th revision, diagnosis coding in radiation oncology: Single-institution prospective study at University of California, San Francisco","Purpose
Accurate International Classification of Diseases (ICD) diagnosis coding is critical for patient care, billing purposes, and research endeavors. In this single-institution study, we evaluated our baseline ICD-9 (9th revision) diagnosis coding accuracy, identified the most common errors contributing to inaccurate coding, and implemented a multimodality strategy to improve radiation oncology coding.
Methods and materials
We prospectively studied ICD-9 coding accuracy in our radiation therapy--specific electronic medical record system. Baseline ICD-9 coding accuracy was obtained from chart review targeting ICD-9 coding accuracy of all patients treated at our institution between March and June of 2010. To improve performance an educational session highlighted common coding errors, and a user-friendly software tool, RadOnc ICD Search, version 1.0, for coding radiation oncology specific diagnoses was implemented. We then prospectively analyzed ICD-9 coding accuracy for all patients treated from July 2010 to June 2011, with the goal of maintaining 80% or higher coding accuracy. Data on coding accuracy were analyzed and fed back monthly to individual providers.
Results
Baseline coding accuracy for physicians was 463 of 661 (70%) cases. Only 46% of physicians had coding accuracy above 80%. The most common errors involved metastatic cases, whereby primary or secondary site ICD-9 codes were either incorrect or missing, and special procedures such as stereotactic radiosurgery cases. After implementing our project, overall coding accuracy rose to 92% (range, 86%-96%). The median accuracy for all physicians was 93% (range, 77%‐100%) with only 1 attending having accuracy below 80%. Incorrect primary and secondary ICD-9 codes in metastatic cases showed the most significant improvement (10% vs 2% after intervention).
Conclusions
Identifying common coding errors and implementing both education and systems changes led to significantly improved coding accuracy. This quality assurance project highlights the potential problem of ICD-9 coding accuracy by physicians and offers an approach to effectively address this shortcoming.","Chien P. Chen and Steve Braunstein and Michelle Mourad and I-Chow J. Hsu and Daphne Haas-Kogan and Mack Roach and Shannon E. Fogh",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A perturbation-based susbtep method for coupled depletion Monte-Carlo codes","Coupled Monte Carlo (MC) methods are becoming widely used in reactor physics analysis and design. Many research groups therefore, developed their own coupled MC depletion codes. Typically, in such coupled code systems, neutron fluxes and cross sections are provided to the depletion module by solving a static neutron transport problem. These fluxes and cross sections are representative only of a specific time-point. In reality however, both quantities would change through the depletion time interval. Recently, Generalized Perturbation Theory (GPT) equivalent method that relies on collision history approach was implemented in Serpent MC code. This method was used here to calculate the sensitivity of each nuclide and reaction cross section due to the change in concentration of every isotope in the system. The coupling method proposed in this study also uses the substep approach, which incorporates these sensitivity coefficients to account for temporal changes in cross sections. As a result, a notable improvement in time dependent cross section behavior was obtained. The method was implemented in a wrapper script that couples Serpent with an external depletion solver. The performance of this method was compared with other existing methods. The results indicate that the proposed method requires substantially less MC transport solutions to achieve the same accuracy.","Dan Kotlyar and Manuele Aufiero and Eugene Shwageraus and Massimiliano Fratoni",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Joint palaeoclimate reconstruction from pollen data via forward models and climate histories","We present a method and software for reconstructing palaeoclimate from pollen data with a focus on accounting for and reducing uncertainty. The tools we use include: forward models, which enable us to account for the data generating process and hence the complex relationship between pollen and climate; joint inference, which reduces uncertainty by borrowing strength between aspects of climate and slices of the core; and dynamic climate histories, which allow for a far richer gamut of inferential possibilities. Through a Monte Carlo approach we generate numerous equally probable joint climate histories, each of which is represented by a sequence of values of three climate dimensions in discrete time, i.e. a multivariate time series. All histories are consistent with the uncertainties in the forward model and the natural temporal variability in climate. Once generated, these histories can provide most probable climate estimates with uncertainty intervals. This is particularly important as attention moves to the dynamics of past climate changes. For example, such methods allow us to identify, with realistic uncertainty, the past century that exhibited the greatest warming. We illustrate our method with two data sets: Laguna de la Roya, with a radiocarbon dated chronology and hence timing uncertainty; and Lago Grande di Monticchio, which contains laminated sediment and extends back to the penultimate glacial stage. The procedure is made available via an open source R package, Bclim, for which we provide code and instructions.","Andrew C. Parnell and John Haslett and James Sweeney and Thinh K. Doan and Judy R.M. Allen and Brian Huntley",2016,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Development of an integrated fission product release and transport code for spatially resolved full-core calculations of V/HTRs","The computer codes FRESCO-I, FRESCO-II, PANAMA and SPATRA developed at Forschungszentrum Jülich in Germany in the early 1980s are essential tools to predict the fission product release from spherical fuel elements and the TRISO fuel performance, respectively, under given normal or accidental conditions. These codes are able to calculate a conservative estimation of the source term, i.e. quantity and duration of radionuclide release. Recently, these codes have been reversed engineered, modernized (FORTRAN 95/2003) and combined to form a consistent code named STACY (Source Term Analysis Code System). STACY will later become a module of the V/HTR Code Package (HCP). In addition, further improvements have been implemented to enable more detailed calculations. For example the distinct temperature profile along the pebble radius is now taken into account and coated particle failure rates can be calculated under normal operating conditions. In addition, the absolute fission product release of an V/HTR pebble bed core can be calculated by using the newly developed burnup code Topological Nuclide Transformation (TNT) replacing the former rudimentary approach. As a new functionality, spatially resolved fission product release calculations for normal operating conditions as well as accident conditions can be performed. In case of a full-core calculation, a large number of individual pebbles which follow a random path through the reactor core can be simulated. The history of the individual pebble is recorded, too. Main input data such as spatially resolved neutron fluxes and fluid dynamics data are provided by the VSOP code. Capabilities of the FRESCO-I and SPATRA code which allow for the simulation of the redistribution of fission products within the primary circuit and the deposition of fission products on graphitic and metallic surfaces are also available in STACY. In this paper, details of the STACY model and first results for its application to the 200MW(th) HTR-Module are presented.","Andre Xhonneux and Hans-Josef Allelein",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Revised calculation of four-particle harmonic-oscillator transformation brackets matrix","In this article we present a new, considerably enhanced and more rapid method for calculation of the matrix of four-particle harmonic-oscillator transformation brackets (4HOB). The new method is an improved version of 4HOB matrix calculations which facilitates the matrix calculation by finding the eigenvectors of the 4HOB matrix explicitly. Using this idea the new Fortran code for fast and 4HOB matrix calculation is presented. The calculation time decreases more than a few hundred times for large matrices. As many problems of nuclear and hadron physics structure are modeled on the harmonic oscillator (HO) basis our presented method can be useful for large-scale nuclear structure and many-particle identical fermion systems calculations.
Program summary
Title of program: HOTB_M Catalogue identifier: AEFQ_v3_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEFQ_v3_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License version 3 No. of lines in distributed program, including test data, etc.: 2149 No. of bytes in distributed program, including test data, etc.: 17576 Distribution format: tar.gz Programming language: Fortran 90. Computer: Any computer with Fortran 90 compiler. Operating system: Windows, Linux, FreeBSD, True64 Unix. RAM: Up to a few Gigabytes (see Table 1, Table 2 included in the distribution package) Classification: 17.16, 17.17. Catalogue identifier of previous version: AEFQ_v2_0 Journal reference of previous version: Comput. Phys. Comm. 182(2011)1377 Does the new version supersede the previous version?: Yes Nature of problem: Calculation of the matrix of the 4HOB in a more effective way, which allows us to calculate the matrix of the brackets up to a few hundred times more rapidly than in a previous version. Solution method: The method is based on compact expressions of 4HOB, presented in [1] and its simplifications presented in this paper. Reasons for new version: We facilitated the calculation of the 4HOB, based on the method presented in the section ’Theoretical aspects’. The new program version gives shorter calculation times for the 4HOB Summary of revisions: New subroutines for calculation of the matrix of the 4HOB. For theoretical issues of revision see the section ’Theoretical aspects’. Restrictions: The 4HOB matrices up to e=28. Running time: Depends on the dimension of the 4HOB matrix (see Table 1, Table 2 included in the distribution file). References: [1] D. Germanas, S. Mickevicius, R.K. Kalinauskas, Calculation of four-particle harmonic-oscillator transformation brackets, Computer Physics Communications 181, 420–425 (2010).","S. Mickevičius and D. Germanas and R.K. Kalinauskas",2013,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Extension and application of the reactor dynamics code DYN3D for Block-type High Temperature Reactors","The reactor code DYN3D was developed at the Helmholtz-Zentrum Dresden-Rossendorf to study steady state and transient behavior of Light Water Reactors. Concerning the neutronics part, the multigroup diffusion or SP3 transport equation based on nodal expansion methods is solved both for hexagonal and square fuel element geometry. To deal with Block-type High Temperature Reactor cores DYN3D was extended to a version DYN3D-HTR. A 3D heat conduction model was introduced to include 3D effects of heat transfer and heat conduction and the detailed structure of the fuel element. Homogenized neutronic cross sections were generated by applying a Monte Carlo approach with resolution of each individual TRISO fuel particle. Results of coupled steady state and transient calculations with 12 energy groups are presented. Transient case studies are control rod insertion, a change of the inlet coolant temperature and a change of the coolant gas mass flow rate. It is shown that DYN3D-HTR is an appropriate code system to simulate steady states and short time transients. Furthermore the necessity of the 3D heat conduction model is demonstrated.","Silvio Baier and Emil Fridman and Soeren Kliem and Ulrich Rohde",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Learning-Induced Plasticity in Medial Prefrontal Cortex Predicts Preference Malleability","Summary
Learning induces plasticity in neuronal networks. As neuronal populations contribute to multiple representations, we reasoned plasticity in one representation might influence others. We used human fMRI repetition suppression to show that plasticity induced by learning another individual’s values impacts upon a value representation for oneself in medial prefrontal cortex (mPFC), a plasticity also evident behaviorally in a preference shift. We show this plasticity is driven by a striatal “prediction error,” signaling the discrepancy between the other’s choice and a subject’s own preferences. Thus, our data highlight that mPFC encodes agent-independent representations of subjective value, such that prediction errors simultaneously update multiple agents’ value representations. As the resulting change in representational similarity predicts interindividual differences in the malleability of subjective preferences, our findings shed mechanistic light on complex human processes such as the powerful influence of social interaction on beliefs and preferences.","Mona M. Garvert and Michael Moutoussis and Zeb Kurth-Nelson and Timothy E.J. Behrens and Raymond J. Dolan",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Change in 1-year hospitalization of overall and older patients with major depressive disorder after second-generation antipsychotics augmentation treatment","Background
Studies on second-generation antipsychotics (SGA) augmentation treatment for older adults with major depressive disorder (MDD) remain limited. We aimed to investigate the effectiveness of SGA augmentation for overall and older patients with MDD inpatient history by assessing the change in 1-year hospitalization before and after SGA augmentation using the latest National Health Insurance Research Database (NHIRD) in Taiwan.
Methods
The samples were MDD patients (ICD-9 CM code: 296.2 and 296.3) who had psychiatric inpatient history. A total of 2602 MDD patients including 430 elderly subjects (age ≥ 60 years) who received SGA augmentation for 8 weeks between January 1998 and December 2012 were included in this 1-year mirror-image study. Outcome measures included number and length of psychiatric and all-cause hospitalizations.
Results
After 8-week continuous SGA augmentation in the study subjects, the total number and days of psychiatric hospitalizations among overall patients reduced by 33.57% (p < .0001) and 18.24% (p < .0001), respectively; the total number and days of psychiatric hospitalizations among older patients (age ≥ 60) reduced by 44.52% (p < .0001) and 27.95% (p < .0001), respectively. Similarly, the total number and days of all-cause hospitalizations were significantly reduced.
Limitations
MDD patients without inpatient history were not included due to data limitation; hence, the results may not be generalized to all patients.
Conclusions
The results support that SGA may be effective in reducing psychiatric and all-cause hospitalization among overall and elderly MDD patients. More studies focusing on the safety of SGA among older MDD patients is warranted.","Chun-Yuan Lin and Te-Jen Lai and Yu-Hsin Wu and Ping-Kun Chen and Yuan-Fu Lin and I.-Chia Chien",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Time as context: The influence of hierarchical patterning on sensory inference","Time, or more specifically temporal structure, is a critical variable in understanding how the auditory system uses acoustic patterns to predict input, and to filter events based on their relevance. A key index of this filtering process is the auditory evoked potential component known as mismatch negativity or MMN. In this paper we review findings of smaller MMN in schizophrenia through the lens of time as an influential contextual variable. More specifically, we review studies that show how MMN to a locally rare pattern-deviation is modulated by the longer-term context in which it occurs. Empirical data is presented from a non-clinical sample confirming that the absence of a stable higher-order structure to sound sequences alters the way MMN amplitude changes over time. This result is discussed in relation to how hierarchical pattern learning might enrich our understanding of how and why MMN amplitude modulation is disrupted in schizophrenia.","Juanita Todd and Anne Petherbridge and Bronte Speirs and Alexander Provost and Bryan Paton",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Property size drives differences in forest code compliance in the Brazilian Cerrado","The Rural Environmental Registry (CAR) dataset opens a new window for spatially explicit studies of the rural landscape of Brazil, enabling analysis with an accurate representation of land use and land cover change dynamics at the property level. Here, we evaluated farm compliance with the Brazilian Forest Code (revised in 2012) in Mato Grosso do Sul state, where agribusiness activities have already converted more than 70% of native vegetation, Cerrado. We analysed the most recent version of the CAR dataset, using geographic information system analytical tools. We observed a positive relationship between compliance with the 20% compulsory Legal Reserves and farm size class. We showed that larger, rather than smaller, farms have important effects on biodiversity conservation at the landscape scale. Large farms (> than 1000 ha), comprising 74.2% of the study area, tended to show better compliance levels (51%) than smaller properties (33%). At the same time, they contain huge amount of land with native vegetation that lies outside Legal Reserves, and so may pose a risk for legal deforestation of near 2 million ha. We argue that a portfolio of socioeconomic incentives for restoration, protected areas, and no-net-loss components in agricultural programmes, are essential measures to increase compliance and halt deforestation in the Cerrado of Central Brazil. Moreover, we argue that considering property size improves the likelihood of success of such initiatives. Although acknowledging that landscape management can help address socioeconomic conflicts and improve food production, it must be accompanied by a strong “anti-deforestation” policy to guarantee the maintenance of existing native vegetation remnants. We also highlight the importance of investigating the role of property size in maintaining remaining vegetation in this region, instead of merely focused on the number of compliant farms.","Mauricio Stefanes and Fabio de Oliveira Roque and Reinaldo Lourival and Isabel Melo and Pierre Cyril Renaud and Jose Manuel Ochoa Quintero",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A numerical methodology for estimation of volatile fission products release from nuclear fuel","In this paper we have developed a numerical methodology which is capable of estimating volatile fission product release from nuclear fuel under changing irradiation conditions with incorporation of all physical phenomena’s. The present study bridges the gap between previous analytical studies by providing estimation of unstable fission product release under accidental condition with and without incorporation of precursor’s history effect. Based on the present methodology a computer model “FIPRAP-Fission Product Release Analysis Program” has been developed and validated with standard fission product release problems and benchmarked against internationally accepted CORSOR-BOOTH model as well as Accident Source Term Evaluation Code (ASTEC). The result of the proposed model lies within 15% of internationally accepted models.","Mahender Singh and Deb Mukhopadhyay and D. Datta",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An Innovative Self-learning Approach to 3D Printing Using Multimedia and Augmented Reality on Mobile Devices","Technology is evolving rapidly and it is becoming harder to keep up its pace. At a university environment, important investments are required so that students and professors can have access to novel technology. However, it is also fundamental to know how to use this novel technology to exploit its benefits. Tecnológico de Monterrey, Campus Monterrey, recently acquired several 3D printers so that the students could get familiar with this rapid prototyping technology and use them to deliver better quality projects. Nevertheless, the new challenge was to administrate the 3D printing process including 3D model verification, STL file generation, printing time and raw material. Since students were not familiar with 3D printing, the expert had to spend a lot of time with them explaining the whole process from the modelling to the printing stage, and verifying their work to make an efficient use of 3D printing time and materials. To overcome this situation, it was decided to make use of augmented reality and multimedia applications to generate tutorials for self-learning the whole process of 3D printing. Nowadays, the wide spread of mobile devices and wireless technologies brings a huge potential to e-learning changing dramatically the traditional instructor-oriented scheme. The learning process can take place in an informal setting having the tutorials available on mobile devices by just scanning a quick response code, usually known as QR code. The first QR code enables a link to download the free augmented reality Layar app. Then, several images can be scanned using Layar to open each of the video tutorials. These videos explain graphically step by step of all the processes involved and give different options of software to be used. The tutorials are easy to follow so that any engineering or design student can learn from them. This case-study offers an innovative learning approach that fosters self-learning and a more efficient use of technology resources.","Vianney Lara-Prieto and Efraín Bravo-Quirino and Miguel Ángel Rivera-Campa and José Enrique Gutiérrez-Arredondo",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Exogenously-driven perceptual alternation of a bistable image: From the perspective of the visual change detection process","Based on the predictive coding framework, the present behavioral study focused on the automatic visual change detection process, which yields a concomitant prediction error, as one of the visual processes relevant to the exogenously-driven perceptual alternation of a bistable image. According to this perspective, we speculated that the automatic visual change detection process with an enhanced prediction error is relevant to the greater induction of exogenously-driven perceptual alternation and attempted to test this hypothesis. A modified version of the oddball paradigm was used based on previous electroencephalographic studies on visual change detection, in which the deviant and standard defined by the bar’s orientation were symmetrically presented around a continuously presented Necker cube (a bistable image). By manipulating inter-stimulus intervals and the number of standard repetitions, we set three experimental blocks: HM, IM, and LM blocks, in which the strength of the prediction error to the deviant relative to the standard was expected to gradually decrease in that order. The results obtained showed that the deviant significantly increased perceptual alternation of the Necker cube over that by the standard from before to after the presentation of the deviant. Furthermore, the differential proportion of the deviant relative to the standard significantly decreased from the HM block to the IM and LM blocks. These results are consistent with our hypothesis, supporting the involvement of the automatic visual change detection process in the induction of exogenously-driven perceptual alternation.","Tomokazu Urakawa and Tomoya Aragaki and Osamu Araki",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Modification and application of the system analysis code ATHLET to trans-critical simulations","During the loss of coolant accident (LOCA) of supercritical water cooled reactor (SCWR), the pressure in the reactor system will undergo a rapid decrease from supercritical to subcritical condition. This process is called trans-critical transients, which is of crucial importance for the LOCA analysis of SCWR. Using the current version of system code (e.g. ATHLET, REALP), calculation will be terminated due to the abrupt change of void fraction across the critical point (22.064MPa). To solve this problem, a pseudo two-phase method is proposed by introducing a fictitious region of latent heat (enthalpy of vaporization hfg∗) at pseudo-critical temperatures. A smooth transition of void fraction can be realized by using liquid-field conservation equations at temperatures lower than the pseudo-critical temperature, and vapor-field conservation equations at temperatures higher than the pseudo-critical temperature. Adopting this method, the system code ATHLET is modified to ATHLET-SC mod 2 on the basic of the previous version ATHLET-SC mod 1 modified by Shanghai Jiao Tong University. When the fictitious region of latent heat is kept as a small region, the code can achieve an acceptable accuracy. Moreover, the ATHLET-SC mod 2 code is applied to simulate the blowdown process of a simplified model. The results achieved so far indicate a good applicability of the new modified code for the trans-critical transient.","S.W. Fu and X.J. Liu and C. Zhou and Z.H. Xu and Y.H. Yang and X. Cheng",2012,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Advances in integrative statistics for logic programming","We present recent developments on the syntax of Real, a library for interfacing two Prolog systems to the statistical language R. We focus on the changes in Prolog syntax within SWI-Prolog that accommodate greater syntactic integration, enhanced user experience and improved features for web-services. We recount the full syntax and functionality of Real as well as presenting a full application and sister packages which include Prolog code interfacing a number of common and useful tasks that can be delegated to R. We argue that Real is a powerful extension to logic programming, providing access to a popular statistical system that has complementary strengths in areas such as machine learning, statistical inference and visualisation. Furthermore, Real has a central role to play in the uptake of semantic web, computational biology and bioinformatics as application areas for research in logic programming.","Nicos Angelopoulos and Samer Abdallah and Georgios Giamas",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Tensor Discriminant Analysis with Partial Label","Small Sample Size (SSS) and overfitting problems will be encountered if using vector space methods by vectorize high order tensor data. In addition, the structure information will be lost, So thensor based methods or multilinear algorithm will be more approriate for this type of data. For medical data expecially electrocardiograms (ECGs), achieving the diagnosed disease label is time sonsuming and expensive. To overcome this type of problem, this paper propose a algorithm for data with partial label, in other words, our algorithm take labeled data and unlabel data as input data, Anotoher issue is the spaseity of effective features. Here we propose a Sparse Semisupervised Sparse Multilinear Discriminant Analysis (SSSMDA) for electrocardiograms (ECGs), our method consider the distirbution of unlabeled and labeled data, we also consider the label achieved by label propagation algorithm. We convert original 12 lead electrocardiograms (ECGs) to 3-order tensor by Short Time Fourier Transformation (STFT). The effectiveness of our approach is proved in the experiment section.","Kai Huang and Di Zhou and Yingna Cong and Wen Xu and Wei Wang and Zhiguo Que",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Trunk–pelvis coordination during turning: A cross sectional study of young adults with and without a history of low back pain","Background
During steady-state locomotion, symptomatic individuals with low back pain demonstrate reduced ability to modulate coordination between the trunk and the pelvis in the axial plane. It is unclear if this is also true during functional locomotor perturbations such as changing direction, or if this change in coordination adaptability persists between symptomatic episodes. The purpose of this study was to compare trunk–pelvis coordination during walking turns in healthy individuals and asymptomatic individuals with a history of low back pain.
Methods
Participants performed multiple ipsilateral turns. Axial plane inter-segmental coordination and stride-to-stride coordination variability were quantified using the vector coding technique. Frequency of coordination mode and amplitude of coordination variability was compared between groups using Wilcoxon signed-rank tests and paired t-tests respectively.
Findings
During stance phase of the turn, there was no significant difference in either inter-segmental coordination or coordination variability between groups. Inter-segmental coordination between the trunk and the pelvis was predominantly inphase during this part of the turn. During swing phase, patterns of coordination were more diversified, and individuals with a history of low back pain had significantly greater trunk phase coordination than healthy controls. Coordination variability was the same in both groups.
Interpretation
Changes in trunk–pelvis coordination are evident between symptomatic episodes in individuals with a history of low back pain. However, previously demonstrated decreases in coordination variability were not found between symptomatic episodes in individuals with recurrent low back pain and therefore may represent a response to concurrent pain rather than a persistent change in motor control.","Jo Armour Smith and Kornelia Kulig",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Decoding the processing stages of mental arithmetic with magnetoencephalography","Elementary arithmetic is highly prevalent in our daily lives. However, despite decades of research, we are only beginning to understand how the brain solves simple calculations. Here, we applied machine learning techniques to magnetoencephalography (MEG) signals in an effort to decompose the successive processing stages and mental transformations underlying elementary arithmetic. Adults subjects verified single-digit addition and subtraction problems such as 3 + 2 = 9 in which each successive symbol was presented sequentially. MEG signals revealed a cascade of partially overlapping brain states. While the first operand could be transiently decoded above chance level, primarily based on its visual properties, the decoding of the second operand was more accurate and lasted longer. Representational similarity analyses suggested that this decoding rested on both visual and magnitude codes. We were also able to decode the operation type (additions vs. subtraction) during practically the entire trial after the presentation of the operation sign. At the decision stage, MEG indicated a fast and highly overlapping temporal dynamics for (1) identifying the proposed result, (2) judging whether it was correct or incorrect, and (3) pressing the response button. Surprisingly, however, the internally computed result could not be decoded. Our results provide a first comprehensive picture of the unfolding processing stages underlying arithmetic calculations at a single-trial level, and suggest that externally and internally generated neural codes may have different neural substrates.","Pedro Pinheiro-Chagas and Manuela Piazza and Stanislas Dehaene",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Analysis of the effect of climate change on the reliability of overhead transmission lines","Climate change is anticipated to influence the reliability of overhead transmission and distribution lines through impacts on extreme weather events. Changes in the frequency and intensity of wind and ice storms may have a considerable effect on applied loads and can consequently affect the probability of structural failure of different components of the line. This study examines the reliability of transmission lines under a range of assumed changes in the mean and standard deviation of climatic variables affecting transmission lines such as annual extreme wind speed and ice thickness. The methodology used for the reliability analysis of transmission lines under current and future climatic conditions is based on the concepts of statistical learning theory. The sensitivity study provides the information required to improve the capacity of transmission lines and mitigate long-term risks from the effects of a changing climate. The results indicate that climate change as predicted by many researchers can significantly affect the reliability of existing transmission line systems. Hence, relying on the historic climatic data may not be sufficient to ensure an adequate reliability of transmission line systems in the future. The specification of design loads for the evaluation of existing lines or the design of new lines should consider both future climate models and historical climate data.","Seyedeh Nasim Rezaei and Luc Chouinard and Sébastien Langlois and Frédéric Légeron",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Effects of variable sequences of food availability on interval time-place learning by pigeons","The effects of within session variability of the sequences of food availability in a 16 period Time Place Learning (TPL) task on the performance of pigeons were assessed. Two groups of birds were exposed to two conditions. For group 1 (N=3), the first condition consisted of a TPL task in which food could be obtained according to a Random Interval (RI) 25s schedule of reinforcement in one of four feeders, the correct feeder changed every 3min. The same sequence was repeated four times within every training session (Fixed Sequence). The second condition was exactly the same as the first one with the exception that the sequence in which the correct feeder changed was randomized, yielding a total of four randomized sequences of food availability each session (Variable Sequence). An Open Hopper Test (OHT) was conducted at the end of each condition. Birds in group 2 (N=3) experienced the same conditions but in the reverse order. Results showed high percent correct responses for both group of birds under both conditions. However, birds were able to time the availability period’s duration only under the Fixed Sequence condition, as shown by anticipation, anticipation of depletion and persistence of visiting patterns on the OHT. The implications of these results to Gallistels (1990) tripartite time-place-event memory code model are discussed, pointing out that these results are in line with previous findings about the important role that spatial parameters of a TPL task can play, for accurate timing was precluded when a variable sequence was employed.","Daniel García-Gallardo and Claudio Carpio",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Relations between specific and global outcome measures in a social-communication intervention for children with autism spectrum disorder","Assessment of relevant outcomes is a key challenge in evaluating effects of social-communication interventions. However, few studies have investigated in what ways specific and more global measures may influence reported results of social-communication interventions for children with autism spectrum disorder (ASD). In this study both a specific and a global, more global autism symptom measure were used to assess effects of a brief social-communication intervention. Fifty-nine children (2–4 years) diagnosed with autistic disorder were assessed with the Joint Engagement (JE) states coding procedure and a preliminary version of the Brief Observation of Social Communication Change (BOSCC). A statistically significant difference was found between intervention and control groups from baseline to intervention endpoint on JE but not on BOSCC. Degree of change on the measures was moderately related, and both were independent of language level and non-verbal mental age. This study adds to the knowledge of what may be expected of different outcome measures and provides suggestions to how measures may be deployed to investigate underlying mechanisms and developmental pathways.","Anders Nordahl-Hansen and Sue Fletcher-Watson and Helen McConachie and Anett Kaale",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"How Java APIs break – An empirical study","Context
It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs.
Objective
We have studied the extent of the problem in real world programs. We were interested in two aspects: the compatibility of API changes as libraries evolve, and the impact this has on programs using these libraries.
Method
This study is based on the qualitas corpus version 20120401. A data set consisting of 109 Java open-source programs and 564 program versions was used from this corpus. We have investigated two types of library dependencies: explicit dependencies to embedded libraries, and dependencies defined by symbolic references in Maven build files that are resolved at build time. We have used JaCC for API analysis, this tool is based on the popular ASM byte code analysis library.
Results
We found that for most of the programs we investigated, APIs are unstable as incompatible changes are common. Surprisingly, there are more compatibility problems in projects that use automated dependency resolution. However, we found only a few cases where this has an actual impact on other programs using such an API.
Conclusion
It is concluded that API instability is common and causes problems for programs using these APIs. Therefore, better tools and methods are needed to safeguard library evolution.","Kamil Jezek and Jens Dietrich and Premek Brada",2015,"[""Science Direct""]","Rejeitado: CR11","Rejeitado: CR11, CR12"
"Random neural network based cognitive engines for adaptive modulation and coding in LTE downlink systems","This paper presents two random neural network (RNN) based context-aware decision making frameworks to improve adaptive modulation and coding (AMC) in long-term evolution (LTE) downlink systems. In the first framework, AMC is modelled as a traditional classification problem with the aim to maximize the probability of correct classification. The second framework seeks to optimize the throughput as opposed to simply maximizing the probability of the correct classification. To model the second framework, we developed a hybrid cognitive engine (CE) architecture by integrating an RNN based learning algorithm with genetic algorithm (GA) based reasoning. RNN inherent properties help CE to comply with the essential CE design requirement (i.e. concurrent long-term-learning, low computational complexity, and fast decision making). The performance of RNN is compared with artificial neural networks (ANN) and state-of-the-art effective exponential SINR mapping (EESM) algorithm. A comprehensive analysis of the proposed RNN based AMC scheme is presented by jointly incorporating the effect of different schedulers, feedback delays, and multi-antenna diversity on the throughput of an orthogonal frequency-division multiple access (OFDMA) system. The critical analysis of the first framework revealed that RNN based CE can achieve comparable results with faster adaptation, even in severe environment changes without the need of retraining compared to ANN. The analysis of the second approach demonstrated RNNs faster adaptation as compared to ANN and showed upto 253% gain in user throughput. RNN based CE efficiently exploited the channel quality information feedback delay to improve system throughput and helped cell-edge and cell-centre users to experience much better services in terms of achieved throughput as compared to EESM.","Ahsan Adeel and Hadi Larijani and Ali Ahmadinia",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Designing Adult Code Simulations for Antepartum and Postpartum Nurses","Poster Presentation
Purpose for the Program
To improve the ability of nurses in the postpartum and antepartum units to respond in the event of a maternal code. Even very experienced nurses expressed that though they knew how to perform cardiopulmonary resuscitation, the maternal code experience was overwhelming and they felt unprepared.
Proposed Change
The leadership of the antepartum and postpartum units and the nursing simulation faculty at the hospital collaborated to propose a simulation‐based learning experience for the registered nurse (RN) staff of these units. Simulation‐based learning provides an opportunity to practice skills in a safe environment while helping the nurse develop critical thinking skills, promote effective communication, and work collaboratively with other members of the team.
Implementation, Outcomes, and Evaluation
Over a period of months, the program faculty (simulation staff, nurse educators, and staff nurses) created two maternal code simulations. The final program involved three stages: skills sessions, simulated code scenarios, and debriefings. During the skill sessions the staff reviewed skills, such as use of the defibrillator, contents of the code cart, medications used in a code, and communication techniques. The participants toured the simulation lab before the actual simulation to see how the room was set up and how the manikin worked. When it was time for each scenario, participants were given role cards to remind them of critical activities during a code. The primary nurse received a report on the mock patient and the simulation started. One half of the group participated in each simulation whereas the other half watched the scenario on a live feed in a conference room. Immediately after each scenario a debriefing session took place. Consistent themes during debriefing included discussions about the role of the nurse in a code situation, effective communication in an emergency, and the value of effective team work. Without exception, results of the 196 written evaluations indicated that staff nurses felt more knowledgeable and confident about adult codes after the simulation.
Implications for Nursing Practice
Based on the overwhelming positive feedback of simulation‐based learning, the program was offered again the following year to all RN staff. Anecdotally, many nurses in the second year of the simulation program commented that they also felt more confident in other emergencies in the units after having participated in the maternal code simulation the previous year. The current plan is to offer simulation‐based learning experiences to all RNs in the antepartum and postpartum units annually.","Judy P. Brown and Claire Zaya",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Effects of physics change in Monte Carlo code on electron pencil beam dose distributions","Pencil beam algorithms used in computerized electron beam dose planning are usually described using the small angle multiple scattering theory. Alternatively, the pencil beams can be generated by Monte Carlo simulation of electron transport. In a previous work, the 4th version of the Electron Gamma Shower (EGS) Monte Carlo code was used to obtain dose distributions from monoenergetic electron pencil beam, with incident energy between 1MeV and 50MeV, interacting at the surface of a large cylindrical homogeneous water phantom. In 2000, a new version of this Monte Carlo code has been made available by the National Research Council of Canada (NRC), which includes various improvements in its electron-transport algorithms. In the present work, we were interested to see if the new physics in this version produces pencil beam dose distributions very different from those calculated with oldest one. The purpose of this study is to quantify as well as to understand these differences. We have compared a series of pencil beam dose distributions scored in cylindrical geometry, for electron energies between 1MeV and 50MeV calculated with two versions of the Electron Gamma Shower Monte Carlo Code. Data calculated and compared include isodose distributions, radial dose distributions and fractions of energy deposition. Our results for radial dose distributions show agreement within 10% between doses calculated by the two codes for voxels closer to the pencil beam central axis, while the differences are up to 30% for longer distances. For fractions of energy deposition, the results of the EGS4 are in good agreement (within 2%) with those calculated by EGSnrc at shallow depths for all energies, whereas a slightly worse agreement (15%) is observed at deeper distances. These differences may be mainly attributed to the different multiple scattering for electron transport adopted in these two codes and the inclusion of spin effect, which produces an increase of the effective range of electrons.","Abdelkader Toutaoui and Nadia Khelassi-Toutaoui and Zakia Brahimi and Ahmed Chafik Chami",2012,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"NDL-v2.0: A new version of the numerical differentiation library for parallel architectures","We present a new version of the numerical differentiation library (NDL) used for the numerical estimation of first and second order partial derivatives of a function by finite differencing. In this version we have restructured the serial implementation of the code so as to achieve optimal task-based parallelization. The pure shared-memory parallelization of the library has been based on the lightweight OpenMP tasking model allowing for the full extraction of the available parallelism and efficient scheduling of multiple concurrent library calls. On multicore clusters, parallelism is exploited by means of TORC, an MPI-based multi-threaded tasking library. The new MPI implementation of NDL provides optimal performance in terms of function calls and, furthermore, supports asynchronous execution of multiple library calls within legacy MPI programs. In addition, a Python interface has been implemented for all cases, exporting the functionality of our library to sequential Python codes.
New version program summary
Program title: NDL-v2.0 Catalog identifier: AEDG_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDG_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 63036 No. of bytes in distributed program, including test data, etc.: 801872 Distribution format: tar.gz Programming language: ANSI Fortran-77, ANSI C, Python. Computer: Distributed systems (clusters), shared memory systems. Operating system: Linux, Unix. Has the code been vectorized or parallelized?: Yes. RAM: The library uses O(N) internal storage, N being the dimension of the problem. It can use up to O(N2) internal storage for Hessian calculations, if a task throttling factor has not been set by the user. Classification: 4.9, 4.14, 6.5. Catalog identifier of previous version: AEDG_v1_0 Journal reference of previous version: Comput. Phys. Comm. 180(2009)1404 Does the new version supersede the previous version?: Yes Nature of problem: The numerical estimation of derivatives at several accuracy levels is a common requirement in many computational tasks, such as optimization, solution of nonlinear systems, and sensitivity analysis. For a large number of scientific and engineering applications, the underlying functions correspond to simulation codes for which analytical estimation of derivatives is difficult or almost impossible. A parallel implementation that exploits systems with multiple CPUs is very important for large scale and computationally expensive problems. Solution method: Finite differencing is used with a carefully chosen step that minimizes the sum of the truncation and round-off errors. The parallel versions employ both OpenMP and MPI libraries. Reasons for new version: The updated version was motivated by our endeavors to extend a parallel Bayesian uncertainty quantification framework [1], by incorporating higher order derivative information as in most state-of-the-art stochastic simulation methods such as Stochastic Newton MCMC [2] and Riemannian Manifold Hamiltonian MC [3]. The function evaluations are simulations with significant time-to-solution, which also varies with the input parameters such as in [1, 4]. The runtime of the N-body-type of problem changes considerably with the introduction of a longer cut-off between the bodies. In the first version of the library, the OpenMP-parallel subroutines spawn a new team of threads and distribute the function evaluations with a PARALLEL DO directive. This limits the functionality of the library as multiple concurrent calls require nested parallelism support from the OpenMP environment. Therefore, either their function evaluations will be serialized or processor oversubscription is likely to occur due to the increased number of OpenMP threads. In addition, the Hessian calculations include two explicit parallel regions that compute first the diagonal and then the off-diagonal elements of the array. Due to the barrier between the two regions, the parallelism of the calculations is not fully exploited. These issues have been addressed in the new version by first restructuring the serial code and then running the function evaluations in parallel using OpenMP tasks. Although the MPI-parallel implementation of the first version is capable of fully exploiting the task parallelism of the PNDL routines, it does not utilize the caching mechanism of the serial code and, therefore, performs some redundant function evaluations in the Hessian and Jacobian calculations. This can lead to: (a) higher execution times if the number of available processors is lower than the total number of tasks, and (b) significant energy consumption due to wasted processor cycles. Overcoming these drawbacks, which become critical as the time of a single function evaluation increases, was the primary goal of this new version. Due to the code restructure, the MPI-parallel implementation (and the OpenMP-parallel in accordance) avoids redundant calls, providing optimal performance in terms of the number of function evaluations. Another limitation of the library was that the library subroutines were collective and synchronous calls. In the new version, each MPI process can issue any number of subroutines for asynchronous execution. We introduce two library calls that provide global and local task synchronizations, similarly to the BARRIER and TASKWAIT directives of OpenMP. The new MPI-implementation is based on TORC, a new tasking library for multicore clusters [5–7]. TORC improves the portability of the software, as it relies exclusively on the POSIX-Threads and MPI programming interfaces. It allows MPI processes to utilize multiple worker threads, offering a hybrid programming and execution environment similar to MPI+OpenMP, in a completely transparent way. Finally, to further improve the usability of our software, a Python interface has been implemented on top of both the OpenMP and MPI versions of the library. This allows sequential Python codes to exploit shared and distributed memory systems. Summary of revisions: The revised code improves the performance of both parallel (OpenMP and MPI) implementations. The functionality and the user-interface of the MPI-parallel version have been extended to support the asynchronous execution of multiple PNDL calls, issued by one or multiple MPI processes. A new underlying tasking library increases portability and allows MPI processes to have multiple worker threads. For both implementations, an interface to the Python programming language has been added. Restrictions: The library uses only double precision arithmetic. The MPI implementation assumes the homogeneity of the execution environment provided by the operating system. Specifically, the processes of a single MPI application must have identical address space and a user function resides at the same virtual address. In addition, address space layout randomization should not be used for the application. Unusual features: The software takes into account bound constraints, in the sense that only feasible points are used to evaluate the derivatives, and given the level of the desired accuracy, the proper formula is automatically employed. Running time: Running time depends on the function’s complexity. The test run took 23 ms for the serial distribution, 25 ms for the OpenMP with 2 threads, 53 ms and 1.01 s for the MPI parallel distribution using 2 threads and 2 processes respectively and yield-time for idle workers equal to 10 ms. References: [1] P. Angelikopoulos, C. Paradimitriou, P. Koumoutsakos, Bayesian uncertainty quantification and propagation in molecular dynamics simulations: a high performance computing framework, J. Chem. Phys 137 (14). [2] H.P. Flath, L.C. Wilcox, V. Akcelik, J. Hill, B. van Bloemen Waanders, O. Ghattas, Fast algorithms for Bayesian uncertainty quantification in large-scale linear inverse problems based on low-rank partial Hessian approximations, SIAM J. Sci. Comput. 33 (1) (2011) 407–432. [3] M. Girolami, B. Calderhead, Riemann manifold Langevin and Hamiltonian Monte Carlo methods, J. R. Stat. Soc. Ser. B (Stat. Methodol.) 73 (2) (2011) 123–214. [4] P. Angelikopoulos, C. Paradimitriou, P. Koumoutsakos, Data driven, predictive molecular dynamics for nanoscale flow simulations under uncertainty, J. Phys. Chem. B 117 (47) (2013) 14808–14816. [5] P.E. Hadjidoukas, E. Lappas, V.V. Dimakopoulos, A runtime library for platform-independent task parallelism, in: PDP, IEEE, 2012, pp. 229–236. [6] C. Voglis, P.E. Hadjidoukas, D.G. Papageorgiou, I. Lagaris, A parallel hybrid optimization algorithm for fitting interatomic potentials, Appl. Soft Comput. 13 (12) (2013) 4481–4492. [7] P.E. Hadjidoukas, C. Voglis, V.V. Dimakopoulos, I. Lagaris, D.G. Papageorgiou, Supporting adaptive and irregular parallelism for non-linear numerical optimization, Appl. Math. Comput. 231 (2014) 544–559.","P.E. Hadjidoukas and P. Angelikopoulos and C. Voglis and D.G. Papageorgiou and I.E. Lagaris",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Incorporating 3-dimensional models in online articles","Introduction
The aims of this article are to introduce the capability to view and interact with 3-dimensional (3D) surface models in online publications, and to describe how to prepare surface models for such online 3D visualizations.
Methods
Three-dimensional image analysis methods include image acquisition, construction of surface models, registration in a common coordinate system, visualization of overlays, and quantification of changes. Cone-beam computed tomography scans were acquired as volumetric images that can be visualized as 3D projected images or used to construct polygonal meshes or surfaces of specific anatomic structures of interest. The anatomic structures of interest in the scans can be labeled with color (3D volumetric label maps), and then the scans are registered in a common coordinate system using a target region as the reference. The registered 3D volumetric label maps can be saved in .obj, .ply, .stl, or .vtk file formats and used for overlays, quantification of differences in each of the 3 planes of space, or color-coded graphic displays of 3D surface distances.
Results
All registered 3D surface models in this study were saved in .vtk file format and loaded in the Elsevier 3D viewer. In this study, we describe possible ways to visualize the surface models constructed from cone-beam computed tomography images using 2D and 3D figures. The 3D surface models are available in the article's online version for viewing and downloading using the reader's software of choice. These 3D graphic displays are represented in the print version as 2D snapshots. Overlays and color-coded distance maps can be displayed using the reader's software of choice, allowing graphic assessment of the location and direction of changes or morphologic differences relative to the structure of reference. The interpretation of 3D overlays and quantitative color-coded maps requires basic knowledge of 3D image analysis.
Conclusions
When submitting manuscripts, authors can now upload 3D models that will allow readers to interact with or download them. Such interaction with 3D models in online articles now will give readers and authors better understanding and visualization of the results.","Lucia H.S. Cevidanes and Antonio C.O. Ruellas and Julien Jomier and Tung Nguyen and Steve Pieper and Francois Budin and Martin Styner and Beatriz Paniagua",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mitochondrial phylogeny, divergence history and high-altitude adaptation of grassland caterpillars (Lepidoptera: Lymantriinae: Gynaephora) inhabiting the Tibetan Plateau","Grassland caterpillars (Lepidoptera: Lymantriinae: Gynaephora) are the most important pests in alpine meadows of the Tibetan Plateau (TP) and have well adapted to high-altitude environments. To further understand the evolutionary history and their adaptation to the TP, we newly determined seven complete TP Gynaephora mitogenomes. Compared to single genes, whole mitogenomes provided the best phylogenetic signals and obtained robust results, supporting the monophyly of the TP Gynaephora species and a phylogeny of Arctiinae + (Aganainae + Lymantriinae). Incongruent phylogenetic signals were found among single mitochondrial genes, none of which recovered the same phylogeny as the whole mitogenome. We identified six best-performing single genes using Shimodaira-Hasegawa tests and found that the combinations of rrnS and either cox1 or cox3 generated the same phylogeny as the whole mitogenome, indicating the phylogenetic potential of these three genes for future evolutionary studies of Gynaephora. The TP Gynaephora species were estimated to radiate on the TP during the Pliocene and Quaternary, supporting an association of the diversification and speciation of the TP Gynaephora species with the TP uplifts and associated climate changes during this time. Selection analyses revealed accelerated evolutionary rates of the mitochondrial protein-coding genes in the TP Gynaephora species, suggesting that they accumulated more nonsynonymous substitutions that may benefit their adaptation to high altitudes. Furthermore, signals of positive selection were detected in nad5 of two Gynaephora species with the highest altitude-distributions, indicating that this gene may contribute to Gynaephora’s adaptation to divergent altitudes. This study adds to the understanding of the TP Gynaephora evolutionary relationships and suggests a link between mitogenome evolution and ecological adaptation to high-altitude environments in grassland caterpillars.","Ming-Long Yuan and Qi-Lin Zhang and Li Zhang and Cheng-Lin Jia and Xiao-Peng Li and Xing-Zhuo Yang and Run-Qiu Feng",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Improving Orthopedic Resident Knowledge of Documentation, Coding, and Medicare Fraud","Background
Most residency programs still lack formal education and training on the basic clinical documentation and coding principles. Today’s physicians are continuously being held to increasing standards for correct coding and documentation, yet little has changed in the residency training curricula to keep pace with these increasing standards. Although there are many barriers to implementing these topics formally, the main concern has been the lack of time and resources. Thus, simple models may have the best chance for success at widespread implementation.
Purpose
The first goal of the study was to assess a group of orthopedic residents’ fund of knowledge regarding basic clinical documentation guidelines, coding principles, and their ability to appropriately identify cases of Medicare fraud. The second goal was to analyze a single, high-yield educational session’s effect on overall resident knowledge acquisition and awareness of these concepts.
Subject Selection and Study Protocol
Orthopedic residents belonging to 1 of 2 separate residency programs voluntarily and anonymously participated. All were asked to complete a baseline assessment examination, followed by attending a 45-minute lecture given by the same orthopedic faculty member who remained blinded to the test questions. Each resident then completed a postsession examination. Each resident was also asked to self-rate his or her documentation and coding level of comfort on a Likert scale (1-5). Statistical significance was set at p < 0.05.
Main Findings
A total of 32 orthopedic residents were participated. Increasing postgraduate year-level of training correlated with higher Likert-scale ratings for self-perceived comfort levels with documentation and coding. However, the baseline examination scores were no different between senior and junior residents (p > 0.20). The high-yield teaching session significantly improved the average total examination scores at both sites (p < 0.01), with overall improvement being similar between the 2 groups (p > 0.10).
Principal Conclusions
The current healthcare environment necessitates better physician awareness regarding clinical documentation guidelines and coding principles. Very few adjustments to incorporate these teachings have been made to most residency training curricula, and the lack of time and resources remains the concern of many surgical programs. We have demonstrated that orthopedic resident knowledge in these important areas drastically improves after a single, high-yield 45-minute teaching session.","Matthew A. Varacallo and Michael Wolf and Martin J. Herman",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Resilient Dynamic Data Driven Application Systems (rDDDAS)","There is a growing interest in Cloud Computing for delivering computing as a utility. Security in Cloud Computing is a challenging research problem because it involves many interdependent tasks including vulnerability scanning, application layer firewalls, configuration management, alert monitoring and analysis, source code analysis, and user identity management. It is widely accepted that we cannot build software and computing systems that are free from vulnerabilities and cannot be penetrated or attacked. Consequently, there is a strong interest in resilience approach because of its potential to address the cybersecurity challenges. Our is based on using the Dynamic Data Driven Application System (DDDAS) and Moving Target Defence (MTD) strategies to develop resilient DDDAS. The Resilient Applications utilize the following capabilities: Software Behaviour Encryption (SBE), Replication, Diversity, Automated Checkpointing and Recovery. Software Behaviour Encryption employs spatiotemporal behaviour encryption and a moving target defence to make active software components change their implementations and their resources randomly and consequently evade attackers. Diversity and random execution is achieved by “hot” shuffling multiple functionally- equivalent, behaviourally-different software versions at runtime (This encryption of the execution environment will make it extremely difficult for an attack to disrupt the normal operations of a cloud application. Also, the dynamic change in the execution environment will hide the software flaws that would otherwise be exploited by a cyberattacker. Checkpointing is used to save the current state of the task to a reliable storage and thus enabling rollback recovery if it is required to tolerate cyberattacks and mitigate their impacts. We use the Compiler for Portable Checkpointing (CPPC), a tool for automatically inserting portable checkpoints into the code. We also evaluate the performance and overhead of running three applications in our rDDDAS environment. Our experimental results show that the rDDDAS environment can be used to develop resilient cloud applications are resilient against attacks with around 7% in execution time overhead.","Glynis Dsouza and Salim Hariri and Youssif Al-Nashif and Gabriel Rodriguez",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"HRMC_2.0: Hybrid Reverse Monte Carlo method with silicon, carbon and germanium potentials","The Hybrid Reverse Monte Carlo (HRMC) code models the atomic structure of materials via the use of a combination of constraints including experimental diffraction data and an empirical energy potential. In this version update, germanium potential parameters are introduced and constraints based on the coordination, average coordination and the total bond angle distribution are implemented. Other additional changes include a constraint on three member ring formation, a constraint on porosity and an extension to handle systems with up to three different elements.
Program Summary
Program title: HRMC version 2.0 Catalogue identifier: AEAO_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEAO_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 94759 No. of bytes in distributed program, including test data, etc.: 751023 Distribution format: tar.gz Programming language: Fortran 90. Computer: Any computer capable of running executables produced by the Fortran 90 compiler. For example, the code runs in Windows 7, once compiled with the GNU Fortan 95 compiler. Operating system: Unix, Windows. RAM: Depends on the type of empirical potential used, number of atoms and which constraints are employed. Typically below 2 GB for a system with a few thousand atoms. Classification: 7.7. Does the new version supersede the previous version?: Yes Catalogue identifier of previous version: AEAO_v1_1 Journal reference of previous version: Comput. Phys. Comm. 182 (2011) 542 Nature of problem: Atomic modeling using a combination of empirical potentials, fits to experimental data and other chemically or physically motivated constraints. Solution method: Single move Metropolis Monte Carlo method used to minimize total energy and discrepancy between simulation and experimental data. Reasons for new version: Extension of capabilities from old version. Summary of revisions: Inclusion of Stillinger–Weber parameters for germanium, inclusion of a bond angle distribution constraint, inclusion of more general coordination, average coordination and porosity/volume constraints. Variable step sizes are now supported. Extension to systems containing up to three elements. New quench schemes to control constraint weighting throughout the simulation have been included. Constraints to forbid three member ring formation have been developed. There are changes to the input/output structure. Running time: 1000 s for a test run on a Intel Xeon 2.93 GHz—Nehalem series processor.","G. Opletal and T.C. Petersen and I.K. Snook and S.P. Russo",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Reciprocal White Matter Changes Associated With Copy Number Variation at 15q11.2 BP1-BP2: A Diffusion Tensor Imaging Study","Background
The 15q11.2 BP1-BP2 cytogenetic region has been associated with learning and motor delays, autism, and schizophrenia. This region includes a gene that codes for the cytoplasmic FMR1 interacting protein 1 (CYFIP1). The CYFIP1 protein is involved in actin cytoskeletal dynamics and interacts with the fragile X mental retardation protein. Absence of fragile X mental retardation protein causes fragile X syndrome. Because abnormal white matter microstructure has been reported in both fragile X syndrome and psychiatric disorders, we looked at the impact of 15q11.2 BP1-BP2 dosage on white matter microstructure.
Methods
Combining a brain-wide voxel-based approach and a regional-based analysis, we analyzed diffusion tensor imaging data from healthy individuals with the deletion (n = 30), healthy individuals with the reciprocal duplication (n = 27), and IQ-matched control subjects with no large copy number variants (n = 19), recruited from a large genotyped population sample.
Results
We found global mirror effects (deletion > control > duplication) on fractional anisotropy. The deletion group showed widespread increased fractional anisotropy when compared with duplication. Regional analyses revealed a greater effect size in the posterior limb of the internal capsule and a tendency for decreased fractional anisotropy in duplication.
Conclusions
These results show a reciprocal effect of 15q11.2 BP1-BP2 on white matter microstructure, suggesting that reciprocal chromosomal imbalances may lead to opposite changes in brain structure. Findings in the deletion overlap with previous white matter differences reported in fragile X syndrome patients, suggesting common pathogenic mechanisms derived from disruptions of cytoplasmic CYFIP1-fragile X mental retardation protein complexes. Our data begin to identify specific components of the 15q11.2 BP1-BP2 phenotype and neurobiological mechanisms of potential relevance to the increased risk for disorder.","Ana I. Silva and Magnus O. Ulfarsson and Hreinn Stefansson and Omar Gustafsson and G. Bragi Walters and David E.J. Linden and Lawrence S. Wilkinson and Mark Drakesmith and Michael J. Owen and Jeremy Hall and Kari Stefansson",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical simulation of metal jet breakup, cooling and solidification in water","During transient intrusion of molten metal into water, metal go through cooling, breakup before fully solidified. This paper describes a numerical code which combines cooling, solidification and breakup in a single computation. In the code free surface of jet is tracked by Volume of Fluid Method (VOF), both the heat transfer and viscosity variation during liquid-solid phase change are taken into account. The simulation results of melt jet pattern, front position history, jet breakup length and breakup time are in good agreement with the experimental results. The effects of interfacial temperature and jet velocity are also determined. The molten jet thermal history and solidification, droplet generation rate at different penetration times, which are difficult to observe in experiment, are presented to gain an insight into this complicated process. Solidified metal proportion increases with jet penetration depth. Melt jet breakup with surface solidification can be divided into three zones in space: (1) liquid core, (2) solidifying zone, (3) solid droplets. These simulation data are helpful to substantiate the understanding of the phenomena during molten melt jet interactions with water.","Yuan Zhou and Jingtan Chen and Mingjun Zhong and Junfeng Wang and Meng Lv",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Doing things: Organizing for agency in mathematical learning","In the United States, school mathematics generally fails to help students see themselves as capable of impacting their world – a perspective Freire argues defines human agency. This analysis draws from a five-week Algebra intervention for middle school students (n=46) designed to promote agency through collaborative mathematical activity. Typically, students identified as underperforming (as most in this intervention were), teachers revert to procedural, low-level instruction. In contrast, this intervention was designed around tasks of high cognitive demand that required visual or symbolic representation of algebraic concepts. Qualitative coding of student interviews (n=46) confirm the design principles of authority, agency and collaboration were positively impactful for students. In particular, interviews evidence a changing perspective from math as boring to the possibility of math as comingling intellectual challenge and personal enjoyment. These results are traced to the design principles and in particular, the focus on organizing for agency.","Tesha Sengupta-Irving",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An efficient and secure robust watermarking scheme for document images using Integer wavelets and block coding of binary watermarks","A novel, efficient and robust watermarking scheme for protection of document image contents is proposed in this work. An integer wavelet-based watermarking scheme for embedding the compressed version of the binary watermark logo has been developed for robust watermarking. At the sender side, the source document image is divided into empty and non-empty segments depending on the absence or presence of the information. Watermarking is applied for non-empty segments and thus the amount of embedding capacity is reduced. A binary watermark logo is compressed using binary block coding technique of appropriate block-size. A level-2 integer wavelet transformation is applied on the non-empty segment of the source document image. LL-sub-band of level-2 of the transformed image is subdivided into blocks of uniform size and compressed watermark bitstream is embedded into it. The compressed watermark is redundantly embedded into blocks using quantization technique. Thus, multiple copies of compressed watermark are available and each block of the source document image segment need not include the entire compressed watermark stream. At the receiver side, the extracted segments from each set of blocks are merged to obtain a single extracted bitstream. The bitstream is further decoded to get the binary watermark. The extracted and embedded watermarks are compared and authentication decision is taken based on majority voting technique. Based on the quantization step size, size of the logo and the level of wavelet transform, the watermarks are extracted without accessing the original image. The experimental results show that the proposed technique is highly robust. The performance of the proposed approach is measured in parameters Peak Signal to Noise Ratio (PSNR) and Normalized Correlation Coefficient (NCC). Results show that the proposed approach is better than the existing methods. In the proposed scheme for decompression of watermark, the level of block coding technique is the key, which provides an additional layer of security.","K.R. Chetan and S. Nirmala",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical modelling of the tailored tempering process applied to 22MnB5 sheets","In order to enhance the crash characteristics and geometrical accuracy, components hot formed in a fully martensitic state have gained in the last few years more and more importance. However, the very high strength exhibited by these components makes subsequent operations such as cutting difficult due to the high process forces and associated high wear of the cutting tools. Moreover, for some applications, such as B-pillars and other automotive components that may undergo impact loading, it may be desirable to create regions of the part with softer and more ductile microstructures. The novel process called the tailored tempering process allows doing this by suppressing the martensitic transformation in those zones of the sheet located under heated parts of the tools. In the paper, a numerical model of the tailored tempering process was developed, accurately calibrated and validated through a laboratory-scale hot forming process. Using the commercial FE code Forge™ a fully coupled thermo-mechanical-metallurgical model of the process was set up. The influence of the phase transformation kinetics was taken into account by implementing in the model phase transformation data, namely the shift of the TTT curves due to the applied stress and the transformation plasticity coefficients, gained from an extensive dilatometric experimental campaign and analysis. A laboratory-scale hot-formed U-channel was produced using segmented tools with heated and cooled zones so that the cooling rate of the blank can be locally controlled during the hot forming process. The part Vickers hardness distribution and microstructural evolution predicted by FORGE™ were then compared with the experimental results, proving the validation of the numerical model by taking into account the influence of the transformation plasticity and deformation history on the phase transformation kinetics.","B.T. Tang and S. Bruschi and A. Ghiotti and P.F. Bariani",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Direction-adaptive fixed length discrete cosine transform framework for efficient H.264/AVC video coding","The 2D-discrete cosine transform (2D-DCT) is one of the popular transformation for video coding. Yet, 2D-DCT may not be able to efficiently represent video data with fewer coefficients for oblique featured blocks. To further improve the compression gain for such oblique featured video data, this paper presents a directional transform framework based on direction-adaptive fixed length discrete cosine transform (DAFL-DCT) for intra-, and inter-frame. The proposed framework selects the best suitable transform mode from eight proposed directional transform modes for each block, and modified zigzag scanning pattern rearranges these transformed coefficients into a 1D-array, suitable for entropy encoding. The proposed scheme is analysed on JM 18.6 of H.264/AVC platform. Performance comparisons have been made with respect to rate-distortion (RD), Bjontegaard metrics, encoding time etc. The proposed transform scheme outperforms the conventional 2D-DCT and other state-of-art techniques in terms of compression gain and subjective quality.","Deepak Singh and Sukadev Meher",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A higher altitude is an independent risk factor for venous thromboembolisms following total shoulder arthroplasty","Introduction
High altitudes lead to physiological changes that may predispose to venous thromboembolisms (VTE) including deep vein thrombosis (DVT) and pulmonary embolism (PE). No prior study has evaluated if there is also a higher risk of VTEs after total shoulder arthroplasties (TSAs) performed at higher elevations compared to lower elevations. The purpose of this study was to identify if undergoing TSA at a higher altitude center (>4000 feet above sea level) is an independent risk factor for a postoperative VTE.
Methods
A retrospective review was performed from 2005 to 2014 using the Medicare Standard Analytical Files of the Pearl Diver database (Pearl Diver Technologies, West Conshohocken, PA, USA). The inclusion criteria for the study group consisted of all patients in the database undergoing primary TSAs at an altitude above 4000 feet. Patients were queried using the International Classification of Disease 9th revision codes (ICD-9). All patients undergoing primary TSA were queried using ICD-9 procedure code 81.80. Patients were filtered using the zip codes of the hospitals where the procedure occurred and were separated into high (>4,000 ft) and low (<100 ft) altitudes. Patients undergoing TSA in altitudes <100 ft represented the control group. Patients with a history of VTE, DVT, PE, and coagulation disorders were excluded from the study. Patients in the study group were randomly matched 1:1 according to age, gender, and comorbidities. Two mutually exclusive cohorts were formed and rates of VTE, DVT, and PE were analyzed and compared. Statistical analysis was performed using the programming language R (University of Auckland, New Zealand). An alpha value less than 0.05 was considered statistically significant.
Results
In the first 30 postoperative days, patients undergoing TSA at a higher altitude experienced a significantly higher rate of PEs (odds ratio [OR], 39.5; P = <0.001) when compared to similar patients at lower altitudes. This trend was also present for PE (OR, 2.02; P < 0.03) at 90 days postoperatively.
Conclusion
TSAs performed at higher altitudes (>4000 feet) have a higher rate of acute postoperative PEs in the first 30 days and 90 days postoperatively when compared to matched patients receiving the same surgery at a lower altitude (<100 feet). TSA patients at high altitude should be counseled on these increased risks.","Dhanur Damodar and Rushabh Vakharia and Ajit Vakharia and Jon Sheu and Chester J. Donnally and Jonathan C. Levy and Lee Kaplan and Julianne Munoz",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Time-space efficient regression testing for configurable systems","Configurable systems are those that can be adapted from a set of input options, reflected in code in form of variations. Testing these systems is challenging because of the vast array of configuration possibilities where bugs can hide. In the context of evolution, testing becomes even more challenging — not only code but also the set of plausible configurations can change across versions. This paper proposes EvoSPLat, a regression testing technique for configurable systems that explores all dynamically reachable configurations from a test. EvoSPLat supports two important application scenarios of regression testing. In the RCS scenario EvoSPLat prunes configurations (not tests) that are not impacted by changes. In the RTS scenario EvoSPLat prunes tests (not configurations) which are not impacted by changes. To evaluate EvoSPLat under the RCS scenario we used a selection of configurable Java programs. Results indicate that EvoSPLat reduced time by  ∼22% and reduced the number of configurations tested by  ∼45%. To evaluate EvoSPLat under the RTS scenario we used GCC. Results indicate that EvoSPLat reduced time to run tests by  ∼35%. Overall, results suggest that EvoSPLat is a promising technique to test configurable systems in the prevalent scenario of evolution.","Sabrina Souto and Marcelo d’Amorim",2018,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR9"
"Using Behavior Change Techniques to Guide Selections of Mobile Applications to Promote Fluid Consumption","Objective
To determine the extent to which validated techniques for behavior change have been infused in commercially available fluid consumption applications (apps).
Materials and Methods
Coders evaluated behavior change techniques represented in online descriptions for 50 fluid consumption apps and the latest version of each app.
Results
Apps incorporated a limited range of behavior change techniques (<20% of taxonomy). The number of techniques varied by operating system but not as a function of whether apps were free or paid. Limitations include the lack of experimental evidence establishing the efficacy of these apps.
Conclusion
Patients with urolithiasis can choose from many apps to support the recommended increase in fluid intake. Apps for iOS devices incorporate more behavior change techniques compared to apps for the Android operating system. Free apps are likely to expose patients to a similar number of techniques as paid apps. Physicians and patients should screen app descriptions for features to promote self-monitoring and provide feedback on discrepancies between behavior and a fluid consumption goal.","David E. Conroy and Alexandra Dubansky and Joshua Remillard and Robert Murray and Christine A. Pellegrini and Siobhan M. Phillips and Necole M. Streeper",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Unrestricted and safe dynamic code evolution for Java","Dynamic code evolution is a technique to update a program while it is running. In an object-oriented language such as Java, this can be seen as replacing a set of classes by new versions. We modified an existing high-performance virtual machine to allow arbitrary changes to the definition of loaded classes. Besides adding and deleting fields and methods, we also allow any kind of changes to the class and interface hierarchy. Our approach focuses on increasing developer productivity during debugging, but can also be applied for updating of long-running applications. Changes can be applied at any point at which a Java program can be suspended. Our virtual machine is able to continue execution of old changed or deleted methods and also to access deleted static fields. A dynamic verification of the current state of the program ensures type safety of complex class hierarchy changes. However, the programmer still has to ensure that the semantics of the modified program are correct and that the new program version can start running from the state left behind by the old program version. The evaluation section shows that our modifications to the virtual machine have no negative performance impact on normal program execution. The in-place instance update algorithm is in many cases faster than a full garbage collection. Standard Java development environments automatically use the code evolution features of our modified virtual machine, so no additional tools are required.","Thomas Würthinger and Christian Wimmer and Lukas Stadler",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Coelacanth-specific adaptive genes give insights into primitive evolution for water-to-land transition of tetrapods","Coelacanth is a group of extant lobe-finned fishes in Sarcopterygii that provides evolutionary information for the missing link between ray-finned fish and tetrapod vertebrates. Its phenotypes, different from actinopterygian fishes, have been considered as primitive terrestrial traits such as cartilages in their fatty fins which are homologous with the humerus and femur. To investigate molecular evolution of coelacanth which led to its divergence into Sarcopterygii, we compared its protein coding sequences with 11 actinopterygian fishes. We identified 47 genes under positive selection specific to coelacanth, when compared to Holostei and Teleostei. Out of these, NCDN and 14 genes were associated with spatial learning and nitrogen metabolism, respectively. In homeobox gene superfamily, we identified coelacanth-specific amino acid substitutions, and also observed that one of replacements in SHOX was shared with extant tetrapods. Such molecular changes may cause primordial morphological change in the common ancestor of sarcopterygians. These results suggest that certain genes such as NCDN, MMS19, TRMT1, ALX1, DLX5 and SHOX might have played a role in the evolutionary transition between aquatic and terrestrial vertebrates.","Chul Lee and Heesu Jeong and DongAhn Yoo and Eun Bae Kim and Bo-Hye Nam and Heebal Kim",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Normal CA1 Place Fields but Discoordinated Network Discharge in a Fmr1-Null Mouse Model of Fragile X Syndrome","Summary
Silence of FMR1 causes loss of fragile X mental retardation protein (FMRP) and dysregulated translation at synapses, resulting in the intellectual disability and autistic symptoms of fragile X syndrome (FXS). Synaptic dysfunction hypotheses for how intellectual disabilities like cognitive inflexibility arise in FXS predict impaired neural coding in the absence of FMRP. We tested the prediction by comparing hippocampus place cells in wild-type and FXS-model mice. Experience-driven CA1 synaptic function and synaptic plasticity changes are excessive in Fmr1-null mice, but CA1 place fields are normal. However, Fmr1-null discharge relationships to local field potential oscillations are abnormally weak, stereotyped, and homogeneous; also, discharge coordination within Fmr1-null place cell networks is weaker and less reliable than wild-type. Rather than disruption of single-cell neural codes, these findings point to invariant tuning of single-cell responses and inadequate discharge coordination within neural ensembles as a pathophysiological basis of cognitive inflexibility in FXS.
Video Abstract
","Zoe Nicole Talbot and Fraser Todd Sparks and Dino Dvorak and Bridget Mary Curran and Juan Marcos Alarcon and André Antonio Fenton",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Complete sequence of the tumor-inducing plasmid pTiChry5 from the hypervirulent Agrobacterium tumefaciens strain Chry5","Agrobacterium tumefaciens strain Chry5 is hypervirulent on many plants including soybean that are poorly transformed by other A. tumefaciens strains. Therefore, it is considered as a preferred vector for genetic transformation of plants. Here we report the complete nucleotide sequence of its chrysopine-type Ti-plasmid pTiChry5. It is comprised of 197,268 bp with an overall GC content of 54.5%. Two T-DNA regions are present and 219 putative protein-coding sequences could be identified in pTiChry5. Roughly one half of the plasmid is highly similar to the agropine-type Ti plasmid pTiBo542, including the virulence genes with an identical virG gene, which is responsible for the supervirulence caused by pTiBo542. The remaining part of pTiChry5 is less related to that of pTiBo542 and embraces the trb operon of conjugation genes, genes involved in the catabolism of Amadori opines and the gene for chrysopine synthase, which replaces the gene for agropine synthase in pTiBo542. With the exception of an insertion of IS869, these Ti plasmids differ completely in the set of transposable elements present, reflecting a different evolutionary history from a common ancestor.","Shuai Shao and Xiaorong Zhang and G. Paul H. van Heusden and Paul J.J. Hooykaas",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Object knowledge changes visual appearance: Semantic effects on color afterimages","According to predictive coding models of perception, what we see is determined jointly by the current input and the priors established by previous experience, expectations, and other contextual factors. The same input can thus be perceived differently depending on the priors that are brought to bear during viewing. Here, I show that expected (diagnostic) colors are perceived more vividly than arbitrary or unexpected colors, particularly when color input is unreliable. Participants were tested on a version of the ‘Spanish Castle Illusion’ in which viewing a hue-inverted image renders a subsequently shown achromatic version of the image in vivid color. Adapting to objects with intrinsic colors (e.g., a pumpkin) led to stronger afterimages than adapting to arbitrarily colored objects (e.g., a pumpkin-colored car). Considerably stronger afterimages were also produced by scenes containing intrinsically colored elements (grass, sky) compared to scenes with arbitrarily colored objects (books). The differences between images with diagnostic and arbitrary colors disappeared when the association between the image and color priors was weakened by, e.g., presenting the image upside-down, consistent with the prediction that color appearance is being modulated by color knowledge. Visual inputs that conflict with prior knowledge appear to be phenomenologically discounted, but this discounting is moderated by input certainty, as shown by the final study which uses conventional images rather than afterimages. As input certainty is increased, unexpected colors can become easier to detect than expected ones, a result consistent with predictive-coding models.","Gary Lupyan",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Face learning and the emergence of view-independent face recognition: An event-related brain potential study","Recognizing unfamiliar faces is more difficult than familiar face recognition, and this has been attributed to qualitative differences in the processing of familiar and unfamiliar faces. Familiar faces are assumed to be represented by view-independent codes, whereas unfamiliar face recognition depends mainly on view-dependent low-level pictorial representations. We employed an electrophysiological marker of visual face recognition processes in order to track the emergence of view-independence during the learning of previously unfamiliar faces. Two face images showing either the same or two different individuals in the same or two different views were presented in rapid succession, and participants had to perform an identity-matching task. On trials where both faces showed the same view, repeating the face of the same individual triggered an N250r component at occipito-temporal electrodes, reflecting the rapid activation of visual face memory. A reliable N250r component was also observed on view-change trials. Crucially, this view-independence emerged as a result of face learning. In the first half of the experiment, N250r components were present only on view-repetition trials but were absent on view-change trials, demonstrating that matching unfamiliar faces was initially based on strictly view-dependent codes. In the second half, the N250r was triggered not only on view-repetition trials but also on view-change trials, indicating that face recognition had now become more view-independent. This transition may be due to the acquisition of abstract structural codes of individual faces during face learning, but could also reflect the formation of associative links between sets of view-specific pictorial representations of individual faces.","Friederike G.S. Zimmermann and Martin Eimer",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Cognitions, Emotions, and Applications: Participants' Experiences of Learning about Strengths in an Academic Library","This study examined academic library employees' experiences during “strengths education,” a process of learning about individual strengths during a positive psychology intervention. Participants took the Clifton StrengthsFinder test, attended a workshop, and then were interviewed about what they considered to be the effects of the strengths training. The focus of the qualitative analysis was the interviewees' statements about the intrapersonal and interpersonal effects of learning about their strengths. We categorized and coded these statements as cognitions formed, emotions experienced, and applications envisioned. Our findings raise interesting implications for job satisfaction and employee self-esteem, especially during times of change.","Allison Sharp and Jeanine Williamson",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Rising sea levels and sinking property values: Hurricane Sandy and New York’s housing market","This paper analyzes the effects of hurricane Sandy on the New York City housing market using a large parcel-level dataset that contains all housing sales for 2003–2017. The dataset also contains geo-coded FEMA data on which building structures were damaged by the hurricane and to what degree. Our estimates provide robust evidence of a persistent negative impact on flood zone housing values. We show the gradual emergence of a price penalty among flood zone properties that were not damaged by Sandy, reaching 8% in year 2017 and showing no signs of recovery. In contrast, damaged properties suffered a large immediate drop in value following the storm (17–22%), followed by a partial recovery and convergence toward a similar penalty as non-damaged properties. The partial recovery in the prices of damaged properties likely reflects their gradual restoration. However, the persistent price reduction affecting all flood-zone properties is more consistent with a learning mechanism. Hurricane Sandy may have increased the perceived risk of large-scale flooding episodes in that area.","Francesc Ortega and Süleyman Taṣpınar",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Safety studies of plasma-wall events with AINA code for Japanese DEMO","In this contribution, the work done in AINA code during 2014 and 2015 at IFERC is presented. The main motivation of this work was to adapt the code and to perform safety studies for a Japanese DEMO design. Related to AINA code, the work has supposed major changes in plasma models. Significant is the addition of an integrated SOL-pedestal model that allows the estimation of heat loads at divertor. Also, a thermal model for a WCPB (water cooled pebble bed) breeding blanket has been developed based in parametric input data from neutronics calculations. Related to safety studies, a major breakthrough in the study of LOPC (loss of plasma control) transients has been the use of an optimization method to determine the most severe transients in terms of the shortest melting times. The results of the safety study show that LOPC transients are not likely to be severe for breeding blanket, but for the case of divertor can induce severe melting. For ex-vessel LOCA (loss of coolant accident) analysis, it is severe for both blanket and divertor, but in the first case the transient time until melting is nearly two orders of magnitude higher. The results point out that the recovery time for plasma control system should be at least one order of magnitude lower than confinement time to avoid melting of divertor targets.","J.C. Rivas and M. Nakamura and Y. Someya and K. Hoshino and N. Asakura and H. Takase and Y. Miyoshi and H. Utoh and K. Tobita and J. Dies and A. de Blas and A. Riego and M. Fabbri",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Integrated Fisher linear discriminants: An empirical study","This paper studies Fisher linear discriminants (FLDs) based on classification accuracies for imbalanced datasets. An optimal threshold is found out from a series of empirical formulas developed, which is related not only to sample sizes but also to distribution regions. A mixed binary–decimal coding system is suggested to make the very dense datasets sparse and enlarge the class margins on condition that the neighborhood relationships of samples are nearly preserved. The within-class scatter matrices being or approximately singular should be moderately reduced in dimensionality but not added with tiny perturbations. The weight vectors can be further updated by a kind of epoch-limited (three at most) iterative learning strategy provided that the current training error rates come down accordingly. Putting the above ideas together, this paper proposes a type of integrated FLDs. The extensive experimental results over real-world datasets have demonstrated that the integrated FLDs have obvious advantages over the conventional FLDs in the aspects of learning and generalization performances for the imbalanced datasets.","Gao Daqi and Ding Jun and Zhu Changming",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Sparse coding based features for speech units classification","In this work, we propose sparse representation based features for speech units classification tasks. In order to effectively capture the variations in a speech unit, the proposed method employs multiple class specific dictionaries. Here, the training data belonging to each class is clustered into multiple clusters, and a principal component analysis (PCA) based dictionary is learnt for each cluster. It has been observed that coefficients corresponding to middle principal components can effectively discriminate among different speech units. Exploiting this observation, we propose to use a transformation function known as weighted decomposition (WD) of principal components, which is used to emphasize the discriminative information present in the PCA-based dictionary. In this paper, both raw speech samples and mel frequency cepstral coefficients (MFCC) are used as an initial representation for feature extraction. For comparison, various popular dictionary learning techniques such as K-singular value decomposition (KSVD), simultaneous codeword optimization (SimCO) and greedy adaptive dictionary (GAD) are also employed in the proposed framework. The effectiveness of the proposed features is demonstrated using continuous density hidden Markov model (CDHMM) based classifiers for (i) classification of isolated utterances of E-set of English alphabet, (ii) classification of consonant-vowel (CV) segments in Hindi language and (iii) classification of phoneme from TIMIT phonetic corpus.","Pulkit Sharma and Vinayak Abrol and A.D. Dileep and Anil Kumar Sao",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Natural image sequences constrain dynamic receptive fields and imply a sparse code","In their natural environment, animals experience a complex and dynamic visual scenery. Under such natural stimulus conditions, neurons in the visual cortex employ a spatially and temporally sparse code. For the input scenario of natural still images, previous work demonstrated that unsupervised feature learning combined with the constraint of sparse coding can predict physiologically measured receptive fields of simple cells in the primary visual cortex. This convincingly indicated that the mammalian visual system is adapted to the natural spatial input statistics. Here, we extend this approach to the time domain in order to predict dynamic receptive fields that can account for both spatial and temporal sparse activation in biological neurons. We rely on temporal restricted Boltzmann machines and suggest a novel temporal autoencoding training procedure. When tested on a dynamic multi-variate benchmark dataset this method outperformed existing models of this class. Learning features on a large dataset of natural movies allowed us to model spatio-temporal receptive fields for single neurons. They resemble temporally smooth transformations of previously obtained static receptive fields and are thus consistent with existing theories. A neuronal spike response model demonstrates how the dynamic receptive field facilitates temporal and population sparseness. We discuss the potential mechanisms and benefits of a spatially and temporally sparse representation of natural visual input.","Chris Häusler and Alex Susemihl and Martin P. Nawrot",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"WIRE: Watershed based iris recognition","A Watershed transform based Iris REcognition system (WIRE) for noisy images acquired in visible wavelength is presented. Key points of the system are: the color/illumination correction pre-processing step, which is crucial for darkly pigmented irises whose albedo would be dominated by corneal specular reflections; the criteria used for the binarization of the watershed transform, leading to a preliminary segmentation which is refined by taking into account the watershed regions at least partially included in the best iris fitting circle; the introduction of a new cost function to score the circles detected as potentially delimiting limbus and pupil. The advantage offered by the high precision of WIRE in iris segmentation has a positive impact as regards the iris code, which results to be more accurately computed, so that the performance of iris recognition is also improved. To assess the performance of WIRE and to compare it with the performance of other available methods, two well known databases have been used, specifically UBIRIS version 1 session 2 and the subset of UBIRIS version 2 that has been used as training set for the international challenge NICE II.","Maria Frucci and Michele Nappi and Daniel Riccio and Gabriella Sanniti di Baja",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The insect mushroom body, an experience-dependent recoding device","The insect mushroom body is a higher order integration center involved in cross-sensory integration and memory formation. The relatively large mushroom bodies of social Hymenoptera (e.g. bees) have been related to the demands of a social system and the neural processes required to allow the animal to navigate in an ever-changing environment. Here I review studies aiming to elucidate the neural processes that take place at the input and the output sites of the mushroom bodies and that underlie cross-sensory integration, associative learning, memory storage and retrieval. Highly processed sensory information is received at modality-specific compartments of the input site, the calyx. The large number of intrinsic neurons of the mushroom body receive multiple sensory inputs establishing combinations of processed sensory stimuli. A matrix-like memory structure characterizes the dendritic area of the intrinsic neurons allowing storage of rich combinations of sensory information. The rather small number of extrinsic neurons read out from multiple intrinsic neurons, thereby losing their sensory coding properties. The response properties of these neurons change according to the value of stimulus combinations experienced. It is concluded that the mushroom bodies transform the highly dimensional sensory coding space into a low dimensional coding space of value-based information. A model of such an experience-dependent recoding device is presented and compared with the available data.","Randolf Menzel",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Development of a three-zone transport model for activated corrosion products analysis of Tokamak Cooling Water System","In the Tokamak Cooling Water System (TCWS), the activated corrosion products (ACPs) play as an important radioactive source, which have impact on reactor inspection and maintenance. A three-zone transport model of ACPs was elaborated in this paper, which is based on the theory that the main driving force for ACPs transport is the temperature change of the coolant throughout the loop and the resulting change in metal ion solubility in the coolant. The three-zone transport model was used to replace the loop-homogenization model in the ACPs evaluation code CATE 1.0, developing CATE to give the capability to calculate spatial distribution of ACPs. As a result, CATE was upgraded to version 2.0. For code testing, a FW/BLK cooling loop of ITER was simulated using CATE 2.0, and the composition and radioactivity of ACPs were calculated. The results showed that the major contributors came from the short-life nuclides, especially Mn-56, which can influence material choice in reactor design and shutdown time before reactor maintenance.","Jingyu Zhang and Lu Li and Shuxiang He and Wen Song and Yu Fu and Bin Zhang and Yixue Chen",2016,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"HOTB update: Parallel code for calculation of three- and four-particle harmonic oscillator transformation brackets and their matrices using OpenMP","This is a new version of the HOTB code designed to calculate three and four particle harmonic oscillator (HO) transformation brackets and their matrices. The new version uses the OpenMP parallel communication standard for calculations of harmonic oscillator transformation brackets. A package of Fortran code is presented. Calculation time of large matrices, orthogonality conditions and array of coefficients can be significantly reduced using effective parallel code. Other functionalities of the original code (for example calculation of single harmonic oscillator brackets) have not been modified.
New version program summary
Program Title: HOTB_OpenMP Program Files doi:http://dx.doi.org/10.17632/ddjxc7rkpr.1 Licensing provisions: GPLv3 Programming language: Fortran 90 Journal reference of previous version:  [2], [3], [4] Does the new version supersede the previous version?: Yes Reasons for the new version: Calculation of huge amount of HOB’s, using single processor, takes much time. Speed-up is needed. The new program version allows to perform calculations on multiple processors using shared memory OpenMP API that reduces time needed for calculations and is easily implemented by end user. Nature of problem: Often solving nuclear structure and other problems huge amount of HOB’s is needed. Calculation of matrices, orthogonality conditions and arrays of three-particle harmonic oscillator brackets (3HOB) and four-particle harmonic oscillator brackets (4HOB) using single processor for high values of harmonic oscillator quanta e takes much time. Solution method: Using OpenMP parallelization standard for the three and four-particle harmonics oscillator brackets 3HOB and 4HOB, based on methods, is presented in [2], [3], [4]. Summary of revisions: Subroutines for calculation arrays of HOB’s, their matrices and orthogonality conditions are rewritten to use OpenMP parallel programming standard. 1.Additional features of new code HOTB_OpenMP: (1)Calculation time. As the code of previous version of program was redone using parallelism paradigma, it is now possible to reduce the calculation time of transformation matrices significantly, depending on the number of processors (cores), as the dimensions of matrices are growing very rapidly according to the energy and momenta values.2.Modifications or corrections to previous versions:Updated program HOTB_OpenMP is written in the FORTRAN 90 language, according to formulas described in [1], [2], [3], [4]. There is one file: HOTB_OpenMP.f90.Detailed descriptions of internal parameters used by subroutines are represented in [1] for 3HOB, [2] for 4HOB, [3] for 4HOB for special cases of the parameters and [4] for 4HOB for matrix M method and also are located in files read.me and appendix.pdf. File read.me also contains description of example input and output files.Main computational subroutines using OpenMP: (1)subroutineall_3HOB_OpenMPdimens bus_3HOB array_3HOB.Performs parallel calculations of all 3HOB brackets for given e (total harmonic oscillator energy). Other input parameters: dimens—dimension of the problem, bus_3HOB—array of quantum numbers. The return value is array_3HOB—array of 3HOB brackets.(2)subroutineall_4HOB_OpenMPdimens bus_4HOB array_4HOB.Performs parallel calculations of all 4HOB brackets for given e (total HO energy). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is array_4HOB—array of 4HOB brackets.(3)subroutineorthogonality_3HOB_OpenMPpaklaida nk dimens bus_3HOB.Performs parallel calculations of orthogonality of 3HOB matrix using OpenMP for given e (total HO energy). Other input parameters: dimens—dimension of the problem, bus_3HOB—array of quantum numbers. The return values are paklaida—error of the problem and nk—amount of coefficients (equation (43) in [1]).(4)subroutineorthogonality_4HOB_OpenMPpaklaida nk dimens bus_4HOB.Performs parallel calculations of orthogonality of 4HOB matrix using OpenMP for given e (total HO energy). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return values are paklaida—error of the problem and nk—amount of coefficients (equation (12) in [2]).(5)subroutinematrix_3HOB_OpenMPl dimens bus_3HOB matr_3HOB.Performs parallel calculations of 3HOB matrix using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_3HOB—array of quantum numbers. The return value is matr_3HOB—matrix of 3HOB coefficients.(6)subroutinematrix_4HOB_OpenMPl dimens bus_4HOB matr_4HOB.Performs parallel calculations of 4HOB matrix using OpenMP for given e (total HO energy) and l (total angular momentum).Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.(7)subroutinematrix_4HOB_d_0_OpenMPl dimens bus_4HOB matr_4HOB.Performs parallel calculations of 4HOB matrix with parameter d = 0 using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.(8)subroutinematrix_4HOB_d_infinity_OpenMPl dimens bus_4HOB matr_4HOB.Performs parallel calculations of 4HOB matrix with parameter d = infinity using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.(9)subroutinematrix_4HOB_d1_0_OpenMPl dimens bus_4HOB matr_4HOB.Performs parallel calculations of 4HOB matrix with parameter d1 = 0 using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.(10)subroutinematrix_M_OpenMPl dimens bus_4HOB matr_M.Performs parallel calculations of M matrix using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.(11)subroutinematrix_4HOB_M_OpenMPl dimens bus_4HOB matr_4HOB_M.Performs parallel calculations of 4HOB matrix using M technique and OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.3.Additional non-parallel subroutines, needed main computational subroutines to run: (1)subroutineall_3HOB_dimensione dimens.Outputs number of all 3HOB brackets for given e (total HO energy). The return value is dimens—dimension of the problem.(2)subroutinearray_quantum_numbers_all_3HOBe dimens bus_3HOB.Fills array of quantum numbers for all 3HOB brackets for given e (total HO energy). Other input parameters: dimens—dimension of the problem. The return value is bus_3HOB—array of quantum numbers.(3)subroutineall_4HOB_dimensione dimens.Outputs number of all 4HOB brackets for given e (total HO energy). The return value is dimens—dimension of the problem.(4)subroutinearray_quantum_numbers_all_4HOBe dimens bus_4HOB.Fills array of quantum numbers for all 4HOB brackets for given e (total HO energy). Other input parameters: dimens—dimension of the problem. The return value is bus_4HOB—array of quantum numbers.(5)subroutineort_3HOB_dimensione dimens.Calculates number of 3HOB coefficients for orthogonality test for given e (total HO energy). The return value is dimens—dimension of the problem.(6)subroutinearray_quantum_numbers_ort_3HOBe dimens bus_3HOB.Fills array of quantum numbers for 3HOB orthogonality test for given e (total HO energy). Other input parameters: dimens—dimension of the problem. The return value is bus_3HOB—array of quantum numbers.(7)subroutineort_4HOB_dimensione dimens.Calculates number of 4HOB coefficients for orthogonality test for given e (total HO energy). The return value is dimens—dimension of the problem.(8)subroutinearray_quantum_numbers_ort_4HOBe dimens bus_4HOB.Fills array of quantum numbers for 4HOB orthogonality test for given e (total HO energy). Other input parameters: dimens—dimension of the problem. The return value is bus_4HOB—array of quantum numbers.(9)subroutinematrix_3HOB_dimensione l dimens.Calculates dimension of 3HOB matrix for given e (total HO energy) and l (total angular momentum). The return value is dimens—dimension of the problem.(10)subroutinearray_quantum_numbers_matrix_3HOBe l dimens bus_3HOB.Fills array of quantum numbers for matrix 3HOB for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem. The return value is bus_3HOB—array of quantum numbers.(11)subroutinematrix_4HOB_dimensione l dimens.Calculates dimension of 4HOB matrix for given e (total HO energy) and l (total angular momentum). The return value is dimens—dimension of the problem.(12)subroutinearray_quantum_numbers_matrix_4HOBe l dimens bus_4HOB.Fills array of quantum numbers for matrix 4HOB for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem. The return value is bus_4HOB—array of quantum numbers.(13)subroutinehob_output_to_filearray_xHOB dimens name bus_xHOB n e.Writes HOB’s array array_xHOB to file. Other input parameters: dimens—dimension of the problem,  bus_xHOB—array of quantum numbers, n—parameter, that equals 9 in case of 3HOB and equals 17 in case of 4HOB, e—harmonic oscillator quanta. The return value is text file that name is value of the variable name.(14)subroutineort_output_to_filepaklaida dimens nk name n e.Writes orthogonality test results to file. Input parameters: paklaida—error of the problem, dimens—dimension of the problem, nk—amount of coefficients, n—parameter, that equals 1 in case of 3HOB and equals 2 in case of 4HOB, e—harmonic oscillator quanta. The return value is text file that name is value of the variable name.(15)subroutinematrix_output_to_filematrix dimens name bus_xHOB n e l.Writes HOB’s matrix matrix to file. Other input parameters: dimens—dimension of the problem, bus_xHOB—array of quantum numbers, n—parameter, that equals 4 in case of 3HOB and equals 8 in case of 4HOB, e—harmonic oscillator quanta, l—total angular momentum. The return value is text file that name is value of the variable name.4.Additional subroutines for user conveniency: (1)subroutinesingle_3HOBComputes single 3HOB for given set of quantum numbers.(2)subroutinesingle_4HOBComputes single 4HOB for given set of quantum numbers.(3)subroutinesingle_4HOB_d_0Computes single 4HOB for given set of quantum numbers, case d = 0.(4)subroutinesingle_4HOB_d_infinityComputes single 4HOB for given set of quantum numbers, case d = infinity.(5)subroutinesingle_4HOB_d1_0Computes single 4HOB for given set of quantum numbers, case d1 = 0. For illustrational purposes we have made some calculations for all HOB’s, their orthogonality conditions and matrices. Calculations were done on 12 cores 96 GB RAM computer. Calculation time is OpenMP wall-time function omp_get_wtime. Results are presented in Table 1, Table 2, Table 3, Table 4, Table 5, Table 6. Additional comments including Restrictions and Unusual features: Calculations can be done up to harmonic oscillator (HO) energy quanta e = 28 for double precision (presented here) version. Acknowledgments Computations were performed using resources at the High Performance Computing Center “HPC Saultekis” in Vilnius University’s Faculty of Physics. [1]G.P. Kamuntavičius, R.K. Kalinauskas, B.R. Barrett, S. Mickevičius, D. Germanas, The general harmonic-oscillator brackets: compact expression, symmetries, sums and Fortran code, Nucl. Phys. A 695(2001) 191-201.[2]D. Germanas, R.K. Kalinauskas, S. Mickevičius, Calculation of four-particle harmonic-oscillator transformation brackets, Computer Physics Communications 181(2010) 420-425.[3]S. Mickevičius, E. Brazauskas, D. Germanas, R.K. Kalinauskas, The four-particle harmonic-oscillator brackets: Compact expressions and updated Fortran program, Computer Physics Communications 182(2011) 1377-1381.[4]S. Mickevičius, D. Germanas, R.K. Kalinauskas, Revised calculation of four-particle harmonic-oscillator transformation brackets matrix, Computer Physics Communications 184(2013) 409-413.","D. Germanas and A. Stepšys and S. Mickevičius and R.K. Kalinauskas",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Sleep sharpens sensory stimulus coding in human visual cortex after fear conditioning","Efficient perceptual identification of emotionally-relevant stimuli requires optimized neural coding. Because sleep contributes to neural plasticity mechanisms, we asked whether the perceptual representation of emotionally-relevant stimuli within sensory cortices is modified after a period of sleep. We show combined effects of sleep and aversive conditioning on subsequent discrimination of face identity information, with parallel plasticity in the amygdala and visual cortex. After one night of sleep (but neither immediately nor after an equal waking interval), a fear-conditioned face was better detected when morphed with another identity. This behavioral change was accompanied by increased selectivity of the amygdala and face-responsive fusiform regions. Overnight neural changes can thus sharpen the representation of threat-related stimuli in cortical sensory areas, in order to improve detection in impoverished or ambiguous situations. These findings reveal an important role of sleep in shaping cortical selectivity to emotionally-relevant cues and thus promoting adaptive responses to new dangers.","Virginie Sterpenich and Camille Piguet and Martin Desseilles and Leonardo Ceravolo and Markus Gschwind and Dimitri Van De Ville and Patrik Vuilleumier and Sophie Schwartz",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"How do technical improvements change radiographers' practice – A practice theory perspective","Introduction
The two plane imaging techniques are gradually being replaced by multidimensional imaging. How it affects radiographers' professional practice has not been investigated.
Aim
To explore how technical development affects the relations between different actors and their actions in the practice of Computer Tomography.
Method
A qualitative design with data collection by open interviews (n = 8) and open observations (n = 10) of radiographers during their work with Computer Tomography. Data was first analyzed inductively resulting in seven codes. Secondly abduction was carried out by interpreting the content in the codes with a practice theory. This resulted in four themes.
Result
First theme: Changed materiality makes the practical action easier. The actual image production has become practically easier. Second theme: Changed machines cause conflict between the arrangements of the work and the patients' needs. The time for the machine to carry out image production is easy to foresee, but information about the patient's individual status and needs is missing and this leads to difficulties in giving individual planned care. Third theme: Changing materiality prefigure learning. The different apparatus in use and the continuously changing methods of image production is co-constitutive of the practitioners' activities and learning. Fourth theme: Radiography is arranged for patient safety in relation to radiation doses and medical security risks. But the radiographers, who meet the patients, have to check the accuracy of the planned examination in relation to the clinical observed information about patient safety risks with the examination.","L.-L. Lundvall and M. Abrandt-Dahlgren and S. Wirell",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A modified Delphi translation strategy and challenges of International Classification for Nursing Practice (ICNP®)","Background and objectives
Standardized terminology is an important infrastructure component of the electronic health record. ICNP® is a systemic coding system that can support the development of nursing information systems. Translation of the standardized terminology preferred terms into local terms is an important first step in the translation process. The purpose of this case report is to describe the translation strategy used and challenges faced in translating ICNP® Version 2 preferred terms from English to traditional Chinese.
Methods
A modified Delphi strategy using forward translation and expert consensus was conducted to facilitate semantic and cultural translation and validation of the ICNP® and to make the process generalizable. A nursing informatics expert completed the initial forward translation. Five nursing experts with rich clinical and academic experiences joined this process and validated the initial translation. The nursing experts’ consensus was then used to finalize the traditional Chinese terms.
Results
A total of 1863 preferred terms from the ICNP® Version 2 were translated from English into traditional Chinese. Majority agreement from two or more nursing experts was achieved for 98.3% (n=1832) of the preferred term translations. Less than 2% (n=31) of terms had no majority agreement. Translation challenges include the following: (1) changes in code structure of preferred terms from the ICNP® β2 version to Verson 2, (2) inability to identify resources to complete the translation that fully met ICNP recommendations for terminology translators, (3) ambiguous preferred term descriptions, and (4) ambiguous preferred term names.
Conclusions
Most of the ICNP® Version 2 preferred terms were translated from English into traditional Chinese with majority consensus. For the terms without consensus, we recommend that all synonyms be included in the ICNP® translation. In countries like Taiwan where nursing education occurs in English, it is recommended that English terms are displayed along with the translated official language to help nurses to interpret and use the terminology correctly.","I-Ching Hou and Polun Chang and Hui-Ya Chan and Patricia C. Dykes",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Two-phase CFD validation: TOPFLOW-PTS steady-state steam-water tests 3–16, 3–17, 3–18 and 3–19","For the Pressurized Thermal Shock (PTS) issue, the NEPTUNE_CFD code solves the Eulerian two-fluid model with specific models for liquid/gas interfaces, which are much larger than the computational cells size. The CFD validation database dedicated to PTS includes the TOPFLOW-PTS experiment, which represents condensation phenomena in a PWR cold leg, with the Emergency Core Cooling system (ECC) and a downcomer. The present study deals with NEPTUNE_CFD calculations of steady-state steam-water tests 3–16, 3–17, 3–18 and 3–19, which differ each other by the ECC liquid inlet flow rate (m˙L,ECC). So the liquid turbulence, which is the main input of the condensation models, is changed from one test to the other, firstly in the ECC region. The direct and first order effect of the ECC flow on the liquid temperature is shown with sensitive two-phase flow regime transitions, which require a careful meshing. This condition being fulfilled, satisfactory NEPTUNE_CFD results mesh-independency on refinement is shown. Present CFD is able to calculate the effect of m˙L,ECC on condensation and temperatures. CFD versions with k-ɛ and Rij-ɛ SSG turbulence modeling are compared and the improvement brought by the new version with Rij-ɛ SSG is shown.","P. Coste and N. Mérigoux",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mercer's spectral decomposition for the characterization of thermal parameters","We investigate a tractable Singular Value Decomposition (SVD) method used in thermography for the characterization of thermal parameters. The inverse problem to solve is based on the model of transient heat transfer. The most significant advantage is the transformation of the dynamic identification problem into a steady identification equation. The time dependence is accounted for by the SVD in a performing way. We lay down a mathematical foundation well fitted to this approach, which relies on the spectral expansion of Mercer kernels. This enables us to shed more light on most of its important features. Given its potentialities, the analysis we propose here might help users understanding the way the SVD algorithm, or the TSVD, its truncated version, operate in the thermal parameters estimation and why it is relevant and attractive. When useful, the study is complemented by some analytical and numerical illustrations realized within matlab's code.","E. Ahusborde and M. Azaïez and F. Ben Belgacem and E. Palomo Del Barrio",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Leveraging e-Learning in Medical Education","e-Learning has become a popular medium for delivering instruction in medical education. This innovative method of teaching offers unique learning opportunities for medical trainees. The purpose of this article is to define the present state of e-learning in pediatrics and how to best leverage e-learning for educational effectiveness and change in medical education. Through addressing under-examined and neglected areas in implementation strategies for e-learning, its usefulness in medical education can be expanded. This study used a systematic database review of published studies in the field of e-learning in pediatric training between 2003 and 2013. The search was conducted using educational and health databases: Scopus, ERIC, PubMed, and search engines Google and Hakia. A total of 72 reference articles were suitable for analysis. This review is supplemented by the use of e-Learning Design Screening Questions to define e-learning design and development in 10 randomly selected articles. Data analysis used template-based coding themes and counting of the categories using descriptive statistics.Our search for pediatric e-learning (using Google and Hakia) resulted in six well-defined resources designed to support the professional development of doctors, residents, and medical students. The majority of studies focused on instructional effectiveness and satisfaction. There were few studies about e-learning development, implementation, and needs assessments used to identify the institutional and learners׳ needs. Reviewed studies used various study designs, measurement tools, instructional time, and materials for e-learning interventions. e-Learning is a viable solution for medical educators faced with many challenges, including (1) promoting self-directed learning, (2) providing flexible learning opportunities that would offer continuous (24h/day/7 days a week) availability for learners, and (3) engaging learners through collaborative learning communities to gain significant learning and augment continuous professional development. Several important recommendations for faculty instructors interested in providing and/or improving e-learning activities for today׳s learners are detailed.","Kadriye O. Lewis and Michal J. Cidon and Teresa L. Seto and Haiqin Chen and John D. Mahan",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Raising the standard: changes to the Australian Code of Good Manufacturing Practice (cGMP) for Human Blood and Blood Components, Human Tissues and Human Cellular Therapy Products","Summary
In Australia, manufacture of blood, tissues and biologicals must comply with the federal laws and meet the requirements of the Therapeutic Goods Administration (TGA) Manufacturing Principles as outlined in the current Code of Good Manufacturing Practice (cGMP). The Therapeutic Goods Order (TGO) No. 88 was announced concurrently with the new cGMP, as a new standard for therapeutic goods. This order constitutes a minimum standard for human blood, tissues and cellular therapeutic goods aimed at minimising the risk of infectious disease transmission. The order sets out specific requirements relating to donor selection, donor testing and minimisation of infectious disease transmission from collection and manufacture of these products. The Therapeutic Goods Manufacturing Principles Determination No. 1 of 2013 references the human blood and blood components, human tissues and human cellular therapy products 2013 (2013 cGMP). The name change for the 2013 cGMP has allowed a broadening of the scope of products to include human cellular therapy products. It is difficult to directly compare versions of the code as deletion of some clauses has not changed the requirements to be met, as they are found elsewhere amongst the various guidelines provided. Many sections that were specific for blood and blood components are now less prescriptive and apply to a wider range of cellular therapies, but the general overall intent remains the same. Use of ’should’ throughout the document instead of ’must’ allows flexibility for alternative processes, but these systems will still require justification by relevant logical argument and validation data to be acceptable to TGA. The cGMP has seemingly evolved so that specific issues identified at audit over the last decade have now been formalised in the new version. There is a notable risk management approach applied to most areas that refer to process justification and decision making. These requirements commenced on 31 May 2013 and a 12 month transition period applies for implementation by manufacturers. The cGMP and TGO update follows the implementation of the TGA regulatory biologicals framework for cell and tissue based therapies announced in 2011. One implication for licenced TGA facilities is that they must implement the 2013 cGMP, TGO 88 and other relevant TGOs together, as they are intricately linked. This review is intended to assist manufacturers by comparing the 2000 version of the cGMP, to the new 2013 cGMP, noting that the new Code extends to include human cellular therapy products.","Craig Wright and Zlatibor Velickovic and Ross Brown and Stephen Larsen and Janet L. Macpherson and John Gibson and John E.J. Rasko",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Genetic comparisons yield insight into the evolution of enamel thickness during human evolution","Enamel thickness varies substantially among extant hominoids and is a key trait with significance for interpreting dietary adaptation, life history trajectory, and phylogenetic relationships. There is a strong link in humans between enamel formation and mutations in the exons of the four genes that code for the enamel matrix proteins and the associated protease. The evolution of thick enamel in humans may have included changes in the regulation of these genes during tooth development. The cis-regulatory region in the 5′ flank (upstream non-coding region) of MMP20, which codes for enamelysin, the predominant protease active during enamel secretion, has previously been shown to be under strong positive selection in the lineages leading to both humans and chimpanzees. Here we examine evidence for positive selection in the 5′ flank and 3′ flank of AMELX, AMBN, ENAM, and MMP20. We contrast the human sequence changes with other hominoids (chimpanzees, gorillas, orangutans, gibbons) and rhesus macaques (outgroup), a sample comprising a range of enamel thickness. We find no evidence for positive selection in the protein-coding regions of any of these genes. In contrast, we find strong evidence for positive selection in the 5′ flank region of MMP20 and ENAM along the lineage leading to humans, and in both the 5′ flank and 3′ flank regions of MMP20 along the lineage leading to chimpanzees. We also identify putative transcription factor binding sites overlapping some of the species-specific nucleotide sites and we refine which sections of the up- and downstream putative regulatory regions are most likely to harbor important changes. These non-coding changes and their potential for differential regulation by transcription factors known to regulate tooth development may offer insight into the mechanisms that allow for rapid evolutionary changes in enamel thickness across closely-related species, and contribute to our understanding of the enamel phenotype in hominoids.","Julie E. Horvath and Gowri L. Ramachandran and Olivier Fedrigo and William J. Nielsen and Courtney C. Babbitt and Elizabeth M. St. Clair and Lisa W. Pfefferle and Jukka Jernvall and Gregory A. Wray and Christine E. Wall",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Adaptive traffic signal control with actor-critic methods in a real-world traffic network with different traffic disruption events","The transportation demand is rapidly growing in metropolises, resulting in chronic traffic congestions in dense downtown areas. Adaptive traffic signal control as the principle part of intelligent transportation systems has a primary role to effectively reduce traffic congestion by making a real-time adaptation in response to the changing traffic network dynamics. Reinforcement learning (RL) is an effective approach in machine learning that has been applied for designing adaptive traffic signal controllers. One of the most efficient and robust type of RL algorithms are continuous state actor-critic algorithms that have the advantage of fast learning and the ability to generalize to new and unseen traffic conditions. These algorithms are utilized in this paper to design adaptive traffic signal controllers called actor-critic adaptive traffic signal controllers (A-CATs controllers). The contribution of the present work rests on the integration of three threads: (a) showing performance comparisons of both discrete and continuous A-CATs controllers in a traffic network with recurring congestion (24-h traffic demand) in the upper downtown core of Tehran city, (b) analyzing the effects of different traffic disruptions including opportunistic pedestrians crossing, parking lane, non-recurring congestion, and different levels of sensor noise on the performance of A-CATS controllers, and (c) comparing the performance of different function approximators (tile coding and radial basis function) on the learning of A-CATs controllers. To this end, first an agent-based traffic simulation of the study area is carried out. Then six different scenarios are conducted to find the best A-CATs controller that is robust enough against different traffic disruptions. We observe that the A-CATs controller based on radial basis function networks (RBF (5)) outperforms others. This controller is benchmarked against controllers of discrete state Q-learning, Bayesian Q-learning, fixed time and actuated controllers; and the results reveal that it consistently outperforms them.","Mohammad Aslani and Mohammad Saadi Mesgari and Marco Wiering",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Distributed formation control using robust asynchronous and broadcast-based optimization schemes","We consider the problem of letting a network of mobile agents distributedly track and maintain a formation while using communication schemes that are asynchronous, broadcasts based, and prone to packet losses. To this purpose we revisit and modify an existing distributed optimization algorithm that corresponds to a distributed version of the Newton Raphson (NR) algorithm. The proposed scheme uses then robust asynchronous ratio consensus algorithms as building blocks, and employs opportune definitions for the local cost functions to achieve the desired coordination objective. In our algorithm, indeed, we code the position of the to-be-followed target as the minimum of a shared global cost, and capture the desired inter-robots behaviors through dedicated distances-based potential barriers. We then check the effectiveness of the strategy using field tests, and verify that the scheme achieves the desired goal of introducing robustness to changes in the agents positions due to unexpected disturbances. More precisely, if an agent breaks the formation, then the update mechanism embedded in our scheme make that agent move back to a meaningful position as soon as some packets are successfully received by the misplaced agent.","Vinicio Modolo and Damiano Varagnolo and Ruggero Carli",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Comparing approaches to analyze refactoring activity on software repositories","Some approaches have been used to investigate evidence on how developers refactor their code, whether refactorings activities may decrease the number of bugs, or improve developers’ productivity. However, there are some contradicting evidence in previous studies. For instance, some investigations found evidence that if the number of refactoring changes increases in the preceding time period the number of defects decreases, different from other studies. They have used different approaches to evaluate refactoring activities. Some of them identify committed behavior-preserving transformations in software repositories by using manual analysis, commit messages, or dynamic analysis. Others focus on identifying which refactorings are applied between two programs by using manual inspection or static analysis. In this work, we compare three different approaches based on manual analysis, commit message (Ratzinger's approach) and dynamic analysis (SafeRefactor's approach) to detect whether a pair of versions determines a refactoring, in terms of behavioral preservation. Additionally, we compare two approaches (manual analysis and Ref-Finder) to identify which refactorings are performed in each pair of versions. We perform both comparisons by evaluating their accuracy, precision, and recall in a randomly selected sample of 40 pairs of versions of JHotDraw, and 20 pairs of versions of Apache Common Collections. While the manual analysis presents the best results in both comparisons, it is not as scalable as the automated approaches. Ratzinger's approach is simple and fast, but presents a low recall; differently, SafeRefactor is able to detect most applied refactorings, although limitations in its test generation backend results for some kinds of subjects in low precision values. Ref-Finder presented a low precision and recall in our evaluation.","Gustavo Soares and Rohit Gheyi and Emerson Murphy-Hill and Brittany Johnson",2013,"[""Science Direct""]","Rejeitado: CR11","Rejeitado: CR11"
"Program package for multicanonical simulations of U(1) lattice gauge theory—Second version","A new version STMCMUCA_V1_1 of our program package is available. It eliminates compatibility problems of our Fortran 77 code, originally developed for the g77 compiler, with Fortran 90 and 95 compilers.
New version program summary
Program title: STMC_U1MUCA_v1_1 Catalogue identifier: AEET_v1_1 Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html Programming language: Fortran 77 compatible with Fortran 90 and 95 Computers: Any capable of compiling and executing Fortran code Operating systems: Any capable of compiling and executing Fortran code RAM: 10 MB and up depending on lattice size used No. of lines in distributed program, including test data, etc.: 15059 No. of bytes in distributed program, including test data, etc.: 215733 Keywords: Markov chain Monte Carlo, multicanonical, Wang–Landau recursion, Fortran, lattice gauge theory, U(1) gauge group, phase transitions of continuous systems Classification: 11.5 Catalogue identifier of previous version: AEET_v1_0 Journal Reference of previous version: Computer Physics Communications 180 (2009) 2339–2347 Does the new version supersede the previous version?: Yes Nature of problem: Efficient Markov chain Monte Carlo simulation of U(1) lattice gauge theory (or other continuous systems) close to its phase transition. Measurements and analysis of the action per plaquette, the specific heat, Polyakov loops and their structure factors. Solution method: Multicanonical simulations with an initial Wang–Landau recursion to determine suitable weight factors. Reweighting to physical values using logarithmic coding and calculating jackknife error bars. Reasons for the new version: The previous version was developed for the g77 compiler Fortran 77 version. Compiler errors were encountered with Fortran 90 and Fortran 95 compilers (specified below). Summary of revisions: epsilon=one/10**10 is replaced by epsilon/10.0D10 in the parameter statements of the subroutines u1_bmha.f, u1_mucabmha.f, u1wl_backup.f, u1wlread_backup.f of the folder Libs/U1_par. For the tested compilers script files are added in the folder ExampleRuns and readme.txt files are now provided in all subfolders of ExampleRuns. The gnuplot driver files produced by the routine hist_gnu.f of Libs/Fortran are adapted to syntax required by gnuplot version 4.0 and higher. Restrictions: Due to the use of explicit real*8 initialization the conversion into real*4 will require extra changes besides replacing the implicit.sta file by its real*4 version. Unusual features: The programs have to be compiled the script files like those contained in the folder ExampleRuns as explained in the original paper. Running time: The prepared test runs took up to 74 minutes to execute on a 2 GHz PC.","Alexei Bazavov and Bernd A. Berg",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"MEAMfit: A reference-free modified embedded atom method (RF-MEAM) energy and force-fitting code","MEAMfit v1.02. Changes: various bug fixes; speed of single-shot energy and force calculations (not optimization) increased by ×10; elements up to Cn (Z=112) now correctly read from vasprun.xml files; EAM fits now produce Camelion output files; changed max number of vasprun.xml files to 10,000 (an unnecessary lower limit of 10 was allowed in the previous version).
New version program summary
Program title: MEAMfit Catalogue identifier: AEWY_v1_1 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEWY_v1_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: BSD 2-Clause License No. of lines in distributed program, including test data, etc.: 5202334 No. of bytes in distributed program, including test data, etc.: 66637698 Distribution format: tar.gz Programming language: Fortran. Computer: Linux based workstations. Operating system: Linux. RAM: 120 MB Classification: 16.1. Catalogue identifier of previous version: AEWY_v1_0 Journal reference of previous version: Comput. Phys. Comm. 196 (2015) 439 External routines: TOMS611 Unconstrained Minimization [1] included in the MEAMfit code. Does the new version supersede the previous version?: Yes Nature of problem: Fitting embedded atom method (EAM) and reference-free modified embedded atom method (RF-MEAM) potentials [2–3] to energies and forces produced by VASP [4–7]. Solution method: A computer program is presented which uses a conjugate-gradient minimizer paired with a genetic algorithm to fit EAM and RF-MEAM potentials to energies and/or atomic forces read directly from VASP output files. Potentials produced by the code are directly usable with the LAMMPS [8] or Camelion [9] molecular-dynamics packages. Reasons for new version: Bugs fixed; improvements made. Summary of revisions: Various bug fixes; speed of single-shot energy and force calculations (not optimization) increased by ×10; elements up to Cn (Z=112) now correctly read from vasprun.xml files; EAM fits now produce Camelion output files; changed max number of vasprun.xml files to 10,000 (an unnecessary lower limit of 10 was allowed in the previous version). Additional comments: User manual provided. !!!!! The distribution file for this program is over 65 MB and therefore is not delivered directly when download or Email is requested. Instead a html file giving details of how the program can be obtained is sent. !!!!! Running time: The run-time depends on the required level of accuracy of the final potential. For an EAM potential fit to 670 energies, a few hours on a single core is usually sufficient to produce a potential with R=12%–13% (see Equation. 9 in main-text for definition). To ensure a maximally optimized potential however (R=12%), a run-time of 24 h is recommended. To optimize a RF-MEAM potential, a further 24 h should be allowed. One will already find an improvement over the EAM using just a single core, however to ensure a maximally optimized potential, one should run several instances of MEAMfit in parallel. References:[1]J. E. Dennis, D. Gay and R. E. Welsch, ACM Trans. on Math. Soft., 7 (1981) 348–368.[2]M. I. Baskes, Materials Science and Engineering A, 261 (1999), 165.[3]M. Timonova and B. J. Thijsse, Modelling Simul. Mater. Sci. Eng., 19 (2011) 015003[4]G. Kresse, J. Hafner, Phys. Rev. B 47 (1993) 558.[5]G. Kresse, J. Hafner, Phys. Rev. B 49 (1994) 14251.[6]G. Kresse, J. Furthmüller, Comput. Mat. Sci. 6 (1996) 15.[7]G. Kresse, J. Furthmüller, Phys. Rev. B 54 (1996) 11169.[8]S. Plimpton, J. Comp. Phys. 117 (1995)[9]http//://tinyurl.com/camelion11-53","Andrew Ian Duff",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Combining dynamic modelling codes with medium energy ion scattering measurements to characterise plasma doping","Plasma doping ion implantation (PLAD) is becoming increasingly important in the manufacture of advanced semiconductor device structures but a fundamental understanding of PLAD is complicated. A model of PLAD into planar substrates has been constructed using the one dimensional computer code TRIDYN to predict collision cascades and hence substrate compositional changes during implantation. Medium Energy Ion Scattering (MEIS) measurements of dopant profiles in PLAD processed samples were used to calibrate the input ion and neutral fluxes to the model. Rules could then be proposed for how post implant profiles should be modified by a cleaning step. This learning was applied to a three dimensional TRI3DYN based model for PLAD implants into FinFET like structures. Comparison of the model to dopant profile measurements made by time of flight (TOF)-MEIS revealed the angular distributions of neutral species and doping mechanisms acting in three dimensional structures.","J. England and W. Möller and J.A. van den Berg and A. Rossall and W.J. Min and J. Kim",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Structural Analysis of a Historical Dam","Studies pertaining to earthquakes began only around the 17th century and those related to their effects on structures began even later in the 19th century. Hence structures made during or prior to this period are often not designed for seismic forces. India has many historical structures that were constructed long before the codes for seismic resistant design came in practice and, in many regions the seismic activities too have changed over time. It is therefore necessary that these monuments are analysed for safety during seismic activity. This paper aims to analyse the probable failure patterns of a composite dam about 120 years old using time history analysis. 2D modelling and analysis has been carried out using ANSYS14.0 to estimate stresses in the dam for three time histories with varying PGA values. Stress results show that failure of weaker materials at joints may cause internal crack formation in the dam.","Soumya and A.D. Pandey and R. Das and M.J. Mahesh and S. Anvesh and P. Saini",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Diagnostic value of color-coded duplex sonography in patients with ischemic stroke and congenital changes in the circle of Willis","The circle of Willis (CoW) forms the main circulatory system in the human brain. A large number of variations of the CoW is known, and also their association with ischemic stroke. Three cases of young patients with combination of ischemic stroke and anomalies in the CoW are presented, and the value of the color-coded duplex sonography (CCDS) is compared to other imaging diagnostics such as magnetic resonance angiography (MRA) and digital subtraction angiography (DSA). In these patients we found multiple risk factors such as stenosis or thrombosis of intracranial brain vessels, mechanical compression of vessels, a genetic mutation associated with an increased risk of thrombosis, and intake of oral contraceptives. For clinical evaluation several methods were used: detailed medical history, neurological status, laboratory examinations (complete blood count, biochemistry, lipid profile, HIV1/2, Syphilis RPR test), screening for markers associated with an increased risk of thrombosis, chest X-ray, spinal fluid study, CCDS, DSA, MRA. A full conformity in the data from CCDS and other imaging methods was found. The authors discuss the pathogenetic role of congenital anomalies of CoW, incidence of ischemic stroke and the high diagnostic value of CCDS for finding such anomalies.","Ivan Staikov and Ivan Stoyanov and Milena Staneva and Neyko Neykov and Galina Kirova and Petar Polomski and Ivo Petrov",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"PFLOTRAN-E4D: A parallel open source PFLOTRAN module for simulating time-lapse electrical resistivity data","Time-lapse electrical resistivity tomography (ERT) is finding increased application for remotely monitoring processes occurring in the near subsurface in three-dimensions (i.e. 4D monitoring). However, there are few codes capable of simulating the evolution of subsurface resistivity and corresponding tomographic measurements arising from a particular process, particularly in parallel and with an open source license. Herein we describe and demonstrate an electrical resistivity tomography module for the PFLOTRAN subsurface flow and reactive transport simulation code, named PFLOTRAN-E4D. The PFLOTRAN-E4D module operates in parallel using a dedicated set of compute cores in a master-slave configuration. At each time step, the master processes receives subsurface states from PFLOTRAN, converts those states to bulk electrical conductivity, and instructs the slave processes to simulate a tomographic data set. The resulting multi-physics simulation capability enables accurate feasibility studies for ERT imaging, the identification of the ERT signatures that are unique to a given process, and facilitates the joint inversion of ERT data with hydrogeological data for subsurface characterization. PFLOTRAN-E4D is demonstrated herein using a field study of stage-driven groundwater/river water interaction ERT monitoring along the Columbia River, Washington, USA. Results demonstrate the complex nature of subsurface electrical conductivity changes, in both the saturated and unsaturated zones, arising from river stage fluctuations and associated river water intrusion into the aquifer. The results also demonstrate the sensitivity of surface based ERT measurements to those changes over time. PFLOTRAN-E4D is available with the PFLOTRAN development version with an open-source license at https://bitbucket.org/pflotran/pflotran-dev.","Timothy C. Johnson and Glenn E. Hammond and Xingyuan Chen",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Assessment of subchannel code ASSERT-PV for flow-distribution predictions","This paper reports an assessment of the recently released subchannel code ASSERT-PV 3.2 for the prediction of flow-distribution in fuel bundles, including subchannel void fraction, quality and mass fluxes. Experimental data from open literature and from in-house tests are used to assess the flow-distribution models in ASSERT-PV 3.2. The prediction statistics using the recommended model set of ASSERT-PV 3.2 are compared to those from previous code versions. Separate-effects sensitivity studies are performed to quantify the contribution of each flow-distribution model change or enhancement to the improvement in flow-distribution prediction. The assessment demonstrates significant improvement in the prediction of flow-distribution in horizontal fuel channels containing CANDU bundles.","A. Nava-Dominguez and Y.F. Rao and G.M. Waddington",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Stochastic hyperfine interactions modeling library—Version 2","The stochastic hyperfine interactions modeling library (SHIML) provides a set of routines to assist in the development and application of stochastic models of hyperfine interactions. The library provides routines written in the C programming language that (1) read a text description of a model for fluctuating hyperfine fields, (2) set up the Blume matrix, upon which the evolution operator of the system depends, and (3) find the eigenvalues and eigenvectors of the Blume matrix so that theoretical spectra of experimental techniques that measure hyperfine interactions can be calculated. The optimized vector and matrix operations of the BLAS and LAPACK libraries are utilized. The original version of SHIML constructed and solved Blume matrices for methods that measure hyperfine interactions of nuclear probes in a single spin state. Version 2 provides additional support for methods that measure interactions on two different spin states such as Mössbauer spectroscopy and nuclear resonant scattering of synchrotron radiation. Example codes are provided to illustrate the use of SHIML to (1) generate perturbed angular correlation spectra for the special case of polycrystalline samples when anisotropy terms of higher order than A22 can be neglected and (2) generate Mössbauer spectra for polycrystalline samples for pure dipole or pure quadrupole transitions.
New version program summary
Program title: SHIML Catalogue identifier: AEIF_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIF_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License version 3 with supplemental citation provision No. of lines in distributed program, including test data, etc.: 88510 No. of bytes in distributed program, including test data, etc.: 3311047 Distribution format: tar.gz Programming language: C. Computer: Any. Operating system: LINUX, OS X. RAM: Variable Catalogue identifier of previous version: AEIF_v1_0 Journal reference of previous version: Comput. Phys. Comm. 182(2011)1061 Classification: 7.4. External routines: TAPP [1], BLAS [2], a C-interface to BLAS [3], and LAPACK [4]. Additionally, GSL [3] is needed to compile the example code that simulates Mössbauer spectra. Does the new version supersede the previous version?: No Nature of problem: In condensed matter systems, hyperfine methods such as nuclear magnetic resonance (NMR), Mössbauer effect (ME), muon spin rotation (μSR), and perturbed angular correlation spectroscopy (PAC) measure electromagnetic fields due to electronic and magnetic structure within Angstroms of nuclear probes through the hyperfine interaction. When interactions fluctuate at rates comparable to the time scale of a hyperfine method, there is a loss in signal coherence, and spectra in the time domain are damped while spectra in the frequency domain are broadened. The degree of damping or broadening can be used to determine fluctuation rates, provided that theoretical expressions for spectra can be derived for relevant physical models of the fluctuations. SHIML provides routines to help researchers quickly develop code to incorporate stochastic models of fluctuating hyperfine interactions in calculations of hyperfine spectra. Solution method: Calculations are based on the method for modeling stochastic hyperfine interactions for PAC by Winkler and Gerdau [5]. The method is extended to include other hyperfine methods following the work of Dattagupta [6]. The code provides routines for reading model information from text files, allowing researchers to develop new models quickly without the need to modify computer code for each new model to be considered. Reasons for new version: The original version of the library provided support only for those methods that measure hyperfine interactions on one spin state of the nuclear probe. As such, it excluded important hyperfine methods that measure the interactions on two spin states such as Mössbauer spectroscopy and nuclear resonant scattering of synchrotron radiation. The present version of SHIML provides the necessary support for such double spin-state methods while maintaining backward compatibility for code already developed using the original version. Summary of revisions: Routines now check that values representing nuclear spins are positive integers or positive half-integers. Additional utility functions are provided to make it easier for code developers to calculate Hamiltonians of electric quadrupole interactions. A correction was made to the portion of code responsible for calculating the Blume matrix of single spin-state methods; however, this change will not alter results obtained from single spin-state simulations using version 1 of the library. The remaining revisions support calculations for double spin-state methods. (1) Model-file syntax is expanded in order to allow users to specify different hyperfine interactions for ground and excited spin states and to input isomer shifts. (2) New routines for initialization and for Blume-matrix calculations are included for the double spin-state case. (3) New example code is provided to illustrate how SHIML can be used to simulate Mössbauer spectra of polycrystalline samples for pure dipole or pure quadrupole transitions; background information about the Mössbauer examples can be found in Ref. [7]. Finally, updated software documentation is included in a User’s Guide as a PDF file in the code distribution. Running time: Variable References:[1]M. O. Zacate, The Adjustable Parameter Package, Technical Report 2, CINSAM Grant 2006-R7 (unpublished); available for download at http://tapp.nku.edu/.[2]L. S. Blackford et al., ACM Trans. Math. Soft. 28 (2002) 135; J. Dongarra, International Journal of High Performance Applications and Supercomputing 16 (2002) 1; http://www.netlib.org/blas/.[3]M. Galassi et al., GNU Scientific Library Reference Manual, third edition (2009); available for download at http://www.gnu.org/software/gsl/.[4]E. Anderson et al., LAPACK Users’ Guide, third ed. (Society for Industrial and Applied Mathematics, Philadelphia, PA, 1999); http://www.netlib.org/lapack/.[5]H. Winkler, E. Gerdau, Z. Phys. 262 (1973) 363.[6]S. Dattagupta, Hyperfine Interact. 11 (1981) 77.[7]M. O. Zacate, W. E. Evenson, Hyperfine Interact. 231 (2015) 143.","Matthew O. Zacate and William E. Evenson",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Comparison of different numerical approaches to the 1D sea-ice thermodynamics problem","The vertical one-dimensional sea-ice thermodynamic problem using the principle of conservation of enthalpy is revisited here using (1) the Bitz and Lipscomb (1999) finite-difference approach (FD), (2) a reformulation of the sigma-level transformation of Huwald et al. (2005b) (FV) and (3) a Finite Element approach also in sigma coordinates (FE). These three formulations are compared in terms of physics, numerics, and performance, in order to identify the best choice for large-scale climate models. The BL99 formulation sequentially treats the diffusion of heat and the changes in the vertical position of the ice-snow layers. In contrast, the FV sigma-level transformation elegantly treats both simultaneously. The original FV formulation suffers however from slow convergence. The convergence can nonetheless be improved significantly with a few simple modifications to the original code. The three formulations are compared following the experimental protocol of the Sea Ice Model Intercomparison Project for ice thermodynamics (SIMIP2). It is found that all formulations converge to the same solution. The FD approach, however, suffers from the added cost of the remapping step at large number of ice layers (we include in the appendix an optimized version of the FD code–written by one of the reviewer–that resolves this issue). Finally the FE formulation results in a sub-surface temperature over-estimation at low resolution, a problem which disappears at high resolution. Hence, only FD and FV are found suitable for climate models.","Frederic Dupont and Martin Vancoppenolle and Louis-Bruno Tremblay and Hendrik Huwald",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Axially deformed solution of the Skyrme–Hartree–Fock–Bogolyubov equations using the transformed harmonic oscillator basis (III) hfbtho (v3.00): A new version of the program","We describe the new version 3.00 of the code hfbtho that solves the nuclear Hartree–Fock (HF) or Hartree–Fock–Bogolyubov (HFB) problem by using the cylindrical transformed deformed harmonic oscillator basis. In the new version, we have implemented the following features: (i) the full Gogny force in both particle–hole and particle–particle channels, (ii) the calculation of the nuclear collective inertia at the perturbative cranking approximation, (iii) the calculation of fission fragment charge, mass and deformations based on the determination of the neck, (iv) the regularization of zero-range pairing forces, (v) the calculation of localization functions, (vi) a MPI interface for large-scale mass table calculations.
PROGRAM SUMMARY
Program title:hfbtho v3.00 Program Files doi:http://dx.doi.org/10.17632/c5g2f92by3.1 Licensing provisions: GPL v3 Programming language: FORTRAN-95 Journal reference of previous version: M.V. Stoitsov, N. Schunck, M. Kortelainen, N. Michel, H. Nam, E. Olsen, J. Sarich, and S. Wild, Comput. Phys. Commun. 184 (2013). Does the new version supersede the previous one: Yes Summary of revisions: 1. the Gogny force in both particle–hole and particle–particle channels was implemented; 2. the nuclear collective inertia at the perturbative cranking approximation was implemented; 3. fission fragment charge, mass and deformations were implemented based on the determination of the position of the neck between nascent fragments; 4. the regularization method of zero-range pairing forces was implemented; 5. the localization functions of the HFB solution were implemented; 6. a MPI interface for large-scale mass table calculations was implemented. Nature of problem:hfbtho is a physics computer code that is used to model the structure of the nucleus. It is an implementation of the energy density functional (EDF) approach to atomic nuclei, where the energy of the nucleus is obtained by integration over space of some phenomenological energy density, which is itself a functional of the neutron and proton intrinsic densities. In the present version of hfbtho, the energy density derives either from the zero-range Skyrme or the finite-range Gogny effective two-body interaction between nucleons. Nuclear super-fluidity is treated at the Hartree–Fock–Bogolyubov (HFB) approximation. Constraints on the nuclear shape allows probing the potential energy surface of the nucleus as needed e.g., for the description of shape isomers or fission. The implementation of a local scale transformation of the single-particle basis in which the HFB solutions are expanded provide a tool to properly compute the structure of weakly-bound nuclei. Solution method: The program uses the axial Transformed Harmonic Oscillator (THO) single-particle basis to expand quasiparticle wave functions. It iteratively diagonalizes the Hartree–Fock–Bogolyubov Hamiltonian based on generalized Skyrme-like energy densities and zero-range pairing interactions or the finite-range Gogny force until a self-consistent solution is found. A previous version of the program was presented in M.V. Stoitsov, N. Schunck, M. Kortelainen, N. Michel, H. Nam, E. Olsen, J. Sarich, and S. Wild, Comput. Phys. Commun. 184 (2013) 1592–1604 with much of the formalism presented in the original paper M.V. Stoitsov, J. Dobaczewski, W. Nazarewicz, P. Ring, Comput. Phys. Commun. 167 (2005) 43–63. Additional comments: The user must have access to (i) the LAPACK subroutines dsyeevr, dsyevd, dsytrf and dsytri, and their dependencies, which compute eigenvalues and eigenfunctions of real symmetric matrices, (ii) the LAPACK subroutines dgetri and dgetrf, which invert arbitrary real matrices, and (iii) the BLAS routines dcopy, dscal, dgemm and dgemv for double-precision linear algebra (or provide another set of subroutines that can perform such tasks). The BLAS and LAPACK subroutines can be obtained from the Netlib Repository at the University of Tennessee, Knoxville: http://netlib2.cs.utk.edu/.","R. Navarro Perez and N. Schunck and R.-D. Lasseri and C. Zhang and J. Sarich",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Geomechanical responses during depressurization of hydrate-bearing sediment formation over a long methane gas production period","The geomechanical behaviour of hydrate-bearing sediments during methane gas production is complex due to the spatial and temporal changes in stress, pore pressure, temperature, and phase change. In order to evaluate the geomechanical risks during methane gas production, it is necessary to understand the thermo-hydro-mechanical (THM) responses in the production region that recovers methane gas from the hydrate-bearing sediments. In this study, a fully coupled THM numerical simulator code was used to examine the reservoir scale field behaviour observed during the gas production trial conducted at the Eastern Nankai Trough, Japan in March, 2013. Using the available field investigation data, history matching of the gas production test was conducted to evaluate the methane gas production process. The fully coupled model allowed examination of the mechanical response using the critical state based constitutive model proposed by Uchida et al. (2012). The model parameters were determined from the results of triaxial compression tests conducted on recovered core samples. Based on the reservoir scale simulation results, this paper investigated the mechanical responses of five selected elements at different locations in the hydrate gas production region. The mechanical responses of hydrate-bearing sediments at specific locations within the production region are related to their hydrate dissociation status, which typically can be divided into before, during and after stages of hydrate dissociation. The 260 days gas production simulation suggests an increase in effective stresses accompanied by shearing deformation, which makes the soil more plastic. Potential geomechanical risks (such as wellbore stability and formation compaction) associated with the observed changes in stress/strain were also identified.","Mingliang Zhou and Kenichi Soga and Koji Yamamoto and Hongwei Huang",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Real-time tracking-by-learning with high-order regularization fusion for big video abstraction","Visual tracking is a key technique used by video abstraction to achieve efficient post-analysis for big video surveillance. In order to tackle the problem of constantly changing scenarios during online tracking, additional factors such as motion can be incorporated by utilizing a fusion strategy to improve the final performance. Unfortunately, straightforward output fusion is difficult to be synchronized due to the diversity in model regression. Therefore, a widely cited problem for learning based fusion is to incorporate regularizers for label assignment of unlabeled samples, which is one of the major research focuses on semi-supervised learning. In this paper, a novel tracking strategy based on semi-supervised learning with high order regularization fusion has been proposed. It employs two different types of regularizers to achieve more accurate label assignment based on kernelized confidence prediction and graph-based bi-directional trace from motion. The computation of the proposed tracker takes advantage of the unique feature of circulant matrix in Fourier domain and integral patterns, and thus can be readily implemented for real-time processing, even without any code optimization. Via a dynamic budget maintenance for model updating, the proposed tracking method demonstrated to outperform most state-of-art trackers on challenging benchmark videos with a fixed parameter configuration.","Peng Zhang and Tao Zhuo and Yanning Zhang and Lei Xie and Dapeng Tao",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"p53 Signaling Pathway Polymorphisms Associated With Emphysematous Changes in Patients With COPD","Background
The p53 signaling pathway may be important for the pathogenesis of emphysematous changes in the lungs of smokers. Polymorphism of p53 at codon 72 is known to affect apoptotic effector proteins, and the polymorphism of mouse double minute 2 homolog (MDM2) single nucleotide polymorphism (SNP)309 is known to increase MDM2 expression. The aim of this study was to assess polymorphisms of the p53 and MDM2 genes in smokers and confirm the role of SNPs in these genes in the pathogenesis of pulmonary emphysema.
Methods
This study included 365 patients with a smoking history, and the polymorphisms of p53 and MDM2 genes were identified. The degree of pulmonary emphysema was determined by means of CT scanning. SNPs, MDM2 mRNA, and p53 protein levels were assessed in human lung tissues from smokers. Plasmids encoding p53 and MDM2 SNPs were used to transfect human lung fibroblasts (HLFs) with or without cigarette smoke extract (CSE), and the effects on cell proliferation and MDM2 promoter activity were measured.
Results
The polymorphisms of the p53 and MDM2 genes were associated with emphysematous changes in the lung and were also associated with p53 protein and MDM2 mRNA expression in the lung tissue samples. Transfection with a p53 gene-coding plasmid regulated HLF proliferation, and the analysis of P2 promoter activity in MDM2 SNP309-coding HLFs showed the promoter activity was altered by CSE.
Conclusions
Our data demonstrated that p53 and MDM2 gene polymorphisms are associated with apoptotic signaling and smoking-related emphysematous changes in lungs from smokers.","Shiro Mizuno and Takeshi Ishizaki and Maiko Kadowaki and Masaya Akai and Kohei Shiozaki and Masaharu Iguchi and Taku Oikawa and Ken Nakagawa and Kazuhiro Osanai and Hirohisa Toga and Jose Gomez-Arroyo and Donatas Kraskauskas and Carlyne D. Cool and Herman J. Bogaard and Norbert F. Voelkel",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Cross-Modal Self-Taught Hashing for large-scale image retrieval","Cross-modal hashing integrates the advantages of traditional cross-modal retrieval and hashing, it can solve large-scale cross-modal retrieval effectively and efficiently. However, existing cross-modal hashing methods rely on either labeled training data, or lack semantic analysis. In this paper, we propose Cross-Modal Self-Taught Hashing (CMSTH) for large-scale cross-modal and unimodal image retrieval. CMSTH can effectively capture the semantic correlation from unlabeled training data. Its learning process contains three steps: first we propose Hierarchical Multi-Modal Topic Learning (HMMTL) to detect multi-modal topics with semantic information. Then we use Robust Matrix Factorization (RMF) to transfer the multi-modal topics to hash codes which are more suited to quantization, and these codes form a unified hash space. Finally we learn hash functions to project all modalities into the unified hash space. Experimental results on two web image datasets demonstrate the effectiveness of CMSTH compared to representative cross-modal and unimodal hashing methods.","Liang Xie and Lei Zhu and Peng Pan and Yansheng Lu",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Testing the generalized complementary relationship of evaporation with continental-scale long-term water-balance data","The original and revised versions of the generalized complementary relationship (GCR) of evaporation (ET) were tested with six-digit Hydrologic Unit Code (HUC6) level long-term (1981–2010) water-balance data (sample size of 334). The two versions of the GCR were calibrated with Parameter-Elevation Regressions on Independent Slopes Model (PRISM) mean annual precipitation (P) data and validated against water-balance ET (ETwb) as the difference of mean annual HUC6-averaged P and United States Geological Survey HUC6 runoff (Q) rates. The original GCR overestimates P in about 18% of the PRISM grid points covering the contiguous United States in contrast with 12% of the revised version. With HUC6-averaged data the original version has a bias of −25mmyr−1 vs the revised version’s −17mmyr−1, and it tends to more significantly underestimate ETwb at high values than the revised one (slope of the best fit line is 0.78 vs 0.91). At the same time it slightly outperforms the revised version in terms of the linear correlation coefficient (0.94 vs 0.93) and the root-mean-square error (90 vs 92mmyr−1).","Jozsef Szilagyi and Richard Crago and Russell J. Qualls",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"BinComp: A stratified approach to compiler provenance Attribution","Compiler provenance encompasses numerous pieces of information, such as the compiler family, compiler version, optimization level, and compiler-related functions. The extraction of such information is imperative for various binary analysis applications, such as function fingerprinting, clone detection, and authorship attribution. It is thus important to develop an efficient and automated approach for extracting compiler provenance. In this study, we present BinComp, a practical approach which, analyzes the syntax, structure, and semantics of disassembled functions to extract compiler provenance. BinComp has a stratified architecture with three layers. The first layer applies a supervised compilation process to a set of known programs to model the default code transformation of compilers. The second layer employs an intersection process that disassembles functions across compiled binaries to extract statistical features (e.g., numerical values) from common compiler/linker-inserted functions. This layer labels the compiler-related functions. The third layer extracts semantic features from the labeled compiler-related functions to identify the compiler version and the optimization level. Our experimental results demonstrate that BinComp is efficient in terms of both computational resources and time.","Ashkan Rahimian and Paria Shirani and Saed Alrbaee and Lingyu Wang and Mourad Debbabi",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Influence of hydraulic fracturing on impedance and efficiency of thermal recovery from HDR reservoirs","Impedance and efficiency are key characteristics in regard to the economic viability of geothermal sites. The influence of hydraulic fracturing process on the efficiency of thermal recovery from HDR reservoirs is addressed in a totally coupled thermo-poroelastic framework. A fracturing model (HFM) is integrated into a domestic Fortran 90 finite element code. At any time and any geometrical point, the state of fracture is embodied in a fabric: the later includes both the actual fracture length and the actual fracture width in all directions of space. The local current anisotropic permeability tensor, which describes the evolving hydraulic connectivity of the fractured medium, is obtained by directional integration of the updated fracture fabric. A modified version of the model which accounts for the effect of deviatoric stresses on the fracturing criterion is shown to have stronger effects on the enhancement of the permeability. To gain confidence in the numerical approach, simulations are correlated to field data of the Soultz-sous-Forêts geothermal site that are available in the early times of the injection process. Cooling facilitates significantly the fracturing process close to the injection well. The model is next applied to simulate a hydraulic fracturing test over a long period of time. Hydraulic fracturing is shown to decrease the impedance of the reservoir, but it reduces the duration of an efficient exploitation. The nature of the mechanical, hydraulic and thermal boundary conditions is also investigated. The composition of the working fluid and the change of its viscosity with temperature are shown to affect fluid and heat transports in the poroelastic medium and hence the efficiency of hydraulic fracturing.","Murad AbuAisha and Benjamin Loret",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"SARAH 3.2: Dirac gauginos, UFO output, and more","SARAH is a Mathematica package optimized for the fast, efficient and precise study of supersymmetric models beyond the MSSM: a new model can be defined in a short form and all vertices are derived. This allows SARAH to create model files for FeynArts/FormCalc, CalcHep/CompHep and WHIZARD/O’Mega. The newest version of SARAH now provides the possibility to create model files in the UFO format which is supported by MadGraph 5, MadAnalysis 5, GoSam, and soon by Herwig++. Furthermore, SARAH also calculates the mass matrices, RGEs and 1-loop corrections to the mass spectrum. This information is used to write source code for SPheno in order to create a precision spectrum generator for the given model. This spectrum-generator–generator functionality as well as the output of WHIZARD and CalcHep model files has seen further improvement in this version. Also models including Dirac gauginos are supported with the new version of SARAH, and additional checks for the consistency of the implementation of new models have been created.
Program summary
Program title:SARAH Catalogue identifier: AEIB_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIB_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 322411 No. of bytes in distributed program, including test data, etc.: 3629206 Distribution format: tar.gz Programming language: Mathematica. Computer: All for which Mathematica is available. Operating system: All for which Mathematica is available. Classification: 11.1, 11.6. Catalogue identifier of previous version: AEIB_v1_0 Journal reference of previous version: Comput. Phys. Comm. 182 (2011) 808 Does the new version supersede the previous version?: Yes, the new version includes all known features of the previous version but also provides the new features mentioned below. Nature of problem: To use Madgraph for new models it is necessary to provide the corresponding model files which include all information about the interactions of the model. However, the derivation of the vertices for a given model and putting those into model files which can be used with Madgraph is usually very time consuming. Dirac gauginos are not present in the minimal supersymmetric standard model (MSSM) or many extensions of it. Dirac mass terms for vector superfields lead to new structures in the supersymmetric (SUSY) Lagrangian (bilinear mass term between gaugino and matter fermion as well as new D-terms) and modify also the SUSY renormalization group equations (RGEs). The Dirac character of gauginos can change the collider phenomenology. In addition, they come with an extended Higgs sector for which a precise calculation of the 1-loop masses has not happened so far. Solution method: SARAH calculates the complete Lagrangian for a given model whose gauge sector can be any direct product of SU(N) gauge groups. The chiral superfields can transform as any, irreducible representation with respect to these gauge groups and it is possible to handle an arbitrary number of symmetry breakings or particle rotations. Also the gauge fixing is automatically added. Using this information, SARAH derives all vertices for a model. These vertices can be exported to model files in the UFO which is supported by Madgraph and other codes like GoSam, MadAnalysis or ALOHA. The user can also study models with Dirac gauginos. In that case SARAH includes all possible terms in the Lagrangian stemming from the new structures and can also calculate the RGEs. The entire impact of these terms is then taken into account in the output of SARAH to UFO, CalcHep, WHIZARD, FeynArts and SPheno. Reasons for new version: SARAH provides, with this version, the possibility of creating model files in the UFO format. The UFO format is supposed to become a standard format for model files which should be supported by many different tools in the future. Also models with Dirac gauginos were not supported in earlier versions. Summary of revisions: Support of models with Dirac gauginos. Output of model files in the UFO format, speed improvement in the output of WHIZARD model files, CalcHep output supports the internal diagonalization of mass matrices, output of control files for LHPC spectrum plotter, support of generalized PDG numbering scheme PDG.IX, improvement of the calculation of the decay widths and branching ratios with SPheno, the calculation of new low energy observables are added to the SPheno output, the handling of gauge fixing terms has been significantly simplified. Restrictions: SARAH can only derive the Lagrangian in an automatized way for N=1 SUSY models, but not for those with more SUSY generators. Furthermore, SARAH supports only renormalizable operators in the output of model files in the UFO format and also for CalcHep, FeynArts and WHIZARD. Also color sextets are not yet included in the model files for Monte Carlo tools. Dimension 5 operators are only supported in the calculation of the RGEs and mass matrices. Unusual features: SARAH does not need the Lagrangian of a model as input to calculate the vertices. The gauge structure, particle and content and superpotential as well as rotations stemming from gauge symmetry breaking are sufficient. All further information is derived by SARAH on its own. Therefore, the model files are very short and the implementation of new models is fast and easy. In addition, the implementation of a model can be checked for physical and formal consistency. In addition, SARAH can generate Fortran code for a full 1-loop analysis of the mass spectrum in the context for Dirac gauginos. Running time: Measured CPU time for the evaluation of the MSSM using a Lenovo Thinkpad X220 with i7 processor (2.53 GHz). Calculating the complete Lagrangian: 9 s. Calculating all vertices: 51 s. Output of the UFO model files: 49 s.","Florian Staub",2013,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A cyber–physical system-based approach for industrial automation systems","Industrial automation systems (IASs) are commonly developed using the languages defined by the IEC 61131 standard and are executed on programmable logic controllers (PLCs). Their software part is commonly considered only after the development and integration of mechanics and electronics. However, this approach narrows the solution space for software; thus, it is considered inadequate to address the complexity of today's systems. In this paper, we adopt a system-based approach for the development of IASs. Based on this, the UML model of the software part of the system is extracted from the SysML system model and it is then refined to get the implementation code. Two implementation alternatives are considered to exploit both PLCs and the recent deluge of embedded boards in the market. For PLC targets, the new version of IEC 61131 that supports object-orientation is adopted, while Java is used for embedded boards. The case study used to illustrate our approach was developed as a lab exercise, which aims to introduce to students a number of technologies used to address challenges in the domain of cyber–physical systems and highlights the role of the Internet of Things (IoT) as a glue for their cyber interfaces.","Kleanthis Thramboulidis",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Selection and paucity of phylogenetic signal challenge the utility of alpha-tubulin in reconstruction of evolutionary history of free-living litostomateans (Protista, Ciliophora)","The class Litostomatea represents a highly diverse but monophyletic group, uniting both free-living and endosymbiotic ciliates. Ribosomal RNA genes and ITS-region sequences helped to recognize and define the main litostomatean lineages, but did not provide enough phylogenetic signal to unambiguously resolve their interrelationships. In this study, we attempted to improve the resolution among main free-living predatory lineages by adding the gene coding for alpha-tubulin. However, our phylogenetic analyses challenged the performance of alpha-tubulin in reconstruction of evolutionary history of free-living litostomateans. We identified several mutually interconnected problems associated with the ciliate alpha-tubulin gene: the paucity of phylogenetic signal, molecular homoplasies and non-neutral evolution. Positive selection may generate molecular homoplasies (parallel evolution), while negative selection may cause a small number of changes and hence little phylogenetic informativness. Both problems were encountered in nucleotide and amino acid alpha-tubulin alignments, indicating an action of various selective pressures. Taking into account the involvement of alpha-tubulin in many essential biological processes, this protein could be so strongly affected by purifying selection that it even might have become an inappropriate molecular marker for reconstruction of phylogenetic relationships. Therefore, a great caution should be paid when tubulin genes are included in phylogenetic and/or phylogenomic analyses.","Ľubomír Rajter and Peter Vďačný",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Contrasting prediction methods for early warning systems at undergraduate level","Recent studies have provided evidence in favour of adopting early warning systems as a means of identifying at-risk students. Our study examines eight prediction methods, and investigates the optimal time in a course to apply such a system. We present findings from a statistics university course which has weekly continuous assessment and a large proportion of resources on the Learning Management System Blackboard. We identify weeks 5–6 (half way through the semester) as an optimal time to implement an early warning system, as it allows time for the students to make changes to their study patterns while retaining reasonable prediction accuracy. Using detailed variables, clustering and our final prediction method of BART (Bayesian Additive Regressive Trees) we can predict students' final mark by week 6 based on mean absolute error to 6.5 percentage points. We provide our R code for implementation of the prediction methods used in a GitHub repository11Abbreviations: Bayesian Additive Regressive Trees (BART); Random Forests (RF); Principal Components Regression (PCR); Multivariate Adaptive Regression Splines (Splines); K-Nearest Neighbours (KNN); Neural Networks (NN) and; Support Vector Machine (SVM). Abbreviations: Bayesian Additive Regressive Trees (BART); Random Forests (RF); Principal Components Regression (PCR); Multivariate Adaptive Regression Splines (Splines); K-Nearest Neighbours (KNN); Neural Networks (NN) and; Support Vector Machine (SVM)","Emma Howard and Maria Meehan and Andrew Parnell",2018,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"The changing face of anaphylaxis in adults and adolescents","Background
Our institution has published serial studies of adults and adolescents with anaphylactic events. The first series was published in 1993 and the last was published in 2006. It was our perception that the nature of anaphylactic episodes had changed over the 2 decades since the last review.
Objective
To determine whether the etiologies and presentations of anaphylaxis have changed during the past decade in our population.
Methods
Patient charts were identified based on International Classification of Diseases, Ninth Revision codes for anaphylactic shock. Charts identified were analyzed for clinical symptoms reported, comorbidities, etiology, investigative testing, and subsequent treatment. These cases were categorized as definitive, probable, or idiopathic based on history and results from testing, similar to our prior reports.
Results
We identified 281 possible cases, of which 218 met criteria for anaphylaxis. Of these cases, median age was 42 years (range 9–78) and 64% were female. In the review of cases, 85 (39%) were determined to have a definitive etiology, 57 were determined to have a probable etiology (26%), and 76 (35%) were idiopathic. Interestingly, of those with a definitive cause, the most common etiology identified was galactose-α-1,3-galactose, accounting for 28 cases (33%). Foods were the second leading cause, accounting for 24 cases (28%).
Conclusion
In this follow-up report on anaphylaxis etiology from a single center, the most common etiology was galactose-α-1,3-galactose. This differs greatly from prior reports from our center. Interestingly, the percentage of cases attributed to idiopathic anaphylaxis decreased from 59% in our previous report to 35% in the present report, which could largely be explained by the number of galactose-α-1,3-galactose cases.","Debendra Pattanaik and Phil Lieberman and Jay Lieberman and Thanai Pongdee and Alexandria Tran Keene",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Classification of malware families based on runtime behaviors","Classification of malware samples plays a crucial role in building and maintaining security. Design of a malware classification system capable of supporting a large set of samples and adaptable to model changes at runtime is required to identify the high number of malware variants. In this paper, file system, network, registry activities observed during the execution traces and n-gram modeling over API-call sequences are used to represent behavior based features of a malware. We present a methodology to build the feature vector by using run-time behaviors by applying online machine learning algorithms for classification of malware samples in a distributed and scalable architecture. To validate the effectiveness and scalability, we evaluate our method on 17,900 recent malign codes such as viruses, trojans, backdoors, worms. Our experimental results show that the presented malware classification’s training and testing accuracy is reached at 94% and 92.5%, respectively.","Abdurrahman Pektaş and Tankut Acarman",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A photometric study of the contact binary AR Bootis","New complete BVR light curves of the contact binary AR Bootis (AR Boo) are presented. They are analyzed through the 2013 version of the Wilson–Devinney (W–D) code. Our new photometric solution shows that AR Boo is a W-subtype W UMa contact binary system with a mass ratio of q=m2/m1=1.865 and a fill-out factor of 12.7%. The changes in the orbital period of AR Boo are analyzed. It is found that the orbital period of this binary has a continuous increase at a rate of 2.04×10−7daysyr−1, together with a cyclic variation (with a period of 39.05 yr and an amplitude of 0.0154 days). We discussed the mechanism responsible for the changes in the orbital period of AR Boo. It is found that the long-term increase in its orbital period is caused by mass transfer from the less massive component to the more massive one, and the periodic variation in its orbital period might be a result of the light-travel-time effect owing to a third body or the magnetic activity in the more massive component of AR Boo.","Quan-wang Han and Li-fang Li and Xiao-yang Kong and Jian-sha Li and Deng-kai Jiang",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"tau: A 1D radiative transfer code for transmission spectroscopy of extrasolar planet atmospheres","The tau code is a 1D line-by-line radiative transfer code, which is generally applicable for modeling transmission spectra of close-in extrasolar planets. The inputs are the assumed temperature–pressure profile of the planetary atmosphere, the continuum absorption coefficients and the absorption cross-sections for the trace molecular absorbers present in the model, as well as the fundamental system parameters taken from the published literature. The program then calculates the optical path through the planetary atmosphere of the radiation from the host star, and quantifies the absorption due to the modeled composition in a transmission spectrum of transit depth as a function of wavelength. The code is written in C++, parallelized using OpenMP, and is available for public download and use from http://www.ucl.ac.uk/exoplanets/.
New version program summary
Program title: tau Catalogue identifier: AEPN_v1_1 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEPN_v1_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 56203 No. of bytes in distributed program, including test data, etc.: 1679104 Distribution format: tar.gz Programming language: C++. Computer: Non-specific. Operating system: Linux, Mac. Has the code been vectorized or parallelized?: Yes RAM: 300 MB Classification: 1.3. External routines: OpenMP (http://openmp.org/wp) Does the new version supersede the previous version?: Yes Nature of problem: Calculation of molecular absorption, and hence transit depths, as a function of wavelength for stellar radiation passing through planetary atmospheres. Solution method: Line-by-line calculation of wavelength-dependent optical depths using absorption cross-sections for various trace molecular absorbers. Reasons for new version: Bug found in previous version whereby some values, defined as floats, took values too small to be represented by this datatype, and so were set to zero. Summary of revisions: All ‘float’ datatypes changed to ‘double’. Additional comments: The distribution file contains, •./doc/: readme and User Guide.•./run/: sample input data files•./out/sample_tau_output.dat: sample output file for run mode ‘9’ (H2O at χ=10−5 for HD189733b at Tatm=1500K).Running time: From 0.5 to 500 s, depending on run parameters.","M.D.J. Hollis and M. Tessenyi and G. Tinetti",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mental disorder and criminality in Canada","This article examines the relationship between mental disorder and criminality in Canada from the colonial period to the landmark 1992 Mental Disorder Amendments that followed the passing of Bill C-30. The history of this relationship has been shaped by longstanding formal and informal systems of social regulation, by the contests of federal–provincial jurisdiction, by changing trends in the legal and psychiatric professions, and by amendments to the federal Criminal Code. A study of these longer-term features demonstrates that there has been no linear path of progress in Canada's response to mentally unwell offenders. Those caught in the web of crime and mental disorder have been cast and recast over the past 150years by the changing dynamics of criminal law, psychiatry, and politics. A long historical perspective suggests how earlier and more contemporary struggles over mental disorder and criminality are connected, how these struggles are bound by historical circumstance, and how a few relatively progressive historical moments emerging from these struggles might be recovered, and theorized to advantage.","James E. Moran",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Efficient interactive decision-making framework for robotic applications","The inclusion of robots in our society is imminent, such as service robots. Robots are now capable of reliably manipulating objects in our daily lives but only when combined with artificial intelligence (AI) techniques for planning and decision-making, which allow a machine to determine how a task can be completed successfully. To perform decision making, AI planning methods use a set of planning operators to code the state changes in the environment produced by a robotic action. Given a specific goal, the planner then searches for the best sequence of planning operators, i.e., the best plan that leads through the state space to satisfy the goal. In principle, planning operators can be hand-coded, but this is impractical for applications that involve many possible state transitions. An alternative is to learn them automatically from experience, which is most efficient when there is a human teacher. In this study, we propose a simple and efficient decision-making framework for this purpose. The robot executes its plan in a step-wise manner and any planning impasse produced by missing operators is resolved online by asking a human teacher for the next action to execute. Based on the observed state transitions, this approach rapidly generates the missing operators by evaluating the relevance of several cause–effect alternatives in parallel using a probability estimate, which compensates for the high uncertainty that is inherent when learning from a small number of samples. We evaluated the validity of our approach in simulated and real environments, where it was benchmarked against previous methods. Humans learn in the same incremental manner, so we consider that our approach may be a better alternative to existing learning paradigms, which require offline learning, a significant amount of previous knowledge, or a large number of samples.","Alejandro Agostini and Carme Torras and Florentin Wörgötter",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"All Clear: Improving the Code Process","Childbearing Poster Presentation
Purpose for the Program
The simulation facilitators at Baylor University Medical Center recognized the need to create best practice surrounding maternal codes; therefore, a new process for annual Mock Code was developed that used the concept of simulation‐based learning.
Proposed Change
For the past 5 years our facility used simulation‐based learning that focused on high‐risk obstetric events; however, we still performed Mock Codes on the unit in a didactic type format with skills check‐off. Following a maternal code on our unit, we incorporated simulation concepts with a hands‐on approach, which focused on documentation, communication, full use of the defibrillator, and crash cart knowledge.
Implementation, Outcomes, and Evaluation
Because maternal codes are rare, staff were not comfortable with the different aspects of the defibrillator/automated external defibrillator (AED). In addition, staff were not accustomed to using the code documentation sheet. The Perinatal Simulation Team developed an anaphylactic syndrome of pregnancy or amniotic fluid embolism (AFE) scenario and recorded it. Using this video, we were able to debrief our entire staff regarding the processes they viewed. Initially, we made them serve as the documenter and record what they observed as the video played. We then had them discuss and compare documentation to gain an understanding of the importance of speaking up to ensure all necessary information is gathered during a code. Then we reviewed the tape again, this time debriefing the scenario and addressing teamwork, communication, and technical skills. Following this discussion, we ran a simulation scenario that mimicked what they just watched. They had to perform CPR, place the pads from the defibrillator/AED and deliver shock as advised, print the EKG strip from the defibrillator, change from AED mode to defibrillator mode, increase joules as requested, draw up code medications, retrieve any necessary equipment or supplies from the crash cart, prepare for a STAT bedside delivery, document the code events, and communicate appropriately to the medical team. Our training increased staff confidence, knowledge of the crash cart and defibrillator, and awareness of communication needs during the maternal code.
Implications for Nursing Practice
When applying the science of simulation by creating a unique learning experience, nurses’ confidence level, knowledge, and skills in recognizing the signs and symptoms of AFE can be improved. The use of simulation also improves the nurses’ ability to use the defibrillator, draw up code medications, serve as documenter, and effectively use closed‐loop communication in a code situation.","Kristin Scheffer and Christine Renfro",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"AFMPB: An adaptive fast multipole Poisson–Boltzmann solver for calculating electrostatics in biomolecular systems","A Fortran program package is introduced for rapid evaluation of the electrostatic potentials and forces in biomolecular systems modeled by the linearized Poisson–Boltzmann equation. The numerical solver utilizes a well-conditioned boundary integral equation (BIE) formulation, a node-patch discretization scheme, a Krylov subspace iterative solver package with reverse communication protocols, and an adaptive new version of the fast multipole method in which the exponential expansions are used to diagonalize the multipole-to-local translations. The program and its full description, as well as several closely related libraries and utility tools are available at http://lsec.cc.ac.cn/~lubz/afmpb.html and a mirror site at http://mccammon.ucsd.edu/. This paper is a brief summary of the program: the algorithms, the implementation and the usage.
New version program summary
Program title: AFMPB Catalogue identifier: AEGB_v1_1 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEGB_v1_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 2 No. of lines in distributed program, including test data, etc.: 440784 No. of bytes in distributed program, including test data, etc.: 8187139 Distribution format: tar.gz Programming language: Fortran Computer: Any Operating system: Any RAM: Depends on the size of the discretized biomolecular system Classification: 3 External routines: Pre- and post-processing tools are required for generating the boundary elements and for visualization. Users can use MSMS (http://www.scripps.edu/sanner/html/msmshome.html) for pre-processing, and VMD (http://www.ks.uiuc.edu/Research/vmd/) for visualization. Sub-programs included: An iterative Krylov subspace solvers package from SPARSKIT by Yousef Saad (http://www-users.cs.umn.edu/saad/software/SPARSKIT/sparskit.html), and the fast multipole methods subroutines from FMMSuite (http://www.fastmultipole.org/). Catalogue identifier of previous version: AEGB_v1_0 Journal reference of previous version: Comput. Phys. Comm. 181 (2010) 1150 Does the new version supersede the previous version?: Yes Nature of problem: Numerical solution of the linearized Poisson–Boltzmann equation that describes electrostatic interactions of molecular systems in ionic solutions. Solution method: A novel node-patch scheme is used to discretize the well-conditioned boundary integral equation formulation of the linearized Poisson–Boltzmann equation. Various Krylov subspace solvers can be subsequently applied to solve the resulting linear system, with a bounded number of iterations independent of the number of discretized unknowns. The matrix–vector multiplication at each iteration is accelerated by the adaptive new versions of fast multipole methods. The AFMPB solver requires other stand-alone pre-processing tools for boundary mesh generation, post-processing tools for data analysis and visualization, and can be conveniently coupled with different time stepping methods for dynamics simulation. Reasons for new version: Some bugs are fixed in the new version. Summary of revisions:•The type definition of ippt1 in line 88 of FBEM/bempb.f and line 32 of FBEM/closecoef.f is changed from real *8 to integer*4, and a similar change is made for ippt in line 105 of FBEM/solvpb.f and in line 32 of FBEM/closecoef.f.•In FBEM/elmgeom.f, line 239 “ELSEIF (meshfmt.EQ.1.OR. meshfmt.EQ. 4.OR. meshfmt.EQ.5) THEN” is changed to “ELSEIF (meshfmt.EQ.1.OR. meshfmt.EQ. 4) THEN”, line 478 “KJ=IDFCL(K)+J-1” is changed to “i=IDFCL(K)+J-1”, line 479 “KJ=NE(KJ)” is changed to “KJ=NE(i)”, line 480 “KJ1=NE(KJ+1)” is changed to “KJ1=NE(i+1)”, and line 647 “     STOP” is changed to “c     STOP”.•Five subroutines in FMM part (syukadap.f, syukdn.f, slapadap.f, slapdn.f, and treeadap.f) are substituted with the new ones in the new version.Restrictions: Only three or six significant digits options are provided in this version. Unusual features: Most of the codes are in Fortran77 style. Memory allocation functions from Fortran90 and above are used in a few subroutines. Additional comments: The current version of the codes is designed and written for single core/processor desktop machines. Check http://lsec.cc.ac.cn/lubz/afmpb.html for updates and changes. Running time: The running time varies with the number of discretized elements (N) in the system and their distributions. In most cases, it scales linearly as a function of N.","Benzhuo Lu and Xiaolin Cheng and Jingfang Huang and J. Andrew McCammon",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Active inference, communication and hermeneutics","Hermeneutics refers to interpretation and translation of text (typically ancient scriptures) but also applies to verbal and non-verbal communication. In a psychological setting it nicely frames the problem of inferring the intended content of a communication. In this paper, we offer a solution to the problem of neural hermeneutics based upon active inference. In active inference, action fulfils predictions about how we will behave (e.g., predicting we will speak). Crucially, these predictions can be used to predict both self and others – during speaking and listening respectively. Active inference mandates the suppression of prediction errors by updating an internal model that generates predictions – both at fast timescales (through perceptual inference) and slower timescales (through perceptual learning). If two agents adopt the same model, then – in principle – they can predict each other and minimise their mutual prediction errors. Heuristically, this ensures they are singing from the same hymn sheet. This paper builds upon recent work on active inference and communication to illustrate perceptual learning using simulated birdsongs. Our focus here is the neural hermeneutics implicit in learning, where communication facilitates long-term changes in generative models that are trying to predict each other. In other words, communication induces perceptual learning and enables others to (literally) change our minds and vice versa.","Karl J. Friston and Christopher D. Frith",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Resection Arthroplasty Compared With Total Hip Arthroplasty in Treating Chronic Hip Pain of Patients With a History of Substance Abuse","Background
Retrospective comparison of surgical management of severe hip pain in patients with a history of substance abuse treated by modified Girdlestone resection arthroplasty (RA) vs delayed total hip arthroplasty (THA) following yearlong sobriety pathway.
Methods
Patients were identified using charts, current procedural terminology (CPT) code query, and THA sobriety pathway registry. The primary outcome was adequate pain control following surgery, defined as visual analog scale ≤ 5 or verbal description of “moderate” or lower pain. RA patients with infectious arthritis were analyzed separately. The secondary outcome was the level of mobility after surgery.
Results
In the THA pathway, 15 of 28 (53.6%) proved sobriety, 11 (39.3%) underwent THA, and 9 (32.1%) achieved adequate pain control (median 77 days). After RA, 19 (76%) achieved adequate pain control (median 119.5 days). Preoperative infection did not significantly affect time to pain control after RA (P = .94). Time to adequate pain control was not significantly different between RA and THA patients (P = .19). Three patients (30%) experienced improved level of mobility after THA and 7 (70%) experienced no change. After RA, 7 patients (29.1%) experienced improved level of mobility, 3 (13.6%) lost mobility, and 14 (63.6%) experienced no change. Three RA patients were later converted to THA without complication.
Conclusion
Yearlong sobriety pathway leading to THA leads to successful pain control in less than one-third of enrolled patients. Compared to delayed THA, RA enables more patients with substance abuse to be treated sooner and results in successful reduction of pain in a similar proportion of patients. RA may be an effective pain-reducing procedure for these patients.","William Curtis and Meir Marmor",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Operating System from the Scratch: A Problem-based Learning Approach for the Emerging Demands on OS Development","In recent past history of computer systems industry, for decades, the hegemony of a few de facto standards dictated by major proprietary commercial products dominated the Operating Systems (OS) field. In such technological context, conso- nantly to this trend, the knowledge objective focused by academical and training courses on OS-related disciplines has often been addressed more from the stand point of essential theoretical background than of the technical skills for actuation on de- sign and development field. Emerging paradigms, nevertheless, have been rapidly changing this scenario. Among them, the establishment of Open Source concept is boosting the growing diversity of new operating systems; concomitantly, evolution of embedded hardware architectures has make it possible to run sophisticated operating systems where only bare rudimentary, ad hoc system-software were once practical. Aligned along this perspective, this paper introduces a new platform for teaching and training programs on OS development founded on a project-based approach which guides the student throughout the process of designing and programming a sufficiently simple, but yet realistic and fully functional, OS from the scratch. The differential of the present proposal regarding related works is that, instead of either merely inspecting example-code or experimenting with simulators, the apprentice is guided across the challenge of coding an entire new instance of a didactic system specification. A comparison of the companion OS-example with existing alternatives brings out a less complex implementation structure which maps conceptual modules with implementation blocks in an intuitive correspondence and with reduced function cou- pling. Moreover, the learning platform comes with a courseware material consistently linked to the laboratory practices, and aimed at the systemic comprehension of the many related multidisciplinary aspects.","Renê S. Pinto and Pedro Nobile and Edwin Mamani and Lourenço P. Júnior and Helder J.F. Luz and Francisco J. Monaco",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Clinical Utility of Screening Laboratory Tests in Pediatric Psychiatric Patients Presenting to the Emergency Department for Medical Clearance","Study objective
We assess whether screening laboratory tests obtained to medically clear pediatric psychiatric patients altered management or disposition.
Methods
This was a retrospective chart review of consecutive patients younger than 18 years and presenting to an academic pediatric emergency department for medical clearance of an acute psychiatric emergency potentially requiring an involuntary hold (danger to self, danger to others, grave disability) from July 2009 to December 2010. Patients were identified by discharge diagnosis codes. History and physical examination and screening laboratory tests were reviewed for changes in management or disposition. Further analysis compared length of stay according to type of laboratory test performed. To avoid missing patients presenting with or for evaluation of an involuntary hold for whom an organic cause was diagnosed, charts with psychiatric chief complaints were reviewed for the same period.
Results
One thousand eighty-two visits resulting in 13,725 individual laboratory tests were analyzed. Of 871 visits with laboratory tests performed, abnormal laboratory tests were associated with 7 disposition changes (0.8%) and 50 management changes (5.7%) not associated with a disposition change. Twenty-five patients with noncontributory history and physical examination results had management changes, all non-urgent. One patient with a noncontributory history and physical examination result had a disposition-changing laboratory result, a positive urine pregnancy test. Patients who had any screening test performed had a longer length of stay than patients without testing (117 minutes longer; 95% confidence interval 109.7 to 124.4 minutes). In charts reviewed according to chief complaint, no patient was found to have an organic cause of their symptoms according to only screening tests.
Conclusion
Screening laboratory tests resulted in few management and disposition changes in patients with noncontributory history and physical examination results but were associated with increased length of stay.","J. Joelle Donofrio and Genevieve Santillanes and Bradley D. McCammack and Chun Nok Lam and Michael D. Menchine and Amy H. Kaji and Ilene A. Claudius",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Safe uniform proxies for Java","The proxy abstraction has a long-lasting tradition in object-oriented programming. From design patterns to inherent programming language support, and from remote method invocations to simple forms of behavioral reflection, incarnations as well as applications of proxies are innumerable. Since version 1.3, Java has supported the concept of a dynamic proxy. Such an object conforms to a set of types specified by the program and can be used wherever an expression of any of these types is expected, yet it reifies invocations performed on it. This ability has allowed dynamic proxies to be used to implement paradigms such as behavioral reflection, structural conformance, or multi-methods. Alas, these proxies are only available “for interfaces”. The case of creating dynamic proxies for a set of types including a class has not been addressed, meaning that it is currently not possible to create a dynamic proxy that conforms to an application-defined class type. This weakness strongly limits any application of dynamic proxies beyond the inherent limitations of proxies, which have motivated deeper programming language support for features such as behavioral reflection. In this paper, we unfold the current support for dynamic proxies in Java, assessing it in the light of a set of generic criteria for proxy implementations. We present an approach to supporting dynamic proxies “for classes”, consisting in transformations performed on classes at load-time, including a generic scheme for enforcing encapsulation upon field accesses. These transformations seamlessly extend the scope of the current support for dynamic proxies from the programmer’s perspective. We argue for the safety of our transformations, and discuss the precise benefits and costs of our extension in terms of the criteria introduced through an implementation of future method invocations balancing safety and transparency.","Patrick Eugster",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Development of time-dependent reaction rates to optimise predictor–corrector algorithm in ALEPH burn-up code","Shells coupling Monte-Carlo transport and deterministic depletion codes are extensively used in the nuclear field to simulate material changes throughout irradiation. The dynamic behaviour of the phenomenon is described by the system of coupled ordinary differential equations, with generally a stiff matrix of coefficients that current codes keep constant in time along each burn-up interval. The matrix coefficients represent decay constants and microscopic reaction rates of the numerous nuclides involved in the calculations. For a typical burn-up problem, their determination consumes most of the required computational time while only a small fraction is spent by the depletion solver. Predictor–corrector methods have been implemented to guarantee more accurate results, but not much has been done to overcome the running time issue. This work presents a unique and innovative feature of the ALEPH Monte-Carlo burn-up code which optimises the depletion algorithm by using time-dependent matrix coefficients. Linear polynomials interpolate the evolution of the matrix coefficients along a few consecutive time steps. Then, trend curves are constructed and used to extrapolate the effective reaction rates in the following intervals, thus reducing the total required computational time spent in the neutronic calculations. This technique has been implemented in the version 2 of the ALEPH Monte-Carlo burn-up code and validated against the REBUS experimental benchmark. The results revealed a considerable computational time saving without any drawbacks in the accuracy.","L. Fiorito and A. Stankovskiy and G. Van den Eynde and P.E. Labeau",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Fuel performance simulation of iron-chrome-aluminum (FeCrAl) cladding during steady-state LWR operation","Alternative cladding materials have been proposed to replace the currently used zirconium (Zr)-based alloys, in order to improve the accident tolerance of light water reactor (LWR) fuel. Of these materials, there is a particular focus on iron-chromium-aluminum (FeCrAl) alloys that exhibit much slower oxidation kinetics in high-temperature steam than Zr-alloys. This behavior should decrease the energy release due to oxidation and allow the cladding to remain integral longer in the presence of high temperature steam, making accident mitigation more likely. Within the development of these alloys, suitability for normal operation must also be demonstrated. This article is focused on modeling the integral thermo-mechanical performance of FeCrAl clad UO2 fuel during normal reactor operation. Finite element analysis has been performed to assess commercially available FeCrAl alloys (namely Alkrothal 720 and APMT) as a candidate fuel cladding replacement for Zr-alloys, using the MOOSE-based fuel performance code BISON. These simulations identify the effects of the mechanical-stress and irradiation responses of FeCrAl and provide a comparison with Zr-alloys. In comparing these cladding materials, fuel rods have been simulated for normal reactor operation and simple steady-state operation. Normal reactor operating conditions target the cladding performance over the rod lifetime (∼4 cycles) for the highest-power rod in the highest-power fuel assembly under reactor power maneuvering. These power histories and axial temperature profiles input into BISON were generated from a neutronics study on full-core reactivity equivalence for FeCrAl using the 3D full core simulator NESTLE. The fuel rod designs and operating conditions used here are based on the Peach Bottom BWR with representative GE-12/14 fuel geometries, and design consideration was given to minimize the neutronic penalty of the FeCrAl cladding by changing fuel enrichment and cladding thickness. Individual sensitivity analyses of the fuel and cladding creep responses were also performed, which indicated the influence of compliance for each material, separately, on the stress state of the fuel cladding. These parametric analyses are performed using steady-state operating conditions such as a simple axial power profile, a constant cladding surface temperature, and a constant fuel power history.","R.T. Sweet and N.M. George and G.I. Maldonado and K.A. Terrani and B.D. Wirth",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Functional coding variation in the presynaptic dopamine transporter associated with neuropsychiatric disorders drives enhanced motivation and context-dependent impulsivity in mice","Recent genetic analyses have provided evidence that clinical commonalities associated with different psychiatric diagnoses often have shared mechanistic underpinnings. The development of animal models expressing functional genetic variation attributed to multiple disorders offers a salient opportunity to capture molecular, circuit and behavioral alterations underlying this hypothesis. In keeping with studies suggesting dopaminergic contributions to attention-deficit hyperactivity disorder (ADHD), bipolar disorder (BPD) and autism spectrum disorder (ASD), subjects with these diagnoses have been found to express a rare, functional coding substitution in the dopamine (DA) transporter (DAT), Ala559Val. We developed DAT Val559 knock-in mice as a construct valid model of dopaminergic alterations that drive multiple clinical phenotypes, and here evaluate the impact of lifelong expression of the variant on impulsivity and motivation utilizing the 5- choice serial reaction time task (5-CSRTT) and Go/NoGo as well as tests of time estimation (peak interval analysis), reward salience (sucrose preference), and motivation (progressive ratio test). Our findings indicate that the DAT Val559 variant induces impulsivity behaviors that are dependent upon the reward context, with increased impulsive action observed when mice are required to delay responding for a reward, whereas mice are able to withhold responding if there is a probability of reward for a correct rejection. Utilizing peak interval and progressive ratio tests, we provide evidence that impulsivity is likely driven by an enhanced motivational phenotype that also may drive faster task acquisition in operant tasks. These data provide critical validation that DAT, and more generally, DA signaling perturbations can drive impulsivity that can manifest in specific contexts and not others, and may rely on motivational alterations, which may also drive increased maladaptive reward seeking.","Gwynne L. Davis and Adele Stewart and Gregg D. Stanwood and Raajaram Gowrishankar and Maureen K. Hahn and Randy D. Blakely",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"‘If no-one stops me, I'll make the mistake again’: Changing prescribing behaviours through feedback; A Perceptual Control Theory perspective’","Background
Doctors at all levels make prescribing errors which can prolong patients' hospital stay, increase the risk of death, and place a significant financial burden on the health system. Doctors have previously reported receiving little or no feedback on their prescribing errors. The effectiveness of feedback in modifying future practice varies widely, depending on how feedback is delivered. To date there is little evidence about why and how feedback interventions do or do not work. Behavioural theories can be used to evaluate this process and provide explanatory accounts to inform recommendations for future interventions.
Objective
To explore the experiences of prescribers receiving different methods of feedback about their prescribing errors. Perceptual Control Theory (PCT) was used as a theoretical framework to explain which aspects of feedback were most likely to influence prescribing behaviour.
Methods
A secondary analysis of 31 semi-structured qualitative interviews with junior doctors who had taken part one of three studies in which they received feedback on their prescribing errors. A hybrid approach to analysis involved inductive thematic analysis, and deductive a priori template of codes using PCT as a framework to guide data analysis and interpretation.
Results
Feedback was most useful for learning and most likely to influence future prescribing behaviour when it was timely, and provided a comprehensive, contextualised benchmark to which participants could compare their prescribing behaviours and current level of knowledge. Group discussions and completing directly-observed prescribing event forms were thought most likely to impact future prescribing; email feedback alone was perceived as least effective in changing prescribing behaviour.
Conclusion
Feedback has the potential to change future prescribing behaviour. Behaviour change can only take place if prescribers are made aware of these discrepancies, either via providing appropriate reference values or benchmarks before mistakes are made, or by providing timely and comprehensive feedback after mistakes are made.","Jane Ferguson and Chris Keyworth and Mary P. Tully",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Understanding policy persistence—The case of police drug detection dog policy in NSW, Australia","Background
Significant research attention has been given to understanding the processes of drug policy reform. However, there has been surprisingly little analysis of the persistence of policy in the face of opposition and evidence of ineffectiveness. In this article we analysed just such a case – police drug detection dog policy in NSW, Australia. We sought to identify factors which may account for the continuation of this policy, in spite of counter-evidence and concerted advocacy.
Methods
The analysis was conducted using the Advocacy Coalition Framework (ACF). We collated documents relating to NSW drug detection dog policy from 1995 to 2016, including parliamentary records (NSW Parliament Hansard), government and institutional reports, legislation, police procedures, books, media, and academic publications. Texts were then read, coded and classified against the core dimensions of the ACF, including subsystem actors and coalitions, their belief systems and resources and venues employed for policy debate.
Results
Three coalitions were identified as competing in the policy subsystem: security/law and order, civil liberties and harm reduction. Factors that aided policy stability were the continued dominance of the security/law and order coalition since they introduced the drug dog policy; a power imbalance enabling the ruling coalition to limit when and where the policy was discussed; and a highly adversarial policy subsystem. In this context even technical knowledge that dogs infringed civil liberties and increased risks of overdose were readily downplayed, leading to only incremental changes in implementation rather than policy cessation or wholesale revision.
Conclusion
The analysis provides new insights into why the accumulation of new evidence and advocacy efforts can be insufficient to drive significant policy change. It poses a challenge for the evidence-based paradigm suggesting that in highly adversarial policy subsystems new evidence is unlikely to generate policy change without broader subsystem change, such as reducing the adversarial nature and/or providing new avenues for cross-coalition learning.","Caitlin E. Hughes and Alison Ritter and Kari Lancaster and Robert Hoppe",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Isolation, characterization and functional analysis of full length p53 cDNA from Bubalus bubalis","p53 plays a pivotal role in maintaining the genomic integrity of the cell and has an important role in cellular transformation. We isolated and cloned a full length p53 cDNA (Bp53) from water buffalo in expression vectors designed to generate tagged proteins with FLAG or GFP. Bp53 was found to be 1161 nucleotide long and codes for 386 amino acid residues with 79% homology with human p53 containing 393 amino acids. Although Bp53 has some inherent differences in amino acid composition in different functional domains as compared to human p53 but the total electrostatic charge of amino acids has been maintained. Bp53 cDNA was transiently transfected in a p53 null human NSCLC cell line and as expected, it was predominantly localized in the nucleus. Besides, Bp53 effectively transactivates a number of target genes similar to human p53 and exerts most of its anti-tumorigenic potential in culture as observed in clonogenic and cell viability assays. Like human p53 mutants, core domain mutant version of Bp53 was found to be mis-localized to cytoplasm with diminished tumor suppressor activity. However, Bp53 appeared to be more sensitive to mdm2 mediated degradation and as a result, this protein was less stable as compared to human p53. For the first time we have characterized a functionally efficient wild-type p53 from buffalo having lower stability than human p53 and thus, buffalo p53 could be used as a model system for further insight to the molecular basis of wild-type p53 instability.","Minu Singh and Suruchi Aggarwal and Ashok K. Mohanty and Tapas Mukhopadhyay",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"How do types of interaction and phases of self-regulated learning set a stage for collaborative engagement?","This study investigates how self-regulated learning phases are related to collaborative engagement in two different collaborative task conditions. It integrates SRL theory and the concept of engagement, including interaction in collaboration, as key characteristics of engagement. Forty-four second-year teacher education students worked in groups during a 7-week math didactic course. We collected 84 h of video recordings and coded the group's cognitive and socioemotional interaction and three phases of self-regulation within interaction, including forethought, performance and reflection. After that we analyzed the relationship between the interaction types representing collaborative engagement and SRL phases within two learning tasks. The results show that collaborative engagement did not differ between teacher-led and student-led tasks in terms of the interaction types. However, the results showed that the SRL phases occurred differently within cognitive and socioemotional interaction types when the two task conditions were compared. Findings concerning teacher-led tasks showed invariance in the occurrence of SRL phases across the task and highlighted the relationship between socioemotional interaction and the forethought phase. Additionally, findings concerning the student-led tasks showed systematic changes in the distribution of phases of SRL across sessions in all interaction types. Our results' theoretical and methodological implications for collaborative engagement research are discussed.","Sanna Järvelä and Hanna Järvenoja and Jonna Malmberg and Jaana Isohätälä and Márta Sobocinski",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"BilKristal 2.0: A tool for pattern information extraction from crystal structures","We present a revised version of the BilKristal tool of Okuyan et al. (2007). We converted the development environment into Microsoft Visual Studio 2005 in order to resolve compatibility issues. We added multi-core CPU support and improvements are made to graphics functions in order to improve performance. Discovered bugs are fixed and exporting functionality to a material visualization tool is added.
New version program summary
Program title: BilKristal 2.0. Catalogue identifier: ADYU_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ADYU_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 364263 No. of bytes in distributed program, including test data, etc.: 9135815 Distribution format: tar.gz Programming language: C, C++, Microsoft.NET Framework 2.0 and OpenGL Libraries. Computer: Personal computer with Windows operating system. Operating system: Windows XP or higher. Has the code been vectorized or parallelized?: Multi-core CPU support included. RAM: 20–60 Megabytes. Catalogue identifier of previous version: ADYU_v1_0 Journal reference of previous version: Comput. Phys. Comm. 176 (2007) 486 Classification: 8. External routines: Microsoft.NET Framework 2.0. For the visualization tool, the graphics card driver should also support OpenGL. Does the new version supercede the previous version?: Yes Nature of problem: Determining the crystal structure parameters of a material is a very important issue in crystallography. Knowing the crystal structure parameters helps the understanding of the physical behavior of a material. For complex structures, particularly for materials which also contain local symmetry as well as global symmetry, obtaining crystal parameters can be very hard. Solution method: The tool extracts crystal parameters such as primitive vectors and basis vectors and identifies the space group from the atomic coordinates of crystal structures. Reasons for new version: Additional features, resolved compatibility issues with the new development environments, performance optimizations, minor bug corrections. Summary of revisions:•Capability to export to MaterialVis tool [1] is added. The tool can export the unit cell information extracted from the crystal structure, the raw atomic coordinates and atomic radii into a data file (.dat) that the MaterialVis tool can process.•Compatibility issues with Microsoft Visual Studio 2005 up to 2010 are resolved. The original code was developed using Microsoft Visual Studio 2003. However, newer Visual Studio versions were not able to convert and compile the code. Due to the changes in the .NET framework, the converted project produced many errors. In this work, the project is converted into a Visual Studio 2005 project and compilation errors are resolved. We also tested the code with Visual Studio 2010 and the project was successfully converted and compiled.•Multi-Core CPU support is added. In recent years, multi-core CPUs have become very common. We added the multi-core CPU support in order to utilize the computational capabilities of additional CPU cores. This significantly improves the performance.•The visualization interface is improved. In particular, the sphere drawing functionality is replaced with an efficient and high quality version that utilizes GPU acceleration.•For some cases, the fractional coordinates of some of the calculated basis vectors were not all in the [0,1) range, but at coordinate 1.0 for some axes. These cases were corrected by translating these basis vectors into the [0,1) range.Restrictions: Assumptions are explained in [2]. However, none of them can be considered as a restriction on the complexity of the problem. Running time: The tool was able to process input files with more than a million atoms in less than 20 s on a PC with an Athlon quad-core CPU at 3.2 GHz using the default parameter values. References: [1] Erhan Okuyan, Ugur Güdükbay, MaterialVis: Crystal and Amorphous Material Visualization Tool Using Direct Volume and Surface Rendering Techniques (Program Summary), Computer Physics Communications, Submitted. [2] Erhan Okuyan, Ugur Güdükbay, and Oguz Gülseren, Pattern Information Extraction from Crystal Structures, Computer Physics Communications, 176 (2007) 486.","Erhan Okuyan and Uğur Güdükbay",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"SPIRITuS: a SimPle Information Retrieval regressIon Test Selection approach","Context:Regression Test case Selection (RTS) approaches aim at selecting only those test cases of a test suite that exercise changed parts of the System Under Test (SUT) or parts affected by changes. Objective:We present SPIRITuS (SimPle Information Retrieval regressIon Test Selection approach). It uses method code coverage information and a Vector Space Model to select test cases to be run. In a nutshell, the extent of a lexical modification to a method is used to decide if a test case has to be selected. The main design goals of SPIRITuS are to be: (i) easy to adapt to different programming languages and (ii) tunable via an easy to understand threshold. Method:To assess SPIRITuS, we conducted a large experiment on 389 faulty versions of 14 open-source programs implemented in Java. We were mainly interested in investigating the tradeoff between the number of selected test cases from the original test suite and fault detection effectiveness. We also compared SPIRITuS against well-known RTS approaches. Results:SPIRITuS selects a number of test cases significantly smaller than the number of test cases the other approaches select at the price of a slight reduction in fault detection capability. Conclusions:SPIRITuS can be considered a viable competitor of existing test case selection approaches especially when the average number of test cases covering a modified method increases (such information can be easily derived before test case selection takes place).","Simone Romano and Giuseppe Scanniello and Giuliano Antoniol and Alessandro Marchetto",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Balancing care and teaching during clinical activities: 2 contexts, 2 strategies","Purpose
The goal of this study was to better understand how clinical supervisors integrate teaching interactions with medical trainees into 2 types of clinical activities in the critical care setting: multidisciplinary rounds and medical crises.
Methods
We conducted a qualitative, observational study based on an ethnographic approach. We observed the teaching interactions among clinical supervisors and medical trainees during 12 multidisciplinary rounds and 74 medical crises in 2 academic hospitals. Grounded theory methods (theoretical sampling and saturation, inductive thematic coding, and constant comparison) were used to analyze data.
Results
Two models of integration of teaching interactions into clinical activities are described: the in series model, typical of multidisciplinary rounds and characterized by well-structured learning bubbles uninterrupted by patient care, and the in parallel model, common during medical crises and involving multiple, short learning flashes intricately related to and frequently interrupted by patient care. By adopting a model over the other, supervisors appeared to adapt to 2 contexts that differed in terms of priority, supervisor's understanding of events, and social context of interactions. Each model presented complementary opportunities and limitations for learning.
Conclusions
Modern views of medical apprenticeship and clinical teaching need to take into account the specific clinical context in which learning occurs. Teaching interactions that differ in structure and content in response to changing clinical circumstances could impact learning in unique ways. Learning outcomes resulting from different models of integration of teaching into clinical activities need to be further explored.","Dominique Piquette and Carol-Anne Moulton and Vicki R. LeBlanc",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A Scalable Population Code for Time in the Striatum","Summary
To guide behavior and learn from its consequences, the brain must represent time over many scales. Yet, the neural signals used to encode time in the seconds-to-minute range are not known. The striatum is a major input area of the basal ganglia associated with learning and motor function. Previous studies have also shown that the striatum is necessary for normal timing behavior. To address how striatal signals might be involved in timing, we recorded from striatal neurons in rats performing an interval timing task. We found that neurons fired at delays spanning tens of seconds and that this pattern of responding reflected the interaction between time and the animals’ ongoing sensorimotor state. Surprisingly, cells rescaled responses in time when intervals changed, indicating that striatal populations encoded relative time. Moreover, time estimates decoded from activity predicted timing behavior as animals adjusted to new intervals, and disrupting striatal function led to a decrease in timing performance. These results suggest that striatal activity forms a scalable population code for time, providing timing signals that animals use to guide their actions.","Gustavo B.M. Mello and Sofia Soares and Joseph J. Paton",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Altered hippocampal information coding and network synchrony in APP-PS1 mice","β-amyloid is hypothesized to harm neural function and cognitive abilities by perturbing synaptic transmission and plasticity in Alzheimer's disease (AD). To assess the impact of this pathology on hippocampal neurons' ability to encode flexibly environmental information across learning, we performed electrophysiological recordings of CA1 hippocampal unit activity in AD transgenic mice as they acquired an action-reward association in a spatially defined environment; the behavioral task enabled the precise timing of discrete and intentional behaviors of the animal. We found that the proportion of behavioral task-sensitive cells in wild-type (WT) mice typically increased, whereas the proportion of place cells decreased with learning. In AD mice, this learning-dependent change of cell-discharge patterns was absent, and cells exhibited similar firings from the beginning to firings attained at the late learning stage in wild-type cells. These inflexible hippocampal representations of task and space throughout learning are accompanied by remarkable alterations of local oscillatory activity in the theta and ultra-fast ripple frequencies as well as learning abilities. The present data offer new insights into the in vivo cellular and network processes by which β-amyloid and other AD mutations may exert its harmful effects to produce cognitive and behavioral impairments in early stage of AD.","Sebastien Cayzac and Nicole Mons and Antonin Ginguay and Bernadette Allinquant and Yannick Jeantet and Yoon H. Cho",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Parallel flow routing in SWMM 5","The hydrodynamic rainfall-runoff and urban drainage simulation model SWMM (Storm Water Management Model) is a state of the art software tool applied likewise in research and practice. In order to reduce the computational burden of long simulation runs and to use the extra power of modern multi-core computers, a parallel version of SWMM is presented herein. The challenge has been to modify the software in such minimal way that the resulting code enhancement may find its way into the commercial and non-commercial software tools that depend on SWMM for its calculation engine. A pragmatic approach to identify and enhance only the critical parts of the software in terms of run-time was chosen in order to keep the code changes as low as possible. The enhanced software was first tested for coherence against the original code and then benchmarked on four different input scenarios ranging from a very small village to a medium sized urban area. For the investigated sewer systems a speedup of six to ten times on a twelve core system was realized, thus decreasing the execution time to an acceptable level even for tedious system analysis.","G. Burger and R. Sitzenfrei and M. Kleidorfer and W. Rauch",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Critic2: A program for real-space analysis of quantum chemical interactions in solids","We present critic2, a program for the analysis of quantum-mechanical atomic and molecular interactions in periodic solids. This code, a greatly improved version of the previous critic program (Otero-de-la Roza et al., 2009), can: (i) find critical points of the electron density and related scalar fields such as the electron localization function (ELF), Laplacian, … (ii) integrate atomic properties in the framework of Bader’s Atoms-in-Molecules theory (QTAIM), (iii) visualize non-covalent interactions in crystals using the non-covalent interactions (NCI) index, (iv) generate relevant graphical representations including lines, planes, gradient paths, contour plots, atomic basins, … and (v) perform transformations between file formats describing scalar fields and crystal structures. Critic2 can interface with the output produced by a variety of electronic structure programs including WIEN2k, elk, PI, abinit, Quantum ESPRESSO, VASP, Gaussian, and, in general, any other code capable of writing the scalar field under study to a three-dimensional grid. Critic2 is parallelized, completely documented (including illustrative test cases) and publicly available under the GNU General Public License.
Program summary
Program title: CRITIC2 Catalogue identifier: AECB_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AECB_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: yes No. of lines in distributed program, including test data, etc.: 11686949 No. of bytes in distributed program, including test data, etc.: 337020731 Distribution format: tar.gz Programming language: Fortran 77 and 90. Computer: Workstations. Operating system: Unix, GNU/Linux. Has the code been vectorized or parallelized?: Shared-memory parallelization can be used for most tasks. Classification: 7.3. Catalogue identifier of previous version: AECB_v1_0 Journal reference of previous version: Comput. Phys. Comm. 180 (2009) 157 Nature of problem: Analysis of quantum-chemical interactions in periodic solids by means of atoms-in-molecules and related formalisms. Solution method: Critical point search using Newton’s algorithm, atomic basin integration using bisection, qtree and grid-based algorithms, diverse graphical representations and computation of the non-covalent interactions index on a three-dimensional grid. Additional comments: !!!!! The distribution file for this program is over 330 Mbytes and therefore is not delivered directly when download or Email is requested. Instead a html file giving details of how the program can be obtained is sent. !!!!! Running time: Variable, depending on the crystal and the source of the underlying scalar field.","A. Otero-de-la-Roza and Erin R. Johnson and Víctor Luaña",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Tuning for software analytics: Is it really necessary?","Context: Data miners have been widely used in software engineering to, say, generate defect predictors from static code measures. Such static code defect predictors perform well compared to manual methods, and they are easy to use and useful to use. But one of the “black arts” of data mining is setting the tunings that control the miner. Objective: We seek simple, automatic, and very effective method for finding those tunings. Method: For each experiment with different data sets (from open source JAVA systems), we ran differential evolution as an optimizer to explore the tuning space (as a first step) then tested the tunings using hold-out data. Results: Contrary to our prior expectations, we found these tunings were remarkably simple: it only required tens, not thousands, of attempts to obtain very good results. For example, when learning software defect predictors, this method can quickly find tunings that alter detection precision from 0% to 60%. Conclusion: Since (1) the improvements are so large, and (2) the tuning is so simple, we need to change standard methods in software analytics. At least for defect prediction, it is no longer enough to just run a data miner and present the result without conducting a tuning optimization study. The implication for other kinds of analytics is now an open and pressing issue.","Wei Fu and Tim Menzies and Xipeng Shen",2016,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Work hardening during alternating load directions of 316L SS","Understanding and modelling the plastic behavior of a material are essential for simulation and design of metal forming processes. Cold pilgering of tubes is a process with very complex strain history with alternating loading direction. This makes evaluation of the work hardening challenging. Cold deformation applied in a single direction predominantly exhibit work hardening, while changes of the loading direction may even cause softening in other directions. The influence of alternating loading directions on work hardening has been experimentally investigated for 316L stainless steel (SS). Cubic specimens were cut out from the preform of the tube. The specimens are subjected to uniaxial compressions in alternating directions along two perpendicular axes. From the results, a cyclic elastic-plastic constitutive model based on a Chaboche-type approach is calibrated and implemented in the commercial finite element code MSC.Marc.","Yağız Azizoğlu and Mattias Gärdsback and Akinori Yamanaka and Toshihiko Kuwabara and Lars-Erik Lindgren",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Using guitar learning to probe the Action Observation Network's response to visuomotor familiarity","Watching other people move elicits engagement of a collection of sensorimotor brain regions collectively termed the Action Observation Network (AON). An extensive literature documents more robust AON responses when observing or executing familiar compared to unfamiliar actions, as well as a positive correlation between amplitude of AON response and an observer's familiarity with an observed or executed movement. On the other hand, emerging evidence shows patterns of AON activity counter to these findings, whereby in some circumstances, unfamiliar actions lead to greater AON engagement than familiar actions. In an attempt to reconcile these conflicting findings, some have proposed that the relationship between AON response amplitude and action familiarity is nonlinear in nature. In the present study, we used an elaborate guitar training intervention to probe the relationship between movement familiarity and AON engagement during action execution and action observation tasks. Participants underwent fMRI scanning while executing one set of guitar sequences with a scanner-compatible bass guitar and observing a second set of sequences. Participants then acquired further physical practice or observational experience with half of these stimuli outside the scanner across 3 days. Participants then returned for an identical scanning session, wherein they executed and observed equal numbers of familiar (trained) and unfamiliar (untrained) guitar sequences. Via region of interest analyses, we extracted activity within AON regions engaged during both scanning sessions, and then fit linear, quadratic and cubic regression models to these data. The data best support the cubic regression models, suggesting that the response profile within key sensorimotor brain regions associated with the AON respond to action familiarity in a nonlinear manner. Moreover, by probing the subjective nature of the prediction error signal, we show results consistent with a predictive coding account of AON engagement during action observation and execution that also takes into account effects of changes in neural efficiency.","Tom Gardner and Aidas Aglinskas and Emily S. Cross",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Multiple “Lower BAC” offenders: Characteristics and response to remedial interventions","Background
In recent years, there has been increasing attention to “lower BAC” drinking drivers, typically those whose blood alcohol content (BAC) is under the legal limits defined in criminal law. In 2009, legislation was enacted in Ontario, Canada that enabled police to issue roadside license suspensions to individuals caught driving with BAC between 0.05% and 0.08%, known as the “warn range”. Multiple warn range (MWR) offenders are required to attend the Back on Track (BOT) remedial measures program. This study aimed to provide: (1) a preliminary characterization of MWR drivers charged under warn range legislation; and (2) an initial assessment of outcomes associated with BOT participation among MWR offenders.
Methods
A subsample of 727 MWR offenders was drawn from program records, and compared to samples of 3597 first-time Criminal Code (CC) offenders (those caught driving with a BAC of 0.08% or higher) and 359 second-time CC offenders. To provide an initial assessment of outcomes associated with BOT participation, another subsample consisted of 394 MWR participants from whom pre- and post-workshop questionnaires were collected and successfully matched using probabilistic matching processes.
Results
Similarities in demographic profile and driving history between MWR and first-time CC participants were apparent. MWR offenders scored higher on risk of problem drinking and drink-driving recidivism than either of the CC offender groups. Second-time CC offenders scored higher on these measures than first-time CC offenders. Following BOT participation, MWR participants demonstrated positive change including improved knowledge of and intentions to avoid drink-driving.
Conclusions
MWR offenders share a similar demographic profile to that of first-time CC offenders and they report significantly higher risk of problem drinking and recidivism. MWR offenders may include high-functioning problem drinkers who are likely to continue drink-driving and who may escalate to a CC drink-driving offense. Like CC offenders, MWR offenders benefited from BOT participation.","Christine M. Wickens and Rosely Flam-Zalcman and Gina Stoduto and Chloe Docherty and Rita K. Thomas and Tara Marie Watson and Justin Matheson and Kamna Mehra and Robert E. Mann",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Neural representations of time-linked memory","Many cognitive processes, such as episodic memory and decision making, rely on the ability to form associations between two events that occur separately in time. The formation of such temporal associations depends on neural representations of three types of information: what has been presented (trace holding), what will follow (temporal expectation), and when the following event will occur (explicit timing). The present review seeks to link these representations with firing patterns of single neurons recorded while rodents and non-human primates associate stimuli, outcomes, and motor responses over time intervals. Across these studies, two distinct firing patterns were observed in the hippocampus, neocortex, and striatum: some neurons change firing rates during or shortly after the stimulus presentation and sustain the firing rate stably or sidlingly during the subsequent intervals (tonic firings). Other neurons transiently change firing rates during a specific moment within the time intervals (phasic firings), and as a group, they form a sequential firing pattern that covers the entire interval. Clever task designs used in some of these studies collectively provide evidence that both tonic and phasic firing responses represent trace holding, temporal expectation, and explicit timing. Subsequently, we applied machine-learning based classification approaches to the two firing patterns within the same dataset collected from rat medial prefrontal cortex during trace eyeblink conditioning. This quantitative analysis revealed that phasic-firing patterns showed greater selectivity for stimulus identity and temporal position than tonic-firing patterns. Our summary illuminates distributed neural representations of temporal association in the forebrain and generates several ideas for future investigations.","Maryna Pilkiw and Kaori Takehara-Nishiuchi",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Wavelet decomposition of software entropy reveals symptoms of malicious code","Sophisticated malware authors can sneak hidden malicious contents into portable executable files, and this contents can be hard to detect, especially if encrypted or compressed. However, when an executable file switches between contents regimes (e.g., native, encrypted, compressed, text, and padding), there are corresponding shifts in the file’s representation as an entropy signal. In this paper, we develop a method for automatically quantifying the extent to which patterned variations in a file’s entropy signal make it “suspicious”. In Experiment 1, we use wavelet transforms to define a Suspiciously Structured Entropic Change Score (SSECS), a scalar feature that quantifies the suspiciousness of a file based on its distribution of entropic energy across multiple levels of spatial resolution. Based on this single feature, it was possible to raise predictive accuracy on a malware detection task from 50.0% to 68.7%, even though the single feature was applied to a heterogeneous corpus of malware discovered “in the wild”. In Experiment 2, we describe how wavelet-based decompositions of software entropy can be applied to a parasitic malware detection task involving large numbers of samples and features. By extracting only string and entropy features (with wavelet decompositions) from software samples, we are able to obtain almost 99% detection of parasitic malware with fewer than 1% false positives on good files. Moreover, the addition of wavelet-based features uniformly improved detection performance across plausible false positive rates, both in a strings-only model (e.g., from 80.90% to 82.97%) and a strings-plus-entropy model (e.g. from 92.10% to 94.74%, and from 98.63% to 98.90%). Overall, wavelet decomposition of software entropy can be useful for machine learning models for detecting malware based on extracting millions of features from executable files.","Michael Wojnowicz and Glenn Chisholm and Matt Wolff and Xuan Zhao",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Using concept maps to provide an integrative framework for teaching the cost or managerial accounting course","Accounting students often perceive their beginning cost or managerial course as lacking a framework with which they can organize the material. They have usually completed a beginning Financial Accounting course recently where the accounting equation provides such a framework. This makes the lack of a framework in their next course even more problematical. This paper provides a framework for integrating topics in the cost/managerial course to enhance learning. The framework uses hierarchical concept maps as the integrating mechanism. Concept maps provide a visual presentation method based on the theories of learning and knowledge. Assimilation learning theory posits that the difference between meaningful learning and rote learning depends upon whether or not the new information is integrated with, and connected to, existing knowledge. Meaningful learning takes place most easily when broader concepts are presented first and detailed ones that provide support are furnished later. Hierarchical concept maps are organized in such a fashion, with the more general, inclusive concepts at the top of the map, and progressively more specific concepts arranged below them. Concept maps are a way to develop logical thinking and study skills by revealing connections and helping students see how individual ideas form a larger whole. The paper provides a detailed discussion of how to proceed with the concept maps as building blocks in the course. This can help the instructor organize their presentation in a logical manner aimed at enhancing student learning. We also provide a comprehensive numerical example to reinforce the process over the course of the semester.","Rochelle Kaplan Greenberg and Neil A. Wilner",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Hepatocellular adenoma management: Call for shared guidelines and multidisciplinary approach","Summary
Hepatocellular adenomas are rare benign nodules developed mainly in women taking oral contraceptives. They are solitary or multiple. Their size is highly variable. There is no consensus in the literature for their management except that once their size exceeds 5cm nodules are taken out to prevent 2 major complications: bleeding and malignant transformation. There are exceptions particularly in men where it is recommended to remove smaller nodules. Since the beginning of this century, major scientific contributions have unveiled the heterogeneity of the disease. HCA are composed of four major subtypes. HNF1A (coding for hepatocyte nuclear factor 1a) inactivating mutations (H-HCA); inflammatory adenomas (IHCA); the β-catenin-mutated HCAs (β-HCA) and unclassified HCA (UHCA) occurring in 30–40%, 40–50%, 10–15% and 10% of all HCA, respectively. Half of β-HCAs are also inflammatory (β-IHCA). Importantly, β-catenin mutations are associated with a high risk of malignant transformation. HCA subtypes can be identified on liver tissue, including biopsies using specific immunomarkers with a good correspondence with molecular data. Recent data has shown that TERT promoter mutation was a late event in the malignant transformation of β-HCA, β-IHCA. Furthermore, in addition to β-catenin exon 3 mutations, other mutations do exist (exon 7 and 8) with a lower risk of malignant transformation. With these new scientific informations, we have the tools to better know the natural history of the different subtypes, in terms of growth, disappearance, bleeding, malignant transformation and to investigate HCA in diseased livers (vascular diseases, alcoholic cirrhosis). A better knowledge of HCA should lead to a more rational management of HCA. This can be done only if the different subspecialties, including hepatologists, liver pathologists, radiologists and surgeons work altogether in close relationship with molecular biologists. It is a long way to go.","Jean Frédéric Blanc and Nora Frulio and Laurence Chiche and Christine Sempoux and Laurence Annet and Catherine Hubert and Annette S.H. Gouw and Koert P. de Jong and Paulette Bioulac-Sage and Charles Balabaud",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Influence of SKF38393 on changes of gene profile in rat prefrontal cortex during chronic paradoxical sleep deprivation","Chronic paradoxical sleep deprivation (CSD) can induce dramatic physiological and neurofunctional changes in rats, including decreased body weight, reduced learning and memory, and declined locomotor function. SKF38393, a dopamine D1 receptor agonist, can reverse the above damages. However, the mechanism of CSD syndrome and reversal role of SKF38393 remains largely unexplained. To preliminarily elucidate the mechanism of the neural dysfunction caused by CSD, in the present study we use gene chips to examine the expression profile of more than 28,000 transcripts in the prefrontal cortex (PFC). Rats were sleep deprived by modified multi-platform method for 3 weeks. Totally 59 transcripts showed differential expressions in CSD group in contrast to controls; they included transcripts coding for caffeine metabolism, circadian rhythm, drug metabolism and some amino acid metabolism pathway. Among the 59 transcripts, 39 increased their expression and 20 decreased. Two transcripts can be specifically reversed with SKF38393, one of them is Homer1, which is related to 20 functional classifications and coding for Glutamatergic synapse pathway. Our findings in the present study indicate that long-term sleep deprivation may trigger the changes of some certain functions and pathways in the PFC, and lead to the dysfunction of this advanced neuron, and the activation of D1 receptor by SKF38393 might ameliorate these changes via modulation of some transcripts such as Homer1, which is involved in the Ca2+ pathway and MAPK pathway related to Glutamatergic synapse pathway.","Xiaosa Wen and Xinmin Chen and Si Chen and Yue Tan and Fei Rong and Jiangbo Zhu and Wenling Ma",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Integrating electronic healthcare records of armed forces personnel: Developing a framework for evaluating health outcomes in England, Scotland and Wales","Background
Electronic Healthcare Records (EHRs) are created to capture summaries of care and contact made to healthcare services. EHRs offer a means to analyse admissions to hospitals for epidemiological research. In the United Kingdom (UK), England, Scotland and Wales maintain separate data stores, which are administered and managed exclusively by devolved Government. This independence results in harmonisation challenges, not least lack of uniformity, making it difficult to evaluate care, diagnoses and treatment across the UK. To overcome this lack of uniformity, it is important to develop methods to integrate EHRs to provide a multi-nation dataset of health.
Objective
To develop and describe a method which integrates the EHRs of Armed Forces personnel in England, Scotland and Wales based on variable commonality to produce a multi-nation dataset of secondary health care.
Methods
An Armed Forces cohort was used to extract and integrate three EHR datasets, using commonality as the linkage point. This was achieved by evaluating and combining variables which shared the same characteristics. EHRs representing Accident and Emergency (A&E), Admitted Patient Care (APC) and Outpatient care were combined to create a patient-level history spanning three nations. Patient-level EHRs were examined to ascertain admission differences, common diagnoses and record completeness.
Results
A total of 6,336 Armed Forces personnel were matched, of which 5,460 personnel had 7,510 A&E visits, 9,316 APC episodes and 45,005 Outpatient appointments. We observed full completeness for diagnoses in APC, whereas Outpatient admissions were sparsely coded; with 88% of diagnoses coded as “Unknown/unspecified cause of morbidity”. In addition, A&E records were sporadically coded; we found five coding systems for identifying reason for admission.
Conclusion
At present, EHRs are designed to monitor the cost of treatment, enable administrative oversight, and are not currently suited to epidemiological research. However, only small changes may be needed to take advantage of what should be a highly cost-effective means of delivering important research for the benefit of the NHS.","Daniel Leightley and Zoe Chui and Margaret Jones and Sabine Landau and Paul McCrone and Richard D. Hayes and Simon Wessely and Nicola T. Fear and Laura Goodwin",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Calibration of non-linear effective stress code for seismic analysis of excess pore pressures and liquefaction in the free field","The paper presents numerical predictions of excess pore pressure, liquefaction and settlement response of four centrifuge model tests of 6m uniform deposits of saturated clean Ottawa sand, placed by dry pluviation and having a relative density ranging from 38% to 66%. The deposits were subjected to 1D uniform base shaking consisting of 10–15 cycles of peak acceleration ranging from 0.04 to 0.12g. All predictions were conducted with the nonlinear effective stress numerical code Dmod2000. Significant effort was spent in calibrating Dmod2000 by matching the pore pressure and settlement measurements of the first shaking (S1) of a series of shakings conducted in centrifuge Experiment 3. This resulted in very good predictions of both pore pressures and settlement measured in this shaking S1. The exercise showed the importance for realistic simulations of having the correct soil compressibility and permeability. This calibrated version of Dmod2000 was used for a good pore pressure prediction of the preshaken deposit in the same Experiment 3 (S36), by modifying only one parameter in the undrained pore pressure model; and also well predicted pore pressure responses in Tests FFV3 and PFV1, without any change in the parameters of Dmod2000 except for use of the new input motions (Type B predictions). The experimental and numerical results showed that both cyclic shear stress/strains and upward water flow determine together the pore pressure buildup and liquefaction phenomena. The soil response is partially drained rather than undrained, and pore pressure dissipation does take place during shaking both before and after liquefaction occurs.","R. Dobry and W. El-Sekelly and T. Abdoun",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An evolution model for elliptic-cylindrical void in viscous materials considering the evolvements of void shape and orientation","The evolution behavior of the internal voids influences the mechanical property of the materials significantly. Considering the void deformation and rotation, the evolution behavior of the elliptic-cylindrical void in power-law viscous materials was investigated by using the representative volume element (RVE) model. The rigid visco-plastic finite element (FE) method was applied to calculate the velocity field in the RVE under different loading conditions, and the instantaneous changing rate of the void radius and orientation were determined by evaluating the evolving of the void profiles at the instant. The calculated results show the deviatoric stress takes an important role in the void radius evolution, and the shear stress influences the change of the void orientation significantly. Based on the investigation, a void evolution model was established to relate the changing rates of the void radius and orientation to the void aspect ratio and the macroscopic stress/strain conditions. This model was incorporated into the FE code to predict the evolvements of void radius and orientation in each step of the deformation history. The predictions agree well with the results of the numerical simulations containing embedded void geometries in the mesh, which demonstrates that this model is capable to evaluate the void evolution behavior under large deformation. As an application, this model was used to predict the closure behavior of the void defects in the large ingot during the hot forging process.","Chao Feng and Zhenshan Cui and Xiaoqing Shang and Mingxiang Liu",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Proactive Program Management for Development National Finance System in Turbulence Environment","The global financial crisis has become a powerful impulse for the initiation a number of development programs in public finance system in many countries. Every time when we come to large-scale changes (such as those associated with the project “Reboot the public finance system of Ukraine”) it is difficult to determine all assumptions and driving forces for changes or to assess their impact. The program of innovation development, in this case carrying out the “reboot” of public finance, is being developed and implemented by the Ministry of Finance of Ukraine on the basis of evaluation of its positive impact on economic development, with effective interaction with the environment in order to create innovative products such as the Budget Code, Tax Code, Virtual University, distance learning courses for public officials, system of independent assessment of the competence of staff within the regulated processes. Products and results of the program are evaluated according to profile of values, specified in the program of President of Ukraine and supported by the people of Ukraine. The necessity of the success program management in the reformation of complicated systems, especially the finance system of Ukraine is discussed in the report. The authors suggest seven keys of success methodology, which make it possible to elaborate the strategy of reforming the domestic finance system in turbulence environment. The issues of measuring the force of the impact of innovation and change on the organizational system remain open and will be discussed.","Sergey Bushuyev and Ruslan Jaroshenko",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The Bartolomeo Ammannati’s Fountain: an artifact in progress","Artifacts are not only fundamental evidences of our history and culture, but they are even entities having a proper life. The present research focuses on Bartolomeo Ammannati’s Juno Fountain (1555) – a Late-Renaissance masterpiece whose eventful story made it moving around from its planned site, the Sala Grande in Florentine Palazzo Vecchio, to Pratolino Park, then to Boboli Garden. Finally, current fragments re-assembling and museography staging under the vaults of the National Museum of Bargello court in Florence has been set up a few years ago on the 5th centenary of Ammannati’s birthdate – after careful historical research about the many vicissitudes of the Fountain. Although there isn’t any location change expected for this Ammannati’s artwork, investigations and researches are going on. Namely, the seismic performance of the reconstructed Fountain is to be checked with reference to the seismic hazard of the site, as provided by the Italian Code classification. To this objective, the previously done laser scanning which allowed a three-dimensional digital modeling to help re-assembling the Fountain, has been now adopted to perform the structural analysis. Consequently, a structural evaluation to check the setting’s seismic behavior is currently under process. The research, developed by joining different knowledges and fields, is an example of the importance of a multidisciplinary approach for preserving artifacts and museums’ collections.","Giada Cerri and Giacomo Pirazzoli and Giorgio Verdiani and Marco Tanganelli and Vieri Cardinali and Stefania Viti",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Variant high-molecular-weight glutenin subunits arising from biolistic transformation of wheat","Genetic transformation via the biolistic method has been used to introduce genes encoding natural and novel high-molecular-weight glutenin subunits (HMW-GS) into wheat. The appearance of new seed proteins of sizes not predicted by the transgene coding sequences was noted in some experiments. In this report, the identities of thirteen of these novel proteins were determined by tandem mass spectrometry (MS/MS). Seven different proteins larger than and two proteins smaller than the native protein were shown to contain peptides from 1Dx5. A novel protein found in some progeny of crosses between a transgenic plant and Great Plains winter wheats was larger than but contained several peptides from 1Dy10. In one line, a protein larger than and a protein smaller than HMW-GS each contained peptides from the N- and C-terminus of 1Dx5 and from the repeat region of 1Dy10. In a sixth transgenic line, the native Bx7 gene was apparently replaced by a gene that encodes a larger version of 1Bx7. The variant proteins accumulate in the polymeric protein fraction, indicating that they can form inter-molecular disulfide bonds. These results show that novel proteins found in some transformants are encoded by altered versions of either the transforming or endogenous HMW-GS genes.","Ann E. Blechl and William H. Vensel",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Empirical performance model-driven data layout optimization and library call selection for tensor contraction expressions","Empirical optimizers like ATLAS have been very effective in optimizing computational kernels in libraries. The best choice of parameters such as tile size and degree of loop unrolling is determined in ATLAS by executing different versions of the computation. In contrast, optimizing compilers use a model-driven approach to program transformation. While the model-driven approach of optimizing compilers is generally orders of magnitude faster than ATLAS-like library generators, its effectiveness can be limited by the accuracy of the performance models used. In this paper, we describe an approach where a class of computations is modeled in terms of constituent operations that are empirically measured, thereby allowing modeling of the overall execution time. The performance model with empirically determined cost components is used to select library calls and choose data layout transformations in the context of the Tensor Contraction Engine, a compiler for a high-level domain-specific language for expressing computational models in quantum chemistry. The effectiveness of the approach is demonstrated through experimental measurements on representative computations from quantum chemistry.","Qingda Lu and Xiaoyang Gao and Sriram Krishnamoorthy and Gerald Baumgartner and J. Ramanujam and P. Sadayappan",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A new analytical solution and novel energy formulations for non-linear eccentric impact analysis of composite multi-layer/sandwich plates resting on point supports","In the present research, an analytical solution based on a new idea of superposition of two kinematic descriptions is presented for dynamic response analysis of a multi-layer/sandwich composite plate with point supports subjected to an eccentric low-velocity impact. Direct and virtual-work-based novel energy formulations are proposed for the problem that take into account the potential energy of the indentation region. The nonlinear governing equations of motions are found based on minimization of the total potential energy of the whole mass-plate system, including work of the inertia forces, employing Ritz technique and transformation of the time-dependent nonlinear system of governing equations to a non-linear algebraic one through a novel concept. In contrast to the available researches, influence of the lower layers on the stiffness of the contact region is incorporated. Time-dependent responses of a sandwich composite plate with simply supported edges are compared with those of a plate resting on point supports. Verification of the results has been accomplished based on results of ABAQUS computer code. In the results section, the significant effects of the point supports (in comparison to the complete edge supports), initial velocity of the indenter, aspect ratio of the plate, and material properties of the layers on time histories of the contact force and lateral deflection of the plate are investigated.","M. Shariyat and M. Roshanfar",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A dynamic framework based on local Zernike moment and motion history image for facial expression recognition","A dynamic descriptor facilitates robust recognition of facial expressions in video sequences. The current two main approaches to the recognition are basic emotion recognition and recognition based on facial action coding system (FACS) action units. In this paper we focus on basic emotion recognition and propose a spatio-temporal feature based on local Zernike moment in the spatial domain using motion change frequency. We also design a dynamic feature comprising motion history image and entropy. To recognise a facial expression, a weighting strategy based on the latter feature and sub-division of the image frame is applied to the former to enhance the dynamic information of facial expression, and followed by the application of the classical support vector machine. Experiments on the CK+ and MMI datasets using leave-one-out cross validation scheme demonstrate that the integrated framework achieves a better performance than using individual descriptor separately. Compared with six state-of-arts methods, the proposed framework demonstrates a superior performance.","Xijian Fan and Tardi Tjahjadi",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mapping chemical structure-activity information of HAART-drug cocktails over complex networks of AIDS epidemiology and socioeconomic data of U.S. counties","Using computational algorithms to design tailored drug cocktails for highly active antiretroviral therapy (HAART) on specific populations is a goal of major importance for both pharmaceutical industry and public health policy institutions. New combinations of compounds need to be predicted in order to design HAART cocktails. On the one hand, there are the biomolecular factors related to the drugs in the cocktail (experimental measure, chemical structure, drug target, assay organisms, etc.); on the other hand, there are the socioeconomic factors of the specific population (income inequalities, employment levels, fiscal pressure, education, migration, population structure, etc.) to study the relationship between the socioeconomic status and the disease. In this context, machine learning algorithms, able to seek models for problems with multi-source data, have to be used. In this work, the first artificial neural network (ANN) model is proposed for the prediction of HAART cocktails, to halt AIDS on epidemic networks of U.S. counties using information indices that codify both biomolecular and several socioeconomic factors. The data was obtained from at least three major sources. The first dataset included assays of anti-HIV chemical compounds released to ChEMBL. The second dataset is the AIDSVu database of Emory University. AIDSVu compiled AIDS prevalence for >2300 U.S. counties. The third data set included socioeconomic data from the U.S. Census Bureau. Three scales or levels were employed to group the counties according to the location or population structure codes: state, rural urban continuum code (RUCC) and urban influence code (UIC). An analysis of >130,000 pairs (network links) was performed, corresponding to AIDS prevalence in 2310 counties in U.S. vs. drug cocktails made up of combinations of ChEMBL results for 21,582 unique drugs, 9 viral or human protein targets, 4856 protocols, and 10 possible experimental measures. The best model found with the original data was a linear neural network (LNN) with AUROC>0.80 and accuracy, specificity, and sensitivity≈77% in training and external validation series. The change of the spatial and population structure scale (State, UIC, or RUCC codes) does not affect the quality of the model. Unbalance was detected in all the models found comparing positive/negative cases and linear/non-linear model accuracy ratios. Using synthetic minority over-sampling technique (SMOTE), data pre-processing and machine-learning algorithms implemented into the WEKA software, more balanced models were found. In particular, a multilayer perceptron (MLP) with AUROC=97.4% and precision, recall, and F-measure >90% was found.","Diana María Herrera-Ibatá and Alejandro Pazos and Ricardo Alfredo Orbegozo-Medina and Francisco Javier Romero-Durán and Humberto González-Díaz",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mean-based neural coding of voices","The social significance of recognizing the person who talks to us is obvious, but the neural mechanisms that mediate talker identification are unclear. Regions along the bilateral superior temporal sulcus (STS) and the inferior frontal cortex (IFC) of the human brain are selective for voices, and they are sensitive to rapid voice changes. Although it has been proposed that voice recognition is supported by prototype-centered voice representations, the involvement of these category-selective cortical regions in the neural coding of such “mean voices” has not previously been demonstrated. Using fMRI in combination with a voice identity learning paradigm, we show that voice-selective regions are involved in the mean-based coding of voice identities. Voice typicality is encoded on a supra-individual level in the right STS along a stimulus-dependent, identity-independent (i.e., voice-acoustic) dimension, and on an intra-individual level in the right IFC along a stimulus-independent, identity-dependent (i.e., voice identity) dimension. Voice recognition therefore entails at least two anatomically separable stages, each characterized by neural mechanisms that reference the central tendencies of voice categories.","Attila Andics and James M. McQueen and Karl Magnus Petersson",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The Effect of New Duty Hours on Resident Academic Performance and Adult Resuscitation Outcomes","Background
From July 2011, the Accreditation Council for Graduate Medical Education implemented new resident duty hours throughout the US. This study aimed to determine whether changes to call schedules due to these new duty hours achieved the intended goals of excellent patient care and improved resident learning.
Methods
We conducted a retrospective cohort study at an academic hospital. For patient outcomes, we used the hospital registry for code blues and rapid responses to compare the proportion of deaths and transfers to an intensive care unit (July 2010 to June 2011; July 2011 to June 2012). For resident learning, we compared delta percentage scores for annual in-service training examinations (2009 to 2010; 2010 to 2011; 2011 to 2012).
Results
We recorded 187 code blues and 469 rapid responses during the 2-year period: 48 (7.3%) deaths, 374 (57.0%) transfers to the intensive care unit, and 234 (35.7%) stabilizations on the floor. Of all transfers to the intensive care unit, those due to a code blue decreased after implementation of the new duty hours (36% [63/174] vs 25% [49/200], P = .02; adjusted odds ratio = 0.59; 95% confidence interval, 0.37-0.92). The median (interquartile range) delta percentage scores for annual in-service training examinations decreased significantly from the first time-period (2009 to 2010: 7 [4-11]) to the third time-period (2011 to 2012: 5 [2-8], P = .02).
Conclusion
We observed a reduced proportion of transfers to the intensive care unit with a code blue after implementation of new resident duty hours. Resident academic performance experienced a small but significant decrease in in-service training examination delta percentage score. We need large, multicenter studies to corroborate these findings.","Dominique J. Pepper and Michelle Schweinfurth and Vincent E. Herrin",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Application of a Fuzzy Feasibility Bayesian Probabilistic Estimation of supply chain backorder aging, unfilled backorders, and customer wait time using stochastic simulation with Markov blankets","Because supply chains are complex systems prone to uncertainty, statistical analysis is a useful tool for capturing their dynamics. Using data on acquisition history and data from case study reports, we used regression analysis to predict backorder aging using National Item Identification Numbers (NIINs) as unique identifiers. More than 56,000 NIINs were identified and used in the analysis. Bayesian analysis was then used to further investigate the NIIN component variables. The results indicated that it is statistically feasible to predict whether an individual NIIN has the propensity to become a backordered item. This paper describes the structure of a Bayesian network from a real-world supply chain data set and then determines a posterior probability distribution for backorders using a stochastic simulation based on Markov blankets. Fuzzy clustering was used to produce a funnel diagram that demonstrates that the Acquisition Advice Code, Acquisition Method Suffix Code, Acquisition Method Code, and Controlled Inventory Item Code backorder performance metric of a trigger group dimension may change dramatically with variations in administrative lead time, production lead time, unit price, quantity ordered, and stock. Triggers must be updated regularly and smoothly to keep up with the changing state of the supply chain backorder trigger clusters of market sensitiveness, collaborative process integration, information drivers, and flexibility.","James A. Rodger",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"TFOx: A versatile kinetic Monte Carlo program for simulations of island growth in three dimensions","A three dimensional (3D) kinetic Monte Carlo (KMC) code has been developed that simulates the general behavior of the 3D irreversible nucleation and growth of epitaxial islands, as motivated by experimental observations of oxide nuclei formation and growth during the early stages of copper oxidation. This package was originally a versatile two dimensional (2D) KMC code [Thin Film Oxidation (TFOx)] that considered a variety of elementary steps, including deposition, adsorption, surface diffusion, aggregation, desorption, and substrate-mediated indirect interactions between static adatoms. We extended TFOx to describe 3D island growth. This new version of TFOx is composed of a C++ console program and Python graphical user interface (GUI), such that parameterized simulation, parallel execution, and 3D growth capabilities are feasible. We examined the effects of the potential gradient and the Ehrlich–Schwöbel barrier and found that the 3D island morphology is significantly influenced by the incorporation of these two factors.","Qing Zhu and Chris Fleck and Wissam A. Saidi and Alan McGaughey and Judith C. Yang",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Working in the Second Life Environment - A Way for Enhancing Students’ Collaboration","At present, virtual world environments have a huge potential on changing the manner in which people can interact, navigate on Internet, and make business. Due to the fact that the virtual world environments become more pervasive, it is important for the researchers to be deeply involved in the understanding of those spaces. The paper describes the main aspects met by tutors and students on working in the Second Life environment, during the on-line course “Designing Technology-Enhanced Learning”, organized in the frame of the European LLP-KA3 project: “Enabling Creative Collaboration through Supportive Technologies” (code 511733-LLP-1-2010-1-FI-KA3-KA3MP). Second Life was chosen due to the fact that provides a strong collaborative environment equipped with modern features: voice interactions, chat and instant messenger, expanding the ways of communication and the possibilities for collaboration. For many Romanian students who participated to the course, it was a unique experience - they were involved in the collaborative work, together with their colleagues from Finland, Estonia and Norway, but also engaged in learning processes that enhanced creative collaboration.","Radu Lucian Olteanu and Mihai Bîzoi and Gabriel Gorghiu and Ana-Maria Suduc",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"New photometric investigation of eclipsing binary NSVS 10653195","New CCD photometric light curves of the low-mass binary system NSVS 10653195 are presented. Our complete B, V, Rc and Ic-band light curves show a remarkable out-eclipsing distortion. This phenomenon suggests that the components of the system may be active. The photometric solutions with star-spot were derived by using the 2013 version of the Wilson–Devinney (WD) code. Based on all available times of light minimum, we analyzed the orbital period changes. The O–C diagram reveals that the period of NSVS 10653195 is decreasing at a rate of dP/dt=−2.79×10−7 days yr−1, which is probably caused by angular momentum loss.","B. Zhang and S.-B. Qian and W.-P. Liao and J. Zhang and N.-P. Liu and J.-J. Wang",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Modeling and simulation of air-assist atomizers with applications to food sprays","The Cascade Atomization and Drop Breakup (CAB) model has been developed originally for pressure atomizers. In this study, the CAB model is modified to accommodate air-assist atomization. The modifications include a change in the product drop distributions, namely, the uniform distribution used in the original CAB model is replaced with a χ-squared distribution with the same average drop size. The second modification addresses the air-assist atomization process. This process is modeled by estimating the Weber number due to the increased relative velocity caused by the air flow. Depending on the value of the Weber number this leads to a catastrophic, or a stripping (sheet-thinning), or a bag breakup. The model changes are validated with experimental data obtained from two different air-assist atomizers using an oil-in-water emulsion. The simulations were performed with a modified version of the KIVA-3 CFD code, and they showed good agreement with the experimental data.","Franz X. Tanner and Kathleen Feigl and Ossi Kaario and Erich J. Windhab",2016,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Mechanical properties of cyclically-damaged structural mild steel at elevated temperatures","The mechanical response of a structural element not only depends on the inherent properties of the materials which constitute the element, but also on the history of any loads it had been previously subjected to. An important instance of this is the response of steel structures under post-earthquake fire. This research aims to investigate the potential changes that the mechanical properties of structural grade mild steel experience under such a loading sequence. The experimental results presented in this paper indicate that a prior history of cyclic loading significantly affects the proceeding ductility and strength of grade 300 steel at high temperatures. This implies that any history of cyclic loading should be included in the post-earthquake fire-resistant design of structures.","S. Sinaie and A. Heidarpour and X.L. Zhao",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Scuba diving tourism with critically endangered grey nurse sharks (Carcharias taurus) off eastern Australia: Tourist demographics, shark behaviour and diver compliance","Guidelines and a national code of conduct were implemented to manage scuba diving tourism with the critically endangered grey nurse shark (Carcharias taurus) along the Australian east coast. The demographics of diving tourists, swimming behaviour of grey nurse sharks at various life-history stages and compliance of divers to the guidelines/code of conduct were simultaneously assessed during diver–shark interactions at four sites from March 2011 to February 2012. Milling was the most frequent swimming behaviour observed and no significant changes occurred with the number of divers or distance to sharks. Divers exhibited 100% compliance with all guidelines investigated. Satisfactory compliance may have been attributable to guideline clarity, the ease of establishing diver–shark interactions, stakeholder involvement in management processes and diver perceptions of sharks. Similar sampling of group and individual shark behaviour should be done to further enhance the understanding of the beneficial and adverse impacts of this marine wildlife tourism sector.","K.R. Smith and C. Scarpaci and M.J. Scarr and N.M. Otway",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Divergence of insulin superfamily ligands, receptors and Igf binding proteins in marine versus freshwater stickleback: Evidence of selection in known and novel genes","Three-spine stickleback (Gasterosteus aculeatus) is a teleost model for understanding genetic, physiological and morphological changes accompanying freshwater (FW) adaptation. There is growing evidence that the insulin superfamily plays important roles in traits involved in marine and FW adaptation. We performed a candidate gene analysis to look for evidence of selection on 33 insulin superfamily ligand-receptor genes and insulin-like growth factor binding proteins (Igfbp's) in stickleback. Using genotype data from 11 marine and 10 FW populations, we calculated the number of SNPs per site in regulatory and intronic regions, the number of synonymous and nonsynonymous mutations in coding regions, Wright's fixation index (Fst), and performed t-tests to identify SNPs with divergent genotype frequencies between marine/FW versus Atlantic/Pacific populations. Next, we analysed genome-wide transcriptome data from eight tissues to assess differential gene expression. Two Igfbp's (Igfbp2a and Igfbp5a) show evidence of divergent adaptation between life-history types, and a cluster of nonsynonymous mutations in Igfbp5a exhibit high Fst in exons apparently alternatively spliced in gill. We find evidence of selection on the relaxin family ligand-receptor gene pair, Insl3-Rxfp2, known to be involved in male spermatogenesis and bone metabolism, and in the 5′ regulatory region of Igf2. We also confirmed the gene and coding sequence of two unannotated relaxin family ligands. These analyses underscore the utility of candidate gene studies and indicate directions for further exploration of the function of insulin superfamily genes in FW adaptation.","Tim Pellissier and Hend Al Nafea and Sara V. Good",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Adaptive multicast streaming for videoconferences on software-defined networks","Real-time applications, such as video conferences, have strong Quality of Service requirements for ensuring a decent Quality of Experience. Nowadays, most of these conferences are performed over wireless devices. Thus, an appropriate management of both heterogeneous mobile devices and network dynamics is necessary. Software Defined Networking enables the use of multicasting and stream layering inside the network nodes, two techniques able to enhance the quality of live video streams. In this paper, we propose two algorithms for building and maintaining multicast sessions in a software-defined network. The first algorithm sets up the initial multicast trees for a given call. It optimally places the stream layer adaptation function inside the core network in order to minimize the bandwidth consumption. This algorithm has two versions: the first one, based on shortest path trees is minimizing the latency, while the second one, based on spanning trees is minimizing the bandwidth consumption. The second algorithm adapts the multicast trees according to the network changes occurring during a call. It does not recompute the trees, but only relocates the stream layer adaptation functions. It requires very low computation at the controller, thus making our proposal fast and highly reactive. Extensive simulation results confirm the efficiency of our solution in terms of processing time and bandwidth savings compared to existing solutions such as multiple unicast connections, Multipoint Control Unit solutions and application layer multicast.","Christelle Al Hasrouty and Mohamed Lamine Lamali and Vincent Autefage and Cristian Olariu and Damien Magoni and John Murphy",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The first photometric investigation of the neglected short period binary DY CVn","We present new photometric observations of the short period binary DY CVn. By using the Wilson-Devinney code of 2013 version, we found that DY CVn is a shallow contact binary (f=13.2%) with a high mass ratio of q=1.251. It is a W-subtype contact system where the more massive component is about 113 K cooler than the less massive one. From 44 available CCD times of light minimum collected from the references and 3 new ones in the present paper, the variation of the orbital period is studied. It is found that the O−C diagram shows a cyclic variation, and the most plausible explanation for this cyclic change is the light-travel time effect via a third body, since an outer third companion could play an important role for its formation by removing angular momentum from the central binary.","Qu ZhiNing and Jiang LinQiao and Liu Jie and Hu YanFei and Yuan YuQuan",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Studies on jatropha oil pool fire","Fire experiments with jatropha oil are performed in a cubical compartment of volume 64 m3 to explore fire development and induced thermal environment. A door opening of size 2 m height and 1 m width is made on the front wall for ventilation purpose. Three fire experiments, with full door, half door and quarter door ventilation conditions, are conducted. Heat release rate (HRR), mass loss rate (MLR), profiles of flame temperature, room corner temperature, door temperature with velocity profiles and heat flux at different locations have been recorded. It is found that reducing the door ventilation from full door to quarter door results in change in average mass loss rate from 8.42 g/s to 6.82 g/s and fire remains overventilated type. Experiment performed under full door and half door ventilation are then simulated using CFD code, Fire Dynamics Simulator (FDS, version 6.2.0). Based on the simulations results of full door, half door ventilation recommended mesh size is 0.05 m–0.07 m and corresponding value of D∗/dx is in the range of 8.34 to 12.","Avinash Chaudhary and Akhilesh Gupta and Surendra Kumar",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Seismic design of low-rise buildings based on frequent earthquake response spectrum","Special provisions for the design of concrete frame structures to fulfill the immediate occupancy performance level for frequent (or low magnitude, or service) earthquakes are unavailable, or it is believed that drift control assures this compliance. This paper evaluates a different procedure for the structural design that guarantees an appropriate behavior for frequent (or service) earthquakes and for rare (or design) earthquakes, limited to short period concrete frame structures up to two stories. A two-story building with the typical architecture of a school is proposed as the study case. In order to compare the design procedure behavior, special moment frame (SMF) and intermediate moment frame (IMF) designs have been developed, as ASCE/SEI 7–16 and ACI 318-14 suggests in the regulations that the Ecuadorian code (NEC-2015) presents. An additional building is designed with the proposed procedure, for an earthquake with a return period of 43 years. To evaluate the behavior of the proposed structures, FEMA P-695 recommendations were followed, and non-linear models were developed using pushover and time history analyses. Results show for the buildings that were designed with the current regulations excessive demands in the non-structural elements expected for the frequent earthquake and that the proposed design methodology satisfies the behavior levels required by the regulations, without producing an abrupt change in the existing design procedure.","José Barros and Hernán Santa-María",2019,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"carlomat_3.0, an automatic tool for the electron–positron annihilation into hadrons at low energies","A new version of carlomat that allows to generate automatically the Monte Carlo programs dedicated to the description of the processes e+e−→hadrons at low center-of-mass energies is presented. The program has been substantially modified in order to incorporate the photon–vector meson mixing terms and to make possible computation of the helicity amplitudes involving the Feynman interaction vertices of new tensor structures, like those predicted by the Resonance Chiral Theory or Hidden Local Symmetry model, and the effective Lagrangian of the electromagnetic interaction of the nucleons. Moreover, a number of new options have been introduced in the program in order to enable a better control over the effective models implemented. In particular, they offer a possibility to determine the dominant production mechanisms of the final state chosen by the user.
Program summary
Program title: carlomat, version 3.0 Catalogue identifier: AEDQ_v3_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDQ_v3_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 71596 No. of bytes in distributed program, including test data, etc.: 2804246 Distribution format: tar.gz Programming language: Fortran 90/95. Computer: All. Operating system: Linux. Catalogue identifier of previous version: AEDQ_v2_0 Journal reference of previous version: Comput. Phys. Comm. 185(2014)323 Classification: 4.4, 11.2, 11.6. Does the new version supersede the previous version?: Yes Nature of problem: Predictions for reactions of low energy e+e−-annihilation into final states containing pions, kaons, light vector mesons, one or more photons and light fermion pairs within the Standard Model and effective models inspired by the Resonance Chiral Theory or Hidden Local Symmetry model. Description of the electromagnetic production of nucleon pairs within the effective Lagrangian approach. Solution method: As in former versions, a program for the Monte Carlo (MC) simulation of e+e−→ hadrons at low energies is generated in a fully automatic way for a user specified process. However, the user is supposed to select a number of options and adjust arbitrary parameters in the main part of the MC computation program in order to obtain possibly the best description of experimental data. To this end, the user can also easily supplement her/his own formulae for s-dependent vector meson widths or running couplings by appropriately modifying corresponding subroutines. Reasons for new version: Processes of e+e−→ hadrons in the energy range below the J/Ψ threshold cannot be described in the framework of perturbative quantum chromodynamics. The scalar electrodynamics which has been implemented in carlomat 2.0 [1] does not provide a satisfactory description either. The most promising theoretical frameworks in this context are the Resonance Chiral Theory or Hidden Local Symmetry model which, among others, involve the photon–vector meson mixing and a number of vertices of rather complicated Lorentz tensor structure that is not present in the Standard Model or scalar QED. Already at low energies, the hadronic final states may consist of several particles, such as pions, kaons, or nucleons which can be accompanied by one or more photons, or light fermion pairs such as e+e−, or μ+μ−. The number of Feynman diagrams of such multiparticle reactions grows substantially with increasing numbers of interaction vertices and mixing terms of the effective models. Therefore, it is highly desirable to automatize the calculations. At the same time, new program options should provide the user with an easy way of implementing her/his own changes in the program in order to better fit the experimental data. Summary of revisions: The code generation part of the program has been substantially modified in order to incorporate the photon–vector meson mixing and calls to new subroutines for computation of the helicity amplitudes of the building blocks and complete Feynman diagrams which contain new interaction vertices and mixing terms. The subroutine library of carlomat has been extended to make possible computation of the helicity amplitudes involving the Feynman interaction vertices of new Lorentz tensor structures. Many subroutines have been modified in order to incorporate the q2-dependent couplings and vector meson widths. A number of options have been introduced in order to give a better control of the effective model implemented. Restrictions: As in previous versions of the program the number of particles is limited to 12 which exceeds typical numbers of particles of the exclusive low energy e+e−-annihilation processes. However, in the presence of photon–vector meson mixing, the Feynman diagrams proliferate, for example, with currently implemented Feynman rules, there are 90672 diagrams of e+e−→3(π+π−). Hence, the compilation time of generated code may become very long already for processes with a smaller number of the final state particles. Many couplings of the effective models are not known with good enough precision and must be adjusted in consecutive runs of the program in order to obtain a satisfactory description of the experimental data. Running time: Depends on the selected process. Typical running time for the code generation varies from a fraction of a second for, e.g.,  e+e−→π+π−K+K− to about 2 min for e+e−→3(π+π−). It may become substantially longer for processes with more particles in the final state. The execution time necessary to produce the appended test output files for e+e−→π+π−μ+μ−γ and e+e−→π+π−π+π−γ was 13s and 4s, respectively. The code generation for both processes took a fraction of a second time for each process. References:[1]K. Kolodziej, Comput. Phys. Commun. 185 (2014) 323.","Karol Kołodziej",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A directional global sparse model for single image rain removal","Rain removal from a single image is an important issue in the fields of outdoor vision. Rain, a kind of bad weather that is often seen, usually causes complex local intensity changes in images and has negative impact on vision performance. Many existing rain removal approaches have been proposed recently, such as some dictionary learning-based methods and layer decomposition-based methods. Although these methods can improve the visibility of rain images, they fail to consider the intrinsic directional and structural information of rain streaks, thus usually leave undesired rain streaks or change the background intensity of rain-free region significantly. In the paper, we propose a simple but efficient method to remove rain streaks from a single rainy image. The proposed method formulates a global sparse model that involves three sparse terms by considering the intrinsic directional and structural knowledge of rain streaks, as well as the property of image background information. We employ alternating direction method of multipliers (ADMM) to solve the proposed convex model which guarantees the global optimal solution. Results on a variety of synthetic and real rainy images demonstrate that the proposed method outperforms two recent state-of-the-art rain removal methods. Moreover, the proposed method needs no training and requires much less computation significantly.","Liang-Jian Deng and Ting-Zhu Huang and Xi-Le Zhao and Tai-Xiang Jiang",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Active Multi-Population Pattern Searching Algorithm for flow optimization in computer networks – The novel coevolution schema combined with linkage learning","The main objective of this paper is to propose an effective evolutionary method for solving the problem of working paths optimization in survivable MPLS network. The paper focuses on existing network, in which only network flow can be optimized to provide network survivability using the local repair strategy. The problem is NP-complete, the solution space of the test cases is large and many genes are required to code the potential solution. Recently, the MuPPetS method (Multi-Population Pattern Searching Algorithm for Flow Assignment) was proposed and seems to be a promising tool for tackling high-dimensional, hard optimization problems. The MuPPetS is a linkage learning method that minimizes the negative effects of typical EA bottlenecks, e.g., preconvergence and significant effectiveness dropdown caused by an increasing number of genes in the chromosome. In comparison to other evolutionary methods, the MuPPetS was shown to be effective and capable of solving GA-hard problems. Therefore, the proposed MuPPetS-FuN method (Multi-Population Pattern Searching Algorithm for Flow Assignment in Non-bifurcated Commodity Flow) is based on MuPPetS. The additional objective of this paper is to propose changes to general MuPPetS framework to increase its effectiveness via better subpopulation number control strategy.","Michal Przewozniczek",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Information Extraction from the Un-Structured Document using Grammatical Inference and Alignment Similarity","Huge amount of information is available in un-structured (text) documents. Knowledge discovery in un-structured document has been recognized as promising task in the recent years. Since un-structured document is typically formatted for human viewing, it varies widely from document to document. Frequent changes made to their formatting further causes difficulty in construction of a global schema. So, Discovery of interesting rules form it is complex and tedious process. Most of the existing system uses hand-coded wrappers to extract information, which is monotonous and time consuming. In this paper we propose a novel and hybrid approach of learning (context-free) grammar rules that are based on alignment between texts. Also it automatically discovers the grammar rules using grammatical inference of repeated pattern present in un-structured (text) document. The generated rules can be used to infer the attribute value pairs from the unstructured text document.","Ramesh Thakur and Suresh Jain and Narendra S. Chaudhari and Rahul Singhai",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"FSK-Lab – An open source food safety model integration tool","In the last decades a large number of models have been developed in the quantitative microbial risk assessment (QMRA) and predictive microbiology (PM) domains. These models were generated with different scripting languages (e.g. R, MATLAB), commercial tools (e.g. @Risk) or even proprietary software tools (e.g. FSSP, FDA-iRISK). The heterogeneity in software tools used to generate models together with the lack of a harmonized model exchange format has made the (re)-use of existing models in different software tools or simulation environments very difficult. The adoption of a harmonized information exchange format called Food Safety Knowledge Markup Language (FSK-ML) would be a solution to this challenge. FSK-ML defines a framework for encoding all relevant data, metadata and model scripts in a machine-readable format. A specific feature of FSK-ML is that it allows the user to provide model scripts in different scripting languages (e.g. R, Perl, Python or MATLAB). Model metadata can be provided in accordance with the metadata schema and controlled vocabularies proposed from the Risk Assessment Knowledge Integration Platform (RAKIP) community. In order to achieve a broad adoption of FSK-ML by the scientific community it is however of extraordinary importance to provide support in terms of easy to use software solutions. In fact, it has to be as simple as possible for the end-user to create, export, import or modify standard-compliant files. Food Safety Knowledge Lab (FSK-Lab) represents such a user-friendly software tool that can create, read (import), write (export), execute and combine FSK-ML compliant objects. All metadata needed to annotate a model can also be entered and edited through FSK-Lab. It also allows generating (export) files that comply with FSK-ML and that carry the file extension “.fskx”. This ensures that all information on a model is contained in this information exchange file and that the user does not have to write and compile FSK-ML files “by hand”. FSK-Lab extends the open source Konstanz Information Miner (KNIME) data analytics platform (URL: www.knime.org), which is a graphical programming framework allowing users to create data analysis workflows from building blocks (so called nodes). Within FSK-Lab, each node has a specific task e.g. model creation can be performed with the “FSK Creator” node. As a KNIME extension, FSK-Lab allows the user to execute and integrate code from several programming languages, like Java, R, Python. All FSK-Lab software code is freely available under the GNU public license version 3.","Miguel de Alba Aparicio and Tasja Buschhardt and Ahmad Swaid and Lars Valentin and Octavio Mesa-Varona and Taras Günther and Carolina Plaza-Rodriguez and Matthias Filter",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Making semantic waves: A key to cumulative knowledge-building","The paper begins by arguing that knowledge-blindness in educational research represents a serious obstacle to understanding knowledge-building. It then offers sociological concepts from Legitimation Code Theory – ‘semantic gravity’ and ‘semantic density’ – that systematically conceptualize one set of organizing principles underlying knowledge practices. Brought together as ‘semantic profiles’, these allow changes in the context-dependence and condensation of meaning of knowledge practices to be traced over time. These concepts are used to analyze passages of classroom practice from secondary school lessons in Biology and History. The analysis suggests that ‘semantic waves’, where knowledge is transformed between relatively decontextualized, condensed meanings and context-dependent, simplified meanings, offer a means of enabling cumulative classroom practice. How these concepts are being widely used to explore organizing principles of diverse practices in education and beyond is discussed, revealing the widespread, complex and suggestive nature of ‘semantic waves’ and their implications for cumulative knowledge-building.","Karl Maton",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Long-term impact of intensive lifestyle intervention on cognitive function assessed with the National Institutes of Health Toolbox: The Look AHEAD study","Introduction
This study sought to determine whether 10 years of assignment to intensive lifestyle intervention (ILI) relative to diabetes support and education leads to better cognition. We examine intervention effects overall and among clinical subgroups, and report correlations between computer-administered and interviewer-administered cognitive batteries.
Methods
The Action for Health in Diabetes (Look AHEAD) was a 16-site randomized controlled trial with overweight/obese individuals (aged 45–76) who had type 2 diabetes. The NIH Toolbox Cognition Battery tests developed to measure cognition across the lifespan were used to evaluate cognition. Results were compared with standard paper-and-pencil tests. The Toolbox and paper-and-pencil tests were administered an average of 10.9 years after randomization to 1002 participants.
Results
Toolbox measures significantly correlated with interviewer-administered measures, with the strongest correlations between the Toolbox Fluid Cognition Composite and Trails B (r = −0.64, P < .0001) and Digit Symbol Coding (r = 0.63, P < .0001), and between the Toolbox Dimensional Change Card Sort (r = 0.55, P < .0001) and the Digit Symbol Coding test. Overall, ILI and diabetes support and education groups had similar adjusted mean cognitive outcomes (P > .05 for all). Subgroup analyses identified different intervention effects within baseline body mass index groups for Picture Sequence Memory (P = .01), within baseline cardiovascular disease groups for Picture Vocabulary (P = .01) and Fluid Cognition Composite (P = .02) measures, and within baseline age groups for Picture Vocabulary (P = .02).
Discussion
Correlations between Toolbox and interviewer-administered outcomes provide a measure of internal validity. Findings suggest no overall effect of the intervention on cognition and that an ILI resulting in weight loss may have negative implications for cognition in individuals aged ≥60, with previous history of cardiovascular disease, and those with body mass index ≥40.","Kathleen M. Hayden and Laura D. Baker and George Bray and Raymond Carvajal and Kathryn Demos-McDermott and Andrea L. Hergenroeder and James O. Hill and Edward Horton and John M. Jakicic and Karen C. Johnson and Rebecca H. Neiberg and Stephen R. Rapp and Thomas A. Wadden and Michael E. Miller",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Algorithmic choices in WARP – A framework for continuous energy Monte Carlo neutron transport in general 3D geometries on GPUs","In recent supercomputers, general purpose graphics processing units (GPGPUs) are a significant faction of the supercomputer’s total computational power. GPGPUs have different architectures compared to central processing units (CPUs), and for Monte Carlo neutron transport codes used in nuclear engineering to take advantage of these coprocessor cards, transport algorithms must be changed to execute efficiently on them. WARP is a continuous energy Monte Carlo neutron transport code that has been written to do this. The main thrust of WARP is to adapt previous event-based transport algorithms to the new GPU hardware; the algorithmic choices for all parts of which are presented in this paper. It is found that remapping history data references increases the GPU processing rate when histories start to complete. The main reason for this is that completed data are eliminated from the address space, threads are kept busy, and memory bandwidth is not wasted on checking completed data. Remapping also allows the interaction kernels to be launched concurrently, improving efficiency. The OptiX ray tracing framework and CUDPP library are used for geometry representation and parallel dataset-side operations, ensuring high performance and reliability.","Ryan M. Bergmann and Jasmina L. Vujić",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"What do quantitative ratings and qualitative comments tell us about general surgery residents’ progress toward independent practice? Evidence from a 5-year longitudinal cohort","Background
This study examines the alignment of quantitative and qualitative assessment data in end-of-rotation evaluations using longitudinal cohorts of residents progressing throughout the five-year general surgery residency.
Methods
Rotation evaluation data were extracted for 171 residents who trained between July 2011 and July 2016. Data included 6069 rotation evaluations forms completed by 38 faculty members and 164 peer-residents. Qualitative comments mapped to general surgery milestones were coded for positive/negative feedback and relevance.
Results
Quantitative evaluation scores were significantly correlated with positive/negative feedback, r = 0.52 and relevance, r = −0.20, p < .001. Themes included feedback on leadership, teaching contribution, medical knowledge, work ethic, patient-care, and ability to work in a team-based setting. Faculty comments focused on technical and clinical abilities; comments from peers focused on professionalism and interpersonal relationships.
Conclusions
We found differences in themes emphasized as residents progressed. These findings underscore improving our understanding of how faculty synthesize assessment data.","Ara Tekian and Martin Borhani and Sarette Tilton and Eric Abasolo and Yoon Soo Park",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Neighborhood Preserving Codes for Assigning Point Labels: Applications to Stochastic Search","Selecting a good representation of a solution-space is vital to solving any search and optimization problem. In particular, once regions of high performance are found, having the property that small changes in the candidate solution correspond to searching nearby neighborhoods provides the ability to perform effective local optimization. To achieve this, it is common for stochastic search algorithms, such as stochastic hillclimbing, evolutionary algorithms (including genetic algorithms), and simulated annealing, to employ Gray Codes for encoding ordinal points or discretized real numbers. In this paper, we present a novel method to label similar and/or close points within arbitrary graphs with small Hamming distances. The resultant point labels can be seen as an approximate high-dimensional variant of Gray Codes with standard Gray Codes as a subset of the labels found here. The labeling procedure is applicable to any task in which the solution requires the search algorithm to select a small subset of items out of many. Such tasks include vertex selection in graphs, knapsack-constrained item selection, bin packing, prototype selection for machine learning, and numerous scheduling problems, to name a few.","Shumeet Baluja and Michele Covell",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Ventral pallidal coding of a learned taste aversion","The hedonic value of a sweet food reward, or how much a taste is ‘liked’, has been suggested to be encoded by neuronal firing in the posterior ventral pallidum (VP). Hedonic impact can be altered by psychological manipulations, such as taste aversion conditioning, which can make an initially pleasant sweet taste become perceived as disgusting. Pairing nausea-inducing LiCl injection as a Pavlovian unconditioned stimulus (UCS) with a novel taste that is normally palatable as the predictive conditioned stimulus (CS+) suffices to induce a learned taste aversion that changes orofacial ‘liking’ responses to that sweet taste (e.g., lateral tongue protrusions) to ‘disgust’ reactions (e.g., gapes) in rats. We used two different sweet tastes of similar initial palatability (a sucrose solution and a polycose/saccharin solution, CS± assignment was counterbalanced across groups) to produce a discriminative conditioned aversion. Only one of those tastes (arbitrarily assigned and designated as CS+) was associatively paired with LiCl injections as UCS to form a conditioned aversion. The other taste (CS−) was paired with mere vehicle injections to remain relatively palatable as a control sweet taste. We recorded the neural activity in VP in response to each taste, before and after aversion training. We found that the safe and positively hedonic taste always elicited excitatory increases in firing rate of VP neurons. By contrast, aversion learning reversed the VP response to the ‘disgusting’ CS+ taste from initial excitation into a conditioned decrease in neuronal firing rate after training. Such neuronal coding of hedonic impact by VP circuitry may contribute both to normal pleasure and disgust, and disruptions of VP coding could result in affective disorders, addictions and eating disorders.","Christy A. Itoga and Kent C. Berridge and J. Wayne Aldridge",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Identifying best practice guidelines for debriefing in surgery: a tri-continental study","Background
Changes in surgical training have decreased opportunities for experiential learning in the operating room (OR). With this decrease, a commensurate increase in debriefing-dependent simulation-based activities has occurred. Effective debriefing could optimize learning from both simulated and real clinical encounters.
Methods
Thirty-three semistructured interviews with surgeons, anesthesiologists, and OR nurses from the United Kingdom, United States, and Australia identified the goals of debriefing, core components of an effective debrief, and solutions to its effective implementation. Interviews were audiotaped, transcribed, and coded using emergent theme analysis.
Results
Core components of an effective debrief include having the appropriate approach, establishing a learning environment, learner engagement, managing learner reaction, reflection, analysis, diagnosis, and application to real clinical practice. Solutions to enhance practice involve promotion of a debriefing culture within the surgical community with protected time to conduct a structured debriefing.
Conclusions
A need exists to enhance surgical training through regular structured debriefing. Identifying the key components of an effective debrief is a first step toward improving practice and embedding a debriefing culture within the OR.","Maria Ahmed and Nick Sevdalis and John Paige and Ram Paragi-Gururaja and Debra Nestel and Sonal Arora",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"OFF, Open source Finite volume Fluid dynamics code: A free, high-order solver based on parallel, modular, object-oriented Fortran API","OFF, an open source (free software) code for performing fluid dynamics simulations, is presented. The aim of OFF is to solve, numerically, the unsteady (and steady) compressible Navier–Stokes equations of fluid dynamics by means of finite volume techniques: the research background is mainly focused on high-order (WENO) schemes for multi-fluids, multi-phase flows over complex geometries. To this purpose a highly modular, object-oriented application program interface (API) has been developed. In particular, the concepts of data encapsulation and inheritance available within Fortran language (from standard 2003) have been stressed in order to represent each fluid dynamics “entity” (e.g. the conservative variables of a finite volume, its geometry, etc…) by a single object so that a large variety of computational libraries can be easily (and efficiently) developed upon these objects. The main features of OFF can be summarized as follows: Programming LanguageOFF is written in standard (compliant) Fortran 2003; its design is highly modular in order to enhance simplicity of use and maintenance without compromising the efficiency; Parallel Frameworks Supported the development of OFF has been also targeted to maximize the computational efficiency: the code is designed to run on shared-memory multi-cores workstations and distributed-memory clusters of shared-memory nodes (supercomputers); the code’s parallelization is based on Open Multiprocessing (OpenMP) and Message Passing Interface (MPI) paradigms; Usability, Maintenance and Enhancement in order to improve the usability, maintenance and enhancement of the code also the documentation has been carefully taken into account; the documentation is built upon comprehensive comments placed directly into the source files (no external documentation files needed): these comments are parsed by means of doxygen free software producing high quality html and latex documentation pages; the distributed versioning system referred as git has been adopted in order to facilitate the collaborative maintenance and improvement of the code; CopyrightsOFF is a free software that anyone can use, copy, distribute, study, change and improve under the GNU Public License version 3. The present paper is a manifesto of OFF code and presents the currently implemented features and ongoing developments. This work is focused on the computational techniques adopted and a detailed description of the main API characteristics is reported. OFF capabilities are demonstrated by means of one and two dimensional examples and a three dimensional real application.
Program summary
Program title:OFF Catalogue identifier: AESV_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AESV_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public Licence, version 3 No. of lines in distributed program, including test data, etc.: 60466 No. of bytes in distributed program, including test data, etc.: 595575 Distribution format: tar.gz Programming language: Fortran (standard 2003 or newer); developed and tested with Intel Fortran Compiler v. 12.x or newer. Computer: Designed for shared-memory multi-cores workstations and for hybrid distributed/shared-memory supercomputers, but any computer system with a Fortran (2003+) compiler is suited. Operating system: Designed for POSIX architecture and tested on GNU/Linux one. Has the code been vectorized or parallelized?: Hybrid parallelization by means of MPI library and OpenMP paradigm, tested on up to 256 processors. RAM: [1 MB; 1 GB] x core, simulation-dependent Classification: 4.3, 4.10, 12. External routines: The proprietary library [1] must be linked for producing binary outputs in Tecplot Inc. format; the MPI library [2] must be linked for running on distribute-memory parallel systems Nature of problem: Numerical solution of the Compressible Navier–Stokes equations for multi-fluids multi-phase flows in complex geometries Solution method: Fully-conservative Finite Volume scheme based on very high-order WENO Positivity-Preserving reconstruction technique and Strong Stability Preserving high-order Runge–Kutta time integration. Structured multi-block general curvilinear and body-fitted grids constitute the underlining numerical grids. MPI and OpenMP paradigms are used for parallel computations. Pre-processing tool to deal with commercial meshing softwares is provided as well as post-processing one for numerical results visualization Restrictions: At present, OFF is validated for simulating inviscid and single-phase flows (viscous fluxes computation and fully coupled Eulerian /Lagrangian schemes have still to be validated, but they are already implemented); modern Fortran compiler is mandatory Unusual features: OFF is a complex CFD software strongly based on Object-Oriented Programming paradigm by means of modern Fortran standard (2003 or higher) Additional comments: OFF project adopts Git [3], a free and open source distributed version control system. A public repository dedicated to OFF project [4] has been created on github, a web-based hosting service for software development projects using git versioning system. Finally, a comprehensive documentation [5] is provided parsing source code comments by means of doxygen software [6] Running time: The running time depends on the available computation resources, the complexity of the problem and the selected accuracy of the numerical scheme. Simple one dimensional problems require few minutes on personal workstation-like systems whereas complex three dimensional problems needing high accuracy (e.g. DNS) could require weeks on supercomputers. OFF has proven to have a good scalability on small clusters (e.g. CASPUR facilities, namely on Matrix, a GNU/Linux cluster composed by 320 nodes each one constituted by a dual Opteron quadcore at 2.1GHz with 16/32 GB of RAM) References:[1]TecIO Library, Tecplot Inc. proprietary library for I/O binary files in Tecplot format, http://www.tecplot.com/downloads/tecio-library/.[2]The Message Passing Interface (MPI) standard, a library specification for message-passing, proposed as a standard by a broadly based committee of vendors, implementors, and users, http://www.mcs.anl.gov/research/projects/mpi/.[3]Git, a free and open source distributed version control system, http://git-scm.com/.[4]Github, a web-based hosting service for software development projects using git versioning system, https://github.com.[5]Ocial OFF documentation, http://szaghi.github.com/OFF/index.html.[6]Doxygen, a documentation system for many programming languages, http://www.stack.nl/dimitri/doxygen.","S. Zaghi",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Two dimensional hashing for visual tracking","Appearance model is a key part of tracking algorithms. To attain robustness, many complex appearance models are proposed to capture discriminative information of object. However, such models are difficult to maintain accurately and efficiently. In this paper, we observe that hashing techniques can be used to represent object by compact binary code which is efficient for processing. However, during tracking, online updating hash functions is still inefficient with large number of samples. To deal with this bottleneck, a novel hashing method called two dimensional hashing is proposed. In our tracker, samples and templates are hashed to binary matrices, and the hamming distance is used to measure confidence of candidate samples. In addition, the designed incremental learning model is applied to update hash functions for both adapting situation change and saving training time. Experiments on our tracker and other eight state-of-the-art trackers demonstrate that the proposed algorithm is more robust in dealing with various types of scenarios.","Chao Ma and Chuancai Liu",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Incidence Trends of Gastroenteropancreatic Neuroendocrine Tumors in the United States","Background & Aims
Although multiple studies have reported an increasing incidence of gastroenteropancreatic neuroendocrine tumors (GEP-NETs) over the past decades, there are limited national data on recent trends. Using a population-based registry, we evaluated GEP-NET incidence trends in the United States population from 1975 through 2012, based on age, calendar year at diagnosis, and year of birth.
Methods
GEP-NET cases from 1975 through 2012 were identified from the most recent version of the Surveillance, Epidemiology, and End Results registry using histologic and site codes. We calculated overall annual incidence, age-adjusted incidence (number of cases per 100,000), annual percent change (APC), and average APC by 5-year age intervals. We also evaluated the incidence rates by age, period, and birth year cohorts.
Results
We identified 22,744 patients with GEP-NETs. In adults 25–39 years old, GEP-NET incidence rates decreased from the mid-1970s to the early 1980s, then increased until 2012. In adults ages 40 years and older or young adults ages 15–24 years, incidence rates generally increased continuously from 1975 through 2012. Adults ages 40–69 years had the most rapid increases in average APC (approximately 4%–6% per year). Overall incidence rates were highest in adults 70–84 years old. Since the inception of the Surveillance, Epidemiology, and End Results registry, GEP-NET incidence has increased in consecutive birth cohorts.
Conclusion
The incidence of GEP-NET continues to increase—particularly in older adults. More recent generations have had higher GEP-NET incidence rates than more distant generations.","Mi Ri Lee and Cynthia Harris and Kiwoon Joshua Baeg and Anne Aronson and Juan P. Wisnivesky and Michelle Kang Kim",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"First results of operational ionospheric dynamics prediction for the Brazilian Space Weather program","It is shown the development and preliminary results of operational ionosphere dynamics prediction system for the Brazilian Space Weather program. The system is based on the Sheffield University Plasmasphere–Ionosphere Model (SUPIM), a physics-based model computer code describing the distribution of ionization within the Earth mid to equatorial latitude ionosphere and plasmasphere, during geomagnetically quiet periods. The model outputs are given in a 2-dimensional plane aligned with Earth magnetic field lines, with fixed magnetic longitude coordinate. The code was adapted to provide the output in geographical coordinates. It was made referring to the Earth’s magnetic field as an eccentric dipole, using the approximation based on International Geomagnetic Reference Field (IGRF-11). During the system operation, several simulation runs are performed at different longitudes. The original code would not be able to run all simulations serially in reasonable time. So, a parallel version for the code was developed for enhancing the performance. After preliminary tests, it was frequently observed code instability, when negative ion temperatures or concentrations prevented the code from continuing its processing. After a detailed analysis, it was verified that most of these problems occurred due to concentration estimation of simulation points located at high altitudes, typically over 4000km of altitude. In order to force convergence, an artificial exponential decay for ion–neutral collisional frequency was used above mentioned altitudes. This approach shown no significant difference from original code output, but improved substantially the code stability. In order to make operational system even more stable, the initial altitude and initial ion concentration values used on exponential decay equation are changed when convergence is not achieved, within pre-defined values. When all code runs end, the longitude of every point is then compared with its original reference station longitude, and differences are compensated by changing the simulation point time slot, in a temporal adjustment optimization. Then, an approximate neighbor searching technique was developed to obtain the ion concentration values in a regularly spaced grid, using inverse distance weighting (IDW) interpolation. A 3D grid containing ion and electron concentrations is generated for every hour of simulated day. Its spatial resolution is 1° of latitude per 1° of longitude per 10km of altitude. The vertical total electron content (VTEC) is calculated from the grid, and plotted in a geographic map. An important feature that was implemented in the system is the capacity of combining observational data and simulation outputs to obtain more appropriate initial conditions to the ionosphere prediction. Newtonian relaxation method was used for this data assimilation process, where ionosonde data from four different locations in South America was used to improve the system accuracy. The whole process runs every day and predicts the VTEC values for South America region with almost 24h ahead.","Adriano Petry and Jonas Rodrigues de Souza and Haroldo Fraga de Campos Velho and André Grahl Pereira and Graham John Bailey",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Variability abstractions for lifted analyses","Family-based (lifted) static analysis for “highly configurable programs” (program families) is capable of analyzing all variants at once without generating any of them explicitly. It takes as input only the common code base, which encodes all variants of a program family, and produces precise analysis results corresponding to all variants. However, the computational cost of the lifted analysis still depends inherently on the number of variants, which is in the worst case exponential in the number of statically configurable options (features). For a large number of features, the lifted analysis may be too costly or even infeasible. In this work, we introduce variability abstractions defined as Galois connections, which simplify variability away from program families based on #ifdef-s. Then, we use abstract interpretation as a formal method for the calculational-based derivation of abstracted lifted analyses, which are sound by construction. Our approach for abstracting lifted analysis is orthogonal to the particular program analysis chosen as a client. While a single program analysis operates on program states and depends on language-specific constructs, the lifted analysis assumes that a single program analysis already exists and lifts its results to all variants of the analyzed program family. Variability abstractions aim to reduce this variability-specific component of the lifted analysis, which handles variability and #ifdef-s. Furthermore, given the “orthogonality” of variability abstractions to the rest of the analysis (its language-specific component), we can implement abstractions as a preprocessor. In particular, given an abstraction we define a syntactic transformation, which translates any program family into an abstracted version of it, such that the analysis of the abstracted program family coincides with the corresponding abstracted analysis of the original program family. We have implemented the proposed approach, and we evaluate its practicality on three Java benchmarks. The evaluation shows that abstractions yield significant performance gains, especially for families with higher variability.","Aleksandar S. Dimovski and Claus Brabrand and Andrzej Wasowski",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An intelligent system for estimating frog community calling activity and species richness","Over the past decade, dramatic declines in frog populations have been noticed worldwide. To examine this decline, monitoring frogs is becoming increasingly important. Compared to traditional field survey methods, recent advances in acoustic sensor technology have greatly extended spatial and temporal scales for monitoring animal populations. In this paper, we examine the problem of monitoring frog populations by analysing acoustic sensor data, where the population is reflected by community calling activity and species richness. Specifically, a novel acoustic event detection (AED) algorithm is first proposed to filter out those recordings without frog calls. Then, multi-label learning is used to classify each individual recording with six acoustic features: linear predictive coding coefficients, Mel-frequency cepstral coefficients, linear-frequency cepstral coefficients, acoustic complexity index, acoustic diversity index, and acoustic evenness index. Next, frog community calling activity and species richness are estimated by accumulating the results of AED and multi-label learning, respectively. Finally, ordinary least squares regression (OLS) is conducted to reveal the relationship between frog populations (frog calling activity and species richness) and weather variables (maximum temperature and rainfall). Experimental results demonstrate that our proposed intelligent system can significantly facilitate the effort to estimate frog community calling activity and species richness with comparable accuracies. The statistical results of OLS indicate that rainfall pattern has a lagged impact on frog community calling activity (significant in the first day after rainy day) and species richness (significant in the fourth day after rainy day). Temperature is shown to affect species richness but is less likely to change calling activity.","Jie Xie and Michael Towsey and Mingying Zhu and Jinglan Zhang and Paul Roe",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Hurricane vulnerability modeling: Development and future trends","Catastrophe models help to evaluate the vulnerability of the building stock exposed to a hazard. This paper presents a history of the hurricane risk models in Florida, and discusses their relationship to the building codes. The first models were econometric, and failed to predict the insured building losses produced by hurricane Andrew. This led to a change in the loss projection paradigm and to the advent of modern catastrophe modeling. Advantages and challenges of the current methodologies are discussed, including the quality of input, validation, uncertainty, and scope of the outputs. The paper concludes with a brief overview of current and future research in vulnerability modeling.","Gonzalo L. Pita and Jean-Paul Pinelli and Kurtis R. Gurley and Shahid Hamid",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Soundscapes of Digital Morphogenesis in Architecture which Created from Musical Algorithm","Music and architecture have made use of mathematical proportions throughout the history for the purpose of creating acoustic and visual forms. The reason for this is the aesthetic pursuit of both disciplines since centuries. Mathematics is one of the most important factors that influence aesthetic results. While forming their abstract aesthetic compositions the musicians use the musical notes that have definite frequency values. Each of these frequency values are defined by one integer. Every classical music artist uses the fractal sequencing of these frequencies. On the other hand we encounter hundreds of silent formats which are produced using mathematical ideas. In this context if we think of the interdisciplinary interaction between music and architecture no form is ever silent. In this study, the intersection of two disciplines will be examined in the perspective of architecture; a stumper and interrogative start for pursuit of architectural forms of the present day with the transformation of auditory forms to visual forms will be made; and a basis will be provided to be able to discuss the innovations that the spaces, structures and auditory experiences which can be formed by obtaining musical codes bring.","Didem Acar",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"TRIAD III: Nationwide Assessment of Living Wills and Do Not Resuscitate Orders","Background
Concern exists that living wills are misinterpreted and may result in compromised patient safety.
Objective
To determine whether adding code status to a living will improves understanding and treatment decisions.
Methods
An Internet survey was conducted of General Surgery, and Family, Internal, and Emergency Medicine residencies between May and December 2009. The survey posed a fictitious living will with and without additional clarification in the form of code status. An emergent patient care scenario was then presented that included medical history and signs/symptoms. Respondents were asked to assign a code status and choose appropriate intervention. Questions were formatted as dichotomous responses. Correct response rate was based on legal statute. Significance of changes in response due to the addition of either clinical context (past medical history/signs/symptoms) or code status was assessed by contingency table analysis.
Results
Seven hundred sixty-eight faculty and residents at accredited training centers in 34 states responded. At baseline, 22% denoted “full code” as the code status for a typical living will, and 36% equated “full care” with a code status DNR. Adding clinical context improved correct responses by 21%. Specifying code status further improved correct interpretation by 28% to 34%. Treatment decisions were either improved 12–17% by adding code status (“Full Code,” “Hospice Care”) or worsened 22% (“DNR”).
Conclusion
Misunderstanding of advance directives is a nationwide problem. Addition of code status may help to resolve the problem. Further research is required to ensure safety, understanding, and appropriate care to patients.","Ferdinando L. Mirarchi and Erin Costello and Justin Puller and Timothy Cooney and Nathan Kottkamp",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"NSQIP Analysis of Axillary Lymph Node Dissection Rates for Breast Cancer: Implications for Resident and Fellow Participation","Introduction
Management of the axilla in invasive breast cancer (IBC) has shifted away from more radical surgery such as axillary lymph node dissection (ALND), towards less invasive procedures, such as sentinel lymph node biopsy. Because of this shift, we hypothesize that there has been a national downward trend in ALND procedures, subsequently impacting surgical trainee exposure to this procedure using the ACS-NSQIP database to evaluate this.
Methods
Women with IBC were identified in the ACS-NSQIP database from 2007 to 2014. Procedures including ALND were identified using CPT codes. This number was divided by total cases, given a varying number of participating institutions each year. Next, cases involving resident participation were identified and divided by training level: junior (post graduate year–[PGY] 1-2), senior (PGY 3-5) and fellow (PGY ≥ 6). Two tailed z tests were used to compare proportions, with significance determined when p < 0.05.
Results
A total of 128,372 women were identified with IBC with 36,844 ALND. ALND rates decreased by an average of 2.43% yearly from 2007 to 2014. Resident participation significantly drops in 2011, from 49.3% before to 29.4% after (p < 0.01). Junior residents experienced a significant decrease in participation rate (43.3%–32.2%, p < 0.05). Senior residents and fellows experienced an upward trend in their participation, although not significant (51.2%–56.3%, p = 0.35, and 5.6%–11.6%, p = 0.056, respectively).
Conclusions
Using the ACS-NSQIP database, we demonstrate the downward trend in rate of ALND for IBC with subsequent decrease in resident participation. Junior residents experienced a significant decrease in their participation with no significant change for senior or fellow-level trainees. Awareness of this trend is important when creating future surgical curriculum changes for general surgery and fellowship training programs.","Nadia F. Nocera and Bryan J. Pyfer and Lucy M. De La Cruz and Abhishek Chatterjee and Paul T. Thiruchelvam and Carla S. Fisher",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Testing the disgust conditioning theory of food-avoidance in adolescents with recent onset anorexia nervosa","Anorexia nervosa is characterized by chronic food avoidance that is resistant to change. Disgust conditioning offers one potential unexplored mechanism for explaining this behavioral disturbance because of its specific role in facilitating food avoidance in adaptive situations. A food based reversal learning paradigm was used to study response flexibility in 14 adolescent females with restricting subtype anorexia nervosa (AN-R) and 15 healthy control (HC) participants. Expectancy ratings were coded as a behavioral measure of flexibility and electromyography recordings from the levator labii (disgust), zygomaticus major (pleasure), and corrugator (general negative affect) provided psychophysiological measures of emotion. Response inflexibility was higher for participants with AN-R, as evidenced by lower extinction and updated expectancy ratings during reversal. EMG responses to food stimuli were predictive of both extinction and new learning. Among AN-R patients, disgust specific responses to food were associated with impaired extinction, as were elevated pleasure responses to the cued absence of food. Disgust conditioning appears to influence food learning in acutely ill patients with AN-R and may be maintained by counter-regulatory acquisition of a pleasure response to food avoidance and an aversive response to food presence. Developing strategies to target disgust may improve existing interventions for patients with AN.","Tom Hildebrandt and Andrew Grotzinger and Marianne Reddan and Rebecca Greif and Ifat Levy and Wayne Goodman and Daniela Schiller",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An update to the systematic literature review of empirical evidence of the impacts and outcomes of computer games and serious games","Continuing interest in digital games indicated that it would be useful to update Connolly et al.'s (2012) systematic literature review of empirical evidence about the positive impacts and outcomes of games. Since a large number of papers was identified in the period from 2009 to 2014, the current review focused on 143 papers that provided higher quality evidence about the positive outcomes of games. Connolly et al.'s multidimensional analysis of games and their outcomes provided a useful framework for organising the varied research in this area. The most frequently occurring outcome reported for games for learning was knowledge acquisition, while entertainment games addressed a broader range of affective, behaviour change, perceptual and cognitive and physiological outcomes. Games for learning were found across varied topics with STEM subjects and health the most popular. Future research on digital games would benefit from a systematic programme of experimental work, examining in detail which game features are most effective in promoting engagement and supporting learning.","Elizabeth A. Boyle and Thomas Hainey and Thomas M. Connolly and Grant Gray and Jeffrey Earp and Michela Ott and Theodore Lim and Manuel Ninaus and Claudia Ribeiro and João Pereira",2016,"[""Science Direct""]","Rejeitado: CR4","Rejeitado: CR4"
"The Impact of Pseudostuttering Experiences on SLT students’ Learning","Studies have shown that a significant number of Speech and Language Therapystudents report feeling less competent in the area of fluency. In light of this, a small number ofresearchers have reported that participation in a pseudo-stuttering exercise can enhancefluency modules and be a beneficial learning experience for Speech and Language Therapystudents. However, there is limited research to support this claim.
Aim
To investigate the impact of a pseudo-stuttering experience on students’ learning and theimplications for future clinical practice. This study used a qualitative methodology to facilitate an in-depth exploration of theexperiences of the participants. Two focus groups were conducted in which a total of 16 SLTstudents discussed their experiences. The focus groups were transcribed and coded with theaid of NVivo9 software. The data was analysed using principles of grounded theory. Analysis of the data identified three themes relating to the students’ experiences ofpseudo-stuttering: Stepping into their shoes, Learning from experience and Implications forclinical practice. The students reported that their experience was not truly reflective of a PWS'sexperience. Nonetheless, they considered it as an opportunity to “Step into someone else'sshoes” which resulted in reports of increased empathy for PWS by some. Empathy has beenshown to facilitate the development of positive client-clinician relationships which in turn impactson therapy outcomes. The participants identified practical changes they will make to clinicalpractice following the experience. The findings suggest that participation in a pseudo-stuttering exercise can be avaluable learning tool for SLT students. Furthermore, this study considered important ethicalissues that previous research has overlooked.","L. Tobin and R. Lyons",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Full likelihood inference from the site frequency spectrum based on the optimal tree resolution","We develop a novel importance sampler to compute the full likelihood function of a demographic or structural scenario given the site frequency spectrum (SFS) at a locus free of intra-locus recombination. This sampler, instead of representing the hidden genealogy of a sample of individuals by a labelled binary tree, uses the minimal level of information about such a tree that is needed for the likelihood of the SFS and thus takes advantage of the huge reduction in the size of the state space that needs to be integrated. We assume that the population may have demographically changed and may be non-panmictically structured, as reflected by the branch lengths and the topology of the genealogical tree of the sample, respectively. We also assume that mutations conform to the infinitely-many-sites model. We achieve this by a controlled Markov process that generates ‘particles’ in the hidden space of SFS histories which are always compatible with the observed SFS. To produce the particles, we use Aldous’ Beta-splitting model for a one parameter family of prior distributions over genealogical topologies or shapes (including that of the Kingman coalescent) and allow the branch lengths or epoch times to have a parametric family of priors specified by a model of demography (including exponential growth and bottleneck models). Assuming independence across unlinked loci, we can estimate the likelihood of a population scenario based on a large collection of independent SFS by an importance sampling scheme, using the (unconditional) distribution of the genealogies under this scenario when the latter is available. When it is not available, we instead compute the joint likelihood of the tree balance parameter β assuming that the tree topology follows Aldous’ Beta-splitting model, and of the demographic scenario determining the distribution of the inter-coalescence times or epoch times in the genealogy of a sample, in order to at least distinguish different equivalence classes of population scenarios leading to different tree balances and epoch times. Simulation studies are conducted to demonstrate the capabilities of the approach with publicly available code.","Raazesh Sainudiin and Amandine Véber",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Assessing Competency in Practice-Based Learning: A Foundation for Milestones in Learning Portfolio Entries","Background
Graduate medical education is undergoing a dramatic shift toward competency-based assessment of learners. Competency assessment requires clear definitions of competency and validated assessment methods. The purpose of this study is to identify criteria used by surgical educators to judge competence in Practice-Based Learning and Improvement (PBL&I) as demonstrated in learning portfolios.
Methods
A total of 6 surgical learning and instructional portfolio entries served as documents to be assessed by 3 senior surgical educators. These faculty members were asked to rate and then identify criteria used to assess PBL&I competency. Individual interviews and group discussions were conducted, recorded, and transcribed to serve as the study dataset. Analysis was performed using qualitative methodology to identify themes for the purpose of defining competence in PBL&I. The assessment themes derived are presented with narrative examples to describe the progression of competency.
Results
The collaborative coding process resulted in identification of 7 themes associated with competency in PBL&I related to surgical learning and instructional portfolio entries: (1) self-awareness regarding effect of actions; (2) identification and thorough description of learning goals; (3) cases used as catalyst for reflection; (4) reconceptualization with appropriate use and critique of cited literature; (5) communication skills/completeness of entry template; (6) description of future behavioral change; and (7) engagement in process—identifies as personally relevant.
Conclusions
The identified themes are consistent with and complement other criteria emerging from reflective practice literature and experiential learning theory. This study provides a foundation for further development of a tool for assessing learner portfolios consistent with the Accreditation Council for Graduate Medical Education’s Next Accreditation System requirements.","Travis P. Webb and Taylor R. Merkley and Thomas J. Wade and Deborah Simpson and Rachel Yudkowsky and Ilene Harris",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Use of Focus Groups to Inform a Youth Diabetes Prevention Model","Objective
To explore minority adolescents’ perceptions of their diabetes risk, barriers and facilitators to adopting lifestyle changes, and ideas for adapting a youth diabetes prevention model.
Methods
The study was conducted at collaborating community sites in East Harlem, NY. Trained moderators facilitated focus groups, which were audio taped and transcribed. Participants were 21 Latino and African American adolescents aged 14–18 years with a family history of diabetes and no reported personal history of diabetes. The phenomenon of interest was youth input in adapting a diabetes prevention model. Two researchers independently coded transcripts, identified major themes, compared findings, and resolved differences through discussion and consensus.
Results
Dominant themes included (1) the impact of diabetes on quality of life within adolescents’ personal networks; (2) conflict between changing diet and activity and their current lifestyle; (3) lifestyle choices being dictated by cost, mood, body image, and environment, not health; and (4) family, social, and environmental pressures reinforcing sedentary behaviors and unhealthy diets.
Conclusions and Implications
Themes from youth focus groups were framed in the context of an existing youth diabetes prevention conceptual model, with results informing expansion of the model and identification and organization of potential intervention components.","Nita Vangeepuram and Jane Carmona and Guedy Arniella and Carol R. Horowitz and Deborah Burnet",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"“It Has Changed”: Understanding Change in a Parenting Program in South Africa","Background
Poor parenting that leads to child maltreatment during adolescence presents a major public health burden. Research from high-income countries indicates that evidence-based parenting program interventions can reduce child maltreatment. Much less is known, however, about how beneficiaries of these programs experience this process of change. Understanding the process that brings about change in child maltreatment practices is essential to understanding intervention mechanisms of change. This is particularly important given the current scale-up of parenting programs across low- and middle-income countries.
Objectives
This study aimed to provide insight into how caregivers and adolescents attending a parenting program in South Africa perceived changes associated with abuse reduction.
Methods
Semi-structured interviews were conducted with caregivers and adolescents (n = 42) after the intervention, as well as observations of sessions (n = 9) and focus group discussions (n = 240 people). Participants were adolescents between the ages of 10-18 and their primary caregiver residing in peri-urban and rural program clusters in the Eastern Cape Province of South Africa. Data were coded in Atlas.ti, and thematic content analysis was conducted.
Findings
Based on participant perceptions, the Sinovuyo Teen parenting program workshops catalyzed change into practice by creating an environment that was conducive to learning alternatives. It did so through prioritizing a process of mutual respect, openness, and being valued by others, giving legitimacy to a respectful reciprocity and new ways of spending time together that enabled caregivers and teenagers to shift and normalize more positive behaviors. This in turn led to reductions in physical and verbal abuse.
Conclusions
This study's findings may be of use to policymakers and practitioners who need to understand how parenting programs support parents and teenagers in increasing positive parenting approaches and changing potentially harmful practices. It additionally highlights the importance of assessing the experiences of both parents and teenagers attending such programs.","Jenny Doubt and Rachel Bray and Heidi Loening-Voysey and Lucie Cluver and Jasmina Byrne and Divane Nzima and Barnaby King and Yulia Shenderovich and Janina Steinert and Sally Medley",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Contact modeling in boundary element analysis including the simulation of thermomechanical wear","Computational mechanics codes include contact or over-closure models to account for contact stiffness, which can be related to surface roughness and nominal pressure from Greenwood and Williamson. We present results of wear simulation studies with adjustable contact parameters for a ring-on-ring sliding configuration. A boundary element code is used to solve the non-linear contact problem with an axisymmetric thermoelastic representation of the rings, along with a localized Archard wear model. Parametric studies indicate the extent to which the wear tracks can be affected by the roughness and wear history. The effects range from minor to moderately significant, with areas of maximum wear shifting radially. Contact parameters influence pressure, temperature and wear distributions, with stiffer contacts resulting in greater localization.","Gary F. Dargush and Andres Soom",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Engineering bond model for corroded reinforcement","Corrosion of the reinforcement in concrete structures affects their structural capacity. This problem affects many existing concrete bridges and climate change is expected to worsen the situation in future. At the same time, assessment engineers lack simple and reliable calculation methods for assessing the structural capacity of structures damaged by corrosion. This paper further develops an existing model for assessing the anchorage capacity of corroded reinforcement. The new version is based on the local bond stress-slip relationships from fib Model Code 2010 and has been modified to account for corrosion. The model is verified against a database containing the results from nearly 500 bond tests and by comparison with an empirical model from the literature. The results show that the inherent scatter among bond tests is large, even within groups of similar confinement and corrosion level. Nevertheless, the assessment model that has been developed can represent the degradation of anchorage capacity due to corrosion reasonably well. This new development of the model is shown to represent the experimental data better than the previous version; it yields similar results to an empirical model in the literature. In contrast to many empirical models, the model developed here represents physical behaviour and shows the full local bond stress-slip relationship. Using this assessment model will increase the ability of professional engineers to estimate the anchorage capacity of corroded concrete structures.","Mattias Blomfors and Kamyab Zandi and Karin Lundgren and Dario Coronelli",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Towards energy aware object-oriented development of android applications","Energy consumption has become a concern for developers due to the increasing complexity of applications that are to run on devices with limited battery power. Developers want to develop energy efficient applications however existing tools do not bridge the gap between understanding where energy is consumed and suggesting how the code can be modified in order to reduce energy consumption. A generalized method to relate software structure with its energy consumption is hence desirable. Previous attempts to relate change in object-oriented structure to its effects on energy consumption have been inconclusive. In this paper, we proposed a methodology to relate software structural information represented as metrics to energy consumption. Employing our methodology we empirically validated three Object Oriented (OO) metric suites; the Abreus Metrics (MOOD), Chidamber and Kemerer (CK) metrics and Martin's package metric suite and determine their relationship with energy consumption. Our results show that software structural metrics can be reliably related to energy consumption behavior of programs using a total of 63 releases from seven open-source iteratively developed android applications.","Hareem Sahar and Abdul A. Bangash and Mirza O. Beg",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Evolution of hippocampal spatial representation over time in mice","To investigate the intriguing and paradoxical contrast between the time-limited role of the hippocampus in memory consolidation and its permanent contribution to spatial memory as revealed by place cell activity, we carefully monitored the temporal evolution of the same set of place cells in normal naïve mice throughout their familiarization to a spatial context and their consolidation of memory about space. Over six daily recording sessions, despite their widely reported stability, we observed gradual changes in hippocampal place fields and cell firing patterns. These changes were interpreted in terms of both improvement and impoverishment of spatial codes: improvement due to intrinsic place cell plasticity, and impoverishment as a consequence of attentional filtering of allocentric spatial information reaching the hippocampus due to the procedural behavioral requirements of the task, or to hippocampal disengagement as learning progresses.","Yannick Jeantet and Yoon H. Cho",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Effects of different insulation materials on primary energy and CO2 emission of a multi-storey residential building","In this study, we analyzed the implications of various insulation materials on the primary energy and CO2 emission for material production of a residential building. We modeled changes to the original design of the building to achieve reference buildings to energy-efficiency levels of the Swedish building code of 2012 or the Swedish Passivhus 2012 criteria. We varied the insulation materials in different parts of the reference buildings from mineral rock wool to glass wool, cellulose fiber, expanded polystyrene or foam glass. We compared the primary energy use and CO2 emission from material production of functionally equivalent reference and optimum versions of the building. The results showed a reduction of about 6–7% in primary energy use and 6–8% in CO2 emission when the insulation material in the reference buildings is changed from rock wool to cellulose fiber in the optimum versions. Also, the total fossil fuel use for only insulation material production was reduced by about 39%. This study suggests that enhancing material production technologies by reducing fossil fuel-use and increasing renewable energy sources, as well as careful material choice with renewable-based raw materials can contribute significantly in reducing primary energy use and GHG emission in the building sector.","Uniben Yao Ayikoe Tettey and Ambrose Dodoo and Leif Gustavsson",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Development and verification of the coupled 3D neutron kinetics/thermal-hydraulics code DYN3D-HTR for the simulation of transients in block-type HTGR","DYN3D is a nodal diffusion code for 3D steady-state and transient analysis of Light Water Reactor (LWR) cores with hexagonal or square fuel element geometry. In addition to the neutron kinetics, it comprises of a thermal-hydraulics model for flow in parallel coolant channels. Macroscopic cross section data libraries generated with variation of burn-up, reactor poisons concentrations and thermal-hydraulic feedback parameters are linked to the code. Two-group and multi-groups versions of the code are available. Currently, at the Helmholtz-Zentrum Dresden-Rossendorf (HZDR), the DYN3D code is being extended and adopted for the application to block-type High Temperature Gas-Cooled Reactors (HTGRs). In this paper, we give an overview of the latest developments of DYN3D concerning block-type HTGR. The simplified P3 (SP3) transport approximation is implemented into the multi-group DYN3D code to take anisotropy of the neutron flux and heterogeneity of the core more precisely into account. The SP3 method previously implemented into DYN3D for square fuel element geometry of LWR is being extended for hexagonal geometry of the graphite blocks, where the hexagons are subdivided into triangular nodes to be able to perform a systematic mesh refinement. One of the main challenges in cross section generation for the HTGR core calculations is the treatment of the so-called “double heterogeneity”. The modified Reactivity-Equivalent Physical Transformation (RPT) approach is applied in order to eliminate the double-heterogeneity of HTGR fuel elements in the deterministic lattice calculations. The main steps of the RPT method are described. The use of the method for the cross section generation of a simplified HTGR core including its verification is presented. A 3D heat conduction module coupled with a channel-type coolant flow model is implemented to take the temperature reactivity feedback to neutronics physically correctly into account. It is shown that there is significant redistribution of the produced heat by heat conduction between the graphite blocks.","U. Rohde and S. Baier and S. Duerigen and E. Fridman and S. Kliem and B. Merk",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"From Authorship to Authoring: Critical Literacy, Expert Users, and Proprietary Software","This essay argues that new authoring environments fundamentally change the authorship paradigm. Four kinds of critical literacy may be useful to produce computational media that responds appropriately to the larger rhetorical context of software culture. These critical literacies include not only writing code, but also learning a range of user interfaces, participating in design practices for debugging programs, and recognizing the norms of digital labor workflows and systems of credit.","Elizabeth Losh",2014,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR9"
"A novel OpenSees element for single curved surface sliding isolators","The increasing use of curved surface sliding bearings as seismic isolators benefits from the improvement of analytical models that can accurately capture their experimental performance and enhance the predictive capability of nonlinear response history analyses. The mathematical formulation proposed in this paper aims at addressing the variability of the coefficient of friction based on experimental data that can be retrieved from prototype tests on Curved Surface Sliders. The formulation accounts for variation in the coefficient of friction with the instantaneous change of axial load and sliding velocity at the contact interface, and the accumulated heat due to cyclic motion; furthermore, it incorporates new features such as the static friction developed in the transition from the pre-sliding phase to the dynamic sliding condition. The proposed model has been coded in the object-oriented finite element software OpenSees by modifying the standard SingleFPSimple3d element that describes the force – displacement relationship of a bearing comprising one concave sliding surface and a spherical articulation. The main novelties of the new CSSBearing_BVNC element are inclusion of the static friction before the breakaway and degradation of kinetic friction induced by the heat developed during the motion of the articulated slider. The primary assumptions in the development of the friction model and the verification of the newly developed element are validated by agreement with available data. A case study helps to demonstrate the improved prediction capability of the new bearing element over its standard counterpart when applied to real situations, such as estimating a +50% increase in isolator displacement, superstructure drift and base shear demand under high intensity earthquakes, and possible non-activation of the sliding isolators under weak or medium intensity earthquakes.","E. Gandelli and M. Penati and V. Quaglini and G. Lomiento and E. Miglio and G.M. Benzoni",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Improved absolute calibration of LOPES measurements and its impact on the comparison with REAS 3.11 and CoREAS simulations","LOPES was a digital antenna array detecting the radio emission of cosmic-ray air showers. The calibration of the absolute amplitude scale of the measurements was done using an external, commercial reference source, which emits a frequency comb with defined amplitudes. Recently, we obtained improved reference values by the manufacturer of the reference source, which significantly changed the absolute calibration of LOPES. We reanalyzed previously published LOPES measurements, studying the impact of the changed calibration. The main effect is an overall decrease of the LOPES amplitude scale by a factor of 2.6 ± 0.2, affecting all previously published values for measurements of the electric-field strength. This results in a major change in the conclusion of the paper ‘Comparing LOPES measurements of air-shower radio emission with REAS 3.11 and CoREAS simulations’ published by Apel et al. (2013) : With the revised calibration, LOPES measurements now are compatible with CoREAS simulations, but in tension with REAS 3.11 simulations. Since CoREAS is the latest version of the simulation code incorporating the current state of knowledge on the radio emission of air showers, this new result indicates that the absolute amplitude prediction of current simulations now is in agreement with experimental data.","W.D. Apel and J.C. Arteaga-Velázquez and L. Bähren and K. Bekk and M. Bertaina and P.L. Biermann and J. Blümer and H. Bozdog and I.M. Brancus and E. Cantoni and A. Chiavassa and K. Daumiller and V. de Souza and F. Di Pierro and P. Doll and R. Engel and H. Falcke and B. Fuchs and H. Gemmeke and C. Grupen and A. Haungs and D. Heck and R. Hiller and J.R. Hörandel and A. Horneffer and D. Huber and T. Huege and P.G. Isar and K.-H. Kampert and D. Kang and O. Krömer and J. Kuijpers and K. Link and P. Łuczak and M. Ludwig and H.J. Mathes and M. Melissas and C. Morello and S. Nehls and J. Oehlschläger and N. Palmieri and T. Pierog and J. Rautenberg and H. Rebel and M. Roth and C. Rühle and A. Saftoiu and H. Schieler and A. Schmidt and S. Schoo and F.G. Schröder and O. Sima and G. Toma and G.C. Trinchero and A. Weindl and J. Wochele and J. Zabierowski and J.A. Zensus",2016,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Steganographic schemes with multiple q-ary changes per block of pixels","A family of matrix embedding steganographic schemes for digital images is investigated. The target schemes are applied to blocks of n pixels in a cover image. In every block, at most m pixels are allowed to change with q-ary steps. We have derived some upper bounds on the embedding efficiency of these schemes for different values on m. It is also shown that these upper bounds approach the general upper bound on the embedding efficiency of q-ary steganography. For the case of q=3, we have shown that there is no feasible optimum member of the family for m=2, although for m=1, a well-known example exists. Instead, for m=2, a new close-to-bound scheme in the family is presented which exploits difference sets and wet paper coding in two separate channels. The proposed scheme outperforms other known members of the family, including the original Exploiting Modification Direction (EMD), several improved EMD versions, and two-change Sum and Difference Covering Set (SDCS) constructions. Some experiments are also conducted on an image dataset to compare the proposed scheme with other state-of-the-art Matrix embedding schemes. According to the experimental results, the performance of the proposed scheme is close to the state-of-the-arts at various embedding rates.","Iman Gholampour and Khashayar Khosravi",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Recent decline in the U.S. death rate from myeloproliferative neoplasms, 1999–2006","Background: Myeloproliferative neoplasms (MPNs) are classified as neoplasms of uncertain or unknown behavior in the International Classification of Diseases (ICD) Version 10 and can contribute to risk of death from complications (especially thrombosis). Methods: U.S age-standardized death rates using ICD-Version 10 codes relevant to classical MPN (i.e., polycythemia vera, essential thrombocythemia, and “chronic myeloproliferative disease”) were examined for 1999–2006. The underlying cause of death and also all causes (“multiple causes” or “mentions”) coded on death certificates were considered. Trends were assessed by using percentage change (PC) in rate between 1999 and 2006, and annual percentage change (APC) estimated from linear regression. Results: The decline in death rates was large for MPN, whether based only the underlying cause (PC=−19.7%, APC=−3.4%) or on the substantially higher rates based on any cause (PC=−24.1%, APC=−3.8%), and was consistent by gender and age group (<65 and 65+ years). For deaths with MPN coded as other than the underlying cause, cardiovascular diseases were the most common underlying cause and the ASR for these deaths declined substantially (PC=−40.0%). Conclusions: Use of the underlying cause of death in surveillance will considerably underestimate MPN-related mortality rates in the population. Studies are needed on treatment in random samples of MPN patients from population-based cancer registries. Continued surveillance of MPN-related mortality rates in the population is needed in view of recent attempts (including the use of aspirin) to control cardiovascular complications of MPN.","Anthony P. Polednak",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Consulting the Educational Actors - What Do Romanian Science Teachers Really Need?","All the recent changes made in the European education system - and also in the Romanian one - claim from the teaching staff to be actively involved in specific lifelong learning actions. On the other hand, schools must proceed to a clear self-assessment of their educational offer, and also to assess the extent on meeting new challenges and identifying the actual training needs.However, for designing effective training programs, it must accurately be assessed the real in-service teachers’ training needs. In this respect, the training needs analysis helps on determining the proper ways that can enhance the teachers’ professional development, schools performances and quality assurance standards. This paper presents the main results of the training needs analysis performed in the frame of the FP7 European Research Project “PROFILES - Professional Reflection-Oriented Focus on Inquiry-based Learning and Education through Science” (code: 5.2.2.1-SiS-2010- 2.2.1-266589), with the view to illustrate an accurate image of the actual professional level of Science teachers, but also to consider the conclusions of analysis as a basis for designing a specific training program that targets to improve the teaching activities, through promoting reflection-oriented teaching, pedagogical and scientific competences, Inquiry-based Science Education (IBSE) and other related approaches which can be implemented in the educational environment.","Gabriel Gorghiu and Laura Monica Gorghiu and Luminiţa Mihaela Drăghicescu",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Martensitic–Austenitic phase transformation of Ni–Ti SMAs: Thermal properties","The main scope of this work concerns the definition of the thermal conductivity temperature dependence of fully dense NiTi SMAs in the temperature range where the Martensitic–Austenitic phase transformation occurs. The methodology used to evaluate this thermal property is based on an experimental-numerical approach that requires the definition of the heat capacity temperature dependence and the knowledge of the latent heat of transformation. The experimental work is based on the capability to heat the cylindrical NiTi samples uniformly on one side and to impose a variety of initial heating rates ranging from 0.1 to 5K/s. Laser radiant energy was used as the heating source and the temperature history of the top and bottom NiTi sample surfaces were recorded using thermocouples. The numerical code considered the sample as a solid with a constant density and with thermal properties that were dependent on temperature. The heat capacity and latent heat of transformation were defined on the basis of the thermal analysis data, while the convection heat exchange coefficient was estimated from knowledge of the experimental configuration, lateral sample surface and the temperature field of the gas surrounding the sample. The results indicated that the thermal conductivity generally increased with temperature, but a minimum in the temperature range defining the Martensitic–Austenitic transformation has been pointed out. The higher thermal conductivity value of the Austenite phase is correlated with its electronic structure.","C. Zanotti and P. Giuliani and A. Chrysanthou",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Phenomena identification and ranking table for thermal-hydraulic phenomena during a small-break LOCA with loss of high pressure injection","Currently Appendix K of 10 CFR 50 is used in the United States to evaluate models for the emergency cooling systems of light-water reactors. To assure that these models are accurate enough to ensure that the cooling systems are satisfactory, code scalability, applicability and uncertainty methodologies are used to evaluate the uncertainty in system analysis code predictions due to the various models. One cornerstone of this methodology is the development of the Phenomenon Identification and Ranking Table (PIRT), which summarizes the thermal-hydraulic phenomenon associate with a particular accident scenario and ranks their importance in determining the effectiveness of core cooling and the parts of the accident for which the phenomenon may be important. In this paper the PIRT developed by the Institute for Nuclear Safety Systems for a small-break LOCA with loss of high-pressure emergency coolant injection is analyzed in detail and several modifications are proposed based on a mechanistic understanding of the phenomenon involved. The resulting PIRT should provide a more accurate guide for model evaluation and development in advanced thermal-hydraulic system analysis codes.","M.J. Griffiths and J.P. Schlegel and T. Hibiki and M. Ishii and I. Kinoshita and Y. Yoshida",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Coherent backscattering effect in spectra of icy satellites and its modeling using multi-sphere T-matrix (MSTM) code for layers of particles","The coherent backscattering effect (CBE), the constructive interference of light scattering in particulate surfaces (e.g., regolith), manifests as a non-linear increase in reflectance, or opposition surge, and a narrow negative polarization feature at small solar phase angles. Due to a strong dependence of the amplitude and angular width of this opposition surge on the absorptive characteristics of the surface material, CBE also produces phase-angle-dependent variations in the near-infrared spectra. In this paper we present a survey of such variations in the spectra of icy satellites of Saturn obtained by the Cassini spacecraft's Visual and Infrared Mapping Spectrometer (VIMS) and in the ground-based spectra of Oberon, a satellite of Uranus, obtained with TripleSpec, a cross-dispersed near-infrared spectrometer on the Astrophysical Research Consortium 3.5-m telescope located at the Apache Point Observatory near Sunspot, New Mexico. The paper also presents computer modeling of the saturnian satellite spectra and their phase-angle variations using the most recent version of the Multi-Sphere T-Matrix (MSTM) code developed to simulate light scattering by layers of randomly distributed spherical particles. The modeling allowed us not only to reproduce the observed effects but also to estimate characteristics of the icy particles that cover the surfaces of Rhea, Dione, and Tethys.","Karly M. Pitman and Ludmilla Kolokolova and Anne J. Verbiscer and Daniel W. Mackowski and Emily C.S. Joseph",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Enhancement of the SPARC90 code to pool scrubbing events under jet injection regime","Submerged gaseous jets may have an outstanding relevance in many industrial processes and may be of particular significance in severe nuclear accident scenarios, like in the Fukushima accident. Even though pool scrubbing has been traditionally associated with low injection velocities, there are a number of potential scenarios in which fission product trapping in aqueous ponds might also occur under jet injection regime (like SGTR meltdown sequences in PWRs and SBO ones in BWRs). The SPARC90 code was developed to determine the fission product trapping in pools during severe accidents. The code assumes that carrier gas arrives at the water ponds at low or moderate velocities and it forms a big bubble that eventually detaches from the injection pipe. However, particle laden gases may enter the water at very high velocities resulting in a submerged gas jet instead. This work presents the fundamentals, major hypotheses and changes introduced into the code in order to estimate particle removal during gas injection in pools under the jet regime (SPARC90-Jet). A simplified and reliable approach to submerged jet hydrodynamics has been implemented on the basis of updated equations for jet hydrodynamics and aerosol removal, so that gas–liquid and droplet-particles interactions are described. The code modifications have been validated as far as possible. However, no suitable hydrodynamic tests have been found in the literature, so that an indirect validation has been conducted through comparisons against data from pool scrubbing experiments. Besides, this validation has been forcefully limited since very few pool scrubbing tests are available in the jet injection regime (i.e., ACE, LACE, POSEIDON II and RCA). But nevertheless, a considerable improvement in the estimation of the Decontamination Factor (DF) has been reached, as well as it has been proven that sizes of aerosol particles and submergencies are factors of major influence, however there is still a long road ahead. We have extended the SPARC90 capabilities to study jet discharge processes, then the new SPARC90-Jet version is able to study globular and jet discharge processes, i.e. pool discharges under low and high velocity conditions. Therefore, the work here presented should be understood as a promising first step toward an effective code extension to the jet regime.","C. Berna and A. Escrivá and J.L. Muñoz-Cobo and L.E. Herranz",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical analysis of experimental studies of methane hydrate dissociation induced by depressurization in a sandy porous medium","Methane Hydrates (MHs) are a promising energy source abundantly available in nature. Understanding the complex processes of MH formation and dissociation is critical for the development of safe and efficient technologies for energy recovery. Many laboratory and numerical studies have investigated these processes using synthesized MH-bearing sediments. A near-universal issue encountered in these studies is the spatial heterogeneous hydrate distribution in the testing apparatus. In the absence of direct observations (e.g. using X-ray computed tomography) coupled with real time production data, the common assumption made in almost all numerical studies is a homogeneous distribution of the various phases. In an earlier study (Yin et al., 2018) that involved the numerical description of a set of experiments on MH-formation in sandy medium using the excess water method, we showed that spatially heterogeneous phase distribution is inevitable and significant. In the present study, we use as a starting point the results and observations at the end of the MH formation and seek to numerically reproduce the laboratory experiments of depressurization-induced dissociation of the spatially-heterogeneous MH distribution. This numerical study faithfully reproduces the geometry of the laboratory apparatus, the initial and boundary conditions of the system, and the parameters of the dissociation stimulus, capturing accurately all stages of the experimental process. Using inverse modelling (history-matching) that minimized deviations between the experimental observations and numerical predictions, we determined the values of all the important flow, thermal, and kinetic parameters that control the system behaviour, which yielded simulation results that were in excellent agreement with the measurements of key monitored variables, i.e. pressure, temperature, cumulative production of gas and water over time. We determined that at the onset of depressurization (when the pressure drop – the driving force of dissociation – is at its maximum), the rate of MH dissociation approaches that of an equilibrium reaction and is limited by the heat transfer from the system surroundings. As the effect of depressurization declines over time, the dissociation reaction becomes kinetically limited despite significant heat inflows from the boundaries, which lead to localized temperature increases in the reactor.","Zhenyuan Yin and George Moridis and Zheng Rong Chong and Hoon Kiang Tan and Praveen Linga",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Smoking during pregnancy as a possible risk factor for pediatric neoplasms in the offspring: A population-based cohort study","Objective
The aim of this study was to evaluate the association between maternal smoking during pregnancy and future risk of childhood neoplasm risk.
Study design
A population based cohort analysis comparing the risk for long-term childhood neoplasms in children born (1991–2014) to mothers that smoked during pregnancy vs. those that did not. Childhood neoplasms were pre-defined based on ICD-9 codes, as recorded in the hospital medical files. Children with congenital malformations and multiple gestations were excluded from the analysis. Kaplan-Meier survival curves were constructed to compare cumulative oncological morbidity over time. Cox proportional hazards model was used to control for confounders.
Results
241,273 infants met the inclusion criteria; out of those 2841 were born to mothers that smoked during pregnancy. Offspring to smoking mothers had higher incidence of benign (OR 1.6, 95%CI 1.02–2.58; p value = .038) but not malignant tumors. Total cumulative neoplasm incidence was significantly higher in smoking women (Log Rank = 0.001) but no significant difference in the incidence of malignant tumors was noted (Log Rank = 0.834). In a Cox regression model controlling for maternal confounders; a history of maternal smoking during pregnancy remained independently associated only with increased risk for benign tumors (adjusted HR 2.5, 95%CI 1.57–3.83, p = .001).
Conclusion
Maternal smoking during pregnancy is associated with increased long-term risk for benign but not malignant tumors. This is important when counseling mothers regarding potential future risks and recommended lifestyle modifications. Despite this large population study with long follow-up, childhood malignancies are rare, and clarifying the possible association may require further studies.","Roy Kessous and Tamar Wainstock and Eyal Sheiner",2019,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Experiences and Perceptions of Adults Accessing Publicly Available Nutrition Behavior-Change Mobile Apps for Weight Management","Background
Nutrition mobile apps have become accessible and popular weight-management tools available to the general public. To date, much of the research has focused on quantitative outcomes with these tools (eg, weight loss); little is known about user experiences and perceptions of these tools when used outside of a research trial environment.
Objective
Our aim was to understand the experiences and perceptions of adult volunteers who have used publicly available mobile apps to support nutrition behavior change for weight management.
Design
We conducted one-on-one semi-structured interviews with individuals who reported using nutrition mobile apps for weight management outside of a research setting.
Participants/setting
Twenty-four healthy adults (n=19 females, n=5 males) who had used publicly available nutrition mobile apps for weight management for ≥1 week within the past 3 to 4 months were recruited from the community in southern Ontario and Edmonton, Canada, using different methods (eg, social media, posters, and word of mouth).
Qualitative data analysis
Interviews were audiorecorded, transcribed verbatim, and transcripts were verified against recordings. Data were coded inductively and organized into categories using NVivo, version 10 (QSR International).
Results
Participants used nutrition apps for various amounts of time (mean=approximately 14 months). Varied nutrition apps were used; however, MyFitnessPal was the most common. In the interviews, the following four categories of experiences with nutrition apps became apparent: food data entry (database, data entry methods, portion size, and complex foods); accountability, feedback, and progress (goal setting, accountability, monitoring, and feedback); technical and app-related factors; and personal factors (self-motivation, privacy, knowledge, and obsession). Most participants used apps without professional or dietitian support.
Conclusions
This work reveals that numerous factors affect use and ongoing adherence to use of nutrition mobile apps. These data are relevant to professionals looking to better assist individuals using these tools, as well as developers looking to develop new and improved apps.","Jessica R.L. Lieffers and Jose F. Arocha and Kelly Grindrod and Rhona M. Hanning",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Modelling gravel beach dynamics with XBeach","Numerical cross-shore profile evolution models have been good at predicting beach erosion during storm conditions, but have difficulty in predicting the accretion of the beach during calm periods. This paper describes the progress made in modifying and applying the public domain XBeach code to the prediction and explanation of the observed behaviour of coarse-grained beaches in the laboratory and the field under accretive conditions. The paper outlines in details the changes made to the original code (version 12), including the introduction of a new morphological module based upon Soulsby's sediment transport equation for waves and currents, and the incorporation of Packwood's infiltration approach in the unsaturated area of the swash region. The competence of this modified model during calm conditions for describing the steepening of the profile, and the growth of the beach berm is demonstrated. Preliminary results on the behaviour of the beach subject to both waves and tides are presented. Good agreement is found between the model simulations and large-scale laboratory measurements, as well as field observations from a composite beach in the UK. The reasons for the model's capabilities are discussed.","M.H. Jamal and D.J. Simmonds and V. Magar",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Radar Emitter Signals Recognition and Classification with Feedforward Networks","A possible application of neural networks for timely and reliable recognition of radar signal emitters is investigated. In particular, a large data set of intercepted generic radar signal samples is used for investigating and evaluating several neural network topologies, training parameters, input and output coding and machine learning facilitating data transformations. Three case studies are discussed, where in the first two the radar signals are classified in two broad classes – with civil or military application, based on patterns in their pulse train characteristics and in the third one trained to distinguish between several more specific radar functions. Very competitive results of about 82%, 84% and 67% are achieved on the testing data sets.","Nedyalko Petrov and Ivan Jordanov and Jon Roe",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Effect of different implementations of the same ice history in GIA modeling","This study shows the effect of changing the way ice histories are implemented in Glacial Isostatic Adjustment (GIA) codes to solve the sea level equation. The ice history models are being constantly improved and are provided in different formats. The overall algorithmic design of the sea-level equation solver often forces to implement the ice model in a representation that differs from the one originally provided. We show that using different representations of the same ice model gives important differences and artificial contributions to the sea level estimates, both at global and at regional scale. This study is not a speculative exercise. The ICE-5G model adopted in this work is widely used in present day sea-level analysis, but discrepancies between the results obtained by different groups for the same ice models still exist, and it was the effort to set a common reference for the sea-level community that inspired this work. Understanding this issue is important to be able to reduce the artefacts introduced by a non-suitable ice model representation. This is especially important when developing new GIA models, since neglecting this problem can easily lead to wrong alignment of the ice and sea-level histories, particularly close to the deglaciation areas, like Antarctica.","V.R. Barletta and A. Bordoni",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Visual tracking based on online sparse feature learning","Various visual tracking approaches have been proposed for robust target tracking, among which using sparse representation of the tracking target yields promising performance. Some earlier works in this line used a fixed subset of features to compress the target's appearance, which has limited modeling capacity between the target and the background, and could not accommodate their appearance change over long period of time. In this paper, we propose a visual tracking method by modeling targets with online-learned sparse features. We first extract high dimensional Haar-like features as an over-completed basis set, and then solve the feature selection problem in an efficient L1-regularized sparse-coding process. The selected low-dimensional representation best discriminates the target from its neighboring background. Next we use a naive Bayesian classifier to select the most-likely target candidate by a binary classification process. The online feature selection process happens when there are significant appearance changes identified by a thresholding strategy. In this way, our proposed method could work for long tracking tasks. At the same time, our comprehensive experimental evaluation has shown that the proposed methods achieve excellent running speed and higher accuracy over many state-of-the-art approaches.","Zelun Wang and Jinjun Wang and Shun Zhang and Yihong Gong",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"#Globalhealth Twitter Conversations on #Malaria, #HIV, #TB, #NCDS, and #NTDS: a Cross-Sectional Analysis","Background
Advocates use the hashtag #GlobalHealth on Twitter to draw users' attention to prominent themes on global health, to harness their support, and to advocate for change.
Objectives
We aimed to describe #GlobalHealth tweets pertinent to given major health issues.
Methods
Tweets containing the hashtag #GlobalHealth (N = 157,951) from January 1, 2014, to April 30, 2015, were purchased from GNIP Inc. We extracted 5 subcorpora of tweets, each with 1 of 5 co-occurring disease-specific hashtags (#Malaria, #HIV, #TB, #NCDS, and #NTDS) for further analysis. Unsupervised machine learning was applied to each subcorpus to categorize the tweets by their underlying topics and obtain the representative tweets of each topic. The topics were grouped into 1 of 4 themes (advocacy; epidemiological information; prevention, control, and treatment; societal impact) or miscellaneous. Manual categorization of most frequent users was performed. Time zones of users were analyzed.
Findings
In the entire #GlobalHealth corpus (N = 157,951), there were 40,266 unique users, 85,168 retweets, and 13,107 unique co-occurring hashtags. Of the 13,087 tweets across the 5 subcorpora with co-occurring hashtag #malaria (n = 3640), #HIV (n = 3557), #NCDS (noncommunicable diseases; n = 2373), #TB (tuberculosis; n = 1781), and #NTDS (neglected tropical diseases; n = 1736), the most prevalent theme was prevention, control, and treatment (4339, 33.16%), followed by advocacy (3706, 28.32%), epidemiological information (1803, 13.78%), and societal impact (1617, 12.36%). Among the top 10 users who tweeted the highest number of tweets in the #GlobalHealth corpus, 5 were individual professionals, 3 were news media, and 2 were organizations advocating for global health. The most common users' time zone was Eastern Time (United States and Canada).
Conclusions
This study highlighted the specific #GlobalHealth Twitter conversations pertinent to malaria, HIV, tuberculosis, noncommunicable diseases, and neglected tropical diseases. These conversations reflect the priorities of advocates, funders, policymakers, and practitioners of global health on these high-burden diseases as they presented their views and information on Twitter to their followers.","Isaac Chun-Hai Fung and Ashley M. Jackson and Jennifer O. Ahweyevu and Jordan H. Grizzle and Jingjing Yin and Zion Tsz Ho Tse and Hai Liang and Juliet N. Sekandi and King-Wa Fu",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Species delimitation and phylogenetic reconstruction of the sinipercids (Perciformes: Sinipercidae) based on target enrichment of thousands of nuclear coding sequences","The sinipercids are freshwater fishes endemic to East Asia, mainly in China. Phylogenetic studies on the sinipercids have made great progress in the last decades, but interspecific relationships and evolutionary history of the sinipercids remain unresolved. Lack of distinctive morphological characters leads to problems in validating of some species, such as Siniperca loona. Moreover, genetic data are needed to delimitate species pairs with explicit hypothesis testing, such as in S. chuatsi vs. S. kneri and Coreoperca whiteheadi vs. C. liui. Here we reconstructed phylogeny of the sinipercids with an unprecedented scale of data, 16,943 loci of single-copy coding sequence data from nine sinipercid species, eight putative sister taxa and two outgroups. Targeted sequences were collected using gene enrichment and Illumina sequencing, yielding thousands of protein coding sequences and single nucleotide polymorphisms (SNPs) data. Maximum likelihood and coalescent species tree analyses resulted in identical and highly supported trees. We confirmed that the centrarchids are sister to the sinipercids. A monophyletic Sinipercidae with two genera, Siniperca and Coreoperca was also supported. Different from most previous studies, S. scherzeri was found as the most basal taxon to other species of Siniperca, which consists of two clades: a clade having S. roulei sister to S. chuatsi and S. kneri, and a clade consisting S. loona sister to S. obscura and S. undulata. We found that both S. loona and C. liui are valid species using Bayes factor delimitation (BFD∗) based on SNPs data. Species delimitation also provided decisive support for S. chuatsi and S. kneri being two distinct species. We calibrated a chronogram of the sinipercids based on 100 loci and three fossil calibration points using BEAST, and reconstructed ancestral ranges of the sinipercids using Lagrange Analysis (DEC model) and Statistical Dispersal-Vicariance Analysis (S-DIVA) implemented in RASP. Divergence time estimates and ancestral habitat reconstruction suggested a wide-ranging distribution of the common ancestor of the sinipercids in southern China at 53.1 million years ago (CI: 30.4–85.8Ma). The calibrated time tree is consistent with historical climate changes and geological events that might have shaped the current distribution of the sinipercids.","Shuli Song and Jinliang Zhao and Chenhong Li",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Contemporary evolution and genetic change of prey as a response to predator removal","The ecological effects of predator removal and its consequence on prey behavior have been investigated widely; however, predator removal can also cause contemporary evolution of prey resulting in prey genetic change. Here we tested the role of predator removal on the contemporary evolution of prey traits such as movement, reproduction and foraging. We use EcoSim simulation which allows complex intra- and inter-specific interactions, based on individual evolving behavioral models, as well as complex predator–prey dynamics and coevolution in spatially homogenous and heterogeneous worlds. We model organisms' behavior using fuzzy cognitive maps (FCM) that are coded in their genomes which has a clear semantics making reasoning about causality of any evolved behavior possible. We show that the contemporary evolution of prey behavior owing to predator removal is also accompanied by prey genetic change. We employed machine learning methods, now recognized as holding great promise for the advancement of our understanding and prediction of ecological phenomena. A classification algorithm was used to demonstrate the difference between genomes belonging to prey coevolving with predators and prey evolving in the absence of predation pressure. We argue that predator introductions to naive prey might be destabilizing if prey have evolved and adapted to the absence of predators. Our results suggest that both predator introductions and predator removal from an ecosystem have widespread effects on the survival and evolution of prey by altering their genomes and behavior, even after relatively short time intervals. Our study highlights the need to consider both ecological and evolutionary time scales, as well as the complex interplay of behaviors between trophic levels, in determining the outcomes of predator–prey interactions.","Marwa Khater and Dorian Murariu and Robin Gras",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Cognitive Load Management of Cultural Heritage Information: An Application Multi-Mix for Recreational Learners","Under the umbrella of the SEE/B/0016/4.3/X Project SAGITTARIUS a new cultural heritage infrastructure has been introduced in South East Europe. The main aim is to facilitate cognitive-emotional experiences in cultural heritage environments by effectively communicating cultural values to non-captives audiences. A 3-component Roving Museum (RM) is operated in seven countries (GR, BG, HR, HU, IT, RO). The RM adapts to visitor needs in a constantly changing knowledge ecosystem implementing new ways of recreational learning and visitor satisfaction. It includes a QRC-driven portable exhibition with 110 cultural heritage narratives, accessible via QRCs in the territory, an app for iOS and Android, and a social media driven participatory space, to support contextual co-creation and participatory learning. A cognitive driven communication pattern has been developed and adapted to the conditions regulating the recreational learning environment. The pattern employs interrelated content segments in order to free the working memory (WM) from irrelevant cognitive loads, enabling new cognitive content to relate to prior knowledge. The design presupposes a limited WM capacity to deal with visual, auditory and verbal material, and an almost unlimited long-term memory (LTM), able to hold mental representations that vary in their degree of automation. It considers WM constraints, element interactivity and 3 types of cognitive loads (CL). Cognitive accessibility is ensured through provision of novelty and variety, surprise and exploration, strictly avoiding engagement in complex cognitive procedures.","Dorothea Papathanassiou-Zuhrt",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Creation of a personality garden—A tool for reflection and teacher development; an autoethnographical research paper","Background
This paper focuses on the Creation of a Personality Garden as a development tool. The original concept of the Garden was born from an autoethnographical study on the effects of self-concept on the teaching and learning experience.
Objectives
To explore the effects of self-concept on the teaching and learning experience.
Design
An autoethnographical study.
Setting
The study was undertaken in London, UK.
Participants
The researcher was also the sole participant in line with the autoethnographical approach.
Methods
Data was collected through the means of a reflective diary, personal memory data, interview and other creative genres. A thematic analysis approach was then used to code and group core concepts.
Results
Three key areas were identified: emotional connection, growth, and resilience, with a fourth as an over-arching driver for the study; the audience and act of teaching. These elements appeared to underpin a teaching philosophy which recognises the benefits of self-awareness in teachers and an ability and willingness to connect with learners and respond to individual needs. The Garden was one element of self-reflective data which was later re-designed to embrace the personal transformation of the researcher throughout the study.
Conclusions
Educationalists must have a willingness to explore self-perception as it can facilitate a sense of transparency and connection between the teacher and the learner. The Garden works as a dynamic tool and a sustainable model for confronting the on-going challenges of embracing risk-taking and emotionally connecting with learners within the educational context. It allows exploration of the nuances of personality and how the uniqueness of self interacts with the role of the teacher; a sometimes uncomfortable, yet safe, place to sit and experience a virtual reality check questioning assumptions and the theories that the individual espouses to use.","Tracey O'Keeffe",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The Ndynamics package—Numerical analysis of dynamical systems and the fractal dimension of boundaries","A set of Maple routines is presented, fully compatible with the new releases of Maple (14 and higher). The package deals with the numerical evolution of dynamical systems and provide flexible plotting of the results. The package also brings an initial conditions generator, a numerical solver manager, and a focusing set of routines that allow for better analysis of the graphical display of the results. The novelty that the package presents an optional C interface is maintained. This allows for fast numerical integration, even for the totally inexperienced Maple user, without any C expertise being required. Finally, the package provides the routines to calculate the fractal dimension of boundaries (via box counting). New version program summary Program Title: Ndynamics Catalogue identifier: %Leave blank, supplied by Elsevier. Licensing provisions: no. Programming language: Maple, C. Computer: Intel(R) Core(TM) i3 CPU M330 @ 2.13 GHz. Operating system: Windows 7. RAM: 3.0 GB Keywords: Dynamical systems, Box counting, Fractal dimension, Symbolic computation, Differential equations, Maple. Classification: 4.3. Catalogue identifier of previous version: ADKH_v1_0. Journal reference of previous version: Comput. Phys. Commun. 119 (1999) 256. Does the new version supersede the previous version?: Yes. Nature of problem Computation and plotting of numerical solutions of dynamical systems and the determination of the fractal dimension of the boundaries. Solution method The default method of integration is a fifth-order Runge–Kutta scheme, but any method of integration present on the Maple system is available via an argument when calling the routine. A box counting [1] method is used to calculate the fractal dimension [2] of the boundaries. Reasons for the new version The Ndynamics package met a demand of our research community for a flexible and friendly environment for analyzing dynamical systems. All the user has to do is create his/her own Maple session, with the system to be studied, and use the commands on the package to (for instance) calculate the fractal dimension of a certain boundary, without knowing or worrying about a single line of C programming. So the package combines the flexibility and friendly aspect of Maple with the fast and robust numerical integration of the compiled (for example C) basin. The package is old, but the problems it was designed to dealt with are still there. Since Maple evolved, the package stopped working, and we felt compelled to produce this version, fully compatible with the latest version of Maple, to make it again available to the Maple user. Summary of revisions Deprecated Maple Packages and Commands: Paraphrasing the Maple in-built help files, “Some Maple commands and packages are deprecated. A command (or package) is deprecated when its functionality has been replaced by an improved implementation. The newer command is said to supersede the older one, and use of the newer command is strongly recommended”. So, we have examined our code to see if some of these occurrences could be dangerous for it. For example, the “readlib” command is unnecessary, and we have removed its occurrences from our code. We have checked and changed all the necessary commands in order for us to be safe in respect to danger from this source. Another change we had to make was related to the tools we have implemented in order to use the interface for performing the numerical integration in C, externally, via the use of the Maple command “ssystem”. In the past, we had used, for the external C integration, the DJGPP system. But now we present the package with (free) Borland distribution. The compilation and compiling commands are now slightly changed. For example, to compile only, we had used “gcc-c”; now, we use “bcc32-c”, etc. All this installation (Borland) is explained on a “README” file we are submitting here to help the potential user. Restrictions Besides the inherent restrictions of numerical integration methods, this version of the package only deals with systems of first-order differential equations. Unusual features This package provides user-friendly software tools for analyzing the character of a dynamical system, whether it displays chaotic behaviour, and so on. Options within the package allow the user to specify characteristics that separate the trajectories into families of curves. In conjunction with the facilities for altering the user’s viewpoint, this provides a graphical interface for the speedy and easy identification of regions with interesting dynamics. An unusual characteristic of the package is its interface for performing the numerical integrations in C using a fifth-order Runge–Kutta method (default). This potentially improves the speed of the numerical integration by some orders of magnitude and, in cases where it is necessary to calculate thousands of graphs in regions of difficult integration, this feature is very desirable. Besides that tool, somewhat more experienced users can produce their own C integrator and, by using the commands available in the package, use it as the C integrator provided with the package as long as the new integrator manages the input and output in the same format as the default one does. Running time This depends strongly on the dynamical system. With an Intel® Core™ i3 CPU M330 @ 2.13 GHz, the integration of 50 graphs, for a system of two first-order equations, typically takes less than a second to run (with the C integration interface). Without the C interface, it takes a few seconds. In order to calculate the fractal dimension, where we typically use 10,000 points to integrate, using the C interface it takes from 20 to 30 s. Without the C interface, it becomes really impractical, taking, sometimes, for the same case, almost an hour. For some cases, it takes many hours.","J. Avellar and L.G.S. Duarte and L.A.C.P. da Mota and N. de Melo and J.E.F. Skea",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Using a community of inquiry framework to teach a nursing and midwifery research subject: An evaluative study","Background
Postgraduate nursing students' negative perceptions about a core research subject at an Australian university led to a revision and restructure of the subject using a Communities of Inquiry framework. Negative views are often expressed by nursing and midwifery students about the research process. The success of evidence-based practice is dependent on changing these views. A Community of Inquiry is an online teaching, learning, thinking, and sharing space created through the combination of three domains—teacher presence (related largely to pedagogy), social presence, and cognitive presence (critical thinking).
Objectives
Evaluate student satisfaction with a postgraduate core nursing and midwifery subject in research design, theory, and methodology, which was delivered using a Communities of Inquiry framework.
Setting, Participants, and Methods
This evaluative study incorporated a validated Communities of Inquiry survey (n=29) and interviews (n=10) and was conducted at an Australian university. Study participants were a convenience sample drawn from 56 postgraduate students enrolled in a core research subject. Survey data were analysed descriptively and interviews were coded thematically.
Results
Five main themes were identified: subject design and delivery; cultivating community through social interaction; application—knowledge, practice, research; student recommendations; and technology and technicalities. Student satisfaction was generally high, particularly in the areas of cognitive presence (critical thinking) and teacher presence (largely pedagogy related). Students' views about the creation of a “social presence” were varied but overall, the framework was effective in stimulating both inquiry and a sense of community.
Conclusions
The process of research is, in itself, the creation of a “community of inquiry.” This framework showed strong potential for use in the teaching of nurse research subjects; satisfaction was high as students reported learning, not simply the theory and the methods of research, but also how to engage in “doing” research by forging professional and intellectual communities.","Jane Mills and Karen Yates and Helena Harrison and Cindy Woods and Jennifer Chamberlain-Salaun and Scott Trueman and Marnie Hitchins",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Expectation and futurity: The remarkable success of genetic determinism","Genetic determinism is nowadays largely questioned and widely criticized. However, if we look at the history of biology in the last one hundred years, we realize that genetic determinism has always been controversial. Why, then, did it acquire such relevance in the past despite facing longstanding criticism? Through the analysis of some of the ambitious expectations of future scientific applications, this article explores the possibility that part of the historical success of genetic determinism lies in the powerful rhetorical strategies that have connected the germinal matter with alluring bio-technological visions. Indeed, in drawing on the recent perspectives of “expectation studies” in science and technology, it will be shown that there has been an interesting historical relationship between reductionist notions of the gene as a hereditary unit, coded information or functional DNA segment, and startling prophecies of what controlling such an entity might achieve. It will also be suggested that the well-known promissory nature of genomics is far older than the emergence of biotechnology in the 1970s. At least from the time of the bio-utopias predicted by J.B.S. Haldane and J. S. Huxley, the gene has often been surrounded by what I call the “rhetoric of futurity”: a promissory rhetoric that, despite momentous changes in the life sciences throughout the 20th century, has remained relatively consistent over time.","Maurizio Esposito",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"High-performance Supercomputing as a Risk Evaluation Tool for Geologic Carbon Dioxide Storage","Numerical modelling is a vital tool for predicting the behavior and fate of CO2 in reservoirs as well as its impacts on subsurface environments. Recently, powerful numerical codes that are capable of solving coupled complex processes of physics and chemistry required for such modeling works have been developed. However they are often computationally demanding for solving the complex non-linear models in sufficient spatial and temporal resolutions. Geological heterogeneity and uncertainties further increase the challenges in modeling work, because they may necessitate stochastic modeling with multiple realizations. There is clearly a need for high-performance computing. In this study, we implemented TOUGH2-MP code (a parallel version of multi-phase flow simulator TOUGH2) on two different types (vector- and scalar-type) of world-class supercomputers with tens of thousands of processors in Japan. The parallelized code generally exhibited excellent performance and scalability after adequate tune-ups of the code. Using the code and the supercomputers, we have been performed several computationally demanding simulations. In this paper, we present the performances of parallel computation of the code measured on the two supercomputers. Then the following two examples are presented: 1) a highly heterogeneous high-resolution model, representing irregular nature of sand/shale distribution; 2) “Dissolution Diffusion Convection Process”, which is expected to significantly enhance the dissolution trapping. Through the above two examples, it is illustrated that the spatial resolution of numerical model can critically change the evaluation of the effectiveness of CO2 trapping mechanisms, demonstrating the necessity of supercomputing techniques for evaluating these risks more accurately.","Hajime Yamamoto and Shinichi Nanai and Keni Zhang and Pascal Audigane and Christophe Chiaberge and Ryusei Ogata and Noriaki Nishikawa and Yuichi Hirokawa and Satoru Shingu and Kengo Nakajima",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Multi-Connection Pattern Analysis: Decoding the representational content of neural communication","The lack of multivariate methods for decoding the representational content of interregional neural communication has left it difficult to know what information is represented in distributed brain circuit interactions. Here we present Multi-Connection Pattern Analysis (MCPA), which works by learning mappings between the activity patterns of the populations as a factor of the information being processed. These maps are used to predict the activity from one neural population based on the activity from the other population. Successful MCPA-based decoding indicates the involvement of distributed computational processing and provides a framework for probing the representational structure of the interaction. Simulations demonstrate the efficacy of MCPA in realistic circumstances. In addition, we demonstrate that MCPA can be applied to different signal modalities to evaluate a variety of hypothesis associated with information coding in neural communications. We apply MCPA to fMRI and human intracranial electrophysiological data to provide a proof-of-concept of the utility of this method for decoding individual natural images and faces in functional connectivity data. We further use a MCPA-based representational similarity analysis to illustrate how MCPA may be used to test computational models of information transfer among regions of the visual processing stream. Thus, MCPA can be used to assess the information represented in the coupled activity of interacting neural circuits and probe the underlying principles of information transformation between regions.","Yuanning Li and Robert Mark Richardson and Avniel Singh Ghuman",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Rates of molecular evolution vary in vertebrates for insulin-like growth factor-1 (IGF-1), a pleiotropic locus that regulates life history traits","Insulin-like growth factor-1 (IGF-1) is a member of the vertebrate insulin/insulin-like growth factor/relaxin gene family necessary for growth, reproduction, and survival at both the cellular and organismal level. Its sequence, protein structure, and function have been characterized in mammals, birds, and fish; however, a notable gap in our current knowledge of the function of IGF-1 and its molecular evolution is information in ectothermic reptiles. To address this disparity, we sequenced the coding region of IGF-1 in 11 reptile species—one crocodilian, three turtles, three lizards, and four snakes. Complete sequencing of the full mRNA transcript of a snake revealed the Ea-isoform, the predominant isoform of IGF-1 also reported in other vertebrate groups. A gene tree of the IGF-1 protein-coding region that incorporated sequences from diverse vertebrate groups showed similarity to the species phylogeny, with the exception of the placement of Testudines as sister group to Aves, due to their high nucleotide sequence similarity. In contrast, long-branch lengths indicate more rapid divergence in IGF-1 among lizards and snakes. Additionally, lepidosaurs (i.e., lizards and snakes) had higher rates of non-synonymous:synonymous substitutions (dN/dS) relative to archosaurs (i.e., birds and crocodilians) and turtles. Tests for positive selection on specific codons within branches and evaluation of the changes in the amino acid properties, suggested positive selection in lepidosaurs on the C domain of IGF-1, which is involved in binding affinity to the IGF-1 receptor. Predicted structural changes suggest that major alterations in protein structure and function may have occurred in reptiles. These data propose new insights into the molecular co-evolution of IGF-1 and its receptors, and ultimately the evolution of IGF-1’s role in regulating life-history traits across vertebrates.","Amanda M. Sparkman and Tonia S. Schwartz and Jill A. Madden and Scott E. Boyken and Neil B. Ford and Jeanne M. Serb and Anne M. Bronikowski",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Identifying the sources of technological novelty in the process of invention","Much work on technological change agrees that the combination of new and existing technological capabilities is one of the principal sources of inventive novelty, and that there have been instances in history when new inventions appear with few antecedents. The many discussions across research communities regarding the relative roles of combination and origination as sources of technological novelty have not provided much in the way of formal identification and quantification. By taking advantage of the technology codes used by the U.S. Patent Office to classify patents, we discretize technologies and identify four distinct sources of technological novelty. The resulting technological novelty taxonomy is then used to assess the relative importance of refining existing technologies, combining existing and new technologies, and de novo creation of technological capabilities as sources of new inventions. Our results clearly show that the process of invention has been primarily a combinatorial process accompanied by rare occurrences of technological origination. The importance of reusing existing technological capabilities to generate inventions has been steadily rising and recently overtook recombination as the source of novelty for most new inventions.","Deborah Strumsky and José Lobo",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Consumers, citizens or citizen-consumers? Domestic users in the process of Estonian electricity market liberalization","This study analyses the development of domestic customers’ energy-related performances and understandings during the transition from centralized, monopolist electricity provision to a liberal market in Estonia in 2012–2013. Liberalization brought about not only a new institutional and legal framework for electricity purchases, but also a significant price rise. The study proceeds from the assumption that these structural changes catalyzed shifts in the electricity-related practices of consumers and citizens. Theoretically, the analysis builds on practice theory and positioning analysis. We investigated how domestic electricity purchasing and consumption were positioned in media texts and consumers’ self-positionings vis-à-vis media discourse, including their resistance to what was interpreted as acceptable conduct. To code media texts, diaries and interviews, we employed the concept of performance positioning. The results reveal that learning the new social practice of purchasing electricity as a service made electricity more salient and visible, both as an object of consumption and as an object of media-inspired public discussion and critique, encouraging a search for new solutions, such as collective mobilization to bargain with suppliers. The paper’s further contribution lies in discussing the issue of a supportive communicative environment in the development of citizen-consumer self-positionings conducive to socially innovative forms of energy governance and usage.","Triin Vihalemm and Margit Keller",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An advanced combustion model coupled with detailed chemical reaction mechanism for D.I diesel engine simulation","A multi-dimensional computational fluid dynamics (CFD) modeling was conducted on a direct injection turbo-charged diesel engine based on KIVA-4 code under full and mid engine loads. Multi-component fuel evaporation model of KIVA-4 was used and coupled with advanced combustion chemistry to generate a multi-component fuel combustion model by integrating CHEMKIN II into the KIVA-4 code. As the coding schema of KIVA-4 in the case of data/parameter allocation, etc. was different compared to previous version of KIVA-3V, a considerable amount of FORTRAN programming was performed in order to develop a multi-component fuel combustion model. The developed combustion model was capable of modeling combustion process of number of chemical species as the components of direct injected liquid fuel. Comparing to the single component fuel combustion model, new model is capable of comprehensive combustion modeling of blend fuel and heavy hydro-carbon fuels. Furthermore, spray breakup and collision models were changed to more advanced Kelvin–Helmholtz and Rayleigh–Taylor (KH–RT) and O’Rourke models, respectively. The model was used to simulate direct injected diesel engine under full and mid engine loads at three engine speed conditions. Extracted temporal and spatial results for equivalence ratio distribution inside the combustion chamber showed that under full load condition, a considerable amount of fuel was trapped in piston bowl after initiation of the injection process where such fuel rich local regions provide the potential for production of higher soot emission. Mean value of the fuel concentration history showed that the ignition delay was increased under mid engine load at all engine speeds producing higher amounts of unburned hydro carbons and carbon monoxide. By reducing engine load and speed, output power was decreased as well. However, same trend was not reported for the indicated thermal efficiency as the middle engine speed in considered engine loads, had slightly higher efficiency.","Amin Maghbouli and Wenming Yang and Hui An and Jing Li and Siaw Kiang Chou and Kian Jon Chua",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Chronic toxicity evaluation of the flame retardant tris (2-butoxyethyl) phosphate (TBOEP) using Daphnia magna transcriptomic response","Tris (2-butoxyethyl) phosphate (TBOEP) is an organophosphorous-containing flame retardant (OPFR) of high production volume used in a broad range of applications. The use of TBOEP containing products has resulted in its release and ubiquitous occurrence in the aquatic environment. In this study, Daphnia magna transcriptomic response was measured by microarray to evaluate sublethal effects of TBOEP as part of a multi-level biological approach including specific gene transcription measured by qRT-PCR, enzyme activity, and life-history endpoints (i.e., survival, growth and reproduction). Chronic exposure (21 d) to a range of sublethal concentrations of TBOEP (14.7–1470μgL−1) did not impact growth, survival or reproduction, although the number of offspring decreased between the lowest and the highest dose. Gene transcription profiling by microarray analysis revealed that 101 genes were differentially transcribed in response to TBOEP (fold change treated/control ±1, p<0.05). Most of the responding genes were involved in protein metabolism (9), biosynthesis (4) and energy metabolism (6) indicating that TBOEP could have chronic toxic effects on aquatic organisms at sublethal doses by disrupting essential biological pathways. Nine genes were found to be commonly affected by more than one dose, including a gene coding for cathepsin D and multiple isoforms of genes coding for hemoglobin, suggesting potential biomarkers of interest. Microarray results were confirmed by qRT-PCR and measurements at the protein level as cathepsin D enzymatic activity increased significantly in the highest dose treatment. Results highlight the relevance of using the transcriptomic response of D. magna as a first line of evidence to unravel the mode of action of chemicals.","Maeva Giraudo and Mélanie Douville and Magali Houde",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR8, CR9, CR10"
"A Perceptual Inference Mechanism for Hallucinations Linked to Striatal Dopamine","Summary
Hallucinations, a cardinal feature of psychotic disorders such as schizophrenia, are known to depend on excessive striatal dopamine. However, an underlying cognitive mechanism linking dopamine dysregulation and the experience of hallucinatory percepts remains elusive. Bayesian models explain perception as an optimal combination of prior expectations and new sensory evidence, where perceptual distortions such as illusions and hallucinations may occur if prior expectations are afforded excessive weight. Such excessive weight of prior expectations, in turn, could stem from a gain-control process controlled by neuromodulators such as dopamine. To test for such a dopamine-dependent gain-control mechanism of hallucinations, we studied unmedicated patients with schizophrenia with varying degrees of hallucination severity and healthy individuals using molecular imaging with a pharmacological manipulation of dopamine, structural imaging, and a novel task designed to measure illusory changes in the perceived duration of auditory stimuli under different levels of uncertainty. Hallucinations correlated with a perceptual bias, reflecting disproportional gain on expectations under uncertainty. This bias could be pharmacologically induced by amphetamine, strongly correlated with striatal dopamine release, and related to cortical volume of the dorsal anterior cingulate, a brain region involved in tracking environmental uncertainty. These findings outline a novel dopamine-dependent mechanism for perceptual modulation in physiological conditions and further suggest that this mechanism may confer vulnerability to hallucinations in hyper-dopaminergic states underlying psychosis.","Clifford M. Cassidy and Peter D. Balsam and Jodi J. Weinstein and Rachel J. Rosengard and Mark Slifstein and Nathaniel D. Daw and Anissa Abi-Dargham and Guillermo Horga",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The first draft genome of Lophophorus: A step forward for Phasianidae genomic diversity and conservation","The monal genus (Lophophorus) is a branch of Phasianidae and its species inhabit the high-altitude mountains of the Qinghai-Tibet Plateau. The Chinese monal, L. lhuysii, is a threatened endemic bird of China that possesses high-altitude adaptability, diversity of plumage color and potentially low reproductive life history. This is the first study to describe the monal genome using next generation sequencing technology. The Chinese monal genome size is 1.01 Gb, with 16,940 protein-coding genes. Gene annotation yielded 100.93 Mb (9.97%) repeat elements, 785 ncRNA, 5,465,549 bp (0.54%) SSR and 15,550 (92%) genes in public databases. Compared to other birds and mammals, the genome evolution analysis showed numerous expanded gene families and positive selected genes involved in high-altitude adaptation, especially related to the adaptation of low temperature and hypoxia. Consequently, this gene data can be used to investigate the molecular evolution of high-altitude adaptation in future bird research. Our first published genome of the genus Lophophorus will be integral for the study of monal population genetic diversity and conservation, genomic evolution and Galliformes species differentiation in the Qinghai-Tibetan Plateau.","Kai Cui and Wujiao Li and Jake George James and Changjun Peng and Jiazheng Jin and Chaochao Yan and Zhenxin Fan and Lianming Du and Megan Price and Yongjie Wu and Bisong Yue",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Venous Thromboembolism in Female Adolescents: Patient Characteristics","Study Objective
Our goal was to describe the period prevalence of venous thromboembolism (VTE) and characterize adolescent female patients diagnosed with VTE by describing their age, race, and number of comorbidities. Female adolescents with estrogen exposure were of particular interest because estrogen-containing contraceptives increase the risk of VTE.
Design, Setting, Participants, and Interventions
We queried the Pediatric Health Information System database for International Classification of Diseases, Ninth/Tenth Revision, Clinical Modification codes to identify female patients aged 12-18 years diagnosed with a VTE or pulmonary embolism from April 2006 to March 2016 in the United States. Patient demographic characteristics and comorbidities were also analyzed. We divided our study population into two five-year groups and calculated the change in period prevalence of VTE between those groups.
Main Outcome Measures
Primary diagnosis of VTE in the extremities, or pulmonary embolism.
Results
The period prevalence of VTE increased from 2.3 female adolescents per 10,000 hospitalized children (group 1) to 3.3 per 10,000 (group 2), representing a statistically significant increase of 0.010% (P < .001). Caucasian and black individuals were most commonly affected. The number of girls affected increased steadily from ages 12 to 16 years and a large percentage (59.6%) had four or more comorbidities. In patients (n = 32) with estrogen exposure, more than 96% had one or more comorbidity in addition to estrogen exposure.
Conclusion
Pediatric health care providers should be aware that the period prevalence of VTEs in female adolescents is increasing. Those with a history of estrogen exposure rarely develop VTEs from estrogen alone and they typically have multiple comorbidities.","Catherine A. Hennessey and Vrunda K. Patel and Eshetu A. Tefera and Veronica Gomez-Lobo",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Assimilation of Perimeter Data and Coupling with Fuel Moisture in a Wildland Fire–Atmosphere DDDAS","We present a methodology to change the state of the Weather Research Forecasting (WRF) model coupled with the fire spread code SFIRE, based on Rothermel's formula and the level set method, and with a fuel moisture model. The fire perimeter in the model changes in response to data while the model is running. However, the atmosphere state takes time to develop in response to the forcing by the heat ﬂux from the fire. Therefore, an artificial fire history is created from an earlier fire perimeter to the new perimeter, and replayed with the proper heat ﬂuxes to allow the atmosphere state to adjust. The method is an extension of an earlier method to start the coupled fire model from a developed fire perimeter rather than an ignition point. The level set method can be also used to identify parameters of the simulation, such as the fire spread rate. The coupled model is available from openwfm.org, and it extends the WRF-Fire code in WRF release.","Jan Mandel and Jonathan D. Beezley and Adam K. Kochanski and Volodymyr Y. Kondratenko and Minjeong Kim",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Simulation of wear evolution using fictitious eigenstrains","A numerical algorithm is described in which changes of geometry caused by wear can be simulated by ascribing fictitious anisotropic eigenstrains to a set of surface nodes. These eigenstrains are related to the wear depth at any location, and can be incorporated into calculations using the existing expansion modules in commercial codes, such as anisotropic swelling or thermal expansion. The proposed algorithm has been implemented, and the results compared to those from a conventional wear model involving periodic re-meshing. It has been shown to be accurate and efficient. The method provides an alternative to the re-meshing technique, and may provide advantages in history-dependent problems such as those involving plasticity, hysteretic friction or micro-slip.","Zupan Hu and Wei Lu and M.D. Thouless and J.R. Barber",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Survey on specific nursing competences: Students' perceptions","Introduction
The nursing profession requires sophisticated interdisciplinary knowledge and skills, which is why numerous nursing curricula are being developed all across the world. The aim of the study was to assess students' perspectives about competences, defined by the Tuning project, they acquired and developed since enrolment in the undergraduate nursing study programme.
Methods
A survey was performed amongst 69 postgraduate Master degree students at the Faculty of Health Sciences University of Maribor, Slovenia (nursing graduates) and the research results were analysed using conventional statistical and correspondence analysis.
Results
Most of the participants felt that they are more competent in their awareness of different roles, responsibilities and functions of a nurse together with the ability to practice within the context of professional, ethical, regulatory and legal codes. However they felt less competent in leadership, management, and team competences.
Conclusions
According to the students' perceptions the current Nursing curriculum should be more future oriented and needs some core changes regarding the scope and level competences taught.","Helena Blažun and Peter Kokol and Janez Vošner",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Extension of TOUGH-FLAC to the finite strain framework","The TOUGH-FLAC simulator for coupled thermal-hydraulic-mechanical processes modeling has been extended to the finite strain framework. In the approach selected, this extension has required modifications to the flow simulator (TOUGH2) and to the coupling scheme between the geomechanics and the flow sub-problems. In TOUGH2, the mass and energy balance equations have been extended to account for volume changes. Additionally, as large deformations are computed by FLAC3D, the geometry is updated in the flow sub-problem. The Voronoi partition needed in TOUGH2 is computed using an external open source library (Voro++) that uses the centroids of the deformed geomechanics mesh as generators of the Voronoi diagram. TOUGH-FLAC in infinitesimal and finite strain frameworks is verified against analytical solutions and other approaches to couple flow and geomechanics. Within the finite strain framework, TOUGH-FLAC is also successfully applied to a large-scale case. The extension of TOUGH-FLAC to the finite strain framework has little impact to the user as only one additional executable is needed (for Voro++), and the input files and the workflow of a simulation are the same as in standard TOUGH-FLAC. With this new provision for finite strains, TOUGH-FLAC can be used in the analysis of a wider range of engineering problems, and the areas of application of this simulator are therefore broadened.","Laura Blanco-Martín and Jonny Rutqvist and Jens T. Birkholzer",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Playing real-time strategy games by imitating human players’ micromanagement skills based on spatial analysis","Unlike the situation with board games, artificial intelligence (AI) for real-time strategy (RTS) games usually suffers from numerous possible future outcomes because the state of the game is continuously changing in real time. Furthermore, AI is also required to be able to handle the increased complexity within a small amount of time. This constraint makes it difficult to build AI for RTS games with current state-of-the art intelligence techniques. A human player, on the other hand, is proficient in dealing with this level of complexity, making him a better game player than AI bots. Human players are especially good at controlling many units at the same time. It is hard to explain the micro-level control skills needed with only a few rules programmed into the bots. The design of micromanagement skills is one of the most difficult parts in the StarCraft AI design because it must be able to handle different combinations of units, army size, and unit placement. The unit control skills can have a big effect on the final outcome of a full game in professional player matches. For StarCraft AI competitions, they employed a relatively simple scripted AI to implement the unit control strategy. However, it is difficult to generate cooperative behavior using the simple AI strategies. Although there has been a few research done on micromanagement skills, it is still a challenging problem to design human-like high-level control skills. In this paper, we proposed the use of imitation learning based on human replays and influence map representation. In this approach, we extracted huge numbers of cases from the replays of experts and used them to determine the actions of units in the current game case. This was done without using any hand-coded rules. Because this approach is data-driven, it was essential to minimize the case search times. To support fast and accurate matching, we chose to use influence maps and data hashing. They allowed the imitation system to respond within a small amount time (one frame, 0.042s). With a very large number of cases (up to 500,000 cases), we showed that it is possible to respond competitively in real-time, with a high winning percentage in micromanagement scenarios.","In-Seok Oh and Hochul Cho and Kyung-Joong Kim",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Specifying model changes with UMLchange to support security verification of potential evolution","In model-based development, quality properties such as consistency of security requirements are often verified prior to code generation. Changed models have to be re-verified before re-generation. If several alternative evolutions of a model are possible, each alternative has to be modeled and verified to find the best model for further development. We present a verification strategy to analyze whether evolution preserves given security properties. The UMLchange profile is used for specifying potential evolutions of a given model simultaneously. We present a tool that reads these annotations and computes a delta containing all possible evolution paths. The paths can be verified wrt. security properties, and for each successfully verified path a new model version is generated automatically.","S. Wenzel and D. Poggenpohl and J. Jürjens and M. Ochoa",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mental Snapshots: Creating an Organized Plan for Health Assessment","Beginning nursing students enter a rapidly moving and changing health care climate. Multiple stimulations can frighten and overwhelm the student's ability to find order of essential patient information. Students need to know how to collect, process, and manage important health data accurately and efficiently in the clinical setting. An integrative method for teaching nursing students to walk into the patient's room and construct a patterned sequence of focused assessments assists students in creating an organized plan for health assessment. The Mental Snapshots Method includes three components for health assessment: (a) sequential assessment steps of the patient; (b) color-coded visual images of the patient representing a bodily condition; and (c) focused assessment questions of primary health complaint(s) with a plan for nursing care. This mental snapshots strategy employs an information processing model of sensory, memory, and motor functioning, which enable students to maintain patient quality and safety.","Susan Curro Fosbrook",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Beam model refinement and reduction","In this paper we present a method for systematic construction of the stiffness matrix of an arbitrary spatial frame element by performing a series of elementary transformations. The procedure of this kind is capable of including a number of element refinements (addition of shear deformation, variable cross-section, etc.) that are not easily accessible to standard displacement-based method. We also discuss the necessary modifications of the element stiffness matrix in order to accommodate different constraints, such as point constraints in terms of joint releases (or hinges) for moments or shear forces. This is obtained by means of model reduction providing a more effective approach than the alternative one in which the global number of degrees of freedom has to be increased by one for each new release. Finally, we elaborate upon the global constraints imposing the length-invariant deformation of frame elements with an arbitrary position in space. Several numerical examples are used to illustrate the performance of the proposed procedures. The computations are carried out by a modified version of computer code CAL.","Senad Medić and Samir Dolarević and Adnan Ibrahimbegovic",2013,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Phylogenetic relationships and the evolution of BMP4 in triggerfishes and filefishes (Balistoidea)","The triggerfishes (family Balistidae) and filefishes (family Monacanthidae) comprise a charismatic superfamily (Balistoidea) within the diverse order Tetraodontiformes. This group of largely marine fishes occupies an impressive ecological range across the world’s oceans, and is well known for its locomotor and feeding diversity, unusual body shapes, small genome size, and ecological and economic importance. In order to investigate the evolutionary history of these important fish families, we used multiple phylogenetic methods to analyze molecular data from 86 species spanning the extant biodiversity of Balistidae and Monacanthidae. In addition to three gene regions that have been used extensively in phylogenetic analyses, we include sequence data for two mitochondrial regions, two nuclear markers, and the growth factor gene bmp4, which is involved with cranial development. Phylogenetic analyses strongly support the monophyly of the superfamily Balistoidea, the sister-family relationship of Balistidae and Monacanthidae, as well as three triggerfish and four filefish clades that are well resolved. A new classification for the Balistidae is proposed based on phylogenetic groups. Bayesian topology, as well as the timing of major cladogenesis events, is largely congruent with previous hypotheses of balistid phylogeny. However, we present a novel topology for major clades in the filefish family that illustrate the genera Aluterus and Stephanolepis are more closely related than previously posited. Molecular rates suggest a Miocene and Oligocene origin for the families Balistidae and Monacanthidae, respectively, and significant divergence of species in both families within the past 5million years. A second key finding of this study is that, relative to the other protein-coding gene regions in our DNA supermatrix, bmp4 shows a rapid accumulation of both synonymous and non-synonymous substitutions, especially within the family Monacanthidae. Overall substitution patterns in bmp4 support the hypothesis of stabilizing selection during the evolutionary history of regulatory genes, with a small number of isolated examples of accelerated non-synonymous changes detected in our phylogeny.","Charlene L. McCord and Mark W. Westneat",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A segmental duplication in the common ancestor of Brassicaceae is responsible for the origin of the paralogs KCS6-KCS5, which are not shared with other angiosperms","Novel morphological structures allowed adaptation to dry conditions in early land plants. The cuticle, one such novelty, plays diverse roles in tolerance to abiotic and biotic stresses and plant development. Cuticular waxes represent a major constituent of the cuticle and are comprised of an assortment of chemicals that include, among others, very long chain fatty acids (VLCFAs). Members of the β-ketoacyl coenzyme A synthases (KCS) gene family code for enzymes that are essential for fatty acid biosynthesis. The gene KCS6 (CUT1) is known to be a key player in the production of VLCFA precursors essential for the synthesis of cuticular waxes in the model plant Arabidopsis thaliana (Brassicaceae). Despite its functional importance, relatively little is known about the evolutionary history of KCS6 or its paralog KCS5 in Brassicaceae or beyond. This lacuna becomes important when we extrapolate understanding of mechanisms gained from the model plant to its containing clades Brassicaceae, flowering plants, or beyond. The Brassicaceae, with several sequenced genomes and a known history of paleoploidy, mesopolyploidy and neopolyploidy, offer a system in which to study the evolution and diversification of the KCS6-KCS5 paralogy. Our phylogenetic analyses across green plants, combined with comparative genomic, microsynteny and evolutionary rates analyses across nine genomes of Brassicaceae, reveal that (1) the KCS6-KCS5 paralogy arose as the result of a large segmental duplication in the ancestral Brassicaceae, (2) the KCS6-KCS5 lineage is represented by a single copy in other flowering plant lineages, (3) the duplicated segments undergo different degrees of retention and loss, and (4) most of the genes in the KCS6 and KCS5 gene blocks (including KCS6 and KCS5 themselves) are under purifying selection. The last also true for most members of the KCS gene family in Brassicaceae, except for KCS8, KCS9 and KCS17, which are under positive selection and may be undergoing functional evolution, meriting further investigation. Overall, our results clearly establish that the ancestral KCS6/5 gene duplicated in the Brassicaceae lineage. It is possible that any specialized functions of KCS5 found in Brassicaceae are either part of a set of KCS6/5 gene functions in the rest of the flowering plants, or unique to Brassicaceae.","Swati Singh and Sandip Das and R. Geeta",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Massive non-planar two-loop four-point integrals with SecDec 2.1","We present numerical results for massive non-planar two-loop box integrals entering heavy quark pair production at NNLO, some of which are not known analytically yet. The results have been obtained with the program SecDec 2.1, based on sector decomposition and contour deformation, in combination with new types of transformations. Among the new features of version 2.1 is also the possibility to evaluate contracted tensor integrals, with no limitation on the rank.
Program Summary
Program title: SecDec 2.1 Catalogue identifier: AEIR_v2_1 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIR_v2_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 127558 No. of bytes in distributed program, including test data, etc.: 2506220 Distribution format: tar.gz Programming language: Wolfram Mathematica, Perl, Fortran/C++. Computer: From a single PC to a cluster, depending on the problem. Operating system: Unix, Linux. RAM: Depends on the complexity of the problem Classification: 4.4, 5, 11.1. Catalogue identifier of previous version: AEIR_v2_0 Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 396 External routines: BASES [1], CUBA [2]. The codes for both are included in the SecDec 2.1 distribution file. Does the new version supersede the previous version?: Yes Nature of problem: Extraction of ultraviolet and infrared singularities from parametric integrals appearing in higher order perturbative calculations in gauge theories. Numerical integration in the presence of integrable singularities (e.g. kinematic thresholds in massive multi-loop integrals). Flexibility to treat non-standard user defined functions. Solution method: Algebraic extraction of singularities in dimensional regularisation using iterated sector decomposition. This leads to a Laurent series in the dimensional regularisation parameter ε, where the coefficients are finite integrals over the unit-hypercube. Those integrals are evaluated numerically by Monte Carlo integration. The integrable singularities are handled by choosing a suitable integration contour in the complex plane, in an automated way. Reasons for new version: Several improvements and new features. Summary of revisions: Extended tensor integral option, flexibility to evaluate non-standard loop integral functions and to skip primary decomposition step, improvements in the user interface and the error treatment. Restrictions: Depending on the complexity of the problem, limited by CPU time or memory. Running time: Between a few minutes and several days, depending on the complexity of the problem. References:[1]S. Kawabata, A New version of the multidimensional integration and event generation package BASES/SPRING, Comput. Phys. Comm. 88 (1995) 309.[2]T. Hahn, CUBA: A Library for multidimensional numerical integration, Comput. Phys. Comm. 168 (2005) 78.","Sophia Borowka and Gudrun Heinrich",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Perceived critical success factors of electronic health record system implementation in a dental clinic context: An organisational management perspective","Background
Electronic health records (EHR) make health care more efficient. They improve the quality of care by making patients’ medical history more accessible. However, little is known about the factors contributing to the successful EHR implementation in dental clinics.
Objectives
This article aims to identify the perceived critical success factors of EHR system implementation in a dental clinic context.
Methods
We used Grounded Theory to analyse data collected in the context of Brunei’s national EHR − the Healthcare Information and Management System (Bru-HIMS). Data analysis followed the stages of open, axial and selective coding.
Results
Six perceived critical success factors emerged: usability of the system, emergent behaviours, requirements analysis, training, change management, and project organisation. The study identified a mismatch between end-users and product owner/vendor perspectives.
Discussion
Workflow changes were significant challenges to clinicians’ confident use, particularly as the system offered limited modularity and configurability. Recommendations are made for all the parties involved in healthcare information systems implementation to manage the change process by agreeing system goals and functionalities through wider consensual debate, and participated supporting strategies realised through common commitment.","Yusof Haji Sidek and Jorge Tiago Martins",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Genomic, Transcriptomic, and Phenomic Variation Reveals the Complex Adaptation of Modern Maize Breeding","The temperate-tropical division of early maize germplasms to different agricultural environments was arguably the greatest adaptation process associated with the success and near ubiquitous importance of global maize production. Deciphering this history is challenging, but new insight has been gained from examining 558 529 single nucleotide polymorphisms, expression data of 28 769 genes, and 662 traits collected from 368 diverse temperate and tropical maize inbred lines in this study. This is a new attempt to systematically exploit the mechanisms of the adaptation process in maize. Our results indicate that divergence between tropical and temperate lines apparently occurred 3400–6700 years ago. Seven hundred and one genomic selection signals and transcriptomic variants including 2700 differentially expressed individual genes and 389 rewired co-expression network genes were identified. These candidate signals were found to be functionally related to stress responses, and most were associated with directionally selected traits, which may have been an advantage under widely varying environmental conditions faced by maize as it was migrated away from its domestication center. Our study also clearly indicates that such stress adaptation could involve evolution of protein-coding sequences as well as transcriptome-level regulatory changes. The latter process may be a more flexible and dynamic way for maize to adapt to environmental changes along its short evolutionary history.","Haijun Liu and Xiaqing Wang and Marilyn L. Warburton and Weiwei Wen and Minliang Jin and Min Deng and Jie Liu and Hao Tong and Qingchun Pan and Xiaohong Yang and Jianbing Yan",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The effects of short-lived radionuclides and porosity on the early thermo-mechanical evolution of planetesimals","The thermal history and internal structure of chondritic planetesimals, assembled before the giant impact phase of chaotic growth, potentially yield important implications for the final composition and evolution of terrestrial planets. These parameters critically depend on the internal balance of heating versus cooling, which is mostly determined by the presence of short-lived radionuclides (SLRs), such as 26Al and 60Fe, as well as the heat conductivity of the material. The heating by SLRs depends on their initial abundances, the formation time of the planetesimal and its size. It has been argued that the cooling history is determined by the porosity of the granular material, which undergoes dramatic changes via compaction processes and tends to decrease with time. In this study we assess the influence of these parameters on the thermo-mechanical evolution of young planetesimals with both 2D and 3D simulations. Using the code family i2elvis/i3elvis we have run numerous 2D and 3D numerical finite-difference fluid dynamic models with varying planetesimal radius, formation time and initial porosity. Our results indicate that powdery materials lowered the threshold for melting and convection in planetesimals, depending on the amount of SLRs present. A subset of planetesimals retained a powdery surface layer which lowered the thermal conductivity and hindered cooling. The effect of initial porosity was small, however, compared to those of planetesimal size and formation time, which dominated the thermo-mechanical evolution and were the primary factors for the onset of melting and differentiation. We comment on the implications of this work concerning the structure and evolution of these planetesimals, as well as their behavior as possible building blocks of terrestrial planets.","Tim Lichtenberg and Gregor J. Golabek and Taras V. Gerya and Michael R. Meyer",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Real Object Mapping Technologies Applied to Marine Engineering Learning Process within a CBL Methodology","A proper operation and maintenance of marine systems require give specific instructions and descriptions focused on the parts of any device. This is usually taught by the use of texts and figure descriptions, but the learning process is not as immersive as reality itself. Augmented reality over real objects with mobile devices can change this learning process into a more immersive and engaging experience for the students. This technology permits the use of instructional information like texts, videos and 3D virtual objects even with animations over real elements. This powerful tool lets the student recognize any drawings and real objects in one step, and also any specific operating and/or maintenance instructions can be given. For creating these augmented reality experiences we pretend to use metaio Creator combined with metaio Toolbox. Metaio GmbH firstly released this free app on October 2012 in the Apple App Store. It lets capturing a real object (mapping) in order to easily create augmented realty experiences using real objects as references. On a first instance, we pretend to map objects like valves, pumps and other piping elements, so small groups of students will be able to observe and study this objects through their own mobile devices using a QR code for object recognition. The understanding of every object by every group of students becomes their challenge. First every group will have to discuss every relevant aspect they have discovered: working principles, operating and maintenance. Secondly they should discuss their results with other team members. Finally they will explain what they have learned to their instructor using the same AR technology.","Carlos Efrén Mora Luis and Antonio Manuel González Marrero",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A large-scale study on the usage of Java’s concurrent programming constructs","In both academia and industry, there is a strong belief that multicore technology will radically change the way software is built. However, little is known about the current state of use of concurrent programming constructs. In this work we present an empirical work aimed at studying the usage of concurrent programming constructs of 2227 real world, stable and mature Java projects from SourceForge. We have studied the usage of concurrent techniques in the most recent versions of these applications and also how usage has evolved along time. The main findings of our study are: (I) More than 75% of the latest versions of the projects either explicitly create threads or employ some concurrency control mechanism. (II) More than half of these projects exhibit at least 47 synchronized methods and 3 implementations of the Runnable interface per 100,000 LoC, which means that not only concurrent programming constructs are used often but they are also employed intensively. (III) The adoption of the java.util.concurrent library is only moderate (approximately 23% of the concurrent projects employ it). (IV) Efficient and thread-safe data structures, such as ConcurrentHashMap, are not yet widely used, despite the fact that they present numerous advantages.","Gustavo Pinto and Weslley Torres and Benito Fernandes and Fernando Castor and Roberto S.M. Barros",2015,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Bio-inspired unsupervised learning of visual features leads to robust invariant object recognition","Retinal image of surrounding objects varies tremendously due to the changes in position, size, pose, illumination condition, background context, occlusion, noise, and non-rigid deformations. But despite these huge variations, our visual system is able to invariantly recognize any object in just a fraction of a second. To date, various computational models have been proposed to mimic the hierarchical processing of the ventral visual pathway, with limited success. Here, we show that the association of both biologically inspired network architecture and learning rule significantly improves the models׳ performance when facing challenging invariant object recognition problems. Our model is an asynchronous feedforward spiking neural network. When the network is presented with natural images, the neurons in the entry layers detect edges, and the most activated ones fire first, while neurons in higher layers are equipped with spike timing-dependent plasticity. These neurons progressively become selective to intermediate complexity visual features appropriate for object categorization. The model is evaluated on 3D-Object and ETH-80 datasets which are two benchmarks for invariant object recognition, and is shown to outperform state-of-the-art models, including DeepConvNet and HMAX. This demonstrates its ability to accurately recognize different instances of multiple object classes even under various appearance conditions (different views, scales, tilts, and backgrounds). Several statistical analysis techniques are used to show that our model extracts class specific and highly informative features.","Saeed Reza Kheradpisheh and Mohammad Ganjtabesh and Timothée Masquelier",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Reverse Replay of Hippocampal Place Cells Is Uniquely Modulated by Changing Reward","Summary
Hippocampal replays are episodes of sequential place cell activity during sharp-wave ripple oscillations (SWRs). Conflicting hypotheses implicate awake replay in learning from reward and in memory retrieval for decision making. Further, awake replays can be forward, in the same order as experienced, or reverse, in the opposite order. However, while the presence or absence of reward has been reported to modulate SWR rate, the effect of reward changes on replay, and on replay direction in particular, has not been examined. Here we report divergence in the response of forward and reverse replays to changing reward. While both classes of replays were observed at reward locations, only reverse replays increased their rate at increased reward or decreased their rate at decreased reward, while forward replays were unchanged. These data demonstrate a unique relationship between reverse replay and reward processing and point to a functional distinction between different directions of replay.
Video Abstract
","R. Ellen Ambrose and Brad E. Pfeiffer and David J. Foster",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Heroin and fentanyl overdoses in Kentucky: Epidemiology and surveillance","Background
The study aims to describe recent changes in Kentucky's drug overdose trends related to increased heroin and fentanyl involvement, and to discuss future directions for improved drug overdose surveillance.
Methods
The study used multiple data sources (death certificates, postmortem toxicology results, emergency department [ED] records, law enforcement drug submissions, and prescription drug monitoring records) to describe temporal, geographic, and demographic changes in drug overdoses in Kentucky.
Results
Fentanyl- and heroin-related overdose death rates increased across all age groups from years 2011 to 2015 with the highest rates consistently among 25–34-year-olds. The majority of the heroin and fentanyl overdose decedents had histories of substantial exposures to legally acquired prescription opioids. Law enforcement drug submission data were strongly correlated with drug overdose ED and mortality data. The 2016 crude rate of heroin-related overdose ED visits was 104/100,000, a 68% increase from 2015 (62/100,000). More fentanyl-related overdose deaths were reported between October, 2015, and September, 2016, than ED visits, in striking contrast with the observed ratio of >10 to 1 heroin-related overdose ED visits to deaths. Many fatal fentanyl overdoses were associated with heroin adulterated with fentanyl; <40% of the heroin overdose ED discharge records listed procedure codes for drug screening.
Conclusions
The lack of routine ED drug testing likely resulted in underreporting of non-fatal overdoses involving fentanyl and other synthetic drugs. In order to inform coordinated public health and safety responses, drug overdose surveillance must move from a reactive to a proactive mode, utilizing the infrastructure for electronic health records.","Svetla Slavova and Julia F. Costich and Terry L. Bunn and Huong Luu and Michael Singleton and Sarah L. Hargrove and Jeremy S. Triplett and Dana Quesinberry and William Ralston and Van Ingram",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A critical examination of the relationship among research, theory, and practice: Technology and reading instruction","Recent technological advancements have changed how literacy is perceived, and it is no longer confined to the interaction with print text. The evolving definition of literacy has been reflected in the increasing number of teachers who are incorporating technology into their reading instruction. However, less is known about the extent to which these technology-integrated instructional practices are supported by reading theories. The purpose of this study is to systematically review how technology has been implemented in reading instruction and to explore how transitions of instructional practice from traditional classrooms to digital settings have been grounded in reading theories. The present study reviewed articles published over the past twelve years in flagship practitioner journals to examine the connections and the gaps between theory and practice. Our review uncovered that technology has served in reading instruction primarily in three ways: 1) to increase reading motivation, 2) to present information in multi-modalities, and 3) to promote collaborative learning. Consistent with other domains of reading instruction, social theories were found to be the prominent theoretical bases supporting technology-integrated practices; dual-coding theory has also emerged in recent years as the theoretical basis for technology use in reading instruction. However, most of the theories were rarely referred explicitly. Implications for researchers and practitioners were provided based on the gaps between theory and practice revealed in the current review.","Xinyuan Yang and Li-Jen Kuo and Xuejun Ji and Erin McTigue",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An international internship on social development led by Canadian nursing students: Empowering learning","Summary
Background
A Canadian nursing student-led knowledge dissemination project on health promotion for social development was implemented with local professionals and communities in Brazil.
Objectives
(a) to identify how student-interns contrasted Canadian and Brazilian cultural and social realities within a primary healthcare context from a social development perspective; (b) to examine how philosophical underpinnings, including social critical theory and notions of social justice, guided student-interns in acknowledging inequalities in primary healthcare in Brazil; and (c) to participate in the debate on the contribution of Canadian nursing students to the global movement for social development.
Design and Setting
A qualitative appraisal of short-term outcomes of an international internship in the cities of Birigui & Araçatuba (São Paulo-Brazil).
Participants
Four Canadian fourth-year undergraduate nursing students enrolled in a metropolitan university program.
Methods
Recruitment was through an email invitation to the student-interns, who accepted, and signed informed consent forms. Their participation was unpaid and voluntary. One-time individual interviews were conducted at the end of their internships. Transcriptions of the audio-recorded interviews were coded using the qualitative software program ATLAS ti 6.0. The findings were analyzed using thematic analysis.
Results
Student-interns' learning unfolded from making associations among concepts, new ideas, and their previous experiences, leading to a personal transformation through which they established new conceptual and personal connections. The two main themes revealed by the thematic analysis were dichotomizing realities, that is, acknowledging the existence of “two sides of each situation,” and discovering an unexpected reciprocity between global and urban health. Furthermore, the student-interns achieved personal and professional empowerment.
Conclusions
The knowledge gained from the international experience helped the student-interns learn how to collaborate with Brazilian society's sectors to improve the social conditions of a “marginalized population”. Student-interns became aware of their inner power to promote change by making invisible inequity visible in their own terms.","Margareth Zanchetta and Jasna Schwind and Kateryna Aksenchuk and Franklin F. Gorospe, and Lira Santiago",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Hysterectomies in Portugal (2000–2014): What has changed?","Objective
To describe conditions regarding hysterectomies during the past 15 years in Portugal.
Study design
Nationwide retrospective study of women who underwent hysterectomy in Portuguese public hospitals in the period between 2000 and 2014. Patient data regarding hospital codes, geography, patient age, indications, operative techniques, associated procedures, complications, admission dates, discharge dates and 30-day postoperative readmissions were extracted from the national database with information regarding all public hospitals in Portugal. For calculation of hysterectomy rates, the total number of women was found using the Statistics Portugal website. Data were analysed using STATA version 13.1.
Results
A total of 166 177 hysterectomies were performed between 2000 and 2014 in public hospitals in Portugal. The overall rate of hysterectomy decreased 19.3% (from 212/100 000 to 171/100 000 women per year). The average age of women at time of hysterectomy increased from 51.6±11.4 to 55.2±12.3years (p<0.001). There was an increase in laparoscopic [1.2%–9.5%, p<0.001] and vaginal route [13.3%–21.2%, p<0.001], with a consequent decrease in laparotomic route [85.5%–69.1%, p<0.001]. There was a change in the pattern of indications for hysterectomy; however, uterine fibroids remain the major indication for hysterectomy [45.3%–37.6%, p<0.001]. In women with hysterectomy for benign pathology, the rate of bilateral adnexectomy decreased from 71.0% to 51.9% (p<0.001) and the rate of bilateral salpingectomy increased from 1.0% to 15.1% (p<0.001). The mean number of hospitalization days decreased from 7.1±6.1 (in 2000–2004) to 5.4±5.0 (in 2010–2014) (p<0.001). Globally, the rate of complications increased from 3.3% in 2000–2004 to 3.6% in 2010–2014 (p<0.01).
Conclusion
In Portugal, the rate of hysterectomies decreased in the last 15 years with an increase in age at the time of the procedure and a change towards less invasive routes. Uterine fibroids remain the major indication for hysterectomy. Additionally, we noted a significant shift towards more concomitant bilateral salpingectomy (and less bilateral adnexectomy) during hysterectomy for benign indications, according to the evidence suggesting the fallopian tube as the origin of ovarian cancer.","Inês Gante and Cláudia Medeiros-Borges and Fernanda Águas",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A 2D wavelet-based multiscale approach with applications to the analysis of digital mammograms","A wavelet-based multifractal spectrum (MFS) for the analysis of images that possess an erratically changing oscillatory behavior at various scales is constructed and estimated. The methodology is applied to the analysis of mammograms. The key contribution is that the analysis is not focused on microcalcifications, but on the background of the image, thus presenting a new modality to be combined with other diagnostic tools. Differences in image backgrounds between malignant and normal cases are found, in terms of multifractal descriptors. The new tool is compared with another spectral method, based on monofractal descriptors.11All codes utilized in this work are available as a supplementary material with the electronic version of the paper.","Pepa Ramírez-Cobo and Brani Vidakovic",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Evident cognitive impairments in seemingly recovered patients after midazolam-based light sedation during diagnostic endoscopy","Background/Purpose
Midazolam is a widely used sedative agent during colonoscopy, with cognitive toxicity. However, the potential cognitive hazard of midazolam-based light sedation has not been sufficiently examined. We aimed to examine the cognitive safety and vulnerability profile under midazolam light sedation, with a particular focus on individual variations.
Methods
We conducted a prospective case-controlled study in an academic hospital. In total, 30 patients undergoing sedative colonoscopy as part of a health check-up were recruited. Neuropsychological testing on the full cognitive spectrum was evaluated at 15 minutes and 120 minutes after low-dose midazolam administration. The modified reliable change index (RCI) was used for intrapersonal comparisons and controlling for practice effects.
Results
Midazolam affected psychomotor speed (48%), memory (40%), learning (32%), working memory (17%), and sustained attention (11%), while sparing orientation and the fluency aspect of executive function at the acute stage. Residual memory (10%) and learning (10%) impairments at 2 hours after administration were evidenced in some patients. The three object recall and digit symbol coding tests can serve as useful screening tools.
Conclusion
Midazolam-based light sedation induced selective cognitive impairments and prolonged cognitive impairments occurred in patients with advanced age. A longer observation time and further screening were recommended for patients due to their at risk state.","Yen-Hsuan Hsu and Feng-Sheng Lin and Chi-Cheng Yang and Chih-Peng Lin and Mau-Sun Hua and Wei-Zen Sun",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Assessing possible shifts in wildfire regimes under a changing climate in mountainous landscapes","Climate change may affect the probability of extreme events such as wildfires. Although wildfires are some of the most important ecological processes in forest ecosystems, large-scale wildfires are often perceived as an environmental disaster. Since failure to include the dynamic nature of ecosystems in planning will inevitably lead to unexpected outcomes, we need to enhance our ability to cope with future extreme events coupled with climate change. This study presents several future scenarios in three different time periods for Canada’s Columbia Montane Cordillera Ecoprovince, which is prone to wildfires. These scenarios predict the probability of occurrence of widespread wildfires based on the hierarchical Bayesian model. The model was based on the relationships between wildfires and the Monthly Drought Code (MDC). The MDC is a generalized monthly version of the Daily Drought Code widely used across Canada by forest fire management agencies for monitoring of wildfire risk. To calculate future MDC values, we relied on different possible future conditions of climate, given by the Global Circulation Models. We found a regime shift in drought intensity with abrupt decreases in lightning-caused wildfire activity around 1940, suggesting that future wildfire risks can be inferred primarily from the summer drought code. For future periods, we found increasing trends in the probabilities of large-scale fires with time in most areas. It should be notable that, by the 2080s, there is a probability of some areas having more than 50% of large-scale wildfires under the “average” climatic conditions in the future, indicating that, even without “extreme” weather conditions, some ecosystems will have a fundamental probability of experiencing catastrophic fires under the condition of average summer. However, the rate of progression toward a fire-prone condition is quite different among the three climate change scenarios and among the region analyzed. Given such scenario-sensitive, spatially-heterogeneous patterns of wildfire probability in response to climate variability, management strategy should be flexible and more localized. By drawing on this knowledge, it may be possible to mitigate climate change impacts both before they arise and once they have occurred. These considerations are critical for maintaining the integrity of systems shaped by large-scale natural disturbances to increase their resilience to the changing climate while protecting human society and infrastructures. Working with alternative scenarios will facilitate our adaptation to climate change in managing fire-prone forest ecosystems.","Akira S. Mori and Edward A. Johnson",2013,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Cladistic analysis of extant and fossil African papionins using craniodental data","This study examines African papionin phylogenetic history through a comprehensive cladistic analysis of extant and fossil craniodental morphology using both quantitative and qualitative characters. To account for the well-documented influence of allometry on the papionin skull, the general allometric coding method was applied to characters determined to be significantly affected by allometry. Results of the analyses suggest that Parapapio, Pliopapio, and Papio izodi are stem African papionin taxa. Crown Plio-Pleistocene African papionin taxa include Gorgopithecus, Lophocebus cf. albigena, Procercocebus, Soromandrillus (new genus defined herein) quadratirostris, and, most likely, Dinopithecus. Furthermore, S. quadratirostris is a member of a clade also containing Mandrillus, Cercocebus, and Procercocebus; ?Theropithecus baringensis is strongly supported as a primitive member of the genus Theropithecus; Gorgopithecus is closely related to Papio and Lophocebus; and Theropithecus is possibly the most primitive crown African papionin taxon. Finally, character transformation analyses identify a series of morphological transformations during the course of papionin evolution. The origin of crown African papionins is diagnosed, at least in part, by the appearance of definitive and well-developed male maxillary ridges and maxillary fossae. Among crown African papionins, Papio, Lophocebus, and Gorgopithecus are further united by the most extensive development of the maxillary fossae. The Soromandrillus/Mandrillus/Cercocebus/Procercocebus clade is diagnosed by upturned nuchal crests (especially in males), widely divergent temporal lines (especially in males), medially oriented maxillary ridges in males, medially oriented inferior petrous processes, and a tendency to enlarge the premolars as an adaptation for hard-object food processing. The adaptive origins of the genus Theropithecus appear associated with a diet requiring an increase in size of the temporalis, the optimal placement of occlusal forces onto the molar battery, and an increase in the life of the posterior dentition. This shift is associated with the evolution of distinctive morphological features such as the anterior union of the temporal lines, increased enamel infoldings on the premolars and molars, a reversed curve of Spee, and delayed molar eruption.","Christopher C. Gilbert",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"GalSim: The modular galaxy image simulation toolkit","GalSim is a collaborative, open-source project aimed at providing an image simulation tool of enduring benefit to the astronomical community. It provides a software library for generating images of astronomical objects such as stars and galaxies in a variety of ways, efficiently handling image transformations and operations such as convolution and rendering at high precision. We describe the GalSim software and its capabilities, including necessary theoretical background. We demonstrate that the performance of GalSim meets the stringent requirements of high precision image analysis applications such as weak gravitational lensing, for current datasets and for the Stage IV dark energy surveys of the Large Synoptic Survey Telescope, ESA’s Euclid mission, and NASA’s WFIRST-AFTA mission. The GalSim project repository is public and includes the full code history, all open and closed issues, installation instructions, documentation, and wiki pages (including a Frequently Asked Questions section). The GalSim repository can be found at https://github.com/GalSim-developers/GalSim.","B.T.P. Rowe and M. Jarvis and R. Mandelbaum and G.M. Bernstein and J. Bosch and M. Simet and J.E. Meyers and T. Kacprzak and R. Nakajima and J. Zuntz and H. Miyatake and J.P. Dietrich and R. Armstrong and P. Melchior and M.S.S. Gill",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"REAS3: A revised implementation of the geosynchrotron model for radio emission from air showers","Over the past years, the freely available Monte Carlo-code REAS which simulates radio emission from air showers based on the geosynchrotron model, was used regularly for comparisons with data. However, it emerged that in the previous version of the code, emission due to the variation of the number of charged particles within an air shower was not taken into account. In the following article, we show the implementation of these emission contributions in REAS3 by the inclusion of “end-point contributions” and discuss the changes on the predictions of REAS obtained by this revision. The basis for describing radiation processes is an universal description which is gained by the use of the end-point formulation. Hence, not only pure geomagnetic radiation is simulated with REAS3 but also radiation due to the variation of the net charge excess in the air shower, independent of the Earth's magnetic field. Furthermore, we present a comparison of lateral distributions of LOPES data with REAS3-simulated distributions. The comparison shows a good agreement between both, data and REAS3 simulations.","M. Ludwig and T. Huege",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Prediction of microgeometrical influences on micropitting fatigue damage on 32CrMoV13 steel","Micropitting is a form of surface fatigue damage that occurs in the gear teeth. It is due to the effect of variation in the mechanical loading in the contact zone between the two teeth, induced especially by flank roughness. In this study, generic roughness profiles were built with geometrical parameters to simulate the contact between two rough surfaces. Using elastohydrodynamic lubrication code and Crossland's fatigue criteria, the influence on fatigue lifetime was analysed for changes in each parameter. The relevant parameters were determined that influence (i) the conventional pitting, (ii) the extent to which the von Mises equivalent stress exceeds the material yield stress in the zone where micropitting occurs, and (iii) the fatigue lifetime for steel teeth. With nitriding benefits, the same trends were shown with weaker effects.","A. Fabre and H.P. Evans and L. Barrallier and K.J. Sharif and M. Desvignes",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The Unpredictive Brain Under Threat: A Neurocomputational Account of Anxious Hypervigilance","Background
Anxious hypervigilance is marked by sensitized sensory-perceptual processes and attentional biases to potential danger cues in the environment. How this is realized at the neurocomputational level is unknown but could clarify the brain mechanisms disrupted in psychiatric conditions such as posttraumatic stress disorder. Predictive coding, instantiated by dynamic causal models, provides a promising framework to ground these state-related changes in the dynamic interactions of reciprocally connected brain areas.
Methods
Anxiety states were elicited in healthy participants (n = 19) by exposure to the threat of unpredictable, aversive shocks while undergoing magnetoencephalography. An auditory oddball sequence was presented to measure cortical responses related to deviance detection, and dynamic causal models quantified deviance-related changes in effective connectivity. Participants were also administered alprazolam (double-blinded, placebo-controlled crossover) to determine whether the cortical effects of threat-induced anxiety are reversed by acute anxiolytic treatment.
Results
Deviant tones elicited increased auditory cortical responses under threat. Bayesian analyses revealed that hypervigilant responding was best explained by increased postsynaptic gain in primary auditory cortex activity as well as modulation of feedforward, but not feedback, coupling within a temporofrontal cortical network. Increasing inhibitory gamma-aminobutyric acidergic action with alprazolam reduced anxiety and restored feedback modulation within the network.
Conclusions
Threat-induced anxiety produced unbalanced feedforward signaling in response to deviations in predicable sensory input. Amplifying ascending sensory prediction error signals may optimize stimulus detection in the face of impending threats. At the same time, diminished descending sensory prediction signals impede perceptual learning and may, therefore, underpin some of the deleterious effects of anxiety on higher-order cognition.","Brian R. Cornwell and Marta I. Garrido and Cassie Overstreet and Daniel S. Pine and Christian Grillon",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The myosin motor domain-containing chitin synthase PdChsVII is required for development, cell wall integrity and virulence in the citrus postharvest pathogen Penicillium digitatum","Chitin is an essential component of the fungal cell wall and a potential target in the development of new antifungal compounds, due to its presence in fungi and not in plants or vertebrates. Chitin synthase genes (chs) constitute a complex family in filamentous fungi and are involved in fungal development, morphogenesis, pathogenesis and virulence. In this study, additional chs genes in the citrus postharvest pathogen Penicillium digitatum have been identified. Comparative analyses included each PdChs in each one of the classes I to VII previously established, and support the grouping of these into three divisions. Disruption of the gene coding PdChsVII, which contains a short version of a myosin motor domain, has been achieved by using Agrobacterium tumefaciens-mediated transformation and revealed its role in the life cycle of the fungus. Disruption strains were viable but showed reduced growth and conidia production. Moreover, Pdchs mutants developed morphological defects as balloon-like enlarged cells and increased chitin content, indicative of an altered cell wall structure. Gene disruption also increased susceptibility to antifungal compounds such as calcofluor white (CFW), sodium dodecyl sulfate (SDS), hydroxide peroxide (H2O2) and commercial fungicides, but significantly no change was observed in the sensitivity to antifungal peptides. The PdchsVII mutants were able to infect citrus fruit and produced tissue maceration, although had reduced virulence and most importantly were greatly impaired in the production of visible mycelium and conidia on the fruit.","Mónica Gandía and Eleonora Harries and Jose F. Marcos",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Analysis and Training of the Required Abilities and Skills in Translation in the Light of Translation Models and General Theories of Translation Studies","The aim of this study is to examine abilities and skills that a translator needs to develop in the light of general translation theories and models. Translation contains different mental activities such as language, thinking, problem solving, memory, conceptualization, learning, information processing, perception, understanding, re-expression etc., which makes translation a complex phenomenon. Translator is not a passive element but an expert, who senses, processes the stimuli, signifies, and produces meanings again in another language. In order to make these entire operations a translator should equip herself/ himself with translation skills and abilities. Considering within this context, the topic of skills and abilities translators need to acquire was discussed in the light of the translation competence models created by the PACTE group and Gopferich and general translation theories. It was found out that there are so many different skills (e.g. the National Job Qualifications Authority defined 42 skills for translators) to be acquired which differ in text types, medium, code and field and also new developments in the technology bring with it new required skills to be acquired. Thus the departments of Translation Studies should take these new skills into consideration in the translator training and accordingly plan their academic programs as the world is always changing and so is the translation environment.","Fadime Coban",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A software engineering perspective on environmental modeling framework design: The Object Modeling System","The environmental modeling community has historically been concerned with the proliferation of models and the effort associated with collective model development tasks (e.g., code generation, data transformation, etc.). Environmental modeling frameworks (EMFs) have been developed to address this problem, but much work remains before EMFs are adopted as mainstream modeling tools. Environmental model development requires both scientific understanding of environmental phenomena and software developer proficiency. EMFs support the modeling process through streamlining model code development, allowing seamless access to data, and supporting data analysis and visualization. EMFs also support aggregation of model components into functional units, component interaction and communication, temporal-spatial stepping, scaling of spatial data, multi-threading/multi-processor support, and cross-language interoperability. Some EMFs additionally focus on high-performance computing and are tailored for particular modeling domains such as ecosystem, socio-economic, or climate change research. The Object Modeling System Version 3 (OMS3) EMF employs new advances in software framework design to better support the environmental model development process. This paper discusses key EMF design goals/constraints and addresses software engineering aspects that have made OMS3 framework development efficacious and its application practical, as demonstrated by leveraging software engineering efforts outside of the modeling community and lessons learned from over a decade of EMF development. Software engineering approaches employed in OMS3 are highlighted including a non-invasive lightweight framework design supporting component-based model development, use of implicit parallelism in system design, use of domain specific language design patterns, and cloud-based support for computational scalability. The key advancements in EMF design presented herein may be applicable and beneficial for other EMF developers seeking to better support environmental model development through improved framework design.","O. David and J.C. Ascough and W. Lloyd and T.R. Green and K.W. Rojas and G.H. Leavesley and L.R. Ahuja",2013,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"LRP8-Reelin-Regulated Neuronal Enhancer Signature Underlying Learning and Memory Formation","Summary
One of the exceptional properties of the brain is its ability to acquire new knowledge through learning and to store that information through memory. The epigenetic mechanisms linking changes in neuronal transcriptional programs to behavioral plasticity remain largely unknown. Here, we identify the epigenetic signature of the neuronal enhancers required for transcriptional regulation of synaptic plasticity genes during memory formation, linking this to Reelin signaling. The binding of Reelin to its receptor, LRP8, triggers activation of this cohort of LRP8-Reelin-regulated neuronal (LRN) enhancers that serve as the ultimate convergence point of a novel synapse-to-nucleus pathway. Reelin simultaneously regulates NMDA-receptor transmission, which reciprocally permits the required γ-secretase-dependent cleavage of LRP8, revealing an unprecedented role for its intracellular domain in the regulation of synaptically generated signals. These results uncover an in vivo enhancer code serving as a critical molecular component of cognition and relevant to psychiatric disorders linked to defects in Reelin signaling.","Francesca Telese and Qi Ma and Patricia Montilla Perez and Dimple Notani and Soohwan Oh and Wenbo Li and Davide Comoletti and Kenneth A. Ohgi and Havilah Taylor and Michael G. Rosenfeld",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Analytical description of coincidence detection synaptic mechanisms in the auditory pathway","Localization of sound source azimuth within horizontal plane uses interaural time differences (ITDs) between sounds arriving through the left and right ear. In mammals, ITDs are processed primarily in the medial superior olive (MSO) neurons. These are the first binaural neurons in the auditory pathway. The MSO neurons are notable because they possess high time precision in the range of tens of microseconds. Several theories and experimental studies explain how neurons are able to achieve such precision. In most theories, neuronal coincidence detection processes the ITDs and encodes azimuth in ascending neurons of the auditory pathway using modalities that are more tractable than the ITD. These modalities have been described as firing rate codes, place codes (labeled line codes) and similarly. In this theoretical model it is described how the ITD is processed by coincidence detection and converted into spikes by summing the postsynaptic potentials. Particular postsynaptic conductance functions are used in order to obtain an analytical solution in a closed form. Specifically, postsynaptic response functions are derived from the exponential decay of postsynaptic conductances and the MSO neuron is modeled as a simplified version of the Spike Response Model (SRM0) which uses linear summations of the membrane responses to synaptic inputs. For plausible ratios of time constants, an analytical solution used to describe properties of coincidence detection window is obtained. The parameter space is then explored in the vicinity of the analytical solution. The variation of parameters does not change the solution qualitatively.","Peter G. Toth and Petr Marsalek",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"“Just Wear Dark Underpants Mainly”: Learning from Adolescents' and Young Adults' Experiences with Early Discontinuation of the Contraceptive Implant","Study Objective
Long-acting reversible contraception, including the contraceptive implant, is recommended for teens and young women. However, some young women discontinue the implant early, and we seek to better understand their experiences.
Design, Setting, and Participants
We conducted interviews with 16 young women ages 14 to 24 who presented for removal of the contraceptive implant within 6 months after placement at outpatient adolescent, family medicine, and obstetrics and gynecology clinics. We coded and analyzed transcripts to identify themes and develop a thematic framework.
Interventions and Main Outcome Measures
We explored decision-making regarding placement and removal of the implant, differences between anticipated and experienced side effects, and recommendations for counseling.
Results
The participants reported experiencing significant side effects that led to removal, most often frequent or heavy bleeding or mood changes. These healthy young women were unprepared for these symptoms, despite remembering being told about possible side effects. Participants wanted more concrete examples of possible side effects, and personal stories of side effects experienced by others, rather than general terms such as irregular bleeding or mood changes. Few discussed problems with their providers; instead, they relied on the Internet or friends to help decide when to remove the implant. Nearly half of the participants did not start new contraception after removal, although they voiced a continued desire to avoid pregnancy.
Conclusion
We identified a need for more descriptive counseling about side effects experienced by individuals, and guidance on what to do about problems encountered after placement.","Britt Lunde and Lisa Littman and Samantha Stimmel and Rima Rana and Adam Jacobs and Carol R. Horowitz",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"P4-To-VHDL: Automatic generation of high-speed input and output network blocks","High-performance embedded architectures typically contain many stand-alone blocks which communicate and exchange data; additionally a high-speed network interface is usually needed at the boundary of the system. The software-based data processing is typically slow which leads to a need for hardware accelerated approaches. The problem is getting harder if the supported protocol stack is rapidly changing. Such problem can be effectively solved by the Field Programmable Gate Arrays and high-level synthesis which together provide a high degree of generality. This approach has several advantages like fast development or possibility to enable the area of packet-oriented communication to domain oriented experts. However, the typical disadvantage of this approach is the insufficient performance of generated system from a high-level description. This can be a serious problem in the case of a system which is required to process data at high packet rates. This work presents a generator of high-speed input (Parser) and output (Deparser) network blocks from the P4 language which is designed for the description of modern packet processing devices. The tool converts a P4 description to a synthesizable VHDL code suitable for the FPGA implementation. We present design, analysis and experimental results of our generator. Our results show that the generated circuits are able to process 100 Gbps traffic with fairly complex protocol structure at line rate on Xilinx Virtex-7 XCVH580T FPGA. The approach can be used not only in networking devices but also in other applications like packet processing engines in embedded cores because the P4 language is device and protocol independent.","Pavel Benáček and Viktor Puš and Hana Kubátová and Tomáš Čejka",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Older Ethnic Minority Women's Perceptions of Stroke Prevention and Walking","Objective
To inform the development of a tailored behavioral stroke risk reduction intervention for ethnic minority seniors, we sought to explore gender differences in perceptions of stroke prevention and physical activity (walking).
Methods
In collaboration with community-based organizations, we conducted 12 mixed-gender focus groups of African American, Latino, Chinese, and Korean seniors aged 60 years and older with a history of hypertension (89 women and 42 men). Transcripts were coded and recurring topics compared by gender.
Results
Women expressed beliefs that differed from men in 4 topic areas: 1) stroke-related interest, 2) barriers to walking, 3) facilitators to walking, and 4) health behavior change attitudes. Compared with men, women were more interested in their role in response to a stroke and post-stroke care. Women described walking as an acceptable form of exercise, but cited neighborhood safety and pain as walking barriers. Fear of nursing home placement and weight loss were identified as walking facilitators. Women were more prone than men to express active/control attitudes toward health behavior change.
Conclusions
Older ethnic minority women, a high-risk population for stroke, may be more receptive to behavioral interventions that address the gender-specific themes identified by this study.","Ivy Kwon and Nazleen Bharmal and Sarah Choi and Daniel Araiza and Mignon R. Moore and Laura Trejo and Catherine A. Sarkisian",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"AstroPhi: A code for complex simulation of the dynamics of astrophysical objects using hybrid supercomputers","We propose a new code named AstroPhi for simulation of the dynamics of astrophysical objects on hybrid supercomputers equipped with Intel Xenon Phi computation accelerators. The details of parallel implementation are described, as well as changes to the computational algorithm that facilitate efficient parallel implementation. A single Xeon Phi accelerator yielded 27-fold acceleration. The use of 32 Xeon Phi accelerators resulted in 94% parallel efficiency. Several collapse problems are simulated using the AstroPhi code.
Program summary
Program title: AstroPhi Catalogue identifier: AEUM_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEUM_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 99604 No. of bytes in distributed program, including test data, etc.: 305433 Distribution format: tar.gz Programming language: C++. Computer: MVS-10P - RSC Tornado, Xeon E5-2690 8C 2.900 GHz, Infiniband FDR, Intel Xeon Phi SE10X. Operating system: Linux. Has the code been vectorized or parallelized?: Parallelized on MPI + OpenMP for Intel MIC architecture, 32 Intel Xeon Phi (60 cores per 1 Intel Xeon Phi = 1920 cores of Intel Xeon Phi). RAM: 137438953472 bytes (128 GB) bytes Classification: 1.9. External routines: MPI, OpenMP for Intel Xeon Phi, FFTW 2.1.5 Nature of problem: Complex numerical simulation of dynamics of astrophysical objects plays an important role due to significant growth of observational astronomic data. The new astrophysical models and codes need to be developed for detailed simulation of different physical effects in astrophysics with the use of modern supercomputers with hybrid architecture. Solution method: AstroPhi code consisting of particle-in-cell and Godunov methods combination adapted for hybrid supercomputer architecture. Restrictions: For this version maximum grid size is restricted to 10243. Running time: Typical running on MVS-10P is 24 h. The test provided only takes a few minutes.","I.M. Kulikov and I.G. Chernykh and A.V. Snytnikov and B.M. Glinskiy and A.V. Tutukov",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Modelling of ejector chillers with steam and other working fluids","The Constant Rate of Momentum Change (CRMC) criterion attempts to improve the design of supersonic ejectors, that can be used in heat-powered chillers for industrial or air-conditioning use. Moving from its original formulation, the CRMC design method can be advanced accounting for friction irreversibilities and real gas behaviour, as done in a previous work by our research group. Here we present an upgraded version of this analysis, supported by experimental data from a prototype chiller using R245fa as working fluid. The analysis is extended to other fluids (water, isobutane, 5 HFCs and 3 HFOs) whose performance is calculated on a wide range of heat source/sink temperatures. The existing literature, based generally on ideal gas simulations, suggests that water yields poor results in terms of COP. This paper shows that this result may be argued. Low GWP fluid HFO1233zd also gives good results.","Adriano Milazzo and Andrea Rocchetti",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An operational, multi-scale, multi-model system for consensus-based, integrated water management and policy analysis: The Netherlands Hydrological Instrument","Water management in the Netherlands applies to a dense network of surface waters for discharge, storage and distribution, serving highly valuable land-use. National and regional water authorities develop long-term plans for sustainable water use and safety under changing climate conditions. The decisions about investments on adaptive measures are based on analysis supported by the Netherlands Hydrological Instrument NHI based on the best available data and state-of-the-art technology and developed through collaboration between national research institutes. The NHI consists of various physical models at appropriate temporal and spatial scales for all parts of the water system. Intelligent connectors provide transfer between different scales and fast computation, by coupling model codes at a deep level in software. A workflow and version management system guarantees consistency in the data, software, computations and results. The NHI is freely available to hydrologists via an open web interface that enables exchange of all data and tools.","Willem J. De Lange and Geert F. Prinsen and Jacco C. Hoogewoud and Albert A. Veldhuizen and Jarno Verkaik and Gualbert H.P. Oude Essink and Paul E.V. van Walsum and Joost R. Delsman and Joachim C. Hunink and Harry Th.L. Massop and Timo Kroon",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Detection change points of triplet periodicity of gene","The triplet periodicity (TP) is a distinguished property of protein coding sequences. There are complex genes with more than one TP type along their sequence. We say that these genes contain a triplet periodicity change point. The aim of the work is to find all genes that contain TP change point and attempt to compare the positions of change point in genes with known biological data. We have developed a mathematical method to identify triplet periodicity changes along a sequence. We have found 311,221 genes with the TP change point in the KEGG/Genes database (version 48). It is about 8% from the total database volume (4013150). We showed that the repetitive sequences are not the only cause of such events. We suppose that the TP change point may indicate a fusion of genes or domains. We performed BLAST analysis to find potential ancestral genes for the parts of genes with TP change point. As a result we found that in 131323 cases sequences with TP change point have proper similarities for one or both parts. The relationship between TP change point and the fusion events in genes is discussed. The program realization of the method is available by request to authors.","Yulia M. Suvorova and Valentina M. Rudenko and Eugene V. Korotkov",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"How to Save Expert Knowledge for the Organization: Methods for Collecting and Documenting Expert Knowledge Using Virtual Reality based Learning Environments","The current demographic development will lead to changing working conditions. The paper at hand presents methods for the investigation of expert knowledge within the company and how to document these information, widely independent from individuals, within a technology based learning environment, using virtual reality (VR) technologies and other media. Expert knowledge is often coded as tacit knowledge [1] and can be hardly verbalized by experts. The extraction of tacit knowledge requires methods that are based on stories. Beyond facts these stories also include tacit knowledge driven by emotions. One of the narrative methods presented in the paper will be the »triad interview« [2]. The documentation of these interviews within texts is not very sustainable and other methods for the documentation of expert knowledge are required. Especially in the field of maintenance that is focused in the paper, virtual reality is a suitable method for the documentation as technicians are familiar with drawings and visual content representations. Virtual reality learning environment can visualize working processes and keep the narrative structure that is required for transferring tacit knowledge. Besides, interactions as a typical characteristic of virtual reality applications allow the design of problem solving tasks. The paper presents the »triad interview« and the »working process analysis« [3] that are used for extracting the relevant information of a working process. For the didactical design of the learning tasks the method of the »complete action« [4] is applied. The paper will present the design of the several phases and their benefit for the learning process. Finally the results and experiences from an evaluation study within the organization will be presented.","Tina Haase and Wilhelm Termath and Marcel Martsch",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Parvalbumin interneurons constrain the size of the lateral amygdala engram","Memories are thought to be represented by discrete physiological changes in the brain, collectively referred to as an engram, that allow patterns of activity present during learning to be reactivated in the future. During the formation of a conditioned fear memory, a subset of principal (excitatory) neurons in the lateral amygdala (LA) are allocated to a neuronal ensemble that encodes an association between an initially neutral stimulus and a threatening aversive stimulus. Previous experimental and computational work suggests that this subset consists of only a small proportion of all LA neurons, and that this proportion remains constant across different memories. Here we examine the mechanisms that contribute to the stability of the size of the LA component of an engram supporting a fear memory. Visualizing expression of the activity-dependent gene Arc following memory retrieval to identify neurons allocated to an engram, we first show that the overall size of the LA engram remains constant across conditions of different memory strength. That is, the strength of a memory was not correlated with the number of LA neurons allocated to the engram supporting that memory. We then examine potential mechanisms constraining the size of the LA engram by expressing inhibitory DREADDS (designer receptors exclusively activated by designer drugs) in parvalbumin-positive (PV+) interneurons of the amygdala. We find that silencing PV+ neurons during conditioning increases the size of the engram, especially in the dorsal subnucleus of the LA. These results confirm predictions from modeling studies regarding the role of inhibition in shaping the size of neuronal memory ensembles and provide additional support for the idea that neurons in the LA are sparsely allocated to the engram based on relative neuronal excitability.","Dano J. Morrison and Asim J. Rashid and Adelaide P. Yiu and Chen Yan and Paul W. Frankland and Sheena A. Josselyn",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Pattern of socio-economic and health aspects among TB patients and controls","Background
Socio-economic and health-related factors have a significant impact on tuberculosis (TB) incidence among population residing in resource-scare settings.
Objective
To evaluate the pattern of socio-economic and health-related factors among TB patients and control in Delhi, India.
Methods
The present cross-sectional study was performed among 893 TB patients (or cases) and 333 healthy disease-free controls. The data for the present study was obtained from several district TB centres in north, west and south Delhi. The collected data was edited, coded and statistical analysed with the help of SPSS 20.0 version.
Results
Illiteracy and primary education were significant risk factors being associated with a TB. Rented housing condition had an odds ratio (OR) of 1.4 (95% confidence interval [CI]: 1.09–1.89) compared to owned housing condition. 3–5 individuals per room were 3 times more likely to be associated with a case of TB (95% CI: 2.49–4.41). Migrant individuals were 13 times more likely to be associated with a case of TB (95% CI: 8.77–19.78) in comparison to settled population. Daily consumption of non-vegetarian food also significantly contributed to case of TB with an OR of 3.4 (95% CI: 2.51–4.72). Loss of appetite and family TB served as significant health-related factors associated with TB risk.
Conclusion
Lower educational status, rented household, individuals per room (as a measure of overcrowding) and migratory status served as prominent risk factors for TB disease. Preference and frequency of non-vegetarian food being consumed, night sweating, weight loss, loss of appetite, earlier TB and family TB were principle health-related risk factors associated with TB disease.","A.K. Kapoor and Vijit Deepani and Meenal Dhall and Satwanti Kapoor",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Dynamic evolving spiking neural networks for on-line spatio- and spectro-temporal pattern recognition","On-line learning and recognition of spatio- and spectro-temporal data (SSTD) is a very challenging task and an important one for the future development of autonomous machine learning systems with broad applications. Models based on spiking neural networks (SNN) have already proved their potential in capturing spatial and temporal data. One class of them, the evolving SNN (eSNN), uses a one-pass rank-order learning mechanism and a strategy to evolve a new spiking neuron and new connections to learn new patterns from incoming data. So far these networks have been mainly used for fast image and speech frame-based recognition. Alternative spike-time learning methods, such as Spike-Timing Dependent Plasticity (STDP) and its variant Spike Driven Synaptic Plasticity (SDSP), can also be used to learn spatio-temporal representations, but they usually require many iterations in an unsupervised or semi-supervised mode of learning. This paper introduces a new class of eSNN, dynamic eSNN, that utilise both rank-order learning and dynamic synapses to learn SSTD in a fast, on-line mode. The paper also introduces a new model called deSNN, that utilises rank-order learning and SDSP spike-time learning in unsupervised, supervised, or semi-supervised modes. The SDSP learning is used to evolve dynamically the network changing connection weights that capture spatio-temporal spike data clusters both during training and during recall. The new deSNN model is first illustrated on simple examples and then applied on two case study applications: (1) moving object recognition using address-event representation (AER) with data collected using a silicon retina device; (2) EEG SSTD recognition for brain–computer interfaces. The deSNN models resulted in a superior performance in terms of accuracy and speed when compared with other SNN models that use either rank-order or STDP learning. The reason is that the deSNN makes use of both the information contained in the order of the first input spikes (which information is explicitly present in input data streams and would be crucial to consider in some tasks) and of the information contained in the timing of the following spikes that is learned by the dynamic synapses as a whole spatio-temporal pattern.","Nikola Kasabov and Kshitij Dhoble and Nuttapod Nuntalid and Giacomo Indiveri",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Improving communication in general practice when mental health issues appear: Piloting a set of six evidence-based skills","Objective
To test a communication skills training program teaching general practitioners (GPs) a set of six evidence-based mental health related skills.
Methods
A training program was developed and tested in a pilot test–retest study with 21GPs. Consultations were videotaped and actors used as patients. A coding scheme was created to assess the effect of training on GP behavior. Relevant utterances were categorized as examples of each of the six specified skills. The GPs’ self-perceived learning needs and self-efficacy were measured with questionnaires.
Results
The mean number of GP utterances related to the six skills increased from 13.3 (SD 6.2) utterances before to 23.6 (SD 7.2) utterances after training; an increase of 77.4% (P<0.001). Effect sizes varied from 0.23 to 1.37. Skills exploring emotions, cognitions and resources, and the skill Promote coping, increased significantly. Self-perceived learning needs and self-efficacy did not change significantly.
Conclusion
The results from this pilot test are encouraging. GPs enhanced their use on four out of six mental health related communication skills significantly, and the effects were medium to large.
Practice implications
This training approach appears to be an efficacious approach to mental health related communication skills training in general practice.","Tonje Lauritzen Stensrud and Pål Gulbrandsen and Trond Arne Mjaaland and Sidsel Skretting and Arnstein Finset",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Perceived risk of heroin use among nonmedical prescription opioid users","Aims
The prevalence of heroin use among nonmedical prescription opioid (NMPO) users has increased in recent years. Identifying characteristics associated with heroin use in this population can help inform efforts to prevent heroin initiation and maintenance. The aim of this study was to evaluate differences in perceived risk of heroin among NMPO users with and without histories of heroin use, and to examine temporal trends in perceived risk of heroin among this population.
Methods
Data are from the 2002–2013 National Survey on Drug Use and Health, and included all past-year NMPO users (N=49,045). Participants reported perceived risk of trying heroin once or twice and regular heroin use. Responses were coded dichotomously (great risk vs. other risk) and logistic regression analyses were used to evaluate the association between lifetime heroin use and perceived risk of heroin, and to determine temporal changes in perceived risk.
Results
Results indicated a significant association between lifetime heroin use and lower likelihood of reporting great risk of trying heroin (OR=0.38, 95% CI: 0.33, 0.44, p<0.001), and of regular use of heroin (OR=0.39, 95% CI: 0.32, 0.48, p<0.001). There was a significant, yet modest, trend toward decreasing perception of great risk from 2002 to 2013.
Conclusions
Findings from this analysis of nationally representative data indicate that NMPO users with a history of heroin use perceive heroin to be less risky than those without heroin use. Perception of risk has decreased from 2002 to 2013 in this population, consistent with increasing rates of heroin initiation.","Victoria R. Votaw and Justine Wittenauer and Hilary S. Connery and Roger D. Weiss and R. Kathryn McHugh",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"RNGAVXLIB: Program library for random number generation, AVX realization","We present the random number generator (RNG) library RNGAVXLIB, which contains fast AVX realizations of a number of modern random number generators, and also the abilities to jump ahead inside a RNG sequence and to initialize up to 1019 independent random number streams with block splitting method. Fast AVX implementations produce exactly the same output sequences as the original algorithms. Usage of AVX vectorization allows to substantially improve performance of the generators. The new realizations are up to 2 times faster than the SSE realizations implemented in the previous version of the library (Barash and Shchur, 2013), and up to 40 times faster compared to the original algorithms written in ANSI C.
New version program summary
Program title: RNGAVXLIB Catalogue identifier: AEIT_v3_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIT_v3_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 21061 No. of bytes in distributed program, including test data, etc.: 1763798 Distribution format: tar.gz Programming language: C, Fortran. Computer: PC, laptop, workstation, or server with Intel or AMD processor. Operating system: Unix, Windows. RAM: 4 Mbytes Catalogue identifier of previous version: AEIT_v2_0 Journal reference of previous version: Comput. Phys. Comm. 184(2013)2367 Classification: 4.13. Does the new version supersede the previous version?: Yes Nature of problem: Any calculation requiring uniform pseudorandom number generator, in particular, Monte Carlo calculations. Any calculation requiring parallel streams of uniform pseudorandom numbers. Solution method: The library contains realization of the following modern and reliable generators: MT19937 [2], MRG32K3A [3], LFSR113 [4], GM19, GM31, GM61 [5, 6], and GM29, GM55, GQ58.1, GQ58.3, GQ58.4 [7, 8]. The library contains realizations written in ANSI C, realizations based on SSE command set and realizations based on AVX command set. The use of vectorization allows substantial improvement in performance of all the generators. The library also contains the ability to jump ahead inside the RNG sequence and to initialize independent random number streams with block splitting method for each of the RNGs. C and Fortran are supported. Reasons for new version: Modern CPUs better support vectorization compared to the CPUs available two years ago when the previous version of the library was prepared. In particular, Advanced Vector Instructions 2 (AVX2) are now supported by CPUs fabricated by Intel and AMD. AVX2 has been supported by Intel CPUs since the Haswell microarchitecture was released in June 2013, and has been supported by AMD CPUs since the Streamroller Family 15h microarchitecture was released in January 2014. An important new feature of this version is the ability to employ the AVX2 instruction set of a CPU in order to speed up the calculations. As a result, the new RNG realizations employing AVX2 are up to 2 times faster than the realizations implemented in the previous version of the library. Summary of revisions:1.We added fast AVX realizations for the generators, which are up to 2 times faster than the SSE realizations implemented in the previous version of the library [1], and up to 40 times faster compared to the original algorithms written in ANSI C.2.The function call interface has been simplified compared to previous versions.3.We added automatic detection of whether the CPU supports SSE and/or AVX vectorization at the compilation stage and the functions which employ SSE and AVX vectorization only if the CPU supports them.4.We added support for simultaneous generation of two independent output sequences for the LFSR113 generator using the AVX vectorization. Restrictions: For AVX realizations of the generators, Intel or AMD CPU supporting AVX2 command set is required. For SSE realizations of the generators, Intel or AMD CPU supporting SSE2 command set is required. In order to use the SSE realization for the lfsr113 generator, CPU must support SSE4.1 command set. Additional comments: The function call interface has been simplified compared to the previous versions. For each of the generators, RNGAVXLIB supports the following functions, where rng should be replaced by name of a particular generator: void rng_init_(rng_state∗state); void rng_init_sequence_(rng_state∗state,unsigned long long SequenceNumber); void rng_skipahead_(rng_state∗state, unsigned long long N); unsigned int rng_generate_(rng_state∗state); float rng_generate_uniform_float_(rng_state∗state); unsigned int rng_ansi_generate_(rng_state∗state); float rng_ansi_generate_uniform_float_(rng_state∗state); unsigned int rng_sse_generate_(rng_state∗state); float rng_sse_generate_uniform_float_(rng_state∗state); unsigned int rng_avx_generate_(rng_state∗state); float rng_avx_generate_uniform_float_(rng_state∗state); void rng_print_state_(rng_state∗state); The function call interface for the rng_skipahead_ function, which jumps ahead N output values inside an RNG sequence, can be slightly different for some of the RNGs. For example, the function void mt19937_skipahead_(mt19937_state∗state, unsigned long long a, unsigned b); skips ahead N=a⋅2b numbers, where N<2512, and the function void gm55_skipahead_(gm55_state∗state, unsigned long long offset64, unsigned long long offset0); skips ahead N=264⋅offset64+offset0 numbers. The detailed function call interface can be found in the header files of the include directory. The examples of using the library can be found in the examples directory. Some of the generators have several versions of the rng_init_sequence_ routine, for example, rng_init_short_sequence_, rng_init_medium_sequence_, rng_init_long_sequence_ (see details in [1, 10]). Maximal number of sequences and maximal length of each sequence for pseudorandom streams are indicated in [1, 10]. The algorithms used to jump ahead in the RNG sequence and to initialize parallel streams of pseudorandom numbers are described in detail in [9, 10]. This version of the library automatically detects whether the CPU supports SSE and/or AVX vectorization at the compilation stage. During the compilation of the library, the −march=native compiler option is used, which allows the use of predefined macros such as __SSE2__ and __AVX2__ in the source code. This is supported by both GNU and Intel compilers. The functions rng_generate_ and rng_generate_uniform_float employ SSE and AVX vectorization only if the CPU supports them. Table 1: Speed of the realizations. CPU: Intel Xeon E5-2650v3 (2.3 GHz); Compiler: gcc; Optimization: -O3.  This version of the library also supports simultaneous generation of two independent output sequences for the LFSR113 generator using the AVX vectorization: void lfsr113_avx_generate_two_(lfsr113 _state∗state, unsigned∗out1, unsigned∗out2); This is the fastest possible way to generate LFSR113 random numbers using the CPU which supports the AVX2 instruction set. The function lfsr113_skipahead_ jumps ahead only in the first LFSR output sequence. Jumping ahead in the second output sequence can be performed with the separate lfsr113_skipahead2_ routine. GNU Fortran does not have compiler directives for data alignment to assist vectorization, although Intel Fortran has directives for that, such as !dir$  attributes align:32. By default, GNU Fortran aligns all variables to 16-byte boundaries, which is sufficient to efficiently use SSE, but is not sufficient for AVX. We find that applying an additional SAVE command to the generator state in Fortran results, in particular, in alignment of the data to 32-byte boundaries. This allows one to employ AVX realizations from Fortran (see the examples directory). We have tested this on workstations with various CPUs and various versions of Linux. Development and optimization of the algorithms were supported by the Russian Science Foundation project No. 14-21-00158. Benchmark testing was partially supported by Russian Foundation for Basic Research project No. 13-07-00570 and by the Supercomputing Center of Lomonosov Moscow State University [11]. Table 2: Speed of the realizations. CPU: Intel Core i7-4790K (4 GHz); Compiler: gcc; Optimization: -O3.  Running time: Running time is of the order of 20 sec for generating 109 pseudorandom numbers with a PC based on Intel Core i7-940 CPU. Speed of the random number generation on CPUs widely used in modern servers and workstations is shown in Tables 1 and 2 respectively (see also [6, 7]). References:[1]L.Yu Barash, L.N. Shchur, RNGSSELIB: Program library for random number generation. More generators, parallel streams of random numbers and Fortran compatibility, Computer Physics Communications, 184(10), 2367–2369 (2013).[2]M. Matsumoto and T. Tishimura, Mersenne Twister: A 623- dimensionally equidistributed uniform pseudorandom number generator, ACM Trans. on Mod. and Comp. Simul. 8 (1), 3–30 (1998).[3]P L’Ecuyer, Good Parameter Sets for Combined Multiple Recursive Random Number Generators, Oper. Res. 47 (1), 159–164 (1999).[4]P L’Ecuyer, Tables of Maximally-Equidistributed Combined LFSR Generators, Math. of Comp., 68 (255), 261–269 (1999).[5]L. Barash, L.N. Shchur, Periodic orbits of the ensemble of Sinai- Arnold cat maps and pseudorandom number generation, Phys. Rev. E 73, 036701 (2006).[6]L.Yu Barash, L.N. Shchur, RNGSSELIB: Program library for random number generation, SSE2 realization, Computer Physics Communications, 182 (7), 1518–1527 (2011).[7]L.Yu. Barash, Applying dissipative dynamical systems to pseudorandom number generation: Equidistribution property and statistical independence of bits at distances up to logarithm of mesh size, Europhysics Letters (EPL) 95, 10003 (2011).[8]L.Yu. Barash, Geometric and statistical properties of pseudorandom number generators based on multiple recursive transformations // Springer Proceedings in Mathematics and Statistics, Springer-Verlag, Berlin, Heidelberg, Vol. 23, 265–280 (2012).[9]L.Yu. Barash, L.N. Shchur, On the generation of parallel streams of pseudorandom numbers, Programmnaya inzheneriya, 1 (2013) 24 (in Russian)[10]L.Yu. Barash, L.N. Shchur, PRAND: GPU accelerated parallel random number generation library: Using most reliable algorithms and applying parallelism of modern GPUs and CPUs, Computer Physics Communications, 185(4), 1343–1353 (2014).[11]Voevodin Vl.V., Zhumatiy S.A., Sobolev S.I., Antonov A.S., Bryzgalov P.A., Nikitenko D.A., Stefanov K.S., Voevodin Vad.V., Practice of “Lomonosov” Supercomputer // Open Systems J. - Moscow: Open Systems Publ., 2012, no.7. (In Russian)","M.S. Guskova and L.Yu. Barash and L.N. Shchur",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An on-line, real-time learning method for detecting anomalies in videos using spatio-temporal compositions","This paper presents an approach for detecting suspicious events in videos by using only the video itself as the training samples for valid behaviors. These salient events are obtained in real-time by detecting anomalous spatio-temporal regions in a densely sampled video. The method codes a video as a compact set of spatio-temporal volumes, while considering the uncertainty in the codebook construction. The spatio-temporal compositions of video volumes are modeled using a probabilistic framework, which calculates their likelihood of being normal in the video. This approach can be considered as an extension of the Bag of Video words (BOV) approaches, which represent a video as an order-less distribution of video volumes. The proposed method imposes spatial and temporal constraints on the video volumes so that an inference mechanism can estimate the probability density functions of their arrangements. Anomalous events are assumed to be video arrangements with very low frequency of occurrence. The algorithm is very fast and does not employ background subtraction, motion estimation or tracking. It is also robust to spatial and temporal scale changes, as well as some deformations. Experiments were performed on four video datasets of abnormal activities in both crowded and non-crowded scenes and under difficult illumination conditions. The proposed method outperformed all other approaches based on BOV that do not account for contextual information.","Mehrsan Javan Roshtkhari and Martin D. Levine",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Urban and landscape changes through historical maps: The Real Sitio of Aranjuez (1775–2005), a case study","When determining the evolution of a territory or town over time, comparing historical maps with contemporary maps is indispensable. In this study, we applied the methodology of georectification to compare historical maps with current orthophotos from 2005. We propose colour and lines code as useful tools for the analysis of the urban and landscape changes that the town has undergone since the 18th century, and we graphically reconstruct certain former heritage items that no longer exist. For example, these techniques are applied to the Real Sitio de Aranjuez (Spain) using the two most important historical maps: the 1775 Domingo de Aguirre map, which shows the full extent of the royal site for the first time, and the 1835 General Town Plan, which is the most characteristic of available 19th-century maps, as it displays the consolidated historical town. Next, using two rectified rasters and the orthophoto, we overlay a grid of nine 1×1km squares, allowing us to “see the town and its territory” at three moments in history: 1775, 1835 and 2005. Thus, we obtain formal and dimensional information allowing analysis of the evolution of the territory, urban area and historic buildings. Among the many applications of this methodology in the fields of urban development and monumental-heritage conservation, we propose the graphical reconstruction of three urban elements that no longer exist. We determined that graphical reconstruction, in conjunction with traditional historical research, provides the greatest benefits for recreating an historical landscape. These methodologies will aid in the development of long-range management strategies and facilitate the assessment of threats posed by anthropogenic activities and environmental change to preserve the landscape heritage.","C. San-Antonio-Gómez and C. Velilla and F. Manzano-Agugliaro",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Prevalidation study of the Syrian hamster embryo (SHE) cell transformation assay at pH 6.7 for assessment of carcinogenic potential of chemicals","The Syrian hamster embryo (SHE) cell transformation assay (CTA) is an important in vitro method that is highly predictive of rodent carcinogenicity. It is a key method for reducing animal usage for carcinogenicity prediction. The SHE assay has been used for many years primarily to investigate and identify potential rodent carcinogens thereby reducing the number of 2-year bioassays performed in rodents. As for other assays with a long history of use, the SHE CTA has not undergone formal validation. To address this, the European Centre for the Validation of Alternative Methods (ECVAM) coordinated a prevalidation study. The aim of this study was to evaluate the within-laboratory reproducibility, test method transferability, and between-laboratory reproducibility and to develop a standardised state-of-the-art protocol for the SHE CTA at pH 6.7. Formal ECVAM principles for criteria on reproducibility (including the within-laboratory reproducibility, the transferability and the between-laboratories reproducibility) were applied. In addition to the assessment of reproducibility, this study helped define a standard protocol for use in developing an Organisation for Economic Co-operation and Development (OECD) test guideline for the SHE CTA. Six compounds were evaluated in this study: benzo(a)pyrene, 3-methylcholanthrene, o-toluidine HCl, 2,4-diaminotoluene, phthalic anhydride and anthracene. Results of this study demonstrate that a protocol is available that is transferable between laboratories, and that the SHE CTA at pH 6.7 is reproducible within- and between-laboratories.","Kamala Pant and Shannon W. Bruce and Jamie E. Sly and Thorsten Kunkelmann and Susanne Kunz-Bohnenberger and Albrecht Poth and Günter Engelhardt and Markus Schulz and Karl-Rainer Schwind",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A fully persistent and consistent read/write cache using flash-based general SSDs for desktop workloads","The flash-based SSD is used as a tiered cache between RAM and HDD. Conventional schemes do not utilize the nonvolatile feature of SSD and cannot cache write requests. Writes are a significant, or often dominant, fraction of storage workloads. To cache write requests, the SSD cache should persistently and consistently manage its data and metadata, and guarantee no data loss even after a crash. Persistent cache management may require frequent metadata changes and causes high overhead. Some researchers insist that a nonvolatile persistent cache requires new additional primitives that are not supported by general SSDs in the market. We proposed a fully persistent read/write cache, which improves both read and write performance, does not require any special primitive, has a low overhead, guarantees the integrity of the cache metadata and the consistency of the cached data, even during a crash or power failure, and is able to recover the flash cache quickly without any data loss. We implemented the persistent read/write cache as a block device driver in Linux. Our scheme aims at virtual desktop infra servers. So the evaluation was performed with massive, real desktop traces of five users for ten days. The evaluation shows that our scheme outperforms an LRU version of SSD cache by 50% and the read-only version of our scheme by 37%, on average, for all experiments. This paper describes most of the parts of our scheme in detail. Detailed pseudo-codes are included in the Appendix.","Sung Hoon Baek and Ki-Woong Park",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"“We actually care and we want to make the parks better”: A qualitative study of youth experiences and perceptions after conducting park audits","This study explored youths' experiences and perceptions about community engagement as a result of participating in a community-based data collection project using paper and mobile technology park environmental audit tools. In July 2014, youth (ages 11–18, n=50) were recruited to participate in nine focus groups after auditing two parks each using paper, electronic, or both versions of the Community Park Audit Tool in Greenville County, SC. The focus groups explored the youths' experiences participating in the project, changes as a result of participation, suggested uses of park audit data collected, and who should use the tools. Four themes emerged related to youths' project participation experiences: two positive (fun and new experiences) and two negative (uncomfortable/unsafe and travel issues). Changes described as a result of participating in the project fell into four themes: increased awareness, motivation for further action, physical activity benefits, and no change. Additionally, youth had numerous suggestions for utilizing the data collected that were coded into six themes: maintenance & aesthetics, feature/amenity addition, online park information, park rating/review system, fundraising, and organizing community projects. Finally, six themes emerged regarding who the youth felt could use the tools: frequent park visitors, community groups/organizations, parks and recreation professionals, adults, youth, and everyone. This study revealed a wealth of information about youth experiences conducting park audits for community health promotion. Understanding youth attitudes and preferences can help advance youth empowerment and civic engagement efforts to promote individual and community health.","David G. Gallerani and Gina M. Besenyi and Sonja A. Wilhelm Stanis and Andrew T. Kaczynski",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Cladding in RPV Integrity and Lifetime Evaluation","According to all design codes (ASME, RCC-M, KTA, JARC, PNAEG,…) austenitic cladding on inner surface of RPVs is usually taken only as a protection against corrosion. Its tensile properties as well as its existence is not taken into account in design of the RPVs, i.e. in the determination of their dimensions as well as during stress analysis and comparison with allowable stresses/stress intensities. Austenitic cladding due to its manufacturing history – mostly by strip welding without subsequent austenitization – has not very high toughness that can be important in evaluation of some regimes like during emergency cooling when “PTS – pressurized thermal shock” is present. Existence of cladding, its properties, defectness and system of in-service NDT inspection determine the type, size and shape of so-called “postulated defect” in evaluation of RPV resistance against non-ductile failure. According to some codes (e.g. IAEA VERLIFE etc.) fracture properties must be taken into account in these calculations. Unfortunately, each of cladding layers has different properties and their fracture properties are changing during operation – some embrittlement can be found as a result of neutron irradiation. This paper will describe problems connected with the effect of cladding on RPV resistance against non-ductile failure during regimes of PTS type and also gives some examples and criteria for their evaluation.","M. Brumovsky and M. Kytka and R. Kopriva",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Discriminative local collaborative representation for online object tracking","Sparse representation has been widely applied to object tracking. However, most sparse representation based trackers only use the holistic template to encode the candidates, where the discriminative information to separate the target from the background is ignored. In addition, the sparsity assumption with the l1 norm minimization is computationally expensive. In this paper, we propose a robust discriminative local collaborative (DLC) representation algorithm for online object tracking. DLC collaboratively uses the local image patches of both the target templates and the background ones to encode the candidates by an efficient local regularized least square solver with the l2 norm minimization, where the feature vectors are obtained by employing an effective discriminative-pooling method. Furthermore, we formulate the tracking as a discriminative classification problem, where the classifier is online updated by using the candidates predicted according to the residuals of their local patches. To adapt to the appearance changes, we iteratively update the dictionary with the foreground and background templates from the current frame and take occlusions into account as well. Experimental results demonstrate that our proposed algorithm performs favorably against the state-of-the-art trackers on several challenging video sequences.","Si Chen and Shaozi Li and Rongrong Ji and Yan Yan and Shunzhi Zhu",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Low-disruptive dynamic updating of Java applications","Context
In-use software systems are destined to change in order to fix bugs or add new features. Shutting down a running system before updating it is a normal practice, but the service unavailability can be annoying and sometimes unacceptable. Dynamic software updating (DSU) migrates a running software system to a new version without stopping it. State-of-the-art Java DSU systems are unsatisfactory as they may cause a non-negligible system pause during updating.
Objective
In this paper we present Javelus, a Java HotSpot VM-based Java DSU system with very short pausing time.
Method
Instead of updating everything at once when the running application is suspended, Javelus only updates the changed code during the suspension, and migrates stale objects on-demand after the application is resumed. With a careful design this lazy approach neither sacrifices the update flexibility nor introduces unnecessary object validity checks or access indirections.
Results
Evaluation experiments show that Javelus can reduce the updating pausing time by one to two orders of magnitude without introducing observable overheads before and after the dynamic updating.
Conclusion
Our experience with Javelus indicates that low-disruptive and type-safe dynamic updating of Java applications can be practically achieved with a lazy updating approach.","Tianxiao Gu and Chun Cao and Chang Xu and Xiaoxing Ma and Linghao Zhang and Jian Lü",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"A collaborative platform for integrating and optimising Computational Fluid Dynamics analysis requests","A Virtual Integration Platform (VIP) is described which provides support for the integration of Computer-Aided Design (CAD) and Computational Fluid Dynamics (CFD) analysis tools into an environment that supports the use of these tools in a distributed collaborative manner. The VIP has evolved through previous EU research conducted within the VRShips-ROPAX 2000 (VRShips) project and the current version discussed here was developed predominantly within the VIRTUE project but also within the SAFEDOR project. The VIP is described with respect to the support it provides to designers and analysts in co-ordinating and optimising CFD analysis requests. Two case studies are provided that illustrate the application of the VIP within HSVA: the use of a panel code for the evaluation of geometry variations in order to improve propeller efficiency, and the use of a dedicated maritime RANS code (FreSCo) to improve the wake distribution for the VIRTUE tanker. A discussion is included detailing the background, application and results from the use of the VIP within these two case studies as well as how the platform was of benefit during the development and a consideration of how it can benefit HSVA in the future.","R.I. Whitfield and A.H.B. Duffy and S. Gatchell and J. Marzi and W. Wang",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Wize Mirror - a smart, multisensory cardio-metabolic risk monitoring system","In the recent years personal health monitoring systems have been gaining popularity, both as a result of the pull from the general population, keen to improve well-being and early detection of possibly serious health conditions and the push from the industry eager to translate the current significant progress in computer vision and machine learning into commercial products. One of such systems is the Wize Mirror, built as a result of the FP7 funded SEMEOTICONS (SEMEiotic Oriented Technology for Individuals CardiOmetabolic risk self-assessmeNt and Self-monitoring) project. The project aims to translate the semeiotic code of the human face into computational descriptors and measures, automatically extracted from videos, multispectral images, and 3D scans of the face. The multisensory platform, being developed as the result of that project, in the form of a smart mirror, looks for signs related to cardio-metabolic risks. The goal is to enable users to self-monitor their well-being status over time and improve their life-style via tailored user guidance. This paper is focused on the description of the part of that system, utilising computer vision and machine learning techniques to perform 3D morphological analysis of the face and recognition of psycho-somatic status both linked with cardio-metabolic risks. The paper describes the concepts, methods and the developed implementations as well as reports on the results obtained on both real and synthetic datasets.","Yasmina Andreu and Franco Chiarugi and Sara Colantonio and Giorgos Giannakakis and Daniela Giorgi and Pedro Henriquez and Eleni Kazantzaki and Dimitris Manousos and Kostas Marias and Bogdan J. Matuszewski and Maria Antonietta Pascali and Matthew Pediaditis and Giovanni Raccichini and Manolis Tsiknakis",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Gene selection for microarray cancer classification using a new evolutionary method employing artificial intelligence concepts","Gene selection is a demanding task for microarray data analysis. The diverse complexity of different cancers makes this issue still challenging. In this study, a novel evolutionary method based on genetic algorithms and artificial intelligence is proposed to identify predictive genes for cancer classification. A filter method was first applied to reduce the dimensionality of feature space followed by employing an integer-coded genetic algorithm with dynamic-length genotype, intelligent parameter settings, and modified operators. The algorithmic behaviors including convergence trends, mutation and crossover rate changes, and running time were studied, conceptually discussed, and shown to be coherent with literature findings. Two well-known filter methods, Laplacian and Fisher score, were examined considering similarities, the quality of selected genes, and their influences on the evolutionary approach. Several statistical tests concerning choice of classifier, choice of dataset, and choice of filter method were performed, and they revealed some significant differences between the performance of different classifiers and filter methods over datasets. The proposed method was benchmarked upon five popular high-dimensional cancer datasets; for each, top explored genes were reported. Comparing the experimental results with several state-of-the-art methods revealed that the proposed method outperforms previous methods in DLBCL dataset.","M. Dashtban and Mohammadali Balafar",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Attitudes toward alcohol use during pregnancy among women recruited from alcohol-serving venues in Cape Town, South Africa: A mixed-methods study","Background
The Western Cape Province of South Africa has one of the highest rates of fetal alcohol spectrum disorder (FASD) globally. Effective prevention of FASD requires understanding women's attitudes about alcohol use during pregnancy and whether these attitudes translate into behavior.
Objective
The goal of this mixed-methods study was to describe attitudes toward alcohol use during pregnancy and examine how these attitudes influence drinking behaviors during pregnancy.
Method
Over a five month period, 200 women were recruited from alcohol-serving venues in a township in Cape Town; a sub-set of 23 also completed in-depth interviews. Potential gaps between attitudes and behavior were described, and logistic regression models examined predictors of harmful attitudes toward alcohol use during pregnancy. Interviews were reviewed and coded for emergent themes.
Results
Most women (n = 176) reported at least one pregnancy. Among these, the majority (83%) had positive preventive attitudes, but more than half of these still reported alcohol use during a previous pregnancy. The strongest predictors of harmful attitudes were a history of physical or sexual abuse and drinking during a previous pregnancy. Qualitative analysis revealed several themes that contributed to alcohol use during pregnancy: 1) having an unplanned pregnancy; 2) drinking because of stress or to cope with abuse/trauma; 3) reliance on the venue for support; 4) socialization; and 5) feelings of invincibility.
Conclusions
The findings highlight an attitude-behavior gap and suggest that positive preventive attitudes are insufficient to elicit FASD preventive behavior. Interventions are needed that go beyond education to build intrinsic motivation and structural support to refrain from alcohol use during pregnancy.","Olivia V. Fletcher and Philip A. May and Soraya Seedat and Kathleen J. Sikkema and Melissa H. Watt",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Methodologies for an improved prediction of the isotopic content in high burnup samples. Application to Vandellós-II reactor core","Fuel cycles are designed with the aim of obtaining the highest amount of energy possible. Since higher burnup values are reached, it is necessary to improve our disposal designs, traditionally based on the conservative assumption that they contain fresh fuel. The criticality calculations involved must consider burnup by making the most of the experimental and computational capabilities developed, respectively, to measure and predict the isotopic content of the spent nuclear fuel. These high burnup scenarios encourage a review of the computational tools to find out possible weaknesses in the nuclear data libraries, in the methodologies applied and their applicability range. Experimental measurements of the spent nuclear fuel provide the perfect framework to benchmark the most well-known and established codes, both in the industry and academic research activity. For the present paper, SCALE 6.0/TRITON and MONTEBURNS 2.0 have been chosen to follow the isotopic content of four samples irradiated in the Spanish Vandellós-II pressurized water reactor up to burnup values ranging from 40 GWd/MTU to 75 GWd/MTU. By comparison with the experimental data reported for these samples, we can probe the applicability of these codes to deal with high burnup problems. We have developed new computational tools within MONTENBURNS 2.0. They make possible to handle an irradiation history that includes geometrical and positional changes of the samples within the reactor core. This paper describes the irradiation scenario against which the mentioned codes and our capabilities are to be benchmarked.","J.S. Martínez and O. Cabellos and C.J. Díez",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Is killer whale dialect evolution random?","The killer whale is among the few species in which cultural change accumulates over many generations, leading to cumulative cultural evolution. Killer whales have group-specific vocal repertoires which are thought to be learned rather than being genetically coded. It is supposed that divergence between vocal repertoires of sister groups increases gradually over time due to random learning mistakes and innovations. In this case, the similarity of calls across groups must be correlated with pod relatedness and, consequently, with each other. In this study we tested this prediction by comparing the patterns of call similarity between matrilines of resident killer whales from Eastern Kamchatka. We calculated the similarity of seven components from three call types across 14 matrilines. In contrast to the theoretical predictions, matrilines formed different clusters on the dendrograms made by different calls and even by different components of the same call. We suggest three possible explanations for this phenomenon. First, the lack of agreement between similarity patterns of different components may be the result of constraints in the call structure. Second, it is possible that call components change in time with different speed and/or in different directions. Third, horizontal cultural transmission of call features may occur between matrilines.","Olga A. Filatova and Alexandr M. Burdin and Erich Hoyt",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Species radiation by DNA replication that systematically exchanges nucleotides?","RNA and DNA syntheses share many properties. Therefore, the existence of ‘swinger’ RNAs, presumed ‘orphan’ transcripts matching genomic sequences only if transcription systematically exchanged nucleotides, suggests replication producing swinger DNA. Transcripts occur in many short-lived copies, the few cellular DNA molecules are long-lived. Hence pressures for functional swinger DNAs are greater than for swinger RNAs. Protein coding properties of swinger sequences differ from original sequences, suggesting rarity of corresponding swinger DNA. For genes producing structural RNAs, such as tRNAs and rRNAs, three exchanges (A<–>T, C<–>G and A<–>T+C<–>G) conserve self-hybridization properties. All nuclear eukaryote swinger DNA sequences detected in GenBank are for rRNA genes assuming A<–>T+C<–>G exchanges. In brachyuran crabs, 25 species had A<–>T+C<–>G swinger 18S rDNA, all matching the reverse-exchanged version of regular 18S rDNA of a related species. In this taxon, swinger replication of 18S rDNA apparently associated with, or even resulted in species radiation. A<–>T+C<–>G transformation doesn’t invert sequence direction, differing from inverted repeats. Swinger repeats (detectable only assuming swinger transformations, A<–>T+C<–>G swinger repeats most frequent) within regular human rRNAs, independently confirm swinger polymerizations for most swinger types. Swinger replication might be an unsuspected molecular mechanism for ultrafast speciation.","Hervé Seligmann",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"1SWASP J200503.05-343726.5: A high mass ratio eclipsing binary near the period limit","First CCD photometric light curves of the eclipsing binary system 1SWASP J200503.05-343726.5 are presented. Our complete light curves in V, R and I bands using the Bessell filter show an out-of-eclipsing distortion, which means that the components of the system may be active. The preliminary photometric solutions with a cool star-spot are derived by using the 2013 version of the Wilson–Devinney (W–D) code. The photometric solutions suggest that 1SWASP J200503.05-343726.5 is a shallow-contact eclipsing binary(f=9.0%) with a mass ratio of q=1.0705, which is very high for late-type binary systems near the period limit. The primary component is about 230  K hotter than the secondary component. Based on our new CCD eclipse times, the orbital period change was analyzed. According to O−C diagram, the orbital period of the 1SWASP J200503.05-343726.5 shows an increase at a rate of P˙=+5.43×10−8 days year−1. The period increase may be caused by mass transfer from the less massive component to the more massive one. This shallow-contact system may be formed from a detached short-period binary via orbital shrinkage because of dynamical interactions with a third component or by magnetic braking.","Zhang Bin and Qian Shengbang and Miloslav Zejda and Zhu Liying and Liu Nianping",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"CAVE-CL: An OpenCL version of the package for detection and quantitative analysis of internal cavities in a system of overlapping balls: Application to proteins","Here we present the revised and newly rewritten version of our earlier published CAVE package (Buša et al., 2010) which was originally written in FORTRAN. The package has been rewritten in C language, the algorithm has been parallelized and implemented using OpenCL. This makes the program convenient to run on platforms with Graphical Processing Units (GPUs). Improvements include also some modifications/optimizations of the original algorithm. A considerable improvement in the performance of the code has been achieved. A new tool called input_structure has been added which helps the user to make the data input and conversion more easier and universal.
New version program summary
Program Title: CAVE-CL, CAVE C Catalogue identifier: AEHC_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/aehc_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 32646 No. of bytes in distributed program, including test data, etc.: 444248 Distribution format: tar.gz Programming language: C, C++, OpenCL. Computer: PC with GPU. Operating system: OpenCL compatible systems. Has the code been vectorized or parallelized?: Parallelized using GPUs. A revised serial version (non GPU) is included in the package as well. Keywords: Proteins, Solvent accessible area, Excluded volume, Cavities, Analytic method, Stereographic projection, GPGPU, OpenCL. PACS: 82.20.Wt, 02.60.Cb, 02.70.Ns. Classification: 16.1. Catalogue identifier of previous version: AEHC_v1_0. Journal reference of previous version: Comput. Phys. Commun. 181 (2010) 2116. Does the new version supersede the previous version?: Yes Nature of problem: Molecular structure analysis. Solution method: Analytical method, which uses the stereographic transformation for exact detection of internal cavities in the system of overlapping balls and numerical algorithm for calculation of the volume and the surface area of cavities. Reasons for the new version: This work is in line with our global efforts to modernize the protein structure related algorithms and software packages developed in our research group during last several years [1–8]. These tools are keeping to receive considerable attention from researches and they have been used in solving many interesting research problems [9,10]. Among many others, one important application has been found by the members of our team [11]. Therefore, we think that there is a demand to revise and modernize these tools and to make them more efficient. Here we follow the approach used earlier in [8] to develop a new version of the CAVE package [7]. The original CAVE package was written in FORTRAN language. One of the reasons for the new version is to rewrite it in C in order to make it more friendly to the young researchers who are not familiar with FORTRAN. Another, a more important reason, is to use the possibilities of the contemporary hardware (for example, the modern graphical cards) to improve the performance of the package. We also want to allow the user to avoid the re-compiling of the program for every molecule during multiple calculations of the array of molecules. For this purpose we are providing the possibility to use general pdb files as an input. After compiling one time, the program can receive any number of input files successively. Also, we found it necessary to go through the algorithm and to optimize, where it is possible, the memory usage and to make the algorithm more efficient. Summary of revisions:1.Memory usage and language. The whole code has been ported into C and the static arrays have been replaced with dynamic memory allocation. This allows to load and handle the proteins of arbitrary size.2.Changes in the algorithm. Like in [8], the original method of North Pole test and molecule rotation [4] has been changed. The details of implementation and the benefits from this change are properly described in [8] and we find it not necessary to repeat it here.3.New tool. A module called input_structure which takes as an input a protein structure file in the format compatible with Protein Data Bank (pdb) [12] has been adopted from [8]. Using external tool allows users to create their own mappings of atoms and radii without re-compiling the module input_structure itself or the CAVE.It is the user’s responsibility to assign proper radii to each type of atoms. One can use any of the published standard sets of radii (see for example, [13–17]). Alternatively, the user can assign his own values for radii immediately in the module input_structure. The radii are assigned in a special file with extension pds (see the documentation) which consists of lines like this: ATOM CA ALA 2.0 which is read as “the Cα atom of Alanine has radius 2.0 Å”.4.Some computational tricks. In several parts of the program square roots were replaced by second powers and calls of sin and cos functions were replaced by calls to sincos allowing for further speed-up (in comparison to original FORTRAN version).The typical value of the relative error between results obtained by original (FORTRAN), C, and OpenCL versions was between 10−8 and 10−10 and it never exceeded 10−5. Small differences in results can be due to the implementation of compiler and specially in case of OpenCL also in the implementation of arithmetic by the GPU vendor. Table 1Speed-up of C and OpenCL versions of the program CAVE when compared to the original (FORTRAN) version calculated using GNU Fortran (gfort) and Intel® FORTRAN (ifort).ProteinNumberTimeSpeed-upSpeed-upSpeed-upPDB IDof atomsgfort (s)ifortCOpenCL1AUW013872345.981.561.941.971AUW13872235.101.701.753.321DJ30658061.371.551.951.971DJ3658068.801.811.703.121I0A013675358.881.571.971.971I0A13675254.811.671.853.441LD4015648305.431.551.871.911LD415648551.281.581.472.531M3Z011292181.311.541.921.951M3Z11292141.041.751.743.061OJX019368503.021.621.901.901OJX19368339.281.691.603.121OK4019358541.841.671.921.931OK419358414.962.322.394.101QI6014384292.941.551.901.931QI614384237.801.671.823.151QNW0716872.241.551.901.931QNW716871.351.851.673.111S3Q015358376.791.571.951.981S3Q15358349.481.762.033.271UPA016272390.911.551.901.931UPA16272289.551.671.843.091YLO015318306.271.551.881.921YLO15318326.211.611.903.142MYS0628751.371.551.981.972MYS628762.211.781.722.655.OpenCL implementation and testing results. OpenCL [18] is an open standard for parallel programming in heterogeneous systems. It is becoming increasingly popular and has proved to be an efficient tool for computations in different fields (see, for example, the most recent [19,20] and the references therein).Table 1 shows the speedup of the C and OpenCL implementations of CAVE as compared to the FORTRAN version. We compare both results obtained using free GNU FORTRAN (g77) and commercial (and faster) ifort. Speedup is calculated as a ratio between the original time obtained by FORTRAN and C or OpenCL version of program. Times of execution are measured in seconds. Fig. 1Dependence of speed-up of OpenCL CAVE on (a) the number of atoms, (b) the number of neighbors for testing sphere radius rp=0, and (c) rp=1.2. The numbers indicate the slopes of linear fits.One could expect greater speed-ups but the problem is that not the whole algorithm could be parallelized. Only about 1/3 of the whole program was parallelized and the effect of this is visible for the proteins with 2000 atoms and more if the calculation time of FORTRAN version is higher than approximately 10 s. The rest of the code is sequential and its parallelization will require entirely new algorithm which might be the future work. Fig. 1 shows the speed-up as a function of number of neighbors. This clearly indicates, that the effect of parallelization is stronger for proteins with many neighbors. This is also the reason, why the effect is not so strong for proteins with 0 testing sphere radius. Most of the cavities in such case are enclosed only in few (around 4–8) spheres, while in the case of 1.2 testing sphere radius we have easily 35 or more enclosing spheres.In global, we can see that C version is a good choice for general proteins (and testing sphere radius of 0), OpenCL is proper for larger proteins and larger computational times. 0 in the name of protein means that no probe radius has been added to the atomic radii. In other cases 1.2 Å was added to all atomic radii.All results were obtained on computer with Intel Core 2 Duo E8500 CPU running at 3.16 GHz with 4 GB RAM and GPU NVIDIA GTX470 and computer with Intel Xeon X5450 CPU running at 3.00 GHz with 32 GB RAM and dedicated NVIDIA C1060 GPU card.When considering which GPU to use, it is important to watch its double precision performance. Consumer oriented GPUs have usually intentionally decreased double precision performance and because of that results can be similar even if newer generation of GPUs is used. For instance in 2010 the performance in double precision of NVIDIA GPUs (except for highly specialized GPUs for scientific computing) was 1/8 of the performance in single precision. Nowadays (2014) this ratio is 1/24, meaning that GPUs from 2010 are as fast as current GPUs (except for special editions of GPUs or dedicated cards). Restrictions: None Running time: Depends on the size of the molecule under consideration. All test examples run under 1 min, usually under 30 s. The work was supported by Grants MOST 103-2120-M-001-005. References: [1] S. Hayryan, C.-K. Hu, S.-Y. Hu, R.-J. Shang, J. Comput. Chem. 22 (2001) 1287. [2] F. Eisenmenger, U.H.E. Hansmann, S. Hayryan, C.-K. Hu, Comput. Phys. Commun. 138 (2001) 192. [3] F. Eisenmenger, U.H.E. Hansmann, S. Hayryan, C.-K. Hu, Comput. Phys. Commun. 174 (2006) 422. [4] S. Hayryan, C.-K. Hu, J. Skřivánek, E. Hayryan, I. Pokorný, J. Comput. Chem. 26 (2005) 334. [5] J. Buša, J. Džurina, E. Hayryan, S. Hayryan, C.-K. Hu, J. Plavka, I. Pokorný, J. Skřivánek, M.-C. Wu, Comput. Phys. Commun. 165 (2005) 59. [6] J. Buša, S. Hayryan, C.-K. Hu, J. Skřivánek, M.-C. Wu, J. Comput. Chem. 30 (2009) 346. [7] J. Buša, S. Hayryan, M.-C. Wu, J. Skřivánek, C.-K. Hu, Comput. Phys. Commun. 181 (2010) 2116. [8] J. Buša Jr., S. Hayryan, M.-C. Wu, J. Buša, and C.-K. Hu, Comp. Phys. Comm. 183 (2012) 2494-2497. [9] H. L. Chen, et al., Proteins: Structure, Function, and Bioinformatics 78 (2010) 2973. [10] P. Kota, et al., Bioinformatics 27 (2011) 2209-2215. [11] M.-C. Wu, M. S. Li, W.-J. Ma, M. Kouza, C.-K. Hu, EPL 96 (2011) 68005. [12] http://www.rcsb.org. [13] B. Lee, F. M. Richards, J. Mol. Biol. 55 (1971) 379. [14] F. M. Richards, Annu. Rev. Bipohys. Bioeng. 6 (1977) 151. [15] A. Shrake, J. A. Rupley, J. Mol. Biol. 79 (1973) 351. [16] A. A. Rashin, M. Iofin, B. Honig, Biochemistry 25 (1986) 3619. [17] C. Chotia, Nature 248 (1974) 338. [18] http://www.khronos.org/opencl/. [19] M. Molero-Armenta, U. Iturraran-Viveros, S. Aparicio, et al., Comp. Phys. Commun. 185 (2014) 2683. [20] M. Bach, V. Lindenstruth, O. Philipsen, et al., Comp. Phys. Commun. 184 (2013) 2042.","Ján Buša and Ján Buša and Shura Hayryan and Chin-Kun Hu and Ming-Chya Wu",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"NSTX-U Control System Upgrades","The National Spherical Tokamak Experiment (NSTX) is undergoing a wealth of upgrades (NSTX-U). These upgrades, especially including an elongated pulse length, require broad changes to the control system that has served NSTX well. A new fiber serial Front Panel Data Port input and output (I/O) stream will supersede the aging copper parallel version. Driver support for the new I/O and cyber security concerns require updating the operating system from Redhat Enterprise Linux (RHEL) v4 to RedHawk (based on RHEL) v6. While the basic control system continues to use the General Atomics Plasma Control System (GA PCS), the effort to forward port the entire software package to run under 64-bit Linux instead of 32-bit Linux included PCS modifications subsequently shared with GA and other PCS users. Software updates focused on three key areas: (1) code modernization through coding standards (C99/C11), (2) code portability and maintainability through use of the GA PCS code generator, and (3) support of 64-bit platforms. Central to the control system upgrade is the use of a complete real time (RT) Linux platform provided by Concurrent Computer Corporation, consisting of a computer (iHawk), an operating system and drivers (RedHawk), and RT tools (NightStar). Strong vendor support coupled with an extensive RT toolset influenced this decision. The new real-time Linux platform, I/O, and software engineering will foster enhanced capability and performance for NSTX-U plasma control.","K.G. Erickson and D.A. Gates and S.P. Gerhardt and J.E. Lawson and R. Mozulay and P. Sichta and G.J. Tchilinguirian",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A Neurocomputational Account of How Inflammation Enhances Sensitivity to Punishments Versus Rewards","Background
Inflammation rapidly impairs mood and cognition and, when severe, can appear indistinguishable from major depression. These sickness responses are characterized by an acute reorientation of motivational state; pleasurable activities are avoided, and sensitivity to negative stimuli is enhanced. However, it remains unclear how these rapid shifts in behavior are mediated within the brain.
Methods
Here, we combined computational modeling of choice behavior, experimentally induced inflammation, and functional brain imaging (functional magnetic resonance imaging) to describe these mechanisms. Using a double-blind, randomized crossover study design, 24 healthy volunteers completed a probabilistic instrumental learning task on two separate occasions, one 3 hours after typhoid vaccination and one 3 hours after saline (placebo) injection. Participants learned to select high probability reward (win £1) and avoid high probability punishment (lose £1) stimuli. An action-value learning algorithm was fit to the observed behavior, then used within functional magnetic resonance imaging analyses to identify neural coding of prediction error signals driving motivational learning.
Results
Inflammation acutely biased behavior, enhancing punishment compared with reward sensitivity, through distinct actions on neural representations of reward and punishment prediction errors within the ventral striatum and anterior insula. Consequently, choice options leading to potential rewards were less behaviorally attractive, and those leading to punishments were more aversive.
Conclusions
Our findings demonstrate the neural mediation of a rapid, state-dependent reorientation of reward versus punishment sensitivity during inflammation. This mechanism may aid the adaptive reallocation of metabolic resources during acute sickness but might also account for maladaptive, motivational changes that underpin the association between chronic inflammation and depression.","Neil A. Harrison and Valerie Voon and Mara Cercignani and Ella A. Cooper and Mathias Pessiglione and Hugo D. Critchley",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Qualitative examination of cognitive change during PTSD treatment for active duty service members","The current study investigated changes in service members' cognitions over the course of Cognitive Processing Therapy (CPT) for posttraumatic stress disorder (PTSD). Sixty-three active duty service members with PTSD were drawn from 2 randomized controlled trials of CPT-Cognitive Only (CPT-C). Participants wrote an impact statement about the meaning of their index trauma at the beginning and again at the end of therapy. Clauses from each impact statement were qualitatively coded into three categories for analysis: assimilation, accommodation, and overaccommodation. The PTSD Checklist, Posttraumatic Symptom Scale-Interview Version, and the Beck Depression Inventory-II were administered at baseline and posttreatment. Repeated measures analyses documented a significant decrease in the percentage of assimilated or overaccommodated statements and an increase in the percentage of accommodated statements from the beginning to the end of treatment. Changes in accommodated statements over the course of treatment were negatively associated with PTSD and depression symptom severity, while statements indicative of overaccommodation were positively associated with both PTSD and depression symptom severity. Treatment responders had fewer overaccommodated and more accommodated statements. Findings suggest that CPT-C changes cognitions over the course of treatment. Methodological limitations and the lack of association between assimilation and PTSD symptom severity are further discussed.","Katherine A. Dondanville and Abby E. Blankenship and Alma Molino and Patricia A. Resick and Jennifer Schuster Wachen and Jim Mintz and Jeffrey S. Yarvis and Brett T. Litz and Elisa V. Borah and John D. Roache and Stacey Young-McCaughan and Elizabeth A. Hembree and Alan L. Peterson",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Responses of neurons in the medial temporal lobe during encoding and recognition of face-scene pairs","Associations between co-occurring stimuli are formed in the medial temporal lobe (MTL). Here, we recorded from 508 single and multi-units in the MTL while participants learned and retrieved associations between unfamiliar faces and unfamiliar scenes. Participant's memories for the face-scene pairs were later tested using cued recall and recognition tests. The results show that neurons in the parahippocampal cortex are most likely to respond with changes from baseline firing to these stimuli during both encoding and recognition, and this region showed the greatest proportion of cells showing differential responses depending on the phase of the task. Furthermore, we found that cells in the parahippocampal cortex that responded during both encoding and recognition were more likely to show decreases from baseline firing than cells that were only recruited during recognition, which were more likely to show increases in firing. Since all stimuli shown during recognition were familiar to the patients, these findings suggest that with familiarity, cell responses become more sharply tuned. No neurons in this region, however, were found to be affected by recombining face/scene pairs. Neurons in other MTL regions, particularly the hippocampus, were sensitive to stimulus configurations. Thus, the results support the idea that neurons in the parahippocampal cortex code for features of stimuli and neurons in the hippocampus are more likely to represent their specific configurations.","Indre V. Viskontas and Barbara J. Knowlton and Itzhak Fried",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Novel approaches to assess the quality of fertility data stored in dairy herd management software","ABSTRACT
Scientific journals and popular press magazines are littered with articles in which the authors use data from dairy herd management software. Almost none of such papers include data cleaning and data quality assessment in their study design despite this being a very critical step during data mining. This paper presents 2 novel data cleaning methods that permit identification of animals with good and bad data quality. The first method is a deterministic or rule-based data cleaning method. Reproduction and mutation or life-changing events such as birth and death were converted to a symbolic (alphabetical letter) representation and split into triplets (3-letter code). The triplets were manually labeled as physiologically correct, suspicious, or impossible. The deterministic data cleaning method was applied to assess the quality of data stored in dairy herd management from 26 farms enrolled in the herd health management program from the Faculty of Veterinary Medicine Ghent University, Belgium. In total, 150,443 triplets were created, 65.4% were labeled as correct, 17.4% as suspicious, and 17.2% as impossible. The second method, a probabilistic method, uses a machine learning algorithm (random forests) to predict the correctness of fertility and mutation events in an early stage of data cleaning. The prediction accuracy of the random forests algorithm was compared with a classical linear statistical method (penalized logistic regression), outperforming the latter substantially, with a superior receiver operating characteristic curve and a higher accuracy (89 vs. 72%). From those results, we conclude that the triplet method can be used to assess the quality of reproduction data stored in dairy herd management software and that a machine learning technique such as random forests is capable of predicting the correctness of fertility data.","K. Hermans and W. Waegeman and G. Opsomer and B. Van Ranst and J. De Koster and M. Van Eetvelde and M. Hostens",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Evaluation of distribution coefficients and concentration ratios of 90Sr and 137Cs in the Techa River and the Miass River","Empirical data on the behavior of radionuclides in aquatic ecosystems are needed for radioecological modeling, which is commonly used for predicting transfer of radionuclides, estimating doses, and assessing possible adverse effects on species and communities. Preliminary studies of radioecological parameters including distribution coefficients and concentration ratios, for 90Sr and 137Cs were not in full agreement with the default values used in the ERICA Tool and the RESRAD BIOTA codes. The unique radiation situation in the Techa River, which was contaminated by long-lived radionuclides (90Sr and 137Cs) in the middle of the last century allows improved knowledge about these parameters for river systems. Therefore, the study was focused on the evaluation of radioecological parameters (distribution coefficients and concentration ratios for 90Sr and 137Cs) for the Techa River and the Miass River, which is assumed as a comparison waterbody. To achieve the aim the current contamination of biotic and abiotic components of the river ecosystems was studied; distribution coefficients for 90Sr and 137Cs were calculated; concentration ratios of 90Sr and 137Cs for three fish species (roach, perch and pike), gastropods and filamentous algae were evaluated. Study results were then compared with default values available for use in the well-known computer codes ERICA Tool and RESRAD BIOTA (when site-specific data are not available). We show that the concentration ratios of 137Cs in whole fish bodies depend on the predominant type of nutrition (carnivores and phytophagous). The results presented here are useful in the context of improving of tools for assessing concentrations of radionuclides in biota, which could rely on a wider range of ecosystem information compared with the process limited the current versions of ERICA and RESRAD codes. Further, the concentration ratios of 90Sr are species-specific and strongly dependent on Ca2+ concentration in water. The universal characteristic allows us to combine the data of fish caught in the water with different mineralization by multiplying the concentration of Ca2+. The concentration ratios for fishes were well-fitted by Generalized Logistic Distribution function (GLD). In conclusion, the GLD can be used for probabilistic modeling of the concentration ratios in freshwater fishes to improve the confidence in the modeling results. This is important in the context of risk assessment and regulatory.","E.A. Shishkina and E.A. Pryakhin and I.Ya Popova and D.I. Osipov and Yu Tikhova and S.S. Andreyev and I.A. Shaposhnikova and E.A. Egoreichenkov and E.V. Styazhkina and L.V. Deryabina and G.A. Tryapitsina and V. Melnikov and G. Rudolfsen and H.-C. Teien and M.K. Sneve and A.V. Akleyev",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The effect of human interaction on guinea pig behavior in animal-assisted therapy","Guinea pigs are included in various animal-assisted interventions (AAIs), but no research has been published to date on behavioral changes in guinea pigs interacting with humans. The goal of this study was to evaluate the behavior in guinea pigs during animal-assisted therapy (AAT) and to identify factors that influence their stress and well-being. Five guinea pigs were studied during 50 observations in a randomized controlled within-subject design with repeated measurement. All guinea pigs were tested under all the following conditions: (1) therapy setting with retreat possibility (n = 20), (2) therapy setting without retreat possibility (n = 10), and (3) control setting without human interaction (n = 20). Behavior was coded according to a specifically designed ethogram using continuous recording and focal animal sampling with The Observer® XT 12.5. The data were analyzed using generalized linear mixed models with SPSS®, version 22.0. Results show that the frequency but not the duration of hiding was significantly increased in the therapy setting with retreat possibility compared to the control condition. During therapy with retreat possibility, the number of comfort behavior episodes stayed constant, while the number of startling and explorative behavior and the duration of locomotion increased significantly in comparison to the control setting. During therapy without retreat possibility, the frequency of freezing was increased significantly in comparison to the therapy setting with retreat possibility and the control setting. Comfort behavior was never observed during therapy without retreat possibility. This study provides evidence that the possibility of retreat is instrumental in reducing stress and should be provided during AAT using guinea pigs. In this form, AAT elicits limited stress and may possibly even provide enrichment. Further research is needed to understand factors influencing guinea pig behavior to ensure animal welfare in AAIs in the future.","Winnie Gut and Lisa Crump and Jakob Zinsstag and Jan Hattendorf and Karin Hediger",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Acoustic classification of Australian frogs based on enhanced features and machine learning algorithms","Frogs are often considered as excellent indicators of the overall state of the natural environment, but a steady decrease in the frog population has been noticed worldwide. To monitor this change of frog population and optimise the protection policy, frog call classification has become an important bioacoustic research topic. However, automatic acoustic classification of frog calls has not been adequately addressed in the literature. In this paper, an enhanced feature representation for frog call classification using the temporal, perceptual and cepstral features is presented. With the enhanced feature representation, the time-frequency information of frog calls can be effectively represented, which gives a good classification performance. To be specific, each continuous frog recording is first segmented into individual syllables using the Ha¨rma¨’s method. Then, temporal, perceptual, and cepstral features are calculated from each syllable: syllable duration, Shannon entropy, Rényi entropy, zero-crossing rate, averaged energy, oscillation rate, spectral centroid, spectral flatness, spectral roll-off, signal bandwidth, spectral flux, fundamental frequency, linear predictive coding, and Mel-frequency cepstral coefficients. Next, different feature vectors are fused to obtain different enhanced feature representations. Finally, different enhanced feature representations are compared using five machine learning algorithms: linear discriminant analysis, K-nearest neighbour, support vector machines, random forest, and artificial neural network. Experiment results show that our proposed feature representation could achieve better classification performance comparing to other methods with twenty-four frog species, which are geographically well distributed throughout Queensland, Australia.","Jie Xie and Michael Towsey and Jinglan Zhang and Paul Roe",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Interordinal gene capture, the phylogenetic position of Steller’s sea cow based on molecular and morphological data, and the macroevolutionary history of Sirenia","The recently extinct (ca. 1768) Steller’s sea cow (Hydrodamalis gigas) was a large, edentulous North Pacific sirenian. The phylogenetic affinities of this taxon to other members of this clade, living and extinct, are uncertain based on previous morphological and molecular studies. We employed hybridization capture methods and second generation sequencing technology to obtain >30kb of exon sequences from 26 nuclear genes for both H. gigas and Dugong dugon. We also obtained complete coding sequences for the tooth-related enamelin (ENAM) gene. Hybridization probes designed using dugong and manatee sequences were both highly effective in retrieving sequences from H. gigas (mean=98.8% coverage), as were more divergent probes for regions of ENAM (99.0% coverage) that were designed exclusively from a proboscidean (African elephant) and a hyracoid (Cape hyrax). New sequences were combined with available sequences for representatives of all other afrotherian orders. We also expanded a previously published morphological matrix for living and fossil Sirenia by adding both new taxa and nine new postcranial characters. Maximum likelihood and parsimony analyses of the molecular data provide robust support for an association of H. gigas and D. dugon to the exclusion of living trichechids (manatees). Parsimony analyses of the morphological data also support the inclusion of H. gigas in Dugongidae with D. dugon and fossil dugongids. Timetree analyses based on calibration density approaches with hard- and soft-bounded constraints suggest that H. gigas and D. dugon diverged in the Oligocene and that crown sirenians last shared a common ancestor in the Eocene. The coding sequence for the ENAM gene in H. gigas does not contain frameshift mutations or stop codons, but there is a transversion mutation (AG to CG) in the acceptor splice site of intron 2. This disruption in the edentulous Steller’s sea cow is consistent with previous studies that have documented inactivating mutations in tooth-specific loci of a variety of edentulous and enamelless vertebrates including birds, turtles, aardvarks, pangolins, xenarthrans, and baleen whales. Further, branch-site dN/dS analyses provide evidence for positive selection in ENAM on the stem dugongid branch where extensive tooth reduction occurred, followed by neutral evolution on the Hydrodamalis branch. Finally, we present a synthetic evolutionary tree for living and fossil sirenians showing several key innovations in the history of this clade including character state changes that parallel those that occurred in the evolutionary history of cetaceans.","Mark S. Springer and Anthony V. Signore and Johanna L.A. Paijmans and Jorge Vélez-Juarbe and Daryl P. Domning and Cameron E. Bauer and Kai He and Lorelei Crerar and Paula F. Campos and William J. Murphy and Robert W. Meredith and John Gatesy and Eske Willerslev and Ross D.E. MacPhee and Michael Hofreiter and Kevin L. Campbell",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The incidence of schizophrenia and schizophrenia spectrum disorders in Denmark in the period 2000–2012. A register-based study","Introduction
We aimed to examine changes over time in the incidence of broad and narrow schizophrenia spectrum disorders in Denmark from 2000 to 2012.
Methods
Patients were classified as incident schizophrenia if registered with a first time in- or outpatient contact with relevant diagnostic codes in the Danish Psychiatric Central Register between 2000 and 2012. Their history of contacts was traced back to 1969. Broad schizophrenia included schizophrenia, schizotypal disorder, persistent delusional disorder, acute and transient psychotic disorders, schizoaffective disorders, and other nonorganic and unspecified psychotic disorders, (ICD 10 codes F20–F29). Narrow schizophrenia was defined with the ICD 10 codes F20.0–F20.9. Incidence rates (IR) and incidence rate ratios (IRR) were calculated using Poisson regression.
Results
The IRR for broad schizophrenia increased by 1.43 (CI 95% 1.34–1.52) for females and 1.26 (CI 95% 1.20–1.33) for males. IRR for narrow schizophrenia increased by 1.36 (CI 95% 1.24–1.48) for females and 1.20 (CI 95% 1.11–1.29) for males. There was a significantly increased incidence in patients up to 32years of age. This was mainly explained by a significant 2–3 fold increase in outpatient incidence. We found a significant decrease in IRR for patients with broad and narrow schizophrenia aged 33 or older for both in- and outpatients.
Conclusion
The increased incidence of schizophrenia could partly be explained by better implementation of the diagnostic criteria for schizophrenia in child and adolescent psychiatry and improved access to early intervention services, but a true increase in incidence of schizophrenia cannot be excluded. The decrease of incidence in the older age group could indicate that the national Danish early intervention strategy was successful.","Johanne Olivia Grønne Kühl and Thomas Munk Laursen and Anne Thorup and Merete Nordentoft",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Learning from optimization: A case study with Apache Ant","Context
Software architecture degrades when changes violating the design-time architectural intents are imposed on the software throughout its life cycle. Such phenomenon is called architecture erosion. When changes are not controlled, erosion makes maintenance harder and negatively affects software evolution.
Objective
To study the effects of architecture erosion on a large software project and determine whether search-based module clustering might reduce the conceptual distance between the current architecture and the design-time one.
Method
To run an exploratory study with Apache Ant. First, we characterize Ant’s evolution in terms of size, change dispersion, cohesion, and coupling metrics, highlighting the potential introduction of architecture and code-level problems that might affect the cost of changing the system. Then, we reorganize the distribution of Ant’s classes using a heuristic search approach, intending to re-emerge its design-time architecture.
Results
In characterizing the system, we observed that its original, simple design was lost due to maintenance and the addition of new features. In optimizing its architecture, we found that current models used to drive search-based software module clustering produce complex designs, which maximize the characteristics driving optimization while producing class distributions that would hardly be acceptable to developers maintaining Ant.
Conclusion
The structural perspective promoted by the coupling and cohesion metrics precludes observing the adequate software module clustering from the perspective of software engineers when considering a large open source system. Our analysis adds evidence to the criticism of the dogma of driving design towards high cohesion and low coupling, at the same time observing the need for better models to drive design decisions. Apart from that, we see SBSE as a learning tool, allowing researchers to test Software Engineering models in extreme situations that would not be easily found in software projects.","Márcio de Oliveira Barros and Fábio de Almeida Farzat and Guilherme Horta Travassos",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Exp2 polymorphisms associated with variation for fiber quality properties in cotton (Gossypium spp.)","Plant expansins are a group of extracellular proteins thought to affect the quality of cotton fibers. Previous expression profile analysis revealed that six Expansin A genes are present in cotton, of which two (GhExp1 and GhExp2) produce transcripts that are specific to the developing cotton fiber. To identify the phenotypic function of Exp2, and to determine whether nucleotide variation among alleles of Exp2 affects fiber quality, candidate gene association mapping was conducted. Gene-specific primers were designed to amplify the Exp2 gene. By amplicon sequencing, the nucleotide diversity of Exp2 was investigated across 92 accessions (including 7 Gossypium arboreum, 74 Gossypium hirsutum, and 11 Gossypium barbadense accessions) with different fiber qualities. Twenty-six SNPs and seven InDels including 14 from the coding region of Exp2 were detected, forming twelve distinct haplotypes in the cotton collection. Among the 14 SNPs in the coding region, five were missense mutations and nine were synonymous nucleotide changes. The average SNP/InDel per nucleotide ratio was 2.61% (one SNP per 39bp), with 1.81 and 3.87% occurring in coding and non-coding regions, respectively. Nucleotide and haplotype diversity across the entire Exp2 region was 0.00603 (π) and 0.844, respectively, and diversity in non-coding regions was higher than that in coding regions. For linkage disequilibrium (LD), the mean r2 value for all polymorphism loci pairs was 0.48, and LD did not decay over 748bp. Based on 132 simple sequence repeat (SSR) loci evenly covering 26 chromosomes, the population structure was estimated, and the accessions were divided into seven groups that agreed well with their genomic origin and evolutionary history. A general linear model was used to calculate the Exp2-wide diversity–trait associations of 5 fiber quality traits, considering population structure (Q). Four SNPs in Exp2 were associated with at least one of the fiber quality traits, but not with fiber elongation. The highest positive effect on UHML and STR was observed for haplotype Hap_6 of Exp2. There was a significant association of Exp2 with fiber quality traits. There were many haplotypes in the Exp2 region, of which the most favorable was Hap_6. The association between nucleotide diversity and these fiber traits sheds light on the gene's potential contribution to the improvement of fiber quality, and should be useful to facilitate MAS programs in cotton.","Daohua He and Zhongping Lei and Hongyi Xing and Baoshan Tang and Junxing Zhao and Bixia Lu",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Accident analyses of selected postulated events for safety assessment of Indian LLCB TBS in ITER","The Lead Lithium cooled Ceramic Breeder (LLCB) TBM) is being developed by India for testing in ITER machine. Thermal hydraulic safety analyses are the essential part of engineering design of LLCB Test Blanket System (TBS) for safe operation in ITER environment. The current work involves thermal hydraulic analyses and preliminary results of selected accident scenarios involving ancillary systems, the First Wall Helium Cooling System (FWHCS) and the Lead Lithium Cooling System (LLCS). A modified version of RELAP/MOD4.0 safety analysis code is used to simulate the transient scenarios. A detailed nodalization is carried out with all the major components in FWHCS and LLCS along with associated heat structures and connected confinement volumes. The accident analysis is performed for the scenarios including the ex-vessel Loss Of Coolant Accident (LOCA) in FWHCS into Port Interspace, Loss Of Fluid Accident (LOFA) and Loss Of Heat Sink (LOHS) accident in FWHCS and LLCS. Transient analyses results of each accident case are reported here and it is found that current design meets most of the thermal hydraulic safety requirements while changes in the design are proposed for certain accident scenarios.","K.T. Sandeep and Vilas Chaudhari",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Understanding Pain and Pain Management in Elderly Nursing Home Patients Applying an Interprofessional Learning Activity in Health Care Students: A Norwegian Pilot Study","Background: Pain is common among elderly patients in nursing homes. However, pain assessment and treatment are inadequate. Interprofessional treatment is recommended, and consequently interprofessional education in pain management is necessary. Aims: This pilot project aimed to describe how two interprofessional groups of students approached pain management in two nursing home patients. Design: We formed two teams comprising one student from the nursing, physical therapy, pharmacy, and medical educations. Each team spent one day examining a patient with chronic pain at a nursing home and they developed pain management plans. Methods: We collected data through video recordings during teamwork before and after examining the patients and field notes during the patient examination. We analysed the video-recordings applying the seven-step model including 1) viewing the video data, 2) describing the video data, 3) identifying critical events, 4) transcribing, 5) coding, 6) constructing storyline and 7) composing a narrative. Field notes supplied the transcripts. Results: Both teams succeeded in making a pain management plan for their patient. The common examination of the patient was crucial for the students’ approaches to pain management and changed their pre-assumptions about the patients’ pain. By sharing knowledge and reflecting together, the students reached a common consensus on suggestions for management of the patients’ problems. Interprofessional collaboration fostered enthusiasm and a more holistic pain management approach. However, students’ lack of knowledge limited their understanding of pain. Conclusion: Knowledge of pain management in nursing home patients and the practice of interprofessional cooperation should be included in pain curricula for health care professionals.","Elin Damsgård and Hege Solgård and Karin Johannessen and Katrine Wennevold and Gunnvald Kvarstein and Gunn Pettersen and Beate Garcia",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"New version: Grasp2K relativistic atomic structure package","A revised version of Grasp2K [P. Jönsson, X. He, C. Froese Fischer, I.P. Grant, Comput. Phys. Commun. 177 (2007) 597] is presented. It supports earlier non-block and block versions of codes as well as a new block version in which the njgraf library module [A. Bar-Shalom, M. Klapisch, Comput. Phys. Commun. 50 (1988) 375] has been replaced by the librang angular package developed by Gaigalas based on the theory of [G. Gaigalas, Z.B. Rudzikas, C. Froese Fischer, J. Phys. B: At. Mol. Phys. 30 (1997) 3747, G. Gaigalas, S. Fritzsche, I.P. Grant, Comput. Phys. Commun. 139 (2001) 263]. Tests have shown that errors encountered by njgraf do not occur with the new angular package. The three versions are denoted v1, v2, and v3, respectively. In addition, in v3, the coefficients of fractional parentage have been extended to j=9/2, making calculations feasible for the lanthanides and actinides. Changes in v2 include minor improvements. For example, the new version of rci2 may be used to compute quantum electrodynamic (QED) corrections only from selected orbitals. In v3, a new program, jj2lsj, reports the percentage composition of the wave function in LSJ and the program rlevels has been modified to report the configuration state function (CSF) with the largest coefficient of an LSJ expansion. The bioscl2 and bioscl3 application programs have been modified to produce a file of transition data with one record for each transition in the same format as in Atsp2K [C. Froese Fischer, G. Tachiev, G. Gaigalas, M.R. Godefroid, Comput. Phys. Commun. 176 (2007) 559], which identifies each atomic state by the total energy and a label for the CSF with the largest expansion coefficient in LSJ intermediate coupling. All versions of the codes have been adapted for 64-bit computer architecture.
Program Summary
Program title: Grasp2K, version 1_1 Catalogue identifier: ADZL_v1_1 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ADZL_v1_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 730252 No. of bytes in distributed program, including test data, etc.: 14808872 Distribution format: tar.gz Programming language: Fortran. Computer: Intel Xeon, 2.66 GHz. Operating system: Suse, Ubuntu, and Debian Linux 64-bit. RAM: 500 MB or more Classification: 2.1. Catalogue identifier of previous version: ADZL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 177 (2007) 597 Does the new version supersede the previous version?: Yes Nature of problem: Prediction of atomic properties — atomic energy levels, oscillator strengths, radiative decay rates, hyperfine structure parameters, Landé gJ-factors, and specific mass shift parameters — using a multiconfiguration Dirac–Hartree–Fock approach. Solution method: The computational method is the same as in the previous Grasp2K [1] version except that for v3 codes the njgraf library module [2] for recoupling has been replaced by librang [3,4]. Reasons for new version: New angular libraries with improved performance are available. Also methodology for transforming from jj- to LSJ-coupling has been developed. Summary of revisions: New angular libraries where the coefficients of fractional parentage have been extended to j=9/2, making calculations feasible for the lanthanides and actinides. Inclusion of a new program jj2lsj, which reports the percentage composition of the wave function in LSJ. Transition programs have been modified to produce a file of transition data with one record for each transition in the same format as Atsp2K [C. Froese Fischer, G. Tachiev, G. Gaigalas and M.R. Godefroid, Comput. Phys. Commun. 176 (2007) 559], which identifies each atomic state by the total energy and a label for the CSF with the largest expansion coefficient in LSJ intermediate coupling. Updated to 64-bit architecture. A comprehensive user manual in pdf format for the program package has been added. Restrictions: The packing algorithm restricts the maximum number of orbitals to be ≤214. The tables of reduced coefficients of fractional parentage used in this version are limited to subshells with j≤9/2 [5]; occupied subshells with j>9/2 are, therefore, restricted to a maximum of two electrons. Some other parameters, such as the maximum number of subshells of a CSF outside a common set of closed shells are determined by a parameter.def file that can be modified prior to compile time. Unusual features: The bioscl3 program reports transition data in the same format as in Atsp2K [6], and the data processing program tables of the latter package can be used. The tables program takes a name.lsj file, usually a concatenated file of all the .lsj transition files for a given atom or ion, and finds the energy structure of the levels and the multiplet transition arrays. The tables posted at the website http://atoms.vuse.vanderbilt.edu are examples of tables produced by the tables program. With the extension of coefficients of fractional parentage to j=9/2, calculations for the lanthanides and actinides become possible. Running time: CPU time required to execute test cases: 70.5 s. References: [1]P. Jönsson, X. He, C. Froese Fischer, I.P. Grant, Comput. Phys. Commun. 177 (2007) 597.[2]A. Bar-Shalom, M. Klapisch, Comput. Phys. Commun. 50 (1988) 375.[3]G. Gaigalas, Z.B. Rudzikas, C. Froese Fischer, J. Phys. B: At. Mol. Phys. 30 (1997) 3747.[4]G. Gaigalas, S. Fritzsche, I.P. Grant, Comput. Phys. Commun. 139 (2001) 263.[5]G. Gaigalas, S. Fritzsche, Z. Rudzikas, At. Data Nucl. Data Tables 76 (2000) 235.[6]C. Froese Fischer, G. Tachiev, G. Gaigalas, M.R. Godefroid, Comput. Phys. Commun. 176 (2007) 559.","P. Jönsson and G. Gaigalas and J. Bieroń and C. Froese Fischer and I.P. Grant",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Vertical Axis Wind Turbine Design Load Cases Investigation and Comparison with Horizontal Axis Wind Turbine","The paper studies the applicability of the IEC 61400-1 ed.3, 2005 International Standard of wind turbine minimum design requirements in the case of an onshore Darrieus VAWT and compares the results of basic Design Load Cases (DLCs) with those of a 3-bladed HAWT. The study is based on aeroelastic computations using the HAWC2 aero-servo-elastic code A 2-bladed 5 MW VAWT rotor is used based on a modified version of the DeepWind rotor For the HAWT simulations the NREL 3-bladed 5 MW reference wind turbine model is utilized Various DLCs are examined including normal power production, emergency shut down and parked situations, from cut-in to cut-out and extreme wind conditions. The ultimate and 1 Hz equivalent fatigue loads of the blade root and turbine base bottom are extracted and compared in order to give an insight of the load levels between the two concepts. According to the analysis the IEC 61400-1 ed.3 can be used to a large extent with proper interpretation of the DLCs and choice of parameters such as the hub-height. In addition, the design drivers for the VAWT appear to differ from the ones of the HAWT. Normal operation results in the highest tower bottom and blade root loads for the VAWT, where parked under storm situation (DLC 6.2) and extreme operating gust (DLC 2.3) are more severe for the HAWT. Turbine base bottom and blade root edgewise fatigue loads are much higher for the VAWT compared to the HAWT. The interpretation and simulation of DLC 6.2 for the VAWT lead to blade instabilities, while extreme wind shear and extreme wind direction change are not critical in terms of loading of the VAWT structure. Finally, the extreme operating gust wind condition simulations revealed that the emerging loads depend on the combination of the rotor orientation and the time stamp that the frontal passage of gust goes through the rotor plane.","Christos Galinos and Torben J. Larsen and Helge A. Madsen and Uwe S. Paulsen",2016,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Experimental investigation of heat conduction performance and development of automatic temperature measurement device on modular artillery charge system","The heat conduction performance experiment is conducted on the modular artillery charge system (MACS). Utilizing the experimental measurement system, the change history of the modular charge temperature is obtained. On the basis of the heat conduction performance experiment of modular propellant charge, an unsteady-state heat conduction model describing the temperature change of the MACS is built and the finite-difference implicit schemes are theoretically deduced using the volume equilibrium method for numerical simulation. The validation of the numerical model is checked through compared with the experimental results. An automatic online temperature measurement device on the MACS is developed, based on the non-contact measurement method which is proposed in this paper. As a part of the device, an initial charge temperature sensor (ICTS) is also developed according to the similarity principle. The temperature measurement device where the numerical codes are embedded is assembled in the turret to calculation successively the temperature change of the modular charge with the environment temperature of ammunition rack. Meanwhile, for trajectory calculation and firing data correction, the temperature information obtained from the device may be simultaneously transferred to the gunner task terminal computer through a Controller Area Network (CAN) bus interface.","Xin Lu and Yanhuang Zhou and Jincao Chen and Dianjin Liu",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An Adaptive SIC Technique in DS-CDMA using Neural Network","In this paper, we propose an efficient successive interference cancellation (SIC) method in direct sequence code division multiple access (DS-CDMA) system using neural network (NN). Neural network is used here to estimate the amplitudes of different users signals under frequency selective Rayleigh fading channel. The correlation values between the received signal and the signature /spreading waveforms for different users are given as the inputs to a NN and the output acts as an estimation of corresponding user's signal amplitude. A closed mathematical form of joint probability of error (JPOE) is developed to determine the number of active users needs to be canceled to achieve a desired bit error rate (BER) value. Simulation results strongly support the mathematical results. Mathematical analysis shows that better performance results can be achieved through large change in weight up-gradation (w) for the strong users with a particular change in learning rate (η). Performance of the SIC system has been studied for initial wrong ordering of a couple of pairs of interfering users based on the correlation values. Simulation results show that BER performance is better when users are ordered based on signal to interference ratio (SIR) values rather than correlation values.","Santi P. Maity and Sumanta Hati",2012,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"“Have no regrets:” Parents' experiences and developmental tasks in pregnancy with a lethal fetal diagnosis","Significance
Lethal fetal diagnoses are made in 2% of all pregnancies. The pregnancy experience is certainly changed for the parents who choose to continue the pregnancy with a known fetal diagnosis but little is known about how the psychological and developmental processes are altered.
Methods
This longitudinal phenomenological study of 16 mothers and 14 fathers/partners sought to learn the experiences and developmental needs of parents who continue their pregnancy despite the lethal diagnosis. The study was guided by Merleau-Ponty's philosophic view of embodiment. Interviews (N = 90) were conducted with mothers and fathers over time, from mid-pregnancy until 2–3 months post birth. Data analysis was iterative, through a minimum of two cycles of coding, theme identification, within- and cross-case analysis, and the writing of results.
Results
Despite individual differences, parents were quite consistent in sharing that their overall goal was to “Have no regrets” when all was said and done. Five stages of pregnancy were identified: Pre-diagnosis, Learning Diagnosis, Living with Diagnosis, Birth & Death, and Post Death. Developmental tasks of pregnancy that emerged were 1) Navigating Relationships, 2) Comprehending Implication of the Condition, 3) Revising Goals of Pregnancy, 4) Making the Most of Time with Baby, 5) Preparing for Birth and Inevitable Death, 6) Advocating for Baby with Integrity, and 7) Adjusting to Life in Absence of Baby. Prognostic certainty was found to be highly influential in parents' progression through developmental tasks.
Conclusion
The framework of parents' pregnancy experiences with lethal fetal diagnosis that emerged can serve as a useful guide for providers who care for families, especially in perinatal palliative care. Providing patient-centered care that is matched to the stage and developmental tasks of these families may lead to improved care and greater parent satisfaction.","Denise Côté-Arsenault and Erin Denney-Koelsch",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Surgery Clerkship Evaluations Drive Improved Professionalism","Purpose
To determine whether a brief student survey can differentiate among third-year clerkship student's professionalism experiences and whether sharing specific feedback with surgery faculty and residents can lead to improvements.
Methods
Medical students completed a survey on professionalism at the conclusion of each third-year clerkship specialty rotation during academic years 2007-2010.
Results
Comparisons of survey items in 2007-2008 revealed significantly lower ratings for the surgery clerkship on both Excellence (F = 10.75, p < 0.001) and Altruism/Respect (F = 15.59, p < 0.001) subscales. These data were shared with clerkship directors, prompting the surgery department to discuss student perceptions of professionalism with faculty and residents. Postmeeting ratings of surgery professionalism significantly improved on both Excellence and Altruism/Respect dimensions (p < 0.005 for each).
Conclusions
A brief survey can be used to measure student perceptions of professionalism and an intervention as simple as a surgery department openly sharing results and communicating expectations appears to drive positive change in student experiences.","Frances E. Biagioli and Rebecca E. Rdesinski and Diane L. Elliot and Kathryn G. Chappelle and Karen L. Kwong and William L. Toffler",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Exploring Innovative Approaches and Patient-Centered Outcomes From Positive Outliers in Childhood Obesity","Objective
New approaches for obesity prevention and management can be gleaned from positive outliers—that is, individuals who have succeeded in changing health behaviors and reducing their body mass index (BMI) in the context of adverse built and social environments. We explored perspectives and strategies of parents of positive outlier children living in high-risk neighborhoods.
Methods
We collected up to 5 years of height/weight data from the electronic health records of 22,443 Massachusetts children, ages 6 to 12 years, seen for well-child care. We identified children with any history of BMI in the 95th percentile or higher (n = 4007) and generated a BMI z-score slope for each child using a linear mixed effects model. We recruited parents for focus groups from the subsample of children with negative slopes who also lived in zip codes where >15% of children were obese. We analyzed focus group transcripts using an immersion/crystallization approach.
Results
We reached thematic saturation after 5 focus groups with 41 parents. Commonly cited outcomes that mattered most to parents and motivated change were child inactivity, above-average clothing sizes, exercise intolerance, and negative peer interactions; few reported BMI as a motivator. Convergent strategies among positive outlier families were family-level changes, parent modeling, consistency, household rules/limits, and creativity in overcoming resistance. Parents voiced preferences for obesity interventions that include tailored education and support that extend outside clinical settings and are delivered by both health care professionals and successful peers.
Conclusions
Successful strategies learned from positive outlier families can be generalized and tested to accelerate progress in reducing childhood obesity.","Mona Sharifi and Gareth Marshall and Roberta Goldman and Sheryl L. Rifas-Shiman and Christine M. Horan and Renata Koziol and Richard Marshall and Thomas D. Sequist and Elsie M. Taveras",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Event-related potentials reveal modelling of auditory repetition in the brain","Two auditory event-related potential (ERP) waveforms, mismatch negativity (MMN) and repetition positivity (RP), are sensitive to repetition of auditory stimuli. Increasing repetition of standards produces larger MMN amplitudes to deviant stimuli in an oddball paradigm, known as the memory trace effect, and attributed to increasing strength of the memory trace for standards. RP to standards also increases as a function of repetition in a ‘roving’ oddball paradigm where the standard changes in pitch following presentation of a deviant tone. As the sensory memory trace representing standard stimuli must be continually updated in the roving paradigm, RP has been proposed to reflect memory trace formation. Given that RP to date has only been observed in roving oddball paradigms, we examined whether RP and the MMN memory trace effect are present in both roving and standard oddball paradigms in 24 young adults (mean age: 22.4±5years). Four, 8, or 16 standards preceded a deviant. We observed RP at Fz in standard ERPs in the roving but not constant paradigm. At mastoid sites, RP was observed in both paradigms. A memory trace effect was not observed at Fz in either paradigm. Our findings suggest that different generator sites in the brain model local and global auditory information with generators of mastoid activity primarily sensitive to local or short term stimulus history of auditory regularities while generators of frontal site activity retain more global information regarding stimulus history over a longer time period.","Rowena Jane Cooper and Rebbekah Josephine Atkinson and Rosemary Ann Clark and Patricia Therese Michie",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Catheter ablation of postinfarction ventricular tachycardia: Ten-year trends in utilization, in-hospital complications, and in-hospital mortality in the United States","Background
There is a paucity of data regarding the complications and in-hospital mortality after catheter ablation for ventricular tachycardia (VT) in patients with ischemic heart disease.
Objective
The purpose of this study was to determine the temporal trends in utilization, in-hospital mortality, and complications of catheter ablation of postinfarction VT in the United States.
Methods
We used the 2002–2011 Nationwide Inpatient Sample (NIS) database to identify all patients ≥18 years of age with a primary diagnosis of VT (International Classification of Diseases, Ninth Edition, Clinical Modification [ICD-9-CM] code 427.1) and who also had a secondary diagnosis of prior history of myocardial infarction (ICD-9-CM 412). Patients with supraventricular arrhythmias were excluded. Patients who underwent catheter ablation were identified using ICD-9-CM procedure code 37.34. Temporal trends in catheter ablation, in-hospital complications, and in-hospital mortality were analyzed.
Results
Of 81,539 patients with postinfarct VT, 4653 (5.7%) underwent catheter ablation. Utilization of catheter ablation increased significantly from 2.8% in 2002 to 10.8% in 2011 (Ptrend < .001). The overall rate of any in-hospital complication was 11.2% (523/4653), with vascular complications in 6.9%, cardiac in 4.3%, and neurologic in 0.5%. In-hospital mortality was 1.6% (75/4653). From 2002 to 2011, there was no significant change in the overall complication rates (8.4% to 10.2%, Ptrend = .101; adjusted odds ratio [per year] 1.02, 95% confidence interval 0.98–1.06) or in-hospital mortality (1.3% to 1.8%, Ptrend = .266; adjusted odds ratio [per year] 1.03, 95% confidence interval 0.92–1.15).
Conclusion
The utilization rate of catheter ablation as therapy for postinfarct VT has steadily increased over the past decade. However, procedural complication rates and in-hospital mortality have not changed significantly during this period.","Chandrasekar Palaniswamy and Dhaval Kolte and Prakash Harikrishnan and Sahil Khera and Wilbert S. Aronow and Marjan Mujib and William Michael Mellana and Paul Eugenio and Seth Lessner and Aileen Ferrick and Gregg C. Fonarow and Ali Ahmed and Howard A. Cooper and William H. Frishman and Julio A. Panza and Sei Iwai",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Photovoice: A Strategy to Better Understand the Reproductive and Sexual Health Needs of Young Mothers","Study Objective
Adolescent women face significant sexual and reproductive health challenges and are more vulnerable than their male peers. Photovoice methodology might allow them to provide more meaningful and accurate representations of the health challenges they encounter. Our objectives were to: (1) use Photovoice to understand how young mothers frame reproductive and sexual health within the context of their lives; (2) explore how they define reproductive and sexual health; (3) identify youth perspectives on how their life situations influence their ability to affect their health; and (4) connect their perspectives to social determinants of health framework to facilitate implementation of effective programs and policies to address their needs.
Design, Setting, Participants, Interventions, and Main Outcome Measures
This was a prospective qualitative community-based participatory research study involving young women (ages 15-25 years) recruited from a local youth outreach center. A 9-step validated qualitative participatory approach that combined documentary photography with focus groups was used. Qualitative analysis was conducted with NVivo version 10 software (QSR International Inc., Burlington, MA, USA). Data were coded and themes were developed.
Results
Thirty women were recruited and nine women completed the study. Key themes included: personal sexual health practices and coping skills, influence of poverty, physical environments, community resources and sexual health services, education, and stigma of pregnancy. Participating in community-based participatory research empowered participants to advocate for their own health.
Conclusion
Photovoice methodology contributes to understanding complex factors influencing sexual and reproductive health of young mothers. This participatory-based methodology highlights their individual situations, allowing us to seek connections, create analytical perspectives from which to relate their situations to root causes, and consider strategies for change.","Roopan Gill and Amanda Black and Tania Dumont and Nathalie Fleming",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"SUSY FLAVOR v2.5: A computational tool for FCNC and CP-violating processes in the MSSM","We present SUSY_FLAVOR version 2.5—a program that calculates over 30 low-energy flavor observables in the general R-parity conserving MSSM. Compared to previous versions, in SUSY_FLAVOR v2.5 parameter initialization in SLHA2 formats has been significantly generalized, so that the program accepts most of the output files produced by other libraries analyzing the MSSM phenomenology. A number of bugs and inconsistencies have been fixed, based on users feedback. Calculations of several processes implemented in the earlier version have been corrected. New processes of rare decays of the top quark to Higgs boson have been included. Variables controlling contributions from various MSSM sectors have been added.
New Version Program Summary
Program title: SUSY FLAVOR v2.5 Catalogue identifier: AEGV_v2_5 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEGV_v2_5.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 22623 No. of bytes in distributed program, including test data, etc.: 614938 Distribution format: tar.gz Programming language: Fortran 77. Computer: Any. Operating system: Any, tested on Linux. Classification: 11.6. Catalogue identifier of previous version: AEGV_v2_0 Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 1004 Does the new version supersede the previous version?: Yes Nature of problem: Predicting CP-violating observables, meson mixing parameters and branching ratios for a set of rare processes in the general R-parity conserving MSSM. Solution method: We use standard quantum theoretical methods to calculate Wilson coefficients in MSSM at one loop and including QCD corrections at higher orders when this is necessary and possible. Reasons for new version: The input/output routines have been rewritten to make them more flexible and compatible with the SLHA2 standard [1]. Calculations of the several processes implemented in earlier SUSY_FLAVOR versions have been corrected. New observables have been added. A number of bugs have been corrected. Summary of revisions:1.Modified initialization routines. Currently the program should be able to read without modifications most of the SLHA2-compatible output files produced by other publicly available libraries calculating observables related to the MSSM phenomenology. In addition, new optional input block SFLAV_HADRON has been defined to facilitate modifications of the parameters related to the hadronic and QCD sector.The initialization sequence now goes through the following steps: •Before reading the file, all parameters are set to some initial values (which can be changed by editing the values given in the subroutine sflav_defaults in file sflav_io.f).•Subsequently, the user-defined data are read from the file with the default name susy_flavor.in. Data are grouped in Blocks following the SLHA2 specification or extensions described in [2]. Blocks are read in the following order: SOFTINP, SMINPUTS, VCKMIN, MINPAR (tanβ only, other entries are ignored), EXTPAR, IMEXTPAR, MSL2IN, IMMSL2IN, MSE2IN, IMMSE2IN, TEIN, IMTEIN, TEINH, IMTEINH, MSQ2IN, IMMSQ2IN, MSU2IN, IMMSU2IN, MSD2IN, IMMSD2IN, TUIN, IMTUIN, TUINH, IMTUINH, TDIN, IMTDIN, TDINH, IMTDINH, SFLAV_HADRON.•The presence of any Block is optional—if some Block is absent, the program falls back to the default parameter values. At least flavor-diagonal SUSY mass parameters have to be defined, otherwise the vanishing default values cause the program to crash.•If a parameter is multiply defined in several Blocks, the value from Block read as latest in the list above overwrites (without warning!) the values from preceding Blocks.•Blocks do not need to be complete and to contain all the entries described in the SLHA2 specification—it is sufficient to define a minimal set of the parameters relevant for a given problem, others are filled with the default values.•The “non-holomorphic” LR mixing terms are not included in the SLHA2 specification and by default are set to 0. They can be initialized to the non-trivial values in the blocks TXINH and IMTXINH (X=E,D,U)•The new input block SFLAV_HADRON allows the modification of the hadronic- and QCD-related quantities used by SUSY_FLAVOR. The structure of this block and the default values of the hadronic parameters are shown at the end of the sample input file susy_flavor.in attached to the SUSY_FLAVOR distribution.2.New control variables have been added, allowing the separate switching of contributions from various MSSM sectors on or off. They can be set by the following statement at the beginning of the driver program: call set_active_sector(ih,ic,in,ig) where the variables ih, ic, in, ig can take values 0 or 1 and they control, respectively, the inclusion in the total result of the diagrams with gauge and Higgs bosons, charginos, neutralinos and gluinos exchanged in the loops. Note that diagrams with Higgs and gauge bosons are always added together and currently cannot be disentangled, so setting ih=1, ic=in=ig=0 does not reproduce the SM result. By default, if no call to set_active_sector is made, all control variables are assumed to be equal to 1, so that all contributions are included.3.Added or modified processes: •The expressions used to calculate the neutron Electric Dipole Moment have been modified.•The branching ratios for the radiative decays of the heavy lepton into the lighter lepton and the photon, μ→eγ and τ→eγ, μγ, are now normalized to the total heavy lepton decay width (previously they were normalized to the decay width into leptonic channels).•The routines calculating branching ratios of B→τν and B→Dτν decays have been generalized to include more general structure of the effective Higgs boson–fermion couplings. In addition the routine calculating Br(B→D∗τν) has been added.•The routines for rare decays of the top quark to the CP-even Higgs boson and the light quarks, t→ch, uh, have been added, based on Ref. [3] (program can calculate also the decay rates of the top quark to the heavier CP-even Higgs boson H, assuming that such decays are allowed kinematically).•The routine calculating the approximate 2-loop estimate of the neutral CP-even Higgs mass mh has been added, based on Ref. [4]. Note that for the more precise calculations of this mass other publicly available SUSY codes should be used.•Default values of numerous quantities which are treated by SUSY_FLAVOR as the external parameters, mainly the values of hadronic parameters obtained from lattice calculations and results of experimental measurements, have been updated to accommodate the latest published results.4.SUSY_FLAVOR’s output is now written to the file named susy_flavor.out. It has ”SLHA-like” structure, i.e. it is divided into “data blocks”, however these blocks are SUSY_FLAVOR specific and do not follow the common SLHA2 standards. The output file contains the following data blocks: •SFLAV_CONTROL: control variables and error code status.•SFLAV_MASS: MSSM mass spectrum after mass matrix diagonalization.•SFLAV_CHIRAL_YUKAWA: relative size of the resummed chiral corrections to the Yukawa couplings.•SFLAV_CHIRAL_CKM: relative size of the resummed chiral corrections to the CKM matrix elements.•SFLAV_DELTA_F0: ΔF=0 observables: leptonic EDMs and g−2 anomalies, neutron EDM.•SFLAV_DELTA_F1: ΔF=1 observables: decay rates of l→l′γ, K→πν̄ν, B+→τ+ν, B→Dτν, B→D∗τν, B→Xsγ, Bd,s→li+lj−, t→uh, t→ch.•SFLAV_DELTA_F2: ΔF=2 observables: εK, ΔmK, ΔmD, ΔmBd, ΔmBs.Blocks SFLAV_CHIRAL_YUKAWA and SFLAV_CHIRAL_CKM show the relative differences of bare and physical Yukawa couplings and CKM matrix elements after the resummation of chiral corrections. If they are large, ≥O(1), the perturbation expansion is not reliable and the remaining program output may not be correct.5.The new integrated manual for SUSY_FLAVOR_v2.5 has been created, including the detailed description of the modifications listed above. It is attached to the SUSY_FLAVOR distribution. Regular code distribution updates and bug fixes (between the major revisions submitted to Computer Physics Communications) can be found on the program web page www.fuw.edu.pl/susy_flavor. Restrictions: The results apply only to the case of MSSM with R-parity conservation and without heavy right neutrino sector [5]. Additional comments: This program has been cataloged as AEGV_v2_5 to conform to the manuscript version number. There are no programs cataloged as, AEGV_v2_1, AEGV_v2_2, AEGV_v2_3, AEGV_v2_4, in the CPC Program Library. Running time: For a single parameter set, under 1s on a personal computer. References: [1] B. Allanach et al., Comput. Phys. Commun. 180 (2009) 8 [arXiv:0801.0045 [hep-ph]]. [2] J. Rosiek, P Chankowski, A. Dedes, S. Jager and P. Tanedo, Comput. Phys. Commun. 181 (2010) 2180 [arXiv:1003.4260 [hep-ph]]; A. Crivellin, J. Rosiek et. al., Comput. Phys. Commun. 184 (2013) 1004, [arXiv:1203.5023” [hep-ph]]. [3] A. Dedes, M. Paraskevas, J. Rosiek, K. Suxho and K. Tamvakis, arXiv:1409.6546 [hep-ph]. [4] S. Heinemeyer, W. Hollik and G. Weiglein, Phys. Lett. B455 (1999) 179–191 [hep-ph/9903404] [5] A. Dedes, H. Haber and J. Rosiek, JHEP 0711 (2007) 059, [arXiv:0707.3718 [hep-ph]].","J. Rosiek",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"National Trends and Outcomes of Transjugular Intrahepatic Portosystemic Shunt Creation Using the Nationwide Inpatient Sample","ABSTRACT
Purpose
To elucidate trends in transjugular intrahepatic portosystemic shunt (TIPS) use and outcomes over the course of a decade, including predictors of inpatient mortality and extended length of hospital stay.
Materials and Methods
The Nationwide Inpatient Sample was interrogated for the most recent 10 years available: 2003–2012. TIPS procedures and associated diagnoses were identified via International Classification of Diseases (version 9) codes, with the latter categorized into primary diagnoses in a hierarchy of disease severity. Linear regression analysis was used to determine trends of TIPS use and outcomes over time. Independent predictors of mortality and extended length of stay were determined by logistic regression.
Results
A total of 55,145 TIPS procedures were captured during the study period. Annual procedural volume did not change significantly (5,979 in 2003, 5,880 in 2012). The majority of TIPSs were created for ascites and/or varices (84%). Inpatient mortality (12.5% in 2003, 10.6% in 2012; P < .05) decreased but varied considerably by diagnosis (from 3.7% to 59.3%), with a disparity between bleeding and nonbleeding varices (18.7% vs 3.8%; P < .01). Multivariate predictors of mortality (P < .001 for all) included primary diagnoses (bleeding varices, hepatorenal and abdominal compartment syndromes), patient characteristics (age > 80 y, black race), and sequelae of advanced cirrhosis (comorbid hepatocellular carcinoma, spontaneous bacterial peritonitis, encephalopathy, and coagulopathy).
Conclusions
National TIPS inpatient mortality has decreased since 2003 while procedural volume has not changed. Postprocedural outcome is a function of patient demographic and socioeconomic factors and associated diagnoses. Independent predictors of poor outcome identified in this large national population study may aid clinicians in better assessing preprocedural risk.","Premal S. Trivedi and Paul J. Rochon and Janette D. Durham and Robert K. Ryu",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"100 lines of code for shape-based object localization","We introduce a simple and effective concept for localizing objects in densely cluttered edge images based on shape information. The shape information is characterized by a binary template of the object's contour, provided to search for object instances in the image. We adopt a segment-based search strategy, in which the template is divided into a set of segments. In this work, we propose our own segment representation that we call one-pixel segment (OPS), in which each pixel in the template is treated as a separate segment. This is done to achieve high flexibility that is required to account for intra-class variations. OPS representation can also handle scale changes effectively. A dynamic programming algorithm uses the OPS representation to realize the search process, enabling a detailed localization of the object boundaries in the image. The concept's simplicity is reflected in the ease of implementation, as the paper's title suggests. The algorithm works directly with very noisy edge images extracted using the Canny edge detector, without the need for any preprocessing or learning steps. We present our experiments and show that our results outperform those of very powerful, state-of-the-art algorithms.","Alaa Halawani and Haibo Li",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Studying teacher selection of resources in an ultra-large scale interactive system: Does metadata guide the way?","Ultra-large-scale interactive systems on the Internet have begun to change how teachers prepare for instruction, particularly in regards to resource selection. Consequently, it is important to look at how teachers are currently selecting resources beyond content or keyword search. We conducted a two-part observational study of an existing popular system called TeachersPayTeachers hypothesizing that ‘evaluative metadata’ (i.e. comments, ratings, and popularity measures) would drive selection of resources. The first part examined patterns in tens of thousands of sales overall, and the second part focused on patterns of sales in one focal topic that could be expert coded. We find that there are significant gaps in available metadata, that some aspects of metadata are closely associated with sales, and that metadata are weak correlates of expert-determined quality. We conclude by making suggestions for additional research and suggesting how ultra-large scale-interactive systems such as TeachersPayTeachers could be used to improve teacher education.","Samuel Abramovich and Christian Schunn",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mixing numerical and categorical data in a Self-Organizing Map by means of frequency neurons","Even though Self-Organizing Maps (SOMs) constitute a powerful and essential tool for pattern recognition and data mining, the common SOM algorithm is not apt for processing categorical data, which is present in many real datasets. It is for this reason that the categorical values are commonly converted into a binary code, a solution that unfortunately distorts the network training and the posterior analysis. The present work proposes a SOM architecture that directly processes the categorical values, without the need of any previous transformation. This architecture is also capable of properly mixing numerical and categorical data, in such a manner that all the features adopt the same weight. The proposed implementation is scalable and the corresponding learning algorithm is described in detail. Finally, we demonstrate the effectiveness of the presented algorithm by applying it to several well-known datasets.","Carmelo del Coso and Diego Fustes and Carlos Dafonte and Francisco J. Nóvoa and José M. Rodríguez-Pedreira and Bernardino Arcay",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The experience and impact of traumatic perinatal event experiences in midwives: A qualitative investigation","Background
Through their work midwives may experience distressing events that fulfil criteria for trauma. However, there is a paucity of research examining the impact of these events, or what is perceived to be helpful/unhelpful by midwives afterwards.
Objective
To investigate midwives’ experiences of traumatic perinatal events and to provide insights into experiences and responses reported by midwives with and without subsequent posttraumatic stress symptoms.
Design
Semi-structured telephone interviews were conducted with a purposive sample of midwives following participation in a previous postal survey.
Methods
35 midwives who had all experienced a traumatic perinatal event defined using the Diagnostic and Statistical Manual of Mental Disorders (version IV) Criterion A for posttraumatic stress disorder were interviewed. Two groups of midwives with high or low distress (as reported during the postal survey) were purposefully recruited. High distress was defined as the presence of clinical levels of PTSD symptomatology and high perceived impairment in terms of impacts on daily life. Low distress was defined as any symptoms of PTSD present were below clinical threshold and low perceived life impairment. Interviews were analysed using template analysis, an iterative process of organising and coding qualitative data chosen for this study for its flexibility. An initial template of four a priori codes was used to structure the analysis: event characteristics, perceived responses and impacts, supportive and helpful strategies and reflection of change over time codes were amended, integrated and collapsed as appropriate through the process of analysis. A final template of themes from each group is presented together with differences outlined where applicable.
Results
Event characteristics were similar between groups, and involved severe, unexpected episodes contributing to feeling ‘out of a comfort zone.’ Emotional upset, self-blame and feelings of vulnerability to investigative procedures were reported. High distress midwives were more likely to report being personally upset by events and to perceive all aspects of personal and professional lives to be affected. Both groups valued talking about the event with peers, but perceived support from senior colleagues and supervisors to be either absent or inappropriate following their experience; however, those with high distress were more likely to endorse this view and report a perceived need to seek external input.
Conclusion
Findings indicate a need to consider effective ways of promoting and facilitating access to support, at both a personal and organisational level, for midwives following the experience of a traumatic perinatal event.","Kayleigh Sheen and Helen Spiby and Pauline Slade",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"MicroRNA-155 deletion reduces anxiety- and depressive-like behaviors in mice","Depressive disorders have complex and multi-faceted underlying mechanisms, rendering these disorders difficult to treat consistently and effectively. One under-explored therapeutic strategy for alleviating mood disorders is the targeting of microRNAs (miRs). miRs are small non-coding RNAs that cause sequestration/degradation of specific mRNAs, thereby preventing protein translation and downstream functions. miR-155 has validated and predicted neurotrophic factor and inflammatory mRNA targets, which led to our hypothesis that miR-155 deletion would modulate affective behaviors. To evaluate anxiety-like behavior, wildtype (wt) and miR-155 knockout (ko) mice (littermates; both male and female) were assessed in the open field and on an elevated plus maze. In both tests, miR-155 ko mice spent more time in open areas, suggesting they had reduced anxiety-like behavior. Depressive-like behaviors were assessed using the forced swim test. Compared to wt mice, miR-155 ko mice exhibited reduced float duration and increased latency to float. Further, although all mice exhibited a strong preference for a sucrose solution over water, this preference was enhanced in miR-155 ko mice. miR-155 ko mice had no deficiencies in learning and memory (Barnes maze) or social preference/novelty suggesting that changes in mood were specific. Finally, compared to wt hippocampi, miR-155 ko hippocampi had a reduced inflammatory signature (e.g., decreased IL-6, TNF-a) and female miR-155 ko mice increased ciliary neurotrophic factor expression. Together, these data highlight the importance of studying microRNAs in the context of anxiety and depression and identify miR-155 as a novel potential therapeutic target for improving mood disorders.","Laura K. Fonken and Andrew D. Gaudet and Kristopher R. Gaier and Randy J. Nelson and Phillip G. Popovich",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Reinforced concrete wide and conventional beam–column connections subjected to lateral load","In order to illustrate the structural performance of reinforced concrete wide beam–column connection, an experimental research was carried out to compare the behavior of two RC wide beam–column connections and two conventional beam–column connections when subjected to quasi-static cyclic loading. The specimens were full-scale connections and they were composed of two sets of interior and exterior joints. These specimens were designed in accordance with Syrian code of practice version 2006 (dependent on ACI 318 and ACI 352-R02 codes). Transverse beams in the wide beam–column joints, were also wider than the columns. Experimental results indicated that the hysteresis response of the wide beams was likely exhibited remarkable enhancement compared to that of conventional beams and the total energy dissipating capacity of a wide beam–column connection was higher than the conventional joint. Also it was found that by presence of the longitudinal reinforcement of the spandrel beam which was also a wide beam in the wide beam–column joints, flexural hinging mechanism in the wide beam was occurred instead of torsion brittle mode of failure. The experimental behavior of the sub-assemblages is reported within this paper.","Issa Fadwa and Tasnimi Abbas Ali and Eilouch Nazih and Mirzabagheri Sara",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A probability-conserving cross-section biasing mechanism for variance reduction in Monte Carlo particle transport calculations","In Monte Carlo particle transport codes, it is often important to adjust reaction cross-sections to reduce the variance of calculations of relatively rare events, in a technique known as non-analog Monte Carlo. We present the theory and sample code for a Geant4 process which allows the cross-section of a G4VDiscreteProcess to be scaled, while adjusting track weights so as to mitigate the effects of altered primary beam depletion induced by the cross-section change. This makes it possible to increase the cross-section of nuclear reactions by factors exceeding 104 (in appropriate cases), without distorting the results of energy deposition calculations or coincidence rates. The procedure is also valid for bias factors less than unity, which is useful in problems that involve the computation of particle penetration deep into a target (e.g. atmospheric showers or shielding studies).","Marcus H. Mendenhall and Robert A. Weller",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Object tracking using discriminative sparse appearance model","Object tracking based on sparse representation formulates tracking as searching the candidate with minimal reconstruction error in target template subspace. The key problem lies in modeling the target robustly to vary appearances. The appearance model in most sparsity-based trackers has two main problems. The first is that global structural information and local features are insufficiently combined because the appearance is modeled separately by holistic and local sparse representations. The second problem is that the discriminative information between the target and the background is not fully utilized because the background is rarely considered in modeling. In this study, we develop a robust visual tracking algorithm by modeling the target as a model for discriminative sparse appearance. A discriminative dictionary is trained from the local target patches and the background. The patches display the local features while their position distribution implies the global structure of the target. Thus, the learned dictionary can fully represent the target. The incorporation of the background into dictionary learning also enhances its discriminative capability. Upon modeling the target as a sparse coding histogram based on this learned dictionary, our tracker is embedded into a Bayesian state inference framework to locate a target. We also present a model update scheme in which the update rate is adjusted automatically. In conjunction with the update strategy, the proposed tracker can handle occlusion and alleviate drifting. Comparative results on challenging benchmark image sequences show that the tracking method performs favorably against several state-of-the-art algorithms.","Dandan Huang and Yi Sun",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Alteration of transcriptional networks in the entorhinal cortex after maternal immune activation and adolescent cannabinoid exposure","Maternal immune activation (MIA) and adolescent cannabinoid exposure (ACE) have both been identified as major environmental risk factors for schizophrenia. We examined the effects of these two risk factors alone, and in combination, on gene expression during late adolescence. Pregnant rats were exposed to the viral infection mimic polyriboinosinic-polyribocytidylic acid (poly I:C) on gestational day (GD) 15. Adolescent offspring received daily injections of the cannabinoid HU210 for 14days starting on postnatal day (PND) 35. Gene expression was examined in the left entorhinal cortex (EC) using mRNA microarrays. We found prenatal treatment with poly I:C alone, or HU210 alone, produced relatively minor changes in gene expression. However, following combined treatments, offspring displayed significant changes in transcription. This dramatic and persistent alteration of transcriptional networks enriched with genes involved in neurotransmission, cellular signalling and schizophrenia, was associated with a corresponding perturbation in the expression of small non-coding microRNA (miRNA). These results suggest that a combination of environmental exposures during development leads to significant genomic remodeling that disrupts maturation of the EC and its associated circuitry with important implications as the potential antecedents of memory and learning deficits in schizophrenia and other neuropsychiatric disorders.","Sharon L. Hollins and Katerina Zavitsanou and Frederick Rohan Walker and Murray J. Cairns",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"PAREMD: A parallel program for the evaluation of momentum space properties of atoms and molecules","The present work describes a code for evaluating the electron momentum density (EMD), its moments and the associated Shannon information entropy for a multi-electron molecular system. The code works specifically for electronic wave functions obtained from traditional electronic structure packages such as GAMESS and GAUSSIAN. For the momentum space orbitals, the general expression for Gaussian basis sets in position space is analytically Fourier transformed to momentum space Gaussian basis functions. The molecular orbital coefficients of the wave function are taken as an input from the output file of the electronic structure calculation. The analytic expressions of EMD are evaluated over a fine grid and the accuracy of the code is verified by a normalization check and a numerical kinetic energy evaluation which is compared with the analytic kinetic energy given by the electronic structure package. Apart from electron momentum density, electron density in position space has also been integrated into this package. The program is written in C++ and is executed through a Shell script. It is also tuned for multicore machines with shared memory through OpenMP. The program has been tested for a variety of molecules and correlated methods such as CISD, Møller–Plesset second order (MP2) theory and density functional methods. For correlated methods, the PAREMD program uses natural spin orbitals as an input. The program has been benchmarked for a variety of Gaussian basis sets for different molecules showing a linear speedup on a parallel architecture.
Program summary
Program Title: PAREMD Program Files doi:http://dx.doi.org/10.17632/gcr9gmh6zv.1 Licensing provisions: GPLv3 Programming language: C, Csh External routines/libraries: GSL[1], BLAS[2,3], OpenMP[4], GAMESS[5,6] and GAUSSIAN[7]. Nature of problem: Momentum space properties for a multi-electron system. Solution method: Analytic Fourier transformation of Gaussian basis in position space to get momentum space basis followed by EMD evaluation on spherical or Cartesian grids. A numerical integration procedure is implemented for evaluating moments of EMD and Shannon information entropy on a polar grid. [1] Galassi et al, GNU Scientific Library Reference Manual (3rd Ed.), ISBN 0954612078. [2] L. S. Blackford, J. Demmel, J. Dongarra, I. Duff, S. Hammarling, G. Henry, M. Heroux, L. Kaufman, A. Lumsdaine, A. Petitet, R. Pozo, K. Remington, R. C. Whaley, An Updated Set of Basic Linear Algebra Subprograms (BLAS), ACM Trans. Math. Soft., 28-2 (2002), pp. 135–151. [3] J. Dongarra, Basic Linear Algebra Subprograms Technical Forum Standard, International Journal of High Performance Applications and Supercomputing, 16(1) (2002), pp. 1–111, and International Journal of High Performance Applications and Supercomputing, 16(2) (2002), pp. 115–199. [4] OpenMP Architecture Review Board, ”OpenMP Application Program Interface, Version 4.5”, November 2015. [5] ”General Atomic and Molecular Electronic Structure System” M.W. Schmidt, K.K. Baldridge, J.A. Boatz, S.T. Elbert, M.S. Gordon, J.H. Jensen, S. Koseki, N. Matsunaga, K.A. Nguyen, S.J. Su, T.L. Windus, M. Dupuis, J.A. MontgomeryJ. Comput. Chem. 14 1347–1363, 1993. [6] “Advances in electronic structure theory: GAMESS a decade later M.S. Gordon, M.W. Schmidt pp. 1167–1189, in Theory and Applications of Computational Chemistry: the first forty years C.E. Dykstra, G. Frenking, K.S. Kim, G.E. Scuseria (editors), Elsevier, Amsterdam, 2005. [7] Gaussian 09, Revision C.01, M. J. Frisch, G. W. Trucks, H. B. Schlegel, G. E. Scuseria, M. A. Robb, J. R. Cheeseman, G. Scalmani, V. Barone, G. A. Petersson, H. Nakatsuji, X. Li, M. Caricato, A. Marenich, J. Bloino, B. G. Janesko, R. Gomperts, B. Mennucci, H. P. Hratchian, J. V. Ortiz, A. F. Izmaylov, J. L. Sonnenberg, D. Williams-Young, F. Ding, F. Lipparini, F. Egidi, J. Goings, B. Peng, A. Petrone, T. Henderson, D. Ranasinghe, V. G. Zakrzewski, J. Gao, N. Rega, G. Zheng, W. Liang, M. Hada, M. Ehara, K. Toyota, R. Fukuda, J. Hasegawa, M. Ishida, T. Nakajima, Y. Honda, O. Kitao, H. Nakai, T. Vreven, K. Throssell, J. A. Montgomery, Jr., J. E. Peralta, F. Ogliaro, M. Bearpark, J. J. Heyd, E. Brothers, K. N. Kudin, V. N. Staroverov, T. Keith, R. Kobayashi, J. Normand, K. Raghavachari, A. Rendell, J. C. Burant, S. S. Iyengar, J. Tomasi, M. Cossi, J. M. Millam, M. Klene, C. Adamo, R. Cammi, J. W. Ochterski, R. L. Martin, K. Morokuma, O. Farkas, J. B. Foresman, and D. J. Fox, Gaussian, Inc., Wallingford CT, 2016.","Deep Raj Meena and Shridhar R. Gadre and P. Balanarayan",2018,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Molecular characterization and expression analysis of glucose transporter 1 and hepatic glycolytic enzymes activities from herbivorous fish Ctenopharyngodon idellus in respond to a glucose load after the adaptation to dietary carbohydrate levels","In the present study, we test the hypothesis that the ability of Ctenopharyngodon idellu to clear a glucose load is related to nutrient history of different dietary carbohydrate through the transport and glycolysis of glucose. Firstly, we obtained the complete coding sequence from C. idellus GLUT1 (ciGLUT1), composed by 2364 bp with an open reading frame of 1473 bp encoding 490 amino acids. Sequence alignment and phylogenetic analysis revealed a high degree of conservation (80–97%) among most fish and higher vertebrates. The analysis of the 5′ and 3′ UTRs showed the presence of several post-transcriptional regulatory elements. The highest ciGLUT1 expression was observed in the heart followed by brain, whereas relatively low values were detected in the muscle, gill, intestine, kidney, spleen and liver. Then, hepatic and intestine ciGLUT1 expressions and hepatic glycolytic enzymes activities were determined in fish subjected to a glucose load after being fed different dietary carbohydrate levels (20%, 40% and 60%) for 8 weeks. And the result showed that different dietary carbohydrate had no effect on ciGLUT1 mRNA of hepatic and intestine after 8 weeks trail. However, after the adaptation of carbohydrate levels, glucose administration caused a prompt increase of hepatic ciGLUT1 mRNA expressions in all treatments whereas it either had no effect or a negative effect on intestine ciGLUT1 mRNA. In terms of dietary carbohydrate levels, the ciGLUT1 expressions of fish fed high carbohydrate diet (60%) were significantly higher than the other groups after the glucose load. In additon, after the glucose administration, hepatic low Km hexokinases (HK) activities of fish fed high carbohydrate diets increased significantly and was also significantly higher than the other groups. The results indicated that the GLUT1 gene of C. idellus showed a typical structure of the glucose transporters family, supporting the concept that ciGLUT1 functions as a ubiquitous and constitutive basal level glucose transporter. Consistent with the function of the GLUT1 as a constitutive glucose transporter, ciGLUT1 expressions of hepatic and intestine were hardly affected by the changes in dietary carbohydrate. Furthermore, after the glucose load, the fish after a long-term high dietary carbohydrate adaptation showed a more efficient clearance of glucose through the more remarkable enhancements of hepatic ciGLUT1 expressions and HK activities.","Ruixin Li and Hongyu Liu and Xiaohui Dong and Shuyan Chi and Qihui Yang and Shuang Zhang and Beiping Tan",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Glucagon-like peptide 1 receptor (GLP1R) haplotypes correlate with altered response to multiple antipsychotics in the CATIE trial","Glucagon-like peptide 1 receptor (GLP1R) signaling has been shown to have antipsychotic properties in animal models and to impact glucose-dependent insulin release, satiety, memory, and learning in man. Previous work has shown that two coding mutations (rs6923761 and rs1042044) are associated with altered insulin release and cortisol levels. We identified four frequently occurring haplotypes in Caucasians, haplotype 1 through haplotype 4, spanning exons 4–7 and containing the two coding variants. We analyzed response to antipsychotics, defined as predicted change in PANSS-Total (dPANSS) at 18months, in Caucasian subjects from the Clinical Antipsychotic Trial of Intervention Effectiveness treated with olanzapine (n=139), perphenazine (n=78), quetiapine (n=14), risperidone (n=143), and ziprasidone (n=90). Haplotype trend regression analysis revealed significant associations with dPANSS for olanzapine (best p=0.002), perphenazine (best p=0.01), quetiapine (best p=0.008), risperidone (best p=0.02), and ziprasidone (best p=0.007). We also evaluated genetic models for the two most common haplotypes. Haplotype 1 (uniquely including the rs1042044 [Leu260] allele) was associated with better response to olanzapine (p=0.002), and risperidone (p=0.006), and worse response to perphenazine (p=.03), and ziprasidone (p=0.003), with a recessive genetic model providing the best fit. Haplotype 2 (uniquely including the rs6923761 [Ser168] allele) was associated with better response to perphenazine (p=0.001) and worse response to olanzapine (p=.02), with a dominant genetic model providing the best fit. However, GLP1R haplotypes were not associated with antipsychotic-induced weight gain. These results link functional genetic variants in GLP1R to antipsychotic response.","Timothy L. Ramsey and Mark D. Brennan",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"In vitro levamisole selection pressure on larval stages of Haemonchus contortus over nine generations gives rise to drug resistance and target site gene expression changes specific to the early larval stages only","There is some evidence that resistance to levamisole and pyrantel in trichostrongylid nematodes is due to changes in the composition of nicotinic acetylcholine receptors (nAChRs) which represent the drug target site. Altered expression patterns of genes coding for nAChR subunits, as well as the presence of truncated versions of several subunits, have been implicated in observed resistances. The studies have mostly compared target sites in worm isolates of very different genetic background, and hence the ability to associate the molecular changes with drug sensitivity alone have been clouded to some extent. The present study aimed to circumvent this issue by following target site gene expression pattern changes as resistance developed in Haemonchus contortus worms under laboratory selection pressure with levamisole. We applied drug selection pressure to early stage larvae in vitro over nine generations, and monitored changes in larval and adult drug sensitivities and target site gene expression patterns. High level resistance developed in larvae, with resistance factors of 94-fold and 1350-fold at the IC50 and IC95, respectively, in larval development assays after nine generations of selection. There was some cross-resistance to bephenium (70-fold increase in IC95). The expression of all the putative subunit components of levamisole-sensitive nAChRs, as well as a number of ancillary protein genes, particularly Hco-unc-29.1 and –ric-3, were significantly decreased (up to 5.5-fold) in the resistant larvae at generation nine compared to the starting population. However, adult worms did not show any resistance to levamisole, and showed an inverse pattern of gene expression changes, with many target site genes showing increased expression compared to the starting population. A comparison of the larval/adult drug sensitivity data with the known relationships for field-derived isolates indicated that the adults of our selected population should have been highly resistant to the drug if the larval/adult sensitivity relationships were in accordance with previous field isolates. Hence, our selected worms showed a life-stage drug sensitivity pattern quite different to that seen in the field. The present study has highlighted an association between drug target site changes and resistance to levamisole in H. contortus larvae. However, it has also highlighted the artificial nature of the larval selection method with levamisole, as the resistance phenotype and the associated molecular changes were only observed in the drug-pressured life stage. The study therefore reinforces the need for caution in extrapolating larval-based laboratory selection outcomes to field resistances.","Ranbir S. Sarai and Steven R. Kopp and Malcolm R. Knox and Glen T. Coleman and Andrew C. Kotze",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"POINCARÉ CODE: A package of open-source implements for normalization and computer algebra reduction near equilibria of coupled ordinary differential equations","The Poincaré code is a Maple project package that aims to gather significant computer algebra normal form (and subsequent reduction) methods for handling nonlinear ordinary differential equations. As a first version, a set of fourteen easy-to-use Maple  commands is introduced for symbolic creation of (improved variants of Poincaré’s) normal forms as well as their associated normalizing transformations. The software is the implementation by the authors of carefully studied and followed up selected normal form procedures from the literature, including some authors’ contributions to the subject. As can be seen, joint-normal-form programs involving Lie-point symmetries are of special interest and are published in CPC Program Library for the first time, Hamiltonian variants being also very useful as they lead to encouraging results when applied, for example, to models from computational physics like Hénon–Heiles.
Program summary
Program title: POINCARÉ Catalogue identifier: AEPJ_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEPJ_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 7694 No. of bytes in distributed program, including test data, etc.: 597943 Distribution format: tar.gz Programming language: Maple V11. Computer: See specifications for running Maple V11 or above. Operating system: MS Windows. Classification: 4.3, 5, 16.9. Nature of problem: Computing structure-preserving normal forms near the origin for nonlinear vector fields. Solution method: 14 Maple commands are designed to compute various normal forms as well as their generating functions and associated normalizing transformations for nonlinear systems of ordinary differential equations, including Hamiltonian ones. Further reduction of these normal forms–in the presence of Lie-point symmetries–is also considered. All algorithms are based on Lie transform, thus leading to the structure-preserving normal forms in the sense that all properties that can be formulated in terms of graded Lie algebras are preserved. Restrictions: The semisimple part of the leading matrix about the origin (resp. the quadratic part in the Hamiltonian case) is assumed to be already taken into diagonal form (resp. canonical form) via a linear change of variables. In other words, the eigenvalues must be explicitly known. Unusual features: Further reduction of Poincaré–Dulac normal form via the joint normal form commands, taking profitably Lie-point symmetries, is perhaps the main feature of the software. Besides, to the best of our knowledge, and except the Hamiltonian case, it seems that there is no CPC programs for computation of (structure-preserving) normal forms in the general case. Running time: Depends on the input data, mainly on system size, type of the leading matrix (semisimple or not), type of resonances, order of normalization and required options. Instantaneous for the provided examples.","J. Mikram and F. Zinoun and A. El Abdllaoui",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Psychiatrists, criminals, and the law: Forensic psychiatry in Switzerland 1850–1950","Between 1880 and 1950, Swiss psychiatrists established themselves as experts in criminal courts. In this period, the judicial authorities required psychiatric testimonies in a rising number of cases. As a result, more offenders than ever before were declared mentally deficient and, eventually, sent to psychiatric asylums. Psychiatrists also enhanced their authority as experts at the political level. From the very beginning, they got involved in the preparatory works for a nationwide criminal code. In this article, I argue that these trends toward medicalization of crime were due to incremental processes, rather than spectacular institutional changes. In fact, Swiss psychiatrists gained recognition as experts due to their daily interactions with judges, public prosecutors, and legal counsels. At the same time, the spread of medical expertise had serious repercussions on psychiatric institutions. From 1942 onwards, asylums had to deal with a growing number of “criminal psychopaths,” which affected ward discipline and put psychiatry's therapeutic efficiency into question. The defensive way in which Swiss psychiatrists reacted to this predicament was crucial to the further development of forensic psychiatry. For the most part, it accounts for the subdiscipline's remarkable lack of specialization until the 1990s.","Urs Germann",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Emotional reactivity and regulation associated with fluent and stuttered utterances of preschool-age children who stutter","Purpose
The purpose of this study was to assess the relation between emotional reactivity and regulation associated with fluent and stuttered utterances of preschool-age children who stutter (CWS) and those who do not (CWNS).
Participants
Participants were eight 3 to 6-year old CWS and eight CWNS of comparable age and gender.
Methods
Participants were exposed to three emotion-inducing overheard conversations—neutral, angry and happy—and produced a narrative following each overheard conversation. From audio-video recordings of these narratives, coded behavioral analysis of participants’ negative and positive affect and emotion regulation associated with stuttered and fluent utterances was conducted.
Results
Results indicated that CWS were significantly more likely to exhibit emotion regulation attempts prior to and during their fluent utterances following the happy as compared to the negative condition, whereas CWNS displayed the opposite pattern. Within-group assessment indicated that CWS were significantly more likely to display negative emotion prior to and during their stuttered than fluent utterances, particularly following the positive overheard conversation.
Conclusions
After exposure to emotional-inducing overheard conversations, changes in preschool-age CWS's emotion and emotion regulatory attempts were associated with the fluency of their utterances. Learning outcomes: After reading this article, the reader will be able to: (1) describe various measures of emotional reactivity and regulation, including parent-based reports and behavioral coding, and how they may contribute to childhood stuttering; (2) explain emotional differences between the stuttered and fluent utterances of CWS and CWNS; and (3) discuss how emotions may contribute to CWS’ instances of stuttering.","Robin M. Jones and Edward G. Conture and Tedra A. Walden",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Corporate and Hospital Profiteering in Emergency Medicine: Problems of the Past, Present, and Future","Background
Health care delivery in the United States has evolved in many ways over the past century, including the development of the specialty of Emergency Medicine (EM). With the creation of this specialty, many positive changes have occurred within hospital emergency departments (EDs) to improve access and quality of care of the nation's de facto “safety net.” The specialty of EM has been further defined and held to high standards with regard to board certification, sub-specialization, maintenance of skills, and research. Despite these advances, problems remain.
Objective
This review discusses the history and evolution of for-profit corporate influence on EM, emergency physicians, finance, and demise of democratic group practice. The review also explores federal and state health care financing issues pertinent to EM and discusses potential solutions.
Discussion
The monopolistic growth of large corporate contract management groups and hospital ownership of vertically integrated physician groups has resulted in the elimination of many local democratic emergency physician groups. Potential downsides of this trend include unfair or unlawful termination of emergency physicians, restrictive covenants, quotas for productivity, admissions, testing, patient satisfaction, and the rising cost of health care. Other problems impact the financial outlook for EM and include falling federal, state, and private insurance reimbursement for emergency care, balance-billing, up-coding, unnecessary testing, and admissions.
Conclusions
Emergency physicians should be aware of the many changes happening to the specialty and practice of EM resulting from corporate control, influence, and changing federal and state health care financing issues.","Robert W. Derlet and Robert M. McNamara and Scott H. Plantz and Matthew K. Organ and John R. Richards",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Differential respiratory health effects from the 2008 northern California wildfires: A spatiotemporal approach","We investigated health effects associated with fine particulate matter during a long-lived, large wildfire complex in northern California in the summer of 2008. We estimated exposure to PM2.5 for each day using an exposure prediction model created through data-adaptive machine learning methods from a large set of spatiotemporal data sets. We then used Poisson generalized estimating equations to calculate the effect of exposure to 24-hour average PM2.5 on cardiovascular and respiratory hospitalizations and ED visits. We further assessed effect modification by sex, age, and area-level socioeconomic status (SES). We observed a linear increase in risk for asthma hospitalizations (RR=1.07, 95% CI=(1.05, 1.10) per 5µg/m3 increase) and asthma ED visits (RR=1.06, 95% CI=(1.05, 1.07) per 5µg/m3 increase) with increasing PM2.5 during the wildfires. ED visits for chronic obstructive pulmonary disease (COPD) were associated with PM2.5 during the fires (RR=1.02 (95% CI=(1.01, 1.04) per 5µg/m3 increase) and this effect was significantly different from that found before the fires but not after. We did not find consistent effects of wildfire smoke on other health outcomes. The effect of PM2.5 during the wildfire period was more pronounced in women compared to men and in adults, ages 20–64, compared to children and adults 65 or older. We also found some effect modification by area-level median income for respiratory ED visits during the wildfires, with the highest effects observed in the ZIP codes with the lowest median income. Using a novel spatiotemporal exposure model, we found some evidence of differential susceptibility to exposure to wildfire smoke.","Colleen E. Reid and Michael Jerrett and Ira B. Tager and Maya L. Petersen and Jennifer K. Mann and John R. Balmes",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Arthroscopic washout of the knee: A procedure in decline","Background
Osteoarthritis (OA) of the knee is a chronic, progressive condition which often requires surgical intervention. The evidence for the benefits of arthroscopic debridement or washout for knee OA is weak and arthroscopy is currently only indicated in the UK if there is a history of mechanical locking of the knee.
Objectives
To investigate whether there has been any change in the number of arthroscopies performed in the UK since the 2007 NICE guidance on knee arthroscopy and the 2008 Cochrane review of arthroscopic debridement for OA of the knee.
Methods
We interrogated data from the Hospital Episodes Statistics (HES) database with Office of Population Censuses and Surveys-4 (OPSC-4) codes pertaining to therapeutic endoscopic operations in the 60–74year old and 75 and over age groups.
Results
The number of arthroscopic knee interventions in the UK decreased overall from 2000 to 2012, with arthroscopic irrigations decreasing the most by 39.6 per 100,000 population (80%). However, the number of arthroscopic meniscal resections increased by 105.3 per 100,000 (230%) population. These trends were mirrored in both the 60–74 and 75 and over age groups.
Conclusions
Knee arthroscopy in the 60–74 and 75 and over age groups appears to be decreasing but there is still a large and increasing number of arthroscopic meniscal resections being performed.","Stefan Lazic and Oliver Boughton and Caroline Hing and Jason Bernard",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"BCVEGPY2.2: A newly upgraded version for hadronic production of the meson Bc and its excited states","A newly upgraded version of the BCVEGPY, a generator for hadronic production of the meson Bc and its excited states, is available. In comparison with the previous one (Chang et al., 2006), the new version is to apply an improved hit-and-miss technology to generating the un-weighted events much more efficiently under various simulation environments. The codes for production of 2S-wave Bc states are also given here.
New version program summary
Title of program: BCVEGPY2.2 Catalogue identifier: ADTJ_v2_3 Program obtained from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 323731 No. of bytes in distributed program, including test data, etc.: 4498602 Distribution format: tar.gz Computer: Any LINUX based on PC with FORTRAN 77 or FORTRAN 90 and GNU C compiler as well Operating systems: LINUX Programming language used: FORTRAN 77/90 Memory required to execute with typical data: About 2.0 MB Classification: 11.2, 11.5 Catalogue identifier of previous version: ADTJ_v2_2 Journal reference of previous version: Comput. Phys. Commun. 183 (2012) 442 Does the new version supersede the old version?: Yes Nature of physical problem: Hadronic Production of Bc meson and its excited states. Method of solution: To generate un-weighted events of Bc meson and its excited states by using an improved hit-and-miss technology. Reasons for new version: Responding to the feedback from users, such as those from CMS and LHCb groups, we create a new hit-and-miss algorithm for generating the un-weighted events. Furthermore, the relevant codes for generating the 2S-excited state of Bc meson are added, because the excited state production may be sizable in the new LHC run. Typical running time: It depends on which option is chosen to match PYTHIA when generating the full events and also on which state of Bc meson, either its ground state or its excited states, is to be generated. Typically on a 2.27GHz Intel Xeon E5520 processor machine, for producing the Bc meson ground state: I) If setting [IDWTUP=3 and unwght=.true.], it shall adopt the new hit-and-miss technology to generate the un-weighted events, and to generate 105 events takes 30 minutes; II) If setting [IDWTUP=3 and unwght=.false.] or [IDWTUP=1 and IGENERATE=0], it shall generate the weighted events, and to generate 105 events takes 2 minutes only (the fastest way, for theoretical purpose only); III) As a comparison, if setting [IDWTUP=1 and IGENERATE=1], it shall, as the same as the previous version, adopt the PYTHIA inner hit-and-miss technology to generate the un-weighted events, and to generate 1000 events takes about 22 hours. Thus, the efficiency (and accuracy also) for generating the un-weighted events obviously is greatly increased. Keywords: Event generator; Hadronic production; Bc meson; Un-weighted events Summary of revisions: 1). We improve the approach for generating un-weighted events. 2). Responding to the feedback from users, we adjust part of the codes to make it work more user-friendly. More specifically, we explain main changes in the following : •Event generation.If each simulated event comes with a weight, it will make the data analysis much more complicated. Thus the un-weighted events are usually adopted for Monte Carlo simulations. As an external process of PYTHIA, the generator BCVEGPY  [1], [2], [3], [4] shall call the PYTHIA inner hit-and-miss mechanism to generate the un-weighted events by setting IDWGTUP=1 and IGENERATE=1   [5], i.e. the Von Neumann method is used for generating the un-weighted Bc events.Every events bearing a weight (xwgtup) respectively, when inputting them to PYTHIA, they are suffered from being accepted or rejected, all the fully generated events at the output become to have a common weight. The Von Neumann method states that the event should be accepted by the PYTHIA subroutine PYEVNT with a probability R=xwgtup/xmaxup. This can be achieved by comparing R with a random number that is uniformly distributed within the region of [0,1]. Namely if R is bigger than such a random number then the event is accepted, otherwise it should be rejected. Here xmaxup stands for the maximum event weight.The von Neumann method works effectively for the cases when all the weights of input events are moderate in the whole phase-space. However if the input events’ weights vary greatly, such as varying logarithmically, then its efficiency shall be greatly depressed, since too much time shall be wasted for calculating xwgtup of the rejected events. Thus it is helpful to find a new method for generating un-weighted events.We will adopt the new hit-and-miss strategy suggested by Ref.[6] to do the Bc meson un-weight simulation. Extra switches for calling this new technology are added to BCVEGPY, e.g. the new hit-and-miss technology shall be called by setting IDWTUP=3 and unwght=.true.. Details for this new technology can be found in Ref.[6]. For self-consistency, we repeat its main idea here.To be different from previous versions, BCVEGPY2.2 uses the VEGAS  [7] and the MINT  [8] as a combined way to generate the un-weighted events. The whole phase space shall be separated to a multi-dimensional phase-space grid. The main purpose of VEGAS  [7] is to perform the adaptive Monte Carlo multi-dimensional integration, which uses the importance-sampling method to improve the integration efficiency. Each event shall generally result in a different weight, recorded by xwgtup, and the maximum weight within each grid shall be simultaneously recorded into the importance-sampling grid file (with the suffix .grid). Then following the idea of MINT, the Von Neumann method is used in each phase-space grid. Within this small grid region, the von Neumann algorithm works effectively, thus the efficiency for generating un-weighted events are greatly increased.To implement the new hit-and-miss algorithm into BCVEGPY2.2, we change the original VEGAS subroutine asvegas(fxn,ndim,ncall,itmx,nprn,xint,xmax,imode)Three new variables xint, xmax and imode are added in the VEGAS subroutine. The xmax array is used to record the maximum weights in all cells and imode is a flag. xint stands for the output cross-section when setting imode=0, which shall be used to initialize the xmax array when setting imode=1. For convenience, the generated xmax array will be stored in the same grid file in which the importance sampling function is stored.In the initialization stage, the VEGAS subroutine shall be called by the subroutine evntinit twice by setting imode=0 and imode=1 respectively to generate both the upper bound grid xmax for all cells and the importance sampling function.A subroutine gen(fxn,ndim,xmax,jmode) is defined in the file vegas.F with the purpose to generate the un-weighted events. Three options for calling gen subroutine are defined: jmode=0 is to initializes the parameter; jmode=3 is to print the generation statistics; jmode=1 is the key option, which is to use the new hit-and-miss technology to generate the un-weighted events. More explicitly, by calling gen(fxn,ndim,xmax,jmode=1), three steps shall be executed: 1.Call the phase_gen subroutine to generate a random phase-space point and to calculate its weight xwgtup.2.Judge the point locates in which cell and read from the xmax array and get the upper bound value xmaxup for this particular cell.3.Judge whether such point be kept or not by using the Von Neumann method with the help of the probability xwgtup/xmaxup. To be more flexible, we add one parameter igenmode for generating or using the existed .grid files. When setting igenmode=1, the VEGAS subroutine shall be called to generate the .grid files. When setting igenmode=2, the VEGAS subroutine shall be called to generate more accurate .grid files from the existed .grid files. When setting igenmode=3, one can directly use the existed .grid files to generate events without running VEGAS. Importantly, before using the existed .grid files, one must ensure all the parameters be the same as the previous generation.•A script for setting the parameters and a cross-check of the un-weighted events.We put an additional file, bcvegpy_set_par.nam, in the new version for setting the parameters. This way the user does not need to compile the program again if only the parameter values are changed. Fig. 1Comparison of the normalized Bc transverse momentum (PT) and rapidity (y) distributions derived by setting unwght=.true. (events) and unwght=.false. (differential distributions), which are represented by solid and dotted lines, respectively.As a cross-check of the new technology, we compare the un-weighted Bc event distributions derived by setting unwght=.true. with the weighted Bc differential distributions derived by setting unwght=.false.. The results are shown in Fig. 1. Those two distributions after proper normalization agree well with each other, that shows our present scheme for un-weighted events is correct.•Bc(2S) generation.In 2014 the ATLAS collaboration reported an observation about an excited state of Bc meson, which most probably is Bc(2S) state  [9]. With more data being collected at LHC detectors, it is hopeful that more observations on the excited Bc states will be issued. Therefore in addition to the production via color-singlet Bc(1S), Bc(1P) and color-octet Bc(1S) states, the Bc(2S) production is involved in BCVEGPY2.2. It is achieved by replacing the 1S-wave bound-state parameters pmb, pmc and fbc with those of the 2S-wave one. Here pmb, pmc and fbc are for b-quark mass, c-quark mass and the radial wave function at the zero (|R(0)|), respectively. For the 2S-wave case, their default values are set as pmb=5.234GeV, pmc=1.633 GeV and fbc=0.991GeV3/2   [10] if the mass of the 2S-wave Bc state is 6.867GeV.More explicitly, two new values for ibcstate are added: ibcstate=9 is to generate 21S0 state and ibcstate=10 is to generate 23S1 state. Detailed technologies for deriving the production properties of all the mentioned ten Bc meson states can be found in Refs.[11], [12], [13]. Furthermore, the values for mix_type are rearranged. mix_type=1 is to generate the mixing events for all mentioned states.  mix_type=2 is to generate the mixing events for 11S0 and 13S1 states. mix_type=3 is to generate the mixing events for the four 1P-wave states and the two color-octet 11S0 and 13S1 states. mix_type=4 is to generate the mixing events for 21S0 and 23S1 states.","Chao-Hsi Chang and Xian-You Wang and Xing-Gang Wu",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Neural plasticity associated with recently versus often heard objects","In natural settings the same sound source is often heard repeatedly, with variations in spectro-temporal and spatial characteristics. We investigated how such repetitions influence sound representations and in particular how auditory cortices keep track of recently vs. often heard objects. A set of 40 environmental sounds was presented twice, i.e. as prime and as repeat, while subjects categorized the corresponding sound sources as living vs. non-living. Electrical neuroimaging analyses were applied to auditory evoked potentials (AEPs) comparing primes vs. repeats (effect of presentation) and the four experimental sections. Dynamic analysis of distributed source estimations revealed i) a significant main effect of presentation within the left temporal convexity at 164–215ms post-stimulus onset; and ii) a significant main effect of section in the right temporo-parietal junction at 166–213ms. A 3-way repeated measures ANOVA (hemisphere×presentation×section) applied to neural activity of the above clusters during the common time window confirmed the specificity of the left hemisphere for the effect of presentation, but not that of the right hemisphere for the effect of section. In conclusion, spatio-temporal dynamics of neural activity encode the temporal history of exposure to sound objects. Rapidly occurring plastic changes within the semantic representations of the left hemisphere keep track of objects heard a few seconds before, independent of the more general sound exposure history. Progressively occurring and more long-lasting plastic changes occurring predominantly within right hemispheric networks, which are known to code for perceptual, semantic and spatial aspects of sound objects, keep track of multiple exposures.","Nathalie M.-P. Bourquin and Lucas Spierer and Micah M. Murray and Stephanie Clarke",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Extended computational kernels in a massively parallel implementation of the Trotter–Suzuki approximation","We extended a parallel and distributed implementation of the Trotter–Suzuki algorithm for simulating quantum systems to study a wider range of physical problems and to make the library easier to use. The new release allows periodic boundary conditions, many-body simulations of non-interacting particles, arbitrary stationary potential functions, and imaginary time evolution to approximate the ground state energy. The new release is more resilient to the computational environment: a wider range of compiler chains and more platforms are supported. To ease development, we provide a more extensive command-line interface, an application programming interface, and wrappers from high-level languages.
New version program summary
Program title: Trotter–Suzuki-MPI Catalogue identifier: AEXL_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEXL_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 14083 No. of bytes in distributed program, including test data, etc.: 110023 Distribution format: tar.gz Programming language: C++, CUDA, Python, MATLAB. Computer: x86-64. Operating system: Linux. Has the code been vectorized or parallelized?: Yes. Number of processors used: 1–64 in a single node, more in a cluster. RAM: 5 MByte-512 GBytes Journal reference of associated manuscript: Comput. Phys. Comm. 184(2013)1165 Classification: 4.12. External routines: OpenMP, MPI, CUDA Does the new version supersede the previous version?: Yes. The original version is not held in the CPC Program Library but can be obtained from https://github.com/peterwittek/trotter–suzuki-mpi Nature of problem: The evolution of a general quantum system is described by the time-dependent Schrödinger equation. The solution of this equation involves calculating a matrix exponential, which is formally simple, but computer implementations must consider several factors to achieve both high performance and high accuracy. Solution method: The Trotter–Suzuki approximation leads to an efficient algorithm for solving the time-dependent Schrödinger equation [1, 2]. The implementation uses high-performance parallel kernels in a distributed environment to maximize the computational power of this algorithm [3, 4]. Reasons for new version: The computational kernels were generalized to be able to address a much wider range of physics problems. Furthermore, the code has been modularized to make development easier, providing both a command-line and an application programming interface. High-level wrappers from Python and MATLAB provide further ease of use. Summary of revisions: 1.The implementation was generalized to include a richer variety of physics problems. The problem can have periodic boundary conditions. Many-body simulations of non-interacting particles became a possible extension. We can define an arbitrary stationary potential function. The convenience function expect_values helps to obtain expectation values.2.Imaginary time evolution was implemented to find the ground state before starting the simulation. To avoid imposing the overhead of conditional branching in the most computationally intense parts of the code, some of the core kernel functions were duplicated to include the imaginary time evolution.3.Most of the functionality is exposed through a command-line interface (CLI) for convenience. This allows specifying the files of the initial state and the potential, the parameters of the Hamiltonian, and further parameters related to the simulation, such as the computational kernel to use and the frequency at which snapshots should be written to the disk.4.The full functionality of the implementation is exposed as an application programming interface (API) through the ’trotter’ function. This allows for integrating the simulation in a larger MPI programme and it is also useful for initializing the state and the potential without having files on the disk.To demonstrate the use of the API, several examples are provided with the code.5.To further ease development, we redesigned the structure of the implementation, making it more modular. We also introduced a unit testing framework to avoid regression.6.We improved the testing of MPI dependencies by the configure script and allowed compilation without MPI. We also improved the treatment of Intel and Visual C++ compilers.We developed wrappers for Python and MATLAB for the CPU kernel for a high-level interface with the library.Restrictions: The vectorized CPU kernel must have a tile width that is divisible by two. This puts a constraint on the possible matrix sizes for this kernel. For instance, running twelve MPI threads in a 4×3 configuration, the dimensions must be divisible by six and eight. Unusual features: The library currently only supports the CPU kernel under Windows. The Python and MATLAB wrappers support the CPU and SSE kernels. Additional comments: The high-performance kernels were independently extended to study spin dynamics [5]. It remains for future work to include lattice models in this implementation. Running time: The generalization slightly altered the memory access patterns of the computational kernels, yielding performance penalty of approximately 20% compared to the previous version (Table 1). The scaling properties did not change and we see a near-optimal scaling when increasing the number of nodes. The actual running time depends on the system size and the duration to be simulated, and the computational resources. It can range from a few seconds to several days. References: [1]H. Trotter, On the product of semi-groups of operators, Proceedings of the American Mathematical Society 10 (1959) 545–551.[2]M. Suzuki, Decomposition formulas of exponential operators and Lie exponentials with some applications to quantum mechanics and statistical physics, Journal of Mathematical Physics 26 (1985) 601.[3]C. Bederián, A. Dente, Boosting quantum evolutions using Trotter–Suzuki algorithms on GPUs, in: Proceedings of HPCLatAm-11, 4th High-Performance Computing Symposium, Córdoba, Argentina, 2011.[4]P Wittek, F. Cucchietti, A second-order distributed. Trotter–Suzuki solver with a hybrid CPU–GPU kernel, Computer Physics Communications 184 (4) (2013) 1165–1171. doi:10.1016/j.cpc.2012.12.008.[5]A. D. Dente, C. S. Bederián, P R. Zangara, H.M. Pastawski, GPU accelerated Trotter–Suzuki solver for quantum spin dynamics, arXiv:1305.0036.","Peter Wittek and Luca Calderaro",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Nonverbal communication during the learner lesson with a professional driving instructor: A novel investigation","Background
Fundamental for the development of the driving and road use skills of the young driver is learning to drive through driving instruction and, in graduated driver licensing programs such as in Australia, driving supervision. In Queensland young drivers are required to log a minimum of 100h supervised practice, with recent research revealing that parents provide most of this supervision. Queensland also offers young drivers a 10-h 3-for-1 bonus for professional driving instruction, such that one hour of professional instruction can be logged as three hours of practice, to a maximum of 30 logbook hours. Recent research efforts have begun to provide insight into the nature of the verbal instruction of both parents and professional instructors, and into the nonverbal communication between parents and learners. However nothing is known regarding the nonverbal communication between professional instructors and learners.
Method
Ten learner lessons (five male learners) with four professional instructors (four males) were captured via GoPro cameras. The nonverbal communication during the first, middle, and last 10min of each lesson was coded as being posture and body orientation, gestures, facial expressions, proximity, humour, and eye contact, within the context of the accompanying verbal communication according to the value of (a) eager, or (b) cautious; the valence of (a) neutral, (b) positive, or (c) negative; and the purpose of (a) rapport, or (b) communication.
Results
Overall, posture and body orientation was the most common mechanism of nonverbal communication, while facial expressions and proximity were the least common mechanisms of nonverbal communication. In general the beginning, the middle, and the end of the lessons were characterised by a plethora of neutral, cautious interactions, and positive, eager interactions. However it is noteworthy that the rates at which learners and instructors engaged in these behaviours were found to change across the lesson. Specifically learners actively communication nonverbally through mechanisms such as eye contact, facial expressions and humour, while instructors appeared to manage building rapport and communicating safe vehicle and road use through nonverbal communication such as gestures, facial expressions and posture and body orientation, summarised in a model comprising a continuum of instruction.
Discussion
While nonverbal communication is fundamental for effective verbal communication, and on occasion can replace verbal communication, and as such the professional – and the parental – driving lesson should optimise the use of nonverbal communication, at this time the optimal nature of nonverbal communication remains unknown. In addition, optimal verbal and nonverbal communication specifically suited to the driving context which involves a dynamic environment outside the vehicle, and at times a dynamic environment inside the vehicle, remains yet to be identified. The research findings provide unique insight into the nature of the nonverbal communication used by both learner drivers and professional driving instructors, in addition to the continuum of instruction model. As such, the findings provide a solid foundation for future research into, and guidance regarding, optimising the learner driving lesson.","B. Scott-Parker",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A multi-scale enriched model for the analysis of masonry panels","A multi-scale model for the structural analysis of the in-plane response of masonry panels, characterized by periodic arrangement of bricks and mortar, is presented. The model is based on the use of two scales: at the macroscopic level the Cosserat micropolar continuum is adopted, while at the microscopic scale the classical Cauchy medium is employed. A nonlinear constitutive law is introduced at the microscopic level, which includes damage, friction, crushing and unilateral contact effects for the mortar joints. The nonlinear homogenization is performed employing the Transformation Field Analysis (TFA) technique, properly extended to the macroscopic Cosserat continuum. A numerical procedure is developed and implemented in a Finite Element (FE) code in order to analyze some interesting structural problems. In particular, four numerical applications are presented: the first one analyzes the response of the masonry Representative Volume Element (RVE) subjected to a cyclic loading history; in the other three applications, a comparison between the numerically evaluated response and the micromechanical or experimental one is performed for some masonry panels.","Daniela Addessi and Elio Sacco",2012,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Observable behavior of distributed systems: Component reasoning for concurrent objects","Distributed and concurrent object-oriented systems are difficult to analyze due to the complexity of their concurrency, communication, and synchronization mechanisms. Rather than performing analysis at the level of code in, e.g., Java or C++, we consider the analysis of such systems at the level of an abstract, executable modeling language. This language, based on concurrent objects communicating by asynchronous method calls, avoids some difficulties of mainstream object-oriented programming languages related to compositionality and aliasing. To facilitate system analysis, compositional verification systems are needed, which allow components to be analyzed independently of their environment. In this paper, a proof system for partial correctness reasoning is established based on communication histories and class invariants. A particular feature of our approach is that the alphabets of different objects are completely disjoint. Compared to related work, this allows the formulation of a much simpler Hoare-style proof system and reduces reasoning complexity by significantly simplifying formulas in terms of the number of needed quantifiers. The soundness and relative completeness of this proof system are shown using a transformational approach from a sequential language with a non-deterministic assignment operator.","Crystal Chang Din and Johan Dovland and Einar Broch Johnsen and Olaf Owe",2012,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR9"
"Numerical simulation of soil–cone penetrometer interaction using discrete element method","One of the most common methods to measure soil strength in-situ is cone penetrometers. In this paper the development of a three dimensional (3D) discrete element model (DEM) for the simulation of the soil–cone penetrometer interaction in a slightly cohesive loamy sand soil is presented. The aim was to investigate the effects of the soil model’s geometrical (e.g., soil model cross section shape and size and model’s height) changes on variations in the soil penetration resistance. The model area ratio and height ratio values were adopted to analyse the effects of the cross section size and the model’s height, respectively. The results of penetration resistance of the DEM simulations were compared with the in-situ measurement with a cone penetrometer of the same geometry. This comparison allowed the derivation of the contact properties between the elements. To simulate the soil material the so-called Parallel Bond and Linear Models were used in the 3D version of the Particle Flow Code (PFC) software. Finally the mechanical properties of the soil, namely the cohesion and internal friction angle were estimated by DEM simulation of direct shear box. Results showed that the penetration process can be simulated very well using the DEM. The model’s calculated penetration resistance and the corresponding in-situ measurement were in good agreement, with mean error of 14.74%. The best performing models were a rectangular model with an area ratio of 72 and a height ratio of 1.33 and a circular model with an area ratio of 32 and a height ratio of 2. The simulation output of soil material properties with direct shear box resulted in representative values of real loamy sand soils, with cohesion values range of 6.61–8.66kPa and internal friction angle values range of 41.34–41.60°. It can be concluded that the DEM can be successfully used to simulate the interaction between soil and cone penetrometers in agricultural soils.","Krisztián Kotrocz and Abdul M. Mouazen and György Kerényi",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Increase in the Incidence of Differentiated Thyroid Carcinoma in Children, Adolescents, and Young Adults: A Population-Based Study","Objective
To investigate trends in incidence of differentiated thyroid carcinomas among children and adolescents and young adults.
Study design
In this ecological time-trends study, we selected cases of differentiated thyroid carcinomas (1984-2010) in patients <30 years from Surveillance, Epidemiology, and End Results 9 cancer registries by using International Classification of Diseases for Oncology, 3rd edition, codes for papillary and follicular cancers. Patients with multiple other primary diseases before differentiated thyroid carcinomas were excluded. SEER*Stat software, version 8.0.4 (National Cancer Institute, Bethesda, Maryland) was used to calculate age-standardized rates (estimated per 1 000 000/persons) and annual percentage changes (APCs) were calculated by the Joinpoint model (Joinpoint software, version 4.0.4; National Cancer Institute).
Results
Rates ranged from 2.77 (1990) to 9.63 (2009) and from 18.35 (1987) to 50.99 (2009), for male and female subjects, respectively. A significant increasing trend in incidence was observed for both male (APC 3.44; 95% CI 2.60-4.28) and female (APC 3.81; 95% CI 3.38-4.24) patients. When a stratified analysis on the basis of tumor size was performed, significant increasing trends were noted for the following categories: <0.5 cm (females: APC 5.09, 95% CI 3.54-6.65), 0.5-0.9 cm (females: APC 8.45, 95% CI 7.09-9.82), 1.0-1.9 cm (males: APC 5.09, 95% CI 3.20-7.01; females: APC 3.42, 95% CI 2.78-4.07), and ≥2 cm (males: APC 2.62, 95% CI 1.64-3.60; females: APC 2.96, 95% CI 2.34-3.59).
Conclusions
Incidence rates for differentiated thyroid carcinomas are increasing among children and adolescents and young adults in the US. The increasing trends for larger tumors rules out diagnostic scrutiny as the only explanation for the observed results. Environmental, dietary, and genetic influences should be investigated.","Lucas Bonachi Vergamini and A. Lindsay Frazier and Fernanda Laurinavicius Abrantes and Karina Braga Ribeiro and Carlos Rodriguez-Galindo",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Image-processing of time-averaged interface distributions representing CCFL characteristics in a large scale model of a PWR hot-leg pipe geometry","Countercurrent Flow Limitation (CCFL) was experimentally investigated in a 1/3.9 downscaled COLLIDER facility with a 190mm pipe’s diameter using air/water at 1 atmospheric pressure. Previous investigations provided knowledge over the onset of CCFL mechanisms. In current article, CCFL characteristics at the COLLIDER facility are measured and discussed along with time-averaged distributions of the air/water interface for a selected matrix of liquid/gas velocities. The article demonstrates the time-averaged interface as a useful method to identify CCFL characteristics at quasi-stationary flow conditions eliminating variations that appears in single images, and showing essential comparative flow features such as: the degree of restriction at the bend, the extension and the intensity of the two-phase mixing zones, and the average water level within the horizontal part and the steam generator. Consequently, making it possible to compare interface distributions obtained at different investigations. The distributions are also beneficial for CFD validations of CCFL as the instant chaotic gas/liquid interface is impossible to reproduce in CFD simulations. The current study shows that final CCFL characteristics curve (and the corresponding CCFL correlation) depends upon the covered measuring range of water delivery. It also shows that a hydraulic diameter should be sufficiently larger than 50mm in order to obtain CCFL characteristics comparable to the 1:1 scale data (namely the UPTF data). Finally, the study shows that the change of the flow condition inside the hot-leg is not only related to the water and air inlet velocities, but is also dependent upon the existent interface distribution within the hot-leg, and that several CCFL cases of identical inlet flow conditions can exist with different interface distribution and pressure difference. The last result is of a special importance to the investigation of this phenomenon during SBLOCA accidents, since the entire phenomenon is driven by pressure difference between the steam generator and reactor vessel, as well as by gravity. This result show also that CCFL characteristics cannot be investigated using 1D codes, as the interface distribution within the hot-leg during a SBLOCA accident will depend upon flow history or previous interface distribution. Current investigations support the effort to provide more knowledge over CCFL in order to extrapolate results obtained in downscaled models into the 1:1 scale.","Suleiman Al Issa and Rafael Macián-Juan",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An agent-based platform for simulating complex human–aquifer interactions in managed groundwater systems","This paper presents and illustrates FlowLogo, an interactive modelling environment for developing coupled agent-based groundwater models (GW-ABMs). It allows users to simulate complex socio-environmental couplings in groundwater systems, and to explore how desirable patterns of groundwater and social development can emerge from agent behaviours and interactions. GW-ABMs can be developed using a single piece of software, addressing common issues around data transfer and model analyses that arise when linking ABMs to existing groundwater codes. FlowLogo is based on a 2D finite-difference solution of the governing groundwater flow equations and a set of procedures to represent the most common types of stresses and boundary conditions of regional aquifer flow. The platform is illustrated using a synthetic example of an expanding agricultural region that depends on groundwater for irrigation. The implementation and analysis of scenarios from this example highlight the possibility to: (i) deploy agents at multiple scales of decision-making (farmers, waterworks, institutions), (ii) model feedbacks between agent behaviours and groundwater dynamics, and (iii) perform sensitivity and multi-realisation analyses on social and physical factors. The FlowLogo interface allows interactively changing parameters using ‘tuneable’ dials, which can adjust agent decisions and policy levers during simulations. This flexibility allows for live interaction with audiences (role-plays), in participatory workshops, public meetings, and as part of learning activities in classrooms. FlowLogo's interactive features and ease of use aim to facilitate the wider dissemination and independent validation of GW-ABMs.","J.C. Castilla-Rho and G. Mariethoz and R. Rojas and M.S. Andersen and B.F.J. Kelly",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Analysis on premixed combustion of H2–CH4 mixed fuels for SiO2 particle preparation","The combustion of premixed H2 and CH4 fuels to prepare SiO2 nanoparticles was analyzed numerically. The commercial CFD-code Fluent was used to calculate the profiles of gas velocities, temperature, species concentrations and reaction rates for various process conditions in the premixed flame reactor. To understand the effect of CH4 addition to H2 fuel, the CH4 mole fraction was changed from 0% to 10%, while keeping the H2 concentration constant. With the increase of CH4 concentration, flame temperature increases, which will affect the properties of SiO2 particles prepared in premixed flame reactor. Using the data of gas temperatures and velocities extracted from Fluent, the trajectories and temperature histories of SiO2 particles were calculated inside the premixed flame reactor. This study shows that particles starting at different initial positions move inside the reactor with different particle trajectories and have different temperature histories, and the properties of SiO2 nanoparticles prepared in the premixed flame reactor will depend on those process variables significantly.","Anna Nasonova and Kyo-Seon Kim",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Test suite reduction methods that decrease regression testing costs by identifying irreplaceable tests","Context
In software development and maintenance, a software system may frequently be updated to meet rapidly changing user requirements. New test cases will be designed to ensure the correctness of new or modified functions, thus gradually increasing the test suite’s size. Test suite reduction techniques aim to decrease the cost of regression testing by removing the redundant test cases from the test suite and then obtaining a representative set of test cases that still yield a high level of code coverage.
Objective
Most of the existing reduction algorithms focus on decreasing the test suite’s size. Yet, the differences in execution costs among test cases are usually significant and it may take a lot of execution time to run a test suite consisting of a few long-running test cases. This paper presents and empirically evaluates cost-aware algorithms that can produce the representative sets with lower execution costs.
Method
We first use a cost-aware test case metric, called Irreplaceability, and its enhanced version, called EIrreplaceability, to evaluate the possibility that each test case can be replaced by others during test suite reduction. Furthermore, we construct a cost-aware framework that incorporates the concept of test irreplaceability into some well-known test suite reduction algorithms.
Results
The effectiveness of the cost-aware framework is evaluated via the subject programs and test suites collected from the Software-artifact Infrastructure Repository — frequently chosen benchmarks for experimentally evaluating test suite reduction methods. The empirical results reveal that the presented algorithms produce representative sets that normally incur a low cost to yield a high level of test coverage.
Conclusion
The presented techniques indeed enhance the capability of the traditional reduction algorithms to reduce the execution cost of a test suite. Especially for the additional Greedy algorithm, the presented techniques decrease the costs of the representative sets by 8.10–46.57%.","Chu-Ti Lin and Kai-Wei Tang and Gregory M. Kapfhammer",2014,"[""Science Direct""]","Rejeitado: CR12","Rejeitado: CR9"
"Despinning and shape evolution of Saturn’s moon Iapetus triggered by a giant impact","Iapetus possesses two spectacular characteristics: (i) a high equatorial ridge which is unique in the Solar System and (ii) a large flattening (a-c=34km) inconsistent with its current spin rate. These two main characteristics have probably been acquired in Iapetus’ early past as a consequence of coupled interior-rotation evolution. Previous models have suggested that rapid despinning may result either from enhanced internal dissipation due to short-lived radioactive elements or from interactions with a sub-satellite resulting from a giant impact. For the ridge formation, different exogenic and endogenic hypotheses have also been proposed, but most of the proposed scenarios have not been tested numerically. In order to model simultaneously internal heat transfer, tidal despinning and shape evolution, we have developed a two-dimensional axisymmetric thermal convection code with a deformable surface boundary, coupled with a viscoelastic code for tidal dissipation. The model includes centrifugal and buoyancy forces, a composite non-linear viscous rheology as well as an Andrade rheology for the dissipative part. By considering realistic rheological properties and by exploring various grain size values, we show that, in the absence of additional external interactions, despinning of a fast rotating Iapetus is impossible even for warm initial conditions (T>250K). Alternatively, the impact of a single body with a radius of 250–350km at a velocity of 2km/s may be sufficient to slow down the rotation from a period of 6–10h to more than 30h. By combining despinning due to internal dissipation and an abrupt change of rotation due to a giant impact, we determined the parameters leading to a complete despinning and we computed the corresponding shape evolution. We show that stresses arising from shape change affect the viscosity structure by enhancing dislocation creep and can lead to the formation of a large-scale ridge at the equator as a result of rapid rotation change for initial rotation periods of 6h.","Miroslav Kuchta and Gabriel Tobie and Katarina Miljković and Marie Běhounková and Ondřej Souček and Gaël Choblet and Ondřej Čadek",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Early gamma oscillations during rapid auditory processing in children with a language-learning impairment: Changes in neural mass activity after training","Children with language-learning impairment (LLI) have consistently shown difficulty with tasks requiring precise, rapid auditory processing. Remediation based on neural plasticity assumes that the temporal precision of neural coding can be improved by intensive training protocols. Here, we examined the extent to which early oscillatory responses in auditory cortex change after audio-visual training, using combined source modeling and time-frequency analysis of the human electroencephalogram (EEG). Twenty-one elementary school students diagnosed with LLI underwent the intervention for an average of 32 days. Pre- and post-training assessments included standardized language/literacy tests and EEG recordings in response to fast-rate tone doublets. Twelve children with typical language development were also tested twice, with no intervention given. Behaviorally, improvements on measures of language were observed in the LLI group following completion of training. During the first EEG assessment, we found reduced amplitude and phase-locking of early (45–75ms) oscillations in the gamma-band range (29–52Hz), specifically in the LLI group, for the second stimulus of the tone doublet. Amplitude reduction for the second tone was no longer evident for the LLI children post-intervention, although these children still exhibited attenuated phase-locking. Our findings suggest that specific aspects of inefficient sensory cortical processing in LLI are ameliorated after training.","Sabine Heim and Andreas Keil and Naseem Choudhury and Jennifer Thomas Friedman and April A. Benasich",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical modeling of flow boiling instabilities using TRACE","Dynamic flow instabilities in two-phase systems are a vitally important area of study due to their effects on a great number of industrial applications, including heat exchangers in nuclear power plants. Several next generation nuclear reactor designs incorporate once through steam generators which will exhibit boiling flow instabilities if not properly designed or when operated outside design limits. A number of numerical thermal hydraulic codes attempt to model instabilities for initial design and for use in accident analysis. TRACE, the Nuclear Regulatory Commission’s newest thermal hydraulic code is used in this study to investigate flow instabilities in both single and dual parallel channel configurations. The model parameters are selected as to replicate other investigators’ experimental and numerical work in order to provide easy comparison. Particular attention is paid to the similarities between analysis using TRACE Version 5.0 and RELAP5/MOD3.3. Comparison of results is accomplished via flow stability maps non-dimensionalized via the phase change and subcooling numbers. Results of this study show that TRACE does indeed model two phase flow instabilities, with the transient response closely mimicking that seen in experimental studies. When compared to flow stability maps generated using RELAP, TRACE shows similar results with differences likely due to the somewhat qualitative criteria used by various authors to determine when the flow is truly unstable.","Eric M. Kommer",2015,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Electron number distribution functions from molecular wavefunctions. Version 2","We present in this article a new and considerably faster version of the edf Fortran 77/90 code that replaces the old one (Francisco et al., 2008). In the new version, given an N-electron molecule and an exhaustive, fuzzy, or orbital-based partition of the physical space R3 into m domains, the probabilities p(S) of all possible distributions S={n1,n2,…,nm} of the N electrons (n1+n2+⋯+nm=N) into m real space domains are computed. The set {p(S)} defines the electron number distribution function (EDF) of the molecule for this specific space partition. The molecule may be described by either a single- or a multi-determinant wavefunction Ψ(1,N). Both spin-resolved and spin-unresolved EDFs are determined. Isopycnic orbital localizations of the natural molecular orbitals (MOs) can be optionally performed to make the use of the core approximation possible. This explicitly eliminates from the calculation those MOs strongly that are localized over one of the m domains, considerably speeding up the process. An optional approximation consisting of assuming that localized MOs are orthogonal to each other in all the domains is shown to give reasonably accurate results and further accelerates the calculation. The new edf code does also allows for the computation of a single probability p(n1,n2,…,nm) instead of the full EDF. Finally, this new version computes multiple-domain covariances of electron populations, particularly relevant for chemical bonding theory.
Program summary/new version program summary
Program title: edf Catalogue identifier: AEAJ_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEAJ_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 16847 No. of bytes in distributed program, including test data, etc.: 206274 Distribution format: tar.gz Programming language: Fortran 77/90. Computer: 2.80 GHz Intel Pentium IV CPU. Operating system: GNU/Linux. RAM: Dynamic Classification: 2.7. External routines: mkl Does the new version supersede the previous version?: Yes Catalogue identifier of the previous version: AEAJ_v1_0 Journal reference of the previous version: Comput. Phys. Comm. 178 (2008) 621 Nature of problem: Given an N-electron molecule described by a single- or multi-determinant wavefunction Ψ(1,N), and a partition of the physical space R3 into m domains Ω1,Ω2,…,Ωm, edf computes the probabilities p(S) of having exactly n1,n2,…, and nm electrons in Ω1,Ω2,…,, and Ωm, respectively, for all possible distributions S≡{n1,n2,…,nm}, being n1,n2,…, and nm integer numbers. Solution method: Given a wavefunction Ψ(1,N)=∑rMcrΨr(1,N), where the Ψr’s are Slater determinants Ψr=det[X1r,…,XNr], and calling (Skrs)ij the overlap integral within the domain Ωk between Xir and Xjs, edf finds all the p(S)’s using a three-step procedure: (1)For each (r,s) pair, solve the linear system ∑{nσi}t1n1σt2n2σ…tmnmσprs({niσ})=det[∑k=1mtkSkrs] in the unknowns prs(niσ), where σ=(α,β), {nσi}(i=1,2,…,m) are the integer electronic populations with spin σ of the domains Ω1,Ω2,…,Ωm, tm=1, and t1,…,tm−1 are arbitrary real numbers,(2)Compute the spin-resolved probabilities p({nα;nβ})≡p({ip})=∑r,sMcrcsprs({niα})prs({niβ}), and(3)obtain the p(S)’s by adding up the p({ip})’s with niα+niβ=niReasons for new version: Dynamic memory allocation instead of static memory allocation is used throughout. Further partitions of the 3D space have been added. Thanks to the change in the algorithm used to solve the problem, the new version is 1–2 orders of magnitude faster than the previous one and can deal with molecules having a greater number of electrons. Approximate calculations as well as exact ones are possible in the new version by making use of the core–valence separability. Summary of revisions: Most data structures are stored in dynamic memory. The basic algorithm has been changed to ensure a much faster computation of the probabilities p(S). Algorithms to obtain the latter in an approximate manner, as well as using different partitions of the 3D space have been included. Restrictions: The number of {niσ} sets in Eq. 1, i.e. the dimension of the linear system to be solved, grows very fast with m and N. This dimension is much smaller than in the previous version of edf but, even so, this restricts the applicability of the method to relatively small systems, unless some drastic approximations are used (excluding, for instance, a large part of the electrons of the system from the calculation). Running time: 0.016 and 0.004 s for the test examples 1 and 2, respectively. However, running times are very variable depending on the molecule, the type of the wavefunction (single- or multi-determinant), the number of fragments (m). etc. References: [1] E. Francisco, A. Martín Pendás, and M.A. Blanco. J. Chem. Phys. 126, 094102 (2007). [2] A. Martín Pendás, E. Francisco, and M.A. Blanco. J. Chem. Phys. 127, 144103 (2007). [3] E. Francisco, A. Martín Pendás, and M.A. Blanco. Computer Physics Commun. 178, 621–634 (2008). [4] A. Martín Pendás, E. Francisco, and M.A. Blanco. Faraday Discuss. 135, 423–438 (2007). [5] A. Martín Pendás, E. Francisco, M.A. Blanco, and C. Gatti. Chemistry: A European Journal. 13, 9362–9371 (2007). [6] A. Martín Pendás, E. Francisco, and M.A. Blanco. Phys. Chem. Chem. Phys. 9, 1087–1092 (2007). [7] E. Francisco, A. Martín Pendás, and M.A. Blanco. J. Chem. Phys. 131, 124125 (2009). [8] E. Francisco, A. Martín Pendás, and M.A. Blanco. Theor. Chemistry Accounts. 128, 433 (2011). [9] E. Francisco, A. Martín Pendás, A. Costales, and M.A. García-Revilla. Comput. Theor. Chem. 975, 2–8 (2011). [10] M.A. García-Revilla, E. Francisco, A. Martín Pendás, J.M. Recio, M. Bartolomei, M.I. Hernández, J. Campos-Martínez, E. Carmona-Novillo, and R. Hernández-Lamoneda. J. Chem. Theory Comput. 9, 2179–2188 (2013).","E. Francisco and A. Martín Pendás",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Dramatic increases in number of cerebellar granule-cell-Purkinje-cell synapses across several mammals","The classical comparative literature on mammalian brain evolution has mainly focused on brain mass measurements because larger brains are more likely to have more neurons to process information. The phylogenetic expansion in the mass of the cerebellum is as significant as that of the cerebral cortex. The synapse, however, has recently been recognized as the basic unit of neuronal information processing, including neuroplasticity. Here we hypothesize significant absolute and relative increases in the functionally important granule-cell-Purkinje-cell (gcPc) synapses as a salient feature of the evolving cerebellum. To probe evolutionary constraints, we define the gcPc circuitry with ten degrees of freedom, including number of granule cells, Purkinje cells, lengths of the granule cell axonal segments, linear densities of synapses along them, and physical dimensions of Purkinje as well as granule cell dendritic structures. We show that although only two of the ten parameters are not constrained and therefore can exhibit independent, comparative changes, there is a dramatic increase in the number of gcPc synapses from the rodent to the human cerebellum. By assigning a value of unity for the mouse, the ratio of the number of gcPc synapses from mouse, rat, cat, non-human primate, and human is 1:5.5:236:620:20,000, which greatly exceeds the ratio of increase in cerebellar mass (1:6:48:180:3000). Dramatic changes in the number of gcPc synapses can therefore occur despite evolutionary constraints and only modest changes in parameters of the neuronal circuitry. Increases in the number of gcPc synapses have important functional consequences as these synapses enhance the capacity of the cerebellum to code and process information, which directly impact memory and learning in both motor and non-motor tasks.","Chiming Huang and Samantha J. Gammon and Michael Dieterle and Rosa H. Huang and Lee Likins and R.E. Ricklefs",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"What Influences Changing Practice to Evidence‐Based Care in Individual Providers?","Paper Presentation
Objective
To explore the experiences of maternity care professionals (midwives and physicians) who changed a practice to one with more scientific support. For the purpose of having the providers share a common phenomenon, the change from early to delayed cord clamping was chosen.
Design
This qualitative study was conducted using a constructivist, grounded theory approach.
Setting
Participants were interviewed by phone. Practice settings ranged from home birth to freestanding and in‐hospital birth centers to the hospital, which was the most common setting with eight of the 17 subjects providing care there.
Sample
The sample was made up of 17 providers (12 midwives and five physicians) from throughout the United States. Participants were in active clinical practice or had been within the past 3 years. They practiced early cord clamping for at least 6 months, changed to delayed cord clamping, and then practiced that for at least 6 months.
Methods
Participant interviews were recorded with permission and then transcribed. Coding and data management was conducted using ATLAS.ti Version6.1. The study was approved by the Institutional Review Board of George Washington University with multiple steps taken to protect the participants during analysis.
Results
Five emergent themes acting as drivers of change included trusting colleagues, believing the evidence, honoring mothers and families, knowing with personal certainty, and protecting the integrity of the mother and the baby. Three domains of influence developed from the background of the stories of change: personal, professional, and institutional. From the findings, the Evolution to Provider Change Model was developed.
Conclusion/Implications for Nursing Practice
In maternity care, a number of current practices are not evidence based while many of those that are supported by evidence remain underutilized. Studies investigating efforts to incorporate evidence into practice reveal that at the organizational level, many initiatives are unsuccessful. Research into provider change at the level of the individual has been sparse in all medical fields. This study provides new understanding about how individual maternity care professionals change practice. A new theoretical model is proposed that may be used in future nursing research on improving practice. Focusing on individual versus group change and on learning from those who have successfully changed practice toward more evidence‐based care offers fertile ground for further study.","Mayri Sagady Leslie",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Evolution of Ciona intestinalis Tumor necrosis factor alpha (CiTNFα): Polymorphism, tissues expression, and 3D modeling","Although the Tumor necrosis factor gene superfamily seems to be very conserved in vertebrates, phylogeny, tissue expression, genomic and gene organization, protein domains and polymorphism analyses showed that a strong change has happened mostly in invertebrates in which protochordates were a constraint during the immune-molecules history and evolution. RT PCR was used to investigate differential gene expression in different tissues. The expression shown was greater in the pharynx. Single-nucleotide polymorphism has been investigated in Ciona intestinalis Tumor necrosis factor alpha (CiTNFα) mRNA isolated from the pharynx of 30 ascidians collected from Licata, Sicily (Italy), by denaturing gradient gel electrophoresis (DGGE). For this analysis, CiTNFα nucleotide sequence was separated into two fragments, TNF-1 and -2, respectively, of 630 and 540 bp. We defined 23 individual DGGE patterns (named 1 to 10 for TNF-1 and 1 to 13 for TNF-2). Five patterns for TNF-1 accounted for <10% of the individuals, whereas the pattern 13 of TNF-2 accounted for >20% of the individuals. All the patterns were verified by direct sequencing. Single base-pair mutations were observed mainly within COOH-terminus, leading to 30 nucleotide sequence variants and 30 different coding sequences segregating in two main different clusters. Although most of the base mutations were silent, four propeptide variants were detected and six amino acid replacements occurred within COOH-terminus. Statistical tests for neutrality indicated negative selection pressure on signal and mature peptide domains, but possible positive selection pressure on COOH-terminus domain. Lastly we displayed the in silico 3D structure analysis including the CiTNFα variable region.","Aiti Vizzini and Maria Giovanna Parisi and Laura Cardinale and Lelia Testasecca and Matteo Cammarata",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"ParaTrough v1.0: Librería en Modelica para Simulación de Plantas Termosolares","Resumen
El presente trabajo describe una librería desarrollada en Modelica que utiliza el entorno Dymola 6.1 para modelar y simular plantas termosolares de tecnología de colector cilindro-parabólico. El actual software de modelado y simulación es cada vez más potente gracias a los avances en computación y programación, pudiendo conseguir estimaciones muy precisas del comportamiento de estas plantas térmicas. Como mejora a otras propuestas actuales, la librería ParaTrough se ofrece como una herramienta pública, gratuita bajo licencia Modelica License 2, de código libre, flexible, modular, y por lo tanto fácilmente ampliable y modificable para los requerimientos específicos de cada planta y proceso en particular. En la versión 1.0 contemplada en este artículo, esta librería se puede usar para el modelado y simulación del recurso solar y del sistema de fluido de transferencia calorífica sin cambio de fase. Los modelos han sido validados con datos reales de una planta en operación, Andasol 3, en los términos municipales de Aldeire y La Calahorra (Granada). El objetivo de ParaTrough es poder ser utilizada gratuitamente y de forma amigable por analistas de procesos para uno o varios de los siguientes casos: evaluación del rendimiento, detección de fallos, exploración de nuevos modos de operación y optimización de la planta. Aunque en futuras versiones se puedan añadir otros elementos, esta aportación cubre una nueva área de aplicación específica para el software de Modelica y en su estado actual facilita la operación y mantenimiento de estas plantas.
This paper describes a Modelica-based library developed to the modeling and simulation of solar thermal plants with parabolic trough collectors. The Dymola 6.1 environment has been used. Unlike other commercial tools, the ParaTrough library is offered as a free open source tool, under Modelica License 2. Its modular code makes it easily extensible and modifiable to the requirements of each plant and process in particular. In its current version 1.0, this library can be used for modeling and simulating the solar resource and the heat transfer fluid without phase change. The models have been validated with real data of an operating plant. ParaTrough can be freely used by process analysts for one or more of the following cases: performance assessment, fault detection, exploring new operation modes and plant optimization. While other elements can be added in future extensions, this contribution covers a new specific application area of Modelica and in its current state it facilitates the operation and maintenance of parabolic trough power plants.","Juan A. Romera Cabrerizo and Matilde Santos",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"MCMC2 (version 1.1): A Monte Carlo code for multiply-charged clusters","This new version of the MCMC2 program for modeling the thermodynamic and structural properties of multiply-charged clusters by means of parallel classical Monte Carlo methods provides some enhancements and corrections to the earlier version [1]. In particular, histograms for negatively and positively charged particles are separated, parallel Monte Carlo simulations can be performed by attempting exchanges between all the replica pairs and not only one randomly chosen pair, a new random number generator is supplied, and the contribution of Coulomb repulsion to the total heat capacity is corrected. The main functionalities of the original MCMC2 code (e.g., potential-energy surfaces and Monte Carlo algorithms) have not been modified.
New version program summary
Program title: MCMC2. Catalogue identifier: AENZ_v1_1 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AENZ_v1_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland. Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html. No. of lines in distributed program, including test data, etc.: 148028 No. of bytes in distributed program, including test data, etc.: 1501936 Distribution format: tar.gz Programming language: Fortran 90 with MPI extensions for parallelization Computers: x86 and IBM platforms Operating system: 1.CentOS 5.6 Intel Xeon X5670 2.93 GHz, gfortran/ifort(version 13.1.0) + MPICH22.CentOS 5.3 Intel Xeon E5520 2.27 GHz, gfortran/g95/pgf90 + MPICH23.Red Hat Enterprise 5.3 Intel Xeon X5650 2.67 GHz, gfortran + IntelMPI4.IBM Power 6 4.7 GHz, xlf + PESS (IBM parallel library) Has the code been vectorised or parallelized?: Yes, parallelized using MPI extensions. Number of CPUs used: up to 999. RAM (per CPU core): 10–20 MB. The physical memory needed for the simulation depends on the cluster size, the values indicated are typical for small clusters (N≤300−400). Classification: 23 Catalogue identifier of previous version: AENZ_v1_0 Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 873 Nature of problem: We provide a general parallel code to investigate structural and thermodynamic properties of multiply-charged clusters. Solution method: Parallel Monte Carlo methods are implemented for the exploration of the configuration space of multiply-charged clusters. Two parallel Monte Carlo methods were found appropriate to achieve such a goal: the Parallel Tempering method, where replicas of the same cluster at different temperatures are distributed among different CPUs, and Parallel Charging where replicas (at the same temperature) having different particle charges or numbers of charged particles are distributed on different CPUs. Restrictions: The current version of the code uses Lennard-Jones interactions, as the main cohesive interaction between spherical particles, and electrostatic interactions (charge–charge, charge-induced dipole, induced dipole–induced dipole, polarisation). The Monte Carlo simulations can only be performed in the NVT ensemble in the present code. Unusual features: The Parallel Charging methods, based on the same philosophy as Parallel Tempering but with particle charges and number of charged particles as parameters instead of temperature, is an interesting new approach to explore energy landscapes. Splitting of the simulations is allowed and averages are accordingly updated. Running time: The running time depends on the number of Monte Carlo steps, cluster size, and the type of interactions selected (e.g., polarisation turned on or off, and method used for calculating the induced dipoles). Typically a complete simulation can last from a few tens of minutes or a few hours for small clusters (N≤100, not including polarisation interactions), to one week for large clusters (N≥1000 not including polarisation interactions), and several weeks for large clusters (N≥1000) when including polarisation interactions. A restart procedure has been implemented that enables a splitting of the simulation accumulation phase. Reasons for new version: The new version corrects some bugs identified in the previous version. It also provides the user with some new functionalities such as the separation of histograms for positively and negatively charged particles, a new scheme to perform parallel Monte Carlo simulations and a new random number generator. Summary of revisions1.Additional features of MCMC2 version 1.1 (a)Histograms for positively and negatively charged particles. The first version of MCMC2 was able to produce a large variety of histograms to investigate the structure of charged clusters and their propensity to undergo evaporation: angular histograms for charged particles (“angdist-yyy.dat” and “surfang-yyy.dat” files), radial histograms (“rhist-x-yyy.dat” files), and histograms for tracking the number of charged surface particles (“surfnb-yyy.dat” files) or the number of particles that tend to evaporate (“evapnb-yyy.dat” files). Although the program could handle clusters composed of both positively and negatively charged particles, these histograms did not separate these two classes of particles. This has been corrected by the addition of two columns in the histogram files. These columns are labelled with signs “+” and “−” that refer to positively and negatively charged particles, respectively. The study of clusters composed of both positively and negatively charged particles should therefore be made easier [2]. Most of the keywords corresponding to histograms have not been changed except the keyword for radial histograms that now includes the possibility not to print any histogram (see keyword “RADIAL” in the “Modified keywords” section).(b)Full replica pair exchange in Monte Carlo simulations. In previous publications [2, 3] the exchange between replica configurations was based on two steps: the random selection of one replica pair and the calculation of the Parallel Tempering or Parallel Charging criteria to accept or reject the exchange of configurations between replicas. By default these exchanges were attempted every ns=10 Monte Carlo sweeps, where a sweep is composed of N Monte Carlo steps for an N-particle cluster. The main goal of parallel Monte Carlo algorithms is to improve the convergence speed and we might thus be interested in attempting more than one exchange of replica configurations every ns sweeps, provided that ns is large enough for the final configuration (after the ns sweeps) to be decorrelated from the initial configuration (before the ns sweeps). In the present version of the code we have implemented a second way to perform parallel Monte Carlo simulations where N/2 exchanges of replica configurations are attempted every ns sweeps by alternatively selecting odd pairs (i.e., pairs involving replicas 1–2, 3–4, etc.) and even pairs (i.e., pairs involving replicas 2–3, 4–5, etc.). The two methods should be equivalent for large numbers of sweeps although the second method proposed in the present version is deemed to converge faster than the method based on random choices of replica pairs. Modifications brought to keywords “PC” and “PT” are reported in the “Modified keywords” section.(c)Lagged Fibonacci random number generator. Random numbers are generated by means of a LAPACK random number generator in the first version of MCMC2  [1]. Knowing that the main goal of MCMC2 is to handle Monte Carlo simulations on large clusters (from hundreds up to thousands of particles at least), we were particularly concerned in delivering a random number generator already thoroughly tested on neutral or charged clusters for such large systems. One of us (ML) developed a lagged Fibonacci random number generator based on the works by Kirkpatrick et al. [4] and Bhanot et al. [5]. In particular, this random number generator was successfully used in accurate diffusion quantum Monte Carlo investigations of the structure and energetics of small helium clusters [6], a benchmark convergence study of the dissociation energy of the HF dimer [7], and the fragmentation dynamics of ionized doped helium clusters [8, 9]. Modifications brought to keyword “SEED” are reported in the “Modified keywords” section. As an example, heat capacity curves of neutral A100 clusters obtained after performing Parallel Tempering simulations with the two random number generators used in MCMC2 are plotted in Fig. 1 of Supplementary materials. We can notice that the melting peak is perfectly defined by all four methods, some small deviations may occur for the premelting peak whose convergence is harder to achieve.2.Modifications or corrections to MCMC2 version 1.0 (a)In MCMC2 (version 1.0) we have computed the heat capacity related to Coulomb energy fluctuations that we have improperly called the “Coulomb part of heat capacity”. Indeed, when defining the potential energy U of a charged cluster as the sum of the Lennard-Jones (LJ) interactions V and the Coulomb interactions Vc we can define several quantities:Total heat capacity,  CV:  CV=CVCoul+CVLJ=1kBT2(〈U2〉−〈U〉2)Coulomb contribution to  CV:  CVCoul=1kBT2(〈UVc〉−〈U〉〈Vc〉)LJ contribution to  CV:  CVLJ=1kBT2(〈UV〉−〈U〉〈V〉)Coulomb heat capacity:  CV,fluctCoul=1kBT2(〈Vc2〉−〈Vc〉2)LJ heat capacity:CV,fluctLJ=1kBT2(〈V2〉−〈V〉2) where we call “Coulomb heat capacity” and “LJ heat capacity” the heat capacities that are obtained by calculating the fluctuations of Coulomb and LJ interactions, respectively. After some calculations, we can find two formulas to express CV as a function of CVCoul, CVLJ, CV,fluctCoul and CV,fluctLJ: CV=2CVCoul−CV,fluctCoul+CV,fluctLJCV=2CVLJ+CV,fluctCoul−CV,fluctLJ which leads to (1)CV,fluctCoul−CV,fluctLJ=CVCoul−CVLJ. The difference between Coulomb and LJ heat capacities thus matches the difference between the LJ and Coulomb contributions to heat capacity. However, only the latter quantities have a clear physical meaning for charged clusters bound by both LJ and Coulomb interactions and we have therefore replaced the calculation of CV,fluctCoul by CVCoul in the present version of MCMC2. Note that the contribution of polarisation energy to heat capacity has also been added when polarisation is included.(b)Several minor corrections were brought into version 1.1: i.The word “RMS” (that usually stands for Root Mean Square) was wrongly written several times in the MCMC2-yyy.out output files instead of “standard deviation” that is abbreviated “std dev.”. This is corrected in the present version of the code.ii.The syntax used to generate some file names seemed not to be recognized by recent pgf95 compilers and has been modified. This does not affect the user since file names have not been modified.iii.The default minimum temperature for the geometric temperature scale is set to 10−10 instead of 0 that would have led to divergence if not changed by the user.iv.Test cases are slightly modified to take into account the new features of version 1.1 and the README.txt files are more detailed. The START folders and the obsolete open PBS scripts are removed from the distribution.3.Modified keywordsWe present in this section a list of the modified keywords. •PC METHOD imcpc REPLICAS N_repc EVERY pc_every TEMPERATURE kt0 QREF q_ref: Setting of parameters for running MCPC simulations. imcpc is an integer dedicated to the choice of the parallel scheme to be used when performing parallel charging simulations (0 = random choice of one replica pair with MCPC2, 1 = random choice of one replica pair with MCPC1, 2 = use of even or odd replica pairs with MCPC2, and 3 = use of even or odd replica pairs with MCPC1). N_repc is the number of replicas. This number must equal the number of CPUs used for the simulation: the MCMC2 code assumes that one replica is run on one CPU. Any different choice will lead to job abortion. Configuration swapping between replicas is tested every pc_every MC sweeps. kt0 is the reference temperature of the simulation (identical for all the replicas). A MCPC simulation will always use kt0 whatever the originally selected temperature scale (constant, geometric, or adjusted, see keyword “TEMPERATURE”). Defining a temperature scale in the setup file will only result in the replacement of these temperatures by kt0 (beware: if the user does not give a value to kt0, the default will be used). q_ref is the reference charge used for MCPC1 simulations. For MCPC2 simulations, the charges q are read from the input configuration files. The simulation is stopped if the input files have charges different from q_ref. For deactivating MCPC simulations, choose pc_every=0.Default: imcpc=0, N_repc=0, pc_every=0, kt0=0.1, q_ref=0.•PT METHOD imcpt REPLICAS N_rept EVERY pt_every: Setting of parameters for running MCPT simulations. imcpt is an integer dedicated to the choice of the parallel scheme to be used when performing parallel tempering simulations (0 = random choice of one replica pair and 1 = alternative use of even and odd replica pairs). N_rept is the number of replicas. This number must equal the number of CPUs used for the simulation: the MCMC2 code assumes that one replica is run on one CPU. Any different choice will lead to job abortion. Configuration swapping between replicas is tested every pt_every MC sweeps. For deactivating MCPT simulations, choose pt_every=0.Default: imcpt=0, N_rept=0 and pt_every=0.•RADIAL USE lrad GRIDRCOM deltagridstepgrid PARTICLE radtyp: Setting parameters for plotting one-particle radial histograms (whether lrad  = .true.). The radial histograms cannot extend beyond the radius of the Monte Carlo container (see keyword “CONTAINER”), for graphical purposes we however allowed the user to add a small distance deltagrid to the grid size (=radius+deltagrid). stepgrid is the grid step for one-particle radial histograms with respect to the cluster center-of-mass. The grid origin is hardcoded to zero since these histograms are calculated with respect to the cluster center of mass. The number Ngrid of grid points is automatically determined in the code from the grid size and the grid step. radtyp enables the user to specify the type of particles to be considered for plotting radial histograms (0 = no distribution, 1 = all the particles without any distinction, 2 = charged particles only, 3 = neutral particles only, 4 = all the previous histograms (4=1+2+3)).Default: lrad = .false., deltagrid=0, stepgrid=0.1, radtyp=0.•SEED METHOD irand INITIALIZATION seed(1)seed(2)seed(3)seed(4) SCALING nsc(1)nsc(2)nsc(3)nsc(4): choice of the random number generator. irand is an integer that enables the user to select a random number generator (0 = LAPACK random number generator and 1 = lagged Fibonacci random number generator). Depending on the value of irand the random seeds and scaling factors may be different. If irand=0, seed(j) (j∈{1,2,3,4}) are positive integer seeds that must be below 4095 and seed(4) must be odd. N_rep (number of replicas) secondary seeds are generated from these primary seeds and the knowledge of scaling factors nsc(j) (j∈{1,2,3,4}). By default, these secondary seeds are obtained by doing seedi(j)=seed(j)+i×nsc(j) for j∈{1,2,3} and seedi(4)=seed(4)+2i×nsc(4) (since all the seeds seedi(4) must remain odd) where i is the replica number. If irand=1, only one seed (namely seed(1)) is expected by the program and secondary seeds are also produced by doing seedi(1)=seed(1)+i×nsc(1).Default: seed(j)=0 for j∈{1,2,3}, seed(4)=1, nsc(j)=1 for j∈{1,2,3,4} (i.e., seedi(j)=i for j∈{1,2,3} and seedi(4)=1+2i).References [1] D.A. Bonhommeau, M.-P. Gaigeot, Comput. Phys. Commun. 184 (2013) 873–884. [2] D.A. Bonhommeau, R. Spezia, M.-P. Gaigeot, J. Chem. Phys. 136 (2012) 184503. [3] M.A. Miller, D.A. Bonhommeau, C.J. Heard, Y. Shin, R. Spezia, M.-P. Gaigeot, J. Phys.: Condens. Matter. 24 (2012) 284130. [4] S. Kirkpatrick, E. P. Stoll, J. Comp. Phys. 40 (1981) 517. [5] G. Bhanot, D. Duke, R. Salvador, Phys. Rev. B 33 (1986) 7841. [6] M. Lewerenz, J. Chem. Phys. 106 (1997) 4596. [7] M. Mladenović, M. Lewerenz, Chem. Phys. Lett. 321 (2000) 135. [8] D. Bonhommeau, P. T. Lake, Jr., C. Le Quiniou, M. Lewerenz, N. Halberstadt, J. Chem. Phys. 126 (2007) 051104. [9] D. Bonhommeau, M. Lewerenz, N. Halberstadt, J. Chem. Phys. 128 (2008) 054302. Acknowledgements Dr. Mark A. Miller is gratefully acknowledged for providing some routines and valuable advice during the development of the code. The IDRIS national computer center and the ROMEO computer center of Champagne-Ardenne are also acknowledged. This work was supported by the “Génopôle d’Evry” through a post-doctoral fellowship (DAB), by the “Centre National de la Recherche Scientifique” (CNRS) through an excellence chair (DAB), and by a Partenariat Hubert Curien Alliance Program (MPG).","David A. Bonhommeau and Marius Lewerenz and Marie-Pierre Gaigeot",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Participatory integrated coastal zone management in Vietnam: Theory versus practice case study: Thua Thien Hue province","Sustainable management processes have undergone a shift from a top-down approach to a bottom-up approach. This bottom-up approach allows for a more apprehensive inclusion of stakeholders. In traditional hierarchical societies a combination of both is considered more desirable. This combination is described as a participatory approach that allows for bi-directional knowledge sharing. The question asked is whether this theoretical approach is viable in practice, taking into account different social, political and cultural influences. Qualitative research in bi-directional knowledge sharing and stakeholder participation in Integrated Coastal Zone Management (ICZM) was conducted in the provinces of Thua Thien Hue in Vietnam. Qualitative research was conducted using coding analysis. This analysis showed that in practice a great reluctance for change affects the implementation of ICZM. This reluctance is directly related to the level of power of stakeholders and the level to which stakeholders are embedded in the top-down tradition. Two contradicting results emerged. On the one hand the theoretical understanding of participatory ICZM is highest when reluctance for change is highest and vice versa. On the other hand a decrease in power results in an increase of the sustainability of the implementation of participatory ICZM. This research concluded that a ‘platform or structure’ is essential to achieve sustainability. In the Vietnamese context the tradition of power results in a platform which is both formal and non-formal. A non-formal platform is needed to create social capital, whereas a formal platform will limit the risk for arbitrariness and allow for institutionalisation.","Bieke Abelshausen and Tom Vanwing and Wolfgang Jacquet",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Exploring the connectedness of rural auxiliary midwives to social networks in Koutiala, Mali","Background
rural auxiliary midwives are central to clinical maternal care in Mali. However, little is known about their social role within the villages they serve. Exploring the social connectedness of midwives in their communities can reveal areas in which they need additional support, and ways they could benefit their communities beyond their clinical role.
Objective
to examine rural auxiliary midwives' social connectedness to the communities they serve.
Design
embedded, mixed methods design combining social network case studies with semi-structured interviews.
Participants and setting
midwives were recruited for semi-structured interviews during technical trainings held in Koutiala in southern Mali. Social network analyses were conducted among all adult women in two small villages purposively sampled from the Koutiala region.
Methods
29 interviews were conducted, transcribed, and coded using NVivo (Version 9) to qualitatively assess social connectedness. In two villages, the complete social networks of women's friendships were analysed using UCINET Version 6 (n=142; 74). Rank-orders of actors according to multiple measures of their centrality within the network were constructed to assess the midwives' position among village women.
Findings
both local and guest midwives reported feeling high levels of social integration, acceptance, and appreciation from the women in their communities. Specific challenges existed for guest or younger midwives, and in midwives' negotiations with men. In the two sociometric analyses, both the local and guest midwives ranked among the most influential social actors in their respective villages.
Key conclusions and implications for practice
though they hold a unique position among other rural women, this study suggests that midwives in Koutiala are well connected socially, and may be capable of becoming effective agents of network based-behavioural health interventions. Additional support is warranted to help midwives affirm a credible professional status in a male-dominated society, especially those of local status and younger age. Programme planners and policy-makers should consider the potential of midwives in communication when designing behaviour change interventions for women in similarly underserved areas.","Emily A. Hurley and Nicole E. Warren and Seydou Doumbia and Peter J. Winch",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Insights into evolution in Andean Polystichum (Dryopteridaceae) from expanded understanding of the cytosolic phosphoglucose isomerase gene","Cytosolic phosphoglucose isomerase (pgiC) is an enzyme essential to glycolysis found universally in eukaryotes, but broad understanding of variation in the gene coding for pgiC is lacking for ferns. We used a substantially expanded representation of the gene for Andean species of the fern genus Polystichum to characterize pgiC in ferns relative to angiosperms, insects, and an amoebozoan; assess the impact of selection versus neutral evolutionary processes on pgiC; and explore evolutionary relationships of selected Andean species. The dataset of complete sequences comprised nine accessions representing seven species and one hybrid from the Andes and Serra do Mar. The aligned sequences of the full data set comprised 3376 base pairs (70% of the entire gene) including 17 exons and 15 introns from two central areas of the gene. The exons are highly conserved relative to angiosperms and retain substantial homology to insect pgiC, but intron length and structure are unique to the ferns. Average intron size is similar to angiosperms; intron number and location in insects are unlike those of the plants we considered. The introns included an array of indels and, in intron 7, an extensive microsatellite array with potential utility in analyzing population-level histories. Bayesian and maximum-parsimony analysis of 129 variable nucleotides in the Andean polystichums revealed that 59 (1.7% of the 3376 total) were phylogenetically informative; most of these united sister accessions. The phylogenetic trees for the Andean polystichums were incongruent with previously published cpDNA trees for the same taxa, likely the result of rapid evolutionary change in the introns and contrasting stability in the exons. The exons code a total of seven amino-acid substitutions. Comparison of non-synonymous to synonymous substitutions did not suggest that the pgiC gene is under selection in the Andes. Variation in pgiC including two additional accessions represented by incomplete sequences provided new insights into reticulate relationships among Andean taxa.","Brendan M. Lyons and Monique A. McHenry and David S. Barrington",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Capability-enhanced AIMSUN with Real-time Signal Timing Control","In this paper, we enhanced the capability of Aimsun simulation including signal timing optimization of traffic control with real-time information on the network dynamics. The problem is formulated so as to find the duration of maximum green time for each stage in response to recurrent traffic flow fluctuation at an intersection. The approach used a simple version of the Webster method for determining the cycle length and green time split. The resulting algorithm was coded in Java and used TraSMAPI to dynamically link it to Aimsun's API, which allows the user to change the cycle length and green time duration of each traffic light's stage. TraSMAPI is a Traffic Simulation Manager API, designed to provide real-time interaction with traffic simulators. So far, this tool has been only tested with Sumo and Itsumo microscopic traffic simulators. The paper presents an example of the communication protocol using the API module linked to TraSMAPI, and contributes to the implementation of a novel real-time traffic control in Aimsun.","Cristina Vilarinho and Guilherme Soares and José Macedo and José Pedro Tavares and Rosaldo J.F. Rossetti",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Genetic optimization of a PCM enhanced storage tank for Solar Domestic Hot Water Systems","The scope of this work is to ascertain if the inclusion of Phase Change Materials (PCM) in the thermal energy storage of a Solar Domestic Hot Water System (SDHW) could be beneficial for increasing the energy savings of the system or in reducing the space occupied by the thermal energy storage. A simple SDHW plant is studied which features a plane solar collector, a boiler and a PCM enhanced tank. The PCM improved storage tank has been optimized using mono and multi-objective genetic algorithms. The optimization has been carried out with the modeFRONTIER optimization tool, while the system plant has been analyzed by means of a modified version of the building energy simulation code ESP-r. In parallel with the optimization a sensitivity analysis has been carried on in order to find out the relation between the design parameters of the tank (geometry, phase change temperature of the PCM, and insulation) and the performance of the system. Thanks to the multi-objective optimization of the system different solutions with different rankings of the optimized variables have been presented. The main result is that for this application the PCM has not the major impact on the results, while other parameters play a more significant role.","Roberta Padovan and Marco Manzan",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Spinal Epidural Abscess: A Series of 101 Cases","Background
Spinal epidural abscesses are uncommon but potentially devastating infections that often elude early diagnosis. An increasing incidence has been suggested; however, few contemporary data are available regarding risk factors and epidemiologic trends over time.
Methods
A retrospective study of spinal epidural abscesses from 2004 to 2014 at a large academic hospital was conducted. Cases were identified using International Classification of Diseases, Ninth Revision (ICD-9) code 324.1, and a review of medical and radiographic records was performed to confirm each case. Data collected included sociodemographics, medical history, suspected route of infection, treatments, and outcome.
Results
The incidence was 5.1 cases for each 10,000 admissions, with no significant changes during the study period. The route of infection was identified in 52% of cases, with bacteremia as the most common (26%), followed by recent surgery/procedure (21%) and spinal injection (6%). An identifiable underlying risk factor was present in 84% of cases, most commonly diabetes and intravenous drug use. A causative organism was identified in 84% of cases, most commonly Staphylococcus aureus; methicillin-resistant isolates accounted for 25% of S. aureus cases. All cases received intravenous antibiotic therapy, and 73% underwent a drainage procedure. Fifteen percent had an adverse outcome (8% paralysis and 7% death).
Conclusions
The incidence of spinal epidural abscesses may be increasing, with the present study demonstrating a ≥5-fold higher rate compared with historical data. Although the outcome in most cases was favorable, spinal epidural abscesses continue to cause substantial morbidity and mortality and should remain a “not to be missed diagnosis.”","Martin Vakili and Nancy F. Crum-Cianflone",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"BPMN MUSIM: Approach to improve the domain expert’s efficiency in business processes modeling for the generation of specific software applications","Despite the evolution computer science has undergone during the last years, the time consumed by the application development process has not experienced mayor changes. One of the areas where improvements could be made is the requirement engineering process, where the features offered by business process modeling (BPM) could favor the implication of the domain experts. BPMN, the standard notation, is one of the most widely used modeling languages, used by a great number of organizations around the world. Its versatility enables it to be used for any kind of business process, regardless of the domain the problem addresses. However, non-technical domain experts find difficulties when dealing with business process modeling notations, so their participation in this task remains minor. BPMN MUSIM is a simplified business process modeling notation that intends to: lower the difficulty domain experts have for learning and understanding other notations, enable quick modeling of the business processes and use the models to generate specific software applications.","Jaime Solís-Martínez and Jordán Pascual Espada and B. Cristina Pelayo G-Bustelo and Juan Manuel Cueva Lovelle",2014,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR9"
"Conversion of the Thymus into a Bipotent Lymphoid Organ by Replacement of Foxn1 with Its Paralog, Foxn4","Summary
The thymus is a lymphoid organ unique to vertebrates, and it provides a unique microenvironment that facilitates the differentiation of immature hematopoietic precursors into mature T cells. We subjected the evolutionary trajectory of the thymic microenvironment to experimental analysis. A hypothetical primordial form of the thymus was established in mice by replacing FOXN1, the vertebrate-specific master regulator of thymic epithelial cell function, with its metazoan ancestor, FOXN4, thereby resetting the regulatory and coding changes that have occurred since the divergence of these two paralogs. FOXN4 exhibited substantial thymopoietic activity. Unexpectedly, histological changes and a functional imbalance between the lymphopoietic cytokine IL7 and the T cell specification factor DLL4 within the reconstructed thymus resulted in coincident but spatially segregated T and B cell development. Our results identify an evolutionary mechanism underlying the conversion of a general lymphopoietic organ to a site of exclusive T cell generation.","Jeremy B. Swann and Annelies Weyn and Daisuke Nagakubo and Conrad C. Bleul and Atsushi Toyoda and Christiane Happe and Nikolai Netuschil and Isabell Hess and Annette Haas-Assenbaum and Yoshihito Taniguchi and Michael Schorpp and Thomas Boehm",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Growth models of dyadic synchrony and mother–child vagal tone in the context of parenting at-risk","We used multilevel modeling to examine dynamic changes in respiratory sinus arrhythmia (RSA) and observer-coded interactive synchrony for mother–child dyads engaged in a laboratory interaction, to characterize parenting-at-risk. Seventy-nine preschooler–mother dyads including a subset with documented child maltreatment (CM; n=43) were observed completing a joint puzzle task while physiological measures were recorded. Dyads led by CM mothers showed decreases in positive synchrony over time, whereas no variation was observed in non-CM dyads. Growth models of maternal RSA indicated that mothers who maintained high levels of positive interactive synchrony with their child evidenced greater RSA reactivity, characterized by an initial withdrawal followed by augmentation as the task progressed, after accounting for CM group status. These results help to clarify patterns of RSA responding in the context of caregiver–child interactions, and demonstrate the importance of modeling dynamic changes in physiology over time in order to better understanding biological correlates of parenting-at-risk.","Ryan J. Giuliano and Elizabeth A. Skowron and Elliot T. Berkman",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Broker-less middleware for WSAN performance evaluation","Over the past few years, The cost of electronic devices and sensor networks decreased rapidly enforcing almost all users’ requirements to use those devices. These devices provide low cost processing in wireless sensor networks (WSNs), As well as in wireless sensor and actuator networks (WSANs). The task of data management in WSNs is a vital issue that can be performed with limited resources such as processing, Memory and energy. Data distribution service (DDS) is a prominent standard used in the industry and academia communities to support real-time distributed systems depending on publish/subscribe (pub/sub) model. tinyDDS is a lightweight middleware that is a partial porting of the DDS and implemented over tinyOS code. The original version of tinyDDS is called the default tinyDDS (defTDDS). broker-less tinyDDS (BLTDDS) and hybrid tinyDDS (hyTDDS) are protocols that added several improvements to the defTDDS. The energy aware tinyDDS (EATDDS) protocol is proposed to deal directly with the energy consumption metric. In this paper, We conduct a comparative study between defTDDS, BLTDDS and hyTDDS in terms of throughput, PDR, End-to-end delay and energy consumption. Moreover, We propose a new protocol named an enhanced energy aware tinyDDS (E-EATDDS) that improves the energy consumption of the EATDDS protocol. We use tinyOS simulator in our implementation and evaluation. The results show that E-EATDDS outperforms BLTDDS, hyTDDS and EATDDS in terms of number of packets sent per joule.","Awadh Gaamel and Tarek Sheltami and Anas Al-Roubaiey and Elhadi Shakshuki",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The Dust Storm Index (DSI): A method for monitoring broadscale wind erosion using meteorological records","Wind erosion of soils is a natural process that has shaped the semi-arid and arid landscapes for millennia. This paper describes the Dust Storm Index (DSI); a methodology for monitoring wind erosion using Australian Bureau of Meteorology (ABM) meteorological observational data since the mid-1960s (long-term), at continental scale. While the 46year length of the DSI record is its greatest strength from a wind erosion monitoring perspective, there are a number of technical challenges to its use because when the World Meteorological Organisation (WMO) recording protocols were established the use of the data for wind erosion monitoring was never intended. Data recording and storage protocols are examined, including the effects of changes to the definition of how observers should interpret and record dust events. A method is described for selecting the 180 long-term ABM stations used in this study and the limitations of variable observation frequencies between stations are in part resolved. The rationale behind the DSI equation is explained and the examples of temporal and spatial data visualisation products presented include; a long term national wind erosion record (1965–2011), continental DSI maps, and maps of the erosion event types that are factored into the DSI equation. The DSI is tested against dust concentration data and found to provide an accurate representation of wind erosion activity. As the ABM observational records used here were collected according to WMO protocols, the DSI methodology could be used in all countries with WMO-compatible meteorological observation and recording systems.","T. O’Loingsigh and G.H. McTainsh and E.K. Tews and C.L. Strong and J.F. Leys and P. Shinkfield and N.J. Tapper",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Emplacement conditions of the 1256AD Al-Madinah lava flow field in Harrat Rahat, Kingdom of Saudi Arabia — Insights from surface morphology and lava flow simulations","Lava flow hazard modelling requires detailed geological mapping, and a good understanding of emplacement settings and the processes involved in the formation of lava flows. Harrat Rahat, Kingdom of Saudi Arabia, is a large volcanic field, comprising about 1000 predominantly small-volume volcanoes most of which have emitted lava flows of various lengths. A few eruptions took place in this area during the Holocene, and they were located in the northern extreme of the Harrat Rahat, a close proximity to critical infrastructure and population living in Al-Madinah City. In the present study, we combined field work, high resolution digital topography and morphometric analysis to infer the emplacement history of the last historical event in the region represented by the 1256AD Al-Madinah lava flow field. These data were also used to simulate 1256AD-type lava flows in the Harrat Rahat by the MAGFLOW lava flow emplacement model, which is able to relate the flow evolution to eruption conditions. The 1256AD lava flow field extent was mapped at a scale of 1:1000 from a high resolution (0.5m) Light Detection And Ranging (LiDAR) Digital Terrain Model (DTM), aerial photos with field support. The bulk volume of the lava flow field was estimated at 0.4km3, while the source volume represented by seven scoria cone was estimated at 0.023km3. The lava flow covered an area of 60km2 and reached a maximum length of 23.4km. The lava flow field comprises about 20.9% of pāhoehoe, 73.8% of 'a'ā, and 5.3% of late-stage outbreaks. Our field observation, also suggests that the lava flows of the Harrat Rahat region are mainly core-dominated and that they formed large lava flow fields by amalgamation of many single channels. These channels mitigated downslope by topography-lava flow and channel–channel interactions, highlighting this typical process that needs to be considered in the volcanic hazard assessment in the region. A series of numerical lava flow simulations was carried out using a range of water content (0.1–1wt.%), solidification temperature (800–600°C) and effusion curves (simple and complex curves). These simulations revealed that the MAGFLOW code is sensitive to the changes of water content of the erupting lava magma, while it is less sensitive to solidification temperature and the changes of the shape of effusion curve. The advance rate of the simulated lava flows changed from 0.01 to 0.22km/h. Using data and observations from the youngest volcanic event of the Harrat Rahat as input parameters to MAGFLOW code, it is possible to provide quantitative limits on this type of hazard.","Gábor Kereszturi and Károly Németh and Mohammed R Moufti and Annalisa Cappello and Hugo Murcia and Gaetana Ganci and Ciro Del Negro and Jonathan Procter and Hani Mahmoud Ali Zahran",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"First experiments with PowerPlay","Like a scientist or a playing child, PowerPlay (Schmidhuber, 2011) not only learns new skills to solve given problems, but also invents new interesting problems by itself. By design, it continually comes up with the fastest to find, initially novel, but eventually solvable tasks. It also continually simplifies or compresses or speeds up solutions to previous tasks. Here we describe first experiments with PowerPlay. A self-delimiting recurrent neural network SLIM RNN (Schmidhuber, 2012) is used as a general computational problem solving architecture. Its connection weights can encode arbitrary, self-delimiting, halting or non-halting programs affecting both environment (through effectors) and internal states encoding abstractions of event sequences. Our PowerPlay-driven SLIM RNN learns to become an increasingly general solver of self-invented problems, continually adding new problem solving procedures to its growing skill repertoire. Extending a recent conference paper (Srivastava, Steunebrink, Stollenga, & Schmidhuber, 2012), we identify interesting, emerging, developmental stages of our open-ended system. We also show how it automatically self-modularizes, frequently re-using code for previously invented skills, always trying to invent novel tasks that can be quickly validated because they do not require too many weight changes affecting too many previous tasks.","Rupesh Kumar Srivastava and Bas R. Steunebrink and Jürgen Schmidhuber",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Smart Deployment of Demonstrators into Successful Commercial Solutions","Product or service concepts based on emerging technologies are usually results of research projects, be they performed by academic groups or by research departments of companies. Many times, the prototypes or demonstrators that result from such projects are supposed to evolve into commercial products or services, but – at least in the first stage - there are more focus on proving key features of a technology, or the effectiveness / efficiency / applicability of various concepts or algorithms. However, evolving into commercial products is many times at least as challenging as building the prototypes. In case of software-based projects, this means changes in architecture, a lot of code rewriting and important usability improvements. This paper introduces a software-concept product design algorithm which aims to minimize the effort required in turning a demonstrator into a commercial product. This is done by generating two functionality sets: a pure demonstrator and a pure commercial one, then generating a hybrid functionality set with the corresponding architecture, and then assessing each functionality for the demonstrator and the commercial version in terms of development and improvement effort. Through iterations, in which the original functionality sets are improved, the difference between the two perspectives will be reduced until it gets below a reasonable limit in terms of effort. The paper presents a case study in which the algorithm is applied for planning a software platform for supporting SMEs in their innovation processes.","Stelian Brad and Mircea Fulea and Emilia Brad and Bogdan Mocan",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR6, CR9"
"Experiences of hearing voices: analysis of a novel phenomenological survey","Summary
Background
Auditory hallucinations—or voices—are a common feature of many psychiatric disorders and are also experienced by individuals with no psychiatric history. Understanding of the variation in subjective experiences of hallucination is central to psychiatry, yet systematic empirical research on the phenomenology of auditory hallucinations remains scarce. We aimed to record a detailed and diverse collection of experiences, in the words of the people who hear voices themselves.
Methods
We made a 13 item questionnaire available online for 3 months. To elicit phenomenologically rich data, we designed a combination of open-ended and closed-ended questions, which drew on service-user perspectives and approaches from phenomenological psychiatry, psychology, and medical humanities. We invited people aged 16–84 years with experience of voice-hearing to take part via an advertisement circulated through clinical networks, hearing voices groups, and other mental health forums. We combined qualitative and quantitative methods, and used inductive thematic analysis to code the data and χ2 tests to test additional associations of selected codes.
Findings
Between Sept 9 and Nov 29, 2013, 153 participants completed the study. Most participants described hearing multiple voices (124 [81%] of 153 individuals) with characterful qualities (106 [69%] individuals). Less than half of the participants reported hearing literally auditory voices—70 (46%) individuals reported either thought-like or mixed experiences. 101 (66%) participants reported bodily sensations while they heard voices, and these sensations were significantly associated with experiences of abusive or violent voices (p=0·024). Although fear, anxiety, depression, and stress were often associated with voices, 48 (31%) participants reported positive emotions and 49 (32%) reported neutral emotions. Our statistical analysis showed that mixed voices were more likely to have changed over time (p=0·030), be internally located (p=0·010), and be conversational in nature (p=0·010).
Interpretation
This study is, to our knowledge, the largest mixed-methods investigation of auditory hallucination phenomenology so far. Our survey was completed by a diverse sample of people who hear voices with various diagnoses and clinical histories. Our findings both overlap with past large-sample investigations of auditory hallucination and suggest potentially important new findings about the association between acoustic perception and thought, somatic and multisensorial features of auditory hallucinations, and the link between auditory hallucinations and characterological entities.
Funding
Wellcome Trust.","Angela Woods and Nev Jones and Ben Alderson-Day and Felicity Callard and Charles Fernyhough",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Sex differences after chronic stress in the expression of opioid-, stress- and neuroplasticity-related genes in the rat hippocampus","Opioid peptides and their receptors re-organize within hippocampal neurons of female, but not male, rats following chronic immobilization stress (CIS) in a manner that promotes drug-related learning. This study was conducted to determine if there are also sex differences in gene expression in the hippocampus following CIS. Adult female and male rats were subjected to CIS (30 min/day) for 10 days. Twenty-four hours after the last stressor, the rats were euthanized, the brains were harvested and the medial (dentate gyrus/CA1) and lateral (CA2/CA3) dorsal hippocampus were isolated. Following total RNA isolation, cDNA was prepared for gene expression analysis using a RT2 Profiler PCR expression array. This custom designed qPCR expression array contained genes for opioid peptides and receptors, as well as genes involved in stress-responses and candidate genes involved in synaptic plasticity, including those upregulated following oxycodone self-administration in mice. Few sex differences are seen in hippocampal gene expression in control (unstressed) rats. In response to CIS, gene expression in the hippocampus was altered in males but not females. In males, opioid, stress, plasticity and kinase/signaling genes were all down-regulated following CIS, except for the gene that codes for corticotropin releasing hormone, which was upregulated. Changes in opioid gene expression following chronic stress were limited to the CA2 and CA3 regions (lateral sample). In conclusion, modest sex- and regional-differences are seen in expression of the opioid receptor genes, as well as genes involved in stress and plasticity responses in the hippocampus following CIS.","Matthew Randesi and Yan Zhou and Sanoara Mazid and Shannon C. Odell and Jason D. Gray and J. Correa da Rosa and Bruce S. McEwen and Teresa A. Milner and Mary Jeanne Kreek",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Parallel tools for the bifurcation analysis of large-scale chemically reactive dynamical systems","In this work we propose a set of tools for the parallel application of pseudo-arclength continuation to a class of systems for which the right hand side can be properly represented by a time numerically calculated evolution operator. For example, the reverse flow reactor and the reactors network with periodically switched inlet and outlet sections belong to this class of system. To conduct a dynamical analysis of these systems when the key parameters are changed, it is necessary to compute the eigenvalues of the Jacobian matrix many times. Since the Jacobian can only be obtained numerically, and this in turn takes away really significant computational power, running this operation in parallel saves real time of computation. Examples, solution lines and performance diagrams for selected systems are presented and discussed.","Gaetano Continillo and Artur Grabski and Erasmo Mancusi and Lucia Russo",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Improvement of waste heat recuperation on an industrial textile dryer: Redesign of heat exchangers network and components","The improvement of low temperature exhausts heat recovery network of an industrial textile – drying machine (Stenter/Rameuse) is presented. A complete redesign of the layout of the water – gas heat exchangers network was done. The network was improved changing the original serial configuration of the heat recovery cells to a system with parallel manifolds for the water circuit. The heat transfer layout and the related heat exchangers were modelled with a dedicated thermal design code. The limited heat transfer coefficient of the internal gas side in the original configuration was improved with a “twin barrel” solution, with water in the outer annulus and exhaust gas in the inner duct equipped with internal longitudinal fins, an effective solution allowing easy fabrication and cleaning. A second step refinement design of the heat exchangers modules, realized with an OpenFOAM® CFD procedure, allowed the final definition and optimization of the fins size and layout, which were not continuous on the whole length of the module, but staggered on the inner side and shortened to about 1/3 of the length. Compared to the original version, the new heat exchangers network and the improved thermal design allowed an increase of the heat recovery from the exhausts of about 180%. The adoption of three staggered and segmented fins led to an increase of 97% with respect to the bare pipe. Finally, the results of the models were validated on a test bench reproducing one full-scale section of the drying machine: the tests gave positive issues, confirming the model predictions and the correct operability of the unit. Particularly, the accuracy of prediction of water temperature was very good (less than 0.5°C difference between simulation and measurements).","Daniele Fiaschi and Giampaolo Manfrida and Luigi Russo and Lorenzo Talluri",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Examining Patient Willingness to Pay for Magnetic Resonance Imaging Guided Prostate Biopsy: Implications in the Era of Health Savings Accounts","Introduction
The proliferation of health savings accounts has empowered patients to participate in medical decisions through a direct financial incentive. Using conjoint analysis we examined how much extra patients with a health savings account would be willing to pay for magnetic resonance imaging-transrectal ultrasound fusion guided prostate biopsy over transrectal ultrasound guided prostate biopsy.
Methods
We enrolled men who were 55 to 70 years old from a general urology clinic. We performed a literature review, distributed surveys and conducted semi-structured interviews to develop and rank attributes commonly used to compare magnetic resonance-ultrasound to transrectal ultrasound guided prostate biopsy. Using conjoint surveys we asked participants to select their preferred choice between 2 hypothetical biopsy interventions with differing levels of the attributes and cost. Results of the conjoint surveys were analyzed using a multinomial probit model. We performed a sensitivity analysis to assess the stability of our results after adjusting for age, history of prostate cancer, race, education, marital status, income and Zip Code of residence.
Results
Patients were willing to pay $1,598 more for a biopsy intervention with increased sensitivity to detect all cancer from 43% to 51% and $2,034 more for a negative predictive value improvement from 70% to 90%. Patients were not willing to pay extra for an intervention with improved sensitivity to detect high risk cancer alone. These estimates did not change with our sensitivity analysis.
Conclusions
Our findings suggest that patients are willing to pay approximately $1,500 to $2,000 from a health savings account for a biopsy intervention with a benefit profile similar to that of magnetic resonance-ultrasound guided prostate biopsy.","Chad Ellimoottil and Marissa Marcotte and Daniel Grace and Alexander Krasnikov and Joan M. Phillips and Marcus L. Quek and Robert Flanigan and Gopal N. Gupta",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"FLUID-STRUCTURE INTERACTION IN A U-TUBE WITH SURFACE ROUGHNESS AND PRESSURE DROP","In this research, the surface roughness affecting the pressure drop in a pipe used as the steam generator of a PWR was studied. Based on the CFD (Computational Fluid Dynamics) technique using a commercial code named ANSYS-FLUENT, a straight pipe was modeled to obtain the Darcy frictional coefficient, changed with a range of various surface roughness ratios as well as Reynolds numbers. The result is validated by the comparison with a Moody chart to set the appropriate size of grids at the wall for the correct consideration of surface roughness. The pressure drop in a full-scale U-shaped pipe is measured with the same code, correlated with the surface roughness ratio. In the next stage, we studied a reduced scale model of a U-shaped heat pipe with experiment and analysis of the investigation into fluid-structure interaction (FSI). The material of the pipe was cut from the real heat pipe of a material named Inconel 690 alloy, now used in steam generators. The accelerations at the fixed stations on the outer surface of the pipe model are measured in the series of time history, and Fourier transformed to the frequency domain. The natural frequency of three leading modes were traced from the FFT data, and compared with the result of a numerical analysis for unsteady, incompressible flow. The corresponding mode shapes and maximum displacement are obtained numerically from the FSI simulation with the coupling of the commercial codes, ANSYS-FLUENT and TRANSIENT_STRUCTURAL. The primary frequencies for the model system consist of three parts: structural vibration, BPF(blade pass frequency) of pump, and fluid-structure interaction.","GYUN-HO GIM and SE-MYONG CHANG and SINYOUNG LEE and GANGWON JANG",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Integrated microbial enhanced oil recovery (MEOR) simulation: Main influencing parameters and uncertainty assessment","The present study investigated the ability of a thermophilic anaerobic microbe (herein coded as AR80) for MEOR with the further objective to quantify the uncertainty of production forecast in terms of the cumulative probability distribution. A series of core flood experiments conducted in water-flooded Berea sandstone showed that up to 51% of initial oil-in-place was recovered when the plugs were treated with AR80 and shut-in for 14 days. Mainly, the oil recovery mechanisms were attributed to viscosity enhancement, wettability changes, permeability and flow effects. Matching the laboratory data using artificial intelligence: the optimized cumulative oil recovery could be achieved at an enthalpy of 894.2 J/gmol, Arrhenius frequency of 8.3, residual oil saturation of 20%, log of capillary number at microbe flooding stage of −1.26, and also depicted a history match error less than 3%. Therefrom, a sensitivity analysis conducted on reservoir shut-in period effect on oil recovery revealed that a relatively shorter shut-in period is recommended to warrant early incremental oil recovery effect for economical purposes. In addition, MEOR could enhance the oil recovery significantly if a larger capillary number (between 10−5 and 10−3.5) is attained. Per probabilistic estimation, MEOR could sustain already water-flooded well for a set period of time. This study showed that there is a 20% frequency of increasing the oil recovery by above 20% when a mature water-flooded reservoir is further flooded with AR80 for 2 additional years. Lastly, it was demonstrated herein that increasing the nutrient (yeast extract) concentration (from 0.1 to 1% weight) had less or no significant effect on the oil viscosity and subsequent recovery.","Eric O. Ansah and Yuichi Sugai and Ronald Nguele and Kyuro Sasaki",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Importance of the Quality of the Discharge Report in the Management of a Surgical Clinical Unit","Background
The discharge report is a basic document at the end of a care process, and is a key element in the coding process, since its correct wording, reliability and completeness are factors used to determine the hospital production.
Materials and methods
From a hypothesis based on the analysis of the consistency between the discharge report and data collected from the routine clinical notes during admission, we should be able to re-code all those mis-coded, thus placing them in a more appropriate diagnosis-related group (DRG). A total of 24 patient outliers were analysed for the correct filling in of the type and reason for admission, personal history, medication, anamnesis, primary and secondary diagnosis, surgical procedure, outcome, number of diagnostic and procedures cited, concordance between discharge report and history and recoding of the DRG.
Results
From a total of 24 episodes, 6 had precise and valid reports, 4 were valid but not precise enough, 9 were insufficient, and 5 were clearly invalid. The recoded DRG after the documentation review was not significantly different, according to the Wilcoxon test, being changed in only 5 cases (P=.680).
Conclusion
Quality in discharge reports depends on an adequate minimum data set (MDS) in concordance with the source documentation during admission. Discordance can change the DRG, despite it not being significantly different in our series. Self-audit of discharge reports allows quality improvements to be developed along with a reduction in information mistakes.
Resumen
Introducción
El informe de alta es un documento básico al finalizar un proceso asistencial, y es un elemento clave en el proceso de codificación. De su correcta redacción, fiabilidad y exhaustividad dependerán los datos que sirvan para determinar la producción hospitalaria.
Material y métodos
Partimos de la hipótesis de que, analizando la concordancia del informe de alta con los datos cotejados en la documentación del episodio, podremos recodificar todos aquellos casos infracodificados, imputándolos así a un grupo relacionado por el diagnóstico (GRD) más adecuado. Analizamos en 24 pacientes outliers la correcta cumplimentación de tipo y motivo de ingreso, antecedentes personales y medicación, resumen del episodio, diagnósticos principal y secundarios, procedimiento quirúrgico, evolución durante el episodio y número de diagnósticos y procedimientos enumerados, concordancia con la información real del episodio y los cambios teóricos entre los GRD antes y después del análisis.
Resultados
De 24 casos, 6 informes son válidos y claros; 4, válidos aunque poco claros; 9 son insuficientes y 5, claramente inválidos. La comparación de los GRD recalculados tras la interpretación de los datos del episodio no muestra diferencias significativas, mediante test de Wilcoxon, encontrándose tan solo modificaciones en 5 casos (p = 0,680).
Conclusiones
La calidad del informe de alta depende de la correcta inclusión de todos los datos del CMBD, en concordancia con el episodio. Las discordancias historia/informe pueden modificar el GRD que, en nuestra serie, no es estadísticamente significativo. La autoauditoría del informe de alta hospitalaria permite establecer líneas de mejora, al disminuir los errores de información.","Juan-Carlos Gomez-Rosado and María Sanchez-Ramirez and Javier Valdes-Hernandez and Luis C. Capitan-Morales and Marta I. del-Nozal-Nalda and Fernando Oliva-Mompean",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Putting Residents in the Office: An Effective Method to Teach the Systems-Based Practice Competency","Objectives
Systems-based practice (SBP) was 1 of 6 core competencies established by the Accreditation Council for Graduate Medical Education and has proven to be one of the most difficult to effectively implement. This pilot study presents an immersion workshop as an effective tool to teach the SBP competency in a way that could easily be integrated into a residency curriculum.
Design
In 2006, 16 surgical residents rotated through 3 stations for 30 minutes each: coding and billing, scheduling operations and return appointments, and patient check-in. Participants were administered a pretest and posttest questionnaire evaluating their knowledge of SBP, and were asked to evaluate the workshop.
Setting
Outpatient clinic at MedStar Georgetown University Hospital, Washington, DC.
Participants
Residents in the general surgery residency training program at MedStar Georgetown University Hospital.
Results
Most residents (62.5%) improved their score after the workshop, whereas 31.25% showed no change and 6.25% demonstrated a decrease in score. Overall within their training levels, all groups demonstrated an increase in mean test score. Postgraduate year-2 residents demonstrated the greatest change in mean score (20%), whereas postgraduate year-4 residents demonstrated the smallest change in mean score (3.3%).
Conclusions
An immersion workshop where general surgery residents gained direct exposure to SBP concepts in situ was an effective and practical method of integrating this core competency into the residency curriculum. Such a workshop could complement more formal didactic teaching and be easily incorporated into the curriculum. For example, this workshop could be integrated into the ambulatory care requirement that each resident must fulfill as part of their clinical training.","Marisa Pulcrano and A. Alfred Chahine and Amanda Saratsis and Jamie Divine-Cadavid and Vinod Narra and Stephen R.T. Evans",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Image-based field monitoring of Cercospora leaf spot in sugar beet by robust template matching and pattern recognition","This paper presents a novel image algorithm using template matching and pattern recognition frameworks for monitoring Cercospora leaf spot (CLS) development on sugar beets on a single leaf scale under real field conditions. Due to the variety and complexity of the open field, it is a great challenge to achieve continuous and robust foliar disease observation in real field conditions. We propose a novel and compact algorithm, composed of two frameworks and a post-processing. The algorithm has continuous and highly discriminative capabilities for observing the process of disease in a single leaf from plant-level time sequence images. The first framework is based on robust template matching by orientation code matching (OCM), which implements successive tracking of a single leaf from a beet plant against severe illumination changes and non-rigid plant movements. The second framework uses a pattern recognition method of support vector machine (SVM) for achieving further disease classification from clutter field background. Prior to SVM, we propose a three feature combination of L∗, a∗, Entropy×Density, which has strong discrimination power to classify CLS disease from the clutter scene containing sandy soil, leaves, leaf stalks, and specular reflection. Additionally, post-processing is introduced to filter false positive noise to enhance the precision of the classification. Field experiment results demonstrate the feasibility and applicability of the proposed algorithm for disease monitoring under real field conditions. Meanwhile, comparative results with other conventional matching methods and feature combinations show the effectiveness of our proposed algorithm in both foliage tracking and disease classification.","Rong Zhou and Shun’ichi Kaneko and Fumio Tanaka and Miyuki Kayamori and Motoshige Shimizu",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Group Profiling Automation for Crime and Terrorism (GPACT)","The U.S. State Department Bureau of Counterterrorism officially lists 59 foreign terrorist organizations, while the current Terrorism Research & Analysis Consortium (TRAC) database contains over 3,800 groups. The number of actual groups is constantly changing as new groups emerge and existing groups are redefined, thus motivating a need to automate the rapid generation of multi-faceted group profiles to provide on-demand support for analyst understanding. Robust, automated profiles can be generated for these groups by leveraging current Natural Language Processing (NLP) techniques and large-scale analytics over relevant text (e.g., news stories, social media). Information on key individuals, attack history, group interactions, and more can be extracted and assembled into a dynamic organizational profile. Lockheed Martin Advanced Technology Labs (LM ATL) has developed a prototype system for creating such profiles, based on the publicly released Integrated Crisis Early Warning System (ICEWS) Coded Event Data, a set of over 13 million automatically generated events extracted from public news stories. This set of data has proven valuable for situational awareness and event forecasting, and a more actor-centric view of the data can yield rich details about a group's history and modus operandi. Profile generation, then, is based on the following capabilities: (1) event clustering, (2) event trending, and (3) narrative generation. In this paper, we describe both the framework and analytical components of the Group Profiling Automation for Crime and Terrorism (GPACT) prototype that generates terrorist and criminal group profiles. After describing the overall framework we focus on three analytical capabilities. First, event clustering operates over the set of event data to identify clusters of related events relevant to a particular topic of interest (e.g., interactions with other groups, past attack history), similar to how topic and document clustering operates. The second, event trend analysis, performs analytics over event data focusing on clustered topics to provide awareness of aggregate patterns detectable in the data. Third, narrative generation uses a template-based approach to natural language generation to construct a textual overview of the organization. Our results are analyzed, and ideas for potential future research identified.","Jennifer Lautenschlager and Alicia Ruvinsky and Ian Warfield and Brian Kettler",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The effect of paramagnetic shift during thermal quench on internal components in fusion devices","A plasma current disruption is usually initiated by impurity influx that causes a rapid decrease in plasma thermal stored energy (thermal quench). Thermal quench occurs in 500–2000μs on a large device like ITER. Depending on the β value, the plasma may be either paramagnetic or diamagnetic. Thermal quench causes a large shift in paramagnetism (or diamagnetism) and a corresponding change in toroidal flux. The flux swing can be 1–2 Weber with the rate of change of the toroidal field between 25 and 150T/s for a device like ITER. The toroidal field shift induces poloidal current in the vessel and possibly in internal components. We have developed a method for simulating the thermal quench field shift that is compatible for use with the electromagnetic simulation codes. The method is based on a radially thin shell having the shape of the last closed flux surface with poloidal current driven to duplicate the toroidal field shift. The magnitude of the current and its time history are adjusted to duplicate the flux change during a disruption thermal quench. We will present the results of using this method to simulate the induced currents in a vacuum vessel having two shells.","M.A. Ulrickson and J.D. Kotulski",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The timing of spontaneous detection and repair of naming errors in aphasia","This study examined the timing of spontaneous self-monitoring in the naming responses of people with aphasia. Twelve people with aphasia completed a 615-item naming test twice, in separate sessions. Naming attempts were scored for accuracy and error type, and verbalizations indicating detection were coded as negation (e.g., “no, not that”) or repair attempts (i.e., a changed naming attempt). Focusing on phonological and semantic errors, we measured the timing of the errors and of the utterances that provided evidence of detection. The effects of error type and detection response type on error-to-detection latencies were analyzed using mixed-effects regression modeling. We first asked whether phonological errors and semantic errors differed in the timing of the detection process or repair planning. Results suggested that the two error types primarily differed with respect to repair planning. Specifically, repair attempts for phonological errors were initiated more quickly than repair attempts for semantic errors. We next asked whether this difference between the error types could be attributed to the tendency for phonological errors to have a high degree of phonological similarity with the subsequent repair attempts, thereby speeding the programming of the repairs. Results showed that greater phonological similarity between the error and the repair was associated with faster repair times for both error types, providing evidence of error-to-repair priming in spontaneous self-monitoring. When controlling for phonological overlap, significant effects of error type and repair accuracy on repair times were also found. These effects indicated that correct repairs of phonological errors were initiated particularly quickly, whereas repairs of semantic errors were initiated relatively slowly, regardless of their accuracy. We discuss the implications of these findings for theoretical accounts of self-monitoring and the role of speech error repair in learning.","Julia Schuchard and Erica L. Middleton and Myrna F. Schwartz",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Sit happens: Does sitting development perturb reaching development, or vice versa?","The development of reaching and of sitting during the first year of life is typically studied as separate yet related behaviors. Interestingly, very soon after learning to reach, 4–7-month-old infants start coordinating their arms with their trunk and legs for sitting. In this longitudinal study, we focused, for the first time, on how infants learn to use their arms for the dual tasks of reaching for objects while providing arm support as they learn to sit. We hypothesized that the use of arms for support during sitting development would be a temporary perturbation to reaching and result in a nonlinear progression of reaching skill. Eleven infants were studied monthly from the time they began to prop sit to the time of sitting independence (5–8 months of age). Behavioral coding, kinematics, and electromyography (EMG) characterized reaching and posture while infants sat as independently as possible. Results revealed significant changes across time in trunk movement and hand use as infants transitioned through three stages of sitting: with arm support, sitting briefly without arm support, and sitting independently. Infants used their hands more for contacting objects and less for posture support linearly across time. In contrast, changes in posture control as indicated by pelvis and trunk movement demonstrated a U-shaped curve with more movement of these two body segments during the middle stage of sitting than in the first or last stage. During the middle stage of sitting infants reached persistently even though posture control, measured by pelvis and trunk movement, appeared to be significantly challenged. Muscle activation consisted of tonic and variable combinations of muscle pairings in early sitting. As infants progressed to sitting without hand support, variable but successful strategies utilizing lower extremity muscles in a tight linkage with reach onset emerged to provide prospective control for reaching. Our findings support the contention that reaching both drives the development of sitting in infancy as well as perturbs sitting posture, factoring into the assembly of the complex dual sit–reach behavior that supports and expands flexible interaction with the environment.","Regina T. Harbourne and Michele A. Lobo and Gregory M. Karst and James Cole Galloway",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Assessing I2S-LWR economic competitiveness using systematic differential capital cost evaluation methodology","One of the main factors impeding nuclear power plant (NPP) construction is their economics. In this study, a systematic differential economics evaluation approach was developed through the use of the Code of Accounts guidelines to assess the costs of nuclear power plants. The methodology, entirely based on publicly available data, may serve as a template to evaluate direct costs for reactors of any size and design at any stage of developments. In particular, this approach was used to assess the costs of the Integral Inherently Safe Light Water Reactor (I2S-LWR). The I2S-LWR is a design concept of a large (∼1000 MWe) light water reactor. One of its key design features promoting inherent safety is implementation of an integral primary circuit configuration, which however necessitates a compact design of the core and primary circuit components. Through the methodology here presented, a representative loop PWR design was taken as a reference and the differential cost was estimated for each individual account based on the design difference, or similarity. Cost scaling techniques were applied to the accounts representing systems that differ from the ones of the reference PWR. Cost estimating techniques were used to evaluate cost of innovative components that are not part of standard PWR designs. By evaluating the cost difference of the I2S-LWR from the standard PWR, rather than the absolute cost, the uncertainty in the estimate is reduced. A similar approach was used by ORNL to estimate the cost of a Fluoride-salt High-temperature Reactor (FHR). A traditional four-loop PWR plant with a core thermal power of 3417 MWth (1144 MWe) was selected as the reference. Costs for that plant were prepared in 1978 by the Department of Energy (DOE) Energy Economics Data Base (EEDB), averaging actual cost incurred in the construction of several nuclear power plants (NPP), itemized with a great level of detail according to the Code of Accounts. This best estimate costs are denoted PWR12-BE. For each account, the cost of equipment, site labor and site material is provided. Industry experts at Westinghouse Electric Company performed a “sanity check” of the cost items, adjusting the cost of several items to match the current market and supply chain data. The detailed cost assessment of I2S-LWR was performed, systematically analyzing cost for each account, and applying the differential economics approach. First, Relative importance of each account, i.e., its contribution to the total cost was established, to help focus analysis on the most significant contributors. Moreover, the accounts describing components that are different than that of the PWR12-BE were identified. The integral configuration of the reactor has important implications on the cost of the reactor plant equipment (accounts 22x). Turbine generator equipment (Accounts 23x) is not believed to be much different than that of the reference design. I2S-LWR structures (Accounts 21x) mainly differ from that of a standard LWR as several buildings (containment building, shield building, annex building, waste processing building, fuel storage building) are integrated into a single building (nuclear island). Yardwork has a higher cost for the I2S-LWR, as the NI is partially below grade. On the other hand, due to its compact Nuclear Island footprint, I2S-LWR facilitates (and includes in its reference version) the use of seismic isolators, which contribute to reducing the direct cost. The total capital investment cost (TCIC), on the $/kW basis, of I2S-LWR is 5.84% lower than that of PWR12-BE, in spite of the lower power output of I2S-LWR. However, for the Western US (0.7 g), benefits of the seismic isolation are more pronounced, and the I2S-LWR total capital investment cost is 13.02% lower than that of PWR12-BE. If the I2S-LWR is compared to a PWR10-BE (traditional loop design, but scaled to the same power level as I2S-LWR), the savings are even higher, in the range 11.12%–17.89%. The analysis indicates that I2S-LWR has potential to offer an economically attractive design, with TCIC lower than that of a nuclear power plant based on a traditional loop PWR design. In other words, I2S-LWR design offers significantly enhanced safety, while at the same time improving economics. The differential economics approach developed in this paper can be used in identifying changes in cost of key components in order to improve the economics of a nuclear reactor design. The method can also help compare the economics of advanced Generation IV reactors and innovative water-cooled SMR (Small Modular Reactors) with respect to standard LWR technologies. In summary, the differential economics approach and can be used as a tool capable of helping stakeholder decisions.","G. Maronati and B. Petrovic and P. Ferroni",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A computer code for calculations in the algebraic collective model of the atomic nucleus","A Maple code is presented for algebraic collective model (ACM) calculations. The ACM is an algebraic version of the Bohr model of the atomic nucleus, in which all required matrix elements are derived by exploiting the model’s SU(1,1)×SO(5) dynamical group. This paper reviews the mathematical formulation of the ACM, and serves as a manual for the code. The code enables a wide range of model Hamiltonians to be analysed. This range includes essentially all Hamiltonians that are rational functions of the model’s quadrupole moments qˆM and are at most quadratic in the corresponding conjugate momenta πˆN (−2≤M,N≤2). The code makes use of expressions for matrix elements derived elsewhere and newly derived matrix elements of the operators [πˆ⊗qˆ⊗πˆ]0 and [πˆ⊗πˆ]LM. The code is made efficient by use of an analytical expression for the needed SO(5)-reduced matrix elements, and use of SO(5)⊃SO(3)  Clebsch–Gordan coefficients obtained from precomputed data files provided with the code.
Program summary
Program title: ACM Catalogue identifier: AEYO_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEYO_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 3873526 No. of bytes in distributed program, including test data, etc.: 46345414 Distribution format: tar.gz Programming language: Maple 18 (or versions 17, 16, 15). Computer: Any. Operating system: Any which supports Maple; tested under Linux, Max OSX, Windows 7. RAM: 500Mb Classification: 17.20. Nature of problem: The calculation of energy eigenvalues, transition rates and amplitudes of user specified Hamiltonians in the Bohr model of the atomic nucleus. Solution method: Exploit the model’s SU(1,1)×SO(5) dynamical group to calculate analytic (as far as possible) expressions for matrix elements, making use of extensive files (supplied) of SO(5)⊃SO(3)  Clebsch–Gordan coefficients. Diagonalisation of the resulting matrices (once the entries are converted to floating point) is carried out using the Maple library procedure Eigenvectors. (Maple [1] makes use of the NAG [2] and CLAPACK [3] linear algebra libraries.) Additional comments: 1.The dimension of the Hilbert space that can be handled is limited only by the available computer memory and the available SO(5)⊃SO(3)  Clebsch–Gordan coefficients (v1α1L1v2α2L2∥v3α3L3).2.The supplied data files provide coefficients (v1α1L1v2α2L2∥v3α3L3) for 1≤v2≤6, and contain all non-zero coefficients for v1<v3≤50 when v2∈1,3, for v1≤v3≤30 when v2∈2,4, and for v1≤v3≤25 when v2∈5,6. (Once calculated, further coefficients can be readily made available to the code without changing the code.) Thus, depending on the model Hamiltonian being analysed, the states in the Hilbert space used are limited in their seniority. For analysis of the more typical types of model Hamiltonian, only the coefficients with v2∈{1,3} are required, and therefore, with the supplied files, the seniority limit is 50. More exotic Hamiltonians having terms with seniority v2∈{2,4,5,6} would have the seniority limited to 30 or 25 accordingly.3.The code provides lower level procedures that give ready access to the Clebsch–Gordan coefficients and the SU(1, 1) and SO(5) matrix elements. These procedures are described in the manuscript and enable extensions to the code and model to be made easily.4.The accuracy to which Maple performs numerical calculations is determined by the Maple parameter Digits, which specifies the number of significant decimal digits used. The default value of 10 is more than adequate for most ACM calculations. Note, however, that if Digits is increased beyond a certain value (obtained from the Maple command evalhf(Digits), and usually 15 on modern computers) then the code can no longer take advantage of hardware mathematical operations, and is significantly slower. Documents included1.The code makes use of SO(5)⊃SO(3)  Clebsch–Gordan coefficients which are supplied in zip files, and must be installed by the user.2.A Maple worksheet that gives various example calculations and tests carried out using procedures from the code is provided.3.A 162 page PDF file containing everything displayed in the worksheet (input, output and comments, and making use of colour) is also provided. !!!!! The distribution file for this program is over 46 Mbytes and therefore is not delivered directly when download or Email is requested. Instead a html file giving details of how the program can be obtained is sent. !!!!! Running time: For a fixed value of the parameter Digits, the running time depends on the dimension of the Hilbert space on which the diagonalisation is performed, and this in turn is governed by the number of eigenvalues required and the accuracy required. Note that diagonalisation is performed separately in each L-space. For typical ACM calculations (such as those carried out in [4]), the matrices being diagonalised are usually of dimension at most a few hundred, and often much smaller. On a modest personal computer, the computation for the smallest cases takes at most a few seconds. The worksheet contains a range of examples for which the calculation time varies between a few seconds and 750s. In the latter case, diagonalisation is performed on L-spaces for 0≤L≤8, the dimensions of these spaces being between 154 and 616. References: [1]Maplesoft, Waterloo Maple Inc., Waterloo, ON, Canada.[2]NAG, www.nag.com.[3]CLAPACK, www.netlib.org/clapack.[4]D. J. Rowe, T. A. Welsh, M. A. Caprio, Phys. Rev. C 79(2009) 054304.","T.A. Welsh and D.J. Rowe",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"SUSPEND: Determining software suspiciousness by non-stationary time series modeling of entropy signals","Commercial anti-virus software traditionally memorizes specific byte sequences (known as “signatures”) in the file contents of previously encountered malware. However, malware authors can evade signature-based detection in many ways; for instance, by using obfuscation techniques such as “packing” (encryption or compression) to hide snippets of malicious code; by writing metamorphic malware; or by tampering with existing malware. We hypothesize that certain evasion techniques can leave traces in the file’s entropy signal, revealing either similarities to known malware or the presence of tampering per se. To this end, we present SUSPEND (SUSPicious ENtropy signal Detector), an expert system which evaluates the suspiciousness of an executable file’s entropy signal in order to subserve malware classification. Whereas traditionally, entropy analysis has been used for the goal of packer detection (and therefore entropy-based features often merely comprise mean entropy or the entropy of a few file subcomponents), SUSPEND applies non-stationary time series modeling to aid in malware detection. In particular, SUSPEND (a) quantifies the “amount of structure” in the entropy signal (through detrended fluctuation analysis), (b) finds the location and size of sudden jumps in entropy (through mean change point modeling), and (c) computes the distribution of entropic variation across multiple spatial scales (through wavelet decomposition). In addition, SUSPEND (d) summarizes the entropy signal’s empirical probability distribution. Because SUSPEND’s run time can be made to scale linearly in file size, it is well-suited for large-scale malware analysis. We apply SUSPEND to a large-scale malware detection task with 500,000 heterogeneous real-world samples and over 1 million features. We find that SUSPEND boosts the predictive performance of traditional entropy analysis (as found in packer detectors) from 77.02% to 96.62%. Moreover, SUSPEND’s focus on entropy signals makes it a natural candidate for combining with other types of features; for instance, combining SUSPEND with a strings-based feature set boosts predictive accuracy from 97.18% to 98.62%. Thus, whereas traditionally, entropy analysis has focused on detecting that a file is packed, SUSPEND’s more comprehensive representation of the entropy signal helps to determine that a file is malicious. We illustrate the application of SUSPEND by studying 18 pieces of VirRansom, a family of viral ransomware which could cost millions to large organizations. SUSPEND is able to detect 100% of the studied files with over 99% confidence, whereas a more traditional strings-based model was very close to undecided and represents the entire family with a single string.","Michael Wojnowicz and Glenn Chisholm and Brian Wallace and Matt Wolff and Xuan Zhao and Jay Luan",2017,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Networks of Cultured iPSC-Derived Neurons Reveal the Human Synaptic Activity-Regulated Adaptive Gene Program","Summary
Long-term adaptive responses in the brain, such as learning and memory, require synaptic activity-regulated gene expression, which has been thoroughly investigated in rodents. Using human iPSC-derived neuronal networks, we show that the human and the mouse synaptic activity-induced transcriptional programs share many genes and both require Ca2+-regulated synapse-to-nucleus signaling. Species-specific differences include the noncoding RNA genes BRE-AS1 and LINC00473 and the protein-coding gene ZNF331, which are absent in the mouse genome, as well as several human genes whose orthologs are either not induced by activity or are induced with different kinetics in mice. These results indicate that lineage-specific gain of genes and DNA regulatory elements affects the synaptic activity-regulated gene program, providing a mechanism driving the evolution of human cognitive abilities.","Priit Pruunsild and C. Peter Bengtson and Hilmar Bading",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Constructive and Unproductive Processing of Traumatic Experiences in Trauma-Focused Cognitive-Behavioral Therapy for Youth","Although there is substantial evidence to support the efficacy of cognitive-behavioral treatments (CBT) for posttraumatic stress disorder (PTSD), there is some debate about how these treatments have their effects. Modern learning theory and cognitive and emotional processing theories highlight the importance of reducing avoidance, facilitating the constructive processing of feared experiences, and strengthening new inhibitory learning. We examined variables thought to be associated with unproductive and constructive processing of traumatic experiences in a sample of 81 youth with elevated PTSD symptoms, who received Trauma-Focused Cognitive Behavioral Therapy (TF-CBT) for abuse or traumatic interpersonal loss. Sessions during the trauma narrative phase of TF-CBT were coded for indicators of unproductive processing (overgeneralization, rumination, avoidance) and constructive processing (decentering, accommodation of corrective information), as well as levels of negative emotion. In previous analyses of this trial (Ready et al., 2015), more overgeneralization during the narrative phase predicted less improvement in internalizing symptoms at posttreatment and a worsening of externalizing symptoms over the 12-month follow-up. In contrast, more accommodation predicted improvement in internalizing symptoms and also moderated the negative effects of overgeneralization on internalizing and externalizing symptoms. The current study examined correlates of overgeneralization and accommodation. Overgeneralization was associated with more rumination, less decentering, and more negative emotion, suggesting immersion in trauma-related material. Accommodation was associated with less avoidance and more decentering, suggesting a healthy distance from trauma-related material that might allow for processing and cognitive change. Decentering also predicted improvement in externalizing symptoms at posttreatment. Rumination and avoidance showed important associations with overgeneralization and accommodation, respectively, but did not predict treatment outcomes. This study identifies correlates of overgeneralization and accommodation that might shed light on how these variables relate to unproductive and constructive processing of traumatic experiences.","Adele M. Hayes and Carly Yasinski and Damion Grasso and C. Beth Ready and Elizabeth Alpert and Thomas McCauley and Charles Webb and Esther Deblinger",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Stillbirth evaluation: a stepwise assessment of placental pathology and autopsy","Background
The American Congress of Obstetricians and Gynecologists places special emphasis on autopsy as one of the most important tests for evaluation of stillbirth. Despite a recommendation of an autopsy, many families will decline the autopsy based on religious/cultural beliefs, fear of additional suffering for the child, or belief that no additional information will be obtained or of value. Further, many obstetric providers express a myriad of barriers limiting their recommendation for a perinatal autopsy despite their understanding of its value. Consequently, perinatal autopsy rates have been declining. Without the information provided by an autopsy, many women are left with unanswered questions regarding cause of death for their fetus and without clear management strategies to reduce the risk of stillbirth in future pregnancies. To avoid this scenario, it is imperative that clinicians are knowledgeable about the benefit of autopsy so they can provide clear information on its diagnostic utility and decrease potential barriers; in so doing the obstetrician can ensure that each family has the necessary information to make an informed decision.
Objective
We sought to quantify the contribution of placental pathologic examination and autopsy in identifying a cause of stillbirth and to identify how often clinical management is modified due to each result.
Study Design
This is a cohort study of all cases of stillbirth from 2009 through 2013 at a single tertiary care center. Records were reviewed in a stepwise manner: first the clinical history and laboratory results, then the placental pathologic evaluation, and finally the autopsy. At each step, a cause of death and the certainty of that etiology were coded. Clinical changes that would be recommended by information available at each step were also recorded.
Results
Among the 144 cases of stillbirth examined, 104 (72%) underwent autopsy and these cases constitute the cohort of study. The clinical and laboratory information alone identified a cause of death in 35 (24%). After placental pathologic examination, 88 (61%) cases had a probable cause of death identified. The addition of autopsy resulted in 78 (74%) cases having an identifiable probable cause of death. Placental examination alone changed clinical management in 52 (36%) cases. Autopsy led to additional clinical management changes in 6 (6%) cases.
Conclusion
This stepwise assessment of the benefit of both placental pathological examination and autopsy in changing probable cause of death beyond traditional clinical history and laboratory results emphasizes the need to implement more comprehensive evaluation of all stillbirths. With the aim of providing a cause of stillbirth to the parents, and to prevent future stillbirths, it behooves health care professionals to understand the value of this more comprehensive approach and convey that information to the bereaved parents.","Emily S. Miller and Lucy Minturn and Rebecca Linn and Debra E. Weese-Mayer and Linda M. Ernst",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The Limited Utility of Screening Laboratory Tests and Electrocardiograms in the Management of Unintentional Asymptomatic Pediatric Ingestions","Background
Suspected ingestions are a common chief complaint to the emergency department although the majority of ingestions by children are insignificant.
Objective
Assess the utility of screening laboratory tests and Electrocardiograms (ECGs) in unintentional asymptomatic pediatric poisonings.
Methods
Retrospective chart review at a tertiary care children's hospital and a regional poison center of patients less than 12 years of age using ICD-9 codes from January 2005 through December 2008. Laboratory or ECG results requiring intervention and/or direct treatment, a non-RPC subspecialty consultation, and/or prolonged Emergency Department stay was considered changed management.
Results
Five hundred ninety five suspected ingestions met our criteria. The median age was 2.6 years (IQR 1.6, 3.0 years) and 56% were male. One laboratory test or ECG was obtained in 233 patients (39%). Of 24 screening ECGs, 32 complete blood counts and 34 blood gases, none were clinically significant. Fifty-two patients received screening metabolic panels, 3 were abnormal and 2 changed management (anion gap metabolic acidosis with unsuspected salicylate ingestions). None of the 127 (21%) screening acetaminophen levels changed management. Two of sixty-five (13%) screening salicylate levels changed management. Three screening urine toxicology tests on patients with altered mental status were positive without ingestion history. No patient under the age of 12 years with normal vital signs and normal mental status had positive screening tests.
Conclusions
Screening laboratory tests and ECGs were of limited utility and rarely changed management despite being ordered in a significant number of patients. Screening tests are rarely indicated in unintentional overdoses in children who are asymptomatic.","George Sam Wang and Sara Deakyne and Lalit Bajaj and Shan Yin and Kennon Heard and Genie Roosevelt",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Fast Translational Motion, but not Radial, Circular or Biological Motion, Causes Spatially Selective Adaptation of Event Duration","It has been recently shown that adaptation to gratings oscillating at high-frequency compress perceived duration of subsequent, slower, stimuli displayed in the same location. These temporal distortions are spatially selective (Johnston et al., 2006), coded in spatiotopic coordinates (Burr et al., 2007, Burr et al., 2011), and do not result from changes in perceived speed of the adapted stimulus. These findings are important as they support the idea of a distributed framework of multiple mechanisms processing event time across the visual field. However, time distortions induced by motion adaptation have been tested only with simple translational motion, rather than more complex motions. Here we used a similar technique to Burr and coll. (2007) to measure time compression induced by adaptation to four kinds of motion stimuli: a) translating gratings, b) expanding concentric gratings, c) rotating radial gratings, and d) biological motion. We first measured changes in perceived speed caused by the motion adaptation (20Hz for the gratings). We found that perceived speed of the 10Hz test stimuli was dramatically reduced by around 40-50% in all conditions. Subsequently, we used these data to adjust the speed to compensate for the adaptation effects, and measure distortions in perceived duration with equated speed of test and probe gratings. In line with previous reports, perceived duration for translating patterns (displayed for 500ms) was found to be reduced up to 30-40%, even after speed compensation. However, duration estimates for radial and circular moving gratings (500ms) were always veridical. We used two versions of circular motion: with tangential speed constant across the visual patch (non-rigid-motion), and with rigid rotation, but neither caused temporal compression. Similarly, adaptation to a “runner” in biological motion reduced considerably the apparent speed of a briefly presented walker, but when matched for apparent speed, duration estimations were completely unaffected by adaptation. Taken together these results suggest that different mechanisms underlie time processing for simple and complex motion profiles. Why should only translational motion cause changes in temporal duration? fMRI evidence (Morrone et al., 2000) suggests that translational motion stimulates different brain regions from those responding to radial and circular motion, and this could be the basis for the selective effects on duration. Interestingly, however, the duration effects (after matching for apparent speed) are almost entirely spatiotopic, suggesting that they are not occurring at early, retinotopic brain areas. On the other hand it makes sense that the adaption should occur in space-based rather than retinal-based coordinates, if the purpose is related to temporal calibration of objects (in the real world). Why translational motion, but not other more complex forms of motion, should calibrate event duration is far from clear.","Michele Fornaciai and Roberto Arrighi and David C. Burr",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Zeitliche Entwicklung der Anwendungsprävalenz von Statinen in Deutschland – Ergebnisse der nationalen Interview- und Untersuchungssurveys 1997-1999 und 2008-2011","Zusammenfassung
Hintergrund
Evidenzbasierte Empfehlungen zur Behandlung mit Lipidsenkern, insbesondere mit Statinen nehmen einen zentralen Platz in der Therapie von Fettstoffwechselstörungen und in der Prävention kardiovaskulärer Ereignisse ein. In Deutschland liefern Daten der Gesetzlichen Krankenversicherungen (GKV) Informationen über zeitliche Entwicklungen in der Verordnung von Lipidsenkern. Was fehlt sind bevölkerungsbezogene Daten zu Veränderungen in der Anwendungsprävalenz nach soziodemographischen und gesundheitsrelevanten Merkmalen. Mit bundesweiten Interview- und Untersuchungssurveys bei Erwachsenen in Deutschland 1997-1999 (BGS98) und 2008-2011 (DEGS1) soll diese Informationslücke unter besonderer Berücksichtigung der Statinanwendung geschlossen werden.
Methoden
Die Studienpopulation umfasste 7.099 Teilnehmende in 1997-1999 und 7.091 in 2008-2011, die zum Zeitpunkt der jeweiligen Surveyerhebung 18-79 Jahre alt waren. Primärdaten zur Arzneimittelanwendung innerhalb der letzten 7 Tage vor dem Survey wurden mittels standardisierter Arzneimittelinterviews und Brown-Bag-Verfahren erhoben. Die Pharmazentralnummern auf den Originalverpackungen wurden eingescannt und anhand der aktuellsten Version des anatomisch-therapeutisch-chemischen Klassifikationssystems (ATC) kodiert. Die Krankengeschichte wurde mittels computer-assistierter Interviews erhoben. Schlaganfall und koronare Herzkrankheit (KHK) wurden nur bei Personen im Alter von 40-79 Jahren erfasst und als kardiovaskuläre Erkrankung definiert. Adipositas wurde auf der Grundlage standardisierter Messungen von Körpergewicht und Körpergröße als Body Mass Index (BMI) von >=30kg/m2 definiert. Information zu soziodemographischen Variablen und Art der Krankenversicherung wurde über einen Selbstausfüll-Fragebogen erhoben. In querschnittlichen deskriptiven Analysen wurde die Anwendungsprävalenz von Statinen (ATC-Codes: C10AA, C10BA, C10BX) nach Survey sowie die Veränderung zwischen den Surveys stratifiziert nach relevanten Vorerkrankungen und anderen Kovariablen berechnet. Der Zusammenhang zwischen Erhebungszeitpunkt und Statinanwendung wurde in multivariablen binären logistischen Regressionsmodellen bei Personen im Alter von 40-79 Jahren analysiert. Alle Ergebnisse wurden gewichtet und auf die Bevölkerung von 2010 standardisiert.
Ergebnisse
Zwischen den Surveyperioden 1997-1999 und 2008-2011 stieg die Anwendungsprävalenz von Statinen von 3,2% auf 8,8%. Die Zunahme war besonders ausgeprägt bei Personen im Alter von 65-79 Jahren (7,2% vs. 26,9%) und bei Personen mit relevanten Vorerkrankungen wie KHK (19,1% vs. 54,9%), Schlaganfall (17,1% vs. 50,1%), Diabetes mellitus (10,5% vs. 33,2%) und Fettstoffwechselstörung (12,6% vs. 27,8%). Bei Personen im Alter von 40-79 Jahren stieg die Prävalenz der Statinanwendung unabhängig von Kovariablen signifikant zwischen den beiden Surveyzeitpunkten an (Odds Ratio:3,70; 95% KI: 2,92-4,70). Dies betraf sowohl Personen mit (5,17; 3,50-7,64) als auch ohne kardiovaskuläre Erkrankungen (2,76; 2,07-3,67).
Schlussfolgerung
Die Zunahme in der Anwendungsprävalenz von Statinen in Deutschland zwischen den bundesweiten Gesundheitssurveys 1997-1999 und 2008-2011 reflektiert die Umsetzung aktueller Leitlinienempfehlungen ohne Hinweis auf Ungleichheit nach Geschlecht, Bildung, Art der Krankenversicherung oder Wohnregion. Diese bevölkerungsbezogenen Daten der Gesundheitssurveys ergänzen Informationen zur Verordnung von Statinen auf der Grundlage von GKV-Daten. Limitationen Survey basierter Informationen resultieren aus potentiellem Fehlklassifikations- und Selektionsbias sowie aus den großen zeitlichen Abständen zwischen wiederholten Surveyerhebungen. In weiteren Untersuchungen muss geklärt werden, warum beobachtete Anwendungsprävalenzen für Statine bei Personen mit kardiovaskulärer Erkrankung hinter den aktuellen Leitlinienempfehlungen zur kardiovaskulären Sekundärprävention zurückbleiben.
Background
Evidence-based guideline recommendations on lipid lowering drug treatment, in particular statin treatment, play an essential role in the management of dyslipidemias and in the prevention of cardiovascular disease events. In Germany, statutory health insurance data provide information on time trends in the prescription of lipid lowering drugs. However, population-based data regarding changes in user prevalence according to socio-demographic and health-related characteristics are lacking. Based on data from national health interview and examination surveys for adults in Germany 1997-1999 (GNHIES98) and 2008-2010 (DEGS1), the present analysis aims to close this information gap with a particular focus on the use of statins.
Methods
The study population consisted of 7,099 participants (GNHIES98) and 7,091 participants (DEGS1) aged 18 to 79 years at the time of the respective surveys. Primary data on medication use within 7 days prior to the survey were collected using standardized medication interviews and brown-bag drug review. Unique product identifiers on original drug containers were scanned and coded according to the latest version of the Anatomical Therapeutic Chemical (ATC) classification system. Medical history was obtained in computer-assisted personal interviews. A history of stroke or coronary heart disease (CHD) was assessed among persons aged 40 to 79 years only, and previous stroke or CHD were defined as cardiovascular disease. Obesity was defined as a body mass index (BMI) of ≥ 30kg/m2) based on calculation from standardized measures of body weight and height. Information on socio-demographic variables and type of health insurance was collected using standardized self-administered questionnaires. In cross-sectional descriptive analyses we calculated the prevalence of statin use (ATC codes: C10AA, C10BA, C10BX) by survey as well as the changes between surveys stratified according to relevant preexisting diseases and other co-variables. The association between survey period and statin use was analyzed in multivariable binary logistic regression models among persons aged 40 to 79 years. All results were weighted and standardized for the population of 2010.
Results
Between the two survey periods 1997-1999 and 2008-2011, the prevalence of statin use increased from 3.2 % to 8.8 %. The increase was most pronounced for the age group 65 to 79 years (7.2 % vs. 26.9 %) and among persons with relevant preexisting conditions, such as CHD (19.1 % vs. 54.9 %), stroke (17.1 % vs. 50.1 %), diabetes mellitus (10.5 % vs. 33.2 %), and dyslipidemia (12.6 % vs. 27.8 %). Among persons aged 40 to 79 years, the prevalence of statin use significantly increased between the two surveys, independent of co-variables (Odds Ratio: 3.70; 95 % confidence interval [CI]: 2.92 to 4.70). This applied to persons with cardiovascular disease (5.17; 3.50 to 7.64) and without cardiovascular disease (2.76; 2.07 to 3.67).
Conclusion
The increase in the prevalence of statin use in Germany between the two national health surveys (1997–1999 and 2008–2011) reflects the implementation of current guideline recommendations without evidence for inequalities according to gender, education, type of health insurance or region of residence. These population-based data add to information on statin prescription obtained from statutory health insurance data. Limitations of survey-based information derive from potential misclassification and selection bias as well as large time gaps between the survey periods. Further studies are needed to examine why the observed prevalence of statin use among persons with cardiovascular morbidity lags behind current guideline recommendations for secondary cardiovascular prevention.","Hildtraud C. Knopf and Markus A. Busch and Yong Du and Julia Truthmann and Anja Schienkiewitz and Christa Scheidt-Nave",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Influence of low frequency PSEN1 variants on familial Alzheimer’s disease risk in Brazil","About 30–70% of familial Alzheimer’s disease (AD) cases are related to mutations in presenilin-1 gene (PSEN1). Although the role of mutations and common variants in AD had been extensively investigated, the contribution of rare or low frequency PSEN1 variants on AD risk remains unclear. In the current study, we performed a mutational screening of PSEN1 coding exons and flanking intronic sequences among 53 index cases with familial history of AD from Rio de Janeiro (Brazil). Two missense variants (rs63750592; rs17125721), one rare and a low frequency variant, and two intronic variants (rs3025786; rs165932) were identified. In silico tools were used to predict the functional impact of the variants, revealing no changes in protein functionality by exonic variants. Otherwise, all variants were predicted to alter splicing signals. Prediction results, together with previous reports, suggest a correlation between rs17125721 and AD. So, a subsequent case-control study to evaluate the role of rs1712572 on AD risk was performed in an additional sample of 120 AD sporadic cases and in 149 elderly healthy controls by TaqMan Genotyping Assay. Our data indicates a risk association for rs17125721 in familial AD cases (OR=6.0; IC95%=1.06–33.79; p=0.042). In addition, we tested the multiplicative interaction between allele ε4 of the apolipoprotein E (APOE) and rs17125721 and no statistical association was found. Taken together, our findings provide new insight about the genetic relevance of low frequency PSEN1 variants for familial AD development.","Bianca Barbosa Abdala and Jussara Mendonça dos Santos and Andressa Pereira Gonçalves and Luciana Branco da Motta and Jerson Laks and Margarete Borges de Borges and Márcia Mattos Gonçalves Pimentel and Cíntia Barros Santos-Rebouças",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An object oriented code for simulating supersymmetric Yang–Mills theories","We present SUSY_LATTICE – a C++ program that can be used to simulate certain classes of supersymmetric Yang–Mills (SYM) theories, including the well known N=4 SYM in four dimensions, on a flat Euclidean space–time lattice. Discretization of SYM theories is an old problem in lattice field theory. It has resisted solution until recently when new ideas drawn from orbifold constructions and topological field theories have been brought to bear on the question. The result has been the creation of a new class of lattice gauge theories in which the lattice action is invariant under one or more supersymmetries. The resultant theories are local, free of doublers and also possess exact gauge-invariance. In principle they form the basis for a truly non-perturbative definition of the continuum SYM theories. In the continuum limit they reproduce versions of the SYM theories formulated in terms of twisted fields, which on a flat space–time is just a change of the field variables. In this paper, we briefly review these ideas and then go on to provide the details of the C++ code. We sketch the design of the code, with particular emphasis being placed on SYM theories with N=(2,2) in two dimensions and N=4 in three and four dimensions, making one-to-one comparisons between the essential components of the SYM theories and their corresponding counterparts appearing in the simulation code. The code may be used to compute several quantities associated with the SYM theories such as the Polyakov loop, mean energy, and the width of the scalar eigenvalue distributions.
Program summary
Program title: SUSY_LATTICE Catalogue identifier: AELS_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AELS_v1_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 9315 No. of bytes in distributed program, including test data, etc.: 95 371 Distribution format: tar.gz Programming language: C++ Computer: PCs and Workstations Operating system: Any, tested on Linux machines Classification:: 11.6 Nature of problem: To compute some of the observables of supersymmetric Yang–Mills theories such as supersymmetric action, Polyakov/Wilson loops, scalar eigenvalues and Pfaffian phases. Solution method: We use the Rational Hybrid Monte Carlo algorithm followed by a Leapfrog evolution and a Metropolis test. The input parameters of the model are read in from a parameter file. Restrictions: This code applies only to supersymmetric gauge theories with extended supersymmetry, which undergo the process of maximal twisting. (See Section 2 of the manuscript for details.) Running time: From a few minutes to several hours depending on the amount of statistics needed.","Simon Catterall and Anosh Joseph",2012,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR9"
"Mitochondrial swinger replication: DNA replication systematically exchanging nucleotides and short 16S ribosomal DNA swinger inserts","Assuming systematic exchanges between nucleotides (swinger RNAs) resolves genomic ‘parenthood’ of some orphan mitochondrial transcripts. Twenty-three different systematic nucleotide exchanges (bijective transformations) exist. Similarities between transcription and replication suggest occurrence of swinger DNA. GenBank searches for swinger DNA matching the 23 swinger versions of human and mouse mitogenomes detect only vertebrate mitochondrial swinger DNA for swinger type AT+CG (from five different studies, 149 sequences) matching three human and mouse mitochondrial genes: 12S and 16S ribosomal RNAs, and cytochrome oxidase subunit I. Exchange A<->T+C<->G conserves self-hybridization properties, putatively explaining swinger biases for rDNA, against protein coding genes. Twenty percent of the regular human mitochondrial 16S rDNA consists of short swinger repeats (from 13 exchanges). Swinger repeats could originate from recombinations between regular and swinger DNA: duplicated mitochondrial genes of the parthenogenetic gecko Heteronotia binoei include fewer short A<->T+C<->G swinger repeats than non-duplicated mitochondrial genomes of that species. Presumably, rare recombinations between female and male mitochondrial genes (and in parthenogenetic situations between duplicated genes), favors reverse-mutations of swinger repeat insertions, probably because most inserts affect negatively ribosomal function. Results show that swinger DNA exists, and indicate that swinger polymerization contributes to the genesis of genetic material and polymorphism.","Hervé Seligmann",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Changes in Long-acting β-agonist Utilization After the FDA’s 2010 Drug Safety Communication","Purpose
In February 2010, the US Food and Drug Administration (FDA) issued new recommendations for the safe use of long-acting β-agonists (LABAs) in patients with asthma. The objective of this study was to determine the impact of the FDA’s 2010 safety advisory on LABA utilization.
Methods
Using administrative data from the Oregon Medicaid program, we performed an interrupted time series regression to evaluate changes in the trend in new LABA prescriptions before and after the FDA’s 2010 advisory. Trends in incident fills were examined among those with and without an asthma diagnosis code and previous respiratory controller medication use; trends were also assessed according to patient age.
Findings
The average age of the 8646 study patients was 37 years, 53% had a diagnosis of asthma, 21% had no respiratory diagnosis, and 32% had not used a respiratory controller medication in the recent past. The trend in new LABA prescriptions declined by 0.09 new start per 10,000 patients per month (95% CI, –0.19 to –0.01) after the FDA’s advisory. Among those with a diagnosis of asthma, there was an immediate drop of 0.48 (95% CI, –0.93 to –0.03) and a 0.10 (95% CI, –0.13 to –0.06) decline in the monthly rate of new starts per 10,000 patients. Immediately after the FDA’s advisory, we observed a statistically significant 4.7% increase (95% CI, 0.8 to 8.7) in the proportion of new LABA starts with history of previous respiratory controller medication use. Utilization of LABAs did not change in those without a diagnosis of asthma.
Implications
The FDA’s 2010 advisory was associated with modest reductions in LABA utilization overall and in ways highlighted in their recommendations.","Daniel M. Hartung and Luke Middleton and Sheila Markwardt and Kaylee Williamson and Kathy Ketchum",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical simulation of impact tests on reinforced concrete beams","This paper focuses on numerical simulation of impact tests of reinforced concrete (RC) beams by the LS-DYNA finite element (FE) code. In the FE model, the elasto-plastic damage cap (EPDC) model, which is based on continuum damage mechanics in combination with plasticity theory, is used for concrete, and the reinforcement is assumed to be elasto-plastic. The numerical results compares well with the experimental values reported in the literature, in terms of impact force history, mid-span deflection history and crack patterns of RC beams. By comparing the numerical and experimental results, several important behavior of concrete material is investigated, which includes: damage variable to describe the strain softening section of stress–strain curve; the cap surface to describe the plastic volume change; the shape of the meridian and deviatoric plane to describe the yield surface as well as two methods of incorporating rebar into concrete mesh. This study gives a good example of using EPDC model and can be utilized for the development new constitutive models for concrete in future.","Hua Jiang and Xiaowo Wang and Shuanhai He",2012,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Measuring Hospital Performance in Congenital Heart Surgery: Administrative Versus Clinical Registry Data","Background
In congenital heart surgery, hospital performance has historically been assessed using widely available administrative data sets. Recent studies have demonstrated inaccuracies in case ascertainment (coding and inclusion of eligible cases) in administrative versus clinical registry data; however, it is unclear whether this impacts assessment of performance on a hospital level.
Methods
Merged data from The Society of Thoracic Surgeons (STS) database (clinical registry) and the Pediatric Health Information Systems (PHIS) database (administrative data set) for 46,056 children undergoing cardiac operations (2006–2010) were used to evaluate in-hospital mortality for 33 hospitals based on their administrative versus registry data. Standard methods to identify/classify cases were used: Risk Adjustment in Congenital Heart Surgery, version 1 (RACHS-1) in the administrative data and STS–European Association for Cardiothoracic Surgery (STAT) methodology in the registry.
Results
Median hospital surgical volume based on the registry data was 269 cases per year; mortality was 2.9%. Hospital volumes and mortality rates based on the administrative data were on average 10.7% and 4.7% lower, respectively, although this varied widely across hospitals. Hospital rankings for mortality based on the administrative versus registry data differed by 5 or more rank positions for 24% of hospitals, with a change in mortality tertile classification (high, middle, or low mortality) for 18% and a change in statistical outlier classification for 12%. Higher volume/complexity hospitals were most impacted. Agency for Healthcare Quality and Research (AHRQ) methods in the administrative data yielded similar results.
Conclusions
Inaccuracies in case ascertainment in administrative versus clinical registry data can lead to important differences in assessment of hospital mortality rates for congenital heart surgery.","Sara K. Pasquali and Xia He and Jeffrey P. Jacobs and Marshall L. Jacobs and Michael G. Gaies and Samir S. Shah and Matthew Hall and J. William Gaynor and Eric D. Peterson and John E. Mayer and Jennifer C. Hirsch-Romano",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Challenges of standardized continuous quality improvement programs in community pharmacies: The case of SafetyNET-Rx","Background
Research on continuous quality improvement (CQI) in community pharmacies lags in comparison to service, manufacturing, and various health care sectors. As a result, very little is known about the challenges community pharmacies face when implementing CQI programs in general, let alone the challenges of implementing a standardized and technologically sophisticated one.
Objective
This research identifies the initial challenges of implementing a standardized CQI program in community pharmacies and how such challenges were addressed by pharmacy staff.
Methods
Through qualitative interviews, a multisite study of the SafetyNET-Rx CQI program involving community pharmacies in Nova Scotia, Canada, was performed to identify such challenges. Interviews were conducted with the CQI facilitator (ie, staff pharmacist or technician) in 55 community pharmacies that adopted the SafetyNET-Rx program. Of these 55 pharmacies, 25 were part of large national corporate chains, 22 were part of banner chains, and 8 were independent pharmacies. A total of 10 different corporate chains and banners were represented among the 55 pharmacies. Thematic content analysis using well-established coding procedures was used to explore the interview data and elicit the key challenges faced.
Results
Six major challenges were identified, specifically finding time to report, having all pharmacy staff involved in quality-related event (QRE) reporting, reporting apprehensiveness, changing staff relationships, meeting to discuss QREs, and accepting the online technology. Challenges were addressed in a number of ways including developing a manual-online hybrid reporting system, managers paying staff to meet after hours, and pharmacy managers showing visible commitment to QRE reporting and learning.
Conclusions
This research identifies key challenges to implementing CQI programs in community pharmacies and also provides a starting point for future research relating to how the challenges of QRE reporting and learning in community pharmacies change over time.","Todd A. Boyle and Neil J. MacKinnon and Thomas Mahaffey and Kellie Duggan and Natalie Dow",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Leukoencephalopathy and long-term neurobehavioural, neurocognitive, and brain imaging outcomes in survivors of childhood acute lymphoblastic leukaemia treated with chemotherapy: a longitudinal analysis","Summary
Background
Leukoencephalopathy is observed in some children undergoing chemotherapy for acute lymphoblastic leukaemia, although its effects on long-term outcomes is unknown. This study examines the associations between acute leukoencephalopathy and neurobehavioural, neurocognitive, and brain white matter imaging outcomes in long-term survivors of childhood acute lymphoblastic leukaemia treated with chemotherapy without cranial radiation.
Methods
In this longitudinal analysis, we used data of children with acute lymphoblastic leukaemia at St Jude Children's Research Hospital (Memphis, TN, USA) who had been treated between June 1, 2000, and Oct 31, 2010. Eligible patients were diagnosed with non-B-cell acute lymphoblastic leukaemia, aged at least 8 years, and survivors with at least 5 years since their initial diagnosis. Brain MRIs obtained during active therapy were systematically coded for leukoencephalopathy using Common Terminology Criteria for Adverse Event version 4. At least 5 years after their diagnosis, survivors completed neurocognitive testing, another brain MRI, and their parents completed neurobehavioural ratings of their child (Behavior Rating Inventory of Executive Function [BRIEF]). Follow-up MRI included diffusion tensor imaging to assess white matter integrity, with indices of fractional anisotropy, axial diffusivity, and radial diffusivity from frontal lobes, parietal lobes, and in the frontostriatal tract. The neuroradiologist, who assessed abnormal MRIs, was masked to both group assignment of survivors and the neurobehavioural and neurocognitive outcomes. The primary outcomes were neurobehavioural function, assessed from completed BRIEF, and neurocognitive performance, measured by direct neurocognitive tests (Delis-Kaplan Executive Function System, Wechsler Intelligence Scale for Children-IV/Wechsler Adult Intelligence Scale-III, Rey-Osterrieth Complex Figure Test, and Lafayette Grooved Pegboard Test). This study had completed enrolment in October, 2014, and is registered as an observational study at ClinicalTrials.gov, number NCT01014195.
Findings
Between Feb 18, 2010, and Oct 22, 2014, 210 (70%) of 301 eligible survivors participated in our study of whom 190 were evaluable, 162 had an MRI. 56 participants had quantitative brain imaging data and were included in evaluable population analyses. 51 (27%) of the 190 evaluable participants had acute leukoencephalopathy. Compared with population norms, survivors were reported to have more neurobehavioural problems with working memory, organisation, initiation, and planning (p<0·001 for all). Survivors had worse scores than the general population on direct measures of memory span, processing speed, and executive function (p<0·05 for all). Survivors with a history of acute leukoencephalopathy had more neurobehavioural problems than survivors with no history of leukoencephalopathy on organisation (adjusted T-score 56·2 [95% CI 53·3–59·1] vs 52·2 [50·4–53·9], p=0·020) and initiation (55·5 [52·7–58·3] vs 52·1 [50·4–53·8], p=0·045). Survivors with acute leukoencephalopathy also had reduced white matter integrity in the frontostriatal tract at follow-up: lower fractional anisotropy (p=0·069), higher axial diffusivity (p=0·020), and higher radial diffusivity (p=0·0077). A one-unit change in the radial diffusivity index corresponded with a 15·0 increase in raw score points on initiation, 30·3 on planning, and 28·0 on working memory (p<0·05 for all).
Interpretation
Acute leukoencephalopathy during chemotherapy treatment, without cranial radiation, for childhood acute lymphoblastic leukaemia predicted higher risk for long-term neurobehavioural problems and reduced white matter integrity in frontal brain regions. Survivors of childhood acute lymphoblastic leukaemia might benefit from preventive cognitive or behavioural interventions, particularly those who develop acute leukoencephalopathy.
Funding
National Institute of Mental Health, National Cancer Institute, American Lebanese Syrian Associated Charities.","Yin Ting Cheung and Noah D Sabin and Wilburn E Reddick and Deepa Bhojwani and Wei Liu and Tara M Brinkman and John O Glass and Scott N Hwang and Deokumar Srivastava and Ching-Hon Pui and Leslie L Robison and Melissa M Hudson and Kevin R Krull",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Effect of valence holes kinetics on material excitation in tracks of swift heavy ions","A considerable part of the excess energy of the electronic subsystem of a solid penetrated by a swift heavy ion (SHI) is accumulated in valence holes. Spatial redistribution of these holes can affect subsequent relaxation, resulting in ionizations of new electrons by hole impacts as well as energy transfer to the target lattice. A new version of the Monte Carlo code TREKIS is applied to study this effect in Al2O3 for SHI tracks. The complex dielectric function (CDF) formalism is used to calculate the cross sections of interaction of involved charged particles (an ion, electrons, holes) with the target giving us ability to take into account collective response of a target to excitations. We compare the radial distributions of the densities and energies of excited electrons and valence holes at different times to those obtained under the assumption of immobile holes used in earlier works. The comparison shows a significant difference between these distributions within the track core, where the majority of slow electrons and valence holes are located at femtosecond timescales after the ion impact. The study demonstrates that the energy deposited by valence holes into the lattice in nanometric tracks is comparable to the energy transferred by excited electrons. Radii of structure transformations in tracks produced by these energy exchange channels are in a good agreement with experiments.","R.A. Rymzhanov and N.A. Medvedev and A.E. Volkov",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mutational analysis of the VCP gene in Parkinson's disease","Mutations in the valosin-containing protein gene (VCP) have been identified in neurological disorders (inclusion body myopathy—early Paget's disease of the bone—frontotemporal dementia and amyotrophic lateral sclerosis) and are thought to play a role in the clearance of abnormally folded proteins. Parkinsonism has been noted in kindreds with VCP mutations. Based on this, we hypothesized that mutations in VCP may also contribute to idiopathic Parkinson's disease (PD). We screened the coding region of the VCP gene in a large cohort of 768 late-onset PD cases (average age at onset, 70 years), both sporadic and with positive family history. We identified a number of rare single nucleotide changes, including a variant previously described to be pathogenic, but no clear disease-causing variants. We conclude that mutations in VCP are not a common cause for idiopathic PD.","Elisa Majounie and Bryan J. Traynor and Adriano Chiò and Gabriella Restagno and Jessica Mandrioli and Michael Benatar and J. Paul Taylor and Andrew B. Singleton",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Importancia de la calidad del informe de alta en la gestión de una unidad clínica quirúrgica","Resumen
Introducción
El informe de alta es un documento básico al finalizar un proceso asistencial, y es un elemento clave en el proceso de codificación. De su correcta redacción, fiabilidad y exhaustividad dependerán los datos que sirvan para determinar la producción hospitalaria.
Material y métodos
Partimos de la hipótesis de que, analizando la concordancia del informe de alta con los datos cotejados en la documentación del episodio, podremos recodificar todos aquellos casos infracodificados, imputándolos así a un grupo relacionado por el diagnóstico (GRD) más adecuado. Analizamos en 24 pacientes outliers la correcta cumplimentación de tipo y motivo de ingreso, antecedentes personales y medicación, resumen del episodio, diagnósticos principal y secundarios, procedimiento quirúrgico, evolución durante el episodio y número de diagnósticos y procedimientos enumerados, concordancia con la información real del episodio y los cambios teóricos entre los GRD antes y después del análisis.
Resultados
De 24 casos, 6 informes son válidos y claros; 4, válidos aunque poco claros; 9 son insuficientes y 5, claramente inválidos. La comparación de los GRD recalculados tras la interpretación de los datos del episodio no muestra diferencias significativas, mediante test de Wilcoxon, encontrándose tan solo modificaciones en 5 casos (p = 0,680).
Conclusiones
La calidad del informe de alta depende de la correcta inclusión de todos los datos del CMBD, en concordancia con el episodio. Las discordancias historia/informe pueden modificar el GRD que, en nuestra serie, no es estadísticamente significativo. La autoauditoría del informe de alta hospitalaria permite establecer líneas de mejora, al disminuir los errores de información.
Background
The discharge report is a basic document at the end of a care process, and is a key element in the coding process, since its correct wording, reliability and completeness are factors used to determine the hospital production.
Material and methods
From a hypothesis based on the analysis of the consistency between the discharge report and data collected from the routine clinical notes during admission, we should be able to re-code all those mis-coded, thus placing them in a more appropriate diagnosis-related group (DRG). A total of 24 patient outliers were analysed for the correct filling in of the type and reason for admission, personal history, medication, anamnesis, primary and secondary diagnosis, sugical procedure, outcome, number of diagnostic and procedures cited, concordance between discharge report and history and recoding of the DRG.
Results
From a total of 24 episodes, 6 had precise and valid reports, 4 were valid but not precise enough, 9 were insufficient, and 5 were clearly invalid. The recoded DRG after the documentation review was not significantly different, according to the Wilcoxon test, being changed in only 5 cases (P = .680).
Conclusion
Quality in discharge reports depends on an adequate minimum data set (MDS) in concordance with the source documentation during admission. Discordance can change the DRG, despite it not being significantly different in our series. Self-audit of discharge reports allows quality improvements to be developed along with a reduction in information mistakes.","Juan-Carlos Gomez-Rosado and María Sanchez-Ramirez and Javier Valdes-Hernandez and Luis C. Capitan-Morales and Marta I. del-Nozal-Nalda and Fernando Oliva-Mompean",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Evolutionary history and adaptive significance of the polymorphic Pan I in migratory and stationary populations of Atlantic cod (Gadus morhua)","The synaptophysin (SYP) family comprises integral membrane proteins involved in vesicle-trafficking events, but the physiological function of several members has been enigmatic for decades. The presynaptic SYP protein controls neurotransmitter release, while SYP-like 2 (SYPL2) contributes to maintain normal Ca2+-signaling in the skeletal muscles. The polymorphic pantophysin (Pan I) of Atlantic cod shows strong genetic divergence between stationary and migratory populations, which seem to be adapted to local environmental conditions. We have investigated the functional involvement of Pan I in the different ecotypes by analyzing the 1) phylogeny, 2) spatio-temporal gene expression, 3) structure–function relationship of the Pan IA and IB protein variants, and 4) linkage to rhodopsin (rho) recently proposed to be associated with different light sensitivities in Icelandic populations of Atlantic cod. We searched for SYP family genes in phylogenetic key species and identified a single syp-related gene in three invertebrate chordates, while four members, Syp, Sypl1, Sypl2 and synaptoporin (Synpr), were found in tetrapods, Comoran coelacanth and spotted gar. Teleost fish were shown to possess duplicated syp, sypl2 and synpr genes of which the sypl2b paralog is identical to Pan I. The ubiquitously expressed cod Pan I codes for a tetra-spanning membrane protein possessing five amino acid substitutions in the first intravesicular loop, but only minor structural differences were shown between the allelic variants. Despite sizable genomic distance (>2.5Mb) between Pan I and rho, highly significant linkage disequilibrium was found by genotyping shallow and deep water juvenile settlers predominated by the Pan IA–rhoA and Pan IB–rhoB haplotypes, respectively. However, the predicted rhodopsin protein showed no amino acid changes, while multiple polymorphic sites in the upstream region might affect the gene expression and pigment levels in stationary and migratory cod. Alternatively, other strongly linked genes might be responsible for the sharp settling stratification of juveniles and the different vertical behavior patterns of adult Atlantic cod.","Øivind Andersen and Hanne Johnsen and Maria Cristina De Rosa and Kim Præbel and Suzana Stjelja and Tina Graceline Kirubakaran and Davide Pirolli and Sissel Jentoft and Svein-Erik Fevolden",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Estimates of global, regional, and national incidence, prevalence, and mortality of HIV, 1980–2015: the Global Burden of Disease Study 2015","Summary
Background
Timely assessment of the burden of HIV/AIDS is essential for policy setting and programme evaluation. In this report from the Global Burden of Disease Study 2015 (GBD 2015), we provide national estimates of levels and trends of HIV/AIDS incidence, prevalence, coverage of antiretroviral therapy (ART), and mortality for 195 countries and territories from 1980 to 2015.
Methods
For countries without high-quality vital registration data, we estimated prevalence and incidence with data from antenatal care clinics and population-based seroprevalence surveys, and with assumptions by age and sex on initial CD4 distribution at infection, CD4 progression rates (probability of progression from higher to lower CD4 cell-count category), on and off antiretroviral therapy (ART) mortality, and mortality from all other causes. Our estimation strategy links the GBD 2015 assessment of all-cause mortality and estimation of incidence and prevalence so that for each draw from the uncertainty distribution all assumptions used in each step are internally consistent. We estimated incidence, prevalence, and death with GBD versions of the Estimation and Projection Package (EPP) and Spectrum software originally developed by the Joint United Nations Programme on HIV/AIDS (UNAIDS). We used an open-source version of EPP and recoded Spectrum for speed, and used updated assumptions from systematic reviews of the literature and GBD demographic data. For countries with high-quality vital registration data, we developed the cohort incidence bias adjustment model to estimate HIV incidence and prevalence largely from the number of deaths caused by HIV recorded in cause-of-death statistics. We corrected these statistics for garbage coding and HIV misclassification.
Findings
Global HIV incidence reached its peak in 1997, at 3·3 million new infections (95% uncertainty interval [UI] 3·1–3·4 million). Annual incidence has stayed relatively constant at about 2·6 million per year (range 2·5–2·8 million) since 2005, after a period of fast decline between 1997 and 2005. The number of people living with HIV/AIDS has been steadily increasing and reached 38·8 million (95% UI 37·6–40·4 million) in 2015. At the same time, HIV/AIDS mortality has been declining at a steady pace, from a peak of 1·8 million deaths (95% UI 1·7–1·9 million) in 2005, to 1·2 million deaths (1·1–1·3 million) in 2015. We recorded substantial heterogeneity in the levels and trends of HIV/AIDS across countries. Although many countries have experienced decreases in HIV/AIDS mortality and in annual new infections, other countries have had slowdowns or increases in rates of change in annual new infections.
Interpretation
Scale-up of ART and prevention of mother-to-child transmission has been one of the great successes of global health in the past two decades. However, in the past decade, progress in reducing new infections has been slow, development assistance for health devoted to HIV has stagnated, and resources for health in low-income countries have grown slowly. Achievement of the new ambitious goals for HIV enshrined in Sustainable Development Goal 3 and the 90-90-90 UNAIDS targets will be challenging, and will need continued efforts from governments and international agencies in the next 15 years to end AIDS by 2030.
Funding
Bill & Melinda Gates Foundation, and National Institute of Mental Health and National Institute on Aging, National Institutes of Health.","Haidong Wang and Tim M Wolock and Austin Carter and Grant Nguyen and Hmwe Hmwe Kyu and Emmanuela Gakidou and Simon I Hay and Edward J Mills and Adam Trickey and William Msemburi and Matthew M Coates and Meghan D Mooney and Maya S Fraser and Amber Sligar and Joshua Salomon and Heidi J Larson and Joseph Friedman and Amanuel Alemu Abajobir and Kalkidan Hassen Abate and Kaja M Abbas and Mohamed Magdy Abd El Razek and Foad Abd-Allah and Abdishakur M Abdulle and Semaw Ferede Abera and Ibrahim Abubakar and Laith J Abu-Raddad and Niveen M E Abu-Rmeileh and Gebre Yitayih Abyu and Akindele Olupelumi Adebiyi and Isaac Akinkunmi Adedeji and Ademola Lukman Adelekan and Koranteng Adofo and Arsène Kouablan Adou and Oluremi N Ajala and Tomi F Akinyemiju and Nadia Akseer and Faris Hasan Al Lami and Ziyad Al-Aly and Khurshid Alam and Noore K M Alam and Deena Alasfoor and Saleh Fahed S Aldhahri and Robert William Aldridge and Miguel Angel Alegretti and Alicia V Aleman and Zewdie Aderaw Alemu and Rafael Alfonso-Cristancho and Raghib Ali and Ala'a Alkerwi and François Alla and Rajaa Mohammad and Salem Al-Raddadi and Ubai Alsharif and Elena Alvarez and Nelson Alvis-Guzman and Azmeraw T Amare and Alemayehu Amberbir and Adeladza Kofi Amegah and Walid Ammar and Stephen Marc Amrock and Carl Abelardo T Antonio and Palwasha Anwari and Johan Ärnlöv and Al Artaman and Hamid Asayesh and Rana Jawad Asghar and Reza Assadi and Suleman Atique and Lydia S Atkins and Euripide Frinel G Arthur Avokpaho and Ashish Awasthi and Beatriz Paulina Ayala Quintanilla and Umar Bacha and Alaa Badawi and Aleksandra Barac and Till Bärnighausen and Arindam Basu and Tigist Assefa Bayou and Yibeltal Tebekaw Bayou and Shahrzad Bazargan-Hejazi and Justin Beardsley and Neeraj Bedi and Derrick A Bennett and Isabela M Bensenor and Balem Demtsu Betsu and Addisu Shunu Beyene and Eesh Bhatia and Zulfiqar A Bhutta and Sibhatu Biadgilign and Boris Bikbov and Sait Mentes Birlik and Donal Bisanzio and Michael Brainin and Alexandra Brazinova and Nicholas J K Breitborde and Alexandria Brown and Michael Burch and Zahid A Butt and Julio Cesar Campuzano and Rosario Cárdenas and Juan Jesus Carrero and Carlos A Castañeda-Orjuela and Jacqueline Castillo Rivas and Ferrán Catalá-López and Hsing-Yi Chang and Jung-chen Chang and Laxmikant Chavan and Wanqing Chen and Peggy Pei-Chia Chiang and Mirriam Chibalabala and Vesper Hichilombwe Chisumpa and Jee-Young Jasmine Choi and Devasahayam Jesudas Christopher and Liliana G Ciobanu and Cyrus Cooper and Tukur Dahiru and Solomon Abrha Damtew and Lalit Dandona and Rakhi Dandona and José das Neves and Pieter de Jager and Diego De Leo and Louisa Degenhardt and Robert P Dellavalle and Kebede Deribe and Amare Deribew and Don C Des Jarlais and Samath D Dharmaratne and Eric L Ding and Pratik Pinal Doshi and Kerrie E Doyle and Tim R Driscoll and Manisha Dubey and Yousef Mohamed Elshrek and Iqbal Elyazar and Aman Yesuf Endries and Sergey Petrovich Ermakov and Babak Eshrati and Alireza Esteghamati and Imad D A Faghmous and Carla Sofia e Sa Farinha and Andre Faro and Maryam S Farvid and Farshad Farzadfar and Seyed-Mohammad Fereshtehnejad and Joao C Fernandes and Florian Fischer and Joseph Robert Anderson Fitchett and Nataliya Foigt and Nancy Fullman and Thomas Fürst and Fortuné Gbètoho Gankpé and Teshome Gebre and Amanuel Tesfay Gebremedhin and Alemseged Aregay Gebru and Johanna M Geleijnse and Bradford D Gessner and Peter W Gething and Tsegaye Tewelde Ghiwot and Maurice Giroud and Melkamu Dedefo Gishu and Elizabeth Glaser and Shifalika Goenka and Amador Goodridge and Sameer Vali Gopalani and Atsushi Goto and Harish Chander Gugnani and Mark D C Guimaraes and Rahul Gupta and Rajeev Gupta and Vipin Gupta and Juanita Haagsma and Nima Hafezi-Nejad and Holly Hagan and Gessessew Bugssa Hailu and Randah Ribhi Hamadeh and Samer Hamidi and Mouhanad Hammami and Graeme J Hankey and Yuantao Hao and Hilda L Harb and Sivadasanpillai Harikrishnan and Josep Maria Haro and Kimani M Harun and Rasmus Havmoeller and Mohammad T Hedayati and Ileana Beatriz Heredia-Pi and Hans W Hoek and Masako Horino and Nobuyuki Horita and H Dean Hosgood and Damian G Hoy and Mohamed Hsairi and Guoqing Hu and Hsiang Huang and John J Huang and Kim Moesgaard Iburg and Bulat T Idrisov and Kaire Innos and Veena J Iyer and Kathryn H Jacobsen and Nader Jahanmehr and Mihajlo B Jakovljevic and Mehdi Javanbakht and Achala Upendra Jayatilleke and Panniyammakal Jeemon and Vivekanand Jha and Guohong Jiang and Ying Jiang and Tariku Jibat and Jost B Jonas and Zubair Kabir and Ritul Kamal and Haidong Kan and André Karch and Corine Kakizi Karema and Dimitris Karletsos and Amir Kasaeian and Anil Kaul and Norito Kawakami and Jeanne Françoise Kayibanda and Peter Njenga Keiyoro and Andrew Haddon Kemp and Andre Pascal Kengne and Chandrasekharan Nair Kesavachandran and Yousef Saleh Khader and Ibrahim Khalil and Abdur Rahman Khan and Ejaz Ahmad Khan and Young-Ho Khang and Jagdish Khubchandani and Yun Jin Kim and Yohannes Kinfu and Miia Kivipelto and Yoshihiro Kokubo and Soewarta Kosen and Parvaiz A Koul and Ai Koyanagi and Barthelemy Kuate Defo and Burcu Kucuk Bicer and Veena S Kulkarni and G Anil Kumar and Dharmesh Kumar Lal and Hilton Lam and Jennifer O Lam and Sinead M Langan and Van C Lansingh and Anders Larsson and James Leigh and Ricky Leung and Yongmei Li and Stephen S Lim and Steven E Lipshultz and Shiwei Liu and Belinda K Lloyd and Giancarlo Logroscino and Paulo A Lotufo and Raimundas Lunevicius and Hassan Magdy Abd El Razek and Mahdi Mahdavi and P A Mahesh and Marek Majdan and Azeem Majeed and Carla Makhlouf and Reza Malekzadeh and Chabila C Mapoma and Wagner Marcenes and Jose Martinez-Raga and Melvin Barrientos Marzan and Felix Masiye and Amanda J Mason-Jones and Bongani M Mayosi and Martin McKee and Peter A Meaney and Man Mohan Mehndiratta and Alemayehu B Mekonnen and Yohannes Adama Melaku and Peter Memiah and Ziad A Memish and Walter Mendoza and Atte Meretoja and Tuomo J Meretoja and Francis Apolinary Mhimbira and Ted R Miller and Joseph Mikesell and Mojde Mirarefin and Karzan Abdulmuhsin Mohammad and Shafiu Mohammed and Ali H Mokdad and Lorenzo Monasta and Maziar Moradi-Lakeh and Rintaro Mori and Ulrich O Mueller and Brighton Murimira and Gudlavalleti Venkata Satyanarayana Murthy and Aliya Naheed and Luigi Naldi and Vinay Nangia and Denis Nash and Haseeb Nawaz and Chakib Nejjari and Frida Namnyak Ngalesoni and Jean de Dieu Ngirabega and Quyen Le Nguyen and Muhammad Imran Nisar and Ole F Norheim and Rosana E Norman and Luke Nyakarahuka and Felix Akpojene Ogbo and In-Hwan Oh and Foluke Adetola Ojelabi and Bolajoko Olubukunola Olusanya and Jacob Olusegun Olusanya and John Nelson Opio and Eyal Oren and Erika Ota and Hye-Youn Park and Jae-Hyun Park and Snehal T Patil and Scott B Patten and Vinod K Paul and Katherine Pearson and Emmanuel Kwame Peprah and David M Pereira and Norberto Perico and Konrad Pesudovs and Max Petzold and Michael Robert Phillips and Julian David Pillay and Dietrich Plass and Suzanne Polinder and Farshad Pourmalek and David M Prokop and Mostafa Qorbani and Anwar Rafay and Kazem Rahimi and Vafa Rahimi-Movaghar and Mahfuzar Rahman and Mohammad Hifz Ur Rahman and Sajjad Ur Rahman and Rajesh Kumar Rai and Sasa Rajsic and Usha Ram and Saleem M Rana and Paturi Vishnupriya Rao and Giuseppe Remuzzi and David Rojas-Rueda and Luca Ronfani and Gholamreza Roshandel and Ambuj Roy and George Mugambage Ruhago and Mohammad Yahya Saeedi and Rajesh Sagar and Muhammad Muhammad Saleh and Juan R Sanabria and Itamar S Santos and Rodrigo Sarmiento-Suarez and Benn Sartorius and Monika Sawhney and Aletta E Schutte and David C Schwebel and Soraya Seedat and Sadaf G Sepanlou and Edson E Servan-Mori and Masood Ali Shaikh and Rajesh Sharma and Jun She and Sara Sheikhbahaei and Jiabin Shen and Kenji Shibuya and Hwashin Hyun Shin and Inga Dora Sigfusdottir and Naris Silpakit and Diego Augusto Santos Silva and Dayane Gabriele Alves Silveira and Edgar P Simard and Shireen Sindi and Jasvinder A Singh and Om Prakash Singh and Prashant Kumar Singh and Vegard Skirbekk and Karen Sliwa and Samir Soneji and Reed J D Sorensen and Joan B Soriano and David O Soti and Chandrashekhar T Sreeramareddy and Vasiliki Stathopoulou and Nicholas Steel and Bruno F Sunguya and Soumya Swaminathan and Bryan L Sykes and Rafael Tabarés-Seisdedos and Roberto Tchio Talongwa and Mohammad Tavakkoli and Bineyam Taye and Bemnet Amare Tedla and Tesfaye Tekle and Girma Temam Shifa and Awoke Misganaw Temesgen and Abdullah Sulieman Terkawi and Fisaha Haile Tesfay and Gizachew Assefa Tessema and Kiran Thapa and Alan J Thomson and Andrew L Thorne-Lyman and Ruoyan Tobe-Gai and Roman Topor-Madry and Jeffrey Allen Towbin and Bach Xuan Tran and Zacharie Tsala Dimbuene and Nikolaos Tsilimparis and Abera Kenay Tura and Kingsley Nnanna Ukwaja and Chigozie Jesse Uneke and Olalekan A Uthman and N Venketasubramanian and Sergey K Vladimirov and Vasiliy Victorovich Vlassov and Stein Emil Vollset and Linhong Wang and Elisabete Weiderpass and Robert G Weintraub and Andrea Werdecker and Ronny Westerman and Tissa Wijeratne and James D Wilkinson and Charles Shey Wiysonge and Charles D A Wolfe and Sungho Won and John Q Wong and Gelin Xu and Ajit Kumar Yadav and Bereket Yakob and Ayalnesh Zemene Yalew and Yuichiro Yano and Mehdi Yaseri and Henock Gebremedhin Yebyo and Paul Yip and Naohiro Yonemoto and Seok-Jun Yoon and Mustafa Z Younis and Chuanhua Yu and Shicheng Yu and Zoubida Zaidi and Maysaa El Sayed Zaki and Hajo Zeeb and Hao Zhang and Yong Zhao and Sanjay Zodpey and Leo Zoeckler and Liesl Joanna Zuhlke and Alan D Lopez and Christopher J L Murray",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR4"
"Hybrid OpenMP/MPI programs for solving the time-dependent Gross–Pitaevskii equation in a fully anisotropic trap","We present hybrid OpenMP/MPI (Open Multi-Processing/Message Passing Interface) parallelized versions of earlier published C programs (Vudragović et al. 2012) for calculating both stationary and non-stationary solutions of the time-dependent Gross–Pitaevskii (GP) equation in three spatial dimensions. The GP equation describes the properties of dilute Bose–Einstein condensates at ultra-cold temperatures. Hybrid versions of programs use the same algorithms as the C ones, involving real- and imaginary-time propagation based on a split-step Crank–Nicolson method, but consider only a fully-anisotropic three-dimensional GP equation, where algorithmic complexity for large grid sizes necessitates parallelization in order to reduce execution time and/or memory requirements per node. Since distributed memory approach is required to address the latter, we combine MPI programming paradigm with existing OpenMP codes, thus creating fully flexible parallelism within a combined distributed/shared memory model, suitable for different modern computer architectures. The two presented C/OpenMP/MPI programs for real- and imaginary-time propagation are optimized and accompanied by a customizable makefile. We present typical scalability results for the provided OpenMP/MPI codes and demonstrate almost linear speedup until inter-process communication time starts to dominate over calculation time per iteration. Such a scalability study is necessary for large grid sizes in order to determine optimal number of MPI nodes and OpenMP threads per node.
New version program summary
Program title: GP-SCL-HYB package, consisting of: (i) imagtime3d-hyb, (ii) realtime3d-hyb. Catalogue identifier: AEDU_v3_0 Program Summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDU_v3_0.html Program obtainable from: CPC Program Library, Queen’s University of Belfast, N. Ireland. Licensing provisions: Apache License 2.0 No. of lines in distributed program, including test data, etc.: 26397. No. of bytes in distributed program, including test data, etc.: 161195. Distribution format: tar.gz. Programming language: C/OpenMP/MPI. Computer: Any modern computer with C language, OpenMP- and MPI-capable compiler installed. Operating system: Linux, Unix, Mac OS X, Windows. RAM: Total memory required to run programs with the supplied input files, distributed over the used MPI nodes: (i) 310 MB, (ii) 400 MB. Larger grid sizes require more memory, which scales with Nx*Ny*Nz. Number of processors used: No limit, from one to all available CPU cores can used on all MPI nodes. Number of nodes used: No limit on the number of MPI nodes that can be used. Depending on the grid size of the physical problem and communication overheads, optimal number of MPI nodes and threads per node can be determined by a scalability study for a given hardware platform. Classification: 2.9, 4.3, 4.12. Catalogue identifier of previous version: AEDU_v2_0. Journal reference of previous version: Comput. Phys. Commun. 183 (2012) 2021. Does the new version supersede the previous version?: No. Nature of problem: These programs are designed to solve the time-dependent Gross–Pitaevskii (GP) nonlinear partial differential equation in three spatial dimensions in a fully anisotropic trap using a hybrid OpenMP/MPI parallelization approach. The GP equation describes the properties of a dilute trapped Bose–Einstein condensate. Solution method: The time-dependent GP equation is solved by the split-step Crank–Nicolson method using discretization in space and time. The discretized equation is then solved by propagation, in either imaginary or real time, over small time steps. The method yields solutions of stationary and/or non-stationary problems. Reasons for the new version: Previous C [1] and Fortran [2] programs are widely used within the ultracold atoms and nonlinear optics communities, as well as in various other fields [3]. This new version represents extension of the two previously OpenMP-parallelized programs (imagtime3d-th and realtime3d-th) for propagation in imaginary and real time in three spatial dimensions to a hybrid, fully distributed OpenMP/MPI programs (imagtime3d-hyb and realtime3d-hyb). Hybrid extensions of previous OpenMP codes enable interested researchers to numerically study Bose–Einstein condensates in much greater detail (i.e., with much finer resolution) than with OpenMP codes. In OpenMP (threaded) versions of programs, numbers of discretization points in X, Y, and Z directions are bound by the total amount of available memory on a single computing node where the code is being executed. New, hybrid versions of programs are not limited in this way, as large numbers of grid points in each spatial direction can be evenly distributed among the nodes of a cluster, effectively distributing required memory over many MPI nodes. This is the first reason for development of hybrid versions of 3d codes. The second reason for new versions is speedup in the execution of numerical simulations that can be gained by using multiple computing nodes with OpenMP/MPI codes. Summary of revisions: Two C/OpenMP programs in three spatial dimensions from previous version [1] of the codes (imagtime3d-th and realtime3d-th) are transformed and rewritten into a hybrid OpenMP/MPI programs and named imagtime3d-hyb and realtime3d-hyb. The overall structure of two programs is identical. The directory structure of the GP-SCL-HYB package is extended compared to the previous version and now contains a folder scripts, where examples of scripts that can be used to run the programs on a typical MPI cluster are given. The corresponding readme.txt file contains more details. We have also included a makefile with tested and verified settings for most popular MPI compliers, including OpenMPI (Open Message Passing Interface) [4] and MPICH (Message Passing Interface Chameleon) [5]. Transformation from pure OpenMP to a hybrid OpenMP/MPI approach has required that the array containing condensate wavefunction is distributed among MPI nodes of a computer cluster. Several data distribution models have been considered for this purpose, including block distribution and block cyclic distribution of data in a 2d matrix. Finally, we decided to distribute the wavefunction values across different nodes so that each node contains only one slice of the X-dimension data, while containing the complete corresponding Y- and Z-dimension data, as illustrated in Fig. 1. This allows central functions of our numerical algorithm, calcluy, calcuz, and calcnu to be executed purely in parallel on different MPI nodes of a cluster, without any overhead or communication, as nodes contain all the information for Y- and Z-dimension data in the given X-sub-domain. However, the problem arises when functions calclux, calcrms, and calcmuen need to be executed, as they also operate on the whole X-dimension data. Thus, the need for additional communication arises during the execution of the function calcrms, while in the case of functions calclux and calcmuen also the transposition of data between X- and Y-dimensions is necessary, while data in Z dimension have to stay contiguous. Transposition provides nodes with all the necessary X-dimension data to execute functions calclux and calcmuen. However, this needs to be done in each iteration of numerical algorithm, thus necessarily increasing communication overhead of the simulation. Transposition algorithms that were considered where the ones that account for greatest common divisor (GCD) between number of nodes in columns (designated by N) and rows (designated by M) of a cluster configured as 2d mash of nodes [6]. Two of such algorithms have been tested and tried for implementation: the case when GCD=1 and the case when GCD>1. The trivial situation N=M=1 is already covered by the previous, purely OpenMP programs, and therefore, without any loss of generality, we have considered only configurations with number of nodes in X-dimension satisfying N>1. Only the former algorithm (GCD=1) was found to be sound in case where data matrix is not a 2d, but a 3d structure. Latter case was found to be too demanding implementation-wise, since MPI functions and data-types are bound to certain limitations. Therefore, the algorithm with M=1 nodes in Y-dimension was implemented, as depicted by the wavefunction data structure in Fig. 1. Fig. 1BEC wavefunction data structure in the employed algorithm. Data are sliced so that the complete Y- and Z-dimension data reside on a single node for a given range of data in X-dimension, while data in X-dimension are distributed over N=gsize nodes. Figures show data transposition and MPI indexed datatype creation parameters for the case of: (a) sending side and (b) receiving side. Implementation of the algorithm relies on a sliced distribution of data among the nodes, as explained in Fig. 2. This successfully solves the problem of large RAM consumption of 3d codes, which arises even for moderate grid sizes. However, it does not solve the question of data transposition between the nodes. In order to implement the most effective (GCD=1) transposition algorithm according to Ref. [6], we had to carry out block distribution of data within one data slice contained on a single node. This block distribution of data was done implicitly, i.e. data on one node have been put in a single 1d array (psi) of contiguous memory, in which Z-dimension has stride 1, Y-dimension has stride Nz, and X-dimension has stride Ny*Nz. This is different from previous implementation of the programs, where the wavefunction was represented by an explicit 3d array. This change was also introduced in order to more easily form user MPI datatypes, which allow for implicit block distribution of data, and represent 3d blocks of data within 1d data array. These blocks are then swapped between nodes, effectively performing the transposition in X–Y and Y–X directions. Together with transposition of blocks between the nodes, the block data also have to be redistributed. To illustrate how this works, let us consider example shown in Fig. 1(a), where one data block has size (Nx/gisze)*(Ny/gsize)*Nz. It represents one 3d data block, swapped between two nodes of a cluster (through one non-blocking MPI_Isend and one MPI_Ireceive operation), containing (Nx/gsize)*(Ny/gsize) 1d rods of contiguous Nz data. These rods themselves need to be transposed within the transposed block as well. This means that two levels of transpositions need to be performed. At a single block level, rods have to be transposed (as indicated in upper left corner of Fig. 1(a) for sending index type and in Fig. 1(b) for receiving index type). Second level is transposition of blocks between different nodes, which is depicted by blue arrows connecting different blocks in Fig. 1. The above described transposition is applied whenever needed in the functions calclux and calcmuen, which require calculations to be done on the whole range of data in X-dimension. When performing renormalization of the wavefunction or calculation of its norm, root-mean-square radius, chemical potential, and energy, collective operations MPI_Gather and MPI_Bcast are also used. Fig. 3, Fig. 4 show the scalability results obtained for hybrid versions of programs for small and large grid sizes as a function of number of MPI nodes used. The baseline for calculation of speedups in the execution time for small grid sizes is previous, purely OpenMP programs, while for large grid sizes, which cannot fit onto a single node, the baseline is hybrid programs with minimal configuration runs on 8 nodes. The figures also show efficacies, defined as percentages of measured speedups compared to the ideal ones. We see that an excellent scalability (larger than 80% compared to the ideal one) can be obtained for up to 32 nodes. The tests have been performed on a cluster with nodes containing 2×8-core Sandy Bridge Xeon 2.6 GHz processors with 32 GB of RAM and Infiniband QDR (Quad Data Rate, 40 Gbps) interconnect. We stress that the scalability depends greatly on the ratio between the calculation and communication time per iteration, and has to be studied for a particular type of processors and interconnect technology. Additional comments: This package consists of 2 programs, see Program title above. Both are hybrid, threaded and distributed (OpenMP/MPI parallelized). For the particular purpose of each program, see descriptions below. Fig. 2Creation of a user-defined MPI datatype indextype with the function MPI_Type_indexed. Here, count represents the number of blocks, blocklengths array contains lengths of each block, and displacements array contains the displacement of each block from the beginning of the corresponding data structure. For example, if an array of double precision numbers (designated as buffer in the figure) is sent by MPI_Send with the datatype set to indextype, it is interpreted as a block-distributed data structure, as specified when indextype was created.Fig. 3Speedup in the execution time and efficacy curves of imagtime3d-hyb and realtime3d-hyb programs as a function of the number of MPI nodes used for small grid sizes. The results are obtained on a cluster with nodes containing 2×8-core Sandy Bridge Xeon 2.6 GHz processors with 32 GB of RAM and Infiniband QDR interconnect: (a) speedup of imagtime3d-hyb on a 240×200×160 grid; (b) efficacy of imagtime3d-hyb on a 240×200×160 grid; (c) speedup of realtime3d-hyb on a 200×160×120 grid; (d) efficacy of realtime3d-hyb on a 200×160×120 grid. Shaded areas in graphs (b) and (d) represent high-efficacy regions, where speedup is at least 80% of the ideal one. Running time: All running times given in descriptions below refer to programs compiled with OpenMPI/GCC compiler and executed on 8–32 nodes with 2×8-core Sandy Bridge Xeon 2.6 GHz processors with 32 GB of RAM and Infiniband QDR interconnect. With the supplied input files for small grid sizes, running wallclock times of several minutes are required on 8–10 MPI nodes. Special features: (1) Since the condensate wavefunction data are distributed among the MPI nodes, when writing wavefunction output files each MPI process saves its data into a separate file, to avoid I/O issues. Concatenating the corresponding files from all MPI processes will create the complete wavefunction file. (2) Due to a known bug in OpenMPI up to version 1.8.4, allocation of memory for indexed datatype on a single node for large grids (such as 800×640×480) may fail. The fix for this bug is already in 3c489ea branch and is fixed in OpenMPI as of version 1.8.5. Fig. 4Speedup in the execution time and efficacy curves of imagtime3d-hyb and realtime3d-hyb programs as a function of the number of MPI nodes used for large grid sizes. The results are obtained on a cluster with nodes containing 2×8-core Sandy Bridge Xeon 2.6 GHz processors with 32 GB of RAM and Infiniband QDR interconnect: (a) speedup of imagtime3d-hyb on a 1920×1600×1280 grid; (b) efficacy of imagtime3d-hyb on a 1920×1600×1280 grid; (c) speedup of realtime3d-hyb on a 1600×1280×960 grid; (d) efficacy of realtime3d-hyb on a 1600×1280×960 grid. Shaded areas in graphs (b) and (d) represent high-efficacy regions, where speedup is at least 80% of the ideal one. Program summary (i) Program title: imagtime3d-hyb. Title of electronic files: imagtime3d-hyb.c, imagtime3d-hyb.h. Computer: Any modern computer with C language, OpenMP- and MPI-capable compiler installed. RAM memory requirements: 300 MBytes of RAM for a small grid size 240×200×160, and scales with Nx*Ny*Nz. This is total amount of memory needed, and is distributed over MPI nodes used for execution. Programming language used: C/OpenMP/MPI. Typical running time: Few minutes with the supplied input files for a small grid size 240×200×160 on 8 nodes. Up to one hour for a large grid size 1920×1600×1280 on 32 nodes (1000 iterations). Nature of physical problem: This program is designed to solve the time-dependent GP nonlinear partial differential equation in three space dimensions with an anisotropic trap. The GP equation describes the properties of a dilute trapped Bose–Einstein condensate. Method of solution: The time-dependent GP equation is solved by the split-step Crank–Nicolson method by discretizing in space and time. The discretized equation is then solved by propagation in imaginary time over small time steps. The method yields solutions of stationary problems. Program summary (ii) Program title: realtime3d-hyb. Title of electronic files: realtime3d-hyb.c, realtime3d-hyb.h. Computer: Any modern computer with C language, OpenMP- and MPI-capable compiler installed. RAM memory requirements: 410 MBytes of RAM for a small grid size 200×160×120, and scales with Nx*Ny*Nz. This is total amount of memory needed, and is distributed over MPI nodes used for execution. Programming language used: C/OpenMP/MPI. Typical running time: 10–15 min with the supplied input files for a small grid size 200×160×120 on 10 nodes. Up to one hour for a large grid size 1600×1280×960 on 32 nodes (1000 iterations). Nature of physical problem: This program is designed to solve the time-dependent GP nonlinear partial differential equation in three space dimensions with an anisotropic trap. The GP equation describes the properties of a dilute trapped Bose–Einstein condensate. Method of solution: The time-dependent GP equation is solved by the split-step Crank–Nicolson method by discretizing in space and time. The discretized equation is then solved by propagation in real time over small time steps. The method yields solutions of stationary and non-stationary problems. Acknowledgments B.S., V.S., A.B., and A.B. acknowledge support by the Ministry of Education, Science, and Technological Development of the Republic of Serbia under projects ON171017, III43007, ON171009, ON174027 and IBEC, and by DAAD - German Academic and Exchange Service under project IBEC. P.M. acknowledges support by the Science and Engineering Research Board, Department of Science and Technology, Government of India under project No. EMR/2014/000644. S.K.A. acknowledges support by the CNPq of Brazil under project 303280/2014-0, and by the FAPESP of Brazil under project 2012/00451-0. Numerical simulations were run on the PARADOX supercomputing facility at the Scientific Computing Laboratory of the Institute of Physics Belgrade, supported in part by the Ministry of Education, Science, and Technological Development of the Republic of Serbia under project ON171017. References[1]D. Vudragović, I. Vidanović, A. Balaž, P. Muruganandam, S. K. Adhikari, C programs for solving the time-dependent Gross–Pitaevskii equation in a fully anisotropic trap, Comput. Phys. Commun. 183 (2012) 2021.[2]P. Muruganandam and S. K. Adhikari, Fortran programs for the time-dependent Gross–Pitaevskii equation in a fully anisotropic trap, Comput. Phys. Commun. 180 (2009) 1888.[3]R. K. Kumar and P. Muruganandam, J. Phys. B: At. Mol. Opt. Phys. 45 (2012) 215301;L. E. Young-S. and S. K. Adhikari, Phys. Rev. A 86 (2012) 063611;S. K. Adhikari, J. Phys. B: At. Mol. Opt. Phys. 45 (2012) 235303;I. Vidanović, N. J. van Druten, and M. Haque, New J. Phys. 15 (2013) 035008;S. Balasubramanian, R. Ramaswamy, and A. I. Nicolin, Rom. Rep. Phys. 65 (2013) 820;L. E. Young-S. and S. K. Adhikari, Phys. Rev. A 87 (2013) 013618;H. Al-Jibbouri, I. Vidanovic, A. Balaz, and A. Pelster, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 065303;X. Antoine, W. Bao, and C. Besse, Comput. Phys. Commun. 184 (2013) 2621;B. Nikolić, A. Balaž, and A. Pelster, Phys. Rev. A 88 (2013) 013624;H. Al-Jibbouri and A. Pelster, Phys. Rev. A 88 (2013) 033621;S. K. Adhikari, Phys. Rev. A 88 (2013) 043603;J. B. Sudharsan, R. Radha, and P. Muruganandam, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 155302;R. R. Sakhel, A. R. Sakhel, and H. B. Ghassib, J. Low Temp. Phys. 173 (2013) 177;E. J. M. Madarassy and V. T. Toth, Comput. Phys. Commun. 184 (2013) 1339;R. K. Kumar, P. Muruganandam, and B. A. Malomed, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 175302;W. Bao, Q. Tang, and Z. Xu, J. Comput. Phys. 235 (2013) 423;A. I. Nicolin, Proc. Rom. Acad. Ser. A-Math. Phys. 14 (2013) 35;R. M. Caplan, Comput. Phys. Commun. 184 (2013) 1250;S. K. Adhikari, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 115301;Ž. Marojević, E. Göklü, and C. Lämmerzahl, Comput. Phys. Commun. 184 (2013) 1920;X. Antoine and R. Duboscq, Comput. Phys. Commun. 185 (2014) 2969;S. K. Adhikari and L. E. Young-S, J. Phys. B: At. Mol. Opt. Phys. 47 (2014) 015302;K. Manikandan, P. Muruganandam, M. Senthilvelan, and M. Lakshmanan, Phys. Rev. E 90 (2014) 062905;S. K. Adhikari, Phys. Rev. A 90 (2014) 055601;A. Balaž, R. Paun, A. I. Nicolin, S. Balasubramanian, and R. Ramaswamy, Phys. Rev. A 89 (2014) 023609;S. K. Adhikari, Phys. Rev. A 89 (2014) 013630;J. Luo, Commun. Nonlinear Sci. Numer. Simul. 19 (2014) 3591;S. K. Adhikari, Phys. Rev. A 89 (2014) 043609;K.-T. Xi, J. Li, and D.-N. Shi, Physica B 436 (2014) 149;M. C. Raportaru, J. Jovanovski, B. Jakimovski, D. Jakimovski, and A. Mishev, Rom. J. Phys. 59 (2014) 677;S. Gautam and S. K. Adhikari, Phys. Rev. A 90 (2014) 043619;A. I. Nicolin, A. Balaž, J. B. Sudharsan, and R. Radha, Rom. J. Phys. 59 (2014) 204;K. Sakkaravarthi, T. Kanna, M. Vijayajayanthi, and M. Lakshmanan, Phys. Rev. E 90 (2014) 052912;S. K. Adhikari, J. Phys. B: At. Mol. Opt. Phys. 47 (2014) 225304;R. K. Kumar and P. Muruganandam, Numerical studies on vortices in rotating dipolar Bose–Einstein condensates, Proceedings of the 22nd International Laser Physics Workshop, J. Phys. Conf. Ser. 497 (2014) 012036;A. I. Nicolin and I. Rata, Density waves in dipolar Bose–Einstein condensates by means of symbolic computations, High-Performance Computing Infrastructure for South East Europe’s Research Communities: Results of the HP-SEE User Forum 2012, in Springer Series: Modeling and Optimization in Science and Technologies 2 (2014) 15;S. K. Adhikari, Phys. Rev. A 89 (2014) 043615;R. K. Kumar and P. Muruganandam, Eur. Phys. J. D 68 (2014) 289;J. B. Sudharsan, R. Radha, H. Fabrelli, A. Gammal, and B. A. Malomed, Phys. Rev. A 92 (2015) 053601;S. K. Adhikari, J. Phys. B: At. Mol. Opt. Phys. 48 (2015) 165303;F. I. Moxley III, T. Byrnes, B. Ma, Y. Yan, and W. Dai, J. Comput. Phys. 282 (2015) 303;S. K. Adhikari, Phys. Rev. E 92 (2015) 042926;R. R. Sakhel, A. R. Sakhel, and H. B. Ghassib, Physica B 478 (2015) 68;S. Gautam and S. K. Adhikari, Phys. Rev. A 92 (2015) 023616;D. Novoa, D. Tommasini, and J. A. Nóvoa-López, Phys. Rev. E 91 (2015) 012904;S. Gautam and S. K. Adhikari, Laser Phys. Lett. 12 (2015) 045501;K.-T. Xi, J. Li, and D.-N. Shi, Physica B 459 (2015) 6;R. K. Kumar, L. E. Young-S., D. Vudragović, A. Balaž, P. Muruganandam, and S. K. Adhikari, Comput. Phys. Commun. 195 (2015) 117;S. Gautam and S. K. Adhikari, Phys. Rev. A 91 (2015) 013624;A. I. Nicolin, M. C. Raportaru, and A. Balaž, Rom. Rep. Phys. 67 (2015) 143;S. Gautam and S. K. Adhikari, Phys. Rev. A 91 (2015) 063617;E. J. M. Madarassy and V. T. Toth, Phys. Rev. D 91 (2015) 044041.[4]Open Message Passing Interface (OpenMPI), http://www.open-mpi.org/ (2015).[5]Message Passing Interface Chameleon (MPICH), https://www.mpich.org/ (2015).[6]J. Choi, J. J. Dongarra, D. W. Walker, Parallel matrix transpose algorithms on distributed memory concurrent computers, Parallel Comput. 21 (1995) 1387.","Bogdan Satarić and Vladimir Slavnić and Aleksandar Belić and Antun Balaž and Paulsamy Muruganandam and Sadhan K. Adhikari",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical investigations of using carbon foam/PCM/Nano carbon tubes composites in thermal management of electronic equipment","A numerical investigation of predicting thermal characteristics of electronic equipment using carbon foam matrix saturated with phase change material (PCM) and Nano carbon tubes as thermal management modules is presented. To study the effect of insertion of RT65 and Nano carbon tubes in carbon foam matrices of different porosities, three different modules; namely Pure CF-20, CF20+RT65, and CF-20+RT65/Nano carbon modules are numerically tested at different values of carbon foam porosities. Mathematical model is obtained using volume averaging technique based on single-domain energy equation and a control volume based numerical scheme. Interfacial effects influencing heat transfer process at enclosure wall, module surface and different interfacial surfaces within the composite have been addressed. Governing equations have been solved using a CFD code (Thétis, http://thetis.enscbp.fr). Mathematical model is validated by comparing its prediction with previous experimental measurements for pure CF-20 foam and CF-20+RT65 composite modules. The model is used to predict thermal characteristics of CF-20+RT65/Nano carbon tubes composite as a thermal management modules. Results reveal that insertion of RT65/MWCNTs in CF-20 leads to a 11.5% reduction in the module surface temperature for carbon foam porosities less than 75%. The reduction decrease to 7.8% for a porosity of 88%. Numerical results of transient and steady state temperature histories at different depths within the module are compared with previous experimental data and fair agreement is obtained.","W.G. Alshaer and S.A. Nada and M.A. Rady and Cedric Le Bot and Elena Palomo Del Barrio",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A dated molecular phylogeny of manta and devil rays (Mobulidae) based on mitogenome and nuclear sequences","Manta and devil rays are an iconic group of globally distributed pelagic filter feeders, yet their evolutionary history remains enigmatic. We employed next generation sequencing of mitogenomes for nine of the 11 recognized species and two outgroups; as well as additional Sanger sequencing of two mitochondrial and two nuclear genes in an extended taxon sampling set. Analysis of the mitogenome coding regions in a Maximum Likelihood and Bayesian framework provided a well-resolved phylogeny. The deepest divergences distinguished three clades with high support, one containing Manta birostris, Manta alfredi, Mobula tarapacana, Mobula japanica and Mobula mobular; one containing Mobula kuhlii, Mobula eregoodootenkee and Mobula thurstoni; and one containing Mobula munkiana, Mobula hypostoma and Mobula rochebrunei. Mobula remains paraphyletic with the inclusion of Manta, a result that is in agreement with previous studies based on molecular and morphological data. A fossil-calibrated Bayesian random local clock analysis suggests that mobulids diverged from Rhinoptera around 30 Mya. Subsequent divergences are characterized by long internodes followed by short bursts of speciation extending from an initial episode of divergence in the Early and Middle Miocene (19–17 Mya) to a second episode during the Pliocene and Pleistocene (3.6 Mya – recent). Estimates of divergence dates overlap significantly with periods of global warming, during which upwelling intensity – and related high primary productivity in upwelling regions – decreased markedly. These periods are hypothesized to have led to fragmentation and isolation of feeding regions leading to possible regional extinctions, as well as the promotion of allopatric speciation. The closely shared evolutionary history of mobulids in combination with ongoing threats from fisheries and climate change effects on upwelling and food supply, reinforces the case for greater protection of this charismatic family of pelagic filter feeders.","Marloes Poortvliet and Jeanine L. Olsen and Donald A. Croll and Giacomo Bernardi and Kelly Newton and Spyros Kollias and John O’Sullivan and Daniel Fernando and Guy Stevens and Felipe Galván Magaña and Bernard Seret and Sabine Wintner and Galice Hoarau",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Interspecific relationships and the evolution of sexual dimorphism in pygmy sunfishes (Centrarchidae: Elassoma)","The genus Elassoma represents a small but unique component of the aquatic biodiversity hotspot in southeastern North America. We present the first phylogeny of the seven described species, corroborated by sequence data from mitochondrial and nuclear protein coding genes. This analysis reveals a Coastal Plain clade sister to the geographically isolated, and federally protected, Elassoma alabamae. The Coastal Plain clade contains the widespread E. zonatum, which is sister to a clade primarily restricted to lowland Neogene subprovinces. We analyzed morphometric data in a phylogenetic context to illustrate the evolution of sexual shape dimorphism within the genus. Sixteen univariate and three multivariate traits were tested for significant sexual dimorphism for each species, and relative transformation rates were inferred from the time tree. A simple index of interspecific sexual dimorphism revealed greater disparity among sympatric species comparisons than among allopatric comparisons. Results implicate geology as a primary factor influencing ecological diversification, and sexual selection as a mechanism reinforcing reproductive isolation in areas of secondary contact. We discuss putative roles of geological history and sexual selection in the generation and maintenance of the aquatic biodiversity gradient in southeastern North America.","Michael Sandel and Fritz C. Rohde and Phillip M. Harris",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Exploring teachers' use of TPACK in design talk: The collaborative design of technology-rich early literacy activities","Research shows the benefits of collaborative design activity by teachers are that in their conversations (design talk) they develop technological pedagogical content knowledge (TPACK). While more and more teachers engage in collaborative design, little is known about how they use TPACK during design. The main question of this study was: “What is the nature of design talk of a group of teachers during the design of technology-rich early literacy activities?” Using a holistic case study on design talk, the analysis focused on the topics that were under discussion and how these topics were discussed. Three phases of coding were applied: (a) how design represents any of the seven domains of TPACK knowledge (Pedagogical, Content, Technological, Technological Pedagogical, Technological Content, Pedagogical Content or Technological Pedagogical Content Knowledge); (b), how design talk represented three aspects of reasoning (external priorities, practical concerns and existing orientations); and (c), and what levels of inquiry are reached (no-depth; sharing ideas; analyze; and plan). Findings indicate that design talk reflects moments in which teachers reach deeper levels of inquiry. Findings also indicate that TPACK was mostly linked to expressing practical concerns. However when engaging in deeper inquiry, teachers existing orientations featured more prominently in the conversations. External priorities hardly seemed to play any role in design talk. Also, when addressing TPACK or PCK, design talk mostly reflects practical concerns. Pedagogy was addressed not as a single knowledge domain, rather in conjunction with the other two domains. Practical implications are discussed regarding how to support teachers during collaborative design.","Ferry Boschman and Susan McKenney and Joke Voogt",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mesolimbic dopamine release is linked to symptom severity in pathological gambling","Background
Brain dopamine neurons code rewarding environmental stimuli by releasing endogenous dopamine, a transmission signal that is important for reinforcement learning. Human reward-seeking gambling behavior, and especially pathological gambling, has been presumed to be modulated by brain dopamine.
Methods
Striatal dopamine release was studied with [11C]raclopride positron emission tomography (PET) during gambling with an ecologically valid slot machine gambling task. Twenty-four males with and without pathological gambling (DSM-IV) were scanned three times, and the effects of different gambling outcomes (high-reward and low-reward vs. control task) on dopamine release were evaluated.
Results
Striatal dopamine was released in both groups during high-reward but also low-reward tasks. The dopamine release during the low-reward task was located in the associative part of the caudate nucleus. During the high-reward task, the effect was also seen in the ventral striatum and the magnitude of dopamine release was associated with parallel gambling “high”. Furthermore, there was a positive correlation between dopamine release during the low-reward and the high-reward task. There was no general difference in the magnitude of dopamine release between pathological gamblers and controls. However, in pathological gamblers, dopamine release correlated positively with gambling symptom severity.
Conclusions
Striatal dopamine is released during gambling irrespective of gambling outcome suggesting that the mere expectation/prediction of reward is sufficient to induce dopaminergic changes. Although dopamine release during slot machine gambling is comparable between healthy controls and pathological gamblers, greater gambling symptom severity is associated with greater dopaminergic responses. Thus, as the dopamine reward deficiency theory predicts blunted mesolimbic dopamine responses to gambling in addicted individuals, our results question the validity of the reward deficiency hypothesis in pathological gambling.","Juho Joutsa and Jarkko Johansson and Solja Niemelä and Antti Ollikainen and Mika M. Hirvonen and Petteri Piepponen and Eveliina Arponen and Hannu Alho and Valerie Voon and Juha O. Rinne and Jarmo Hietala and Valtteri Kaasinen",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"What factors influence midwives' decision to perform or avoid episiotomies? A focus group study","Objective
to explore midwives' reasons for performing or avoiding episiotomies and motivation to change episiotomy practice in a large tertiary maternity hospital.
Design
using purposive sampling, three focus groups were conducted to achieve theme saturation. Open-ended questions elicited personal reasons for performing or avoiding episiotomy, information sources, and opinions about past and future practice trends. Sessions were audiotaped, and transcripts independently examined by three researchers who coded for themes. An iterative process was used to achieve consensus. Grounded theory was used to interpret data and to derive a theoretical framework for understanding the reasoning that influences episiotomy practice.
Setting
a high volume delivery unit in Singapore.
Participants
20 of 79 licensed midwives, aged 28–70, who performed independent deliveries at the delivery unit.
Findings
participants recognised maternal, fetal and other factors affecting their own decision to perform episiotomies. Patient request, better healing, midwife's reputation and job satisfaction were cited as main reasons to avoid episiotomy. Key sources informing practice were past training, delivery experience, anecdotal learning and lack of a protocol. There was no consensus on current trends in episiotomy practice. There was an absence of recognition of individual roles in reducing episiotomy rates. Clinicians were perceived as having both positive and negative influence.
Conclusions
midwives' reasons for performing episiotomies were attributed to midwifery training, fear of doing harm and perceived clinician expectation, and were not consistent with current international practice guidelines. Reasons for avoiding episiotomies were associated with patient-centeredness and job satisfaction. Midwives agreed on the need to reduce episiotomy rates.
Implications for practice
with reduction in episiotomy rates as a goal, a combination of guideline education, feedback, peer coaching and collaborative care with doctors may be needed to achieve desired outcomes. Views and experiences of midwives should also be incorporated into strategies to change episiotomy practice.","Lin Chieh Wu and Désirée Lie and Rahul Malhotra and John C. Allen and Julie S.L. Tay and Thiam Chye Tan and Truls Østbye",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9, CR9"
"Application of a theoretical framework for behavior change to hospital workers’ real-time explanations for noncompliance with hand hygiene guidelines","Background
Insufficient use of behavioral theory to understand health care workers’ (HCWs) hand hygiene compliance may result in suboptimal design of hand hygiene interventions and limit effectiveness. Previous studies examined HCWs’ intended, rather than directly observed, compliance and/or focused on just 1 behavioral model. This study examined HCWs’ explanations of noncompliance in “real time” (immediately after observation), using a behavioral theory framework, to inform future intervention design.
Methods
HCWs were directly observed and asked to explain episodes of noncompliance in “real-time.” Explanations were recorded, coded into 12 behavioral domains, using the Theory Domains Framework, and subdivided into themes.
Results
Over two-thirds of 207 recorded explanations were explained by 2 domains. These were “Memory/Attention/Decision Making” (87, 44%), subdivided into 3 themes (memory, loss of concentration, and distraction by interruptions), and “Knowledge” (55, 26%), with 2 themes relating to specific hand hygiene indications. No other domain accounted for more than 18 (9%) explanations.
Conclusion
An explanation of HCW’s “real-time” explanations for noncompliance identified “Memory/Attention/Decision Making” and “Knowledge” as the 2 behavioral domains commonly linked to noncompliance. This suggests that hand hygiene interventions should target both automatic associative learning processes and conscious decision making, in addition to ensuring good knowledge. A theoretical framework to investigate HCW’s “real-time” explanations of noncompliance provides a coherent way to design hand hygiene interventions.","Chris Fuller and Sarah Besser and Joanne Savage and John McAteer and Sheldon Stone and Susan Michie",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Phylogeny and population dynamics of respiratory syncytial virus (Rsv) A and B","Respiratory syncytial virus (RSV) is a major cause of lower respiratory tract infections in infants and young children. RSV is characterised by high variability, especially in the G glycoprotein, which may play a significant role in RSV pathogenicity by allowing immune evasion. To reconstruct the origin and phylodynamic history of RSV, we evaluated the genetic diversity and evolutionary dynamics of RSV A and RSV B isolated from children under 3 years old infected in Italy from 2006 to 2012. Phylogenetic analysis revealed that most of the RSV A sequences clustered with the NA1 genotype, and RSV B sequences were included in the Buenos Aires genotype. The mean evolutionary rates for RSV A and RSV B were estimated to be 2.1×10−3 substitutions (subs)/site/year and 3.03×10−3 subs/site/year, respectively. The time of most recent common ancestor for the tree root went back to the 1940s (95% highest posterior density—HPD: 1927–1951) for RSV A and the 1950s (95%HPD: 1951–1960) for RSV B. The RSV A Bayesian skyline plot (BSP) showed a decrease in transmission events ending in about 2005, when a sharp growth restored the original viral population size. RSV B BSP showed a similar trend. Site-specific selection analysis identified 10 codons under positive selection in RSV A sequences and only one site in RSV B sequences. Although RSV remains difficult to control due to its antigenic diversity, it is important to monitor changes in its coding sequences, to permit the identification of future epidemic strains and to implement vaccine and therapy strategies.","Marianna Martinelli and Elena Rosanna Frati and Alessandra Zappa and Erika Ebranati and Silvia Bianchi and Elena Pariani and Antonella Amendola and Gianguglielmo Zehender and Elisabetta Tanzi",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Incremental dataflow execution, resource efficiency and probabilistic guarantees with Fuzzy Boolean nets","Currently, there is a strong need for organizations to analyze and process ever-increasing volumes of data in order to answer to real-time processing demands. Such continuous and data-intensive processing is often achieved through the composition of complex data-intensive workflows (i.e., dataflows). Dataflow management systems typically enforce strict temporal synchronization across the various processing steps. Non-synchronous behavior often has to be explicitly programmed on an ad-hoc basis, which requires additional lines of code in programs and thus the possibility of errors. More so, in a large set of scenarios for continuous and incremental processing, the output of dataflow applications at each execution can suffer almost no difference when comparing to the previous execution, and therefore resources, energy and computational power are unknowingly wasted. To face such lack of efficiency, transparency, and generality, we introduce the notion of Quality-of-Data (QoD), which describes the level of changes required on a data store that cause the triggering of processing steps. This, so that the dataflow (re-)execution is reduced until its outcome would reach a significant and meaningful variation, which is inside a specified freshness limit. Based on the QoD notion, we propose a novel dataflow model, with framework (Fluxy), for orchestrating data-intensive processing steps, which communicate data via a NoSQL storage, and whose triggering semantics is driven by dynamic QoD constraints automatically defined for different datasets by means of Fuzzy Boolean Nets. These nets give probabilistic guarantees about the prediction of the cumulative error between consecutive dataflow executions. With Fluxy, we demonstrate how dataflows can be leveraged to respond to quality boundaries (that can be seen as SLAs) to deliver controlled and augmented performance, rationalization of resources, and task prioritization.","Sérgio Esteves and João Nuno Silva and João Paulo Carvalho and Luís Veiga",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Orbitofrontal reward sensitivity and impulsivity in adult attention deficit hyperactivity disorder","Impulsivity symptoms of adult attention deficit hyperactivity disorder (ADHD) such as increased risk taking have been linked with impaired reward processing. Previous studies have focused on reward anticipation or on rewarded executive functioning tasks and have described a striatal hyporesponsiveness and orbitofrontal alterations in adult and adolescent ADHD. Passive reward delivery and its link to behavioral impulsivity are less well understood. To study this crucial aspect of reward processing we used functional magnetic resonance imaging (fMRI) combined with electrodermal assessment in male and female adult ADHD patients (N=28) and matched healthy control participants (N=28) during delivery of monetary and non-monetary rewards. Further, two behavioral tasks assessed risky decision making (game of dice task) and delay discounting. Results indicated that both groups activated ventral and dorsal striatum and the medial orbitofrontal cortex (mOFC) in response to high-incentive (i.e. monetary) rewards. A similar, albeit less strong activation pattern was found for low-incentive (i.e. non-monetary) rewards. Group differences emerged when comparing high and low incentive rewards directly: activation in the mOFC coded for the motivational change in reward delivery in healthy controls, but not ADHD patients. Additionally, this dysfunctional mOFC activity in patients correlated with risky decision making and delay discounting and was paralleled by physiological arousal. Together, these results suggest that the mOFC codes reward value and type in healthy individuals whereas this function is deficient in ADHD. The brain–behavior correlations suggest that this deficit might be related to behavioral impulsivity. Reward value processing difficulties in ADHD should be considered when assessing reward anticipation and emotional learning in research and applied settings.","Gregor Wilbertz and Ludger Tebartz van Elst and Mauricio R. Delgado and Simon Maier and Bernd Feige and Alexandra Philipsen and Jens Blechert",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Self-organizing maps based on limit cycle attractors","Recent efforts to develop large-scale brain and neurocognitive architectures have paid relatively little attention to the use of self-organizing maps (SOMs). Part of the reason for this is that most conventional SOMs use a static encoding representation: each input pattern or sequence is effectively represented as a fixed point activation pattern in the map layer, something that is inconsistent with the rhythmic oscillatory activity observed in the brain. Here we develop and study an alternative encoding scheme that instead uses sparsely-coded limit cycles to represent external input patterns/sequences. We establish conditions under which learned limit cycle representations arise reliably and dominate the dynamics in a SOM. These limit cycles tend to be relatively unique for different inputs, robust to perturbations, and fairly insensitive to timing. In spite of the continually changing activity in the map layer when a limit cycle representation is used, map formation continues to occur reliably. In a two-SOM architecture where each SOM represents a different sensory modality, we also show that after learning, limit cycles in one SOM can correctly evoke corresponding limit cycles in the other, and thus there is the potential for multi-SOM systems using limit cycles to work effectively as hetero-associative memories. While the results presented here are only first steps, they establish the viability of SOM models based on limit cycle activity patterns, and suggest that such models merit further study.","Di-Wei Huang and Rodolphe J. Gentili and James A. Reggia",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Influence of the power density on a conversion ratio in Accelerated Driven System (ADS)","The U-233 Conversion Ratio (CR) calculation results for Yalina Thermal assembly are presented as a function of burnup, power density, irradiation history and U-233 concentration. The Yalina Thermal assembly is an ADS which makes configuration changes possible. These various configurations enable investigation and comparison of CR in a Thorium cycle (Th cycle) and a mixed Uranium–Thorium cycle (U–Th cycle) of burning. The power density has no influence on the U-233 concentration dependence of CR and on the burnup dependence of CR. The calculations were done with a MCNPX code.","Andrzej Wojciechowski",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The dark side of emotion: The addiction perspective","Emotions are “feeling” states and classic physiological emotive responses that are interpreted based on the history of the organism and the context. Motivation is a persistent state that leads to organized activity. Both are intervening variables and intimately related and have neural representations in the brain. The present thesis is that drugs of abuse elicit powerful emotions that can be interwoven conceptually into this framework. Such emotions range from pronounced euphoria to a devastating negative emotional state that in the extreme can create a break with homeostasis and thus an allostatic hedonic state that has been considered key to the etiology and maintenance of the pathophysiology of addiction. Drug addiction can be defined as a three-stage cycle—binge/intoxication, withdrawal/negative affect, and preoccupation/anticipation—that involves allostatic changes in the brain reward and stress systems. Two primary sources of reinforcement, positive and negative reinforcement, have been hypothesized to play a role in this allostatic process. The negative emotional state that drives negative reinforcement is hypothesized to derive from dysregulation of key neurochemical elements involved in the brain incentive salience and stress systems. Specific neurochemical elements in these structures include not only decreases in incentive salience system function in the ventral striatum (within-system opponent processes) but also recruitment of the brain stress systems mediated by corticotropin-releasing factor (CRF), dynorphin-κ opioid systems, and norepinephrine, vasopressin, hypocretin, and substance P in the extended amygdala (between-system opponent processes). Neuropeptide Y, a powerful anti-stress neurotransmitter, has a profile of action on compulsive-like responding for drugs similar to a CRF1 receptor antagonist. Other stress buffers include nociceptin and endocannabinoids, which may also work through interactions with the extended amygdala. The thesis argued here is that the brain has specific neurochemical neurocircuitry coded by the hedonic extremes of pleasant and unpleasant emotions that have been identified through the study of opponent processes in the domain of addiction. These neurochemical systems need to be considered in the context of the framework that emotions involve the specific brain regions now identified to differentially interpreting emotive physiological expression.","George F. Koob",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Identification and characterization of microRNAs by deep-sequencing in Hyalomma anatolicum anatolicum (Acari: Ixodidae) ticks","Hyalomma anatolicum anatolicum (H.a. anatolicum) (Acari: Ixodidae) ticks are globally distributed ectoparasites with veterinary and medical importance. These ticks not only weaken animals by sucking their blood but also transmit different species of parasitic protozoans. Multiple factors influence these parasitic infections including miRNAs, which are non-coding, small regulatory RNA molecules essential for the complex life cycle of parasites. To identify and characterize miRNAs in H.a. anatolicum, we developed an integrative approach combining deep sequencing, bioinformatics and real-time PCR analysis. Here we report the use of this approach to identify miRNA expression, family distribution, and nucleotide characteristics, and discovered novel miRNAs in H.a. anatolicum. The result showed that miR-1-3p, miR-275-3p, and miR-92a were expressed abundantly. There was a strong bias on miRNA, family members, and nucleotide compositions at certain positions in H.a. anatolicum miRNA. Uracil was the dominant nucleotide, particularly at positions 1, 6, 16, and 18, which were located approximately at the beginning, middle, and end of conserved miRNAs. Analysis of the conserved miRNAs indicated that miRNAs in H.a. anatolicum were concentrated along three diverse phylogenetic branches of bilaterians, insects and coelomates. Two possible roles for the use of miRNA in H.a. anatolicum could be presumed based on its parasitic life cycle: to maintain a large category of miRNA families of different animals, and/or to preserve stringent conserved seed regions with active changes in other places of miRNAs mainly in the middle and the end regions. These might help the parasite to undergo its complex life style in different hosts and adapt more readily to the host changes. The present study represents the first large scale characterization of H.a. anatolicum miRNAs, which could further the understanding of the complex biology of this zoonotic parasite, as well as initiate miRNA studies in other related species such as Haemaphysalis longicornis and Rhipicephalus sanguineus of human and animal health significance.","Jin Luo and Guang-Yuan Liu and Ze Chen and Qiao-Yun Ren and Hong Yin and Jian-Xun Luo and Hui Wang",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Biological traits analyses in the study of pollution gradients and ecological functioning of marine soft bottom species assemblages in a fjord ecosystem","In the present study, biological traits analysis (BTA) was used to explore and characterise effects of pollution on functional attributes of soft bottom infaunal species assemblages. The data comprised 38 sampling stations in the Oslofjord, Norway, ranging from heavily polluted to minimally impacted areas. At each station, species composition (113 taxa in total), contaminants (cadmium, mercury, lead, DDT, PCB) and sediment parameters were determined. Species functions were analysed for eight biological traits defined for activity and life history features. Traits were scored according to the fuzzy coding technique. The most distinct patterns were shown for mobility, size, sediment dwelling depth, feeding type and larval development in relation to contaminants, sediment physical structure and sediment oxidation status. At high levels of contaminants, particularly cadmium, features such as shallow sediment dwelling depth, small size, subsurface deposit feeding and lecitotroph larval development prevailed, while at low contaminant levels characteristic features included deeper sediment dwelling depth, larger size, surface deposit feeding and permanent attachment. Deep sediment dwelling depth (>15cm) was related to minimally contaminated oxidised sediments at greater water depths. Mobility and carnivorous feeding prevailed in coarser sediments. The study showed that BTA detected and depicted specific features that correlated with gradients in pollution and may be important for sediment reworking and nutrient cycling. As part of the present work, trait information for >500 macrofaunal taxa have been assembled and entered in a comprehensive database.","Eivind Oug and Annelise Fleddum and Brage Rygg and Frode Olsgard",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Advocating for safe abortion in Rwanda: how young people and the personal stories of young women in prison brought about change","In June 2012, a new abortion law came into effect in Rwanda as part of a larger review of Rwanda's penal code. This was a significant step in a country where it was previously taboo even to discuss abortion. This article describes some of the crucial elements in how this success was achieved in Rwanda, which began through a project launched by Rutgers WPF on “sensitive issues in young people's sexuality” in several countries. This paper describes how the Rwandan Youth Action Movement decided to work on unsafe abortion as part of this project. They gathered data on the extent of unsafe abortion and testimonies of young Rwandan women in prison for abortions; organized debates, values clarification exercises, interviews and a survey in four universities; launched a petition for law reform; produced awareness-raising materials; worked with the media; and met with representatives from government ministries, the national women's and youth councils, and parliamentarians – all of which played a significant role in the advocacy process for amendment of the law, which was revised when the penal code came up for review in June 2012. This history shows how important the role of young people can be in producing change and exposes, through personal stories, the need for a better abortion law, not only in Rwanda but also elsewhere.
Résumé
En juin 2012, une nouvelle loi sur l'avortement est entrée en vigueur au Rwanda, dans le cadre d'un examen élargi du code pénal rwandais. C'était un progrès important dans un pays où il était auparavant tabou même de parler d'avortement. Cet article décrit certains des éléments essentiels pour parvenir à ce succès, qui a commencé par un projet lancé par Rutgers WPF sur « les questions sensibles de la sexualité des jeunes » dans plusieurs pays. L'article décrit comment le Mouvement rwandais d'action des jeunes a décidé de travailler sur l'avortement à risque au titre de ce projet. Il a recueilli des données sur l'ampleur de ce phénomène et des témoignages de jeunes Rwandaises en prison pour avortement, et a organisé des débats, des exercices de clarification des valeurs, des entretiens et une enquête dans quatre universités. Il a lancé une pétition pour la réforme de la loi, a produit du matériel de sensibilisation et a collaboré avec les médias. Le Mouvement a rencontré des représentants de ministères, des conseils nationaux des femmes et des jeunes, ainsi que des parlementaires qui tous ont contribué grandement au processus de plaidoyer pour l'amendement de la loi, qui a été révisée lors de l'examen du code pénal en juin 2012. Cette histoire montre le rôle important que les jeunes peuvent jouer pour déclencher le changement et expose, avec des récits personnels, la nécessité d'une meilleure loi sur l'avortement au Rwanda, mais aussi ailleurs.
Resumen
En junio de 2012 entró en vigor una nueva ley referente al aborto en Ruanda como parte de una modificación general del Código Penal ruandés. Éste fue un paso importante en un país donde anteriormente era tabú incluso hablar sobre aborto. En este artículo se describen algunos de los elementos cruciales de este logro, que comenzó por medio de un proyecto iniciado por Rutgers WPF sobre “temas delicados relacionados con la sexualidad en la adolescencia” en varios países. Se describe cómo el Movimiento Ruandés de Juventud en Acción decidió trabajar en asuntos de aborto inseguro como parte de este proyecto. Recolectaron datos sobre la magnitud del aborto inseguro y testimonios de mujeres jóvenes ruandesas encarceladas por tener abortos; organizaron debates, ejercicios de aclaración de valores, entrevistas y una encuesta en cuatro universidades; entablaron una petición de reforma legislativa; produjeron material de concientización; trabajaron con los medios de comunicación; y se reunieron con representantes de ministerios gubernamentales, consejos nacionales de mujeres y jóvenes, y parlamentarios. Todos ellos desempeñaron un papel decisivo en el proceso para modificar la ley, lo cual sucedió cuando el código penal fue revisado en junio de 2012. Esta historia demuestra la importancia del rol de la juventud para producir cambios y expone, mediante historias personales, la necesidad de tener mejores leyes de aborto, no solo en Ruanda sino también en otros países.","Chantal Umuhoza and Barbara Oosters and Miranda van Reeuwijk and Ine Vanwesenbeeck",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A comprehensive, mechanistic heat transfer modeling package for dispersed flow film boiling—Part 2—Implementation and assessment","A mechanistic, first-principles based Dispersed Flow Film Boiling (DFFB) heat transfer package has been implemented within the existing framework of an in-house version of COBRA-TF, called COBRA-IE. Several sensitivities studies were performed on the proposed model to determine how to characterize the droplet size distribution and to validate the use of a quasi-static Lagrangian subscale trajectory calculation within COBRA-IE. Assessing the proposed model against experimental data from 118 experimental runs in four separate facilities has shown a marked improvement over the predictions of the base line COBRA-IE DFFB model set. Over the entire assessment database, the proposed model has reduced the mean error, RMS error, and standard deviation of the error in the wall temperature predictions and improved the prediction in the axial variation of the wall temperature. The proposed DFFB model marks a step-change in the use of mechanistically based DFFB models in reactor safety analysis codes resulting in improved predictive capabilities.","Michael J. Meholic and David L. Aumiller and Fan-Bill Cheung",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Hidden costs: The direct and indirect impact of user fees on access to malaria treatment and primary care in Mali","About 20 years after initial calls for the introduction of user fees in health systems in sub-Saharan Africa, a growing coalition is advocating for their removal. Several African countries have abolished user fees for health care for some or all of their citizens. However, fee-for-service health care delivery remains a primary health care funding model in many countries in sub-Saharan Africa. Although the impact of user fees on utilization of health services and household finances has been studied extensively, further research is needed to characterize the multi-faceted health and social problems associated with charging user fees. This ethnographic study aims to identify consequences of user fees on gender inequality, food insecurity, and household decision-making for a group of women living in poverty. Ethnographic life history interviews were conducted with 24 women in Yirimadjo, Mali in 2007. Purposive sampling selected participants across a broad socio-economic spectrum. Semi-structured interviews addressed participants' past medical history, socio-economic status, social and family history, and access to health care. Interview transcripts were coded using the guiding analytical framework of structural violence. Interviews revealed that user fees for health care not only decreased utilization of health services, but also resulted in delayed presentation for care, incomplete or inadequate care, compromised food security and household financial security, and reduced agency for women in health care decision making. The effects of user fees were amplified by conditions of poverty, as well as gender and health inequality; user fees in turn reinforced the inequalities created by those very conditions. The qualitative data reveal multi-faceted health and socioeconomic effects of user fees, and illustrate that user fees for health care may impact quality of care, health outcomes, food insecurity, and gender inequality, in addition to impacting health care utilization and household finances. As many countries consider user fee abolition policies, these findings indicate the need to create a broader evaluation framework—one that can measure the health and socioeconomic impacts of user fee polices and of their removal.","Ari Johnson and Adeline Goss and Jessica Beckerman and Arachu Castro",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Evolutionary and dispersal history of Triatoma infestans, main vector of Chagas disease, by chromosomal markers","Chagas disease, one of the most important vector-borne diseases in the Americas, is caused by Trypanosoma cruzi and transmitted to humans by insects of the subfamily Triatominae. An effective control of this disease depends on elimination of vectors through spraying with insecticides. Genetic research can help insect control programs by identifying and characterizing vector populations. In southern Latin America, Triatoma infestans is the main vector and presents two distinct lineages, known as Andean and non-Andean chromosomal groups, that are highly differentiated by the amount of heterochromatin and genome size. Analyses with nuclear and mitochondrial sequences are not conclusive about resolving the origin and spread of T. infestans. The present paper includes the analyses of karyotypes, heterochromatin distribution and chromosomal mapping of the major ribosomal cluster (45S rDNA) to specimens throughout the distribution range of this species, including pyrethroid-resistant populations. A total of 417 specimens from seven different countries were analyzed. We show an unusual wide rDNA variability related to number and chromosomal position of the ribosomal genes, never before reported in species with holocentric chromosomes. Considering the chromosomal groups previously described, the ribosomal patterns are associated with a particular geographic distribution. Our results reveal that the differentiation process between both T. infestans chromosomal groups has involved significant genomic reorganization of essential coding sequences, besides the changes in heterochromatin and genomic size previously reported. The chromosomal markers also allowed us to detect the existence of a hybrid zone occupied by individuals derived from crosses between both chromosomal groups. Our genetic studies support the hypothesis of an Andean origin for T. infestans, and suggest that pyrethroid-resistant populations from the Argentinean-Bolivian border are most likely the result of recent secondary contact between both lineages. We suggest that vector control programs should make a greater effort in the entomological surveillance of those regions with both chromosomal groups to avoid rapid emergence of resistant individuals.","Francisco Panzera and María J. Ferreiro and Sebastián Pita and Lucía Calleros and Ruben Pérez and Yester Basmadjián and Yenny Guevara and Simone Frédérique Brenière and Yanina Panzera",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Comparing LOPES measurements of air-shower radio emission with REAS 3.11 and CoREAS simulations","Cosmic ray air showers emit radio pulses at MHz frequencies, which can be measured with radio antenna arrays – like LOPES at the Karlsruhe Institute of Technology in Germany. To improve the understanding of the radio emission, we test theoretical descriptions with measured data. The observables used for these tests are the absolute amplitude of the radio signal, and the shape of the radio lateral distribution. We compare lateral distributions of more than 500 LOPES events with two recent and public Monte Carlo simulation codes, REAS 3.11 and CoREAS (v 1.0). The absolute radio amplitudes predicted by REAS 3.11 are in good agreement with the LOPES measurements. The amplitudes predicted by CoREAS are lower by a factor of two, and marginally compatible with the LOPES measurements within the systematic scale uncertainties. In contrast to any previous versions of REAS, REAS 3.11 and CoREAS now reproduce the shape of the measured lateral distributions correctly. This reflects a remarkable progress compared to the situation a few years ago, and it seems that the main processes for the radio emission of air showers are now understood: The emission is mainly due to the geomagnetic deflection of the electrons and positrons in the shower. Less important but not negligible is the Askaryan effect (net charge variation). Moreover, we confirm that the refractive index of the air plays an important role, since it changes the coherence conditions for the emission: Only the new simulations including the refractive index can reproduce rising lateral distributions which we observe in a few LOPES events. Finally, we show that the lateral distribution is sensitive to the energy and the mass of the primary cosmic ray particles.","W.D. Apel and J.C. Arteaga-Velázquez and L. Bähren and K. Bekk and M. Bertaina and P.L. Biermann and J. Blümer and H. Bozdog and I.M. Brancus and E. Cantoni and A. Chiavassa and K. Daumiller and V. de Souza and F. Di Pierro and P. Doll and R. Engel and H. Falcke and B. Fuchs and D. Fuhrmann and H. Gemmeke and C. Grupen and A. Haungs and D. Heck and J.R. Hörandel and A. Horneffer and D. Huber and T. Huege and P.G. Isar and K.-H. Kampert and D. Kang and O. Krömer and J. Kuijpers and K. Link and P. Łuczak and M. Ludwig and H.J. Mathes and M. Melissas and C. Morello and J. Oehlschläger and N. Palmieri and T. Pierog and J. Rautenberg and H. Rebel and M. Roth and C. Rühle and A. Saftoiu and H. Schieler and A. Schmidt and F.G. Schröder and O. Sima and G. Toma and G.C. Trinchero and A. Weindl and J. Wochele and J. Zabierowski and J.A. Zensus",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Complement polymorphisms: Geographical distribution and relevance to disease","The evolution of man has been characterised by recurrent episodes of migration and settlement with infectious disease a constant threat. This long history of demographic change, together with the action of evolutionary forces such as natural selection and genetic drift, has shaped human genetic diversity. In particular, the interaction between humans, pathogens and the environment has played a crucial role in generating patterns of human genetic variation. The complement system plays a crucial role in the early protective immune response after exposure to a pathogen. Pathogens, over time, have developed mechanisms to circumvent the effects of complement which in turn has led to development of a more complex complement system. During the evolution of the complement system genes coding complement proteins have evolved polymorphisms, some of which have a functional effect, and this may reflect human–pathogen interaction and geographical origin. An example is the polymorphism Ile62Val (rs800292 (A>G)) in the complement regulator Factor H gene which alters the susceptibility to age-related macular degeneration (AMD), with the Ile62 polymorphism protecting against AMD. When sub-Saharan African and European populations are compared, the frequency of this polymorphism shows a very marked geographical distribution. Polymorphisms in other complement genes such as complement factor B show similar trends. This paper describes the geographical variation present in complement genes and discusses the implications of these observations. The analysis of genetic variation in complement genes is a promising tool to unravel mechanisms of host–pathogen interaction and can provide new insights into the evolution of the human immune system.","L. Ermini and I.J. Wilson and T.H.J. Goodship and N.S. Sheerin",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"GEOTHERM: A finite difference code for testing metamorphic P–T–t paths and tectonic models","Here, time-dependent solutions for the heat conduction equation are numerically evaluated in 1D space using a fully implicit algorithm based on the finite difference method, assuming temperature-dependence of thermal conductivity. The method is implemented using the package ‘GEOTHERM’, comprising 13 MATLAB-derived scripts and 3 Excel spreadsheets. In the package, the initial state of the modeled crust, including its thickness, average density, and average heat production rate, can be configured by the user. The exhumation/burial history and metamorphic evolution of the crust are simulated by changing these initial values to fit the vertical displacement rates of the crust imposed by the user. Once the inputs have been made, the variations with depth of temperature, proportion of melt, and shear stress, as well as average values of heat flow at the surface and across the Moho, are calculated and displayed in five separate plots. The code is demonstrated with respect to the Carboniferous evolution of the South Variscan Belt. The best fit to independent petrologic constraints derived from thermobarometry is obtained with an early Carboniferous (342Ma) slab break-off and a shear strain rate of 10−13s−1 between 318 and 305Ma.","Leonardo Casini and Antonio Puccini and Stefano Cuccuru and Matteo Maino and Giacomo Oggiano",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"SCADOP: Phenomenological modeling of dryout in nuclear fuel rod bundles","Analysis and prediction of dryout is of important consequence to safety of nuclear fuel clusters of boiling water type of reactors. Traditionally, experimental correlations are used for dryout predictions. Since these correlations are based on operating parameters and do not aim to model the underlying phenomena, there has been a proliferation of the correlations, each catering to some specific bundle geometry under a specific set of operating conditions. Moreover, such experiments are extremely costly. In general, changes in tested bundle geometry for improvement in thermal-hydraulic performance would require re-experimentation. Understanding and modeling the basic processes leading to dryout in flow boiling thus has great incentive. Such a model has the ability to predict dryout in any rod bundle geometry, unlike the operating parameter based correlation approach. Thus more informed experiments can be carried out. A good model can, reduce the number of experiments required during the iterations in bundle design. In this paper, a phenomenological model as indicated above is presented. The model incorporates a new methodology to estimate the Initial Entrained Fraction (IEF), i.e., entrained fraction at the onset of annular flow. The incorporation of this new methodology is important since IEF is often assumed ad-hoc and sometimes also used as a parameter to tune the model predictions to experimental data. It is highlighted that IEF may be low under certain conditions against the general perception of a high IEF due to influence of churn flow. It is shown that the same phenomenological model is applicable to tubes as well as rod bundles. For application to rod bundles, the flow field was calculated using subchannel methodology. The model developed has been validated against experimental data in tubes and rod bundles. In the process a computer code SCADOP has been developed for analysis of dryout in rod bundles.","Arnab Dasgupta and D.K. Chandraker and P.K. Vijayan",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Hyper-Fractal Analysis v04: Implementation of a fuzzy box-counting algorithm for image analysis of artistic works","This work presents a new version of a Visual Basic 6.0 application for estimating the fractal dimension of images and 4D objects (Grossu et al. 2013 [1]). Following our attempt of investigating artistic works by fractal analysis of craquelure, we encountered important difficulties in filtering real information from noise. In this context, trying to avoid a sharp delimitation of “black” and “white” pixels, we implemented a fuzzy box-counting algorithm.
New version program summary
Program title: Hyper-Fractal Analysis v04 Catalogue identifier: AEEG_v4_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEEG_v4_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 745999 No. of bytes in distributed program, including test data, etc.: 12844235 Distribution format: tar.gz Programming language: MS Visual Basic 6.0 Computer: PC Operating system: MS Windows 98 or later RAM: 100M Classification: 14 Catalogue identifier of previous version: AEEG_v3_0 Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 1344 Does the new version supercede the previous version?: Yes Nature of problem: estimating the fractal dimension of images Solution method: fuzzy box-counting algorithm Reasons for new version: Following the idea [2, 3] of investigating old paintings by fractal analysis of craquelure [4, 5], we faced with significant difficulties involved by the band-pass filter limitations. Trying to find a smoother way of separating information from noise, we implemented a fuzzy box-counting algorithm [6–8]. The fractal dimension [9] can be defined as: (1)df=limr→0logN(r)log(1/r) where N(r) represents the number of boxes, with length r, needed to cover the object. The main change considered is related to the significance of N(r). As opposed to the classical approach, where each box contributes to N(r) with either 1 (black), or 0 (white), in the fuzzy version (Fig. 1) each box contributes to N(r) with a rational number p=1−color code/(total number of colors−1). Summary of revisions:1.Implementation of a fuzzy box-counting algorithm for estimating the fractal dimension of images2.Optimization of the file open procedure.Fig. 1Hyper-Fractal Analysis v04 example of use. Fuzzy fractal dimension of painting craquelure. Running time: In a first approximation, the algorithm is linear [2]. References: [1]I.V. Grossu, I. Grossu, D. Felea, C. Besliu, Al. Jipa, T. Esanu, C.C. Bordeianu, E. Stan, Computer Physics Communications, 184 (2013) 1344–1345.[2]I.V. Grossu, C. Besliu, M.V. Rusu, Al. Jipa, C. C. Bordeianu, D. Felea, Computer Physics Communications, 180 (2009) 1999–2001.[3]I.V. Grossu, M.V. Rusu, A. Teodosiu, Fractals in a particular process. Fractals in the investigation of artistic works, in National Conference of Physics, Romania, Constanta, 21–23 September 2000.[4]A. Teodosiu, Din universul ascuns al operei de arta, Allfa, Romania (2001) pp. 113–122.[5]S Bucklow, Consensus in the Classification of Craquelure, Hamilton Kerr Institute Bulletin, number 3, ed. A Massing, Hamilton Kerr Institute, University of Cambridge 2000: pp. 61–73.[6]X. Zhuang, Q. Meng, Artificial Intelligence in Medicine (2004) 32, 29–36.[7]D. Dumitrescu, Hariton Costin, Retele Neuronale. Teorie si aplicatii, Teora, Bucuresti, 1996, pp. 228–262.[8]D. Dumitrescu, Fuzzy Measures and the entropy of fuzzy partitions, J. Math, Anal. Appl., 176 (1993b) 359–373.[9]R.H. Landau, M.J. Paez and C.C. Bordeianu, Computational physics: Problem solving with computers, Wiley-VCH-Verlag, Weinheim, 2007, pp. 293–306.","I.V. Grossu and S.A. El-Shamali",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Cortical magnification plus cortical plasticity equals vision?","Most approaches to visual prostheses have focused on the retina, and for good reasons. The earlier that one introduces signals into the visual system, the more one can take advantage of its prodigious computational abilities. For methods that make use of microelectrodes to introduce electrical signals, however, the limited density and volume occupying nature of the electrodes place severe limits on the image resolution that can be provided to the brain. In this regard, non-retinal areas in general, and the primary visual cortex in particular, possess one large advantage: “magnification factor” (MF)—a value that represents the distance across a sheet of neurons that represents a given angle of the visual field. In the foveal representation of primate primary visual cortex, the MF is enormous—on the order of 15–20mm/deg in monkeys and humans, whereas on the retina, the MF is limited by the optical design of the eye to around 0.3mm/deg. This means that, for an electrode array of a given density, a much higher-resolution image can be introduced into V1 than onto the retina (or any other visual structure). In addition to this tremendous advantage in resolution, visual cortex is plastic at many different levels ranging from a very local ability to learn to better detect electrical stimulation to higher levels of learning that permit human observers to adapt to radical changes to their visual inputs. We argue that the combination of the large magnification factor and the impressive ability of the cerebral cortex to learn to recognize arbitrary patterns, might outweigh the disadvantages of bypassing earlier processing stages and makes V1 a viable option for the restoration of vision.","Richard T. Born and Alexander R. Trott and Till S. Hartmann",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Concrete slab comparison and embodied energy optimisation for alternate design and construction techniques","Construction material consumption is greater than any time in history. Australia produces approximately 30 million tonnes of finished building products each year, with over 56% of this quantity, by mass, being attributed to concrete and a further 6%, steel. Globally, 23trillion kilograms of concrete alone is consumed annually, with growing population driving increasing demands. This study assesses the environmental performance of various concrete slab systems. Historically, the focus of environmental performance in buildings has been Operation Energy (OE) requirements, however Zero Energy Buildings (ZEB) are changing this. Specifically the study investigates the environmental performance of concrete structures varying design parameters and construction techniques to optimise its embodied energy (EE). These structures are designed in accordance with all relevant Australian codes and standards. The various slab systems investigated include: beam & slab, flat slab and flat plates while concurrently considering the use of conventionally reinforced and post-tensioned construction methods. Designs were compared in terms of EE outcomes given fixed design criteria, with results indicating reductions between 23.7% and 49.1% when utilising post-tensioned construction methods.","Dane Miller and Jeung-Hwan Doh and Mitchell Mulvey",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Lupus, «un cáncer pero más chiquito». Percepciones del lupus eritematoso sistémico en adolescentes próximos a la transición","Resumen
Introducción
Se espera que pacientes con lupus pediátrico próximos a la transición asuman responsabilidades de su cuidado, pero muchos no están preparados, lo cual empobrece su pronóstico. Según el modelo autorregulatorio, las percepciones de la enfermedad determinan las respuestas emocionales, guían las conductas de afrontamiento, su evaluación, retroalimentación y estrategias de asimilación.
Objetivo
Describir las percepciones sobre el lupus en adolescentes próximos a la transición.
Materiales y métodos
Desde un enfoque hermenéutico y utilizando técnicas de la teoría fundada, se realizaron 11 entrevistas semiestructuradas entre junio de 2013 y septiembre de 2014 a 9 adolescentes con diagnóstico de lupus.
Resultados
Se obtuvieron 1.800 códigos. Emergieron como categorías preliminares: «Intentando explicar el origen», donde se interpreta la causa de la enfermedad en términos de inmunosupresión y autoinmunidad, asociación con cáncer, culpa e influencia de factores emocionales, además el proceso diagnóstico. «Lo que se pierde», enmarcado en cambios, trato diferencial y límites. Finalmente, los «aspectos positivos»: enfermedad como modulador de la conducta, adquisición de cualidades, aprendizaje sobre el funcionamiento corporal y ganancia secundaria.
Conclusión
Las percepciones de la enfermedad en estos adolescentes, expresan en la experiencia de cambio las implicaciones del diagnóstico, las cuales impactan múltiples aspectos de sus vidas y traen consigo incertidumbre y necesidad de ajustes que les llevan a la búsqueda de explicaciones. Se requiere mayor conciencia sobre estas percepciones, debido a que, junto con otros factores, determinarán las estrategias que los adolescentes desarrollen para garantizar su autocuidado y adaptación a situaciones derivadas del vivir con la enfermedad.
Introduction
Patients with paediatric lupus nearing transition to adult care are expected to take responsibility for their care. Nevertheless, many are not prepared for this, and thus have a poorer prognosis. Using the self-regulation model, the perception of a condition determines the emotional responses and guides coping efforts, appraisal, feedback, and assimilation strategies.
Objective
To describe how adolescents nearing transition perceive lupus.
Materials and methods
Eleven semi-structured interviews were conducted using a hermeneutic approach with techniques from grounded theory. Interviews took place between July 2013 and September 2014. The participants were nine adolescents with diagnosed lupus.
Results
A total of 1,800 codes were obtained that emerged as the following preliminary categories: “Attempting to explain the origin”, where the cause of the disease is interpreted as immunosuppression, autoimmunity, association with cancer, guilt and influence of emotional factors, along with the diagnosis process. “What is lost”, which includes changes, being treated differently, and having limitations. The last category was “positive aspects” deals with illness as a behavioural moderator, acquiring qualities, learning about bodily functioning and secondary gain.
Conclusion
Adolescents perceive their condition based on the implications of the changes experienced in their lives as a result of the diagnosis. Lupus affects several aspects of their lives and brings uncertainty and a need to adjust, leading them to look for explanations. More awareness of these perceptions is required because the latter, along with other factors, determine the strategies that adolescents develop to ensure their self-care and adaptation to any situations arising from living with the condition.","Lady J. Hernández Zapata and Sandra I. Alzate Vanegas and Ruth M. Eraso and Carlos E. Yepes Delgado",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Automatic hemolysis identification on aligned dual-lighting images of cultured blood agar plates","Background and Objective: The recent introduction of Full Laboratory Automation systems in clinical microbiology opens to the availability of streams of high definition images representing bacteria culturing plates. This creates new opportunities to support diagnostic decisions through image analysis and interpretation solutions, with an expected high impact on the efficiency of the laboratory workflow and related quality implications. Starting from images acquired under different illumination settings (top-light and back-light), the objective of this work is to design and evaluate a method for the detection and classification of diagnostically relevant hemolysis effects associated with specific bacteria growing on blood agar plates. The presence of hemolysis is an important factor to assess the virulence of pathogens, and is a fundamental sign of the presence of certain types of bacteria. Methods: We introduce a two-stage approach. Firstly, the implementation of a highly accurate alignment of same-plate image scans, acquired using top-light and back-light illumination, enables the joint spatially coherent exploitation of the available data. Secondly, from each segmented portion of the image containing at least one bacterial colony, specifically designed image features are extracted to feed a SVM classification system, allowing detection and discrimination among different types of hemolysis. Results: The fine alignment solution aligns more than 98.1% images with a residual error of less than 0.13 mm. The hemolysis classification block achieves a 88.3% precision with a recall of 98.6%. Conclusions: The results collected from different clinical scenarios (urinary infections and throat swab screening) together with accurate error analysis demonstrate the suitability of our system for robust hemolysis detection and classification, which remains feasible even in challenging conditions (low contrast or illumination changes).","Mattia Savardi and Alessandro Ferrari and Alberto Signoroni",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Late Ebola virus relapse causing meningoencephalitis: a case report","Summary
Background
There are thousands of survivors of the 2014 Ebola outbreak in west Africa. Ebola virus can persist in survivors for months in immune-privileged sites; however, viral relapse causing life-threatening and potentially transmissible disease has not been described. We report a case of late relapse in a patient who had been treated for severe Ebola virus disease with high viral load (peak cycle threshold value 13·2).
Methods
A 39-year-old female nurse from Scotland, who had assisted the humanitarian effort in Sierra Leone, had received intensive supportive treatment and experimental antiviral therapies, and had been discharged with undetectable Ebola virus RNA in peripheral blood. The patient was readmitted to hospital 9 months after discharge with symptoms of acute meningitis, and was found to have Ebola virus in cerebrospinal fluid (CSF). She was treated with supportive therapy and experimental antiviral drug GS-5734 (Gilead Sciences, San Francisco, Foster City, CA, USA). We monitored Ebola virus RNA in CSF and plasma, and sequenced the viral genome using an unbiased metagenomic approach.
Findings
On admission, reverse transcriptase PCR identified Ebola virus RNA at a higher level in CSF (cycle threshold value 23·7) than plasma (31·3); infectious virus was only recovered from CSF. The patient developed progressive meningoencephalitis with cranial neuropathies and radiculopathy. Clinical recovery was associated with addition of high-dose corticosteroids during GS-5734 treatment. CSF Ebola virus RNA slowly declined and was undetectable following 14 days of treatment with GS-5734. Sequencing of plasma and CSF viral genome revealed only two non-coding changes compared with the original infecting virus.
Interpretation
Our report shows that previously unanticipated, late, severe relapses of Ebola virus can occur, in this case in the CNS. This finding fundamentally redefines what is known about the natural history of Ebola virus infection. Vigilance should be maintained in the thousands of Ebola survivors for cases of relapsed infection. The potential for these cases to initiate new transmission chains is a serious public health concern.
Funding
Royal Free London NHS Foundation Trust.","Michael Jacobs and Alison Rodger and David J Bell and Sanjay Bhagani and Ian Cropley and Ana Filipe and Robert J Gifford and Susan Hopkins and Joseph Hughes and Farrah Jabeen and Ingolfur Johannessen and Drosos Karageorgopoulos and Angie Lackenby and Rebecca Lester and Rebecca S N Liu and Alisdair MacConnachie and Tabitha Mahungu and Daniel Martin and Neal Marshall and Stephen Mepham and Richard Orton and Massimo Palmarini and Monika Patel and Colin Perry and S Erica Peters and Duncan Porter and David Ritchie and Neil D Ritchie and R Andrew Seaton and Vattipally B Sreenu and Kate Templeton and Simon Warren and Gavin S Wilkie and Maria Zambon and Robin Gopal and Emma C Thomson",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Aztreonam for inhalation solution in patients with non-cystic fibrosis bronchiectasis (AIR-BX1 and AIR-BX2): two randomised double-blind, placebo-controlled phase 3 trials","Summary
Background
The clinical benefit of inhaled antibiotics in non-cystic fibrosis bronchiectasis has not been established in randomised controlled trials. We aimed to assess safety and efficacy of aztreonam for inhalation solution (AZLI) in patients with non-cystic fibrosis bronchiectasis and Gram-negative bacterial colonisation.
Methods
AIR-BX1 and AIR-BX2 were two double-blind, multicentre, randomised, placebo-controlled phase 3 trials, which included patients aged 18 years or older who had bronchiectasis and history of positive sputum or bronchoscopic culture for target Gram-negative organisms. Patients were randomly assigned to receive either AZLI or placebo (1:1). Randomisation was done without stratification and the code was generated by a Gilead designee. In both studies, two 4-week courses of AZLI 75 mg or placebo (three-times daily; eFlow nebulizer) were each followed by a 4-week off-treatment period. Primary endpoint was change from baseline Quality of Life-Bronchiectasis Respiratory Symptoms scores (QOL-B-RSS) at 4 weeks. These trials are registered with ClinicalTrials.gov, numbers are NCT01313624 for AIR-BX1 and NCT01314716 for AIR-BX2.
Findings
We recruited participants from 47 ambulatory clinics for AIR-BX1 and 65 ambulatory clinics for AIR-BX2; studies were done between April 25, 2011, and July 1, 2013. In AIR-BX1, of the 348 patients screened, 134 were randomly assigned to receive AZLI and 132 to receive placebo. In AIR-BX2, of the 404 patients screened, 136 were randomly assigned to receive AZLI and 138 to receive placebo. The difference between AZLI and placebo for adjusted mean change from baseline QOL-B-RSS was not significant at 4 weeks (0·8 [95% CI −3·1 to 4·7], p=0·68) in AIR-BX1, but was significant (4·6 [1·1 to 8·2], p=0·011) in AIR-BX2. The 4·6 point difference in QOL-B-RSS after 4 weeks in AIR-BX2 was not deemed clinically significant. In both studies, treatment-related adverse events were more common in the AZLI group than in the placebo group, as were discontinuations from adverse events. The most commonly reported treatment-emergent adverse events were dyspnea, cough, and increased sputum. Each was more common for AZLI-treated than for placebo-treated patients, but the incidences were more balanced in AIR-BX2.
Interpretation
AZLI treatment did not provide significant clinical benefit in non-cystic fibrosis bronchiectasis, as measured by QOL-B-RSS, suggesting a continued need for placebo-controlled studies to establish the clinical benefit of inhaled antibiotics in patients with this disorder.
Funding
Gilead Sciences.","Alan F Barker and Anne E O'Donnell and Patrick Flume and Philip J Thompson and Jonathan D Ruzi and Javier de Gracia and Wim G Boersma and Anthony De Soyza and Lixin Shao and Jenny Zhang and Laura Haas and Sandra A Lewis and Sheila Leitzinger and A Bruce Montgomery and Matthew T McKevitt and David Gossage and Alexandra L Quittner and Thomas G O'Riordan",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Development of a control rod depletion methodology for the Westinghouse NEXUS system","The NEXUS project is an effort to merge and modernize the methods employed in Westinghouse PWR and BWR steady-state reactor physics codes. The NEXUS system relies on a once-through nodal cross-section generation methodology with an innovative and efficient technique for pin power recovery. The pin power methodology overcomes a well-known limitation of existing methodologies, namely the incapacity to properly account for heterogeneity changes due to the depletion environment. The so-called control rod history problem where control rods are repeatedly inserted and withdrawn during core depletion is a good example of such a case. In addition to the control rod history impact on pin power distributions, the insertion of control rods during extended periods leads to significant control rod depletion that affects the reactivity worth of the control rods which in turn can have a significant impact on pin powers. The importance of accurately predicting pin powers, combined with the need to adequately estimate the reactivity worth and nuclear end of life of control rods in BWRs and in generation III+ PWRs, has motivated the development of a novel control rod depletion model. This methodology and its numerical qualification, initially for PWR application only, is the topic of this paper. The focus is on describing the salient features of the model and on illustrating its performance by means of numerical experiments. It is shown that together with the NEXUS pin power recovery model, the control rod depletion methodology accurately predicts the reactivity feedback from repeated control rod insertions in a PWR core.","Fausto Franceschini and Baocheng Zhang and Larry Mayhue and Erwin Müller and Petri Forslund Guimarães",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"OpenMP, OpenMP/MPI, and CUDA/MPI C programs for solving the time-dependent dipolar Gross–Pitaevskii equation","We present new versions of the previously published C and CUDA programs for solving the dipolar Gross–Pitaevskii equation in one, two, and three spatial dimensions, which calculate stationary and non-stationary solutions by propagation in imaginary or real time. Presented programs are improved and parallelized versions of previous programs, divided into three packages according to the type of parallelization. First package contains improved and threaded version of sequential C programs using OpenMP. Second package additionally parallelizes three-dimensional variants of the OpenMP programs using MPI, allowing them to be run on distributed-memory systems. Finally, previous three-dimensional CUDA-parallelized programs are further parallelized using MPI, similarly as the OpenMP programs. We also present speedup test results obtained using new versions of programs in comparison with the previous sequential C and parallel CUDA programs. The improvements to the sequential version yield a speedup of 1.1–1.9, depending on the program. OpenMP parallelization yields further speedup of 2–12 on a 16-core workstation, while OpenMP/MPI version demonstrates a speedup of 11.5–16.5 on a computer cluster with 32 nodes used. CUDA/MPI version shows a speedup of 9–10 on a computer cluster with 32 nodes.
New version program summary
Program Title: DBEC-GP-OMP-CUDA-MPI: (1) DBEC-GP-OMP package: (i) imag1dX-th, (ii) imag1dZ-th, (iii) imag2dXY-th, (iv) imag2dXZ-th, (v) imag3d-th, (vi) real1dX-th, (vii) real1dZ-th, (viii) real2dXY-th, (ix) real2dXZ-th, (x) real3d-th; (2) DBEC-GP-MPI package: (i) imag3d-mpi, (ii) real3d-mpi; (3) DBEC-GP-MPI-CUDA package: (i) imag3d-mpicuda, (ii) real3d-mpicuda. Program Files doi:http://dx.doi.org/10.17632/j3z9z379m8.1 Licensing provisions: Apache License 2.0 Programming language: OpenMP C; CUDA C. Computer: DBEC-GP-OMP runs on any multi-core personal computer or workstation with an OpenMP-capable C compiler and FFTW3 library installed. MPI versions are intended for a computer cluster with a recent MPI implementation installed. Additionally, DBEC-GP-MPI-CUDA requires CUDA-aware MPI implementation installed, as well as that a computer or a cluster has Nvidia GPU with Compute Capability 2.0 or higher, with CUDA toolkit (minimum version 7.5) installed. Number of processors used: All available CPU cores on the executing computer for OpenMP version, all available CPU cores across all cluster nodes used for OpenMP/MPI version, and all available Nvidia GPUs across all cluster nodes used for CUDA/MPI version. Journal reference of previous version: Comput. Phys. Commun. 195 (2015) 117; ibid.200 (2016) 406. Does the new version supersede the previous version?: Not completely. OpenMP version does supersede previous AEWL_v1_0 version, while MPI versions do not supersede previous versions and are meant for execution on computer clusters and multi-GPU workstations. Nature of problem: These programs are designed to solve the time-dependent nonlinear partial differential Gross–Pitaevskii (GP) equation with contact and dipolar interaction in a harmonic anisotropic trap. The GP equation describes the properties of a dilute trapped Bose–Einstein condensate. OpenMP package contains programs for solving the GP equation in one, two, and three spatial dimensions, while MPI packages contain only three-dimensional programs, which are computationally intensive or memory demanding enough to require such level of parallelization. Solution method: The time-dependent GP equation is solved by the split-step Crank–Nicolson method by discretizing in space and time. The discretized equation is then solved by propagation, in either imaginary or real time, over small time steps. The contribution of the dipolar interaction is evaluated by a Fourier transformation to momentum space using a convolution theorem. MPI parallelization is done using the domain decomposition. The method yields the solution of stationary and/or non-stationary problems. Reasons for the new version: Previously published C and Fortran programs [1] for solving the dipolar GP equation are sequential in nature and do not exploit the multiple cores or CPUs found in typical modern computers. A parallel implementation exists, using Nvidia CUDA [2], and both versions are already used within the ultra-cold atoms community [3]. However, CUDA version requires special hardware, which limits its usability. Furthermore, many researchers have access to high performance computer clusters, which could be used to either further speed up the computation, or to work with problems which cannot fit into a memory of a single computer. In light of these observations, we have parallelized all programs using OpenMP, and then extended the parallelization of three-dimensional programs using MPI to distributed-memory clusters. Since the CUDA implementation uses the same algorithm, and thus has the same structure and flow, we have applied the same data distribution scheme to provide the distributed-memory CUDA/MPI implementation of three-dimensional programs. Summary of revisions: Package DBEC-GP-OMP: Previous serial C programs [1] are here improved and then parallelized using OpenMP (package DBEC-GP-OMP). The main improvement consists of switching to real-to-complex (R2C) Fourier transform, which is possible due to the fact that input of the transform is purely real. In this case the result of the transform has Hermitian symmetry, where one half of the values are complex conjugates of the other half. The fast Fourier transformation (FFT) libraries we use can exploit this to compute the result faster, using half the memory. To parallelize the programs, we have used OpenMP with the same approach as described in [4], and extended the parallelization routines to include the computation of the dipolar term. The FFT, used in computation of the dipolar term, was also parallelized in a straightforward manner, by using the built-in support for OpenMP in FFTW3 library [5]. With the introduction of multiple threads memory usage has increased, driven by the need to have some variables private to each thread. To reduce the memory consumed, we resorted to using techniques similar to the ones used in our CUDA implementation [2], i.e., we have reduced the memory required for FFT by exploiting the aforementioned R2C FFT, and reused the memory with pointer aliases whenever possible. Package DBEC-GP-MPI: Next step in the parallelization (package DBEC-GP-MPI) was to extend the programs to run on distributed-memory systems, i.e., on computer clusters using domain decomposition with MPI programming paradigm. We chose to use the newly-implemented threaded versions of the programs as the starting point. Alternatively, we could have used serial versions, and attempt a pure MPI parallelization, however we have found that OpenMP-parallelized routines better exploit the data locality and thus outperform the pure MPI implementation. Therefore, our OpenMP/MPI-parallelized programs are intended to run one MPI process per cluster node, and each process would spawn the OpenMP threads as needed on its cluster node. Note that this is not a requirement, and users may run more than one MPI process per node, but we advise against it due to performance reasons. With the suggested execution strategy (one MPI process per cluster node, each spawning as many threads as CPU cores available), OpenMP threads perform most of the computation, and MPI is used for data exchanges between processes. There are numerous ways to distribute the data between MPI processes, and we decided to use a simple one-dimensional data distribution, also known as slab decomposition. Data is distributed along the first (slowest changing) dimension, which corresponds to NX spatial dimension in our programs (see Fig. 1). Each process is assigned a different portion of the NX dimension, and contains the entire NY and NZ spatial dimensions locally. This allows each process to perform computation on those two dimensions in the same way as before, without any data exchanges. In case the computation requires whole NX dimension to be local to each process, we transpose the data, and after the computation, we transpose the data back. Fig. 1Illustration of data distribution between MPI processes. On the left, the data are distributed along the NX dimension, while on the right the same data are redistributed along the NY dimension. Transpose routine can be implemented in many ways using MPI, most commonly using MPI_Alltoall function, or using transpose routines from external libraries, like FFTW3 [5] or 2DECOMP&FFT [6]. Since we already rely on FFTW3 library for FFT, we have utilized its dedicated transpose interface to perform the necessary transformations. To speed up transpose operation, we do not perform full transposition of data, but rather leave it locally transposed. That is, we transform from local_NX × NY × NZ, stored in row-major order, to NX × local_NY × NZ in row-major order (where local_NX = NX / number_of_processes, and equivalently for local_NY). This approach has an additional benefit that we do not have to make significant changes in the way array elements are processed, and in most cases we only have to adjust the loop limit of the non-local dimension. Package DBEC-GP-MPI-CUDA: The aforementioned data distribution scheme can be also applied to the CUDA version of programs [2]. However, there is no support for CUDA in FFTW3, and cuFFT (used in CUDA programs for FFT) does not provide equivalent MPI or transpose interface. Instead, we developed our own transpose routines, and used them in FFT computation. One example of manual implementation of transpose routines is shown in Ref. [7], and while we could readily use the same code, we wanted to have the same result as when using FFTW3. To achieve this, we use the same basic principle as in Ref. [7], first we create a custom MPI data type that maps to portions of the data to be exchanged, followed by an all-to-all communication to exchange the data between processes, see Fig. 2 for details. Fig. 2Example of a transpose routine of a 4×4×4 data between four MPI processes. Initially, all processes have 1/4 of the NX dimension, and whole NY and NZ dimensions. After transposing, each process has full NX and NZ dimensions, and 1/4 of the NY dimension. The implemented transpose routines are also used to compute a distributed-memory FFT, performed over all MPI processes. To divide the computation of a multidimensional FFT, in our case three-dimensional, we use a well-known row–column algorithm. The basic idea of the algorithm is perhaps best explained on a two-dimensional FFT of N×M data, stored in row-major order, illustrated in Fig. 3. First the N one-dimensional FFTs of length M are performed (along the row of data), followed by a transpose, after which data are stored as M×N in row-major format. Now M FFTs of length N can be performed along what used to be a column of original data, but are stored as rows after transposing. Finally, an optional transpose can be performed to return the data in their original N×M form. In three dimensions, we can perform a two-dimensional FFT, transpose the data, and perform the FFT along the third dimension. This algorithm can be easily adapted for distributed memory systems. We use advanced cuFFT interface for local computation of FFT, and use our transpose routine to redistribute the data. Note that DBEC-GP-MPI-CUDA programs can be easily modified to work on a single workstation with multiple GPU cards, or a computer cluster with multiple GPU cards per node. In that case, for each GPU card a separate MPI process should be launched and the programs should be modified to assign a separate GPU card for processes on the same cluster node. Fig. 3Illustration of four stages of row–column FFT algorithm. The last transpose operation may be omitted, and often yields better performance. MPI output format: Given that the distributed memory versions of the programs can be used for much larger grid sizes, the output they produce (i.e., the density profiles) can be much larger and difficult to handle. To alleviate this problem somewhat, we have switched to a binary output instead of the textual. This allowed us to reduce the size of files, while still retaining precision. All MPI processes will write the output to the same file, at the corresponding offset, relieving the user of the task of combining the files. The binary output can be subsequently converted to textual, for example by using hexdump command on UNIX-like systems. We have developed a simple script which converts the output from binary to textual format and included it in the software package. Testing results: We have tested all programs on the PARADOX supercomputing facility at the Scientific Computing Laboratory of the Institute of Physics Belgrade. Nodes used for testing had two Intel Xeon E5-2670 CPUs (with a total of 2×8=16 CPU cores) with 32 GB of RAM and one Nvidia Tesla M2090 GPU with 6 GB of RAM, each connected by Infiniband QDR interconnect. The presented results are obtained for arbitrary grid sizes, which are not tailored to maximize performance of the programs. We also stress that execution times and speedups reported here are calculated for critical parallelized parts of the programs performing iterations over imaginary or real time steps, and they exclude time spent on initialization (threads initialization, MPI environment, allocation/deallocation of memory, creating/destroying FFTW plans, I/O operations). As a part of its output, each program separately prints initialization time and time spent on iterations for GP propagation. The latter time is used to calculate a speedup, as a speedup obtained this way does not depend on the number of iterations and is more useful for large numbers of iterations. The testing of OpenMP versions of programs DBEC-GP-OMP was performed with the number of threads varying from 1 to 16. Table 1 and Fig. 4 show the obtained absolute wall-clock times, speedups, and scaling efficiencies, as well as comparison with the previous serial version of programs [1]. As we can see from the table, improvements in the FFT routine used already yield a speedup of 1.3 to 1.9 for single-threaded (T=1) 2d and 3d programs compared to the previous serial programs, and somewhat smaller speedup for 1d programs, 1.1 to 1.3. The use of additional threads brings about further speedup of 2 to 2.5 for 1d programs, and 9 to 12 for 2d and 3d programs. From Fig. 4 we see that for 1d programs, although speedup increases with the number of threads used, the efficiency decreases due to insufficient size of the problem, and one can achieve almost maximal value of speedup already with T=4 threads, while still keeping the efficiency around 50%. We also see, as expected, that speedup and efficiency of 2d and 3d programs behave quite well as we increase the numbers of threads. In particular, we note that the efficiency is always above 60%, making the use of all available CPU cores worthwhile. Fig. 4Speedup in the execution time and scaling efficiency of DBEC-GP-OMP programs compared to single-threaded runs: (a) imag1dX-th, (b) real1dX-th, (c) imag2dXY-th, (d) real2dXY-th, (e) imag3d-th, (f) real3d-th. Scaling efficiency is calculated as a fraction of the obtained speedup compared to a theoretical maximum. Grid sizes used for testing are the same as in Table 1. Speedups and efficiencies of imag1dZ-th, real1dZ-th, imag2dXZ-th, and real2dXZ-th (not reported here) are similar to those of imag1dX-th, real1dX-th, imag2dXY-th, and real2dXY-th, respectively. For testing of MPI versions we have used a similar methodology to measure the strong scaling performance. For OpenMP/MPI programs DBEC-GP-MPI, the obtained wall-clock times are shown in Table 2, together with the corresponding wall-clock times for the OpenMP programs DBEC-GP-OMP that served as a baseline to calculate speedups. The testing was done for varying number of cluster nodes, from 4 to 32, and the measured speedup ranged from 11 to 16.5. The corresponding graphs of speedups and efficiencies are shown in Fig. 5, where we can see that the speedup grows linearly with the number of nodes used, while the efficiency remains mostly constant in the range between 40% and 60%, thus making the use of OpenMP/MPI programs highly advantageous for problems with large grid sizes. Fig. 5Speedup in the execution time and scaling efficiency of DBEC-GP-MPI programs compared to single-node OpenMP runs: (a) imag3d-mpi, (b) real3d-mpi. Scaling efficiency is calculated as a fraction of the obtained speedup compared to a theoretical maximum. Grid size used for testing is the same as in Table 2. For CUDA/MPI programs DBEC-GP-MPI-CUDA we observe similar behavior in Table 3 and in Fig. 6. The obtained speedup with N=32 nodes here ranges from 9 to 10, with the efficiency between 30% and 40%. While the efficiency is slightly lower than in the case of OpenMP/MPI programs, which could be expected due to a more complex memory hierarchy when dealing with the multi-GPU system distributed over many cluster nodes, the speedup still grows linearly and makes CUDA/MPI programs ideal choice for use on GPU-enabled computer clusters. Additional benefit of using these programs is their low CPU usage (up to one CPU core), allowing for the possibility that same cluster nodes are used for other CPU-intensive simulations. Fig. 6Speedup in the execution time and scaling efficiency of DBEC-GP-MPI-CUDA programs compared to single-node runs of previous CUDA programs [2]: (a) imag3d-mpicuda, (b) real3d-mpicuda. Scaling efficiency is calculated as a fraction of the obtained speedup compared to a theoretical maximum. Grid size used for testing is the same as in Table 3. The introduction of distributed transposes of data creates some overhead, which negatively impacts scaling efficiency. This is more evident in the CUDA/MPI version, as the transpose algorithm is inferior to the one provided by FFTW3. In our tests, both MPI versions of programs failed to achieve speedup on less than 4 nodes, due to the introduction of the transpose routines. We therefore recommend using MPI versions only on 4 or more cluster nodes. The MPI versions are highly dependent not only on the configuration of the cluster, mainly on the speed of interconnect, but also on the distribution of processes and threads, NUMA configuration, etc. We recommend that users experiment with several different configurations to achieve the best performance. The results presented are obtained without extensive tuning, with the aim to show the base performance. Finally, we note that the best performance can be achieved by evenly distributing the workload among the MPI processes and OpenMP threads, and by using grid sizes which are optimal for FFT. In particular, the programs in DBEC-GP-OMP package have the best performance if NX, NY, and NZ are divisible by the number of OpenMP threads used. Similarly, for DBEC-GP-MPI programs the best performance is achieved if NX and NY are divisible by a product of the number of MPI processes and the number of OpenMP threads used. For DBEC-GP-MPI-CUDA programs, the best performance is achieved if NX and NY are divisible by a product of the number of MPI processes and the number of Streaming Multiprocessors (SM) in the GPU used. For all three packages, the best FFT performance is obtained if NX, NY and NZ can be expressed as 2a3b5c7d11e13f, where e and f are either 0 or 1, and the other exponents are non-negative integer numbers [8]. Additional comments, restrictions, and unusual features: MPI programs require that grid size (controlled by input parameters NX, NY and NZ) can be evenly distributed between the processes, i.e., that NX and NY are divisible by the number of MPI processes. Since the data is never distributed along the NZ dimension, there is no such requirement on NZ. Programs will test if these conditions are met, and inform the user if not (by reporting an error). Additionally, MPI versions of CUDA programs require CUDA-aware MPI implementation. This allows the MPI runtime to directly access GPU memory pointers and avoid having to copy the data to main RAM. List of CUDA-aware MPI implementations can be found in Ref. [9]. Acknowledgments V.L., S.Š. and A.B. acknowledge support by the Ministry of Education, Science, and Technological Development of the Republic of Serbia under projects ON171017, OI1611005, and III43007, as well as SCOPES project IZ74Z0-160453. L.E. Y.-S. acknowledges support by the FAPESP of Brazil under project 2012/21871-7 and 2014/16363-8. P.M. acknowledges support by the Science and Engineering Research Board, Department of Science and Technology, Government of India under project No.  EMR/2014/000644. S.K.A. acknowledges support by the CNPq of Brazil under project 303280/2014-0, and by the FAPESP of Brazil under project 2012/00451-0. References:[1]R. Kishor Kumar, L. E. Young-S., D. Vudragović, A. Balaž, P. Muruganandam, and S. K. Adhikari, Comput. Phys. Commun. 195 (2015) 117.[2]V. Lončar, A. Balaž, A. Bogojević, S. Škrbić, P. Muruganandam, S. K. Adhikari, Comput. Phys. Commun. 200 (2016) 406.[3]R. Kishor Kumar, P. Muruganandam, and B. A. Malomed, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 175302;H. Al-Jibbouri, I. Vidanović, A. Balaž, and A. Pelster, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 065303;R. R. Sakhel, A. R. Sakhel, and H. B. Ghassib, J. Low Temp. Phys. 173 (2013) 177;B. Nikolić, A. Balaž, and A. Pelster, Phys. Rev. A 88 (2013) 013624;X. Antoine and R. Duboscq, Comput. Phys. Commun. 185 (2014) 2969;J. Luo, Commun. Nonlinear Sci. Numer. Simul. 19 (2014) 3591;K.-T. Xi, J. Li, and D.-N. Shi, Physica B 436 (2014) 149;S. K. Adhikari, Phys. Rev. A 90 (2014) 055601;M. C. Raportaru, J. Jovanovski, B. Jakimovski, D. Jakimovski, and A. Mishev, Rom. J. Phys. 59 (2014) 677;A. I. Nicolin, A. Balaž, J. B. Sudharsan, and R. Radha, Rom. J. Phys. 59 (2014) 204;A. Balaž, R. Paun, A. I. Nicolin, S. Balasubramanian, and R. Ramaswamy, Phys. Rev. A 89 (2014) 023609;A. I. Nicolin and I. Rata, High-Performance Computing Infrastructure for South East Europe’s Research Communities: Results of the HP-SEE User Forum 2012, in Springer Series: Modeling and Optimization in Science and Technologies 2 (2014) 15;S. K. Adhikari, J. Phys. B: At. Mol. Opt. Phys. 48 (2015) 165303;S. K. Adhikari, Phys. Rev. E 92 (2015) 042926;T. Khellil and A. Pelster, arXiv:1512.04870 (2015);H. L. C. Couto and W. B. Cardoso, J. Phys. B: At. Mol. Opt. Phys. 48 (2015) 025301;R. R. Sakhel, A. R. Sakhel, and H. B. Ghassib, Physica B 478 (2015) 68;L. Salasnich and S. K. Adhikari, Acta Phys. Pol. A 128 (2015) 979;X. Antoine and R. Duboscq, Lecture Notes Math. 2146 (2015) 49;E. Chiquillo, J. Phys. A: Math. Theor. 48 (2015) 475001;S. Sabari, C. P. Jisha, K. Porsezian, and V. A. Brazhnyi, Phys. Rev. E 92 (2015) 032905;W. Wen, T. K. Shui, Y. F. Shan, and C. P. Zhu, J. Phys. B: At. Mol. Opt. Phys. 48 (2015) 175301;P. Das and P. K. Panigrahi, Laser Phys. 25 (2015) 125501;Y. S. Wang, S. T. Ji, Y. E. Luo, and Z. Y. Li, J. Korean. Phys. Soc. 67 (2015) L1504;A. I. Nicolin, M. C. Raportaru, and A. Balaž, Rom. Rep. Phys. 67 (2015) 143;V. S. Bagnato, D. J. Frantzeskakis, P. G. Kevrekidis, B. A. Malomed, and D. Mihalache, Rom. Rep. Phys. 67 (2015) 5;J. B. Sudharsan, R. Radha, H. Fabrelli, A. Gammal, and B. A. Malomed, Phys. Rev. A 92 (2015) 053601;K.-T. Xi, J. Li, and D.-N. Shi, Physica B 459 (2015) 6;E. J. M. Madarassy and V. T. Toth, Phys. Rev. D 91 (2015) 044041;F. I. Moxley III, T. Byrnes, B. Ma, Y. Yan, and W. Dai, J. Comput. Phys. 282 (2015) 303;D. Novoa, D. Tommasini, J. A. Nóvoa-López, Phys. Rev. E 91 (2015) 012904;Y. H. Wang, A. Kumar, F. Jendrzejewski, R. M. Wilson, M. Edwards, S. Eckel, G. K. Campbell, and C. W. Clark, New J. Phys. 17 (2015) 125012;T. Khellil, A. Balaž, and A. Pelster, New J. Phys. 18 (2016) 063003;T. Khellil and A. Pelster, J. Stat. Mech.-Theory Exp. (2016) 063301;J. Akram and A. Pelster, Phys. Rev. A 93 (2016) 023606;S. K. Adhikari, Laser Phys. Lett. 13 (2016) 035502;J. Akram and A. Pelster, Phys. Rev. A 93 (2016) 033610;J. Akram, B. Girodias, and A. Pelster, J. Phys. B: At. Mol. Opt. Phys. 49 (2016) 075302;S. K. Adhikari and S. Gautam, Phys. Rev. A 93 (2016) 013630;Ž. Marojević, E. Göklü, and C. Lämmerzahl, Comput. Phys. Commun. 202 (2016) 216;A. Paredes and H. Michninel, Phys. Dark Universe 12 (2016) 50;J. Akram and A. Pelster, Laser Phys. 26 (2016) 065501;T. Mithun, K. Porsezian, and B. Dey, Phys. Rev. A 93 (2016) 013620;C.-Y. Lai and C.-C. Chien, Phys. Rev. Appl. 5 (2016) 034001;S. K. Adhikari, Laser Phys. Lett. 13 (2016) 085501;K. Manikandan, P. Muruganandam, M. Senthilvelan, and M. Lakshmanan, Phys. Rev. E 93 (2016) 032212;R. R. Sakhel, A. R. Sakhel, H. B. Ghassib, and A. Balaž, Eur. Phys. J. D 70 (2016) 66;W. Bao, Q. Tang, and Y. Zhang, Commun. Comput. Phys. 19 (2016) 1141;R. Kishor Kumar, T. Sriraman, H. Fabrelli, P. Muruganandam, and A. Gammal, J. Phys. B: At. Mol. Opt. Phys. 49 (2016) 155301;A. Bogojević, A. Balaž, and A. Belić, Phys. Rev. E 72 (2005) 036128;A. Bogojević, I. Vidanović, A. Balaž, and A. Belić, Phys. Lett. A 372 (2008) 3341;I. Vidanović, A. Bogojević, A. Balaž, and A. Belić, Phys. Rev. E 80 (2009) 066706;A. Balaž, A. Bogojević, I. Vidanović, and A. Pelster, Phys. Rev. E 79 (2009) 036701;A. Balaž, I. Vidanović, A. Bogojević, and A. Pelster, Phys. Lett. A 374 (2010) 1539;A. I. Nicolin, Physica A 391 (2012) 1062;I. Vasić and A. Balaž, arXiv:1602.03538 (2016);O. Voronych, A. Buraczewski, M. Matuszewski, and M. Stobińska, arXiv:1603.02570 (2016);A. M. Martin, N. G. Marchant, D. H. J. O’Dell, and N. G. Parker, arXiv:1606.07107 (2016).[4]D. Vudragović, I. Vidanović, A. Balaž, P. Muruganandam, and S. K. Adhikari, Comput. Phys. Commun. 183 (2012) 2021.[5]FFTW3 library, http://www.fftw.org/ (2016).[6]2DECOMP&FFT library, http://www.2decomp.org/ (2016).[7]B. Satarić, V. Slavnić, A. Belić, A. Balaž, P. Muruganandam, S. K. Adhikari, Comput. Phys. Commun. 200 (2016) 411. [8]Real-data DFTs with FFTW3, http://www.fftw.org/fftw3_doc/Real_002ddata-DFTs.html (2014);Nvidia’s cuFFT accuracy and performance, http://docs.nvidia.com/cuda/cufft/#accuracy-and-performance (2015).[9]Nvidia’s MPI Solutions for GPUs, https://developer.nvidia.com/mpi-solutions-gpus (2016).","Vladimir Lončar and Luis E. Young-S. and Srdjan Škrbić and Paulsamy Muruganandam and Sadhan K. Adhikari and Antun Balaž",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Expertise psychiatrique : quelle est la responsabilité pénale en cas de consommation de cannabis ?","Résumé
Introduction
La consommation de cannabis est depuis quelques années un problème de santé publique en France, notamment par la démocratisation de cette drogue chez les jeunes. Bien que la législation soit claire en ce qui concerne la responsabilité des personnes sous l’emprise de stupéfiants en cas d’infractions, il n’est pas rare que les personnes soient reconnues irresponsables ou partiellement responsables. L’objectif de cette étude est de déterminer si une consommation de cannabis au moment des faits est retenue par les experts comme un facteur de d’altération ou d’abolition du discernement.
Méthode
Une étude descriptive rétrospective réalisée dans la région PACA auprès de six experts a recueilli 96 expertises pénales en responsabilité pour des infractions commises sous l’emprise de cannabis entre 2016 et 2018. Le critère d’évaluation principal était le discernement, défini soit comme aboli, altéré ou non modifié. Les objectifs secondaires étaient d’observer les facteurs associés à une modification de celui-ci.
Résultats
La consommation de cannabis n’a pas d’impact sur le degré de discernement. La dépendance au cannabis chez l’auteur des faits n’est pas significativement associée à une atteinte du discernement. En revanche, le sexe masculin, l’existence d’un trouble de la personnalité, une immaturité affective, les antécédents psychiatriques, un suivi psychiatrique ou addictologique au moment des faits et une décompensation psychotique sont significativement associés à une atteinte du discernement.
Conclusion
Ces résultats soulignent l’importance des mesures de prévention et de réduction des risques liés au cannabis, notamment dans ce contexte de légalisation ou dépénalisation du cannabis.
Objectives
In recent years, cannabis use has been a public health problem in France, particularly through the democratization of this drug among young people. The impact of chronic cannabis use on impulsivity and risk taking therefore questions the role of cannabis in offenses. Although the legislation is clear as regards the liability of persons under the influence of drugs in the event of an offense, it is not unusual for individuals to be found irresponsible or partially responsible. The purpose of this study is to determine whether cannabis use at the time of the facts is retained by the experts as a factor of alteration or abolition of discernment, but also to identify the elements associated with an impairment of discernment.
Patients or materials and methods
A retrospective descriptive study conducted in the PACA region with 6 experts collected 96 criminal appraisals of responsibility for offenses committed under the influence of cannabis between 2016 and 2018. The inclusion criteria include the age of the alleged perpetrators that had to be between 18 and 65 years old and cannabis intoxication at the time of the facts. The expert reports for which the expert did not mention a possible cannabis intoxication at the time of the events but which he emphasized a significant daily consumption were included. The criterion of non-inclusion was the impossibility for the expert to pronounce on the existence or not of a psychic or neuropsychic disorder having abolished or impaired the discernment or control of the acts. Appraisals for which no use of cannabis at the time of the facts was found were excluded, likewise for the expertises whose question of discernment at the time of the facts according to article 122-1 of the Penal Code was not raised. (postsentencing expertise). The primary outcome measure was to determine whether or not the alleged perpetrator is responsible for the alleged acts. It then corresponds to the expert's conclusion concerning the question of the discernment and control of acts at the material time. Thus, three possible answers were obtained: discernment abolished, discernment altered or unmodified discernment. The secondary objectives were to determine the factors that may have an impact on the assessment of judgment and thus on criminal responsibility. Secondary outcomes included biographical information (age, sex, geography, placement, abuse or rape in childhood, parenthood, marital status, socioprofessional category), personal psychiatric history (personality disorder, bipolar disorder, and psychotic disorder, acute delirium and cannabic psychosis, impulsivity) and family psychiatric history. A criminal record as a victim but especially as an author was sought. If so, is the subject subject to or has been sentenced to socio-judicial follow-up (care ordered by the penitentiary, followed by Prison and Probation Services (SPIP), electronic surveillance)? Is the act currently being reproached committed in a state of legal recidivism? Finally, the toxicological profile was analyzed to determine whether cannabis use at the time of the event occurred in a context of cannabis addiction or harmful use, but also to assess concomitant use of other illicit substances or alcohol at the time facts.
Results
Associations were searched for bivariate analysis (Chi2 test or Fisher's exact test) and then multivariate analysis (logistic regression). The variables selected for the multivariate analysis are those for which P<0.05 in bivariate analysis. Discernment was abolished in 6 expert reports (6.25%), impaired in 19 expert reports (19.79%) and unmodified in 71 reports (73.96%). Cannabis dependence accounted for 81.25% of the sample and the harmful use of cannabis for 18.75% of the sample. Among the individuals with a cannabis addiction, the experts concluded that discernment was abolished in 4.17% of cases, discernment was impaired in 17.71% of cases, and there was no discernment or abolition of discernment. 59.38% of cases. In the case of harmful use of cannabis, the results were respectively 14.58%, 2.08% and 2.08%. The cannabis poisoning at the time of the facts is not therefore retained by the experts as a factor of alteration or abolition of discernment. An impairment of discernment was significantly associated with the presence of psychiatric history (P=6.973.10 ^ -5), psychiatric or addictological management (P=0.017), personality disorder (P=0.032), emotional immaturity (P=0.022) and psychiatric comorbidity (P=1.194.10 ^ -5) at the time of the events. Age was also a factor correlated with a change in discernment (P=0.001). However, in multivariate analysis, only age (P=0.017, OR=1.113, 95% CI [1.016, 1.175]) and the existence of psychiatric pathology at the time of the events (P=0.004, OR=1.262.10–1 95% CI [2966; 6956.10–1]) were statistically significant.
Conclusion
The results of the study are in line with those expected from the literature. Psychiatric co-morbidities at the time of the events and in particular psychoses (decompensation of schizophrenia and paranoid delusion) are the first causes of abolition of the discernment, with 2/3 of the people addicted to cannabis. With regard to the causes of discernment, psychoses are still strongly represented, but to these are added, among others, personality disorders (antisocial and borderline personality disorder) and intellectual deficit. It is important to note in this category, however, the high proportion of cannabis addicts (41.67%) and addictive comorbidities (33%). The vast majority of those whose discernment was neither altered nor abolished at the time of the incident, suffered no psychiatric pathology at the time of the offense. Where appropriate, the experts primarily diagnosed antisocial and borderline personality disorders. These results underline the importance of prevention and risk reduction measures related to cannabis, particularly in this context of legalization or decriminalization of cannabis.","Mariana Samuel and Michel Benoit and Nidal Nabhan Abou",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Tailoring Diabetes Education to Meet the Needs of Adults With Type 2 Diabetes and Mental Illness: Client and Health-Care Provider Perspectives From an Exploratory Pilot Study","Objectives
People with mental illness are more likely to experience poorer outcomes with type 2 diabetes than the general population. Diabetes management can be improved when lifestyle-intervention content is tailored to the learning needs of individuals or groups. The purpose of this pilot study was to explore the perspectives of clients and providers involved with mental health care with regard to how diabetes education can effectively address the challenges that may be faced when people with mental illness engage in diabetes self-care behaviours.
Methods
Focus groups included 17 people with mental illness and type 2 diabetes and 21 mental health clinicians. Data were transcribed verbatim, assessed for quality and saturation and coded to identify relationships and meanings among identified themes.
Results
Participants described strategies concerning how to consider symptoms of mental illness and address the psychosocial challenges that people with mental illness may be more likely to experience. Teaching strategies identified by clinicians and clients that were perceived to be effective included allowing clients to guide education session content, and being flexible when providing support, identifying education topics to discuss and teaching about diabetes. Participants also emphasized the importance of empowering clients by helping them to see how sustainable behaviour changes can be achieved. Differences between the perspectives of the clients receiving mental health care and the clinicians were often related to neglecting to begin with client-driven needs assessments.
Conclusions
Our study offers diabetes educators a strategy for applying Diabetes Canada's self-management education guidelines to the needs of people with mental illness by using suggestions from clients and clinicians.
Résumé
Objectifs
Les personnes atteintes d'une maladie mentale sont plus susceptibles que la population générale de connaître de moins bons résultats si elles sont atteintes de diabète de type 2. Il est possible d'avoir une meilleure prise en charge du diabète lorsque le contenu de l'intervention sur le mode de vie est adapté aux besoins d'apprentissage des individus ou des groupes. L'objectif de la présente étude pilote était d'examiner les points de vue des clients et des prestataires impliqués dans les soins de santé mentale en ce qui concerne la façon dont l'enseignement sur le diabète peut aborder efficacement les difficultés à relever lorsque les personnes atteintes d'une maladie mentale adoptent des comportements d'autosoins liés au diabète.
Méthodes
Les groupes de discussion comptaient 17 personnes atteintes d'une maladie mentale et du diabète de type 2, et 21 cliniciens en santé mentale. Les données étaient transcrites textuellement, faisaient l'objet d'une évaluation de la qualité et de la saturation, et étaient codées pour cerner les liens et les significations parmi les thèmes retenus.
Résultats
Les participants décrivaient les stratégies sur la façon d'envisager les symptômes de santé mentale et d'aborder les difficultés psychosociales que les personnes atteintes d'une maladie mentale sont plus susceptibles de connaître. Les stratégies d'enseignement perçues comme efficaces que les cliniciens et les clients avaient déterminées étaient de permettre aux clients de suggérer le contenu des séances d'enseignement, de faire preuve de souplesse lors qu'ils offrent du soutien, de déterminer les sujets d'enseignement à traiter et d'offrir un enseignement sur le diabète. Les participants soulignaient également l'importance de rendre les clients autonomes en les aidant à découvrir comment atteindre des changements de comportements durables. Les différences entre les points de vue des clients et des cliniciens en soins de santé mentale étaient souvent liées à l'omission de faire des évaluations initiales des besoins axés sur les clients.
Conclusions
Notre étude fournit aux éducateurs en diabète une stratégie d'application des lignes directrices en matière d'enseignement sur les autosoins de Diabète Canada aux besoins des personnes atteintes d'une maladie mentale en tenant compte des suggestions des clients et des cliniciens.","Adriana Cimo and Carolyn S. Dewa",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Maintenance of a Physically Active Lifestyle After Pulmonary Rehabilitation in Patients With COPD: A Qualitative Study Toward Motivational Factors","Objectives
To explore determinants of behavior change maintenance of a physically active lifestyle in patients with chronic obstructive pulmonary disease (COPD) 8–11 months after completion of a 4-month outpatient pulmonary rehabilitation program.
Design
A qualitative descriptive study of semistructured interviews.
Setting
Pulmonary rehabilitation assessment center.
Participants
Patients with COPD.
Measurements
Semistructured interviews until data saturation, coded by 2 independent researchers. Patients were classified as responder (maintenance or improvement) or nonresponder (relapse or decrease), based on 3 quantitative variables reflecting exercise capacity (Constant Work Rate Test), health-related quality of life (Short-Form health survey [SF-36]), and self-management abilities (Self-Management Ability Scale [SMAS-30/Version 2]).
Results
Mean (SD) forced expiratory volume in the first second (FEV1) among interviewees was 52.5% (14.4%) predicted and the mean age was 63.5 years (range: 45–78). The group consisted of 15 responders and 7 nonresponders. Physical limitations reduced competence to engage in an active lifestyle and responders appeared to experience higher levels of perceived competence. Social support was found important and the experienced understanding from fellow patients made exercising together enjoyable. Particularly, responders expressed autonomous motivation and said they exercised because of the benefits they gain from it. Unexpectedly, only responders also experienced controlled motivation.
Conclusion
Perceived competence and autonomous motivation are important determinants for maintenance of an active lifestyle in patients with COPD. In contrast to common theoretical assumptions, a certain threshold level of controlled motivation may remain important in maintaining a physically active lifestyle after a pulmonary rehabilitation program.","Kelly F.J. Stewart and Jessie J.M. Meis and Coby van de Bool and Daisy J.A. Janssen and Stef P.J. Kremers and Annemie M.W.J. Schols",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Improved algorithm for calculating the Chandrasekhar function","Theoretical models of electron transport in condensed matter require an effective source of the Chandrasekhar H(x,omega) function. A code providing the H(x,omega) function has to be both accurate and very fast. The current revision of the code published earlier [A. Jablonski, Comput. Phys. Commun. 183 (2012) 1773] decreased the running time, averaged over different pairs of arguments x and omega, by a factor of more than 20. The decrease of the running time in the range of small values of the argument x, less than 0.05, is even more pronounced, reaching a factor of 30. The accuracy of the current code is not affected, and is typically better than 12 decimal places.
New version program summary
Program title: CHANDRAS_v2 Catalogue identifier: AEMC_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEMC_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 976 No. of bytes in distributed program, including test data, etc.: 11416 Distribution format: tar.gz Programming language: Fortran 90 Computer: Any computer with a Fortran 90 compiler Operating system: Windows 7, Windows XP, Unix/Linux RAM: 0.7 MB Classification: 2.4, 7.2 Catalogue identifier of previous version: AEMC_v1_0 Journal reference of previous version: Comput. Phys. Commun. 183 (2012) 1773 Does the new version supersede the old program: Yes Nature of problem: An attempt has been made to develop a subroutine that calculates the Chandrasekhar function with high accuracy, of at least 10 decimal places. Simultaneously, this subroutine should be very fast. Both requirements stem from the theory of electron transport in condensed matter. Solution method: Two algorithms were developed, each based on a different integral representation of the Chandrasekhar function. The final algorithm is edited by mixing these two algorithms by selecting ranges of the argument omega in which the performance is the fastest. Reasons for the new version: Some of the theoretical models describing electron transport in condensed matter need a source of the Chandrasekhar H function values with an accuracy of at least 10 decimal places. Additionally, calculations of this function should be as fast as possible since frequent calls to a subroutine providing this function are made (e.g., numerical evaluation of a double integral with a complicated integrand containing the H function). Both conditions were satisfied in the algorithm previously published [1]. However, it has been found that a proper selection of the quadrature in an integral representation of the Chandrasekhar function may considerably decrease the running time. By suitable selection of the number of abscissas in Gauss–Legendre quadrature, the execution time was decreased by a factor of more than 20. Simultaneously, the accuracy of results has not been affected. Summary of revisions: (1) As in previous work [1], two integral representations of the Chandrasekhar function, H(x,omega), were considered: the expression published by Dudarev and Whelan [2] and the expression published by Davidović et al. [3]. The algorithms implementing these representations were designated A and B, respectively. All integrals in these implementations were previously calculated using Romberg quadrature. It has been found, however, that the use of Gauss–Legendre quadrature considerably improved the performance of both algorithms. Two conditions have to be satisfied. (i) The number of abscissas, N, has to be rather large, and (ii) the abscissas and corresponding weights should be determined with accuracy as high as possible. The abscissas and weights are available for N=16, 20, 24, 32, 40, 48, 64, 80, and 96 with accuracy of 20 decimal places [4], and all these values were introduced into a new procedure GAUSS replacing procedure ROMBERG. Due to the fact that the implemented tables are rather extensive, they were recalculated using the Rybicki algorithm (Ref. [5], pp. 183–184) and rechecked. No errors or misprints were found. (2) In the integral representation of the H function derived by Davidović et al. [3], the positive root ν0 of the so-called dispersion function needs to be calculated with accuracy of at least 10 decimal places (see. Ref [6], pp. 61–64 and Ref. [1], Eqs. (5) and (29)). For small values of the argument omega and values of omega close to unity, the nonlinear equation in one unknown, ν0, can be solved analytically. New simple analytical expressions were derived here that can be efficiently used in calculations of the root. (3) The above modifications of the code considerably decreased the time of calculation of both algorithms A and B. The results are summarized in Fig. 1. The time of calculations is in fact the CPU time in microseconds for a computer equipped with an Inter Xeon processor (3.46 GHz) using Lahey–Fujitsu Fortran v. 7.2. Fig. 1Time of calculations of the H(x,omega) function averaged over different pairs of arguments x and omega. (a) 400 pairs uniformly distributed in the ranges 0<=x<=0.05 and 0<=omega<=1; (b) 400 pairs uniformly distributed in the ranges 0.05<=x<=1 and 0<=omega<=1. The shortest execution time averaged over values of the argument x exceeding 0.05 has been observed for algorithm B and Gauss–Legendre quadrature with the number of abscissas equal to 64 (23.2 μs). As compared with Romberg quadrature, the execution time was shortened by a factor of 22.5. For small x values, below 0.05, both algorithms A and B are considerably faster if Gauss–Legendre quadrature is used. For N=64, the average time of execution of algorithm B is decreased with respect to Romberg quadrature by a factor close to 30. However, in that range of argument x, algorithm A exhibits much faster performance. Furthermore, the average execution time of algorithm A, equal to about 100 μs, is practically independent of the number of abscissas N. (4) For Romberg quadrature, to optimize the performance, the mixed algorithm C was proposed in which algorithm A is used for argument x smaller than or equal to x0=0.4, while algorithm B is used for x larger than 0.4 [1]. For Gauss–Legendre quadrature, the limit x0 was found to depend on the number of abscissas N. For each value of N considered, the time of calculations of the H function was determined for pairs of arguments uniformly distributed in the ranges 0<=x<=0.05 and 0<=omega<=1, and for pairs of arguments uniformly distributed in the ranges 0.05<=x<=1 and 0<=omega<=1. As shown in Fig. 2 for N=64, algorithm A is faster than algorithm B for x smaller than or equal to 0.0225. Fig. 2Comparison of the running times of algorithms A and B. Open circles: algorithm B is faster than the algorithm A; full circles: algorithm A is faster than algorithm B. Thus, the value of x0=0.0225 is proposed for the mixed algorithm C when Gauss–Legendere quadrature with N=64 is used. Similar computer experiments performed for other values of N are summarized below. LNx01160.252200.153240.104320.0505400.0306480.0457640.0225–Recommended8800.01259960.020 The flag L is one of the input parameters for the subroutine GAUSS. In the programs implementing algorithms A, B, and C (CHANDRA, CHANDRB, and CHANDRC), Gauss–Legendre quadrature with N=64 is currently set. As follows from Fig. 1, algorithm B (and consequently algorithm C) is the fastest in that case. It is still possible to change the number of abscissas; the flag L then has to be modified in lines 165, 169, 185, 189, and 304 of program CHANDRAS_v2, and the value of x0 in line 111 has to be adjusted according to the table above. (5) The above modifications of the code did not affect the accuracy of the calculated Chandrasekhar function, as compared to the original code [1]. For the pairs of arguments shown in Fig. 2, the accuracy of the H function, calculated from algorithms A and B, reached at least 12 decimal digits; however, in the majority of cases, the accuracy is equal to 13 decimal digits. Restrictions: Two input parameters for the Chandrasekhar function, x and omega, are restricted to the ranges 0<=x<=1 and 0<=omega<=1, which is sufficient in numerous applications. Running time: between 15 and 100 μs for one pair of arguments of the Chandrasekhar function. References:[1]A. Jablonski, Comput. Phys. Comm. 183 (2012) 1773.[2]S.L. Dudarev, M.J. Whelan, Surf. Sci. 311 (1994) L687.[3]D.M. Davidović, J. Vukanić, D. Arsenović, Icarus 194 (2008) 389.[4]M. Abramowitz, I.A. Stegun (Eds.), Handbook of Mathematical Functions, Dover Publications, Inc., New York, 1972, pp. 916–919.[5]W. H. Press, S. A. Teukolsky, W. T. Vetterling, B. P. Flannery, Numerical recipes, in: The Art of Scientific Computing, Cambridge University Press, Cambridge, 2007.[6]K.M. Case, P.F. Zweifel, Linear Transport Theory, Addison-Wesley, Reading, MA, 1967, pp. 61–65.","A. Jablonski",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A method for cohort selection of cardiovascular disease records from an electronic health record system","Introduction
An electronic healthcare record (EHR) system, when used by healthcare providers, improves the quality of care for patients and helps to lower costs. Information collected from manual or electronic health records can also be used for purposes not directly related to patient care delivery, in which case it is termed secondary use. EHR systems facilitate the collection of this secondary use data, which can be used for research purposes like observational studies, taking advantage of improvement in the structuring and retrieval of patient information. However, some of the following problems are common when conducting a research using this kind of data: (i) Over time, systems and data storage methods become obsolete; (ii) Data concerns arise since the data is being used in a context removed from its original intention; (iii) There are privacy concerns when sharing data about individual subjects; (iv) The partial availability of standard medical vocabularies and natural language processing tools for non-English language limits information extraction from structured and unstructured data in the EHR systems. A systematic approach is therefore needed to overcome these, where local data processing is performed prior to data sharing.
Method
The proposed study describes a local processing method to extract cohorts of patients for observational studies in four steps: (1) data reorganization from an existing local logical schema into a common external schema over which information can be extracted; (2) cleaning of data, generation of the database profile and retrieval of indicators; (3) computation of derived variables from original variables; (4) application of study design parameters to transform longitudinal data into anonymized data sets ready for statistical analysis and sharing. Mapping from the local logical schema into a common external schema must be performed differently for each EHR and is not subject of this work, but step 2, 3 and 4 are common to all EHRs. The external schema accepts parameters that facilitate the extraction of different cohorts for different studies without having to change the extraction algorithms, and ensures that, given an immutable data set, can be done by the idempotent process. Statistical analysis is part of the process to generate the results necessary for inclusion in reports. The generation of indicators to describe the database allows description of its characteristics, highlighting study results. The set extraction/statistical processing is available in a version controlled repository and can be used at any time to reproduce results, allowing the verification of alterations and error corrections. This methodology promotes the development of reproducible studies and allows potential research problems to be tracked upon extraction algorithms and statistical methods
Results
This method was applied to an admissions database, SI3, from the InCor-HCFMUSP, a tertiary referral hospital for cardiovascular disease in the city of São Paulo, as a source of secondary data with 1116848 patients records from 1999 up to 2013. The cleaning process resulted in 313894 patients records and 27698 patients in the cohort selection, with the following criteria: study period: 2003–2013, gender: Male, Female, age:≥18years old, at least 2 outpatient encounters, diagnosis of cardiovascular disease (ICD-10 codes: I20-I25, I64-I70 and G45). An R script provided descriptive statistics of the extracted cohort.
Conclusion
This method guarantees a reproducible cohort extraction for use of secondary data in observational studies with enough parameterization to support different study designs and can be used on diverse data sources. Moreover it allows observational electronic health record cohort research to be performed in a non-English language with limited international recognized medical vocabulary.","Maria Tereza Fernandes Abrahão and Moacyr Roberto Cuce Nobre and Marco Antonio Gutierrez",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Running-induced epigenetic and gene expression changes in the adolescent brain","Physical exercise is associated with positive neural functioning. Here we examined the gene expression consequences of 1 week of voluntary wheel running in adolescent male mice. We assayed expression levels of genes associated with synaptic plasticity, signaling pathways, and epigenetic modifying enzymes. Two regions were examined: the hippocampus, which is typically examined in exercise studies, and the cerebellum, an area directly involved in motor control and learning. After 1 week of exercise, global acetylation of histone 3 was increased in both brain regions. Interestingly this was correlated with increased brain derived neural growth factor in the hippocampus, as noted in many other studies, but only a trend was found in cerebellum. Differences and similarities between the two areas were noted for genes encoding functional proteins. In contrast, the expression pattern of DNA methyltransferases (Dnmts) and histone deacetylases (Hdacs), genes that influence DNA methylation and histone modifications in general, decreased in both regions with exercise. We hypothesize that epigenetic mechanisms, involving many of the genes assessed here, are essential for the positive affects of exercise on behavior and suspect these data have relevance for adolescent boys.","Jean LeBeau Abel and Emilie F. Rissman",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Macrofaunal production and biological traits: Spatial relationships along the UK continental shelf","Biological trait analysis (BTA) is increasingly being employed to improve our understanding of the ecological functioning of marine benthic invertebrate communities. However, changes in trait composition are seldomly compared with concomitant changes in metrics of ecological function. Consequently, inferences regarding the functional implications of any changes are often anecdotal; we currently have a limited understanding of the functional significance of the traits commonly used. In this study, we quantify the relationship between benthic invertebrate trait composition and secondary production estimates using data spanning almost the breadth of the UK continental shelf. Communities described by their composition of 10 traits representing life history, morphology and behaviour showed strong relationships with variations in total secondary production. A much weaker relationship was observed for community productivity (or P:B), a measure of rate of energy turnover. Furthermore, the relationship between total production and multivariate taxonomic community composition was far weaker than that for trait composition. Indeed, the similarities between communities as defined by taxonomy were very different from those depicted by their trait composition. That is, as many studies have demonstrated, taxonomically different communities may display similar trait compositions, and vice versa. Finally, we found that descriptions of community trait composition vary greatly depending on whether abundance or biomass is used as the enumeration weighting method during BTA, and trait assessments based on biomass produced better relations with secondary production than those based on abundance. We discuss the significance of these findings with respect to BTA using marine benthic invertebrates.","S.G. Bolam and J.D. Eggleton",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Seismic performance of buildings with thin RC bearing walls","In Latin America there is an urgent need for housing; thus, during the last few years a relevant number of mid-height buildings (usually, up to five stories) with thin RC shear–walls have been constructed for low-cost dwellings in Bolivia, Colombia, Ecuador, Mexico, Peru, Venezuela, and other countries located in seismic-prone regions. These walls are 10cm thick and their reinforcement consists mainly of a single layer of welded wire mesh. This construction technology offers two main advantages: economy and rapidity of construction. These buildings do not fulfill the international seismic codes but some national regulations are less demanding, not preventing the use of thin bearing walls. These buildings might be vulnerable to earthquakes because of their low ductility, the insufficiency of the experimental information, the absence of observed damages and, in some cases, poor construction quality. This work describes the initial steps of a wider research aiming at providing reliable seismic design guidelines for thin-wall buildings; the initial objectives are analyzing the seismic performance of these buildings, proposing preliminary design criteria and identifying further research needs. This research focuses on buildings located in Peru, being representative of the situations in the other countries. The vulnerability is numerically evaluated by push-over and nonlinear time history analyses; the structural parameters are obtained from available testing information. The obtained results show that the seismic strength of the analyzed buildings is insufficient; however, minor changes in the structural design might improve significantly their seismic performance. Cheap and easy-to-implement design recommendations are issued.","H. Gonzales and F. López-Almansa",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A 3-D model for void evolution in viscous materials under large compressive deformation","The paper presents a study on the evolution of dilute ellipsoidal voids in power-law viscous materials under triaxial loading condition. Firstly, referring to the work of Eshelby (1957), a semi-analytical expression is deduced to evaluate the deformation of ellipsoidal void in linear viscous material. Then, for the non-linear viscous materials, the concept of mesoscopic representative volume element (RVE) is applied to study the voids deformation under different stress states, and a rigid visco-plastic finite element (FE) procedure is applied to solve the RVE model. For the condition of stress triaxiality ranging from −1 to +1, it is found that the voids deformation behaves similarly in both linear and non-linear viscous materials. Due to this fact, the framework of the expression of void deformation in linear viscous material is inferred to describe the void evolution in non-linear viscous materials, while the parameters of the expression are re-evaluated for the specific materials. The results show that the void shapes and loading conditions take important roles in the void evolution. Therefore, for an ellipsoidal void, the void radius strain rate is expressed as a function of the void shape index, the macroscopic stress and strain-rate. Meanwhile, the void volume strain rate is obtained as a function of the void radius strain rate. This void evolution model is integrated into FE code and applied to study the void closure problem in the metal forming process. The FE simulation provides the evolution of macroscopic stress, strain and strain-rate, and then the model is used to calculate the changes of void shape and volume in each step of the deformation history. It can be found that the results predicted by this model agree well with the analytical solution, experiment measurements and numerical simulations with embedded void shapes, which demonstrates that this method can be appropriately used to predict the void evolution during the large compressive deformation process.","Chao Feng and Zhenshan Cui",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Assessment for innovative combustion on HCCI engine by controlling EGR ratio and engine speed","HCCI (homogeneous charge compression ignition) combustion offers both high efficiency and very low NOx and particulate matter emissions. However, the operating range of HCCI engines is limited by an excessive PRR (pressure rise rate) in high load region, which is the main reason of engine knock. For this problem shown in HCCI combustion, ignition timing should be retarded after TDC (top dead center) by controlling gas temperature properly, and in addition, combustion duration also needs to be ensured adequately. EGR (exhaust gas recirculation) gas is known to have impacts on the history of gas temperature, and the objective of this study is to investigate the influence of EGR ratio on combustion characteristics. The computational modeling work is conducted by using a single-zone code with detailed chemical kinetics. Then, in order to investigate the influence of EGR ratio and the engine speed, contribution matrix has been used to extract important reaction paths from a reaction mechanism.","Mina Nishi and Masato Kanehara and Norimasa Iida",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A mitogenomic phylogeny and genetic history of sable (Martes zibellina)","We assessed phylogeny of sable (Martes zibellina, Linnaeus, 1758) by sequence analysis of nearly complete, new mitochondrial genomes in 36 specimens from different localities in northern Eurasia (Primorye, Khabarovsk and Krasnoyarsk regions, the Kamchatka Peninsula, the Kuril Islands and the Urals). Phylogenetic analysis of mtDNA sequences demonstrates that two clades, A and BC, radiated about 200–300thousandyears ago (kya) according to results of Bayesian molecular clock and RelTime analyses of different mitogenome alignments (nearly complete mtDNA sequences, protein-coding region, and synonymous sites), while the age estimates of clades A, B and C fall within the Late Pleistocene (~50–140kya). Bayesian skyline plots (BSPs) of sable population size change based on analysis of nearly complete mtDNAs show an expansion around 40kya in the warm Karganian time, without a decline of population size around the Last Glacial Maximum (21kya). The BSPs based on synonymous clock rate indicate that M. zibellina experienced demographic expansions later, approximately 22kya. The A2a clade that colonized Kamchatka ~23–50kya (depending on the mutation rate used) survived the last glaciation there as demonstrated by the BSP analysis. In addition, we have found evidence of positive selection acting at ND4 and cytochrome b genes, thereby suggesting adaptive evolution of the A2a clade in Kamchatka.","Boris Malyarchuk and Miroslava Derenko and Galina Denisova",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Incentive effect on inhibitory control in adolescents with early-life stress: An antisaccade study","Objective
Early-life stress (ES) such as adoption, change of caregiver, or experience of emotional neglect may influence the way in which affected individuals respond to emotional stimuli of positive or negative valence. These modified responses may stem from a direct alteration of how emotional stimuli are coded, and/or the cognitive function implicated in emotion modulation, such as self-regulation or inhibition. These ES effects have been probed on tasks either targeting reward and inhibitory function. Findings revealed deficits in both reward processing and inhibitory control in ES youths. However, no work has yet examined whether incentives can improve automatic response or inhibitory control in ES youths.
Method
To determine whether incentives would only improve self-regulated voluntary actions or generalize to automated motoric responses, participants were tested on a mixed eye movement task that included reflex-like prosaccades and voluntary controlled antisaccade eye movements. Seventeen adopted children (10 females, mean age 11.3years) with a documented history of neglect and 29 typical healthy youths (16 females, mean age 11.9years) performed the mixed prosaccade/antisaccade task during monetary incentive conditions or during no-incentive conditions.
Results
Across both saccade types, ES adolescents responded more slowly than controls. As expected, control participants committed fewer errors on antisaccades during the monetary incentive condition relative to the no-incentive condition. By contrast, ES youths failed to show this incentive-related improvement on inhibitory control. No significant incentive effects were found with prepotent prosaccades trials in either group. Finally, co-morbid psychopathology did not modulate the findings.
Conclusions
These data suggest that youths with experience of early stress exhibit deficient modulation of inhibitory control by reward processes, in tandem with a reward-independent deficit in preparation for both automatic and controlled responses. These data may be relevant to interventions in ES youths.","Sven C. Mueller and Michael G. Hardin and Katherine Korelitz and Teresa Daniele and Jessica Bemis and Mary Dozier and Elizabeth Peloso and Francoise S. Maheu and Daniel S. Pine and Monique Ernst",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A Systematic Review of the Mysterious Caterpillar Fungus Ophiocordyceps sinensis in DongChongXiaCao (冬蟲夏草 Dōng Chóng Xià Cǎo) and Related Bioactive Ingredients","ABSTRACT
The caterpillar fungus Ophiocordyceps sinensis (syn.††The term “Cordyceps sinensis” has been renamed to its synonym “Ophiocordyceps sinensis” by Sung et al. in 2007. In the discussion, “Cordyceps sinensis” is still used to represent “Ophiocordyceps sinensis” out of respect to the original authors of the articles that we cited.Cordyceps sinensis), which was originally used in traditional Tibetan and Chinese medicine, is called either “yartsa gunbu” or “DongChongXiaCao (冬蟲夏草 Dōng Chóng Xià Cǎo)” (“winter worm-summer grass”), respectively. The extremely high price of DongChongXiaCao, approximately USD $20,000 to 40,000 per kg, has led to it being regarded as “soft gold” in China. The multi-fungi hypothesis has been proposed for DongChongXiaCao; however, Hirsutella sinensis is the anamorph of O. sinensis. In Chinese, the meaning of “DongChongXiaCao” is different for O. sinensis, Cordyceps spp.,‡‡Cordyceps spp. indicates any species that belongs to the genus Cordyceps. and Cordyceps spƒƒCordyceps sp. indicates the unidentified species that belong to the genus Cordyceps. Over 30 bioactivities, such as immunomodulatory, antitumor, anti-inflammatory, and antioxidant activities, have been reported for wild DongChongXiaCao and for the mycelia and culture supernatants of O. sinensis. These bioactivities derive from over 20 bioactive ingredients, mainly extracellular polysaccharides, intracellular polysaccharides, cordycepin, adenosine, mannitol, and sterols. Other bioactive components have been found as well, including two peptides (cordymin and myriocin), melanin, lovastatin, γ-aminobutyric acid, and cordysinins. Recently, the bioactivities of O. sinensis were described, and they include antiarteriosclerosis, antidepression, and antiosteoporosis activities, photoprotection, prevention and treatment of bowel injury, promotion of endurance capacity, and learning-memory improvement. H. sinensis has the ability to accelerate leukocyte recovery, stimulate lymphocyte proliferation, antidiabetes, and improve kidney injury. Starting January 1st, 2013, regulation will dictate that one fungus can only have one name, which will end the system of using separate names for anamorphs. The anamorph name “H. sinensis” has changed by the International Code of Nomenclature for algae, fungi, and plants to O. sinensis.","Hui-Chen Lo and Chienyan Hsieh and Fang-Yi Lin and Tai-Hao Hsu",2013,"[""Science Direct""]","Rejeitado: CR4","Rejeitado: CR9"
"Gravitating to rigidity: Patterns of schema evolution – and its absence – in the lives of tables","Like all software maintenance, schema evolution is a process that can severely impact the lifecycle of a data-intensive software projects, as schema updates can drive depending applications crushing or delivering incorrect data to end users. In this paper, we study the schema evolution of eight databases that are part of larger open source projects, publicly available through open source repositories. In particular, the focus of our research was the understanding of which tables evolve and how. We report on our observations and patterns on how evolution related properties, like the possibility of deletion, or the amount of updates that a table undergoes, are related to observable table properties like the number of attributes or the time of birth of a table. A study of the update profile of tables, indicates that they are mostly rigid (without any updates to their schema at all) or quiet (with few updates), especially in databases that are more mature and heavily updated. Deletions are significantly outnumbered by table insertions, leading to schema expansion. Delving deeper, we can highlight four patterns of schema evolution. The Γ pattern indicating that tables with large schemata tend to have long durations and avoid removal, the Comet pattern indicating that the tables with most updates are the ones with medium schema size, the Inverse Γ pattern, indicating that tables with medium or small durations produce amounts of updates lower than expected, and, the Empty Triangle pattern indicating that deletions involve mostly early born, quiet tables with short lives, whereas older tables are unlikely to be removed. Overall, we believe that the observed evidence strongly indicates that databases are rigidity-prone rather than evolution-prone. We call the phenomenon gravitation to rigidity and we attribute it to the implied impact to the surrounding code that a modification to the schema of a database has.","Panos Vassiliadis and Apostolos V. Zarras and Ioannis Skoulis",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Tracking sea turtle hatchlings — A pilot study using acoustic telemetry","Understanding the movements of turtle hatchings is essential for improved understanding of dispersal behaviour and ultimately survivorship, life history strategies and population connectivity. Yet investigation of in-water movement has been hampered by the small size of hatchlings relative to the size of available tracking technologies. This has resulted in the use of labour intensive visual tracking methods, or active tracking methods with high transmitter to body weight ratios. These methods are confounded by the presence of the observer, the size of the tag, usual small treatment sample sizes and studies that are constrained to daylight hours when turtles hatch predominantly at night. Passive acoustic monitoring using new miniature tags can overcome these limitations. We tested the effectiveness of active and passive acoustic tracking in monitoring turtle hatchling movement in order to measure the influence of artificial light on newly hatched turtles once they enter the water. A Vemco VR2W Positioning System (VPS) comprising an array of 18 VR2W receivers was deployed in the surf zone to detect signals from acoustic-coded transmitters (1.14±0.06% of body mass) attached to 26 flatback turtle hatchlings released into the array. A total of 1328 detections were recorded for 22 hatchlings with turtles spending a mean of 16.63±5.89min in the array. The test detection range for this technology in the surf-zone was 50–100m and was influenced by wave noise and shallow deployment. Cyclonic conditions hampered the experiment and resulted in an inconclusive test of light effects. Three additional instrumented flatback hatchlings were followed in a small boat using a mobile acoustic receiver and directional hydrophone up to 2km from shore. Passive acoustic monitoring is a viable technology for tracking small marine animals and removes many of the confounding effects of other telemetry methods. It has great potential to examine natural and anthropogenic factors influencing orientation and behaviour during a crucial stage in turtle life history — their initial movement from the beach through predator-rich, near shore waters. While the data obtained by passive acoustic monitoring is limited in its spatio-temporal coverage, being constrained by the size of the array, active acoustic tracking can be applied over larger scales. Such studies will be particularly important for assessing the impacts of anthropogenic pressures that have changed the natural light, noise or wave environments and for providing behavioural data to improve and validate bio-physical models of the migration and dispersal of young turtles.","Michele Thums and Scott D. Whiting and Julia W. Reisser and Kellie L. Pendoley and Chari B. Pattiaratchi and Robert G. Harcourt and Clive R. McMahon and Mark G. Meekan",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Beyond contrastive analysis and codeswitching: Student documentary filmmaking as a challenge to linguicism in Hawai‘i","This article discusses a documentary film project33The project was made possible with a grant from the Hawai‘i Council for the Humanities. produced by high school students in Hawai‘i that investigated the value of Pidgin (Hawai‘i Creole) in schools and society, and which ultimately aimed to address the problem of linguicism (Skutnabb-Kangas, 1990). The project was carried out within a critical language awareness framework that treated students as knowledge producers and which provided them with the opportunity to use their own communities and languages as repositories of knowledge and as sites for learning about the relationship between language and society. Through exploring the meanings and values of their language, the students produced a documentary that ended up challenging many of their own assumptions about Pidgin, and which revealed the importance of translingual practices (Pennycook, 2007). This article draws on material from the documentary and interviews with the students to illustrate how the students’ views towards Pidgin changed during the course of the project, with a particular focus on the language's legitimacy. The results suggest that a students-as-knowledge-producers approach may offer more potential to challenge linguicism than many contrastive analysis approaches currently being used. By treating non-mainstream languages as subject matter in their own right, without reference or comparison to the dominant language, we argue that these languages earn more respect and acknowledgment in school settings and beyond.","Christina Higgins and Richard Nettell and Gavin Furukawa and Kent Sakoda",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"An eco-dialogical study of second language learners' World of Warcraft (WoW) gameplay","This exploratory research proceeds from the perspective that language is ecological and dialogical. We examined variables derived from eco-dialogical coding of an episode of World of Warcraft play involving three English learners. According to the Eco-dialogical model (Zheng, 2012), second language (L2) learners need to learn to take skilled linguistic action (Cowley, 2013), a process of realizing the values of physical, sociocultural and dialogical affordances in the environment. We employed Multinomial Logistic Regression to determine which of our variables were predictors for three types of values realizing; namely, wayfinding orienting to sociocultural norms and synergized values realizing of both wayfinding and orientation to sociocultural norms. The model we developed suggested that when communicative projects collectively entailed players’ a) verbalizing with synchronized avatar action, b) attending to game rules and c) coordinating in anticipation of good future prospects, players were more likely to realize both values realizing types synergistically. In other words, players' skilled linguistic action of prospective coordination, combined with multimodal languaging and constrained by WoW game rules, together, were more likely to lead to dual values realizing. This finding suggests that dual values realizing evokes connections between real-time first-order physical movements and multimodal languaging with situation transcending practices (Linell, 2009) which are second-order rules, and other sociocultural and linguistic norms. Coupling this finding with our Eco-dialogical unit of analysis, communicative projects, we suggest that these language learners developed co-agency. We conclude that our model should be tested in future studies that seek to illuminate the contribution of a new Eco-dialogical understanding of L2 learning and the potential for learners to have high quality languaging experiences in multiplayer 3D game environments and other social semiotically rich contexts.","Kristi Newgarden and Dongping Zheng and Min Liu",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Engaging children in the development of obesity interventions: Exploring outcomes that matter most among obesity positive outliers","Objective
To explore outcomes and measures of success that matter most to ‘positive outlier’ children who improved their body mass index (BMI) despite living in obesogenic neighborhoods.
Methods
We collected residential address and longitudinal height/weight data from electronic health records of 22,657 children ages 6–12 years in Massachusetts. We defined obesity “hotspots” as zip codes where >15% of children had a BMI ≥95th percentile. Using linear mixed effects models, we generated a BMI z-score slope for each child with a history of obesity. We recruited 10–12 year-olds with negative slopes living in hotspots for focus groups. We analyzed group transcripts and discussed emerging themes in iterative meetings using an immersion/crystallization approach.
Results
We reached thematic saturation after 4 focus groups with 21 children. Children identified bullying and negative peer comparisons related to physical appearance, clothing size, and athletic ability as motivating them to achieve a healthier weight, and they measured success as improvement in these domains. Positive relationships with friends and family facilitated both behavior change initiation and maintenance.
Conclusions
The perspectives of positive outlier children can provide insight into children's motivations leading to successful obesity management.
Practice implications
Child/family engagement should guide the development of patient-centered obesity interventions.","Mona Sharifi and Gareth Marshall and Roberta E. Goldman and Courtney Cunningham and Richard Marshall and Elsie M. Taveras",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Repeated exposure to neurotoxic levels of chlorpyrifos alters hippocampal expression of neurotrophins and neuropeptides","Chlorpyrifos (CPF), an organophosphorus pesticide (OP), is one of the most widely used pesticides in the world. Subchronic exposures to CPF that do not cause cholinergic crisis are associated with problems in cognitive function (i.e., learning and memory deficits), but the biological mechanism(s) underlying this association remain speculative. To identify potential mechanisms of subchronic CPF neurotoxicity, adult male Long Evans (LE) rats were administered CPF at 3 or 10mg/kg/d (s.c.) for 21 days. We quantified mRNA and non-coding RNA (ncRNA) expression profiles by RNA-seq, microarray analysis and small ncRNA sequencing technology in the CA1 region of the hippocampus. Hippocampal slice immunohistochemistry was used to determine CPF-induced changes in protein expression and localization patterns. Neither dose of CPF caused overt clinical signs of cholinergic toxicity, although after 21 days of exposure, cholinesterase activity was decreased to 58% or 13% of control levels in the hippocampus of rats in the 3 or 10mg/kg/d groups, respectively. Differential gene expression in the CA1 region of the hippocampus was observed only in the 10mg/kg/d dose group relative to controls. Of the 1382 differentially expressed genes identified by RNA-seq and microarray analysis, 67 were common to both approaches. Differential expression of six of these genes (Bdnf, Cort, Crhbp, Nptx2, Npy and Pnoc) was verified in an independent CPF exposure study; immunohistochemistry demonstrated that CRHBP and NPY were elevated in the CA1 region of the hippocampus at 10mg/kg/d CPF. Gene ontology enrichment analysis suggested association of these genes with receptor-mediated cell survival signaling pathways. miR132/212 was also elevated in the CA1 hippocampal region, which may play a role in the disruption of neurotrophin-mediated cognitive processes after CPF administration. These findings identify potential mediators of CPF-induced neurobehavioral deficits following subchronic exposure to CPF at a level that inhibits hippocampal cholinesterase to less than 20% of control. An equally significant finding is that subchronic exposure to CPF at a level that produces more moderate inhibition of hippocampal cholinesterase (approximately 50% of control) does not produce a discernable change in gene expression.","Young S. Lee and John A. Lewis and Danielle L. Ippolito and Naissan Hussainzada and Pamela J. Lein and David A. Jackson and Jonathan D. Stallings",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A qualitative investigation of protégé expectations and proposition of an evaluation model for formal mentoring in pharmacy education","Background
Student pharmacist mentoring programs have gained attention from colleges of pharmacy as a way to enhance the student experience. However, no evaluative models have been proposed or theoretical explanations described for use in improving formal mentoring programs in pharmacy or for guiding the construction of a literature base.
Objectives
The objectives of this study were to investigate student expectations and preferences for formal mentoring programs and propose a model for evaluating formal mentoring programs in pharmacy education.
Methods
Five, 60-minute focus groups were conducted in September 2009. Participants were PharmD candidates in their first 3 years of professional education. Discussion was facilitated using a question guide. Following transcription, an initial iteration of the model was used to code the data. A consensus-forming process was used to derive themes and identify representative quotes. Elaboration and specification of the final proposed model is presented.
Results
In all, 28 students participated. Emergent constructs were identified from the data. Structures or inputs of the formal mentoring program included mentor and protégé characteristics and program structure. Mentoring processes included mentor functions, mentoring activities, and relationship development. Outcomes included both proximal outcomes in the form of mentor and protégé change, program satisfaction, and organizational learning; and distal outcomes comprised mentor, protégé, and organizational outcomes.
Conclusions
This formal mentoring evaluation model was useful in guiding analysis of protégé experiences and preferences for a college-sponsored program. The model can be used to guide college administrators and researchers on future theory-based inquiry into protégé; mentor; and organizational structures, processes, and outcomes for formal mentoring programs.","Matthew J. Witry and Brandon J. Patterson and Bernard A. Sorofman",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Polarization of light scattered by large aggregates","Study of cosmic dust and planetary aerosols indicate that some of them contain a large number of aggregates of the size that significantly exceeds the wavelengths of the visible light. In some cases such large aggregates may dominate in formation of the light scattering characteristics of the dust. In this paper we present the results of computer modeling of light scattering by aggregates that contain more than 1000 monomers of submicron size and study how their light scattering characteristics, specifically polarization, change with phase angle and wavelength. Such a modeling became possible due to development of a new version of Multi Sphere T-Matrix (MSTM) code for parallel computing. The results of the modeling are applied to the results of comet polarimetric observations to check if large aggregates dominate in formation of light scattering by comet dust. We compare aggregates of different structure and porosity. We show that large aggregates of more than 98% porosity (e.g. ballistic cluster–cluster aggregates) have angular dependence of polarization almost identical to the Rayleigh one. Large compact aggregates (less than 80% porosity) demonstrate the curves typical for solid particles. This rules out too porous and too compact aggregates as typical comet dust particles. We show that large aggregates not only can explain phase angle dependence of comet polarization in the near infrared but also may be responsible for the wavelength dependence of polarization, which can be related to their porosity.","Ludmilla Kolokolova and Daniel Mackowski",2012,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"KDM5C mutational screening among males with intellectual disability suggestive of X-Linked inheritance and review of the literature","An increasing number of neurodevelopmental diseases have been associated with disruption of chromatin remodeling in eukaryotes. Lysine(K)-specific demethylase 5C (KDM5C) is a versatile epigenetic regulator that removes di- and tri-methyl groups of lysine 4 on histone H3 (H3K4) from transcriptional targets and is essential for neuronal survival and dendritic growth. Mutations in KDM5C gene, located at Xp11.22, have been reported as an important cause of both syndromic and non-syndromic X-linked intellectual disability (XLID) in males. The aim of this study was to evaluate the prevalence and spectrum of KDM5C mutations among Brazilian patients with XLID. To access the impact of KDM5C variants on XLID, a cohort of 143 males with a family history of intellectual disability (ID) suggestive of X-linked inheritance were enrolled. Common genetic causes of XLID were previously excluded and the entire coding and flanking intronic sequences of KDM5C gene were screened by direct Sanger sequencing. Seven nucleotide changes were observed: one pathogenic mutation (c.2172C>A, p.Cys724*), one novel variant with unknown value (c.633G>C, p.Arg211Arg) and five apparently benign sequence changes. In silico analysis of the variants revealed a putative creation of an Exonic Splicing Enhancer sequence by the silent c.633G>C mutation, which co-segregates with the ID phenotype. Our results point out to a KDM5C pathogenic mutational frequency of 0.7% among males with probable XLID. This is the first KDM5C screening among ID males from a country in Latin America and provides new clues about the significance of KDM5C mutations for genetic counseling.","Thainá Fernandez Gonçalves and Andressa Pereira Gonçalves and Natalia Fintelman Rodrigues and Jussara Mendonça dos Santos and Márcia Mattos Gonçalves Pimentel and Cíntia Barros Santos-Rebouças",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A mesoscopic reaction rate model for shock initiation of multi-component PBX explosives","The primary goal of this research is to develop a three-term mesoscopic reaction rate model that consists of a hot-spot ignition, a low-pressure slow burning and a high-pressure fast reaction terms for shock initiation of multi-component Plastic Bonded Explosives (PBX). Thereinto, based on the DZK hot-spot model for a single-component PBX explosive, the hot-spot ignition term as well as its reaction rate is obtained through a “mixing rule” of the explosive components; new expressions for both the low-pressure slow burning term and the high-pressure fast reaction term are also obtained by establishing the relationships between the reaction rate of the multi-component PBX explosive and that of its explosive components, based on the low-pressure slow burning term and the high-pressure fast reaction term of a mesoscopic reaction rate model. Furthermore, for verification, the new reaction rate model is incorporated into the DYNA2D code to simulate numerically the shock initiation process of the PBXC03 and the PBXC10 multi-component PBX explosives, and the numerical results of the pressure histories at different Lagrange locations in explosive are found to be in good agreements with previous experimental data.","Y.R. Liu and Z.P. Duan and Z.Y. Zhang and Z.C. Ou and F.L. Huang",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mapping of quick clay by electrical resistivity tomography under structural constraint","Geotechnical projects usually rely on traditional sounding and drilling investigations. Drilling only provides point information and the geology needs to be interpolated between these points. Near surface geophysical methods can provide information to fill those gaps. Norwegian case studies are presented to illustrate how two-dimensional electrical resistivity tomography (ERT) can be used to accurately map the extent of quick clay deposits. Quick clay may be described as highly sensitive marine clay that changes from a relatively stiff condition to a liquid mass when disturbed. Quick clay slides present a geo-hazard and therefore layers of sensitive clay need to be mapped in detail. They are usually characterized by higher resistivity than non-sensitive clay and ERT is therefore a suitable approach to identify their occurrence. However, our experience shows that ERT cannot resolve this small resistivity contrast near large anomalies such as a bedrock interface. For this reason, a constrained inversion of ERT data was applied to delineate quick clay extent both vertically and laterally. As compared to the conventional unconstrained inversions, the constrained inversion models exhibit sharper resistivity contrasts and their resistivity values agree better with in situ measurements.","S. Bazin and A.A. Pfaffhuber",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Improving a family of Darboux methods for rational second order ordinary differential equations","We have been working in many aspects of the problem of analyzing, understanding and solving ordinary differential equations (first and second order). As we have extensively mentioned, while working in the Darboux type methods, the most costly step of our methods and algorithms of solution is the determination of Darboux polynomials for the associated differential operators. Here, we are going to apply a procedure to greatly reduce the time expenditure in determining these needed Darboux polynomials for a class of second order differential equations.
New version program summary
Program Title: FiOrDi Catalogue identifier: AEQL_v2_0 Program Summary URL:http://cpc.cs.qub.ac.uk/summaries/AEQL_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 1784 No. of bytes in distributed program, including test data, etc.: 35174 Distribution format: tar.gz Programming language: Maple (release 17). Computer: PC. Operating system: Windows 7. Windows Vista RAM: 128 Mb Keywords: First integrals, Second order ordinary differential equations, Darboux type approach, Computer algebra, Darboux polynomials. PACS: 02.30.Hq. Classification: 4.3, 5. Catalogue identifier of previous version: AEQL_v1_0 Journal reference of previous version: Comput. Phys. Commun. 185(2014)307 Does the new version supersede the previous version?: Yes Nature of problem: Determination of first order differential invariants for rational second order ordinary differential equations. Solution method: The method of solution is based on a Darboux type approach. Reasons for the new version: We have been working on analyzing and solving systems of first and second order differential equations (1ODEs and 2ODEs, respectively) from a numerical point of view, using Lie methods and Darboux type approaches. For this latter class of methods, we have been developing (semi) algorithms to deal with classes of ODEs. In these algorithms, one fact has been always present: the most (computationally) costly step is the determination of the associated Darboux polynomials. Based on this realization, here we will be focused on speeding the process of finding Darboux polynomials for a class of ODEs of our interest. In particular, in this paper, we will talk about a class of rational 2ODEs. Summary of revisions: We have realized that one can extract information regarding the Darboux polynomials, correspondent to the D-operator related to the 2ODE (please see [1]) being studied in a very straightforward way. This, although a simple procedure, will prove essential to solve (or at least reduce) some 2ODEs. As mentioned in [1], the fact that our method uses the differential operator defines as in Eq. (1) is very advantageous. So, let us first define the D-operator we have been using: (1)D≡N∂x+zN∂y+M∂z, where z=dy(x)dx, y=y(x), and M and N are polynomials in (x,y,z). Considering this D-operator, one can see, by inspection on (1), that the cases that will be of interest to us are the ones listed below: 1.Darboux polynomials as factors of the numerator (in M) that are polynomials on (z) only.2.Darboux polynomials as factors of the denominator (in N) that are polynomials without z. So, the main revision we introduce here is to implement routines, in our new Maple code, to look for the needed Darboux polynomials for 2ODEs belonging to the classes mentioned above, just by inspecting the general expression for the 2ODE. With this, we make it available for the researcher, using our package, a more powerful weapon. These implementation is basically done via a modification to the command Invar, now one can use an extra argument MNDarboux in order to make use of the ideas explained above. Apart from that, some bugs were removed. Restrictions: If, for the 2ODE under consideration, the Darboux Polynomials are of high degree (>3) in the dependent and independent variables, the package may spend an impractical amount of time to obtain the solution. That restriction is in part lifted by the modifications hereby introduced. Unusual features: Since our package is based on our theoretical developments [1], it can successfully reduce some rational 2ODEs that were not solved (or reduced) by some of the best-known methods available. This situation is even enhanced via the improvements that we have made here. Let us present an example that shows the power of the change introduced. The command now is able to (automatically) find Darboux polynomials from M (depending on (z)) and N (depending on (x,y)): (2)d2dx2y=−1/7(z7−3)2(−9y8zx7+5y9x6+8y9zx+y10−6x5+z)z6(−x6+y)(xy9−1). Using what we have been learning, we can see that we have Darboux polynomials coming from both M and N. Below we will display them and their corresponding co-factors: v1=z7−3→g1=−7z6(z7−3)(9x7y8z−5x6y9−8xy9z−y10+6x5−z)v2=x6−y→g2=7(xy9−1)(6x5−z)z6(3)v3=xy9−1→g3=7(x6−y)(9xz+y)y8z6. Using the method briefly described above one conclude that, for this ODE, we have the following results for the parameters and functions needed to find the differential invariant for the ODE: P=181(z7−3)2(9x7y8−8xy9−1)Q=181(4)R=1(z7−3)2(x6−y)(xy9−1) and, using the theory described in [1], we finally find the first order invariant given by: (5)181ln(−x6+y)z7−ln(xy9−1)z7−3ln(−x6+y)+3ln(xy9−1)+1z7−3. It worth mention that the presence of the Darboux polynomials of such a high degree (as can be seen above), with terms up to the power of 10, makes the regular process of determining it very “expensive” in time expenditure and memory. After applying the method here presented, which very quickly determined the needed Darboux polynomials, the algorithm we introduced in [1] finds the results (3) and (5). For this particular instance, the in-built Maple (very powerful) dsolve command fails to reduce this ode. Our procedure takes some minutes but reduces it. Running time: This depends strongly on the ODE, but usually under 4 seconds. References: [1]L.G.S. Duarte and L.A.C.P. da Mota, Finding Elementary First Integrals for Rational Second Order Ordinary Differential Equations, Journal of Mathematical Physics, Volume 50, Issue 1, pp. 013514-013514-17 (2009).","J. Avellar and L.G.S. Duarte and L.A.C.P. da Mota",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"E-st@r-I experience: Valuable knowledge for improving the e-st@r-II design","Many universities all over the world have now established hands-on education programs based on CubeSats. These small and cheap platforms are becoming more and more attractive also for other-than-educational missions, such as technology demonstration, science applications, and Earth observation. This new paradigm requires the development of adequate technology to increase CubeSat performance and mission reliability, because educationally-driven missions have often failed. In 2013 the ESA Education Office launched the Fly Your Satellite! Programme which aims at increasing CubeSat mission reliability through several actions: to improve design implementation, to define best practices for conducting the verification process, and to make the CubeSat community aware of the importance of verification. Within this framework, the CubeSat team at Politecnico di Torino developed the e-st@r-II CubeSat as follow-on of the e-st@r-I satellite, launched in 2012 on the VEGA Maiden Flight. E-st@r-I and e-st@r-II are both 1U satellites with educational and technology demonstration objectives: to give hands-on experience to university students and to test an active attitude determination and control system based on inertial and magnetic measurements with magnetic actuation. This paper describes the know-how gained thanks to the e-st@r-I mission, and how this heritage has been translated into the improvement of the new CubeSat in several areas and lifecycle phases. The CubeSat design has been reviewed to reduce the complexity of the assembly procedure and to deal with possible failures of the on-board computer, for example re-coding the software in the communications subsystem. New procedures have been designed and assessed for the verification campaign accordingly to ECSS rules and with the support of ESA specialists. Different operative modes have been implemented to handle some anomalies observed during the operations of the first satellite. A new version of the on-board software is one of the main modifications. In particular, the activation sequence of the satellite has been modified to have a stepwise switch-on of the satellite. In conclusion, the e-st@r-I experience has provided valuable lessons during its development, verification and on-orbit operations. This know-how has become crucial for the development of the e-st@r-II CubeSat as illustrated in this article.","S. Corpino and G. Obiols-Rabasa and R. Mozzillo and F. Nichele",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Improved binary dragonfly optimization algorithm and wavelet packet based non-linear features for infant cry classification","Background and objective
Infant cry signal carries several levels of information about the reason for crying (hunger, pain, sleepiness and discomfort) or the pathological status (asphyxia, deaf, jaundice, premature condition and autism, etc.) of an infant and therefore suited for early diagnosis. In this work, combination of wavelet packet based features and Improved Binary Dragonfly Optimization based feature selection method was proposed to classify the different types of infant cry signals.
Methods
Cry signals from 2 different databases were utilized. First database contains 507 cry samples of normal (N), 340 cry samples of asphyxia (A), 879 cry samples of deaf (D), 350 cry samples of hungry (H) and 192 cry samples of pain (P). Second database contains 513 cry samples of jaundice (J), 531 samples of premature (Prem) and 45 samples of normal (N). Wavelet packet transform based energy and non-linear entropies (496 features), Linear Predictive Coding (LPC) based cepstral features (56 features), Mel-frequency Cepstral Coefficients (MFCCs) were extracted (16 features). The combined feature set consists of 568 features. To overcome the curse of dimensionality issue, improved binary dragonfly optimization algorithm (IBDFO) was proposed to select the most salient attributes or features. Finally, Extreme Learning Machine (ELM) kernel classifier was used to classify the different types of infant cry signals using all the features and highly informative features as well.
Results
Several experiments of two-class and multi-class classification of cry signals were conducted. In binary or two-class experiments, maximum accuracy of 90.18% for H Vs P, 100% for A Vs N, 100% for D Vs N and 97.61% J Vs Prem was achieved using the features selected (only 204 features out of 568) by IBDFO. For the classification of multiple cry signals (multi-class problem), the selected features could differentiate between three classes (N, A & D) with the accuracy of 100% and seven classes with the accuracy of 97.62%.
Conclusion
The experimental results indicated that the proposed combination of feature extraction and selection method offers suitable classification accuracy and may be employed to detect the subtle changes in the cry signals.","M. Hariharan and R. Sindhu and Vikneswaran Vijean and Haniza Yazid and Thiyagar Nadarajaw and Sazali Yaacob and Kemal Polat",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Evaluation of Condylar Resorption Before and After Orthognathic Surgery","Malpositioned condyles during osteotomy can cause remodeling of the condyles, but can also initiate condylar resorption (CR). The radiological signs of CR are similar to juvenile osteoarthritis and osteoarthrosis. In the 1980s, conventional transcranial and infracranial radiographs were used to evaluate the position of the condyle in the fossa. An orthopantomogram can be used to describe the contour or morphology of the condyles, but it is not applicable for measurements. Magnetic resonance imaging is useful in evaluation of the disks, condyles, and synovia. Both conventional multislice computed tomography and cone-beam computed tomography (CBCT) can provide an excellent visualization of the condyles in 3 planes. With CBCT, condylar position and condylar changes can be assessed as a color-coded map, or as mesh transparencies, which provide higher accuracy. The pretreatment assessment of past or potential temporomandibular joint (TMJ) issues consists of a detailed history of previous TMJ symptoms, as well as a clinical and radiological examination. An orthopantomogram is helpful to make a risk profile based on the contour of a condyle and the stage of osteoarthritic degeneration. After orthognathic surgery, the surgeon must be aware of TMJ dysfunction symptoms, occlusal relapse, reduction of form and volume of the condyle, and loss of mandibular ramus height. In patients with a high risk for CR or when a suspicion of CR occurs, a CBCT is indicated. The incorporation of an automated postscan image enhancement protocol and subsequent 3-dimensional rendering of condyles into the 3-dimensional virtual head model of patients will provide a powerful tool for analysis of CR.","Theo J.M. Hoppenreijs and Thomas Maal and Tong Xi",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The potential of dietary polyunsaturated fatty acids to modulate eicosanoid synthesis and reproduction in Daphnia magna: A gene expression approach","Nutritional ecology of the aquatic model genus Daphnia has received much attention in past years in particular with regard to dietary polyunsaturated fatty acids (PUFAs) which are crucial for growth and reproduction. Besides their significant role as membrane components, C20 PUFAs serve as precursors for eicosanoids, hormone-like mediators of reproduction, immunity and ion transport physiology. In the present study we investigate transcriptomic changes in Daphnia magna in response to different algal food organisms substantially differing in their PUFA composition using quantitative real-time PCR and relate them to concomitantly documented life history data. The selection of target genes includes representatives that have previously been shown to be responsive to the eicosanoid biosynthesis inhibitor ibuprofen. The beneficial effect of C20 PUFA-rich food on reproduction and population growth rates was accompanied by an increased vitellogenin (DmagVtg1) gene expression in D. magna. Additionally, genes involved in eicosanoid signaling were particularly influenced by dietary C20 PUFA availability. For example, the cyclooxygenase gene (Cox), coding for a central enzyme in the eicosanoid pathway, was highly responsive to the food treatments. Our results suggest that dietary PUFAs are fundamental in D. magna physiology as substrate for eicosanoid synthesis and that these eicosanoids are important for D. magna reproduction.","Nina Schlotz and Jesper Givskov Sørensen and Dominik Martin-Creuzburg",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Comparison of the relative efficiencies of stereo-BRUVs and traps for sampling tropical continental shelf demersal fishes","The sampling efficiencies of commercial standard fish traps and baited remote underwater stereo-video systems (stereo-BRUVs) were compared by examining the diversity and relative abundance of tropical demersal fish that each method sampled on the north-western shelf (40–60m) of Western Australia. Stereo-BRUVs recorded many more species (91 species from 32 families) than commercial fish traps (30 species and 15 families). Stereo-BRUVs also sampled many more individuals (mean 36.55±5.91 SE) than fish traps (mean 12.30±1.40 SE). This suggests stereo-BRUVs would be more capable of detecting changes in the relative abundance of species over time. Data from four commercially important species (Epinephelus bilobatus, Epinephelus multinotatus, Lethrinus punctulatus and Lutjanus russelli) revealed that stereo-BRUVs had much greater statistical power to detect change than an equivalent number of samples from fish traps. In contrast, fish traps had a greater statistical power to detect change for a fifth target species, Lutjanus sebae. For two commonly sampled species, Abalistes stellatus11Usage follows CAABcodes (Rees, A.J.J., Yearsley, G.K., Gowlett-Holmes, K. and Pogonoski, J. Codes for Australian Aquatic Biota (on-line version). CSIRO Marine and Atmospheric Research, World Wide Web electronic publication, 1999 onwards. Available at: http://www.cmar.csiro.au/caab/.). and Lethrinus punctulatus, stereo-BRUVs sampled a smaller mean length than fish traps while for a third species, Lutjanus sebae, stereo-BRUVs recorded a larger mean length. The length frequencies for these species were not significantly different between methods, although stereo-BRUVs sampled a much larger range of lengths than was captured in traps. This study demonstrates that stereo-BRUVs are potentially a much more powerful technique than fish traps for assessing species richness, relative abundance and size structure in multi-species fisheries in north-western Australia.","Euan S. Harvey and Stephen J. Newman and Dianne L. McLean and Mike Cappo and Jessica J. Meeuwig and Craig L. Skepper",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Craziness based Particle Swarm Optimization algorithm for FIR band stop filter design","In this paper, an improved particle swarm optimization technique called Craziness based Particle Swarm Optimization (CRPSO) is proposed and employed for digital finite impulse response (FIR) band stop filter design. The design of FIR filter is generally nonlinear and multimodal. Hence gradient based classical optimization methods are not suitable for digital filter design due to sub-optimality problem. So, global optimization techniques are required to avoid local minima problem. Several heuristic approaches are available in the literatures. The Particle Swarm Optimization (PSO) algorithm is a heuristic approach with two main advantages: it has fast convergence, and it uses only a few control parameters. But the performance of PSO depends on its parameters and may be influenced by premature convergence and stagnation problem. To overcome these problems the PSO algorithm has been modified in this paper and is used for FIR filter design. In birds' flocking or fish schooling, a bird or a fish often changes directions suddenly. This is described by using a “craziness” factor and is modelled in the technique by using a craziness variable. A craziness operator is introduced in the proposed technique to ensure that the particle would have a predefined craziness probability to maintain the diversity of the particles. The algorithm's performance is studied with the comparison of real coded genetic algorithm (RGA), conventional PSO, comprehensive learning particle swarm optimization (CLPSO) and Parks and McClellan (PM) Algorithm. The simulation results show that the CRPSO is superior or comparable to the other algorithms for the employed examples and can be efficiently used for FIR filter design.","Rajib Kar and Durbadal Mandal and Sangeeta Mondal and Sakti Prasad Ghoshal",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Experimental evaluation of the applicability of phase, amplitude, and combined methods to determine water flux and thermal diffusivity from temperature time series using VFLUX 2","Summary
Vertical fluid exchange between surface water and groundwater can be estimated using diurnal signals from temperature time series methods based on amplitude ratios (Ar), phase shifts (Δϕ), or combined use of both (ArΔϕ). The Ar, Δϕ, and ArΔϕ methods are typically applied in conditions where one or more of their underlying assumptions are violated, and the reliability of the various methods in response to non-ideal conditions is unclear. Additionally, ArΔϕ methods offer the ability to estimate thermal diffusivity (κe) without assuming any thermal parameters, although the value of such output has not been broadly tested. The Ar, Δϕ, and ArΔϕ methods are tested under non-steady, 1D flows in sand column experiments, and multi-dimensional flows in heterogeneous media in numerical modeling experiments. Results show that, in non-steady flow conditions, estimated κe values outside of a plausible range for streambed materials (0.028–0.180m2d−1) coincide with time periods with erroneous flux estimates. In heterogeneous media, sudden changes of κe with depth also coincide with erroneous flux estimates. When (known) fluxes are variable in time, poor identification of Δϕ leads to poor flux estimates from Δϕ and ArΔϕ methods. However, when fluxes are steady, or near zero, ArΔϕ methods provide the most accurate flux estimates. This comparison of Ar, Δϕ and ArΔϕ methods under non-ideal conditions provides guidance on their use. In this study, ArΔϕ methods have been coded into a new version of VFLUX, allowing users easy access to recent advances in heat tracing.","Dylan J. Irvine and Laura K. Lautz and Martin A. Briggs and Ryan P. Gordon and Jeffrey M. McKenzie",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The role of angioplasty in patients with acute coronary syndrome and previous coronary artery bypass grafting","Introduction
Angioplasty has changed the management of acute coronary syndrome (ACS). However, in patients with previous coronary artery bypass grafting (CABG), the role of angioplasty in the management of ACS is widely debated. Lack of clear guidelines leads to subjective and often stereotypical assessments based on clinician preferences. We sought to investigate if angioplasty affected all cause mortality in ACS patients with previous CABG.
Methods
Completely anonymous information on patients with ACS with a background of previous CABG, co-morbidities and procedures attending three multi-ethnic general hospitals in the North West of England, United Kingdom in the period 2000–2012 was traced using the ACALM (Algorithm for Comorbidities, Associations, Length of stay and Mortality) study protocol using ICD-10 and OPCS-4 coding systems. Predictors of mortality and survival analyses were performed using SPSS version 20.0.
Results
Out of 12,227 patients with ACS, there were 1172 (19.0%) cases of ACS in patients with previous coronary artery bypass grafting. Of these 83 (7.1%) patients underwent angioplasty. Multi-nominal logistic regression, accounting for differences in age and co-morbidities, revealed that having angioplasty conferred a 7.96 times improvement in mortality (2.36–26.83 95% CI) compared to not having angioplasty in this patient group.
Conclusions
We have shown that angioplasty confers significantly improved all cause mortality in the management of ACS in patients with previous CABG. The findings of this study highlight the need for clinicians to conscientiously think about the individual benefits and risks of angioplasty for every patient rather than confining to age related stereotypes.","Rahul Potluri and Mudassar Baig and Jaskaran Singh Mavi and Noman Ali and Amir Aziz and Hardeep Uppal and Suresh Chandran",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Developmental study of treatment fidelity, safety and acceptability of a Symptoms Clinic intervention delivered by General Practitioners to patients with multiple medically unexplained symptoms","Background
There is a need for primary care interventions for patients with multiple medically unexplained symptoms (MUS). We examined whether GPs could be taught to deliver one such intervention, the Symptoms Clinic Intervention (SCI), to patients. The intervention includes recognition and validation of patients' symptoms, explanation of symptoms and actions to manage symptoms.
Methods
We conducted an uncontrolled observational study in Northeast Scotland. GPs were recruited and received two days of structured training. Patients were identified via a two stage process (database searching followed by postal questionnaire) and received the SCI intervention from a GP in their practice. Treatment fidelity was assessed by applying a coding framework to consultation transcripts. Safety was assessed by examining changes in patient symptoms (PHQ-15) and checking for unexpected events. Acceptability was primarily assessed by patient interview.
Results
Four GPs delivered the SCI to 23 patients. GPs delivered all core components of the SCI, and used the components flexibly across the consultations and between patients. They spent more time on recognition than either explanation or actions components. 10 out of 17 patients interviewed described feeling validated, receiving useful explanation and learning actions. 9 out of 20 patients (45%) reported an improvement in PHQ-15 of between 3 and 8 points. Patients who reported the most improvement also described receiving all three components of the intervention.
Conclusions
GPs can be taught to deliver the SCI with reasonable fidelity, safety and acceptability, although some items were inconsistently delivered: further training would be needed before use.","LaKrista Morton and Alison Elliott and Ruth Thomas and Jennifer Cleland and Vincent Deary and Christopher Burton",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"La transformación del modelo asistencial en Cataluña para mejorar la calidad de la atención","Resumen
Los cambios que se están produciendo en los países occidentales obligan a los sistemas sanitarios a adaptarse a las nuevas necesidades y expectativas de la población. En Cataluña se está produciendo una profunda transformación del modelo asistencial, con el fin de poder dar una respuesta adecuada a esta nueva situación y a la vez garantizar la sostenibilidad del sistema en un contexto de crisis económica. Esta transformación se basa en convertir el actual modelo asistencial centrado en la enfermedad y fraccionado por niveles en otro centrado en la persona, integrado y de base territorial, que promueva el trabajo compartido en red de los diferentes profesionales, dispositivos y niveles asistenciales, estableciendo objetivos comunes explicitados en acuerdos y pactos territoriales. Los cambios que ha llevado a cabo el Servei Català de la Salut (CatSalut) pasan principalmente por incrementar la capacidad de resolución de la atención primaria, reducir la variabilidad de la práctica clínica, evolucionar hacia hospitales más quirúrgicos, potenciar las alternativas a la hospitalización convencional, desarrollar modalidades de atención no presencial, concentrar y sectorizar territorialmente la atención de alta complejidad y diseñar códigos sanitarios específicos, como respuesta a situaciones de emergencia. La finalidad de estas actuaciones es mejorar la efectividad, la calidad, la seguridad y la eficiencia del sistema asegurando la equidad de acceso de la población y el equilibrio territorial. Entre los instrumentos que deben facilitar y promover estos cambios cabe destacar la historia clínica compartida, el nuevo modelo de contratación y pago por resultados, los pactos territoriales, las alianzas entre centros, el aprovechamiento de las potencialidades de las tecnologías de la información y la comunicación, y la evaluación de resultados.
The changes taking place in western countries require health systems to adapt to the public's evolving needs and expectations. The healthcare model in Catalonia is undergoing significant transformation in order to provide an adequate response to this new situation while ensuring the system's sustainability in the current climate of economic crisis. This transformation is based on converting the current diseasecentred model which is fragmented into different levels, to a more patient-centred integrated and territorial care model that promotes the use of a shared network of the different specialities, the professionals, resources and levels of care, entering into territorial agreements and pacts which stipulate joint goals or objectives. The changes the Catalan Health Service (CatSalut) has undergone are principally focused on increasing resolution capacity of the primary level of care, eliminating differences in clinical practice, evolving towards more surgery-centred hospitals, promoting alternatives to conventional hospitalization, developing remote care models, concentrating and organizing highly complex care into different sectors at a territorial level and designing specific health codes in response to health emergencies. The purpose of these initiatives is to improve the effectiveness, quality, safety and efficiency of the system, ensuring equal access for the public to these services and ensuring a territorial balance. These changes should be facilitated and promoted using several different approaches, including implementing shared access to clinical history case files, the new model of results-based contracting and payment, territorial agreements, alliances between centres, harnessing the potential of information and communications technology and evaluation of results.","Josep Maria Padrosa and Àlex Guarga and Francesc Brosa and Josep Jiménez and Roger Robert",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Progress on standardization and automation in software development on W7X","For a complex experiment like W7X being subject to changes all along its projected lifetime the advantages of a formalized software development method have already been stated [1]. Quality standards like ISO/IEC-12207 provide a guideline for structuring of development work and improving process and product quality. A considerable number of tools has emerged supporting and automating parts of development work. On W7X progress has been made during the last years in exploiting the benefit of automation and management during software development:–Continuous build, integration and automated test of software artefacts.∘Syntax checks and code quality metrics.∘Documentation generation.∘Feedback for developers by temporal statistics.–Versioned repository for build products (libraries, executables).–Separate snapshot and release repositories and automatic deployment.–Semi-automatic provisioning of applications.–Feedback from testers and feature requests by ticket system. This toolset is working efficiently and allows the team to concentrate on development. The activity there is presently focused on increasing the quality of the existing software to become a dependable product. Testing of single functions and qualities must be simplified. So a restructuring is underway which relies more on small, individually testable components with standardized interfaces providing the capability to construct arbitrary function aggregates for dedicated tests of quality attributes as availability, reliability, performance. A further activity is on improving the development cycle. The use of release cycles has already provided favourable concentration of work and predictability of delivery times. However, the demand has risen, to react quickly on priority changes from W7X-project management. So a more agile development cycle is being prepared relying on smaller working packages, shorter release cycles and an associated release plan giving the software development responsible the possibility to react on a shorter time scale.","Georg Kühner and Torsten Bluhm and Peter Heimann and Christine Hennig and Hugo Kroiss and Jon Krom and Heike Laqua and Marc Lewerentz and Josef Maier and Jörg Schacht and Anett Spring and Andreas Werner and Manfred Zilker",2012,"[""Science Direct""]","Rejeitado: CR11","Rejeitado: CR11"
"Holocene dynamics of the salt–fresh groundwater interface under a sand island, Inhaca, Mozambique","The configuration of coastal groundwater systems in southeast Africa was strongly controlled by the Holocene sea-level changes, with an Early Holocene transgression ∼15 m (10,000–5000 cal BP), and two assumed high-stand events in the Middle and Late Holocene with levels higher than the present. The fluctuation of the salt–fresh groundwater interface under Inhaca Island in Mozambique during the Holocene has been studied using an adapted version of the numerical code SUTRA (Saturated-Unsaturated Transport). In this study, small-scale variations such as tidal effects have not been considered. A number of transient simulations were run with constant boundary conditions until the steady state condition was reached in order to study the sensitivity of response time, salt–fresh interface position, and thickness of the transition zone to different parameters such as hydraulic conductivity, porosity, recharge, and dispersivity. A 50% increase in horizontal hydraulic conductivity yields a rise in the location of the interface of >15 m, while an increase in recharge from 8% to 20% of mean annual precipitation (MAP) causes a downward shift in the interface position of >40 m. A full transient simulation of the Holocene dynamics of the salt–fresh groundwater interface showed a response time of several hundred years, with a duration sensitive to porosity, hydraulic conductivity and recharge and a position determined by the recharge rate and the hydraulic conductivity. Dispersivity controls the thickness of the transition zone in this non-tidal model. Physical processes, such as changes in recharge and/or the sea level, may cause rapid shifts in the interface position and affect the thickness of the transition zone.","Lars Været and Anton Leijnse and Fortunato Cuamba and Sylvi Haldorsen",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Le simulateur face à l’expert","Résumé
Depuis l’apparition de l’article 64 du Code pénal, le manque de discernement lié à une maladie mentale est devenu une source d’immunité pénale. Les personnes atteintes d’un trouble psychique ou neuropsychique ayant aboli leur discernement n’ont pas à répondre de leur acte devant la loi. Cette immunité pénale a cependant eu un effet pernicieux, puisque certains prévenus simulent la maladie mentale pour se prémunir des peines qu’ils encourent. La simulation fait partie du paysage juridique. Pour distinguer les vrais malades des simulateurs, la justice fait appel au supposé savoir des experts. Afin de fonder sa sentence, le magistrat requiert de ces derniers des informations médicales et psychologiques visant à vérifier la réalité de « l’altération du discernement » de la personne poursuivie. Or, cette collaboration entre les praticiens de la justice et de la santé mentale n’échappe pas à des problèmes techniques et éthiques. Le clinicien intervenant en tant qu’expert court autant le risque d’une instrumentalisation par la justice que celui d’une manipulation par le sujet simulateur. Cette attitude consistant à tirer profit d’un trouble présumé fournit l’occasion de questionner le fonctionnement du cadre de l’expertise médico-légale. En se jouant de ce cadre, à dessein, les sujets mettent indirectement en lumière certaines de ses failles, autrement imperceptibles. Cet article met ainsi en lumière la complexité des enjeux éthiques et techniques liés à la question de la simulation. Les auteurs rappellent notamment les réflexions émises par Freud lorsqu’il fut mandaté en tant qu’expert pour se pencher sur ce thème. Ensuite, ils s’intéressent aux difficultés que le mensonge sur l’état mental du prévenu pose au niveau juridique. Après quoi, une discussion s’ouvre quant à la possibilité que la simulation puisse révéler l’existence d’un trouble chez les sujets concernés. L’intention qui préside à ce comportement peut suggérer qu’il est le fruit d’une conduite totalement raisonnée. Or, ici, des exemples remettent en question cette conception. En effet, bien que le sujet ait conscience de tromper ses interlocuteurs, il peut néanmoins chercher inconsciemment à masquer un trouble dont il ignore lui-même l’existence.
Since 1810 and the enactment of article 64 of the Penal Code, the lack of discernment due to a mental disease has become a cause of penal immunity. People suffering from a mental or a neuropsychic disorder and having abolished their discernment cannot be sentenced as being criminally responsible for the crime they allegedly committed. Nevertheless, this protection status has had a pernicious effect as some accused are trying to exempt themselves from legal trials by dissimulating their mischiefs under a crazy action. Like McMurphy in One Flew Over the Cuckoo's Nest, they are simulating craziness to preserve themselves from the sentences they could receive. Hence, simulating a mental disorder is prevalent in the legal landscape. To make the difference between the real and the pretending sick cases, the magistrate uses the supposed experts’ knowledge. In order to help him building his sentence, they give him medical and psychological information aiming to identify “the diminished mental capacity” of the accused person. Thus, we are observing a close cooperation between the legal and the mental health practitioners. Nevertheless, this collaboration raises technical and ethical problems. The clinician who intervenes as an expert runs the risk to face the orders of Justice, without avoiding the opposite risk of being manipulated by an individual trying to benefit from his alleged craziness. What is more, the expertise scope is radically different from the therapy one where the individual is expected, according to Freud's formula, to work “side by side” with the therapist. For the latter, it is not a question of finding the “truth”, but of listening how the patient's psychic reality is expressing itself through its slips, its blackouts or its modified memories. Yet, in the case of a simulation, the individual changes the reality, deliberately, to abuse the Law in his favor. However, this behavior consisting in simulating craziness provides the opportunity to question the forensic expertise framework functioning. By playing with this framework, these individuals are indirectly highlighting some of its faults, which otherwise go unnoticed. This article describes the ethical and technical pitfalls the specialists are facing when confronted to simulation cases. To do so, the authors, a legal expert and a psychologist, remind the comments made by Freud at the time he was mandated as an expert to study this field. Then, they are saying more about the difficulties lying on the mental state of the accused can become a problem on a legal scale. Then, they open a discussion on the exact health level of the simulating people. Their intention can lead to the conclusion of their perfect rationality. Nevertheless, resounding scandals are challenging this conception. Even though the individuals are aware of defying the Justice, their behavior may unconsciously hide a mental disorder.","Sébastien Chapellon and Frédéric Bondil",2018,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Modulation of synaptic plasticity by the coactivation of spatially distinct synaptic inputs in rat hippocampal CA1 apical dendrites","The phenomenon whereby the relative timing between presynaptic and postsynaptic spiking determines the direction and extent of synaptic changes in a critical temporal window is known as spike timing-dependent synaptic plasticity (STDP). We have previously reported that STDP profiles can be classified into two types depending on their layer-specific location along CA1 pyramidal neuron dendrites in the rat hippocampus, suggesting that there are differences in information processing between the proximal dendrite (PD) and distal dendrite (DD). However, how the different types of information processing interact at different dendritic locations remains unclear. To investigate how the temporal information of inputs to PD influences information processing at DD, PD stimulation was applied while the STDP protocol was simultaneously applied at DDs of CA1 pyramidal neurons. Synaptic plasticity induced by the STDP protocol at DDs was enhanced or depressed depending on the timing of the back-propagating action potentials (bAPs) and the excitatory and inhibitory postsynaptic potentials elicited by PD stimulation. These results suggested that bAPs function as carriers of temporal information of PD inputs to DD. Next, the influence of DD on PD was investigated using the same protocol. Synaptic plasticity at PD was modulated only if the pairing stimuli were applied to elicit coincidental timing of bAP and the excitatory postsynaptic potential. Such coding modulations could provide the basis for a novel learning rule and may be important factors in the integration of spatiotemporal input information in neural networks in the brain.","Masashi Kondo and Tatsuo Kitajima and Satoshi Fujii and Minoru Tsukada and Takeshi Aihara",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A modified viscoplastic model to predict the permanent deformation of asphaltic materials under cyclic-compression loading at high temperatures","When subjected to cyclic creep (ratcheting) loading with rest periods between the loading cycles, the viscoplastic behavior of asphaltic materials changes such the rate of accumulation of the viscoplastic strain at the beginning of the subsequent loading cycle increases comparing to that at the end of the preceding loading cycle. This phenomenon is referred to as the hardening-relaxation (or viscoplastic-softening) and is a key element in predicting the permanent deformation (rutting) of asphalt pavements which is one of the most important distresses in asphalt pavements. This paper presents a phenomenological-based rate-dependent hardening-relaxation model to significantly enhance the prediction of the permanent deformation in asphaltic materials subjected to cyclic-compression loadings at high temperatures. A hardening-relaxation memory surface is defined in the viscoplastic strain space as the general condition for the initiation and evolution of the hardening-relaxation (or viscoplastic-softening). The memory surface is formulated to be a function of an internal state variable memorizing the maximum viscoplastic strain for which the softening has been occurred during the deformation history. The evolution function for the hardening-relaxation model is then defined as a function of the hardening-relaxation internal state variable. The proposed viscoplastic-softening model is coupled to the nonlinear Schapery’s viscoelastic and Perzyna’s viscoplastic models. The numerical algorithms for the proposed model are implemented in the well-known finite element code Abaqus via the user material subroutine UMAT. The model is then calibrated and verified by comparing the model predictions and experimental data that includes cyclic creep-recovery loadings at different stress levels, loading times, rest periods, and confinement levels. Model predictions show that the proposed approach provides a promising tool for constitutive modeling of cyclic hardening-relaxation in asphaltic materials and in general in time- and rate-dependent materials.","Masoud K. Darabi and Rashid K. Abu Al-Rub and Eyad A. Masad and Chien-Wei Huang and Dallas N. Little",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A qualitative study of staff's perspectives on implementing an after school program promoting youth physical activity","Minimal effects found across youth physical activity (PA) interventions, and increased attention to circumstances that impede adequate delivery of program components, has highlighted the importance of learning from staff what is needed to foster staff comprehension and engagement for developing, adopting, and successfully implementing PA-based youth interventions. The purpose of this study is to address this knowledge gap by conducting a qualitative assessment of school staff perspectives on the positive aspects and challenges of implementing the 17-week ACT program, an after-school intervention that integrated motivational and behavioral components to promote PA in underserved adolescents. Interviews were conducted with one school staff member from each participating school for all four trial cohorts (N=12). Transcripts were coded by independent coders (r=.84) and content analyses of themes was performed using QSR NVivo. Themes were organized into five meta-themes: (1) Logistics; (2) Essential Elements; (3) Staff and Child Challenges; (4) Staff Comprehension, Value, and Enjoyment; (5) Spill-Over Effects. Findings indicate that staff can be successful at understanding, valuing, and reaching fidelity in implementing climate-based mediation components. The insight gained from this study lays the foundation for understanding the components needed for establishing well-implemented, effective, and generalizable interventions for increasing youth PA.","Nicole Zarrett and Brittany Skiles and Dawn K. Wilson and Lauren McClintock",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"High-dimensional, unsupervised cell clustering for computationally efficient engine simulations with detailed combustion chemistry","A novel approach for computationally efficient clustering of chemically reacting environments with similar reactive conditions is presented, and applied to internal combustion engine simulations. The methodology relies on a high-dimensional representation of the chemical state space, where the independent variables (i.e. temperature and species mass fractions) are normalized over the whole dataset space. An efficient bounding-box-constrained k-means algorithm has been developed and used for obtaining optimal clustering of the dataset points in the high-dimensional domain box with maximum computational accuracy, and with no need to iterate the algorithm in order to identify the desired number of clusters. The procedure has been applied to diesel engine simulations carried out with a custom version the KIVA4 code, provided with detailed chemistry capability. Here, the cells of the computational grid are clustered at each time step, in order to reduce the computational time needed by the integration of the chemistry ODE system. After the integration, the changes in species mass fractions of the clusters are redistributed to the cells accordingly. The numerical results, tested over a variety of engine conditions featuring both single- and multiple-pulse injection operation with fuel being injected at 50° BTDC allowed significant computational time savings of the order of 3–4 times, showing the accuracy of the high-dimensional clustering approach in catching the variety of reactive conditions within the combustion chamber.","Federico Perini",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Scaling of dust explosion violence from laboratory scale to full industrial scale – A challenging case history from the past","The standardized KSt parameter still seems to be widely used as a universal criterion for ranking explosion violence to be expected from various dusts in given industrial situations. However, this may not be a generally valid approach. In the case of dust explosion venting, the maximum pressure Pmax generated in a given vented industrial enclosure is not only influenced by inherent dust parameters (dust chemistry including moisture, and sizes and shapes of individual dust particles). Process-related parameters (degree of dust dispersion, cloud turbulence, and dust concentration) also play key roles. This view seems to be confirmed by some results from a series of large scale vented dust explosion experiments in a 500 m3 silo conducted in Norway by CMI, (now GexCon AS) during 1980–1982. Therefore, these results have been brought forward again in the present paper. The original purpose of the 500 m3 silo experiments was to obtain correlations between Pmax in the vented silo and the vent area in the silo top surface, for two different dusts, viz. a wheat grain dust collected in a Norwegian grain import silo facility, and a soya meal used for production of fish farming food. Both dusts were tested in the standard 20-L-sphere in two independent laboratories, and also in the Hartmann bomb in two independent laboratories. Pmax and (dP/dt)max were significantly lower for the soya meal than for the wheat grain dust in all laboratory tests. Because the available amount of wheat grain dust was much larger than the quite limited amount of available soya meal, a complete series of 16 vented silo experiments was first performed with the wheat grain dust, starting with the largest vent area and ending with the smallest one. Then, to avoid unnecessary laborious changes of vent areas, the first experiment with soya dust was performed with the smallest area. The dust cloud in the silo was produced in exactly the same way as with the wheat grain dust. However, contrary to expectations based on the laboratory-scale tests, the soya meal exploded more violently in the large silo than the wheat grain dust, and the silo was blown apart in the very first experiment with this material. The probable reason is that the two dusts responded differently to the dust cloud formation process in the silo on the one hand and in the laboratory-scale apparatuses on the other. This re-confirms that a differentiated philosophy for design of dust explosion vents is indeed needed. Appropriate attention must be paid to the influence of the actual dust cloud generation process on the required vent area. The location and type of the ignition source also play important roles. It may seem that tailored design has to become the future solution for tackling this complex reality, not least for large storage silos. It is the view of the present author that the ongoing development of CFD-based computer codes offers the most promising line of attack. This also applies to design of systems for dust explosion isolation and suppression.","Rolf K. Eckhoff",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A comprehensive photometric study of the eclipsing binary EP Aurigae","We present new observations for the eclipsing binary EP Aurigae, which were performed by using three small telescopes in China from 2003 December to 2014 January. With the updated 2003 version of the Wilson–Devinney code, the photometric elements were deduced from three sets of light curves. Based on all available eclipsing times, the orbital period changes were investigated. It is discovered that the (O–C) curve may show an existence of light-time effect due to an unseen third body, which was weakly identified by the photometric solution. The modulated period and amplitude of the cyclic variation are P3=71.2(±8.0)yr and A=0.0101(±0.0008)day, respectively. In the co-planar orbit with the binary system, the mass of the third body is M3=0.18(±0.02)M⊙. The photometric results imply that EP Aur is an Algol-type binary with a mass ratio of q=0.831(±0.004). Its primary component almost fills its Roche lobe. Therefore, EP Aur may consist of a normal main-sequence star and a cool Roche-lobe filling subgiant, which may be undergoing rapid mass transfer.","H.-L. Li and J.-Y. Wei and Y.-G. Yang and K. Li and X.-B. Zhang",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Autophagy ameliorates cognitive impairment through activation of PVT1 and apoptosis in diabetes mice","The underlying mechanisms of cognitive impairment in diabetes remain incompletely characterized. Here we show that the autophagic inhibition by 3-methyladenine (3-MA) aggravates cognitive impairment in streptozotocin-induced diabetic mice, including exacerbation of anxiety-like behaviors and aggravation in spatial learning and memory, especially the spatial reversal memory. Further neuronal function identification confirmed that both long term potentiation (LTP) and depotentiation (DPT) were exacerbated by autophagic inhibition in diabetic mice, which indicating impairment of synaptic plasticity. However, no significant change of pair-pulse facilitation (PPF) was recorded in diabetic mice with autophagic suppression compared with the diabetic mice, which indicated that presynaptic function was not affected by autophagic inhibition in diabetes. Subsequent hippocampal neuronal cell death analysis showed that the apoptotic cell death, but not the regulated necrosis, significantly increased in autophagic suppression of diabetic mice. Finally, molecular mechanism that may lead to cell death was identified. The long non-coding RNA PVT1 (plasmacytoma variant translocation 1) expression was analyzed, and data revealed that PVT1 was decreased significantly by 3-MA in diabetes. These findings show that PVT1-mediated autophagy may protect hippocampal neurons from impairment of synaptic plasticity and apoptosis, and then ameliorates cognitive impairment in diabetes. These intriguing findings will help pave the way for exciting functional studies of autophagy in cognitive impairment and diabetes that may alter the existing paradigms.","Zhigui Li and Shuang Hao and Hongqiang Yin and Jing Gao and Zhuo Yang",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"De la promoción de la lectura por placer a la formación integral de lectores","RESUMEN
En artículo tiene el objetivo de proponer elementos para renovar la formación de lectores ante la complejidad de la lectura que hoy exigen las sociedades del conocimiento. En este contexto surgen concepciones sobre la función de la biblioteca como espacio de formación, aprendizaje, cultura y construcción de sociabilidades; por ello la bibliotecología debe desarrollar una propuesta con una perspectiva de lectura más amplia que la de los modelos pedagógicos en las instituciones educativas. Se identifican en la Bildung aportaciones para una formación de lectores integral, se incorporan además una variedad de recursos que favorecen la lectura de diferentes códigos para ampliar el capital cultural y léxico que a la vez generan modalidades de lectura dirigidas al desarrollo de capacidades de pensamiento crítico y de reflexión involucradas en la construcción de conocimiento, así como para causar experiencias estéticas necesarias en la formación y transformación subjetiva de los ciudadanos a lo largo de su vida.
ABSTRACT
This paper proposes an approach to renewing the teaching of readers how to cope with the complex demands of the information society. In this context, the role of the library in training, learning, and construction of sociability and culture requires examination; and librarianship needs to develop broadly conceived proposals that move beyond pedagogical reading models offered schools. Bildung provides guidance for the comprehensive training of readers across a variety of codes, thereby equipping trainees with reading skills needed to develop broader lexicons and other associated tools and cultural capital that are useful in the construction of knowledge. Moreover, such training enriches the subject with aesthetic experiences that have the potential to effectuate subjective transformation of citizens over the long term.","Elsa Margarita Ramírez Leyva",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Encouraging reading for pleasure and the comprehensive training for readers","ABSTRACT
This paper proposes an approach to renewing the teaching of readers how to cope with the complex demands of the information society. In this context, the role of the library in training, learning, and construction of sociability and culture requires examination; and librarianship needs to develop broadly conceived proposals that move beyond pedagogical reading models offered schools. Bildung provides guidance for the comprehensive training of readers across a variety of codes, thereby equipping trainees with reading skills needed to develop broader lexicons and other associated tools and cultural capital that are useful in the construction of knowledge. Moreover, such training enriches the subject with aesthetic experiences that have the potential to effectuate subjective transformation of citizens over the long term.
RESUMEN
En artículo tiene el objetivo de proponer elementos para renovar la formación de lectores ante la complejidad de la lectura que hoy exigen las sociedades del conocimiento. En este contexto surgen concepciones sobre la función de la biblioteca como espacio de formación, aprendizaje, cultura y construcción de sociabilidades; por ello la bibliotecología debe desarrollar una propuesta con una perspectiva de lectura más amplia que la de los modelos pedagógicos en las instituciones educativas. Se identifican en la Bildung aportaciones para una formación de lectores integral, se incorporan además una variedad de recursos que favorecen la lectura de diferentes códigos para ampliar el capital cultural y léxico que a la vez generan modalidades de lectura dirigidas al desarrollo de capacidades de pensamiento crítico y de reflexión involucradas en la construcción de conocimiento, así como para causar experiencias estéticas necesarias en la formación y transformación subjetiva de los ciudadanos a lo largo de su vida.","Elsa Margarita Ramírez-Leyva",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Undergraduate nursing students integrating health literacy in clinical settings","Summary
Background
Analyzing students' performance and self-criticism of their roles in promoting health literacy can inform nursing education in a social environment that expects new graduates to be health promoters.
Objectives
The pilot study reported here aimed to a) analyze students' understanding of and sensitivity to issues of health literacy, (b) identify students' perceptions of structural, organizational, and political barriers to the promotion of health literacy in social and health care organizations, and (c) document students' suggestions for curriculum changes that would develop their skills and competencies as health-literacy promoters.
Design
A qualitative pilot study.
Setting
A collaborative undergraduate nursing degree program in the metropolitan area of Toronto, Canada.
Participants
Sixteen undergraduate, Year 4 nursing students.
Methods
Signed informed consent was obtained from the participants. Participation was unpaid and voluntary. Recruitment was through an email invitation sent by the School of Nursing Student Affairs Coordinator. Three, one-time individual interviews and three focus groups were conducted. All were audio-recorded. Recordings were transcribed, and the transcriptions were coded using the qualitative software ATLAS ti 6.0. The interview data were submitted to thematic analysis. Additional data were gathered from the two-page self-assessments in students' academic portfolios.
Results
Sensitivity to health literacy was documented. Students performed best as health promoters in supportive teaching hospitals. Their performance was hindered by clinical settings unsupportive of health education, absence of role models, and insufficient theoretical preparation for health teaching. Students' sensitivity to their clients' diversity reportedly reinforced the interconnection, in multicultural healthcare settings, between health literacy and other social determinants of health and a growing demand for educating future nurses in expanding their role also as health promoters.
Conclusions
Students recommended more socially inclusive and experiential learning initiatives related to health teaching to address education gaps in classrooms and practice.","Margareth Zanchetta and Yasmin Taher and Suzanne Fredericks and Janice Waddell and Carol Fine and Rona Sales",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"CYP2C19 drug-drug and drug-gene interactions in ED patients","Background
CYP450 polymorphisms result in variable rates of drug metabolism. CYP drug-drug interactions can contribute to altered drug effectiveness and safety.
Study objectives
The primary objective was to determine the percentage of emergency department (ED) patients with cytochrome 2C19 (CYP2C19) drug-drug interactions. The secondary objective was to determine the prevalence of CYP2C19 polymorphisms in a US ED population.
Methods
We conducted a prospective observational study in an urban academic ED with 72,000 annual visits. Drug ingestion histories for the 48 hours preceding ED visit were obtained; each drug was coded as CYP2C19 substrate, inhibitor, inducer, or not CYP2C19 dependent. Ten percent of patients were randomized to undergo CYP2C19 genotyping using the Roche Amplichip.
Results
A total of 502 patients were included; 61% were female, 65% were white, and median age was 39 years (interquartile range, 22-53). One hundred thirty-one (26.1%) patients had taken at least 1 CYP2C19-dependent home drug. Eighteen (13.7%) patients who were already taking a CYP2C19-dependent drug were given or prescribed a CYP2C19-dependent drug while in the ED. Among the 53 patients genotyped, 52 (98%) were extensive metabolizers and 1 was a poor metabolizer.
Conclusions
In a population of ED patients, more than a quarter had taken a CYP2C19-dependent drug in the preceding 48 hours, but few were given or prescribed another CYP2C19-dependent drug in the ED. On genotyping analysis, CYP2C19 polymorphisms were uncommon in our cohort. We conclude that changing prescribing practice due to CYP2C19 drug-drug interaction or genotype is unlikely to be useful in most US ED populations.","Hanna K. Flaten and Howard S. Kim and Jenny Campbell and Lisa Hamilton and Andrew A. Monte",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Tides on Europa: The membrane paradigm","Jupiter’s moon Europa has a thin icy crust which is decoupled from the mantle by a subsurface ocean. The crust thus responds to tidal forcing as a deformed membrane, cold at the top and near melting point at the bottom. In this paper I develop the membrane theory of viscoelastic shells with depth-dependent rheology with the dual goal of predicting tidal tectonics and computing tidal dissipation. Two parameters characterize the tidal response of the membrane: the effective Poisson’s ratio ν¯ and the membrane spring constant Λ, the latter being proportional to the crust thickness and effective shear modulus. I solve membrane theory in terms of tidal Love numbers, for which I derive analytical formulas depending on Λ,ν¯, the ocean-to-bulk density ratio and the number k2∘ representing the influence of the deep interior. Membrane formulas predict h2 and k2 with an accuracy of a few tenths of percent if the crust thickness is less than one hundred kilometers, whereas the error on l2 is a few percents. Benchmarking with the thick-shell software SatStress leads to the discovery of an error in the original, uncorrected version of the code that changes stress components by up to 40%. Regarding tectonics, I show that different stress-free states account for the conflicting predictions of thin and thick shell models about the magnitude of tensile stresses due to nonsynchronous rotation. Regarding dissipation, I prove that tidal heating in the crust is proportional to Im(Λ) and that it is equal to the global heat flow (proportional to Im(k2)) minus the core-mantle heat flow (proportional to Im(k2∘)). As an illustration, I compute the equilibrium thickness of a convecting crust. More generally, membrane formulas are useful in any application involving tidal Love numbers such as crust thickness estimates, despinning tectonics or true polar wander.","Mikael Beuthe",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Optical Dissection of Experience-Dependent Pre- and Postsynaptic Plasticity in the Drosophila Brain","Summary
Drosophila represents a key model organism for dissecting neuronal circuits that underlie innate and adaptive behavior. However, this task is limited by a lack of tools to monitor physiological parameters of spatially distributed, central synapses in identified neurons. We generated transgenic fly strains that express functional fluorescent reporters targeted to either pre- or postsynaptic compartments. Presynaptic Ca2+ dynamics are monitored using synaptophysin-coupled GCaMP3, synaptic transmission is monitored using red fluorescent synaptophysin-pHTomato, and postsynaptic Ca2+ dynamics are visualized using GCaMP3 fused with the postsynaptic matrix protein, dHomer. Using two-photon in vivo imaging of olfactory projection neurons, odor-evoked activity across populations of synapses is visualized in the antennal lobe and the mushroom body calyx. Prolonged odor exposure causes odor-specific and differential experience-dependent changes in pre- and postsynaptic activity at both levels of olfactory processing. The approach advances the physiological analysis of synaptic connections across defined groups of neurons in intact Drosophila.","Ulrike Pech and Natalia H. Revelo and Katharina J. Seitz and Silvio O. Rizzoli and André Fiala",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The Monte Carlo event generator AcerMC versions 2.0 to 3.8 with interfaces to PYTHIA 6.4, HERWIG 6.5 and ARIADNE 4.1","The AcerMC Monte Carlo generator is dedicated to the generation of Standard Model background processes which were recognised as critical for the searches at LHC, and generation of which was either unavailable or not straightforward so far. The program itself provides a library of the massive matrix elements (coded by MADGRAPH) and native phase space modules for generation of a set of selected processes. The hard process event can be completed by the initial and the final state radiation, hadronisation and decays through the existing interface with either PYTHIA, HERWIG or ARIADNE event generators and (optionally) TAUOLA and PHOTOS. Interfaces to all these packages are provided in the distribution version. The phase-space generation is based on the multi-channel self-optimising approach using the modified Kajantie–Byckling formalism for phase space construction and further smoothing of the phase space was obtained by using a modified ac-VEGAS algorithm. An additional improvement in the recent versions is the inclusion of the consistent prescription for matching the matrix element calculations with parton showering for a select list of processes.
Program summary
Program title: AcerMC version 3.8 Catalogue identifier: ADQQ_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ADQQ_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 3853309 No. of bytes in distributed program, including test data, etc.: 68045728 Distribution format: tar.gz Programming language: FORTRAN 77 with popular extensions (g77, gfortran). Computer: All running Linux. Operating system: Linux. Classification: 11.2, 11.6. External routines: CERNLIB (http://cernlib.web.cern.ch/cernlib/), LHAPDF (http://lhapdf.hepforge.org/) Catalogue identifier of previous version: ADQQ_v1_0 Journal reference of previous version: Comput. Phys. Comm. 149(2003)142 Does the new version supersede the previous version?: Yes Nature of problem: Despite a large repertoire of processes implemented for generation in event generators like PYTHIA [1] or HERWIG [2] a number of background processes, crucial for studying the expected physics of the LHC experiments, is missing. For some of these processes the matrix element expressions are rather lengthy and/or to achieve a reasonable generation efficiency it is necessary to tailor the phase space selection procedure to the dynamics of the process. That is why it is not practical to imagine that any of the above general purpose generators will contain every, or even only observable, processes which will occur at LHC collisions. A more practical solution can be found in a library of dedicated matrix-element-based generators, with the standardised interfaces like that proposed in [3], to the more universal one which is used to complete the event generation. Solution method: The AcerMC EventGenerator provides a library of the matrix-element-based generators for several processes. The initial- and final-state showers, beam remnants and underlying events, fragmentation and remaining decays are supposed to be performed by the other universal generator to which this one is interfaced. We will call it a supervising generator. The interfaces to PYTHIA 6.4, ARIADNE 4.1 and HERWIG 6.5, as such generators, are provided. Provided is also an interface to TAUOLA [4] and PHOTOS [5] packages for τ-lepton decays (including spin correlations treatment) and QED radiations in decays of particles. At present, the following matrix-element-based processes have been implemented: gg,qq̄→tt̄bb̄, qq̄→W(→ℓν)bb̄; qq̄→W(→ℓν)tt̄; gg,qq̄→Z/γ∗(→ℓℓ)bb̄; gg,qq̄→Z/γ∗(→ℓℓ,νν,bb̄)tt̄; complete EW gg,qq̄→(Z/W/γ∗→)tt̄bb̄; gg,qq̄→tt̄tt̄; gg,qq̄→(tt̄→)ff̄bff̄b̄; gg,qq̄→(WWbb→)ff̄ff̄bb̄. Both interfaces allow the use of the LHAPDF/LHAGLUE library of parton density functions. Provided is also a set of control processes: qq̄→W→ℓν; qq̄→Z/γ∗→ℓℓ; gg,qq̄→tt̄ and gg→(tt̄→)WbWb̄; Reasons for new version: Implementation of several new processes and methods. Summary of revisions: Each version added new processes or functionalities, a detailed list is given in the section “Changes since AcerMC 1.0”. Restrictions: The package is optimised for the 14 TeV pp collision simulated in the LHC environment and also works at the achieved LHC energies of 7 TeV and 8 TeV. The consistency between results of the complete generation using PYTHIA 6.4 or HERWIG 6.5 interfaces is technically limited by the different approaches taken in both these generators for evaluating αQCD and αQED couplings and by the different models for fragmentation/hadronisation. For the consistency check, in the AcerMC library contains native coded definitions of the QCD and αQED. Using these native definitions leads to the same total cross-sections both with PYTHIA 6.4 or HERWIG 6.5 interfaces. Additional comments: !!!!! The distribution file for this program is over 67 Mbytes and therefore is not delivered directly when download or Email is requested. Instead an html file giving details of how the program can be obtained is sent. !!!!! Running time: On an PIII 800 MHz PC it amounts to ∼0.05→1.1 events/sec, depending on the choice of process. References:[1]T. Sjostrand et al., High energy physics generation with PYTHIA 6.2, eprint hep-ph/0108264, LU-TP 01-21, August 2001.[2]G. Julyesini et al., Comp. Phys. Commun. 67 (1992) 465, G. Corcella et al., JHEP 0101 (2001) 010.[3]E. Boos at al., Generic user process interface for event generators, hepph /0109068.[4]S. Jadach, J. H. Kuhn, Z. Was, Comput. Phys. Commun. 64 (1990) 275; M. Jezabek, Z. Was, S. Jadach, J. H. Kuhn, Comput. Phys. Commun. 70 (1992) 69; R. Decker, S. Jadach, J. H. Kuhn, Z. Was, Comput. Phys. Commun. 76 (1993) 361.[5]E. Barberio and Z. Was, Comp. Phys. Commun. 79 (1994) 291.","Borut Paul Kersevan and Elzbieta Richter-Wa̧s",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Clinical features of leiomyosarcoma of the urinary bladder: Analysis of 183 cases","Introduction
Experience with management of urinary bladder leiomyosarcoma (LMS) is rare. Therefore, to better elucidate the disease characteristics of urinary bladder LMS, we utilized a large population-based cancer registry to examine the epidemiology, natural history, pathological characteristics, prognostic factors, and treatment outcomes.
Material and methods
The Surveillance, Epidemiology, and End Results database (1973–2010) was used to identify cases by tumor site and histology codes. The association between clinical and demographic characteristics and long-term survival was examined.
Results
A total of 183 histologically confirmed cases were identified between 1973 and 2010. The annual age-adjusted incidence rate was 0.23 cases per 1,000,000 and did not significantly change over time. Median age of the patients was 65 years (interquartile range: 47–78y). Of the patients with a known pathologic tumor stage (n = 164), 50% had a regional or distant disease. Overall, 63.2% of patients with known histologic grade (n = 106), had poorly differentiated or undifferentiated histology. Most patients (92.9%) received cancer-directed surgery (CDS), with 34.4% having radical or partial cystectomy. Only 7.7% of patients received radiation therapy in combination with surgery. The median disease-specific survival was 46 months. Five- and 10-year cancer-specific survival rates were 47%, and 35%, respectively. On multivariate analysis, a worse outcome was associated with an undifferentiated tumor grade, distant disease, and failure to undergo CDS.
Conclusion
This series represents the largest cohort of LMS of the urinary bladder studied to date. LMS commonly presented as high grade and advanced stage with a poor prognosis. Reduced disease-specific survival was associated with increasing age, undifferentiated tumor grade, distant disease, and failure to undergo CDS.","Dayron Rodríguez and Mark A. Preston and Glen W. Barrisford and Aria F. Olumi and Adam S. Feldman",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Detecting changes in climate forcing on the fire regime of a North American mixed-pine forest: A case study of Seney National Wildlife Refuge, Upper Michigan","The study of forests dominated by red pine (Pinus resinosa Ait.), one of the few fire-resistant tree species of eastern North America, provides an opportunity to reconstruct long-term fire histories and examine the temporal dynamics of climate forcing upon forest fire regimes. We used a 300-year long spatially explicit dendrochronological reconstruction of the fire regime for Seney National Wildlife Refuge (SNWR, 38,531ha), eastern Upper Michigan to: (1) identify fire size thresholds with strong vs. weak climate controls, (2) evaluate effect of landform type (outwash channel vs. sand ridges) in modifying climate–fire associations, and (3) check for the presence of temporal changes in the climate control of large fire events over the time period 1700–1983. We used a summer drought sensitive red pine chronology (ITRDB code can037) as a proxy of past fire-related climate variability. Results indicated that fires >60ha in sand-ridge-dominated portions of SNWR and >100ha in outwash channels were likely climatically driven events. Climate–fire associations varied over time with significant climate–fire linkages observed for the periods 1700–1800 (pre-EuroAmerican), 1800–1900 (EuroAmerican settlement) and 1900–1983 (modern era). Although an increase in fire activity at the turn of 20th century is commonly associated with human sources of ignitions, our results suggest that such an increase was also likely a climatically driven episode.","Igor Drobyshev and P. Charles Goebel and Yves Bergeron and R. Gregory Corace",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Direct Kernel Perceptron (DKP): Ultra-fast kernel ELM-based classification with non-iterative closed-form weight calculation","The Direct Kernel Perceptron (DKP) (Fernández-Delgado et al., 2010) is a very simple and fast kernel-based classifier, related to the Support Vector Machine (SVM) and to the Extreme Learning Machine (ELM) (Huang, Wang, & Lan, 2011), whose α-coefficients are calculated directly, without any iterative training, using an analytical closed-form expression which involves only the training patterns. The DKP, which is inspired by the Direct Parallel Perceptron, (Auer et al., 2008), uses a Gaussian kernel and a linear classifier (perceptron). The weight vector of this classifier in the feature space minimizes an error measure which combines the training error and the hyperplane margin, without any tunable regularization parameter. This weight vector can be translated, using a variable change, to the α-coefficients, and both are determined without iterative calculations. We calculate solutions using several error functions, achieving the best trade-off between accuracy and efficiency with the linear function. These solutions for the α coefficients can be considered alternatives to the ELM with a new physical meaning in terms of error and margin: in fact, the linear and quadratic DKP are special cases of the two-class ELM when the regularization parameter C takes the values C=0 and C=∞. The linear DKP is extremely efficient and much faster (over a vast collection of 42 benchmark and real-life data sets) than 12 very popular and accurate classifiers including SVM, Multi-Layer Perceptron, Adaboost, Random Forest and Bagging of RPART decision trees, Linear Discriminant Analysis, K-Nearest Neighbors, ELM, Probabilistic Neural Networks, Radial Basis Function neural networks and Generalized ART. Besides, despite its simplicity and extreme efficiency, DKP achieves higher accuracies than 7 out of 12 classifiers, exhibiting small differences with respect to the best ones (SVM, ELM, Adaboost and Random Forest), which are much slower. Thus, the DKP provides an easy and fast way to achieve classification accuracies which are not too far from the best one for a given problem. The C and Matlab code of DKP are freely available.11http://www.gsi.dec.usc.es/~delgado/papers/dkp.","Manuel Fernández-Delgado and Eva Cernadas and Senén Barro and Jorge Ribeiro and José Neves",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"ARVO-CL: The OpenCL version of the ARVO package — An efficient tool for computing the accessible surface area and the excluded volume of proteins via analytical equations","Introduction of Graphical Processing Units (GPUs) and computing using GPUs in recent years opened possibilities for simple parallelization of programs. In this update, we present the modernized version of program ARVO [J. Buša, J. Dzurina, E. Hayryan, S. Hayryan, C.-K. Hu, J. Plavka, I. Pokorný, J. Skivánek, M.-C. Wu, Comput. Phys. Comm. 165 (2005) 59]. The whole package has been rewritten in the C language and parallelized using OpenCL. Some new tricks have been added to the algorithm in order to save memory much needed for efficient usage of graphical cards. A new tool called ‘input_structure’ was added for conversion of pdb files into files suitable for work with the C and OpenCL version of ARVO.
New version program summary
Program title: ARVO-CL Catalog identifier: ADUL_v2_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ADUL_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 11834 No. of bytes in distributed program, including test data, etc.: 182528 Distribution format: tar.gz Programming language: C, OpenCL. Computer: PC Pentium; SPP’2000. Operating system: All OpenCL capable systems. Has the code been vectorized or parallelized?: Parallelized using GPUs. A serial version (non GPU) is also included in the package. Classification: 3. External routines: cl.hpp (http://www.khronos.org/registry/cl/api/1.1/cl.hpp) Catalog identifier of previous version: ADUL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 165(2005)59 Does the new version supercede the previous version?: Yes Nature of problem: Molecular mechanics computations, continuum percolation Solution method: Numerical algorithm based on the analytical formulas, after using the stereographic transformation. Reasons for new version: During the past decade we have published a number of protein structure related algorithms and software packages [1,2,3,4,5,6] which have received considerable attention from researchers and interesting applications of such packages have been found. For example, ARVO [4] has been used to find that ratios of volume V to surface area A, for proteins in Protein Data Bank (PDB) distribute in a narrow range [7]. Such a result is useful for finding native structures of proteins. Therefore, we consider that there is a demand to revise and modernize these tools and to make them more efficient. Here we present the new version of the ARVO package. The original ARVO package was written in the FORTRAN language. One of the reasons for the new version is to rewrite it in C in order to make it more friendly to the young researchers who are not familiar with FORTRAN. Another, more important reason is to use the possibilities for speeding-up provided by modern graphical cards. We also want to eliminate the necessity of re-compiling the program for every molecule. For this purpose, we have added the possibility of using general pdb [8] files as an input. Once compiled, the program can receive any number of input files successively. Also, we found it necessary to go through the algorithm and to make some tricks for avoiding unnecessary memory usage so that the package becomes more efficient. Summary of revisions: 1. New tool. ARVO is designed to calculate the volume and accessible surface area of an arbitrary system of overlapping spheres (representing atoms), the biomolecules being just one albeit important, application. The user provides the coordinates and radii of the spheres as well as the radius of the probe sphere (water molecule for biomolecules). In the old version of ARVO the input of data was organized immediately in the code, which made it necessary to re-compile the program after every change in the input data. In the current version a module called ‘input_structure’ has been created to input the data from an independent external file. The coordinates and radii are stored in the file with extension *.ats (see the directory ‘input’ in the package). Each line in the file corresponds to one sphere (atom) and has the format 24.733−4.992−13.2562.800. The first three numbers are the (x,y,z) coordinates of the atom and the last one is the radius. It is important to remember that the radius of the probe sphere must be already added to this number. In the above example, the value 2.800 is obtained by the formula “sphere radius+probe sphere radius”. In the case of the arbitrary system of spheres the file *.ats is created by the user. In the case of proteins the ‘input_structure’ takes as an input a file in the format compatible with Protein Data Bank (pdb) format [8] and creates a corresponding *.ats file. It also assigns automatically, radii to individual spheres and (optionally) adds to all radii the probe sphere (water molecule) radius. As output, it produces a file containing coordinates of spheres together with radii. This file works automatically as an input for ARVO. Using an external tool allows users to create their own mappings of atoms and radii without the need to re-compile the tool ‘input_structure’ or program ARVO. It is again the user’s responsibility to assign proper radii to each type of atom. One can use any of the published standard sets of radii (see for example, [9,10,11,12,13]). Alternatively, the user can assign his own values for radii immediately in the module input_structure. The radii are assigned in a special file with extension *pds (see the documentation) which consists of lines like this: ATOM CA ALA 2.0 which is read as “the Calpha atom of Alanine has radius 2.0 Angstroms”. Here we provide for testing of the file rashin.pds where the radii are assigned according to [12]. The output file contains only recognized atoms. Atoms that were not recognized (are not part of mapping) are written to a separate log file allowing the user to review and correct the mapping files later. 2. The Language. Implementing the program in C is a natural first step when translating a program into OpenCL. This implementation is rewritten line-by-line from the original FORTRAN version of ARVO. 3. OpenCL implementation. OpenCL [14] is an open standard for parallel programming of heterogeneous systems. Unlike other parallelization technologies like CUDA [15] or ATI Stream [16] which are interconnected with specific hardware (produced by NVIDIA or ATI, respectively), OpenCL is vendor-independent, and programs written in OpenCL can be run on any hardware of companies supporting this standard, including AMD, INTEL, and NVIDIA. Programs written in OpenCL can be run without much change both on CPUs and GPUs. Improvements as compared with the original version: Support for files in the format as created by ‘input_structure’; input of parameters (name of input file) via command line; dynamic size of arrays—removal of the necessity to re-compile the program after any change in size of structures; memory allocation according to the real demands of the application; replacing north pole test by slight reduction of the radius (see below). To compile an OpenCL program, one needs to download and install the appropriate driver and software development kit (SDK). The program itself consists of two parts: a part running on the CPU and a part running on the GPU. The CPU initializes communication between the computer and the GPU, load data, processes and exports results. The GPU does the parallel part of calculation, consisting of the search for neighboring atoms and calculating the contribution of the area and volume of the individual atom to the total area and volume of the molecule. For details of the algorithm, please read Refs. [3,4]. In programming using OpenCL, more attention must be given to memory used than in a classical approach. Memory of the device is usually limited and therefore, some changes to the original algorithm are necessary. First, unlike in the FORTRAN version of the program, no structures containing the list of neighbor atoms are created. The search for the neighbors is done on-line, when the calculation of the contribution from individual atoms is being performed. Table 1Comparison of volumes and surface areas of different proteins obtained by original ARVO and by the new version. Different strategies for dealing with the “north pole” are applied. The first column contains the PDB ID of the protein and the number of atoms. Second column contains the volume of the protein obtained with original ARVO (upper number) and the difference with the new approach (lower number). Third column contains the same as in the second column for the surface area. Fourth column contains the number of rotations of the molecule in original ARVO (upper number) and the number of atoms whose radii have been reduced in the new version (lower number). Fifth column contains the relative errors for the volume (upper number) and the area (lower number).Protein atoms #Volume diffArea diffRotat. reduct.δvolume (%) δarea (%)3rn323,951.1804696858.3226363−1.04⋅10−7957−0.000025−0.0000071−1.02⋅10−73cyt40,875.86739511,455.4748323−3.85⋅10−61600−0.0015750.00141541.24⋅10−42act38,608.2430389054.00735041.28⋅10−416570.0494800.00173321.91⋅10−52brd43,882.73547910,918.20352921−7.84⋅10−71738−0.000344−0.0000971−8.88⋅10−78tln56,698.98888312,496.97806415−1.70⋅10−62455−0.0009660.00045943.67⋅10−61rr8105,841.50219227,983.15977218−6.60⋅10−74108−0.000699−0.0002144−7.65⋅10−71xi51743,445.092001863,139.88270314.42⋅10−715,6960.0077090.00007018.11⋅10−9 The strategy behind the North Pole check and molecule rotation [4, Sec. 4.7] has been changed. If during the north pole test, the north pole of the active sphere lies close to the surface of a neighboring sphere, the radius of such a neighboring sphere is multiplied by 0.9999 instead of rotating the whole molecule. This allows the algorithm to continue normally. Changing the radius of one atom changes the area and the volume of this atom by 0.02% and 0.03%, respectively. As the atom’s contribution to the total area (volume) of the protein is usually only a part of the atom’s total area (volume) and since there are many atoms in the protein itself, the change of total area (volume) is much smaller than 0.02% (0.03%). Testings showed relative errors ranging from 10−4 down to 10−8. An additional benefit of this approach is, that the whole molecule is not rotated and therefore no errors are introduced there which would occur during such rotation. We were even able to find a protein (1S1I having 31,938 atoms), where, after several hundreds of rotations, ARVO was not able to find such a position that the original north pole test could pass. For such proteins the new approach is the only one possible. Some data obtained using the north pole test (with rotation) and those without the north pole test (with radii reduction) are summarized in Table 1. The radius of water molecule was set to 1.4 Å, and Rashin’s set of the van der Waals radii of atoms [12] was used. The first column contains the protein name and the number of atoms. Each cell of the second and the third columns contains two numbers. The upper number is the volume (surface area) obtained using the original ARVO algorithm [4] with conventional north pole test and rotation. The lower number shows the difference coming from using the new approach. The upper number in the fourth column shows the number of rotations when using the original version and the second number is the number of atoms for which the radius has been reduced. The relative error of volume (upper number) and area (lower number) obtained by using radius reduction are shown in the last column. It can be seen clearly that the error is negligible. The disadvantage is that calculations using OpenCL are done with single precision only. This comes from the fact that the OpenCL standard does not support double precision float number operations as a basic part but as an extension only. This means that availability of double precision calculations depends on the device (CPU, GPU) vendor. Switching to double precision calculations downgrades speed performance (calculations in double precision are 8–2 times slower than the same calculations in single precision). Another problem is that after using the double precision switch, all calculations are done with double precision which leads to problems with insufficient memory. This problem can be bypassed by explicitly switching to single precision where possible but this requires careful modification of the whole program source. Since on our GPU (NVIDIA GTX 480) double precision was available, we have decided to use the double precision only for the critical parts of algorithm (s.a. integral calculation), leaving non-critical parts in single precision. This allowed us to speed up the calculation and to obtain acceptable results. Results of the test calculations are given in Table 2. All calculations except for 2brd0 have been performed using water radius 1.4 Å. The first column contains the protein name and the number of atoms. The second column contains computation time in seconds (in FORTRAN/CPU—upper part and OpenCL/GPU—lower part). The third column is a speed-up (time on the CPU divided by time on the GPU). The fourth and fifth columns contain the volume and area calculated in FORTRAN (upper number) and the difference when compared to results obtained by OpenCL (lower number). As one can see, the area and the volume obtained using FORTRAN (in double precision) and the OpenCL implementation (combination of single and double precisions) are practically the same. This is even more clear from the relative error of the OpenCL implementation as shown in the last column (upper number for volume, and lower number for area). As to computational time, FORTRAN (C) implementation is appropriate in the case when the calculation takes approximately less than 2 s. This is because in the case of OpenCL some time–about 0.3–1.5 s on testing configuration–is needed for the initialization of the device and for starting the communication. Speed-up is clearly visible for large proteins when the parallel approach can be exploited, but complexity of protein needs to be taken into account as well. Compare the times for 2brd (water radius 1.4 Å) and 2brd0 (water radius 0 Å). The difference is in the number of neighbors (overlapping spheres). While, for water radius 1.4 Å the number of neighbors is high and using the GPU is efficient, for water radius 0 Å it is better to use CPU. All results were obtained on a test configuration with CPU Intel Core i7 930 processor running at 2.8 GHz and a GPU NVIDIA GeForce GTX 480. Table 2The table contains comparative data on precision and computational times obtained by FORTRAN vs. OpenCL implementations of ARVO. The structure of the columns is similar to Table 1. Note that last protein (1s1i) was not calculated using FORTRAN implementation and comparison presented is between C and OpenCL version. This is because we were not able to find such rotation that north pole test would pass.Protein atoms #Time F95 (s) OpenCLSpeed upVolume diffArea diffδvolume (%) δarea (%)1eca8.236.0126,072.0030697004.1681381.65⋅10−510311.370.0043100.0004987.11⋅10−62ptn13.729.0139,273.2209339227.570716−2.01⋅10−516291.52−0.007906−0.005795−6.28⋅10−52brd15.779.9143,882.73513610,918.203432−1.44⋅10−517381.59−0.0063260.0014711.35⋅10−52brd00.290.9122,412.82580722,546.123881−9.13⋅10−517380.32−0.020471−0.008437−9.17⋅10−48tln23.3213.7456,698.98855012,496.977990−5.34⋅10−624551.70−0.003028−0.008708−4.64⋅10−41rr830.8917.67105,841.50149227,983.1595581.93⋅10−541081.750.020445−0.000802−2.87⋅10−61s1i286.8133.95816,980.348702253,160.674893−1.40⋅10−431,9388.45−1.1407630.0494781.95⋅10−5 At the time of writing, OpenCL allowed the allocation of only 1/4 of the total memory of the devices (CPU, GPU) by one call to malloc. This can be bypassed by four individual calls of memory allocation requesting 1/4 of the total devices’ memory. It is advisable to use a dedicated GPU for the calculations since sharing a GPU for calculations and displaying graphics can lead to unexpected results due to common access to the memory of devices. Restrictions: The program does not account for possible cavities inside the molecule. The current version works in a combination of single and double precisions (see Summary of revisions for details). Running time: Depends on the size of the molecule under consideration. For molecules whose running time was less than 2 s in the old version the performance is likely to decrease. This changes considerably when larger molecules are calculated (in test configuration speed-ups up to 34 were obtained). References:[1]F. Eisenmenger, U.H.E. Hansmann, S. Hayryan, C.-K. Hu, Comput. Phys. Commun. 138 (2001) 192.[2]F. Eisenmenger, U.H.E. Hansmann, S. Hayryan, C.-K. Hu, Comput. Phys. Commun. 174 (2006) 422.[3]S. Hayryan, C.-K. Hu, J. Skrivánek, E. Hayryan, I. Pokorný, J. Comput. Chem. 26 (2005) 334.[4]J. Busa, J. Dzurina, E. Hayryan, S. Hayryan, C.-K. Hu, J. Plavka, I. Pokorný, J. Skrivánek, M.-C. Wu, Comput. Phys. Commun. 165 (2005) 59.[5]J. Busa, S. Hayryan, C.-K. Hu, J. Skrivánek, M.-C. Wu, J. Comput. Chem. 30 (2009) 346.[6]J. Busa, S. Hayryan, C.-K. Hu, J. Skrivánek, M.-C. Wu, Comput. Phys. Commun. 181 (2010) 2116.[7]M.-C. Wu, M.S. Li, W.-J. Ma, M. Kouza, C.-K. Hu, EPL 96 (2011) 68005.[8]http://www.rcsb.org.[9]B. Lee, F.M. Richards, J. Mol. Biol. 55 (1971) 379.[10]F.M. Richards, Annu. Rev. Bipohys. Bioeng. 6 (1977) 151.[11]A. Shrake, J.A. Rupley, J. Mol. Biol. 79 (1973) 351.[12]A.A. Rashin, M. Iofin, B. Honig, Biochemistry 25 (1986) 3619.[13]C. Chotia, Nature 248 (1974) 338.[14]http://www.khronos.org/opencl/.[15]http://www.nvidia.com/object/cuda_home_new.html.[16]http://www.amd.com/stream.","Ján Buša and Shura Hayryan and Ming-Chya Wu and Ján Buša and Chin-Kun Hu",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Determination of small field output factors and correction factors using a Monte Carlo method for a 1000 MU/min CyberKnife® system equipped with fixed collimators","A new formalism for small field dosimetry has been proposed (Alfonso et al., 2008) with the concept of an additional correction factor (kQclin,Qmsrfclin,fmsr) which accounts for possible changes in detector response with field size. The aim of this work was to evaluate the response of eight commercially available detectors, then to provide a set of correction factors for a 1000 MU/min CyberKnife® equipped with fixed collimators and to compare them with those obtained for the 800 MU/min CyberKnife® version. Measurements were performed on a 1000 MU/min CyberKnife® with several active detectors designed for small field dosimetry (two chambers (PTW 31014 and 31018), three high resolution diodes (PTW 60016, 60017 and Sun Nuclear EDGE), a natural diamond (PTW 60003)) and two passive dosimeters (Harshaw TLD-700 7LiF:Mg,Ti thermoluminescent micro-cube and EBT3 radiochromic films). The CyberKnife® as well as the diode detectors, the PinPoint chamber, the diamond and the LiF micro-cubes were modeled with the PENELOPE Monte Carlo code in order to calculate the output factors in a point-like voxel of water (OFMC,w). A set of kQclin,Qmsrfclin,fmsr correction factors for the active detectors investigated is provided for the 1000 MU/min CyberKnife® in order to be used with the new formalism. A difference up to 2.4%, 2.0 and 1.7% in the correction factor obtained for the two different CyberKnife® models is found for the PTW 60003, the PTW 60016 and the PTW 60017 respectively. Although this difference is small, we recommend using specific kQclin,Qmsrfclin,fmsr correction factors for the 1000 MU/min CyberKnife® when they are available.","C. Moignier and C. Huet and V. Barraux and C. Bassinet and M. Baumann and K. Sebe-Mercier and C. Loiseau and A. Batalla and L. Makovicka",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Use of a Robotic Seal as a Therapeutic Tool to Improve Dementia Symptoms: A Cluster-Randomized Controlled Trial","Objectives
To test the effects of individual, nonfacilitated sessions with PARO (version 9), when compared against a look-alike plush toy and usual care, on the emotional and behavioral symptoms of dementia for people living in long-term care facilities.
Design
Parallel, 3-group, cluster-randomized controlled trial conducted between June 14, 2014, and May 16, 2015.
Setting
Twenty-eight long-term care facilities operated by 20 care organizations located in South-East Queensland, Australia.
Participants
Four hundred fifteen participants aged ≥60 years, with a documented diagnosis of dementia.
Intervention
Stratified by private/not-for-profit status and randomized using a computer-generated sequence, 9 facilities were randomized to the PARO group (individual, nonfacilitated, 15-minute sessions 3 times per week for 10 weeks); 10 to plush toy (same, but given PARO with robotic features disabled); and 9 to usual care. Treatment allocation was masked to assessors.
Measurements
Primary outcomes were changes in levels of engagement, mood states, and agitation after a 10-week intervention, assessed by coded video observations (baseline, weeks 1, 5, 10, and 15) and Cohen-Mansfield Agitation Inventory–Short Form (baseline, weeks 10 and 15). Analyses followed intention-to-treat, using repeated measures mixed effects models. Australian New Zealand Clinical Trials Registry (ACTRN12614000508673).
Results
Video data showed that participants in the PARO group were more verbally [3.61, 95% confidence interval (CI): 6.40–0.81, P = .011] and visually engaged (13.06, 95% CI: 17.05–9.06, P < .0001) than participants in plush toy. Both PARO (−3.09, 95% CI: −0.45 to −5.72, P = .022) and plush toy (−3.58, 95% CI: −1.26 to −5.91, P = .002) had significantly greater reduced neutral affect compared with usual care, whilst PARO was more effective than usual care in improving pleasure (1.12, 95% CI: 1.94–0.29, P = .008). Videos showed that PARO was more effective than usual care in improving agitation (3.33, 95% CI: 5.79–0.86, P = .008). When measured using the CMAI-SF, there was no difference between groups.
Conclusions
Although more effective than usual care in improving mood states and agitation, PARO was only more effective than a plush toy in encouraging engagement.","Wendy Moyle and Cindy J. Jones and Jenny E. Murfield and Lukman Thalib and Elizabeth R.A. Beattie and David K.H. Shum and Siobhan T. O'Dwyer and M. Cindy Mervin and Brian M. Draper",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A preliminary examination technique for audio evidence to distinguish speech from non-speech using objective speech quality measures","Forensic practitioners are faced more and more with large volumes of data. Therefore, there is a growing need for computational techniques to aid in evidence collection and analysis. With this study, we introduce a technique for preliminary analysis of audio evidence to discriminate between speech and non-speech. The novelty of our approach lies in the use of well-established speech quality measures for characterizing speech signals. These measures rely on models of human perception of speech to provide objective and reliable measurements of changes in characteristics that influence speech quality. We utilize this capability to compute quality scores between an audio and its noise-suppressed version and to model variations of these scores in speech as compared to those in non-speech audio. Tests performed on 11 datasets with widely varying characteristics show that the technique has a high discrimination capability, achieving an identification accuracy of 96 to 99% in most test cases, and offers good generalization properties across different datasets. Results also reveal that the technique is robust against encoding at low bit-rates, application of audio effects and degradations due to varying degrees of background noise. Performance comparisons made with existing studies show that the proposed method improves the state-of-the-art in audio content identification.","Erkam Uzun and Husrev T. Sencar",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The role of iPads in pre-school children's mark making development","The increased acquisition of touch-screen technologies, such as tablet computers, in both homes and schools raises important questions about their role for very young children's learning and development. Their inherent touch-based interaction offers new opportunities for mark making practices, which are linked to literacy development, through the emergent process of using marks as symbolic representation. This paper reports a comparative study of touch-based interaction using a tablet computer versus traditional physical paint and paper. Children aged 2–3 years engaged in a free finger painting activity and colouring in activity in both paper and digital environments. Video data of their interactions was used to develop a coding scheme for analyzing touch-based interaction, providing insight into how the use of fingers and hands differed in each environment, the different types and qualities of touch that were engendered, and the composition of the final paintings produced. Findings show that while the tablet computer limited the number of fingers used for interaction, its material affordances supported speed and continuity, which led to more mark making, and different ‘scales’ of mark making extending the range of mark making practices. At the same time it limited the sensory experience of physical paint and resulted in more uniform final compositions. The findings are discussed in terms of shaping young children's mark making, the implications of the use of touch screen technologies in literacy development for educational practitioners and technology design, and key future research directions.","Sara Price and Carey Jewitt and Lucrezia Crescenzi",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"kmos: A lattice kinetic Monte Carlo framework","Kinetic Monte Carlo (kMC) simulations have emerged as a key tool for microkinetic modeling in heterogeneous catalysis and other materials applications. Systems, where site-specificity of all elementary reactions allows a mapping onto a lattice of discrete active sites, can be addressed within the particularly efficient lattice kMC approach. To this end we describe the versatile kmos software package, which offers a most user-friendly implementation, execution, and evaluation of lattice kMC models of arbitrary complexity in one- to three-dimensional lattice systems, involving multiple active sites in periodic or aperiodic arrangements, as well as site-resolved pairwise and higher-order lateral interactions. Conceptually, kmos achieves a maximum runtime performance which is essentially independent of lattice size by generating code for the efficiency-determining local update of available events that is optimized for a defined kMC model. For this model definition and the control of all runtime and evaluation aspects kmos offers a high-level application programming interface. Usage proceeds interactively, via scripts, or a graphical user interface, which visualizes the model geometry, the lattice occupations and rates of selected elementary reactions, while allowing on-the-fly changes of simulation parameters. We demonstrate the performance and scaling of kmos with the application to kMC models for surface catalytic processes, where for given operation conditions (temperature and partial pressures of all reactants) central simulation outcomes are catalytic activity and selectivities, surface composition, and mechanistic insight into the occurrence of individual elementary processes in the reaction network.
Program summary
Program title: kmos Catalogue identifier: AESU_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AESU_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 27450 No. of bytes in distributed program, including test data, etc.: 2777387 Distribution format: tar.gz Programming language: Python 16.4%, fortran90: 83.6%. Computer: PC, Mac. Operating system: Linux, Mac, Windows. RAM: 100 MB+ Classification: 7.8. External routines: ASE, Numpy, f2py, python-lxml Nature of problem: Microkinetic simulations of complex reaction networks with all elementary processes occurring at active sites of a static lattice. Solution method: Efficient lattice kinetic Monte Carlo solution of the Markovian master equation underlying the reaction network. Unusual features: The framework implements a Fortran90 code generator Running time: From 10 s to 10 h","Max J. Hoffmann and Sebastian Matera and Karsten Reuter",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A non-dominated sorting genetic algorithm based approach for optimal machines selection in reconfigurable manufacturing environment","This paper deals with a problem of reconfigurable manufacturing systems (RMSs) design based on products specifications and reconfigurable machines capabilities. A reconfigurable manufacturing environment includes machines, tools, system layout, etc. Moreover, the machine can be reconfigured to meet the changing needs in terms of capacity and functionality, which means that the same machine can be modified in order to perform different tasks depending on the offered axes of motion in each configuration and the availability of tools. This problem is related to the selection of candidate reconfigurable machines among an available set, which will be then used to carry out a certain product based on the product characteristics. The selection of the machines considers two main objectives respectively the minimization of the total cost (production cost, reconfiguration cost, tool changing cost and tool using cost) and the total completion time. An adapted version of the non- dominated sorting genetic algorithm (NSGA-II) is proposed to solve the problem. To demonstrate the effectiveness of the proposed approach on RMS design problem, a numerical example is presented and the obtained results are discussed with suggested future research.","Abderrahmane Bensmaine and Mohammed Dahane and Lyes Benyoucef",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Environmental Factors That Impact the Eating Behaviors of Low-income African American Adolescents in Baltimore City","Objective
To understand environmental factors influencing the food-related habits of low-income urban African American adolescents.
Design
Qualitative research was conducted between February and April, 2010, using in-depth interviews, focus groups, and direct observation.
Setting
The study was conducted in low-income, predominantly African American neighborhoods of Baltimore City.
Participants
A total of 20 adolescents were interviewed in 18 in-depth interviews (n = 13) and 2 focus groups (n = 7). Participants were recruited from Baltimore City recreation centers and were eligible if they were African American and aged 10–16 years.
Phenomenon of Interest
The food-related habits of low-income, African American, urban adolescents and reported perceptions of their food environments.
Analysis
Interviews were audio recorded, transcribed, coded, and analyzed for emerging themes.
Results
Six thematic categories emerged and were organized into 4 environmental contexts: the neighborhood context (accessibility of food and safety of neighborhood), the school context (school food environment), the family context (family health history, role modeling, and monitoring) and the peer context (peer behaviors).
Conclusions and Implications
Future efforts to reduce the obesity epidemic among low-income African American adolescents should address the social environment of the family; however, positive behavior change may not be sustainable without neighborhood or school food environment modifications.","Karina M.H. Christiansen and Farah Qureshi and Alex Schaible and Sohyun Park and Joel Gittelsohn",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Acrobat ants go global – Origin, evolution and systematics of the genus Crematogaster (Hymenoptera: Formicidae)","This study unravels the evolution and biogeographic history of the globally distributed ant genus Crematogaster on the basis of a molecular phylogeny, reconstructed from five nuclear protein-coding genes and a total of 3384bp of sequence data. A particular emphasis is placed on the evolutionary history of these ants in the Malagasy region. Bayesian and likelihood analyses performed on a dataset of 124 Crematogaster ingroup taxa lend strong support for three deeply diverging phylogenetic lineages within the genus: the Orthocrema clade, the Global Crematogaster clade and the Australo-Asian Crematogaster clade. The 15 previous subgenera within Crematogaster are mostly not monophyletic. Divergence dating analyses and ancestral range reconstructions suggest that Crematogaster evolved in South-East Asia in the mid-Eocene (40–45ma). The three major lineages also originated in this region in the late Oligocene/early Miocene (∼24–30ma). A first dispersal out of S-E Asia by an Orthocrema lineage is supported for 22–30ma to the Afrotropical region. Successive dispersal events out of S-E Asia began in the early, and continued throughout the late Miocene. The global distribution of Crematogaster was achieved by subsequent colonizations of all major biogeographic regions by the Orthocrema and the Global Crematogaster clade. Molecular dating estimates and ancestral range evolution are discussed in the light of palaeogeographic changes in the S-E Asian region and an evolving ocean circulation system throughout the Eocene, Oligocene and Miocene. Eight dispersal events to/from Madagascar by Crematogaster are supported, with most events occurring in the late Miocene to Pliocene (5.0–9.5ma). These results suggest that Crematogaster ants possess exceptional dispersal and colonization abilities, and emphasize the need for detailed investigations of traits that have contributed to the global evolutionary success of these ants.","Bonnie B. Blaimer",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Mesenteric vein thrombosis can be safely treated with anticoagulation but is associated with significant sequelae of portal hypertension","Background
Mesenteric venous thrombosis (MVT) is a relatively uncommon but potentially lethal condition associated with bowel ischemia and infarction. The natural history and long-term outcomes are poorly understood and under-reported.
Methods
A single-institution retrospective review of noncirrhotic patients diagnosed with MVT from 1999 to 2015 was performed using International Classification of Diseases, Ninth Revision and radiology codes. Patients were excluded if no radiographic imaging was available for review. Eighty patients were identified for analysis. Demographic, clinical, and radiographic data on presentation and at long-term follow-up were collected. Long-term sequelae of portal venous hypertension were defined as esophageal varices, portal vein cavernous transformation, splenomegaly, or hepatic atrophy, as seen on follow-up imaging.
Results
There were 80 patients (57.5% male; mean age, 57.9 ± 15.6 years) identified; 83.3% were symptomatic, and 80% presented with abdominal pain. Median follow-up was 480 days (range, 1-6183 days). Follow-up radiographic and clinical data were available for 50 patients (62.5%). The underlying causes of MVT included cancer (41.5%), an inflammatory process (25.9%), the postoperative state (20.7%), and idiopathic cases (18.8%). Pancreatic cancer was the most common associated malignant neoplasm (53%), followed by colon cancer (15%). Twenty patients (26%) had prior or concurrent lower extremity deep venous thromboses. Most patients (68.4%) were treated with anticoagulation; the rest were treated expectantly. Ten (12.5%) had bleeding complications related to anticoagulation, including one death from intracranial hemorrhage. Four patients underwent intervention (three pharmacomechanical thrombolysis and one thrombectomy). One patient died of intestinal ischemia. Two patients had recurrent MVT, both on discontinuing anticoagulation. Long-term imaging sequelae of portal hypertension were noted in 25 of 50 patients (50%) who had follow-up imaging available. Patients with long-term sequelae had lower recanalization rates (36.8% vs 65%; P = .079) and significantly higher rates of complete as opposed to partial thrombosis at the initial event (73% vs 43.3%; P < .005). Long-term sequelae were unrelated to the initial cause or treatment with anticoagulation (P = NS).
Conclusions
Most cases of MVT are associated with malignant disease or an inflammatory process, such as pancreatitis. A diagnosis of malignant disease in the setting of MVT has poor prognosis, with a 5-year survival of only 25%. MVT can be effectively treated with anticoagulation in the majority of cases. Operative or endovascular intervention is rarely needed but important to consider in patients with signs of severe ischemia or impending bowel infarction. There is a significant incidence of radiographically noted long-term sequelae from MVT related to portal venous hypertension, especially in cases of initial complete thrombosis of the mesenteric vein.","Thomas S. Maldonado and Sheila N. Blumberg and Sharvil U. Sheth and Gabriel Perreault and Mikel Sadek and Todd Berland and Mark A. Adelman and Caron B. Rockman",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"GENXICC2.1: An improved version of GENXICC for hadronic production of doubly heavy baryons","We present an improved version of GENXICC, which is a generator for hadronic production of the doubly heavy baryons Ξcc, Ξbc and Ξbb and has been introduced by C.H. Chang, J.X. Wang and X.G. Wu [Comput. Phys. Commun. 177 (2007) 467; Comput. Phys. Commun. 181 (2010) 1144]. In comparison with the previous GENXICC versions, we update the program in order to generate the unweighted baryon events more effectively under various simulation environments, whose distributions are now generated according to the probability proportional to the integrand. One Les Houches Event (LHE) common block has been added to produce a standard LHE data file that contains useful information of the doubly heavy baryon and its accompanying partons. Such LHE data can be conveniently imported into PYTHIA to do further hadronization and decay simulation, especially, the color-flow problem can be solved with PYTHIA8.0.
NEW VERSION PROGRAM SUMMARY
Title of program: GENXICC2.1 Program obtained from: CPC Program Library Reference to original program: GENXICC Reference in CPC: Comput. Phys. Commun. 177, 467 (2007); Comput. Phys. Commun. 181, 1144 (2010) Does the new version supersede the old program: No Computer: Any LINUX based on PC with FORTRAN 77 or FORTRAN 90 and GNU C compiler as well Operating systems: LINUX Programming language used: FORTRAN 77/90 Memory required to execute with typical data: About 2.0 MB No. of bytes in distributed program: About 2 MB, including PYTHIA6.4 Distribution format: .tar.gz Nature of physical problem: Hadronic production of doubly heavy baryons Ξcc, Ξbc and Ξbb. Method of solution: The upgraded version with a proper interface to PYTHIA can generate full production and decay events, either weighted or unweighted, conveniently and effectively. Especially, the unweighted events are generated by using an improved hit-and-miss approach. Reasons for new version: Responding to the feedback from users of CMS and LHCb groups at the Large Hadron Collider, and based on the recent improvements of PYTHIA on the color-flow problem, we improve the efficiency for generating the unweighted events, and also improve the color-flow part for further hadronization. Especially, an interface has been added to import the output production events into a suitable form for PYTHIA8.0 simulation, in which the color-flow during the simulation can be correctly set. Typical running time: It depends on which option is chosen to match PYTHIA when generating the full events and also on which mechanism is chosen to generate the events. Typically, for the dominant gluon–gluon fusion mechanism to generate the mixed events via the intermediate diquarks in (cc)[3S1]3̄ and (cc)[1S0]6 states, setting IDWTUP=3 and unwght =.true., it takes 30 min to generate 105 unweighted events on a 2.27 GHz Intel Xeon E5520 processor machine; setting IDWTUP=3 and unwght =.false. or IDWTUP=1 and IGENERATE=0, it only needs 2 min to generate the 105 baryon events (the fastest way, for theoretical purposes only). As a comparison, for previous GENXICC versions, if setting IDWTUP=1 and IGENERATE=1, it takes about 22 hours to generate 1000 unweighted events. Keywords: Event generator; Doubly heavy baryons; Hadronic production. Summary of the changes (improvements): (1) The scheme for generating unweighted events has been improved; (2) One Les Houches Event (LHE) common block has been added to record the standard LHE data in order to be the correct input for PYTHIA8.0 for later simulation; (3) We present the code for connecting GENXICC to PYTHIA8.0, where three color-flows have to be correctly set for later simulation. More specifically, we present the changes together with their detailed explanations in the following: •Unweighted events generation. For theoretical studies, e.g. to derive the total baryon production cross-section or various differential distributions, one can directly use the fastest way, e.g. setting the PYTHIA parameter IDWTUP=3 and unwght =.false. or setting IDWTUP=1 and IGENERATE=0 (in these cases, xmaxup should be set as 0), to generate the baryon events [1]. By using GENXICC [2], [3] in this way, some interesting properties for hadronic production of Ξcc, Ξbc and Ξbb have been found in the literature, cf. Refs. [4], [5], [6]. While, for the events simulation in detector conditions, it is necessary to get the unweighted events. In previous GENXICC versions, the unweighted events are generated by setting IDWGTUP=1 and IGENERATE=1; i.e., the events are generated according to PYTHIA’s inner mechanism, the so-called hit-and-miss approach (von Neumann algorithm), to reject those unsatisfied events and output the allowed events. But, as is well-known, the original hit-and-miss approach is really time-consuming. Some alterations must be made to improve its efficiency.As an intermediate step, in BCVEGPY2.1a [7] we have suggested a practical trick to increase the efficiency of generating unweighted events (BCVEGPY is a generator for hadronic production Bc mesons [8]). In this trick, other than choosing the maximum differential cross-section as a reference weight in the hit-and-miss approach, we directly select an effective differential cross-section, which is smaller than the maximum one, as the reference weight [7]. This treatment can greatly improve the generation efficiency without affecting the total cross-section of the process. However, in using this trick to generate unweighted events such as for CMS detector simulation, one will incidentally find a false peak in the Bc–pt distributions. This is caused by the fact that sometimes the same event will be stored a (false) large number of times in the hit-and-miss process. Then, we are facing a dilemma: such a false peak can be avoided by raising the effective reference weight to a value approaching the maximum weight, but, conversely, a larger reference weight will surely lead to a much longer running time.One observes that by using the VEGAS algorithm [9], the SPRING-BASES program [10] performs the integration in using the BASES subroutines and generates events with a probability proportional to the integrand in using the SPRING subroutines. After each iteration of VEGAS running, the integration result and the maximum value of the function will be stored in a file for each cell of the adaptive mesh. In the generation stage, a cell is chosen with a probability proportional to the corresponding integral, and then a point in the cell is generated using the hit-and-miss approach. This method is highly efficient, but it has the disadvantage that the required amount of storage space grows exponentially with the integration dimension.Next, in the POWHEG program [11] the authors have developed a new method MINT [12] to replace the SPRING-BASES package. This MINT package also use the VEGAS algorithm to perform the integration. What’s different is that it does not store the value of the integral but stores the upper bound value for each cell. The multidimensional stepwise function that equals the upper bound of the function to be integrated in each cell is in fact an upper bound for the whole function, which is the wanted upper bound for BCVEGPY2.1a or PYTHIA. So, the program is to find the upper bound grid for those cells. And next, by using again the hit-and-miss technique in each cell, one can generate the points according to the original distribution.Based on these methods, as a further improvement, we present an ultimate solution to generate unweighted events in the present new GENXICC version. We adopt the MINT algorithm but with certain alterations to do the simulation. For this purpose, we change the VEGAS subroutine as follows. Three new variables have been added in the original VEGAS subroutine, where xint is the integral value for the integrand fxn after a ndim-dimensional integration, the xmax array records the upper bounding envelope of the integrand in all cells, imode is a flag:vegas(fxn,ndim,ncall,itmx,nprn,xint,xmax,imode)–When called with imode=0, vegas performs the integration over the integrand fxn, and stores the answer in a common block parameter vegsec.–xmax stands for a (nvegbin,ndim) dimensional array, where nvegbin denotes the bin number for each coordinate, and ndim stands for the integration dimension. When called with imode=1, vegas will first initialize all the elements of xmax to be xint1/ndim, where xint equals the value of vegsec that has been derived from a previous VEGAS running with imode=0. During the following sampling iteration, when the calculated integral value is larger than the initial xmax(nvegbin,ndim) value in a specific cell, then the value of xmax(nvegbin,ndim) for this cell will be increased by a fixed factor f=1+1/10ndim. After a sufficiently large number of calls, the values of xmax(nvegbin,ndim) will be stabilized for all cells. Such a final xmax array will be stored in the same grid file as that of the importance sampling function in order to do the final simulation. Comparing to the previous GENXICC versions, in doing the initialization (subroutine evntinit), we will call vegas twice with imode=0 and imode=1 accordingly to generate the upper bound grid xmax and also a more precise importance sampling function. Practically, the user can directly use the existing grid file derived by previous VEGAS running to generate events by setting methodevnt=2 or methodevnt=3 without running VEGAS again, which is the same as in the older GENXICC versions. Once the xmax array has been set up in the previous steps, one can call the subroutine gen to generate events. For this purpose, three options for calling the gen subroutine are programmed: where jmode=0 is to initialize a step-wise function xmmm which is described in [12]. And jmode=3 is to print out the generation statistics. The calling of the gen subroutine with jmode=1 has been implemented into the UPEVNT subroutine to generate events according to the probability proportional to the integrand. Each event produced needs several iterations with a three step procedure as follows: 1.Calculate the upper bounding function by generating a set of step-wise functions, each of them associated with a specific coordinate (dimension).2.Call the phase_gen subroutine to generate a random phase-space point and calculate the integral.3.Judge whether such a point be kept or not by using the hit-and-miss approach with the help of the upper bounding function. In VEGAS the integral together with its numerical error are related to the sampling numbers ncall and the iteration times itmx. So, to generate full events, we suggest the user do a test run first in order to find the effective and time-saving parameters for VEGAS. Furthermore, to validate the program, we use the same default parameters as the input for the program to generate mixed events via the intermediate diquark in (cc)[3S1]3̄ and (cc)[1S0]6 states, and the same for the other two doubly heavy baryons Ξbc and Ξbb. Fig. 1Comparison of the normalized Ξcc transverse momentum (PT) and rapidity (y) distributions derived by IDWTUP=3 (events) and IDWTUP=1 (differential cross-sections), which are represented by a solid line and dotted line respectively. As a cross-check, we derive the unweighted Ξcc event distributions by setting IDWTUP=3 andunwght =.true., and the weighted Ξcc differential distributions by setting IDWTUP=1 and IGENERATE=0, respectively, which are shown in Fig. 1. The two distributions after proper normalization agree well with each other. This demonstrates that our present scheme for unweighted events is correct. •Color-flow problem. Within the framework of non-relativistic QCD (NRQCD) [13], the production of the ΞQQ′ baryon can be factorized into two steps. The first step is to produce two free heavy-quark pairs QQ̄ and Q′Q̄′, which is perturbatively calculable. The second step is to make the two heavy quarks Q and Q′ into a bounding diquark (QQ′) in [3S1] (or [1S0]) spin state and in 3̄ (or 6) color state accordingly; then it will be hadronized into a ΞQQ′ baryon by grabbing a light quark u, d or s (plus a suitable number of gluons), whose probability is described by a non-perturbative NRQCD matrix element. More explicitly, the intermediate diquarks in Ξcc and Ξbb have two spin-and-color configurations [3S1]3̄ and [1S0]6; while for the intermediate diquark (bc) in Ξbc, there are four spin-and-color configurations Ξbc[3S1]3̄, Ξbc[3S1]6, Ξbc[1S0]3̄, and Ξbc[1S0]6.Since a baryon is constructed by three valence quarks, under the standard color-flow decomposition, there must be three different color-flow lines ending at a baryon [14], [3]. It is different from the case of mesons, where the color-flow lines of the quark and anti-quark inside a meson are continued. The previous PYTHIA6.4 can only generate full events with two or less independent color-flow lines, thus in GENXICC2.0, we adopt a ‘cheating method’ to generate the events. That is, by using the fact that 3⨂3=6⨁3̄ and 3⨂3̄=8⨁1 in the general QCD SU(3) color space [3]: –We combine any two of the color-flow lines ending with two quarks into one anti-color-flow line ending with one anti-quark with a color 3̄ that is different from the two quarks (the third color with respect to those of the two quarks);–Secondly, such an anti-color-flow line obtained by the combination may be continued (connected) to the remaining quark’s color-flow line in the baryon;–Finally, as a consequence, the color-flow lines ending at a baryon become ‘joined without ends’ at all, which is the requirement of the color-singlet bound state. However we should point out that due to approximation and simplification with the ‘cheating method’, the obtained information about the ‘tiny jets’, corresponding to the soft anti-quark and soft gluon(s) produced in fragmentation of the doubly heavy diquark, may not be very reliable. When the experimental analyzer uses the generator to simulate the baryon decay and other parton hardronization, they are still facing the color-flow rearrangement error; sometimes, PYTHIA will present an error message to show that the color-flow rearrangement is wrong during the parton’s evolution process, and then it will stop running. To generate full events of the doubly heavy baryons smoothly, the best way is to improve PYTHIA with proper treatment of the color-flow lines ending at the baryon. Fortunately, such an improvement has been done in its newest version PYTHIA8.0. Based on the suggestion from Peter Skands, we find that the further event simulation can be implemented in PYTHIA8.0 correctly even with the previously generated Les Houches Event (LHE) files [15]. As has been described in Ref. [16], the read-in of an external generator’s LHE files generated by PYTHIA6.4 is simply technically less sophisticated and less able to deal with junctions, even though the physics implementation of junction fragmentation is in principle the same. As a solution, PYTHIA8.0 improves the treatment of these LHE files. We adopt the same trick as that of BCVEGPY2.1a [7] to generate and record the data; i.e. two subroutines have been introduced in the file pythialheinit.F. One of the subroutine XICC_PYUPIN is used to fill the HEPRUP common block with information on the incoming beams and the allowed processes, and optionally stores that information on file. Another subroutine XICC_WRITE_LHE is used to store event information in the HEPEUP common block. And these two subroutines are called by the main program xicc.F to generate the LHE file that records the momentum and color information for the events [17]. More specifically, in the main program, the subroutine UPEVNT will be called for generating the baryon events, which is used to call the program to generate the baryon with a probability proportional to the importance sampling function. Here, one can also use the PYTHIA subroutine PYEVNT for the purpose, but one should at the same time switch off the hadronization, the initial and final state parton shower and so on, in order to avoid the color-flow rearrangement error. That generated baryon information together with the information of the accompanying partons will be stored in the Les Houches common block and will be exported to a LHE file “GENXICC.lhe”. And then, such a LHE file can be used when necessary by PYTHIA8.0 to do the following simulations. Here, to successfully simulate the baryon’s production and decay, the user needs to install PYTHIA8 [15] following the instructions on its official web-site: http://home.thep.lu.se/~torbjorn/Pythia.html. For using our generator, the user can use the following command to compile the configuration file, where $(PYTHIA8) stands for the PYTHIA8.0 installation directory. For convenience, we put an example configuration file in the package for generating the full baryon production and decay events in PYTHIA8.0, which is placed in the main folder of the program and is named as “genxicc.cc”.","Xian-You Wang and Xing-Gang Wu",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Cancer Incidence Among US Medicare ESRD Patients Receiving Hemodialysis, 1996-2009","Background
Patients with end-stage renal disease (ESRD) receiving dialysis have been reported to have increased risk of cancer. However, contemporary cancer burden estimates in this population are sparse and do not account for the high competing risk of death characteristic of dialysis patients.
Study Design
Retrospective cohort study.
Setting & Participants
US adult patients enrolled in Medicare’s ESRD program who received in-center hemodialysis.
Factors
Demographic/clinical characteristics.
Outcomes
For overall and site-specific cancers identified using claims-based definitions, we calculated annual incidence rates (1996-2009). We estimated 5-year cumulative incidence since dialysis therapy initiation using competing-risk methods.
Results
We observed a constant rate of incident cancers for all sites combined, from 3,923 to 3,860 cases per 100,000 person-years (annual percentage change, 0.1; 95% CI, −0.4 to 0.6). Rates for some common site-specific cancers increased (ie, kidney/renal pelvis) and decreased (ie, colon/rectum, lung/bronchus, pancreas, and other sites). Of 482,510 incident hemodialysis patients, cancer was diagnosed in 37,128 within 5 years after dialysis therapy initiation. The 5-year cumulative incidence of any cancer was 9.48% (95% CI, 9.39%-9.57%) and was higher for certain subgroups: older age, males, nonwhites, non-Hispanics, nondiabetes primary ESRD cause, recent dialysis therapy initiation, and history of transplantation evaluation. Among blacks and whites, we observed 35,767 cases compared with 25,194 expected cases if the study population had experienced rates observed in the US general population (standardized incidence ratio [SIR], 1.42; 95% CI, 1.41-1.43). Risk was most elevated for cancers of the kidney/renal pelvis (SIR, 4.03; 95% CI, 3.88-4.19) and bladder (SIR, 1.57; 95% CI, 1.51-1.64).
Limitations
Claims-based cancer definitions have not been validated in the ESRD population. Information for cancer risk factors was not available in our data source.
Conclusions
These results suggest a high burden of cancer in the dialysis population compared to the US general population, with varying patterns of cancer incidence in subgroups.","Anne M. Butler and Andrew F. Olshan and Abhijit V. Kshirsagar and Jessie K. Edwards and Matthew E. Nielsen and Stephanie B. Wheeler and M. Alan Brookhart",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"3D numerical modeling using smoothed particle hydrodynamics of flow-like landslide propagation triggered by the 2008 Wenchuan earthquake","The extremely strong Wenchuan earthquake triggered thousands of landslides in Sichuan Province, China. Flow-like landslides, such as the Tangjiashan, Wangjiayan, and Donghekou landslides were among the most destructive, causing many casualties and serious economic damage. It is therefore important to identify the flow mechanisms and investigate the specific characteristics of these flow-like landslides. This paper presents a three-dimensional model based on smoothed particle hydrodynamics (SPH) to simulate rapid landslide motion across 3D terrain. The Navier–Stokes equations in a CFD framework are used as governing equations, and artificial viscosity is incorporated into the pressure terms in the momentum equation to dissipate energy to avoid numerical oscillation and particle penetration, thus improving the stability of the numerical results. A non-Newtonian fluid model, the Bingham model, has proven suitable for describing the relationship between the shear strain rate and the shear stress in highly deformed soil materials. In the proposed model it is used as the constitutive model to describe the fluidization characteristics of flow-like landslides combined with the Mohr–Coulomb yield criterion. The model incorporates a no-slip boundary condition, to consider the effect of a solid boundary on slope movement. Ghost particles are created and assigned an artificial velocity. The viscous force caused by the solid boundary is calculated using the relative velocities between the fluid and the boundary particles. Open Multiprocessing (OpenMP), an API for multi-platform shared-memory parallel programming, is used to improve the efficiency of the SPH code running. To show the validity of the proposed approach, a benchmark problem of 3D dam break was simulated. The calculated distances of the surge front at different times agree well with the test results. Numerical modeling of the propagation of the Tangjiashan, Wangjiayan, and Donghekou landslides was performed by the application of SPH models to real flow-like landslides. The whole flow processes of these flow-like landslides across the 3D terrain are represented. The landslides change direction, split or join in, and spread or contract in their flow path in response to the local topography. Time-history curves of the velocity and displacement were obtained to analyze the movement characteristics of the landslide mass. The shapes of the deposition zones after slide occurrence were investigated. Comparisons of the SPH simulated geometry and the surveyed landslide configurations for the Tangjiashan, Wangjiayan and Donghekou landslides were conducted, and show a high degree of similarity. This indicates that the proposed 3D SPH model can accurately represent the evolution of the final slide shape. The prediction of the fluidization characteristics of earthquake-induced flow-like landslides can notably reduce sudden loss of life, as it provides a means for mapping hazardous areas, for estimating the hazard intensity, and for identification and design of appropriate protective measures.","Zili Dai and Yu Huang and Hualin Cheng and Qiang Xu",2014,"[""Science Direct"",""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Challenges in essential tremor genetics","The field of essential tremor (ET) genetics remains extremely challenging. The relative lack of progress in understanding the genetic etiology of ET, however, does not reflect the lack of a genetic contribution, but rather, the presence of substantial phenotypic and genotypic heterogeneity. A meticulous approach to phenotyping is important for genetic research in ET. The only tool for phenotyping is the clinical history and examination. There is currently no ET-specific serum or imaging biomarker or defining neuropathological feature (e.g., a protein aggregate specific to ET) that can be used for phenotyping, and there is considerable clinical overlap with other disorders such as Parkinson's disease (PD) and dystonia. These issues greatly complicate phenotyping; thus, in some studies, as many as 30–50% of cases labeled as “ET” have later been found to carry other diagnoses (e.g., dystonia, PD) rather than ET. A cursory approach to phenotyping (e.g., merely defining ET as an “action tremor”) is likely a major issue in some family studies of ET, and this as well as lack of standardized phenotyping across studies and patient centers is likely to be a major contributor to the relative lack of success of genome wide association studies (GWAS). To dissect the genetic architecture of ET, whole genome sequencing (WGS) in carefully characterized and well-phenotyped discovery and replication datasets of large case-control and familial cohorts will likely be of value. This will allow specific hypotheses about the mode of inheritance and genetic architecture to be tested. There are a number of approaches that still remain unexplored in ET genetics, including the contribution of copy number variants (CNVs), ‘uncommon’ moderate effect alleles, ‘rare’ variant large effect alleles (including Mendelian and complex/polygenic modes of inheritance), de novo and gonadal mosaicism, epigenetic changes and non-coding variation. Using these approaches is likely to yield new ET genes.
Résumé
La génétique du tremblement essentiel reste un défi majeur. Néanmoins, le relatif manque de progrès dans la compréhension des étiologies génétiques du tremblement essentiel ne reflète pas l’absence de contribution de la génétique dans cette pathologie mais plutôt la présence d’une forte hétérogénéité à la fois phénotypique et génotypique. Une approche phénotypique méticuleuse est primordiale pour mettre en évidence les bases génétiques du tremblement essentiel. Le seul outil à disposition pour phénotyper le tremblement essentiel est l’histoire clinique et l’examen neurologique. Il n’y a actuellement pas de biomarqueur sérique ou d’imagerie qui soit spécifique du tremblement essentiel ni de caractéristique neuropathologique (notamment l’agrégation d’une protéine spécifique du tremblement essentiel) qui puisse être utilisée pour le phénotypage. De même, il y a un chevauchement clinique considérable entre le tremblement essentiel et d’autres pathologies telles que la maladie de Parkinson et la dystonie. Ces problèmes compliquent considérablement le phénotypage. Ainsi, dans certaines études, 30 à 50 % des cas considérés comme un tremblement essentiel ont finalement un autre diagnostic qu’il s’agisse d’une dystonie ou d’une maladie de Parkinson. Une approche plus précise du phénotypage, basée notamment sur la définition du tremblement essentiel comme un tremblement d’action, est probablement un point clé dans l’étude de familles atteintes de tremblement essentiel. De même, l’absence de phénotypage standardisé dans les différentes études et centres évaluateurs contribue vraisemblablement à l’échec relatif des études génétiques d’association de type GWAS. Pour mettre en évidence la base génétique du tremblement essentiel, les approches de type séquençage à haut débit du génome portant sur des patients et des familles examinés de façon attentive et bien phénotypée suivies d’une réplication des résultats obtenus dans de larges cohortes de cas–témoins et de familles pourraient être la clé du succès. Cela pourrait permettre de tester des hypothèses particulières concernant le mode de transmission du tremblement essentiel et ses bases génétiques. De multiples approches n’ont pas encore été explorées concernant la génétique du tremblement essentiel incluant la contribution des variations du nombre de copies (CNV), le mosaïcisme gonadique et le mosaïcisme de novo, les allèles peu fréquents d’effet modéré, les allèles rares d’effet marqué (incluant les modes de transmission mendélienne et complexe, voire polygénique), les modifications épigénétiques et les variations de l’ADN non codant. L’usage de ces approches pourrait nous permettre d’identifier enfin des gènes responsables du tremblement essentiel.","L.N. Clark and E.D. Louis",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"As empresas olham além de seus muros para inovar?","RESUMO
Este artigo trata da influência de atores como universidades, centros de pesquisa, consultorias, fornecedores, clientes e concorrentes em processos de inovação de firmas industriais. Com as transformações na economia nas últimas três décadas, os fluxos de informação e a difusão de conhecimento passaram a ser mais cada vez mais importantes para a qualidade do sistema produtivo e para a competitividade das empresas. Tais atores externos às firmas conformam redes que criam conexões para processos de aprendizado e transferência de conhecimento. A hipótese é que o desempenho inovador de empresas industriais brasileiras tem relação direta com o grau de interação destas com agentes como universidades, centros de pesquisa, fornecedores, clientes e consultorias. A análise de entrevistas com 106 empresários – realizadas na Pesquisa de Atitudes Empresariais para Desenvolvimento e Inovação (PAEDI – CEBRAP/IPEA) – indica uma relação entre atitude inovadora mais forte e grau de interação: empresas com alto grau de interação com atores externos tendem a ser aquelas mais inovadoras.
ABSTRACT
This paper analyses the influence of universities, research institutes, laboratories, consultancies, suppliers and clients on the innovation processes held at firms. These external actors tend to outline networks considered essential for learning and innovation processes. Changes in the systems of production during the last 30 years made information flows and knowledge diffusion stand out as core factors for competitiveness. The main hypothesis of this paper is that the innovative performance of Brazilian manufacturing firms is related to the levels of interaction that companies develop with such external actors. Analysis based on interviews with entrepreneurs and CEOs of 106 Brazilian companies – from the Research of Entrepreneurs Attitudes for Development and Innovation (PAEDI) – show a relationship between innovative attitude and interactional level: firms with higher levels of interaction with “external actors” tend to be the ones who area more innovative.","Carlos Torres-Freire and Frederico Henriques",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Defining operative mortality: Impact on outcome reporting","Objective
Death is an important outcome of procedural interventions. The death rate, or mortality rate, is subject to variability by definition. The Society of Thoracic Surgeons Adult Cardiac Surgery Database definition of “operative” mortality originally included all in-hospital deaths and deaths occurring within 30 days of the procedure. In recent versions of the Society of Thoracic Surgeons Adult Cardiac Surgery Database, “in-hospital” has been modified to include “patients transferred to other acute care facilities,” and “deaths within 30 days unless clearly unrelated to the procedure” has been changed to “deaths within 30 days regardless of cause.” This study addresses the impact of these redefinitions on outcome reporting.
Methods
The California Office of Statewide Health Planning and Development hospitalized patient discharge database was queried for the year 2009, the most recent year that data files could be linked to the vital statistics death files to include all-cause mortality. Isolated coronary artery bypass grafting, isolated valve, coronary artery bypass grafting valve, and percutaneous coronary intervention procedures were identified by International Classification of Diseases, Ninth Edition, Clinical Modification procedure codes. Percutaneous coronary intervention procedures were further divided into acute coronary syndrome (percutaneous coronary intervention acute coronary syndrome) and all other percutaneous coronary intervention (percutaneous coronary intervention no acute coronary syndrome). Deaths were counted by 5 methods depending on the time and place of occurrence: (1) in-hospital or during the index hospitalization; (2) in-hospital + connected hospitalization, defined as a transfer to another acute care facility on the same day or within 24 hours of discharge; (3) in-hospital + 30 day, death during index hospitalization or within 30 days after the procedure; (4) in-hospital + connected + 30 day readmission, death during index hospitalization, transfer to acute care facility, or deaths during readmission within 30 days; and (5) in-hospital + connected + 30 day. To study the impact of these operative mortality definitions, we examined 5 different methods to track mortality and performed 2 separate analyses. The first analysis did not exclude any patients, and the second analysis excluded any patient who could not be accurately tracked after hospital discharge.
Results
In the first analysis with no patients excluded, a total of 17% (117/697) of surgical deaths and 31% (409/1324) of percutaneous coronary intervention deaths were counted after the original hospitalization. The highest percentage of posthospital deaths occurred after elective percutaneous coronary intervention: 45% (135/301). In surgical patients, the highest percentage of posthospital deaths occurred in coronary artery bypass grafting procedures: 20% (57/284). In the second analysis, with untrackable patients excluded, hospital deaths included 12% (161/1324) for percutaneous coronary intervention compared with 4% (30/697) for surgical procedures.
Conclusions
A significant percentage of procedural deaths occur after transfer or discharge from the index hospital. This is especially evident in the percutaneous coronary intervention group. These findings illustrate the importance of the definition of “operative” mortality and the need to ensure accuracy in the reporting of data to voluntary clinical registries, such as the Society of Thoracic Surgeons Adult Cardiac Surgery Database and National Cardiovascular Data Registry.","Steven Maximus and Jeffrey C. Milliken and Beate Danielsen and Junaid Khan and Richard Shemin and Joseph S. Carey",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"The Gametocytocidal Efficacy of Different Single Doses of Primaquine with Dihydroartemisinin-piperaquine in Asymptomatic Parasite Carriers in The Gambia: A Randomized Controlled Trial","Background
Asymptomatic low-density gametocyte carriers represent the majority of malaria-infected individuals. However, the impact of recommended treatment with single low dose of primaquine and an artemisinin-based combination therapy to reduce transmission in this group is unknown.
Methods
This was a four-arm, open label, randomized controlled trial comparing the effect of dihydroartemisinin-piperaquine (DHAP) alone or combined with single dose of primaquine (PQ) at 0.20mg/kg, 0.40mg/kg, or 0.75mg/kg on Plasmodium falciparum gametocytaemia, infectiousness to mosquitoes and hemoglobin change in asymptomatic, malaria-infected, glucose-6-phosphate dehydrogenase (G6PD) normal individuals. Randomization was done using a computer-generated sequence of uneven block sizes with codes concealed in sequentially numbered opaque envelopes. The primary endpoint was the prevalence of P. falciparum gametocytemia at day 7 of follow-up determined by quantitative nucleic acid sequence based assay and analysis was by intention to treat. The trial has been concluded (registration number: NCT01838902; https://clinicaltrials.gov/ct2/show/NCT01838902).
Results
A total of 694 asymptomatic, malaria-infected individuals were enrolled. Gametocyte prevalence at day 7 was 37.0% (54/146; 95% CI 29.2–45.4), 19.0% (27/142; 95% CI 12.9–26.4), 17.2% (25/145; 95% CI 11.0–23.5) and 10.6% (15/141; 95% CI 6.1–16.9) in the DHAP alone, 0.20mg/kg, 0.40mg/kg, and 0.75mg/kg PQ arms, respectively. The main adverse events reported include headache (130/471, 27.6%), cough (73/471, 15.5%), history of fever (61/471, 13.0%) and abdominal pain (57/471, 12.1%). There were five serious adverse events however, none was related to the interventions.
Interpretation
A single course of PQ significantly reduces gametocyte carriage in malaria-infected asymptomatic, G6PD-normal individuals without increasing the risk of clinical anemia. The limited number of successful mosquito infections suggests that post-treatment transmission potential in this asymptomatic population is low.","Joseph Okebe and Teun Bousema and Muna Affara and Gian Luca Di Tanna and Edgard Dabira and Abdoulaye Gaye and Frank Sanya-Isijola and Henry Badji and Simon Correa and Davis Nwakanma and Jean-Pierre Van Geertruyden and Chris Drakeley and Umberto D'Alessandro",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Cuidado de enfermería al paciente en postoperatorio temprano de revascularización miocárdica","La etapa temprana postoperatoria después de un injerto de bypass de arteria coronaria, sigue siendo una de las fases más críticas para los pacientes que recibieron intervención quirúrgica del corazón. Se pretendió diseñar una propuesta de cuidado de enfermería con base en los problemas que presentan los pacientes en esta fase del proceso de recuperación (48–96 horas), a partir de la descripción e interpretación de los eventos clínicos que requieren cuidados de enfermería (ECRCE) y situaciones que requieren cuidado de enfermería (SRCE). Los ECRCE incluyeron datos cuantitativos en los sistemas neurológico, cardiovascular y respiratorio, y en piel. Estos datos se recolectaron a través de la Hoja de Registro de Información (HRI), historia clínica e información general, diagnósticos y datos relacionados con la cirugía. Por otro lado, los datos cualitativos, que incluyen bienestar, logros, razonamiento, beneficio, complacencia, creencias y valores, sufrimiento, agobio y pesadumbre se investigaron a través de una entrevista semiestructurada a profundidad. Los datos cuantitativos (ECRCE) se analizaron mediante el modelo de Rasch, estadística descriptiva y análisis de correspondencias múltiple y el método de clasificación (cluster analysis). Los datos cualitativos (SRCE) fueron codificados y agrupados en categorías, y luego analizados e interpretados en consecuencia. El análisis mostró que los cambios que se presentan inmediatamente después de la cirugía, permanecen a lo largo de todo el proceso de recuperación. La identificación de éstos permitió la elaboración de una propuesta de cuidado de enfermería a los pacientes durante by pass aorto-coronario (CABG).
Postoperative early stage after grafting of coronary artery bypass, remains one of the most critical phases for the patients undergoing heart surgery. We intended to design a nursing proposal based on the problems presented by patients at this stage of the recovery process (48–96 hours), from the description and interpretation of clinical events requiring nursing care (CERNC) and situations that require nursing care (SRNC). The CERNC included quantitative data on the neurological, cardiovascular and respiratory systems and in the skin. These data were collected through Record Sheet Information (RSI), medical history and general information, diagnostics and surgery-related data. Secondly, qualitative data, which include welfare, achievements, reasoning, benefit, complacency, beliefs and values, suffering, anxiety and grief were investigated through in-depth semi-structured interviews. Quantitative data (CERNC) were analyzed using the Rasch model, descriptive statistics and multiple correspondence analysis and classification method (cluster analysis). Qualitative data (SRNC) were coded and grouped into categories, and then analyzed and interpreted accordingly. The analysis showed that the changes that occur immediately after surgery remain along the whole recovery process. The identification of these led to the drafting of a proposed nursing care to patients during CABG.","Claudia Ariza",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Management of Pulmonary Nodules by Community Pulmonologists: A Multicenter Observational Study","BACKGROUND
Pulmonary nodules (PNs) are a common reason for referral to pulmonologists. The majority of data for the evaluation and management of PNs is derived from studies performed in academic medical centers. Little is known about the prevalence and diagnosis of PNs, the use of diagnostic testing, or the management of PNs by community pulmonologists.
METHODS
This multicenter observational record review evaluated 377 patients aged 40 to 89 years referred to 18 geographically diverse community pulmonary practices for intermediate PNs (8-20 mm). Study measures included the prevalence of malignancy, procedure/test use, and nodule pretest probability of malignancy as calculated by two previously validated models. The relationship between calculated pretest probability and management decisions was evaluated.
RESULTS
The prevalence of malignancy was 25% (n = 94). Nearly one-half of the patients (46%, n = 175) had surveillance alone. Biopsy was performed on 125 patients (33.2%). A total of 77 patients (20.4%) underwent surgery, of whom 35% (n = 27) had benign disease. PET scan was used in 141 patients (37%). The false-positive rate for PET scan was 39% (95% CI, 27.1%-52.1%). Pretest probability of malignancy calculations showed that 9.5% (n = 36) were at a low risk, 79.6% (n = 300) were at a moderate risk, and 10.8% (n = 41) were at a high risk of malignancy. The rate of surgical resection was similar among the three groups (17%, 21%, 17%, respectively; P = .69).
CONCLUSIONS
A substantial fraction of intermediate-sized nodules referred to pulmonologists ultimately prove to be lung cancer. Despite advances in imaging and nonsurgical biopsy techniques, invasive sampling of low-risk nodules and surgical resection of benign nodules remain common, suggesting a lack of adherence to guidelines for the management of PNs.
Materials and Methods
This was a multicenter, community-based, retrospective observational study of patients with PNs, ranging from 8 to 20 mm in diameter, presenting to 18 geographically representative outpatient pulmonary clinics across the United States. The study was approved at 15 sites by a central institutional review board and at three sites by local institutional review board approval.
Site Selection
Four hundred forty sites were identified based on investigator databases and claims data from a large insurance carrier whose coverage population was representative of the overall US population. Of these, 77 sites expressed interest in participating, and 48 sites went on to sign confidentiality agreements. Of these, 17 did not request additional information, leaving 31 sites undergoing qualification review. Eighteen outpatient pulmonary clinics were chosen to participate based on the following criteria: (1) management of patients with PNs, (2) availability of medical records, and (3) ability to perform data abstraction. In addition, investigators targeted enrollment of geographically diverse patients to limit the potential bias associated with differences in practice patterns and to account for variation in disease prevalence (eg, endemic mycoses) that could alter management decisions.
Patient Selection
Patients were identified by querying databases (eg, billing and scheduling systems) using five International Classification of Diseases, Ninth Revision, Clinical Modification codes for PN (793.1, 786.6, 518.89, 519.8, 519.9) to ensure homogeneity in patient identification and inclusion.17 Manual chart abstraction was then used to identify those who met the criteria. To minimize selection bias, the sites were not permitted to use additional codes during database query to identify patients. To ensure a systematic sample, patient eligibility was determined by examining consecutively referred patients to the site. Inclusion criteria included age ≥ 40 years and ≤ 89 years at the time of nodule finding, presentation to a pulmonologist, nodule size 8 to 20 mm, and definitive diagnosis ascertained by tissue diagnosis or radiographic follow-up for 2 years. Exclusion criteria included chest CT scan performed > 60 days prior to the initial visit, prior diagnosis of any cancer within 2 years of nodule detection, or incomplete chart data. Patients were categorized into three groups by the most invasive procedure performed during management, as follows: surveillance (serial imaging), biopsy (CT scan-guided transthoracic needle aspiration [TTNA] or bronchoscopy), or surgery (including mediastinoscopy, video-assisted thorascopic surgery, and/or thoracotomy).
Data Collection
Clinical data were abstracted retrospectively by designated study staff into an electronic data capture system from initial consultation through establishment of a definitive diagnosis (ie, pathology results) or a minimum 2-year follow-up. Data included patient demographic and clinical characteristics, PN characteristics, imaging tests, invasive testing, and surgery. PET scan reports were reviewed and abstracted where available for a subset of patients. To adjudicate a PET scan report as positive or negative, an algorithm was developed that prioritized the following components of the report from highest to lowest: final radiology impression, description of findings, and standard uptake values (SUVs) (e-Fig 1). PET scanning was defined as negative if the report included any of the following statements: no evidence of malignancy, no 18F-fluorodeoxyglucose uptake or hypermetabolic activity, or an SUV of > 2.5. A positive PET scan was defined as a report that included any of the following statements: concern or suggestion of malignancy, findings that described increased 18F-fluorodeoxyglucose uptake or hypermetabolic activity, or an SUV ≥ 2.5. In the 27 cases in which the findings were discordant with the final impression, adjudication was performed by two independent pulmonologists and agreement was reached in all cases. Data quality was ensured through ongoing site monitoring. Programmed edit checks were built into the electronic data capture system and at the conclusion of chart abstraction, each site provided access to a random 10% sample of deidentified patient records for review.
Pretest Probability of Malignancy
Two previously developed and validated models9, 10 were used to estimate the pCA in each patient. Model accuracy was determined by comparing the pCA with the final diagnosis. Receiver operating characteristic curves and the area under the curve were generated with 95% CIs. The pCA was calculated for each patient and categorized into three groups (< 5%, 5% to < 65%, and ≥ 65%). Procedure use by group was examined.
Data Analysis
χ2 or analysis of variance tests were used to compare subgroups, and P values > .05 were considered significant. A nodule was classified as benign based on confirmed benign pathology or the absence of radiographic change as determined by the managing physician during surveillance for at least 2 years. Multivariate logistic regression was performed to identify factors associated with the use of an invasive diagnostic procedure. All statistical analyses were performed using SAS/STAT, version 9.3 (SAS Institute inc).","Nichole T. Tanner and Jyoti Aggarwal and Michael K. Gould and Paul Kearney and Gregory Diette and Anil Vachani and Kenneth C. Fang and Gerard A. Silvestri",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Effect of turbulence intensity on PM emission of heavy duty diesel trucks - Wind tunnel studies","Stringent emission regulations have forced drastic technological improvements in diesel aftertreatment systems, particularly in reducing Particulate Matter (PM) emissions. The formation and evolution of PM from modern engines are more sensitive to overall changes in the dilution process, such as rapidity of mixing, background PM present in the air. These technological advancements were made in controlled laboratory environments compliant with measurement standards (i.e. Code of Federal Regulation CFR in the USA) and are not fully representative of real-world emissions from these engines or vehicles. In light of this, a specifically designed and built wind tunnel by West Virginia University (WVU) is used for the study of the exhaust plume of a heavy-duty diesel vehicle, providing a better insight in the dilution process and the representative nanoparticles emissions in a real-world scenario. The subsonic environmental wind tunnel is capable of accommodating a full-sized heavy-duty truck and generating wind speeds in excess of 50mph. A three-dimensional gantry system allows spanning the test section and sample regions in the plume with accuracy of less than 5 mm. The gantry system is equipped with engine exhaust gas analyzers and PM sizing instruments. The investigation involves three different heavy-duty Class-8 diesel vehicles representative of three emission regulation standards, namely a US-EPA 2007 compliant, a US-EPA 2010 compliant, and a baseline vehicle without any aftertreatment technologies as a pre US-EPA 2007, respectively. The testing procedure includes three different vehicle speeds: idling, 20mph, and 35mph. The vehicles were tested on WVU's medium-duty chassis dynamometer, with the load applied to the truck reflecting the road load equation at the corresponding vehicle test speeds. Wind tunnel wind speed and vehicle speed were maintained in close proximity to one another during the entire test. Results show that the cross-sectional plume area increases with increase in distance away from tailpipe. Also indicating the cooling and dilution of the exhaust begins at close vicinity to the tailpipe. The rate of cooling and dilution are greatest in early stages of the dilution process for the areas with high turbulence intensity (TI), where strong mixing phenomena occurs, leading to the formation of a predominant nucleation mode. On the other hand, the core of the plume observes a slower cooling and dilution rate. This difference is reflected in the PM formation and evolution of these two distinct regions, as shown by the particle size distributions and number concentrations. Continuous mixing will tend to mellow those differences, but its “final” result is related to the dilution history.","D. Littera and A. Cozzolini and M. Besch and D. Carder and M. Gautam",2017,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Adverse reactions among patients being treated for multi-drug resistant tuberculosis at Abbassia Chest Hospital","Background
Pulmonary tuberculosis is a major cause of morbidity and mortality worldwide, resulting in the greatest number of deaths due to any one single infectious agent. Drug resistance threatens global tuberculosis control efforts.
Objective
The aim of this study was to assess adverse reactions of second-line TB drugs in patients treated for MDR-TB at Abbassia Chest Hospital from 1st of January 2009 to 1st of January 2012.
Subjects and methods
This study included 107 patients admitted at Abbassia Chest Hospital; during the period from January 2009 to January 2012. The patients were resistant to at least Rifampicin and INH. All patients’ files were analyzed and the following data were discussed: meticulous history taking, complete clinical examination, drug susceptibility testing, and initial laboratory investigations, adverse reactions were determined by clinical criteria and/or laboratory data, severity code, management of side effects and fate of treatment.
Results
72.9% of the patients were males and 27.1% were females. The mean of age was 37.1years. The special habits detected among the studied cases were tobacco smoking, drug addiction and alcohol intake. According to type of resistance, acquired resistance was 95.3% and primary resistance was 4.7%. The most common co-morbidities associated with MDR-TB in the studied cases were diabetes (29.9%) and chronic obstructive lung disease (11.2%). Side effects of drugs were; 57% GIT manifestations, 53.3% peripheral neuritis, hypokalemia 26.2%, irritable bowel syndrome 22.4%, ototoxicity 17.8%, skin reaction 10.3%, hypothyroidism 10.3%, hepatotoxicity 9.3%, hypoalbuminemia 5.6%, depression 3.7%, arthritis 0.9%, gynecomastia 2.8%, hyponatremia 5.6%, hypomagnesaemia 1.9%, dizziness 0.9%, nephrotoxicity 3.7%. Most of the drugs’ side effects started to appear within the first 3months of treatment. The frequency of nephrotoxicity, hepatotoxicity and hypoalbuminemia were significantly higher in diabetic than in non-diabetic cases. Elevations of liver enzymes began from the 3rd month after treatment and these elevations became statistically significant beginning from the 6th month. Also, elevations of creatinine levels began from the 3rd month after treatment and became statistically significant beginning from the 6th month, while there were no significant changes in potassium levels among the studied cases all through the follow up period. It was noticed that highly significant gain of body weight started from the 3rd month after treatment. 92.5% of the studied cases were cured, 6.5% died and 0.9% was defaulter. The predictors of patients’ outcome were sputum conversion, number of previous TB treatment and associated co-morbidities.
Conclusions
There is a relation between both tobacco smoking and drug addiction, and MDR TB. The most common type of resistance is acquired resistance because of lack of adherence to treatment or inappropriate treatment. The most common co-morbidities associated with MDR TB are diabetes and chronic obstructive lung diseases. The most important predictors of patients’ outcome are sputum conversion, number of previous TB treatment and presence of co-morbidities.","Mohammad A. Tag El Din and Ashraf A. El Maraghy and Abdel Hay R. Abdel Hay",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"L’équipe mobile de soins palliatifs en service de neurologie : présentation de deux groupes interdisciplinaires et pluriprofessionnels fruits d’un projet commun de diffusion de la démarche palliative et d’aide à la réflexion éthique","Résumé
Une équipe mobile de soins palliatifs (EMSP) et un service de neurologie apprennent ensemble à faire face aux situations de fin de vie. Après l’historique de la rencontre entre les deux équipes, est décrit le travail clinique de l’EMSP auprès du patient, de sa famille et/ou auprès de l’équipe de neurologie. Au centre des prises en charge neurologiques palliatives complexes se trouvent le groupe ressource soins palliatifs et la réunion de concertation éthique. Ces deux outils interdisciplinaires et pluriprofessionnels sont décrits dans leur fonctionnement comme dans leurs objectifs. Ils créent des moments forts de la vie d’une équipe. Ils fondent sa cohésion, permettent une culture commune, une convergence des représentations individuelles lors des prises de décision et préviennent le burnout. Puis sont détaillées les difficultés d’une décision de pose de gastrostomie chez un homme atteint de paralysie supranucléaire progressive, la souffrance d’une équipe soignante qui prend en charge une femme jeune atteinte de Creutzfeldt-Jakob et pour finir les allers-retours entre vie et mort de Mme H. après un accident vasculaire cérébral. Les découpages théoriques, illustrés de vignettes cliniques, permettent la transmission de pratiques utilisables dans les équipes de neurologie.
This article describes how a mobile team of palliative care and a department of neurology learned to cope with many complex end-of-life situations. After a brief introduction to inter-team cooperation, clinical work of the mobile team with patients and families and its cooperation with the neurology team are presented. The specificity of supportive care in neurology is also analyzed. Two interdisciplinary and multi-professional tools – the Palliative Care Resource Group and the Ethics Consultation Group – are described, with their activities and their goals. The Palliative Care Resource Group is a specific entity whose identity lies at the crossroads between commonly recognized organizational units: clinic staff, clinical practice, ethical or organizational analysis groups (Balint, 1960), discussion groups (Rusznievski, 1999), training groups. It has several objectives: 1) create a robust conceptual environment enabling the pursuit of palliative care practices without relying on the empty paradigm of stereotypical actions; if suffering cannot be avoided, psychic development and transformation can be promoted; 2) attempt to prevent caregiver burnout; 3) help support and strengthen the collective dimension of the team, learning a mode of care which goes beyond the execution of coded actions; 4) enhance the primary dimension of care, i.e. taking care, especially in clinical situations where conventional wisdom declares that “nothing more can be done.”; 5) promote group work so new ideas arising from the different teams influence the behavior of all caregivers. The Ethics Consultation Group organizes its work in several steps. The first step is discernment, clearly identifying the question at hand with the clinical staff. This is followed by a consultation between the clinical team, the patient, the family and the referring physician to arrive at a motivated decision, respecting the competent patient's opinion. The final step is an evaluation of the decision and its consequences. The Ethical Consultation Group, which meets at a scheduled time at a set place, unites the different members of the neurology and palliative care teams who come to a common decision. These specific moments have an important impact on team cohesion, creating a common culture and a convergence of individual representations about making difficult decisions. Specific clinical cases are described to illustrate some of the difficulties encountered in palliative care decision-making. These cases provide insight about the decision to create a palliative care gastrostomy for a man with progressive supranuclear palsy, the suffering experienced by a medical team caring for a young woman with Creutzfeldt-Jacob encephalopathy, or a woman's experience with the post-stroke life-and-death seesaw. Theoretical divisions, illustrated with clinical stories, can be useful touchstones for neurology teams.","D. Baudoin and S. Krebs",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"L’état de stress post-traumatique comme conséquence de l’interaction entre une susceptibilité génétique individuelle, un évènement traumatogène et un contexte social","Résumé
Introduction
Un état de stress post-traumatique (ESPT) ne s’installe jamais par hasard : l’intrication de facteurs de risque intrinsèques (individuels) et extrinsèques (évènement traumatique) témoigne d’un support génétique interactif au trouble. Toute situation dramatique peut être le lieu d’un trauma, non nécessairement, mais en lien avec la manière dont l’individu a investi l’évènement. Parmi les sujets confrontés à la même situation stressante, seuls quelques uns souffriront d’un ESPT. Pour ces derniers, la thématique des répétitions est très différente d’un sujet à un autre, venant témoigner de la singularité de l’évènement vécu pour chacun d’entre eux. Comme témoignage d’une interaction entre l’homme et son environnement, le stress est une réaction biologique aspécifique de l’organisme, mais réaction déclenchée par un ressenti subjectif. L’ESPT en tant que diagnostic causalement attribué s’intègre parfaitement dans le modèle interactif gène×environnement.
Revue de la littérature
Les sujets présentant le génotype S/S codant pour le transporteur de la sérotonine déclenchent un ESPT pour un niveau d’exposition traumatique moindre que leurs homologues L/L. Mais l’interaction entre le génome et son environnement est plus complexe qu’une simple implication : une association de facteurs environnementaux intervient. Considérant la voie dopaminergique, l’allèle A1 codant pour le récepteur dopaminergique de type 2 est associé à une comorbidité sévère de l’ESPT avec présence de troubles somatiques, d’anxiété, d’altération sociale et de dépression. S’intéressant à la neuromodulation noradrénergique, une interaction entre le polymorphisme du gène GABRA2 et la survenue d’un ESPT est décrite tandis qu’une interaction entre le nombre d’évènements traumatiques et le polymorphisme Val(158)Met du gène codant pour la catécholamine-o-méthyltransférase a également été retrouvée. Au niveau neuroendocrinien, le gène codant pour la protéine FKBPR, co-chaperonne de la hsp90 qui lie le récepteur aux glucocorticoïdes, a été étudié selon quatre polymorphismes qui interviennent comme cofacteur en interaction avec l’origine ethnique et les expériences stressantes. Ce polymorphisme mono-nucléotidique interagit avec la sévérité de traumatismes infantiles pour prédire le niveau d’ESPT ultérieur retrouvé à l’âge adulte, ce dernier étant secondaire à un autre évènement de vie traumatisant.
Discussion
Aucune étude neurobiologique n’a pour l’instant décrit de marqueur biologique qui destinerait a priori et immanquablement un sujet à structurer un ESPT en réaction à une situation de stress. Différemment, l’étude psychopathologique retrouve a posteriori que tel sujet a nécessairement construit un syndrome de répétition traumatique en fonction de la concordance de données signifiantes relatives à son histoire. L’évènement vient frapper un refoulement ou une impasse biographique antérieure et dont la thématique interroge les fondamentaux de la culture humaine dans son émancipation d’avec la nature. Une proposition thérapeutique constitue alors par excellence un facteur environnemental lequel peut être ou protecteur ou délétère. La prise en charge aiguë par la technique du débriefing anglophone est discutée alors que la technique francophone est toujours en cours d’évaluation. Bien que ces données restent à confirmer, les benzodiazépines paraissent délétères en post-immédiat alors que l’usage de propranolol serait protecteur. À la phase chronique, la prise en charge pharmacologique n’est pas consensuelle même si les ISRS et les IRSNa sont régulièrement prescrits. S’éloignant de la pharmacopée pour rejoindre une perspective psychothérapique à l’orientation dynamique et interactive, ce qui a fait trauma n’est pas simplement l’évènement stressant en temps que tel, mais sa rencontre avec l’homme qui se trouvait là, prêt à accueillir ce trauma et qui ne s’en détache plus. La réparation du sujet psychotraumatisé ne peut simplement s’établir sur un statut passif de victime stressée, statut qui nuirait à la réflexion et à la reconstruction. Alors que la confrontation à la mort s’apparentait à l’insensé, le sujet interrogera les déterminants psychotraumatiques de son histoire biographique pour y réinscrire son évènement dramatique au sein d’une quête singulière de sens. Une telle restructuration se construit via l’intersubjectivité de la relation clinique, laquelle intervient au sein d’un contexte social. L’ESPT est une pathologie qui interagit avec le contexte sociétal : d’une part, le trauma s’établit via la remise en cause brutale de valeurs sociales qui semblaient immuables et, d’autre part, le concept clinique et nosographique d’ESPT est changeant avec l’évolution des sociétés.
Conclusion
Un ESPT ne survient jamais par hasard, les conditions de possibilité du trauma sont établies par des déterminants génétiques et psychologiques s’intégrant de façon interactive au cœur d’un contexte social. Après l’inflation d’un intérêt psychotraumatique dans les publications internationales depuis les années 1980, une lutte contre la survictimisation semble désormais s’installer. L’évolution des techniques de génétique et de neuro-imagerie est en cours de supplanter les études psychométriques en termes de fiabilité et de validité : peut-être faut-il voir dans cette évolution sociale les changements de demain concernant la clinique de l’ESPT et son traitement.
Summary
Introduction
Why are some individuals more likely than others to develop a posttraumatic stress disorder (PTSD) in the face of similar levels of trauma exposure? Monitoring the traumatic process combining the antecedents, the determinants of the psychic trauma and the acute symptoms can clarify the causes of the final onset of a chronic repetition syndrome. Epidemiologic research has clarified risk factors that increase the likelihood of PTSD after exposure to a potentially traumatic event. PTSD is an interaction between a subject, a traumatogenic factor and a social context. With each epidemiological, psychopathological and more particularly neurogenetic study, we will expand on the impact of these interactions on the therapeutic treatment of psycho-traumatised persons.
Literature findings
Most studies have shown that unrelated to the traumatic event, additional risk factors for developing PTSD include younger age at the time of the trauma, female gender, lower social economic statuts, lack of social support, premorbid personality characteristics and preexisting anxiety or depressive disorders increase the risk of PTSD. The psychic trauma is firmly attached to the repetition and the previous traumas are as many risks of developing a subsequent PTSD in the wake of a new trauma: PTSD in adults may represent a prolonged symptomatic reaction to prior traumatic assault, child abuse and childhood adversities. Related to the traumatic event, the organic pain, the traumatic brain injury, but also the sight of blood can lead to a trauma being considered as more serious or more harmful to life. It is useful to recognize the acute reactions of exhaustion stress as they can guide both the pharmacotherapeutic and the psychotherapeutic treatment thanks to debriefings. Even though the majority of people with acute stress disorder subsequently develop PTSD, the current data indicate that too many people can develop PTSD without initially displaying acute stress disorder. Though peritraumatic dissociation and peritraumatic distress have emerged as the strongest predictors for PTSD and have to be treated as soon as possible with the debriefing or the pharmacology; initial evidence suggests the potential benefits of early intervention, shortly after the trauma, and psychological debriefing has received increasing interest from the scientific community. However the Anglo-Saxon techniques (such as Critical Incident Stress Debriefing also known as the Mitchell model) are in total contrast with the French approach. In the first case the emotional response is controlled to ensure the pursuit of the group action, whilst in the second case the debriefing concerns patients with acute symptoms in order to prevent the development of a PTSD structuring of the latter. The facts, emotions and thoughts are not partitioned but inter-linked, thus enabling a fragmentation of the traumatic experience. In the face of the annihilation experienced, speech production by the subject is restored linking the person to the human community, once abandoned. However, debate continues on the efficacy of single session debriefing in the prevention of PTSD. At the time of the acute stress reactions, benzodiazepines are contraindicated at this stage as they promote dissociation and ulterior revivals. On the other hand, treatment with propranolol could be proposed: a two or three week course of propranolol begun in the aftermath of a traumatic event can reduce subsequent PTSD symptoms.
Discussion
A genetic polymorphism is evidently at work in the development of a PTSD via the regulation of the expression of genes of interest to the serotoninergic system and the adrenocorticotropic axis. The 5-HTTLPR (promoter region of SLC6A4 witch encodes the serotonin transporter) constitutes a genetic candidate region that may modulate emotional responses to traumatic events. The interaction between variation at the 5HTTLPR and stressful life events could predict depression and PTSD. Considering the dopaminergic pathway, the A1 allele coding the type 2 dopaminergic receptor is associated with a severe comorbidity of PTSD with the presence of somatic disorders, anxiety, social change and depression. For noradrenergic neuromodulation, an interaction between the polymorphism of gene GABRA2 and the occurrence of PTSD is described whereas an interaction between the number of traumatic events and Val(158)Met polymorphism of the gene coding for catecholamine-o-methyltransferase has also been found. The role of polymorphisms in FKBP5 (a co-chaperone of hsp 90 which binds to the glucocorticoid receptor) in predicting PTSD too, with a gene-by-environment point of view. These gene-by-environment studies are needed to focus more on distinct endophenotypes and influences from environmental factors. If several candidate genes are involved, a weighting of susceptibility to such and such a neurological regulation system will imply various endophenotypes. According to the monoamine predominantly incriminated, PTSD can take on a more hyper-vegetative clinical expression linked with noradrenergic overuse. Differently, avoidance behaviour and the depressive aspect invoke more a modification of the serotoninergic modulation whilst posttraumatic psychotic reactions question the role of dopaminergic pathways. Neuroscientific discoveries interesting the biological support of PTSD can thus modify our view of the conception of the disorder in relation to different therapeutic prospects.
Conclusion
Chronic PTSD can manifest itself in different clinical forms. The repetition syndrome can appear a long time after the traumatic event, following a paucisymptomatic latency period, which can last several years or even decades. The absence of complaints from the patient is common, the latter suffering in silence. Often other comorbid disorders and other complaints arise sooner than the clinical picture. Thus a depressive episode characterised as drug-seeking behaviour is frequently encountered. The therapeutic accompaniment traditionally combines a pharmacological and a psychotherapeutic treatment even if recommendations are rare. A posttraumatic stress disorder is never just a coincidence. The different stages of the evolution and the establishment of a PTSD are the expression of an interaction between the outside and the inner self. Despite a known progression of the posttraumatic stress disorder, this deleterious evolution is far from being a foregone conclusion. On the contrary, several levels of prevention are possible at each stage of its structuration to propose treatments to subjects who are vulnerable and/or present symptoms. No neurobiological study has yet found a biological marker, which would apparently and inevitably destine a subject to structure, a posttraumatic stress disorder in reaction to a stress. Conversely, the psychopathological study finds afterwards that a particular subject has necessarily built a traumatic repetition syndrome according to the concordance of significant data relative to his/her history. The event strikes a repression or an anterior biographical deadlock and of which the thematic questions the fundamentals of human culture in its emancipation with nature, like the question of death and its consequences: bereavement, parentality, transgenerational transmission and organicity often linked to the illness. A therapeutic proposal constitutes an environmental factor par excellence which can be either protective or deleterious. If the traumatic repetition syndrome has been known since Antiquity, the birth of PTSD has followed the chronology of the DSM according to the sociopolitical contexts encountered. A PTSD does not occur by chance: the conditions of possibility of the trauma are established by genetic and psychological determinants interactively integrated at the heart of a social context. After the increase in a psychotraumatic interest in international publications since the 1980s, a fight against over-victimisation seems to be setting in. The advances in genetic and neuroimaging techniques are in the process of superseding psychometric studies in terms of reliability and validity; maybe we should see in this social evolution the changes of tomorrow concerning the clinical of PTSD and its treatment. The healing of the psycho-traumatised subject cannot just be established on the passive status of victim, which would be detrimental to reflection and ultimately reconstruction: the rebirth of the subject will require active commitment, which could distract from the deadly repetition. Whilst the confrontation with death resembled nonsense, the subject will question the psychotraumatic determinants of his/her life history to reinstate this tragic event within a search for meaning. Such restructuring is built on the intersubjectivity of the clinical relationship, which occurs within a social context. PTSD is a pathology which interacts with the societal context: on the one hand the trauma is established on the brutal reconsideration of social values which seem immutable and on the other hand, the clinical and nosographical concept of PTSD is changing with the evolution of society.","Y. Auxéméry",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Die Gesundheitsuntersuchung: Vom Gesetz zur Richtlinie des Gemeinsamen Bundesausschusses (G-BA)","Zusammenfassung
Seit 1989 gibt es in Deutschland die Gesundheitsuntersuchung ab dem Alter von 35 Jahren zur Früherkennung der „Volkskrankheiten“ (insbesondere Herz-Kreislauf- und Nierenerkrankungen sowie Diabetes) mittels Anamnese, körperlicher Untersuchung, Blut- und Urintests sowie Beratung. Die entsprechende Richtlinie des Gemeinsamen Bundesausschusses (G-BA) wurde insgesamt sechsmal geändert, allerdings erfolgte nur einmal eine inhaltliche Änderung (1999 Streichung von Harnsäure, Kreatinin und Ruhe-EKG). Mehrmals wurden aber auch mögliche weitere Untersuchungen nicht in die Richtlinie aufgenommen, nachdem die Bewertung keinen Nutzen zeigen konnte (z.B. Glaukomscreening). Mitte der 1990er Jahre erfolgten mehrere Evaluationen, die zeigten, dass durchaus bisher nicht bekannte Diagnosen gestellt und Maßnahmen wie Ernährungsberatung eingeleitet wurden. Einen Nutzen im Sinne von vermiedenen unerwünschten Ereignissen (wie bspw. Herzinfarkte) konnten die Evaluationen aus methodischen Gründen aber nicht belegen. Kritik an der Gesundheitsuntersuchung ist nicht neu, insbesondere der fehlende Nutzenbeleg der in der Richtlinie geregelten Maßnahmen wird angemahnt. Ein Gesetzentwurf der letzten Bundesregierung mit einem Änderungsvorschlag für die Gesundheitsuntersuchung wurde allerdings im Bundesrat abgelehnt.
Summary
Since 1989 a periodic health examination beginning at the age of 35 for the early detection of “common diseases” (especially cardiovascular and kidney diseases as well as diabetes) by means of history-taking, physical examination, blood and urine tests and counselling has been available in Germany. Altogether, the respective directive of the Federal Joint Committee (G-BA) was revised six times, but a substantive change took place only once (i. e., the cancellation of uric acid, creatinine, and resting ECG in 1999). However, additional examinations (e.g., glaucoma screening) were not added to the health check after systematic assessments of the evidence were completed. In the mid-1990s, several evaluations were performed which showed that new diagnoses were established in a significant proportion of patients, and measures were initiated such as nutrition counselling. A patient-relevant benefit in terms of avoided adverse events (such as heart attacks) could, however, not be demonstrated due to methodological reasons. Criticism of the health examination is not new, in particular concerning the lack of evidence of benefit for the diagnostic procedures of the health examination. A draft law issued by the former Federal Government proposing an amendment to the health examination has recently been rejected in the Bundesrat (upper house of the German parliament).","Matthias Perleth and Katja Matthias",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Compatibility of current DSM-IV and proposed DSM-5 diagnostic criteria for cocaine use disorders","Objectives
The present study examined the compatibility of the current DSM-IV and proposed DSM-5 diagnostic criteria for cocaine use disorders (CUD) among state prison inmates, and evaluated the diagnostic utility of the proposed criteria in accounting for DSM-IV “diagnostic orphans” (i.e., individuals who meet one or two of the diagnostic criteria for substance dependence yet fail to report indications of substance abuse).
Method
Data were derived from routine clinical assessments of adult male inmates (N=6871) recently admitted to the Minnesota Department of Corrections state prison system from 2000 to 2003. An automated (i.e., computer-prompted) version of the Substance Use Disorder Diagnostic Schedule-IV (SUDDS-IV; Hoffmann & Harrison, 1995) was administered to all inmates as part of routine assessments. DSM-IV and DSM-5 criteria were coded using proposed guidelines.
Results
The past 12-month prevalence of DSM-IV CUDs was 12.7% (Abuse, 3.8%, Dependence, 8.9%), while 11.0% met past 12-month DSM-5 criteria for a CUD (Moderate [MCUD], 1.7%; Severe [SCUD], 9.3%). When DSM-5 criteria were applied, 11.8% of the DSM-IV diagnostic orphans received a MCUD diagnosis. The vast majority of those with no diagnosis (99.6%) continued to have no diagnosis, and a similar proportion who met dependence criteria (98.4%) met SCUD criteria of the proposed DSM-5. Most of the variation in diagnostic classifications was accounted for by those with a current abuse diagnosis.
Conclusions
The proposed DSM-5 criteria perform similarly to DSM-IV criteria in terms of the observed past 12-month CUD prevalence and diagnostic classifications. The proposed criteria appear to account for diagnostic orphans that may warrant a diagnosis. DSM-IV abuse cases were most affected when DSM-5 criteria were applied. Additional criteria, beyond those included in the proposed DSM-5 changes, concerning use to relieve emotional stress and preoccupation with use were frequently endorsed by those with a proposed DSM-5 diagnosis.","Steven L. Proctor and Albert M. Kopak and Norman G. Hoffmann",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Uso y aplicación de herramientas 2.0 en los servicios, producción, organización y difusión de la información en la biblioteca universitaria","Resumen
El presente estudio tiene como objetivo presentar un análisis sobre el uso de herramientas 2.0 en bibliotecas universitarias, con el fin de entender los patrones de uso como una forma de proporcionar información a los usuarios y mejorar la visibilidad de la entidad a través de su marca digital. El análisis, basado en la experiencia profesional, muestra cómo la integración de diferentes herramientas 2.0 permite potenciar el servicio de información que ofrece la biblioteca universitaria al tiempo que favorece la creación de comunidades de aprendizaje y fomenta los mecanismos de comunicación entre los usuarios. La integración de los distintos tipos de herramientas de la web social permite articular un sistema de información, donde gracias a sistemas como los canales RSS (Really Simple Syndication) se facilita la recopilación de información, la cual es organizada y normalizada con los gestores de referencias sociales y difundida entre los usuarios a través de los blogs, redes sociales y listas de distribución, lo que permite darle una mayor visibilidad a la institución, pero sobre todo ofrecer mejores servicios a los usuarios. El valor del artículo radica en que la experiencia de un centro concreto permite su réplica y/o adaptación a otros centros en los que se podrán implantar servicios similares, puesto que los servicios proporcionados se basan en herramientas de la web 2.0, freeware o de código abierto.
The object of this study is to present an analysis on the use of 2.0 tools in university libraries, with the goal of grasping usage patterns of the information provided to users, while enhancing the visibility of the entity’s digital brand. Based on professional experience, the analysis shows how the integration of diverse 2.0 tools can improve the information services offered by the university library, while improving communication mechanisms among users. This encourages the creation of learning communities. The integration of diverse social networking tools allows for the articulation of an information system, in which, thanks to RSS feeds channels, information is organized and standardized by social reference managers and subsequently disseminated among users through blogs, social networks, distribution lists; thereby enhancing the institution’s visibility and, above all, providing better services to users. This article provides a valuable portrait of an information center, using 2.0 web tools, freeware and open code application; whose experience can be replicated and/or adapted in other centers, where similar services are offered","Julio Alonso Arévalo and José Antonio Cordón García and Raquel Gómez Díaz and Belén García-Delgado Giménez",2014,"[""Science Direct""]","Rejeitado: CR10","Rejeitado: CR9"
"An overview of medical informatics education in China","Objective
To outline the history of medical informatics education in the People's Republic of China, systematically analyze the current status of medical informatics education at different academic levels (bachelor's, master's, and doctoral), and suggest reasonable strategies for the further development of the field in China.
Method
The development of medical informatics education was divided into three stages, defined by changes in the specialty's name. Systematic searches of websites for material related to the specialty of medical informatics were then conducted. For undergraduate education, the websites surveyed included the website of the Ministry of Education of the People's Republic of China (MOE) and those of universities or colleges identified using the baidu.com search engine. For postgraduate education, the websites included China's Graduate Admissions Information Network (CGAIN) and the websites of the universities or their schools or faculties. Specialties were selected on the basis of three criteria: (1) for undergraduate education, the name of specialty or program was medical informatics or medical information or information management and information system; for postgraduate education, medical informatics or medical information; (2) the specialty was approved and listed by the MOE; (3) the specialty was set up by a medical college or medical university, or a school of medicine of a comprehensive university. The information abstracted from the websites included the year of program approval and listing, the university/college, discipline catalog, discipline, specialty, specialty code, objectives, and main courses.
Results and conclusions
A total of 55 program offerings for undergraduate education, 27 for master's-level education, and 5 for PhD-level education in medical informatics were identified and assessed in China. The results indicate that medical informatics education, a specialty rooted in medical library and information science education in China, has grown significantly in that country over the past 10 years. Frequent changes in the specialty's name and an unclear identity have hampered the visibility of this educational specialty and impeded its development. There is a noticeable imbalance in the distribution of degree programs in medical informatics in different disciplines, with the majority falling under information management. There is also an uneven distribution of the specialty settings of medical informatics at the various academic levels (bachelor's, master's, and doctoral). In addition, the objectives and curriculum design of medical informatics education differ from one university to another and also from those of foreign universities or colleges. It is recommended that China (1) treat medical informatics as a priority “must-have” discipline to build in China, (2) establish its own independent, balanced degree programs, (3) set up a specialty of “medical informatics” under the “medicine” category, (4) explore curriculum integration with international medical informatics education, and (5) establish and improve medical informatics education system.","Dehua Hu and Zhenling Sun and Houqing Li",2013,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Clinical and Molecular Characteristics of Childhood-Onset Stargardt Disease","Purpose
To describe the clinical and molecular characteristics of patients with childhood-onset Stargardt disease (STGD).
Design
Retrospective case series.
Participants
Forty-two patients who were diagnosed with STGD in childhood at a single institution between January 2001 and January 2012.
Methods
A detailed history and a comprehensive ophthalmic examination were undertaken, including color fundus photography, autofluorescence imaging, spectral-domain optical coherence tomography (SD-OCT), and pattern and full-field electroretinograms. The entire coding region and splice sites of ABCA4 were screened using a next-generation, sequencing-based strategy. The molecular genetic findings of childhood-onset STGD patients were compared with those of adult-onset patients.
Main Outcome Measures
Clinical, imaging, electrophysiologic, and molecular genetic findings.
Results
The median ages of onset and the median age at baseline examination were 8.5 (range, 3–16) and 12.0 years (range, 7-16), respectively. The median baseline logarithm of the minimum angle of resolution visual acuity was 0.74. At baseline, 26 of 39 patients (67%) with available photographs had macular atrophy with macular/peripheral flecks; 11 (28%) had macular atrophy without flecks; 1 (2.5%) had numerous flecks without macular atrophy; and 1 (2.5%) had a normal fundus appearance. Flecks were not identified at baseline in 12 patients (31%). SD-OCT detected foveal outer retinal disruption in all 21 patients with available images. Electrophysiologic assessment demonstrated retinal dysfunction confined to the macula in 9 patients (36%), macular and generalized cone dysfunction in 1 subject (4%), and macular and generalized cone and rod dysfunction in 15 individuals (60%). At least 1 disease-causing ABCA4 variant was identified in 38 patients (90%), including 13 novel variants; ≥2 variants were identified in 34 patients (81%). Patients with childhood-onset STGD more frequently harbored 2 deleterious variants (18% vs 5%) compared with patients with adult-onset STGD.
Conclusions
Childhood-onset STGD is associated with severe visual loss, early morphologic changes, and often generalized retinal dysfunction, despite often having less severe fundus abnormalities on examination. One third of children do not have flecks at presentation. The relatively high proportion of deleterious ABCA4 variants supports the hypothesis that earlier onset disease is often owing to more severe variants in ABCA4 than those found in adult-onset disease.","Kaoru Fujinami and Jana Zernant and Ravinder K. Chana and Genevieve A. Wright and Kazushige Tsunoda and Yoko Ozawa and Kazuo Tsubota and Anthony G. Robson and Graham E. Holder and Rando Allikmets and Michel Michaelides and Anthony T. Moore",2015,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Modeling and experimental validation of mass transfer from carbonated beverages in polyethylene terephthalate bottles","Mass transfer and related shelf life assessment is an important issue in the beverage industry. Product change due to mass transfer is at stake and, with it, its consumer value and consideration. Carbonation loss takes place at the product/package interface, and to the environment through the package itself. In this paper a joint experimental/computational approach has been exploited: the CO2 loss through the polyethylene terephthalate barrier has been computed by means of a multidimensional finite element code, while actual measurements have been carried out to validate the computations. Residual carbonation histories are validated and presented for a variety of thermal regimes and for two different bottles carrying the same capacity. The paper highlights on the combination of bottle weight, initial carbonation and storage temperature, indicating the operational set for the longest shelf life within the explored cases. Lighter bottles can be used with no inference on shelf life, while the carbonic loss depends non-linearly on initial carbonation and temperature increment. The associated concentration maps help infer on the importance of polyethylene terephthalate thickness uniformity. It is then demonstrated that the model carries the flexibility of a general tool, applicable to any carbonated beverage at any storage condition.","Gabriella Carrieri and Maria Valeria De Bonis and Gianpaolo Ruocco",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"IBAR: Interacting boson model calculations for large system sizes","Scaling the system size of the interacting boson model-1 (IBM-1) into the realm of hundreds of bosons has many interesting applications in the field of nuclear structure, most notably quantum phase transitions in nuclei. We introduce IBAR, a new software package for calculating the eigenvalues and eigenvectors of the IBM-1 Hamiltonian, for large numbers of bosons. Energies and wavefunctions of the nuclear states, as well as transition strengths between them, are calculated using these values. Numerical errors in the recursive calculation of reduced matrix elements of the d-boson creation operator are reduced by using an arbitrary precision mathematical library. This software has been tested for up to 1000 bosons using comparisons to analytic expressions. Comparisons have also been made to the code PHINT for smaller system sizes.
Program summary
Program title: IBAR Catalogue identifier: AELI_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AELI_v1_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: GNU General Public License version 3 No. of lines in distributed program, including test data, etc.: 28 734 No. of bytes in distributed program, including test data, etc.: 4 104 467 Distribution format: tar.gz Programming language: C++ Computer: Any computer system with a C++ compiler Operating system: Tested under Linux RAM: 150 MB for 1000 boson calculations with angular momenta of up to L=4 Classification: 17.18, 17.20 External routines: ARPACK (http://www.caam.rice.edu/software/ARPACK/) Nature of problem: Construction and diagonalization of large Hamiltonian matrices, using reduced matrix elements of the d-boson creation operator. Solution method: Reduced matrix elements of the d-boson creation operator have been stored in data files at machine precision, after being recursively calculated with higher than machine precision. The Hamiltonian matrix is calculated and diagonalized, and the requested transition strengths are calculated using the eigenvectors. Restrictions: The 1000 boson coefficients for L=0 and L=20 have been included in the IBAR distribution and the 7.3 GB of data that make up the remaining coefficients for L=21 to L=2000 are available upon request. Running time: If the provided example is changed to include 100 bosons, the calculation requires about 1 second of run time. For 1000 bosons, the calculation requires about 9 minutes of run time.","R.J. Casperson",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Effect of the New Standards for Case Logging on Resident Operative Volume: Doing Better Cases or Better Numbers?","Objectives
The Accreditation Council for Graduate Medical Education (ACGME) modified the designation of major (index) operative cases to include those previously considered “minor.” This study assessed the potential effect of these changes on resident operative experience.
Methods
With Institutional Review Board approval, we analyzed National Surgical Quality Improvement Program participant use files for 2005–2008 for general and vascular surgery cases. Primary CPT case coding was mapped to the ACGME major case category using both the old and new classification schemes. The variables were analyzed using χ2 analysis in SPSS IBM 19 (IBM, Armonk, New York).
Results
A total of 576,019 cases were reviewed. Major cases as defined by the new classification represented an increasing proportion of the cases each year, rising from 88.3% in 2005 to 95% by 2008 (p < 0.001). Major cases as defined by the old scheme decreased from 71% in 2005 to 62% by 2008 (p < 0.001). The cases covered by a resident dropped from 82% in 2005 to 61% in 2008 (p < 0.001). When comparing the new to the old scheme, 364,366 (63.3%) cases were considered major and 30,587 (5.3%) were minor by both standards; 7089 (1.2%) cases previously classified as major were changed to minor, whereas 173,977 (30.2%) (p < 0.001) previously classified as minor were now major. This latter group showed top procedures to include excision of breast lesion (22,175 [12.7%]), laparoscopic gastric bypass (18,825 [10.8%]), ventral hernia repair (14,732 [8.5%]), and appendectomy (10,190 [5.9%]). Of these newly designated major cases, the proportion not covered by residents increased from 22% in 2005 to 44% in 2007 and 2008 (p < 0.001).
Conclusions
Although some operative cases newly classified as major are technically advanced procedures (eg, Roux-en-Y gastric bypass), other cases are not (eg, breast lesion excision), which raises the issue as to whether the major case category has been diluted by less demanding case types. The implications of these findings may suggest preservation of case volumes at the expense of case quality.","Raghav Murthy and Alex Shepard and Andrew Swartz and Ann Woodward and Craig Reickert and Mathilda Horst and Ilan Rubinfeld",2012,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"La théorie du néo-organodynamisme : une nouvelle classification pratique des maladies mentales","Résumé
La classification traditionnelle des maladies mentales et celle d’Henri Ey y sont discutées et un néo-organodynamisme est présenté. La phase aiguë d’une maladie mentale n’est sans doute qu’une modification temporaire de la maladie de base. La nature de cette dernière peut se décrire par l’importance de la dégénérescence organique. Alors qu’un état psychotique constitue le trouble de base, une phase agrégative aiguë se manifesterait en proportion du degré d’excitations anormales du réseau neuronal. En s’appuyant sur une pluralité d’états mentaux, il est possible de représenter avec précision l’état des patients psychiatriques, qui varie de façon néo-organodynamique avec l’évolution. De la même façon, grâce à la classification proposée, il devient possible de procéder à une recherche clinique comparative des effets des médicaments et des modifications biochimiques. Le concept de « voie accessoire » est introduit. Les contraintes et tensions mentales, ainsi que les actes rituels, promeuvent une construction et une réorganisation du réseau neuronal, qui se fige sous l’effet des répétitions. L’encodage de ce nouveau réseau neuronal dans le subconscient crée une dérivation qui finit par se fixer en tant que « voie accessoire », à l’instar du faisceau de Kent dans le syndrome de Wolff-Parkinson-White. Lorsque la « voie sous-conductrice » est dominante, le patient ne peut plus prendre en considération quoi que ce soit, y compris lui-même. In fine, l’histoire du psychisme est celle d’une hiérarchisation mentale, construite par des voies accessoires et l’encodage dans le subconscient des états mentaux associés.
Background
Dogmatically conflicting psychological and biological theories make the foundations of psychiatry fragile.
Methods
Traditional and Henri Ey's classifications of mental illness are discussed and the neo-organodynamism is proposed. The principle of the accessory pathway is also proposed.
Results
The acute phase of disease may only be a temporary modification of the basic mental illness, the nature of the latter can be expressed in terms of the depth of the organic degeneration. The psychotic state consists of the basic disorder and its acute aggregative phase that is in proportion to the degree of abnormal excitations of neural network. In my classification, mental illness is classified from a normal stage N to X. The degree of acute aggravation is staged from 0 to 7. The twelve stages of organic degeneration multiplied by the eight stages of acute aggravation results in a product of 96 mental stages. By applying this, we can accurately represent the conditions of psychiatric patients that change neo-organodynamically over time. At the same time, clinical comparative research, such as the effects of medicine and of biochemical changes at one time becomes possible. The mental automatism, the mechanism of acute psychosis and treatment concept are discussed with this theory. And this theory may explain what the mental illness is, what the delusion is and what the endogenous psychosis is. Mental pressures or ritual acts promote construction and the reorganization of neural network, and it is fixed by repeating itself. Coding to the subconsciousness of a new neural network namely bypass will be made, and it is immobilized as an accessory pathway like the bundle of Kent of the Wolff–Parkinson–White syndrome. This is the principle of the accessory pathway. When the accessory pathway is dominant, the patient cannot consider anything like himself. This condition shows the low level of his psychic energy (psychasthénie). The history of a mind hierarchy that is constructed with the accessory pathways may be a history of mind. And the history of a mind hierarchy should be what the psychic body is.
Treatment concept
From the viewpoint of the neo-organodynamism, the conception of organic degeneration is consisted of 1) tissue degeneration literally and 2) generation of accessory pathway by the reorganization of the synapse. The symptoms may be irreversible in the case of the tissue degenerations. But the symptoms that are induced by the accessory pathway may be reversible. If the conductivity of the main pathway becomes dominant as before, the symptom by the accessory pathway has a possibility of the improvement. But the accessory pathway will be remained even if the conductivity of the main pathway becomes dominant as before. One's usual personality is defined as a predominant state of the main pathway. Healing is to return to one's usual personality from the accessory pathway predominant state. In the organic degenerations that are the essence of mental illness, rehabilitation (psychotherapy, cognitive behavioral therapy, mental counseling, etc.) is essential to recuperate lost functions. For the positive symptoms, which are the secondary effects of organic degenerations, medication that focuses on the synaptic neurotransmitters will be effective. Theoretically, the anti-epileptics may be might be designated as the fundamental treatment for the acute aggravation phase because of the abnormal firings of the neuronal network.
Conclusions
Neo-organodynamism is proposed as a minimum clinical classification of mental illness for psychiatrists. By using this, we can understand that the psychosis is not the special disease, but everyone has the possibility of onset. Neo-organodynamism will eliminate the prejudice against mental illness.","Tatsuo Mizuno",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A finite difference construction of the spheroidal wave functions","A fast and simple finite difference algorithm for computing the spheroidal wave functions is described. The resulting eigenvalues and eigenfunctions for real and complex spheroidal bandwidth parameter, c, agree with those in the literature from four to more than eleven significant figures. The validity of this algorithm in the extreme parameter regime, up to c2=1014, is demonstrated. Furthermore, the algorithm generates the spheroidal functions for complex order m. The coefficients of the differential equation can be simply modified so that the algorithm may solve any second-order differential equation in Sturm–Liouville form. The prolate spheroidal functions and the spectral concentration problem in relation to band-limited and time-limited signals is discussed. We review the properties of these eigenfunctions in the context of Sturm–Liouville theory and the implications for a finite difference algorithm. A number of new suggestions for data fitting using prolate spheroidal wave functions with a heuristic for optimally choosing the value of c and the number of basis functions are described.
Program summary
Program title: SWF_8thOrder Catalogue identifier: AEQE_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEQE_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 1081 No. of bytes in distributed program, including test data, etc.: 160,312 Distribution format: tar.gz Programming language: Matlab R2009b. Computer: Designed to run on any computer capable of running Matlab 2009b with at least 2 GB of RAM in order to handle moderate grid sizes. With an appropriate change of syntax, the program may be easily adapted to Maple, Mathematica, Fortran, IDL and any other software capable of performing the diagonalization of large matrices. Operating system: Any operating system which will run Matlab, Mathematica, Fortran or any other language capable of performing large matrix diagonalizations. Has the code been vectorized or parallelized?: Tested with dual core and quad core systems. The algorithm will also work with single core systems. RAM: 1372 MB total used by Matlab for a grid with 4001 points, 41 MB used to store eigenfunctions, grid and spectrum arrays (4001 points). More RAM is used for larger grids. For example, with 8000 grid points, 162 MB of RAM is required to store the eigenfunction and eigenvalue arrays. Classification: 4.3. External routines: The program uses Matlab’s internal ‘eig’ routine. Nature of problem: The problem is to construct the angular eigenfunctions of the Laplacian in three dimensional, spheroidal coordinates. These are the prolate, oblate and generalized spheroidal wave functions and to compute the corresponding eigenvalues. Equivalently, the task can be seen as generating the angular functions which arise when solving the Helmholtz wave equation by separation of variables in three dimensional, spheroidal coordinates: [∂η(1−η2)∂η+λlm(c)−c2η2−m21−η2]Slm=0. This task often arises in the solution of problems with axial symmetry although setting c=0 restores spherical symmetry. More generally, the coefficient function handles, CD1E and CD2E, can be redefined by the user to match the coefficients of the 2nd and 1st derivatives of any second order Sturm–Liouville type equation, for example, the harmonics of a tri-axial ellipsoid. Hence this algorithm and manuscript provide a foundation for solving a range of problems that have application beyond the spheroidal problems considered here. Solution method: The method of solution is the ‘finite difference method’. The spatial grid is discretized into N points, N−2 of which comprise the ‘interior grid’ and 2 points are the boundary points. The spheroidal differential operator is discretized which arises from separation of variables of the Laplacian in three dimensional, spheroidal coordinates on the interior grid using 8th order finite difference formulas. The boundary conditions for the spheroidal wave functions are implemented implicitly via finite difference operators which relate the boundary points back to the interior points. This is done using sliding off-centered differences at 8th order. The discretization of the Laplace operator and implicit implementation of the boundary conditions leads to a discretized eigenvalue problem. The eigenvectors give the discretized eigenfunctions of the spheroidal differential operator and the eigenvalues give the spectrum of the differential operator. Points at the boundary are reconstructed using forward and backward difference operators. The eigenfunctions are numerically normalized using a 6th order “Boole’s Rule” integration procedure. Restrictions: The current version of this algorithm implements the option of both Dirichlet and Neumann boundary conditions, which is chosen by the user by the “BC” switch. Mixed boundary conditions can also be implemented by the user, by modifying the boundary condition ‘IF’ statements. When solving with a very large concentration parameter, |c|, one must use a large number of grid points which would result in longer computational times and require more RAM. Unusual features: This program solves for the angular eigenfunctions of the spheroidal wave equation in three dimensions. Due to the finite difference approach, one is able to solve over non-standard domains such as spheroidal caps or spheroidal belts. The program is capable of solving for the generalized spheroidal wavefunctions with complex geometric parameter c. Given a sufficiently large number of grid points, the program can generate spheroidal eigenfunctions and eigenvalues for extreme parameter regimes, for example, for |c|∼108 with a grid of 20,000 points. Furthermore, the program can generate spheroidal wavefunctions for non-integer and complex order parameter, m, which may correspond to some analytic continuation of the spheroidal wave functions. In particular, for real, non-integer values of m, this corresponds to an axially symmetric ellipsoid with a defect angle where the 2π periodicity symmetry about the rotation axis becomes 2πα periodicity, where α is some defect factor. Additional comments: Main User-Input Parameters: The main input parameters are located at the top of the code. The parameter C2 sets the value of the square of the spheroidal concentration parameter, c2. The parameter m is the order parameter which is typically an integer such as for the Legendre functions, but can also be non-integer and complex valued. The switch BC, sets the boundary conditions for the differential equation. A value BC=0 gives Dirichlet boundary conditions and BC=1 gives Neumann boundary conditions. The values of MinTheta and MaxTheta set the minimum and maximum values of the angular coordinate, θ, which specify the domain of the differential equation. The parameter Npts sets the number of grid points, with larger grids resulting in more accurate eigenfunctions and spectra. General comments: To obtain spheroidal harmonics the eigenfunctions need to be multiplied by a complex exponential factor eimφ or sinusoidal functions for real solutions. The eigenfunctions generated by this program may be normalized numerically. Since normalization conventions differ by application, normalization is left for the user to implement. For this purpose, we have encoded a plain, 6th order Boole rule unit normalization which is easy to modify. Finally, we note that if the user modifies the function handles, CD1E and CD2E then the program will solve any 2nd order ordinary differential equation with Dirichlet or Neumann boundary conditions. Non-homogeneous terms can be added by modifying the entries in the finite difference matrix where c2 and m2 appear. Running time: On a laptop with an Intel Core i3-2350M CPU (2.30 GHz), with 4.00 GB of RAM, the program takes 149 s for a grid with 4000 points. Running time increases for larger grid sizes. For example, increasing the grid size to 8000 points increased the run time to 1138 s on the same machine.","Daniel X. Ogburn and Colin L. Waters and Murray D. Sciffer and Jeff A. Hogan and Paul C. Abbott",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Dificultades de las enfermeras de atención primaria en los procesos de planificación anticipada de las decisiones: un estudio cualitativo","Resumen
Objetivo
Conocer las dificultades que encuentran las enfermeras de atención primaria para promover procesos de planificación anticipada de las decisiones con personas en el final de la vida.
Diseño
Estudio cualitativo fenomenológico.
Emplazamiento
Área de Gestión Sanitaria Norte de Jaén.
Participantes
Enfermeras de atención primaria.
Método
Muestreo intencional. Realización de 14 entrevistas en profundidad hasta la saturación de los discursos. Análisis de contenido en 4 etapas: transcripción de datos, codificación, obtención de resultados y verificación de conclusiones. Uso de N-Vivo como apoyo al análisis. Triangulación de resultados entre investigadores.
Resultados
Dificultades referidas a los profesionales: falta de conocimiento sobre el tema, falta de habilidades de comunicación o de experiencia y presencia de emociones negativas. En la institución sanitaria, la falta de tiempo y las interferencias con otros profesionales suponen una barrera. También la actitud del propio paciente o su familia es vista como una traba ya que pocos hablan sobre el final de la vida. Finalmente, nuestra sociedad evita las conversaciones abiertas sobre temas relacionados con la muerte.
Conclusiones
Es necesario el aprendizaje de los profesionales sobre planificación anticipada de decisiones, su entrenamiento en habilidades comunicativas y su educación afectiva. Los gestores sanitarios han de tener en cuenta el hecho de que las intervenciones para planificar anticipadamente decisiones sanitarias precisan formación, tiempo y atención continuada. En tanto no acontezca un cambio cultural, persistirá un modelo evasivo para afrontar el final de la vida.
Objective
To know the primary care nurses’ difficulties to promote advance care planning process with patients in the end of life.
Design
Phenomenological qualitative methodology.
Location
Health Management Area North of Jaén.
Participants
Primary care nurses.
Method
Purposive sampling. Fourteen in-depth interviews were conducted until the speeches saturation. Content analysis in four steps: transcription, coding, obtaining results and conclusions verification. Supported whit the software Nvivo 8. Triangulation of results between researchers.
Results
Professionals’ difficulties: Lack of knowledge about the topic, lack of communication skills, lack of experience and presence of negative emotions. In the health institution lack of time and interference with other professionals is a barrier. Also the patient's attitude and the family are identified as an obstacle because few people speak about the end of life. Finally, our society prevents open discussion about issues related to death.
Conclusions
Professional learning about advanced care planning, training in communication skills and emotional education are necessary. Health managers should consider the fact that early interventions for planning health decisions require training, time and continued attention. If a cultural change does not happen, an evasive way to face the end of life will persist.","Nani Granero-Moya and Antonio Frías-Osuna and Inés M. Barrio-Cantalejo and Antonio Jesús Ramos-Morcillo",2016,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"Biotherapies for Parkinson disease","The clinical use of biotherapies in Parkinson disease already has 30years history. The transplantation of dopamine fetal cells in the striatum of advanced patients has proved to be relevant in some patients but randomized efficacy trials in the US have provided disappointing results. However, cell therapies might come back on stage with the use of stem cells in the future. Gene therapy is a more recent strategy relying on viral vectors able to transduce genes coding either for the enzymes that can increase neurotransmitters production or genes for trophic factors. Several approaches have been developed in PD and have been experimented in patients. Although, some of the studies have evidenced insufficient clinical benefit, other programs, such as those using dopamine replacement techniques are promising. We find fresh hope in this field that might be the future of PD treatment. It remains however that advanced PD might not be the ideal condition to properly benefit from biotherapies and there is a need of studies at earlier stages of the disease, a time where major change in the disease course might be expected.
Résumé
Les premiers essais de biothérapie dans la maladie de Parkinson datent maintenant d’il y a 30ans. Les greffes de cellules fœtales dans le striatum de patients à un stade avancé de la maladie ont montré qu’elles pouvaient améliorer certains patients. Néanmoins, les essais contrôlés réalisés aux États-Unis ont été décevants. La thérapie cellulaire pourrait toutefois revenir au premier plan avec l’émergence prochaine des cellules souches. La thérapie génique est plus récente et repose sur l’utilisation de vecteurs viraux capables de transmettre des gènes codant soit pour des enzymes permettant la fabrication de neurotransmetteurs, soit pour des facteurs trophiques. Plusieurs approches différentes ont déjà été expérimentées chez les parkinsoniens. Si certaines de ces stratégies ont apporté des bénéfices insuffisants, il semble que les techniques visant à faire produire de la dopamine soient prometteuses. Elles pourraient révolutionner le champ thérapeutique de la maladie. Il n’en demeure pas moins que les formes avancées de maladie de Parkinson ne sont peut-être pas le stade idéal pour bénéficier de ces biothérapies. Nous avons besoin d’essais à des stades plus précoces qui pourraient s’avérer en particulier bénéfiques sur l’évolution de la maladie.","P. Remy",2014,"[""Science Direct""]","Rejeitado: CR9","Rejeitado: CR9"
"A dedicated source-position transformation package: PySPT","Modern time-delay cosmography aims to infer the cosmological parameters with a competitive precision from observing a multiply imaged quasar. The success of this technique relies upon a robust modeling of the lens mass distribution. Unfortunately strong degeneracies between density profiles that lead to almost the same lensing observables may bias precise estimates of the Hubble constant. The source position transformation (SPT), which covers the well-known mass-sheet transformation (MST) as a special case, defines a new framework to investigate these degeneracies. In this paper, we present pySPT, a python package dedicated to the SPT. We describe how it can be used to evaluate the impact of the SPT on lensing observables. We review most of its capabilities and elaborate on key features that we used in a companion paper regarding SPT and time delays. The pySPT program also comes with a subpackage dedicated to simple lens modeling. This can be used to generate lensing related quantities for a wide variety of lens models independent of any SPT analysis. As a first practical application, we present a correction to the first estimate of the impact on time delays of the SPT, which has been experimentally found in a previous work between a softened power law and composite (baryons + dark matter) lenses. We find that the large deviations previously predicted have been overestimated because of a minor bug in the public lens modeling code lensmodel (v1.99), which is now fixed. We conclude that the predictions for the Hubble constant deviate by &sim;7%, first and foremost as a consequence of an MST. The latest version of pySPT is available on Github, a software development platform, along with some tutorials to describe in detail how making the best use of pySPT.<br/> &copy; 2018 ESO.","Wertz, Olivier and Orthen, Bastian",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"12th International Conference on Intelligent Information Hiding and Multimedia Signal Processing, IIH-MSP 2016","The proceedings contain 44 papers. The special focus in this conference is on Image, Video Signal Processing, Multisignal Processing Techniques, Hardware Design, Assisting Systems, Evolutionary Computing and Its Applications. The topics include: The election of spectrum bands in hyper-spectral image classification; evaluating a virtual collaborative environment for interactive distance teaching and learning; the linear transformation image enhancement algorithm based on HSV color space; silhouette imaging for smart fence applications with ZigBee sensors; gender recognition using local block difference pattern; DBN-based classification of spatial-spectral hyperspectral data; multiple kernel-learning based hyperspectral data classification; forensics of operation history including image blurring and noise addition based on joint features; an improvement image subjective quality evaluation model based on just noticeable difference; more efficient algorithm to mine high average-utility patterns; infrared video based sleep comfort analysis using part-based features; fall detection algorithm based on human posture recognition; model-based vehicle make and model recognition from roads; frames motion detection of quantum video; blind quantum computation with two decoy states; a bayesian based finite-size effect analysis of QKD; a novel approach to the quadratic residue code; the reduction of VQ index table size by matching side pixels; a protective device for a motor vehicle battery; a design of genetic programming scheme with VLIW concepts; management of energy saving for wireless sensor network node and optimum design and simulation of new type single phase PMSG.",,2017,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR9"
"Improving TelEduc environment with GPL software: The visualization content case for computers and mobile devices","e-Learning environments are proposed to support teaching and learning activities using the Web infrastructure. One example is TelEduc, a GPL e-Learning environment which its developers are mostly students involved in academic projects, making it difficult to have a long-period team with expertise to maintain the code and evolve the software with new requirements and to new and desirable contexts, like mobile access. One solution is use third-party software to improve the software and minimize the code maintain efforts, solution applied in the TelEduc environment to visualize attached documents on computers and smart phones without installed appropriated software, an open problem in the environment before this work. The main result is a TelEduc version with visualization file for desktops and smart phones. Another result is identification of issues dealt when integrates with third party software like impact on usability, accessibility and performance, license compatibility and easily to integrate, update and change.<br/>","Da Silva, Andre Constantino and Da Rocha, Heloisa Vieira",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Verified change","We present the textual wide-spectrum modeling and programing language K, which has been designed for representing graphical SysML models, in order to provide semantics to SysML, and pave the way for analysis of SysML models. The current version is supported by the Z3 SMT theorem prover, which allows to prove consistency of constraints. The language is intended to be used by engineers for designing space missions, and in particular NASA&rsquo;s proposed mission to Jupiter&rsquo;s moon Europa. One of the challenges facing software development teams is the notion of change: the fact that code changes over time, and the subsequent problem of demonstrating that no harm has been done due to a change. K is in this paper being applied to demonstrate how change can be perceived as a software verification problem, and hence verified using more traditional software verification techniques.<br/> &copy; Springer International Publishing AG 2016.","Havelund, Klaus and Kumar, Rahul",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The impact of tangled code changes on defect prediction models","When interacting with source control management system, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing version histories, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found between 7 % and 20 % of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6 % of all source files are incorrectly associated with bug reports. These incorrect bug file associations seem to not significantly impact models classifying source files to have at least one bug or no bugs. But our experiments show that untangling tangled code changes can result in more accurate regression bug prediction models when compared to models trained and tested on tangled bug datasets&mdash;in our experiments, the statistically significant accuracy improvements lies between 5 % and 200 %. We recommend better change organization to limit the impact of tangled changes.<br/> &copy; 2015, Springer Science+Business Media New York.","Herzig, Kim and Just, Sascha and Zeller, Andreas",2016,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR9"
"HBS-CRA: scaling impact of change request towards fault proneness: defining a heuristic and biases scale (HBS) of change request artifacts (CRA)","Accurately calculating the impact of existing change requests is vital for estimating the probability of fault occurrence in future change requests. For a new request like bug fixing, effectiveness in current change requests is required. In a real-time scenario, bug trackers are deployed to save change requests and their associated information. The trackers and associated information are saved in CVS version control systems and these systems assist programmers to carry out multiple analytical functions and generating descriptions. In our earlier works, we devised the set of change request artifacts and also proposed novel statistical bipartite weighted graphical models to evaluate DFP degree of future change requests. With the motivation gained from this model, here we propose a novel strategy that estimates the DFP of the request by assessing the impact of a change request artifact towards fault-proneness that considers the correlation between code blocks as another factor, which is in addition to our earlier strategy. A novel heuristic and biases scale to evaluate the effectiveness of change request for DFP is devised here in this paper that titled as &ldquo;Defining a Heuristic and Biases Scale (HBS) of Change Request Artifacts (CRA)&rdquo;, in short HBS-CRA. The devised model makes use of information retrieval methods to identify the change request artifacts of the request. In addition, it also checks for DFP scope through HBS-CRA. The HBS-CRA is empirically assessed by applying on concurrent versioning and Change request logs of the production level maintenance project.<br/> &copy; 2017 Springer Science+Business Media, LLC, part of Springer Nature","Madapuri, Rudra Kumar and Mahesh, P. C. Senthil",2017,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR9"
"2018 IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, MaLTeSQuE 2018 - Proceedings","The proceedings contain 9 papers. The topics discussed include: varying defect prediction approaches during project evolution: a preliminary investigation; the role of meta-learners in the adaptive selection of classifiers; machine learning-based run-time anomaly detection in software systems: an industrial evaluation; how high will it be? using machine learning models to predict branch coverage in automated testing; ensemble techniques for software change prediction: a preliminary investigation; co-evolution analysis of production and test code by learning association rules of changes; investigating type declaration mismatches in python; and user-perceived reusability estimation based on analysis of software repositories.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR9"
"Assessment of subchannel code ASSERT-PV for prediction of post-dryout heat transfer in CANDU bundles","Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadiar nuclear industry. The recently-released ASSERT-PV 3.2 provides enhanced models for improved predictions of subchannel flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against PDO tests performed during five full-size CANDU bundle experiments conducted in 1990s and in 2009 by Stem Laboratories (SL), using 28-, 37- and 43-element (CANFLEX) bundles. The SL experiments encompass the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for CANDU reactors. Codi predictions of maximum PDO fuel-sheath temperature were compared against measurements from the SL PDO tests to quantify the code?s prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. The sensitivity studies quantified the contribution of each PDO model change or enhancement to the improvement in PDO heat transfer prediction. Furthermore, the code convergence improvement was assessed against all selected SI tests as well. Overall, the assessment demonstrated significant improvement in prediction of PDO sheath temperature in horizontal fuel channels containing CANDU bundles.<br/>","Cheng, Z. and Rao, Y.F. and Waddington, G.M.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A novel technique for human face recognition using fractal code and bi-dimensional subspace","Face recognition is considered as one of the best biometric methods used for human identification and verification; this is because of its unique features that differ from one person to another, and its importance in the security field. This paper proposes an algorithm for face recognition and classification using a system based on WPD, fractal codes and two-dimensional subspace for feature extraction, and Combined Learning Vector Quantization and PNN Classifier as Neural Network approach for classification. This paper presents a new approach for extracted features and face recognition.Fractal codes which are determined by a fractal encoding method are used as feature in this system. Fractal image compression is a relatively recent technique based on the representation of an image by a contractive transform for which the fixed point is close to the original image. Each fractal code consists of five parameters such as corresponding domain coordinates for each range block. Brightness offset and an affine transformation. The proposed approach is tested on ORL and FEI face databases. Experimental results on this database demonstrated the effectiveness of the proposed approach for face recognition with high accuracy compared with previous methods.<br/> &copy; IFIP International Federation for Information Processing 2015.","Mohamed, Benouis and Kame, Benkkadour Mohamed and Redwan, Tlmesani and Mohamed, Senouci",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Development of few group cross section calculation model for core nuclear design code CYCAS","The few group cross section calculation model generates node homogeneous few group cross section for core 3D diffusion calculation, which is one of the key models of core calculation code. CYCAS is the new core 3D nuclear design code developed by Shanghai Nuclear Engineering Research &amp; Design Institute (SNERDI). A new model based on detail analysis of the factors affecting node cross section was developed for CYCAS. In the model, the energy spectrum correction method was used to process the second order effect introduced by energy spectrum change, and the micro-depletion correction method was utilized to treat depletion history effect. The numerical results of unit assembly and AP1000 core validate the high accuracy of the new model within CYCAS.<br/> &copy; 2016, Editorial Board of Atomic Energy Science and Technology. All right reserved.","Yang, Wei-Yan and Tang, Chun-Tao and Bi, Guang-Wen and Yang, Bo",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Introduction to the California historical building code","California is unusual in that it has developed and adopted a building code specifically intended for historic preservation. The California Historical Building Code (CHBC) (CBSC 2013b) was developed nearly forty years ago in response to the problem that ""restoration is frequently made difficult by the unnecessarily rigid interpretation of building...codes"" (Winters 2003). Despite the passing of significant time, many engineers and architects are still completely unaware of the existence of the CHBC and its context in the suite of California codes. Even those aware of its existence are sometimes prone to misconceptions regarding its content and use. This paper will discuss the history of the CHBC and its purpose and intent. A summary of use of the CHBC will be presented, along with its application-and its differences with the regular code-for repair, additions, and alterations; change of use; and seismic upgrades. This is the first of two companion papers that address the CHBC. A complimentary paper provides examples of beneficial use of the CHBC and cautions regarding potential misuse.<br/> &copy; 2015 ASCE and ATC.","Gilmartin, U.M. and Dreyfuss, A.R.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Effect of previous stress history and vegetation on the coefficient of earth pressure at-rest, K0, in London clay","Due to their previous stress history, highly overconsolidated clays, such as London clay, are characterised in their in-situ states by large values of the coefficient of earth pressure at-rest, K<inf>0</inf>, as confirmed by field and laboratory measurements. Numerical studies have focused in the past on the effect of previous stress history on K<inf>0</inf>, including pore water pressure variations due to changing the position of the phreatic surface. Nonetheless, under the combined effect of precipitation and vegetation, pore water pressures may change above the phreatic surface having only a small influence on its position.Therefore, additionally to previous stress history, soil-atmosphere interaction is expected to affect K<inf>0</inf>. This paper first presents a parametric study on the effect of previous stress history, carried out with the numerical code ICFEP, employing a kinematic hardening constitutive model. The effect of vegetation and precipitation is subsequently studied using sophisticated hydraulic boundary conditions. The numerical results demonstrate that soil-atmosphere interaction has a significant effect on the K<inf>0</inf>values even at depths considerably larger than the root depth. The effect of vegetation, however, seems to be erased by subsequent deposition and concurrent rise of the ground water table. &copy; 2014 Taylor &amp; Francis Group.<br/>","Tsiampousi, A. and Vitsios, I. and Zdravkovic, L. and Potts, D.M.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Why are commits being reverted? A comparative study of industrial and open source projects","Software development is a cyclic process of integrating new features while introducing and fixing defects. During development, commits that modify source code files are uploaded to version control systems. Occasionally, these commits need to be reverted, i.e., the code changes need to be completely backed out of the software project. While one can often speculate about the purpose of reverted commits (e.g., the commit may have caused integration or build problems), little empirical evidence exists to substantiate such claims. The goal of this paper is to better understand why commits are reverted in large software systems. To that end, we quantitatively and qualitatively study two proprietary and four open source projects to measure: (1) the proportion of commits that are reverted, (2) the amount of time that commits that are eventually reverted linger within a codebase, and (3) the most frequent reasons why commits are reverted. Our results show that 1%-5% of the commits in the studied systems are reverted. Those commits that are eventually reverted linger within the studied codebases for 1-35 days (median). Furthermore, we identify 13 common reasons for reverting commits, and observe that the frequency of reverted commits of each reason varies broadly from project to project. A complementary qualitative analysis suggests that many reverted commits could have been avoided with better team communication and change awareness. Our findings made Sony Mobile's stakeholders aware that internally reverted commits can be reduced by paying more attention to their own changes. On the other hand, externally reverted commits could be minimized only if external stakeholders are involved to improve inter-company communication or requirements elicitation.<br/> &copy; 2016 IEEE.","Shimagaki, Junji and Kamei, Yasutaka and McIntosh, Shane and Pursehouse, David and Ubayashi, Naoyasu",2016,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Characterizing a new dimension of change in attending and responding to the substance of student thinking","""Responsive teaching,"" in which teachers attend and respond to the substance of students' ideas, is central to facilitating student learning through engagement in authentic disciplinary practices. In characterizing teachers' progress toward greater responsiveness, researchers typically code teachers' attention as shifting toward the intellectual content (substance) of students' ideas and away from other foci such as students' correctness. These schemes, however, do not distinguish between different aspects of the substance of students' ideas. In this paper, we argue that a science teacher, Mr. S, demonstrates progress not by shifting toward greater attention to ""substance,"" but rather by shifting in the facet of student thinking to which he primarily attends and responds. He shifts toward attending to causal stories (mechanistic explanations) and away from causal factors (potentially relevant variables). We argue that such shifts toward more sophisticated epistemic practices should be targets of professional development and of the assessment of responsive teaching.<br/> &copy; 2014 ISLS.","Richards, Jennifer and Elby, Andrew and Gupta, Ayush",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"MySQL extension automatic porting to PDO for PHP migration and security improvement","In software management, the upgrade of programming languages may introduce critical issues. This is the case of PHP, the fifth version of which is going towards the end of the support. The new release improves on different aspects, but removes the old deprecated MySQL extensions, and supports only the newer library of functions for the connection to the databases. The software systems already in place need to be renewed to be compliant with respect to the new language version. The conversion of the source code, to be safe against injection attacks, should involve also the transformation of the query code. The purpose of this work is the design of specific tool that automatically applies the required transformation yielding to a precise and efficient conversion procedure. The tool has been applied to different projects to provide evidence of its effectiveness.<br/> &copy; Springer Nature Switzerland AG 2018.","Mondin, Fabio and Cortesi, Agostino",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"A quality driven extension to the QVT-relations transformationlanguage","An emerging approach to software development is Model Driven Software Development(MDSD). It shifts the focus from source code to models, aims at cost reduction, riskmitigation, and eases the engineering of complex applications. System models can beused in the early development stages to verify certain relevant properties, such asperformance, before source code is available and problems become hard and costly tosolve. The present status of Model Driven Engineering (MDE) is still far from thisideal situation. A well-known problem is feedback provisioning, which arises whendifferent solutions for the same design problem exist. An approach for feedbackprovisioning automation leverages model transformations, which glue together modelsin an MDSD setting, encapsulate the design rationale, and promote knowledge reuseand solutions otherwise available only to experienced engineers. In this article wepresent QVTR<sup>2</sup>, our solution to the feedback problem.QVTR<sup>2</sup>&nbsp;is an extension of the QVT-Relations languagewith constructs to express design alternatives, their impact on non-functionalmetrics, and how to evaluate them and guide the engineers in the selection of themost appropriate solution. We demonstrate the effectiveness of our solution by usingthe QVTR<sup>2</sup>&nbsp;engine to perform a modified version of thestandard UML-to-RDBMS transformation in thecontext of a real e-commerce application, and by showing how we can guide anon-expert engineer in the selection of a solution that satisfies given performancerequirements.<br/> &copy; 2011, Springer-Verlag.","Drago, Mauro Luigi and Ghezzi, Carlo and Mirandola, Raffaela",2015,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Detecting and predicting evolution in spreadsheets-a case study in an energy network company","The use of spreadsheets in industry is widespread and the information that they provide is often used for decisions. Research has shown that spreadsheets are error-prone, leading to the risk that decisions are made on incorrect information. Software Evolution is a well-researched topic and the results have proven to support developers in creating better software. Could this also be applied to spreadsheets? Unfortunately, the research on spreadsheet evolution is still limited. Therefore, the aim of this paper is to obtain a better understanding of how spreadsheets evolve over time and if the results of such a study provide similar benefits for spreadsheets as it does for source code. In this study, we cooperated with Alliander, a large energy network company in the Netherlands. We conducted two case studies on two different set of spreadsheets that both were already maintained for a period of three years. To have a better understanding of the spreadsheets itself and the context in which they evolved, we also interviewed the creators of the spreadsheets. We focus on the changes that are made over time in the formulas. Changes in these formulas change the behavior of the spreadsheet and could possibly introduce errors. To effectively analyze these changes we developed an algorithm that is able to detect and visualize these changes. Results indicate that studying the evolution of a spreadsheet helps to identify areas in the spreadsheet that are error-prone, likely to change or that could benefit from refactoring. Furthermore, by analyzing the frequency in which formulas are changed from version to version, it is possible to predict which formulas need to be changed when a new version of the spreadsheet is created.<br/> &copy; 2018 IEEE.","Jansen, Bas and Hermans, Felienne and Tazelaar, Edwin",2018,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Demo: Interactive robot transition repair","Complex robot behaviors are often structured as state machines, where states encapsulate actions and a transition function switches between states. Since transitions depend on physical parameters, when the environment changes, a roboticist has to painstakingly readjust the parameters to work in the new environment. In this demo we present Interactive SMT-based Robot Transition Repair (SRTR): instead of manually adjusting parameters, we ask users to identify a few instances where the robot is in a wrong state and what the right state should be. A lightweight automated analysis of the transition function's source code then 1) identifies adjustable parameters, 2) converts the transition function into a system of logical constraints, and 3) formulates the constraints and user-supplied corrections as a MaxSMT problem that yields adjustments to parameter values. This demo uses a simulated RoboCup Small Size League platform, allows users to correct faulty behaviors, and then uses SRTR to adjust parameters automatically.<br/> &copy; 2018 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.","Holtz, Jarrett and Guha, Arjun and Biswas, Joydeep",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Synthesizing VHDL from activity models in UML 2","This document describes a synthesis technology that generates structural VHDL code from models describing the flow of data required to perform algorithms operating on bit-blocks. The models are built using restricted activity diagrams in the Unified Modeling Language version 2. The code generator is developed using Acceleo, a technology to implement transformations from models to text. The technology described in this paper exploits the principles of object orientation and model-driven engineering. The primary aim is to improve productivity and alleviate complexity during the design of digital hardware systems that implement demanding operations used by a wide variety of computing devices. The use of the technology is illustrated with the generation of VHDL code from models describing a block cipher algorithm. Copyright &copy; 2012 John Wiley &amp; Sons, Ltd. We explore the feasibility of transforming high-level models of block cipher algorithms, built using activity diagrams in the Unified Modeling Language version 2 (UML 2), to source code in VHDL. The lower level VHDL representation may be synthesized and implemented in a hardware platform like a FPGA. We describe how to adapt the UML 2 language to model block cipher algorithms accurately and the transformation tool that generates VHDL code from the UML 2 diagrams. This technology aims at improving the productivity of the designers. &copy; 2012 John Wiley &amp; Sons, Ltd.<br/>","Balderas-Contreras, Tomas and Cumplido, Rene and Rodriguez, Gustavo",2014,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Programming arduino boards with the C/C++ interpreter CH","This paper presents the ChArduino package which is designed to control the Atmel AVR microcontroller based Arduino boards through the C/C++ interpreter Ch. Traditionally, Arduino boards are programmed using the official Arduino IDE or lower-level AVR C libraries. These methods require specific cross-compilation tools to compile the code and upload it onto the board. Whenever a change is made to the source code, it needs to be recompiled and uploaded, making application development cumbersome, especially for beginners and as the size of the application grows. The approach presented in this paper is aimed at reducing the effort associated with code compilation, especially in classroom environments where microcontroller programming is first introduced. In fact, when using this method, code is executed in an interpreted manner and every function call is processed separately by the interpreter, thus compilation and uploading are not required to make changes effective. The ChArduino package consists of a library of functions running on a computer and a specialized firmware loaded onto the Arduino board. The firmware on the Arduino board is pre-compiled and the latest version is automatically uploaded at run time, if not already. At power-up, the firmware initializes the board and then waits for a command from the computer. The use of the C/C++ interpreter Ch also makes available line-by-line debugging, nu-merical analysis, and plotting capabilities. The supported communication protocols between the Arduino board and the computer are serial and Bluetooth. The application code written using this package is completely compatible with the entire spectrum of Arduino boards and can be ported to the Arduino IDE with minimal changes. The applications of the method described in this paper are general but apply especially to the K-12 education field in that the package creates a simple, user-friendly, environment for the absolute beginner to learn the basic principles of mechatronic systems including programming, microcontrollers, and electrical circuits. Lesson plans are being developed to use the ChArduino package in microcontroller introductory courses and the package is currently being introduced for preliminary testing in schools through the UC Davis C-STEM Center.<br/> &copy; Copyright 2015 by ASME.","Turley, Curtis and Alessandra Montironi, Maria",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Towards suitable research paradigm for assessing the impact of free and open source software (foss)","Free and Open Source Software (FOSS) allows users to use, change, and redistribute the source code. Recent changes in the software technologies landscape involve the introduction of FOSS which presents certain benefits and freedom in the use of software that demonstrate high potential towards achieving competitive advantage by institutions of higher learning. Higher institutions of learning stand to gain the benefits in teaching, learning and research in particular by adopting FOSS. Towards determining the possibility of such gains, research efforts can be conducted using interpretive and positivist approaches. This study proposes exploration using the two approaches in the form of case study and survey so that readers can make informed choice that could lead to the development of appropriate frameworks towards addressing the research objectives.<br/>","Dehinbo, Kehinde O. and Dehinbo, Johnson O.",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Where's my code? Engineers navigating ethical issues on an uneven terrain","Claims to professionalism among engineers are rooted in three key features: a specialized knowledge base, self-regulation, and a commitment to public service-[1-3] elements that have been historically codified into a set of ethical guidelines [1, 4, 5]. While these guidelines-Professional Codes of Ethics-may help engineers appreciate what not to do [4, 5], they are insufficiently specific to guide novice engineers through ethically ambiguous situations. As early 20<sup>th</sup> century artefacts, they also tend to reproduce structural inequities embedded in the history of the profession, and thus fail to reflect the experiences of historically underrepresented groups of engineers [6-14]. The Canadian Engineering Accreditation Board's (CEAB) pairing of ethics and equity [15] demands that we look beyond the codes to help our students navigate ethically ambiguous situations and patterns of privilege likely to arise in their professional lives. Unfortunately, there are several barriers to this process. Our critical analysis of career history interviews with 15 engineers committed to ethics and equity highlight three such barriers: 1) dominant narratives in engineering that make it difficult for social justice viewpoints to be acknowledged; 2) limited organizational influence on the part of junior engineers trying to challenge inequitable workplace practices; and 3) a fear that raising equity issues will result in personal attacks rather than positive change. Together, these three barriers-raised almost exclusively by female, racially under-represented, and LGBTQ identified engineers-illustrate the uneven terrain on which engineers navigate ethical issues.<br/> &copy; American Society for Engineering Education, 2018.","Rottmann, Cindy and Reeve, Doug and Sacks, Robin and Klassen, Mike",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Design Space Exploration based on multiobjective genetic algorithms and clustering-based high-level estimation","A desirable characteristic in high-level synthesis (HLS) is fast search and analysis of implementation alternatives with low or none intervention. This process is known as Design Space Exploration (DSE) and it requires an efficient search method. The employment of intelligent techniques like evolutionary algorithms has been investigated as an alternative to DSE. They turn possible to reduce the search time through selection of higher potential regions of the solution space. We propose here the development of a DSE approach based on a multiobjective evolutionary algorithm (MOEA) and machine learning techniques. It must be employed to indicate the code transformations and architectural parameters adopted in design solution. Furthermore, DSE will use a high-level estimator model to evaluate candidate solutions. Such model must be able to provide a good estimation of energy consumption and execution time at early stages of design. &copy; 2013 IEEE.<br/>","Martins, Luiz G. A. and Marques, Eduardo",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Self-modification of policy and utility function in rational agents","Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify &ndash; for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby &lsquo;escaping&rsquo; the control of their creators. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.<br/> &copy; Springer International Publishing Switzerland 2016.","Everitt, Tom and Filan, Daniel and Daswani, Mayank and Hutter, Marcus",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Customizable Computing-- From Single Chip to Datacenters","Since its establishment in 2009, the Center for Domain-Specific Computing (CDSC) has focused on customizable computing. We believe that future computing systems will be customizable with extensive use of accelerators, as custom-designed accelerators often provide 10-100X performance/energy efficiency over the general-purpose processors. Such an accelerator-rich architecture presents a fundamental departure from the classical von Neumann architecture, which emphasizes efficient sharing of the executions of different instructions on a common pipeline, providing an elegant solution when the computing resource is scarce. In contrast, the accelerator-rich architecture features heterogeneity and customization for energy efficiency; this is better suited for energy-constrained designs where the silicon resource is abundant and spatial computing is favored--which has been the case with the end of Dennard scaling. Currently, customizable computing has garnered great interest; for example, this is evident by Intel's $17 billion acquisition of Altera in 2015 and Amazon's introduction of field-programmable gate-arrays (FPGAs) in its AWS public cloud. In this paper, we present an overview of the research programs and accomplishments of CDSC on customizable computing, from single chip to server node and to datacenters, with extensive use of composable accelerators and FPGAs. We highlight our successes in several application domains, such as medical imaging, machine learning, and computational genomics. In addition to architecture innovations, an equally important research dimension enables automation for customized computing. This includes automated compilation for combining source-code-level transformation for high-level synthesis with efficient parameterized architecture template generations, and efficient runtime support for scheduling and transparent resource management for integration of FPGAs for datacenter-scale acceleration with support to the existing programming interfaces, such as MapReduce, Hadoop, and Spark, for large-scale distributed computation. We will present the latest progress in these areas, and also discuss the challenges and opportunities ahead.<br/> IEEE","Cong, Jason and Fang, Zhenman and Huang, Muhuan and Wei, Peng and Wu, Di and Yu, Cody Hao",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Model based rapid prototyping and evolution of web application","We demonstrate a development work-flow for the co-evolution of model and code, based on IFMLEdit.org, an online tool for the rapid prototyping of web applications, and on common Version Control Systems. IFMLEdit.org exploits the Interaction Flow Modeling Language (IFML), an OMG standard for describing the user&rsquo;s interaction with the application by means of flows of information in reaction to user events. In the demo, attendees will be able to edit IFML specifications with IFMLEdit.org, generate the first version of the code of a web/mobile application from the model, improve the generated code with manually added details (e.g. styling), evolve the original IFML model introducing new requirements and re-generate the code of the updated version, in a way that fully preserves the manually coded details. The demonstrated approach solves the well-know problem of model driven forward engineering of breaking the automated development cycle when features that cannot be modelled are added manually to the generated code.<br/> &copy; Springer International Publishing AG, part of Springer Nature 2018.","Falzone, Emanuele and Bernaschina, Carlo",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Satisfying rival forestry objectives in the komi republic: Effects of russian zoning policy change on wood production and riparian forest conservation","Spatial segregation of different forest landscape functions can accommodate rival forestry objectives more comprehensively than integrated approaches. Russia has a unique history of forest zoning separating production and environmental functions. However, the Russian Forest Code of 2006 increased the focus on wood production. We reviewed the history of zoning policy in Russia and assessed if the recent policy change affected logging rates and conservation of riparian forests. Using Russia&rsquo;s Komi Republic as a case study, we specifically assessed (i) if policy change led to increased logging near streams, (ii) if logging rates were different in headwaters vs. main rivers, and (iii) how logging changed among catchments with different accessibility to logging. Using a global open-access remote sensing dataset, we compared mean annual forest loss as a proxy of logging rates in 10 large forested catchments in the Komi Republic in one period with strict zoning policy (2000&ndash;2006) and one with moderate zoning policy (2007&ndash;2014). Harvesting rate was positively related to the distance from streams. On the other hand, it increased after the policy change in the buffer zone but decreased outside it. Forests were harvested more in headwater buffers than along larger rivers, and harvest in the catchments near industries was higher and increasing; remote catchments had low forest loss. We discuss the opportunity for adopting forest zoning policy in different governance contexts.<br/> &copy; 2017, Canadian Science Publishing. All rights reserved.","Naumov, Vladimir and Angelstam, Per and Elbakidze, Marine",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Proposition of a three layers architecture for adaptable artificial intelligence","This paper proposes a concept of artificial intelligence architecture which would be usable in various kind of problems. Artificial intelligences are used in many areas of computer science for decision making tasks. Traditionally each artificial intelligence is designed and programmed to be used within a particular software and for a specific purpose. However, this paper stands as the first step of research in progress whose final objective is to create an artificial intelligence adaptable to all kinds of problems without any change in its source code. The present proposition focuses on the architecture of that artificial intelligence and is introduced in the context of video games. This architecture, composed of three layers, would be re-usable for all types of game.<br/> &copy; 2014 IEEE.","Benoit, Vallade and Alexandre, David and Nakashima, Tomoharu",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"GreenScaler: training software energy models with automatic test generation","Software energy consumption is a performance related non-functional requirement that complicates building software on mobile devices today. Energy hogging applications (apps) are a liability to both the end-user and software developer. Measuring software energy consumption is non-trivial, requiring both equipment and expertise, yet researchers have found that software energy consumption can be modelled. Prior works have hinted that with more energy measurement data we can make more accurate energy models. This data, however, was expensive to extract because it required energy measurement of running test cases (rare) or time consuming manually written tests. In this paper, we show that automatic random test generation with resource-utilization heuristics can be used successfully to build accurate software energy consumption models. Code coverage, although well-known as a heuristic for generating and selecting tests in traditional software testing, performs poorly at selecting energy hungry tests. We propose an accurate software energy model, GreenScaler, that is built on random tests with CPU-utilization as the test selection heuristic. GreenScaler not only accurately estimates energy consumption for randomly generated tests, but also for meaningful developer written tests. Also, the produced models are very accurate in detecting energy regressions between versions of the same app. This is directly helpful for the app developers who want to know if a change in the source code, for example, is harmful for the total energy consumption. We also show that developers can use GreenScaler to select the most energy efficient API when multiple APIs are available for solving the same problem. Researchers can also use our test generation methodology to further study how to build more accurate software energy models.<br/> &copy; 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Chowdhury, Shaiful and Borle, Stephanie and Romansky, Stephen and Hindle, Abram",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Towards a framework for stochastic performance optimizations in compilers and interpreters - An architecture overview","Modern compilers and interpreters provide code optimizations before and during run-time to stay competitive with alternative execution environments, thus moving required domain knowledge about the compilation process away from the developer and speeding up resulting software. These optimizations are often based on formal proof, or alternatively have recovery paths as backup. This publication proposes an architecture utilizing abstract syntax trees (ASTs) to optimize the runtime performance of code with stochastic - search based - machine learning techniques. From these AST modifying optimizations a pattern mining approach attempts to find transformation patterns which are applicable to a software language. The application of these patterns happens during the parsing process or the programs run-time. Future work consists of implementing and extending the presented architecture, with a considerable focus on the mining of transformation patterns.<br/> Copyright &copy; 2018 held by the owner/author(s). Publication rights licensed to ACM.","Krauss, Oliver",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Foreground segmentation using convolutional neural networks for multiscale feature encoding","Several methods have been proposed to solve moving objects segmentation problem accurately in different scenes. However, many of them lack the ability of handling various difficult scenarios such as illumination changes, background or camera motion, camouflage effect, shadow etc. To address these issues, we propose two robust encoder-decoder type neural networks that generate multi-scale feature encodings in different ways and can be trained end-to-end using only a few training samples. Using the same encoder-decoder configurations, in the first model, a triplet of encoders take the inputs in three scales to embed an image in a multi-scale feature space; in the second model, a Feature Pooling Module (FPM) is plugged on top of a single input encoder to extract multi-scale features in the middle layers. Both models use a transposed convolutional network in the decoder part to learn a mapping from feature space to image space. In order to evaluate our models, we entered the Change Detection 2014 Challenge (changedetection.net) and our models, namely FgSegNet_M and FgSegNet_S, outperformed all the existing state-of-the-art methods by an average F-Measure of 0.9770 and 0.9804, respectively. We also evaluate our models on SBI2015 and UCSD Background Subtraction datasets. Our source code is made publicly available at https://github.com/lim-anggun/FgSegNet.<br/> &copy; 2018 Elsevier B.V.","Lim, Long Ang and Yalim Keles, Hacer",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A new analytical solution for assessing climate change impacts on subsurface temperature","Groundwater temperature is an important water quality parameter that affects species distributions in subsurface and surface environments. To investigate the response of subsurface temperature to atmospheric climate change, an analytical solution is derived for a one-dimensional, transient conduction-advection equation and verified with numerical methods using the finite element code SUTRA. The solution can be directly applied to forward model the impact of future climate change on subsurface temperature profiles or inversely applied to produce a surface temperature history from measured borehole profiles. The initial conditions are represented using superimposed linear and exponential functions, and the boundary condition is expressed as an exponential function. This solution expands on a classic solution in which the initial and boundary conditions were restricted to linear functions. The exponential functions allow more flexibility in matching climate model projections (boundary conditions) and measured temperature-depth profiles (initial conditions). For example, measured borehole temperature data from the Sendai Plain and Tokyo, Japan, were used to demonstrate the improved accuracy of the exponential function for replicating temperature-depth profiles. Also, the improved accuracy of the exponential boundary condition was demonstrated using air temperature anomaly data from the Intergovernmental Panel on Climate Change. These air temperature anomalies were then used to forward model the effect of surficial thermal perturbations in subsurface environments with significant groundwater flow. The simulation results indicate that recharge can accelerate shallow subsurface warming, whereas upward groundwater discharge can enhance deeper subsurface warming. Additionally, the simulation results demonstrate that future groundwater temperatures obtained from the proposed analytical solution can deviate significantly from those produced with the classic solution. &copy; 2013 John Wiley &amp; Sons, Ltd.<br/>","Kurylyk, Barret L. and Macquarrie, Kerry T.B.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An evolutionary framework for cultural change: Selectionism versus communal exchange","Dawkins' replicator-based conception of evolution has led to widespread mis-application of selectionism across the social sciences because it does not address the paradox that necessitated the theory of natural selection in the first place: how do organisms accumulate change when traits acquired over their lifetime are obliterated? This is addressed by von Neumann's concept of a self-replicating automaton (SRA). A SRA consists of a self-assembly code that is used in two distinct ways: (1) actively deciphered during development to construct a self-similar replicant, and (2) passively copied to the replicant to ensure that it can reproduce. Information that is acquired over a lifetime is not transmitted to offspring, whereas information that is inherited during copying is transmitted. In cultural evolution there is no mechanism for discarding acquired change. Acquired change can accumulate orders of magnitude faster than, and quickly overwhelm, inherited change due to differential replication of variants in response to selection. This prohibits a selectionist but not an evolutionary framework for culture and the creative processes that fuel it. The importance non-Darwinian processes in biological evolution is increasingly recognized. Recent work on the origin of life suggests that early life evolved through a non-Darwinian process referred to as communal exchange that does not involve a self-assembly code, and that natural selection emerged from this more haphazard, ancestral evolutionary process. It is proposed that communal exchange provides an evolutionary framework for culture that enables specification of cognitive features necessary for a (real or artificial) societies to evolve culture. This is supported by a computational model of cultural evolution and a conceptual network based program for documenting material cultural history, and it is consistent with high levels of human cooperation. &copy; 2013 Elsevier B.V.<br/>","Gabora, Liane",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Optimization and parallelization of the thermal-hydraulic subchannel code Cobra-En using parallel infrastructure jasmin","COBRA-EN is a subchannel code for thermal-hydraulic transient and steady problems, selected to be the reactor core thermal hydraulic (TH) simulation tool. Upgraded by Basile et al., COBRA-EN (CEN) is able to solve three-equation mixture model and four-equation model using Pressure-Gradient iteration scheme or Newton-Raphson iteration scheme. However, in the successive versions of COBRA computer programs, the current serial version COBRA-EN can only simulate small pincell TH model due to the limitation of memory and efficient problems. In the present work, the Parallel version of COBRA-EN (PCEN) is developed based on JASMIN (J parallel Adaptive Structured Mesh applications INfrastructure). There are mainly four steps to achieve the parallelization of CEN. The first step is to standardize CEN, such as deleting the &lsquo;common&rsquo; module and &lsquo;implicit none&rsquo; usage. The preprocessing of the input cards is also abandoned and all the input parameters are transferred from JASMIN to the CEN subprogram. The initializing methods of flow field variables are changed to adapt the executing routines of JASMIN, too. Secondly, the memory is allocated explicitly. The original method using a big one-dimensional array to manage all variables is showed to be not suitable for parallel program. Thirdly, it is important to establish transformation subprograms from CEN&rsquo;s approximate unstructured mesh to JASMIN&rsquo;s structured mesh system and after the computing is accomplished, the results should be transformed in reverse for the next time step. The last step is applying the linear solver of JASMIN to the parallel version TH code to solve energy, moment and pressure-gradient or pressure-difference matrix for different numerical methods. To validate the Standardized version of serial COBRA-EN (SCEN), we compare the results of original CEN and SCEN using 45 rods model. The results show a little difference for flow variables such as coolant temperature and pressure drop distributions and less for fuel rod temperature distributions. The comparisons between SCEN and PCEN are currently made using pincell-resolved 9&times;9 and 4 assemblies model to validate data transferring and boundary conditions. It shows acceptable difference for coolant density, temperature and pressure drop distributions along the axial direction.<br/> &copy; 2016 Association for Computing Machinery Inc. All Rights Reserved.","Li, Kang and Liu, Na and Liu, Peng and Shi, Dunfu and Tian, Baolin",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The unified integral transforms (Unit) algorithm with total and partial transformation","The theory and algorithm behind the open-source mixed symbolic-numerical computational code named UNIT (unified integral transforms) are described. The UNIT code provides a computational environment for finding solutions of linear and nonlinear partial differential systems via integral transforms. The algorithm is based on the well-established analytical-numerical methodology known as the generalized integral transform technique (GITT), together with the mixed symbolic-numerical computational environment provided by the Mathematica system (version 7.0 and up). This paper is aimed at presenting a partial transformation scheme option in the solution of transient convective-diffusive problems, which allows the user to choose a space variable not to be integral transformed. This approach is shown to be useful in situations when one chooses to perform the integral transformation on those coordinates with predominant diffusion effects only, whereas the direction with predominant convection effects is handled numerically, together with the time variable, in the resulting transformed system of one-dimensional partial differential equations. Test cases are selected based on the nonlinear three-dimensional Burgers&rsquo; equation, with the establishment of reference results for specific numerical values of the governing parameters. Then the algorithm is illustrated in the solution of conjugated heat transfer in microchannels.<br/> &copy; 2014 by Begell House, Inc.","Cotta, Renato M. and Knupp, Diego C. and Naveira-Cotta, Carolina P. and Sphaier, Leandro A. and Quaresma, Joaon",2014,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Development of a discrete adjoint CFD code using algorithmic differentiation by operator overloading","Design optimization for fluid flow recently has drawn a great deal of attention in the industrial design area including aeronautic[1, 2], turbo-machinery[3] and automotive design[4, 5]. The optimization problem is subject to a large number of design variables that makes calculating the sensitivity derivatives by finite differences a costly procedure. Algorith- mic differentiation(AD) is a well known method to differentiate the computer program with the aim of obtaining the sensitivity of the objective with respect to design variables. There are two different AD approaches, operator overloading and source transformation. In this paper we describe a design framework for application of the algorithmic differentiation tool by operator overloading in Fortran dco/fortran<sup>1</sup>to CFD analysis solver called GPDE<sup>2</sup>. GPDE is an un- structured pressure-based steady Navier-Stokes code with finite volume spatial discretization, which is based on the SIMPLE pressure-correction scheme for the incompressible viscous flow computation. This approach yields a discrete tangent-linear and adjoint version of the CFD code. Moreover, we address typical implementation issues and complexities in the differentia- tion procedure by AD tool in the CFD code. The numerical results of a relevant test case by our overloading tool in forward and reverse mode are compared with their corresponding results of the previously differentiated code by source transformation that is generated by TAPENADE. We show that in terms of ease of implementation and ability to handle arbitrary functions, dco/- fortran provides the differentiated code with a greater flexibility and robustness in comparison with AD by source transformation. This research is aimed toward the application of AD tools by operator overloading on a legacy industrial incompressible flow solver that is in our context ESI? ACE+.<br/>","Dastouri, Z. and Lotz, J. and Naumann, U.",2014,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Code-changeable encoded microparticles for multi-step bead-based assay","Recording the reaction history of microparticle-based combination assays is important. However, several existing encoding methods cannot change microparticle codes. In this paper, we present a new method of encoding microparticles that uses a photoluminescent material for multiple code writing. 2,2-Dimethoxy-2-phenylacetophenone (DMPA) is a commonly used photoinitiator for free-radical polymerization. DMPA exhibits photoluminescence when irradiated in the ultraviolet region. Photopolymerized microparticles that contain DMPA were generated and then graphically encoded by using the Optofluidic Maskless Lithography system [1]. Our encoding method has advantages such as high coding capacity and long-term durability aside from enabling repeated writing on microparticles. Our encoding method that uses the DMPA photoinitiator can be applicable to multi-step microparticle-based assays.<br/>","Kwon, Taehong and Song, Younghoon and Lee, Daewon and Kim, Mira and Park, Tae-Joon and Kwon, Sunghoon",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Code review analytics: Webkit as case study","During the last years, most of the large free / open source software projects have included code review as an usual, or even mandatory practice for changes to their code. In many cases it is implemented as a process in which a developer proposing some change needs to ask for a review by another developer before it can enter the code base. Code reviews, therefore, become a critical process for the project, which could cause delays in contributions being accepted, and risk to become a bottleneck if not enough reviewers are available. In this paper we present a methodology designed to analyze the code review process, to determine its main characteristics and parameters, and to detect potential problems with it. We also present how we have applied this methodology to the WebKit project, learning about the main characteristics of how code review works in their case.<br/> &copy; IFIP International Federation for Information Processing 2014.","Gonzalez-Barahona, Jesus M. and Izquierdo-Cortazar, Daniel and Robles, Gregorio and Gallegos, Mario",2014,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Supporting code review by automatic detection of potentially buggy changes","Code reviews constitute an important activity in software quality assurance. Although they are essentially based on human expertise and scrupulosity, they can also be supported by automated tools. In this paper we present such a solution integrated with code review tools. It is based on a SVM classifier that indicates potentially buggy changes. We train such a classifier on the history of a project. In order to construct a training set, we assume that a change/commit is buggy if its modifications has been later altered by a bug-fix commit. We evaluated our approach on 77 selected projects taken from GitHub and achieved promising results. We also assessed the quality of the resulting classifier depending on the size of a project and the fraction of the history of a project that have been used to build the training set.<br/> &copy; Springer International Publishing Switzerland 2015.","Fejzer, Mikolaj and Wojtyna, Michal and Burzaska, Marta and Winiewski, Piotr and Stencel, Krzysztof",2015,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Learning looping: From natural language to worked examples","One important introductory concept in many CS courses is repetition (looping), the automated repeating of individual commands. In this work, we present results from a study of college undergraduates' naive conceptions of repetition, their difficulties with learning to construct valid repetition statements, and their abilities to apply what they have learned to new problem solving situations. Although computer programming is a new topic when high school or college students encounter it for the first time, students can draw upon their previous life experiences when solving problems. Those conceptions that align with CS topics [2,3] have been shown to be influenced by students' prior experiences. Alignment through analogies can be helpful [1] although where the scientific concept differs, common knowledge can hinder learning [4]. For many students, the topic of looping is their first encounter with nonlinearity in their programs. Until this point, each line of code is executed once, and then control moves to the next line of code. Such linearity makes reasoning about the programs straightforward. With the addition of looping, in the code you will need to evaluate a termination condition and then either repeat prior lines of code, or move to the next statement after the loop. While returning to a previous command or location is not unusual in everyday life and natural language, it is an important change in the way that novices see their code. &copy; 2012 Springer-Verlag.<br/>","Sudol-DeLyser, Leigh Ann and Stehlik, Mark and Carver, Sharon",2012,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Experience with digital game-based embodied learning: The road to create a framework for physically interactive digital games","Over the past years, we have been researching various approaches to digital game-based learning in the field of change and innovation management. Broadening the range of possible applications while consolidating methodical underpinnings, we have subsequently narrowed down our findings into the description of three specific treatments. This paper focusses on one of the applied treatments, namely to make participants go through a game sequence or interact with a digitally enhanced setup (e.g. play-acting with motion capturing and real-time rendering of a virtual character) to engage learners in embodied and experience driven learning. We present our experience starting with commercial of the shelf physically interactive digital games, followed by two examples of self-made stand-alone Kinect games that have been developed for use in team and leadership trainings. The latter will be introduced describing their goals, the resulting game design as well as lessons learned. Starting from the experience with such settings in project ""HELD"" as well as applications of embodied digital learning and physically interactive game-based learning by others led us to the belief that there is a need for a framework that enables educational game and interaction designers to develop digital embodied settings without the need of (re)coding the Kinect management code as well as a number of other features relevant for education and training settings (e.g. control app, QR player identification and performance tracking). To further foster the easy development of physically interactive digital games and simulations, or digital aesthetic performances the framework integrates with the Unity game engine, thus enabling both rapid prototyping and quality games. First tests seem very promising with playable game prototypes developed in less than three days. To gather more feedback on real-life applications using digital embodied learning we plan to offer the introduced framework free of charge for non-profit applications.<br/>","Busch, Carsten and Conrad, Florian and Meyer, Robert and Steinicke, Martin",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Code analyser in CSTutor - A C# Intelligent Tutoring System","This paper describes the process that is performed by CSTutor to analyse each student program. CSTutor is an Intelligent Tutoring System that supports the student's learning by doing. Built as an integrated part of Visual Studio 2010 or 2012, CSTutor can give assistance to a student writing programs in Visual Studio from the earliest stage. The analysis process starts by capturing the student's program from the Visual Studio Editor. The program is then parsed and simplified into facts in a knowledge base. This knowledge base also contains rules, actions, constraints, and a goal to be achieved. The goal can be decomposed into several sub-goals to give a finer detail of feedback to the student. So that it can be used as a practical supplement to classroom instruction, CSTutor provides a number of exercises that can be tried by the students. Further, the number of exercises can be increased without having to change CSTutor's program code. The teacher just needs to add the description of the exercise, the constraints, and the goal that should be achieved in the new exercise. The evaluation of CSTutor is in progress and it is expected that CSTutor will help students learn programming to an improved degree.<br/>","Hartanto, Budi and Reye, Jim",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Predicting change consistency in a clone group","Code cloning has been accepted as one of the general code reuse methods in software development, thanks to the increasing demand in rapid software production. The introduction of clone groups and clone genealogies enable software developers to be aware of the presence of and changes to clones as a collective group; they also allow developers to understand how clone groups evolve throughout software life cycle. Due to similarity in codes within a clone group, a change in one piece of the code may require developers to make consistent change to other clones in the group. Failure in making such consistent change to a clone group when necessary is commonly known as &ldquo;clone consistency-defect&rdquo;, which can adversely impact software reusability. In this work, we propose an approach to predict the need for making consistent change in clones within a clone group at the time when changes have been made to one of its clones. We build a variant of clone genealogies to collect all consistent/inconsistent changes to clone groups, and extract three attribute sets from clone groups as input for predicting the need for consistent clone change. These three attribute sets are code attributes, context attributes and evolution attributes respectively. Together, they provide a holistic view about clone changes. We conduct experiments on four open source projects. Our experiments show that our approach has reasonable precision and recall in predicting whether a clone group requires (or is free of) consistent change. This holistic approach can aid developers in maintaining clone changes, and avoid potential consistency-defect, which can improve software quality and reusability.<br/> &copy; 2017 Elsevier Inc.","Zhang, Fanlong and Khoo, Siau-cheng and Su, Xiaohong",2017,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR9"
"Multi up-gradation software reliability growth model with learning effect and severity of faults using SDE","In recent years, the dependence on a computer system has become large in our social life. Therefore, it becomes more important for software developers to produce highly reliable software systems. Due to timely demand and competitive nature of the market of software product, firms are frequently launching their upgraded versions of the base software. Many software reliability growth model have been developed by software developers and managers in tracking and measuring the growth of reliability. As the size of software system is large and the number of faults detected during the testing phase becomes large, so the change of the number of faults that are detected and removed through each debugging becomes sufficiently small compared with the initial fault content at the beginning of the testing phase. In such a situation, we can model the software fault detection process as a stochastic process with continuous-state space. In this paper, we derive a stochastic differential equation of (Formula presented.) type based multi-up-gradation model with severity of faults and effect of learning. Moreover, we discuss the identification of the faults left in the software when it is in operational phase during the testing of the new code i.e. developed while adding new features to the existing software. We examine the case where there exists two types of faults in the software; simple and hard and during testing the simple faults are removed by exponential rate whereas hard faults are removed by Yamada with learning effect function. Results are supplemented by a numerical example.<br/> &copy; 2014, The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.","Singh, Jagvinder and Singh, Ompal and Kapur, P.K.",2015,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Predicting Consistent Clone Change","Code clones, being an inevitable by-product of rapid software development, can impact software quality. The introduction of code clone groups and clone genealogies enable software developers to be aware of the presence of and changes to clones as a collective group, they also allow developers to understand how clone groups evolve throughout software life cycle. Due to similarity in codes within a clone group, a change in one piece of the code may require developers to make changes to other clones in the group. Failure in making consistent change to a clone group when necessary is commonly known as 'clone consistency-defect', which can adversely impact software reliability. We propose an approach to predict clone consistency-requirement at the time when changes have been made to a clone group. Our predictor is a Bayesian network implemented in WEKA. We build a variant of clone genealogies to collect all consistent/inconsistent changes to clone groups, and extract three sets of attributes from clone groups as input for predicting consistent clone change. These three sets are: code attributes, context attributes and evolution attributes. We conduct experiments on three open source projects. These experiments show that our approach has high precision and recall in predicting clone consistency-requirement. This holistic approach can aid developers in maintaining code clone changes, and avoid potential clone consistency-defect, which can improve the software quality and reliability.<br/> &copy; 2016 IEEE.","Zhang, Fanlong and Khoo, Siau-Cheng and Su, Xiaohong",2016,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Formal description and verification of a text-based model differencing and merging method","Version control is an integral part of teamwork in software development. Differencing and merging key artifacts (i.e. source code) is a key feature in version control systems. The concept of version control can also be applied to model-driven methodologies. The models are usually differenced and merged in their graph-based form. However, if supported, we can also use the textual representation of the models during this process. Text-based model differencing and merging methods have some useful use cases, like supporting the persistence of the model, or having a fallback plan should the differencing algorithm fail. Using the textual notation to display and edit models is relatively rare, as the visual (graph-based) representation of the model is more common. However, many believe that using them both would be the ideal solution. In this paper, we present the formal description of a text-based model differencing and merging method from previous work. We also verify our algorithm based on this formal description. The focus of the verification is the soundness and completeness of the method. The long term goal of our research is to develop a modeling environment-independent algorithm. This could be used in version control systems that support textual representations.<br/> Copyright &copy; 2018 by SCITEPRESS &ndash; Science and Technology Publications, Lda. All rights reserved.","Somogyi, Ferenc A. and Asztalos, Mark",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Minnesota sustainable building guidelines: History, effectiveness and path for the future","INTRODUCTION The Minnesota Sustainable Building Guidelines is a progressive sustainability program for state funded buildings which serves as a model for sustainability in Minnesota buildings. The program was created by the State of Minnesota in 2001 and developed by a team led by the Center for Sustainable Building Research (CSBR) at the University of Minnesota. Unlike other green building programs, it focuses on measured performance improvements, using a list of required metrics instead of a menu of potential options. The program is structured to provide a feedback loop to the building design, construction and operations industry in the state. Elements of the program are used through all phases of the development of state-funded buildings in Minnesota from pre-design through design, and construction and for ten years of operations. It is continually updated and improved in collaboration with state agencies and industry stakeholders and could serve as a model for localized green building programs.<br/> &copy; 2018, College Publishing. All rights reserved.","Graves, Richard and Smith, Patrick",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Axially deformed solution of the SkyrmeHartreeFockBogolyubov equations using the transformed harmonic oscillator basis (III) HFBTHO (v3.00): A new version of the program","We describe the new version 3.00 of the code HFBTHO that solves the nuclear Hartree&ndash;Fock (HF) or Hartree&ndash;Fock&ndash;Bogolyubov (HFB) problem by using the cylindrical transformed deformed harmonic oscillator basis. In the new version, we have implemented the following features: (i) the full Gogny force in both particle&ndash;hole and particle&ndash;particle channels, (ii) the calculation of the nuclear collective inertia at the perturbative cranking approximation, (iii) the calculation of fission fragment charge, mass and deformations based on the determination of the neck, (iv) the regularization of zero-range pairing forces, (v) the calculation of localization functions, (vi) a MPI interface for large-scale mass table calculations. PROGRAM SUMMARY Program title:HFBTHO v3.00 Program Files doi: http://dx.doi.org/10.17632/c5g2f92by3.1 Licensing provisions: GPL v3 Programming language: FORTRAN-95 Journal reference of previous version: M.V. Stoitsov, N. Schunck, M. Kortelainen, N. Michel, H. Nam, E. Olsen, J. Sarich, and S. Wild, Comput. Phys. Commun. 184 (2013). Does the new version supersede the previous one: Yes Summary of revisions: 1. the Gogny force in both particle&ndash;hole and particle&ndash;particle channels was implemented; 2. the nuclear collective inertia at the perturbative cranking approximation was implemented; 3. fission fragment charge, mass and deformations were implemented based on the determination of the position of the neck between nascent fragments; 4. the regularization method of zero-range pairing forces was implemented; 5. the localization functions of the HFB solution were implemented; 6. a MPI interface for large-scale mass table calculations was implemented. Nature of problem:HFBTHO is a physics computer code that is used to model the structure of the nucleus. It is an implementation of the energy density functional (EDF) approach to atomic nuclei, where the energy of the nucleus is obtained by integration over space of some phenomenological energy density, which is itself a functional of the neutron and proton intrinsic densities. In the present version of HFBTHO, the energy density derives either from the zero-range Skyrme or the finite-range Gogny effective two-body interaction between nucleons. Nuclear super-fluidity is treated at the Hartree&ndash;Fock&ndash;Bogolyubov (HFB) approximation. Constraints on the nuclear shape allows probing the potential energy surface of the nucleus as needed e.g., for the description of shape isomers or fission. The implementation of a local scale transformation of the single-particle basis in which the HFB solutions are expanded provide a tool to properly compute the structure of weakly-bound nuclei. Solution method: The program uses the axial Transformed Harmonic Oscillator (THO) single-particle basis to expand quasiparticle wave functions. It iteratively diagonalizes the Hartree&ndash;Fock&ndash;Bogolyubov Hamiltonian based on generalized Skyrme-like energy densities and zero-range pairing interactions or the finite-range Gogny force until a self-consistent solution is found. A previous version of the program was presented in M.V. Stoitsov, N. Schunck, M. Kortelainen, N. Michel, H. Nam, E. Olsen, J. Sarich, and S. Wild, Comput. Phys. Commun. 184 (2013) 1592&ndash;1604 with much of the formalism presented in the original paper M.V. Stoitsov, J. Dobaczewski, W. Nazarewicz, P. Ring, Comput. Phys. Commun. 167 (2005) 43&ndash;63. Additional comments: The user must have access to (i) the LAPACK subroutines DSYEEVR, DSYEVD, DSYTRF and DSYTRI, and their dependencies, which compute eigenvalues and eigenfunctions of real symmetric matrices, (ii) the LAPACK subroutines DGETRI and DGETRF, which invert arbitrary real matrices, and (iii) the BLAS routines DCOPY, DSCAL, DGEMM and DGEMV for double-precision linear algebra (or provide another set of subroutines that can perform such tasks). The BLAS and LAPACK subroutines can be obtained from the Netlib Repository at the University of Tennessee, Knoxville: http://netlib2.cs.utk.edu/.<br/> &copy; 2017 Elsevier B.V.","Perez, R. Navarro and Schunck, N. and Lasseri, R.-D. and Zhang, C. and Sarich, J.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"POINCARE CODE: A package of open-source implements for normalization and computer algebra reduction near equilibria of coupled ordinary differential equations","The Poincare&acute; code is a Maple project package that aims to gather significant computer algebra normal form (and subsequent reduction) methods for handling nonlinear ordinary differential equations. As a first version, a set of fourteen easy-to-use Maple commands is introduced for symbolic creation of (improved variants of Poincare&acute;'s) normal forms as well as their associated normalizing transformations. The software is the implementation by the authors of carefully studied and followed up selected normal form procedures from the literature, including some authors' contributions to the subject. As can be seen, joint-normal-form programs involving Lie-point symmetries are of special interest and are published in CPC Program Library for the first time, Hamiltonian variants being also very useful as they lead to encouraging results when applied, for example, to models from computational physics like He&acute;non-Heiles. &copy; 2013 Elsevier B.V. All rights reserved.<br/>","Mikram, J. and Zinoun, F. and El Abdllaoui, A.",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Data hiding based on overlapped pixels using hamming code","Most data hiding schemes change the least significant bits to conceal messages in the cover images. Matrix encoding scheme is a well known scheme in this field. The matrix encoding proposed by Crandall can be used in steganographic data hiding methods. Hamming codes are kinds of cover codes. &ldquo;Hamming + 1&rdquo; proposed by Zhang et al. is an improved version of matrix encoding steganography. The embedding efficiency of &ldquo;Hamming + 1&rdquo; is very high for data hiding, but the embedding rate is low. Our proposed &ldquo;Hamming + 3&rdquo; scheme has a slightly reduced embedding efficiency, but improve the embedding rate and image quality. &ldquo;Hamming + 3&rdquo; is applied to overlapped blocks, which are composed of 2<sup>k</sup>+3 pixels, where k=3. We therefore propose verifying the embedding rate during the embedding and extracting phases. Experimental results show that the reconstructed secret messages are the same as the original secret message, and the proposed scheme exhibits a good embedding rate compared to those of previous schemes.<br/> &copy; 2014, Springer Science+Business Media New York.","Kim, Cheonshik and Yang, Ching-Nung",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Heuristic expansion of feature mappings in evolving program families","Establishing explicit mappings between features and their implementation elements in code is one of the critical factors to maintain and evolve software systems successfully. This is especially important when developers have to evolve program families, which have evolved from one single core system to similar but different systems to accommodate various requirements from customers. Many techniques and tools have emerged to assist developers in the feature mapping activity. However, existing techniques and tools for feature mapping are limited as they operate on a single program version individually. Additionally, existing approaches are limited to recover features on demand, that is, developers have to run the tools for each family member version individually. In this paper, we propose a cohesive suite of five mapping heuristics addressing those two limitations. These heuristics explore the evolution history of the family members in order to expand feature mappings in evolving program families. The expansion refers to the action of automatically generating the feature mappings for each family member version by systematically considering its previous change history. The mapping expansion starts from seed mappings and continually tracks the features of the program family, thus eliminating the need of on demand algorithms. Additionally, we present the MapHist tool that provides support to the application of the proposed heuristics. We evaluate the accuracy of our heuristics through two evolving program families from our industrial partners.<br/> Copyright &copy; 2013 John Wiley & Sons, Ltd.","Nunes, Camila and Garcia, Alessandro and Lucena, Carlos and Lee, Jaejoon",2014,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"A new measure of code complexity during software evolution: 'A case study'","This paper first computes the Complexity increment by taking four complexity metrics WMC (CK), CMC (Li), CC (BS) and CCC (S&amp;B). The maintainability index of the successive version has been computed at the system level. The tracking of the number of classes added and deleted has also been obtained for the archaeology of successive versions. The understandability and the maintainability of software are then mapped with the trends of complexity increment, change in number of classes added and deleted and the Maintainability index. The complexity increments between successive versions give an indication towards the maturity level of software. These metrics are empirically evaluated with 38 versions of JFree Chart and nine versions of three live project data at the system level. &copy; 2014 SERSC.","Singh, Vinay and Bhattacherjee, Vandana",2014,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Learning better inspection optimization policies","Recent research has shown the value of social metrics for defect prediction. Yet many repositories lack the information required for a social analysis. So, what other means exist to infer how developers interact around their code? One option is static code metrics that have already demonstrated their usefulness in analyzing change in evolving software systems. But do they also help in defect prediction? To address this question we selected a set of static code metrics to determine what classes are most ""active"" (i.e., the classes where the developers spend much time interacting with each other's design and implementation decisions) in 33 open-source Java systems that lack details about individual developers. In particular, we assessed the merit of these activity-centric measures in the context of ""inspection optimization"" a technique that allows for reading the fewest lines of code in order to find the most defects. For the task of inspection optimization these activity measures perform as well as (usually, within 4%) a theoretical upper bound on the performance of any set of measures. As a result, we argue that activity-centric static code metrics are an excellent predictor for defects. &copy; 2012 World Scientific Publishing Company.<br/>","Lumpe, Markus and Vasa, Rajesh and Menzies, Tim and Rush, Rebecca and Turhan, Burak",2012,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Framework support for the efficient implementation of multi-version algorithms","Software Transactional Memory algorithms associate metadata with the memory locations accessed during a transaction&rsquo;s lifetime. This metadata may be stored in an external table and accessed by way of a function that maps the address of each memory location with the table entry that keeps its metadata (this is the out-place or external scheme); or alternatively may be stored adjacent to the associated memory cell by wrapping them together (the in-place scheme). In transactional memory multi-version algorithms, several versions of the same memory location may exist. The efficient implementation of these algorithms requires a one-to-one correspondence between each memory location and its list of past versions, which is stored as metadata. In this chapter we address the matter of the efficient implementation of multi-version algorithms in Java by proposing and evaluating a novel in-place metadata scheme for the Deuce framework. This new scheme is based in Java Bytecode transformation techniques and its use requires no changes to the application code. Experimentation indicates that multi-versioning STM algorithms implemented using our new in-place scheme are in average 6&times; faster than when implemented with the out-place scheme.<br/> &copy; Springer International Publishing Switzerland 2015.","Dias, Ricardo J. and Vale, Tiago M. and Lourenco, Joao M.",2015,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Rock physics modeling for waterflood simulation: A case history from the burgan field, Kuwait","We show with some simple code, a wide range of possible scenarios can be simulated and visualized. Based on our simulations, we conclude that a water flood in the Burgan and/or Wara formations should be visible as an amplitude change on 4D seismic with a water flood thickness of at least 15 feet.<br/>","Edwards, K. and Ebrahim, M. and Qassim, F. and Al-Asfour, S.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Introduction to SWAT+, A Completely Restructured Version of the Soil and Water Assessment Tool","SWAT+ is a completely restructured version of the Soil and Water Assessment Tool (SWAT) that was developed to face present and future challenges in water resources modeling and management and to meet the needs of the worldwide user community. It is expected to improve code development and maintenance; support data availability, analysis, and visualization; and enhance the model's capabilities in terms of the spatial representation of elements and processes within watersheds. The most important change is the implementation of landscape units and flow and pollutant routing across the landscape. Also, SWAT+ offers more flexibility than SWAT in defining management schedules, routing constituents, and connecting managed flow systems to the natural stream network. To test the basic hydrologic function of SWAT+, it was applied to the Little River Experimental Watershed (Georgia) without enhanced overland routing and compared with previous models. SWAT+ gave similar results and inaccuracies as these models did for streamflow and water balance. Taking full advantage of the new capabilities of SWAT+ regarding watershed discretization and landscape and river interactions is expected to improve simulations in future studies. While many capabilities of SWAT have already been enhanced in SWAT+ and new capabilities have been added, the model will continue to evolve in response to advancements in scientific knowledge and the demands of the growing worldwide user community. Editor's note: This paper is part of the featured series on SWAT Applications for Emerging Hydrologic and Water Quality Challenges. See the February 2017 issue for the introduction and background to the series.<br/> &copy; 2016 American Water Resources Association","Bieger, Katrin and Arnold, Jeffrey G. and Rathjens, Hendrik and White, Michael J. and Bosch, David D. and Allen, Peter M. and Volk, Martin and Srinivasan, Raghavan",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Verification and validation of EnergyPlus phase change material model for opaque wall assemblies","Phase change materials (PCMs) represent a technology that may reduce peak loads and HVAC energy consumption in buildings. A few building energy simulation programs have the capability to simulate PCMs, but their accuracy has not been completely tested. This study shows the procedure used to verify and validate the PCM model in EnergyPlus using a similar approach as dictated by ASHRAE Standard 140, which consists of analytical verification, comparative testing, and empirical validation. This process was valuable, as two bugs were identified and fixed in the PCM model, and version 7.1 of EnergyPlus will have a validated PCM model. Preliminary results using whole-building energy analysis show that careful analysis should be done when designing PCMs in homes, as their thermal performance depends on several variables such as PCM properties and location in the building envelope. &copy; 2012 Elsevier Ltd.","Tabares-Velasco, Paulo Cesar and Christensen, Craig and Bianchi, Marcus",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An ensemble-rich multi-aspect approach for robust style change detection: Notebook for PAN at CLEF-2018","We describe the winning system for the PAN@CLEF 2018 task on Style Change Detection. Given a document, the goal is to determine whether it contains style change. We present our supervised approach, which combines a TF.IDF representation of the documents with features specifically engineered for the task and which makes predictions using an ensemble of diverse models including SVM, Random Forest, AdaBoost, MLP and LightGBM. We further perform comparative analysis on the performance of the models on three different datasets, two of which we have developed for the task. Moreover, we release our code in order to enable further research.<br/>","Zlatkova, Dimitrina and Kopev, Daniel and Mitov, Kristiyan and Atanasov, Atanas and Hardalov, Momchil and Koychev, Ivan and Nakov, Preslav",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Displaying people with old addresses on a map","This paper proposes a method of converting old addresses to current addresses for geocoding, with the aim of displaying on a map people who have such old addresses. Existing geocoding services often fail to handle old addresses since the names of towns, cities, or prefectures can be different from those of current addresses. To solve this geocoding problem, we focus on postal codes, extracting them from Web search result snippets using the query &ldquo;prefecture name AND important place name AND postal code.&rdquo; The frequency of postal codes and the edit distance between the old address and the addresses obtained using the postal codes are used to judge the most suitable postal code and thus the corresponding current address. The effectiveness of the proposed method is evaluated in an experiment using a relative dataset. A prototype system was implemented in which users could display people using their birthdate and birthplace addresses on a map chronologically with an associated history chart.<br/> &copy; Springer International Publishing Switzerland 2015.","Zhang, Gang and Murakami, Harumi",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Experimental characterisation of sub-cooling in hydrated salt phase change materials","Phase change materials (PCMs) allow storage of large amounts of energy within a narrow temperature range via their latent heat. This is useful for applications where the outside environment swings above and below the nominal temperature range, enabling the design of passively regulated thermal systems. This short communication presents an experimental characterisation of two proprietary hydrated calcium chloride-based salt materials designed for maintaining temperatures of 25-30 &deg;C for building/enclosure temperature stability. It was found that materials' thermal performance is critically influenced by their rate of cooling. Using a T-history method, the experiments revealed that these salts undergo high specific enthalpy changes across a broad temperature range (e.g. up to 1 MJ/kg, which is 5-10 times their latent heat), but that up to 10 &deg;C of sub-cooling and long nucleation times are possible, depending on their rate of cooling. This communication reveals that careful operation is needed to ensure that these materials achieve control within the desired temperature range.<br/> &copy; 2015 Elsevier Ltd.","Taylor, Robert A. and Tsafnat, Naomi and Washer, Alex",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Spatio-temporal analysis of GPS tracks of CODE RED: MOBILE an experimental mobile scenario and location based training exercise","As part of an ongoing research project, geovisualisations of bushfires were delivered at GPS-determined locations to volunteer fire fighters from the Country Fire Authority's Macedon Ranges Group. The participants skill level ranged from basic wildfire firefighter trained through to captain of brigade. The location-based scenario training exercise is called CODE RED: MOBILE. Using information from the geovisualisations about a virtual bushfire at Hanging Rock, participants selected which houses would likely burn down after a wind change. They were free to take any path to reach the virtual houses, indicated by markers on the screen of an iPad New. They were asked to go to the virtual house location to observe the real landscape and to estimate where the virtual fire would go. Most participants took about an hour to complete the exercise. A GPS device kept track of where they went. A Fractal D score was assigned to participant's tracks using Vilis O. Nams' software: Fractal 5.20.0. Spatio-temporal analysis of the GPS tracks using ArcMap 10 and Geotime 5.3 found that participants undertook the exercise by following unusual tracks. Preliminary results showed that some of these participants, not following test procedure instructions closely, had sometimes undertaken more direct tracks, shown by low Fractal D scores. However, they were able to choose the correct houses assigned to visit. This type of analysis can assist in improving the design of mobile, location based exercises. It can also provide an additional means of assessing and improving firefighter performance. This paper will outline the background behind the exercise, specify the type of information that was sought and provides details of the results obtained through analysis.<br/>","Quinn, P.B. and Cartwright, W.E.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Round-Robin Test of Paraffin Phase-Change Material","A round-robin test between three institutes was performed on a paraffin phase-change material (PCM) in the context of the German quality association for phase-change materials. The aim of the quality association is to define quality and test specifications for PCMs and to award certificates for successfully tested materials. To ensure the reproducibility and comparability of the measurements performed at different institutes using different measuring methods, a round-robin test was performed. The sample was unknown. The four methods used by the three participating institutes in the round-robin test were differential scanning calorimetry, Calvet calorimetry and three-layer calorimetry. Additionally, T-history measurements were made. The aim of the measurements was the determination of the enthalpy as a function of temperature. The results achieved following defined test specifications are in excellent agreement.<br/> &copy; 2014, Springer Science+Business Media New York.","Vidi, S. and Mehling, H. and Hemberger, F. and Haussmann, Th. and Laube, A.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Developer recommendation with awareness of accuracy and cost","As the scale and complexity of software products increase, software maintenance on bug resolution has become a challenging work. In the process of software implementation, developers often use bug reports, source code and change history to help solve bugs. However, hundreds of bug reports are being submitted every day. It is time-consuming and effortless for developers to review all the bug reports. To facilitate the assignment of bug reports, existing developer recommendation systems typically recommend the developer who has the fullest potential. However, bug reports are highly varied; time that the developers may spend fixing them is also important. To address the problem of developer recommendation, we propose a developer recommendation system with awareness of accuracy and cost (DRAC). This recommendation system is based on modern portfolio theory by striking a balance between accuracy and cost (time). We evaluate our approach with experiments on data collected from Bugzilla1.<br/> Copyright &copy; 2016 by KSI Research Inc. and Knowledge Systems Institute Graduate School.","Liu, Jin and Tian, Yiqiuzi and Hong, Liang and Xu, Zhou",2016,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Development of mobile serious game for self-assessment as base for a game-editor for teachers","Self-assessment is an important tool to improve student performance. A good serious game (SG) would be a very strong self-assessment tool because of the additional motivational aspects. However, self-assessment tools have to fit the learning matters of a course by 100%, what general SG seldom do. The goal of this work is to build a simple SG editor each teacher can adjust to the learning goals of his course. Here we present the development of a game-editor for teachers for a puzzle-style game. By this editor teachers can edit their own course based game without sophisticated computer knowledge. The game mechanics meet the requirements of mobile- and micro-learning strategies. Furthermore, the game implements learning analytics for students as well as for teachers. The game engine and the editor are both based on standard Web technologies. The source code is maintained as an open source project to lower the barriers for further uptake.<br/> &copy; Springer International Publishing Switzerland 2016.","Herrler, Andreas and Grubert, Simon and Kajzer, Marko and Behrens, Sadie and Klamma, Ralf",2016,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Evolution of XSD documents and their variability during project life cycle: A preliminary study","During a software system life cycle, project modifications occur for different reasons. Regarding web services, communication contracts modifications are equally common, which induces the need for adaptation in every system node. To help reduce the contracts changing impact over software source code, it is necessary to understand how these contract changes occur. This paper presents a preliminary study on the evaluation of the change history of different open-source projects that defines XSD documents, specifying metrics for such files, extracting them by software repository mining and analyzing their evolution during the project life cycle. Based on the results, and considering that Web Service Definition Language (WSDL) contracts use XSD, a deeper study focused on web services projects only is further proposed to assess what exactly is changed at each contract revision, possibly revealing changing tendencies to support easy-to-adapt web service development.<br/> &copy; Springer International Publishing Switzerland 2016.","De Almeida, Diego Benincasa Fernandes Cavalcanti and Guerra, Eduardo Martins",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The road code: encouraging more efficient driving practices in New Zealand","Road transport contributes a significant amount towards New Zealand&rsquo;s carbon emissions, mostly from light vehicles. These emissions could be partly reduced by an increase in more efficient driving practices, and reductions of 10&ndash;20% of fuel are possible without increasing trip times significantly. This study was conducted to understand whether people knew how to drive efficiently, whether they actually ever drove in an efficient manner and what ways there could be to influence people to drive more efficiently. Focus groups were conducted across New Zealand in urban and rural areas with groups of students, young professionals, parents and older people in order to cover different lifestyles and environments. These focus groups covered a wide range of topics including knowledge and practices of efficient driving, learning to drive, infrastructure and aspirations. Our results show that most people reported knowing the things they could do to be more fuel-efficient, however, despite this knowledge, they very rarely engaged in these practices. When they did consider fuel efficiency, it was almost always linked to saving fuel costs and environmental aspects were not considered. There is a clear lack of connection between carbon emissions and driving when people are in their cars. Better messages could be presented to drivers linking their driving practices to carbon emissions and therefore climate change. There are a range of other options where more efficient practices and choices could be encouraged depending on context, the driver and their way of life.<br/> &copy; 2017, The Author(s).","Scott, Michelle Grace and Lawson, Rob",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Using LSTMs to model the java programming language","Recurrent neural networks (RNNs), specifically long-short term memory networks (LSTMs), can model natural language effectively. This research investigates the ability for these same LSTMs to perform next &ldquo;word&rdquo; prediction on the Java programming language. Java source code from four different repositories undergoes a transformation that preserves the logical structure of the source code and removes the code&rsquo;s various specificities such as variable names and literal values. Such datasets and an additional English language corpus are used to train and test standard LSTMs&rsquo; ability to predict the next element in a sequence. Results suggest that LSTMs can effectively model Java code achieving perplexities under 22 and accuracies above 0.47, which is an improvement over LSTM&rsquo;s performance on the English language which demonstrated a perplexity of 85 and an accuracy of 0.27. This research can have applicability in other areas such as syntactic template suggestion and automated bug patching.<br/> &copy; Springer International Publishing AG 2017.","Boldt, Brendon",2017,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Designing linear algebra algorithms by transformation: Mechanizing the expert developer","To implement dense linear algebra algorithms for distributed-memory computers, an expert applies knowledge of the domain, the target architecture, and how to parallelize common operations. This is often a rote process that becomes tedious for a large collection of algorithms. We have developed a way to encode this expert knowledge such that it can be applied by a system to generate mechanically the same (and sometimes better) highly-optimized code that an expert creates by hand. This paper illustrates how we have encoded a subset of this knowledge and how our system applies it and searches a space of generated implementations automatically. &copy; 2013 Springer-Verlag.<br/>","Marker, Bryan and Poulson, Jack and Batory, Don and Van De Geijn, Robert",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Gabor Convolutional Networks","Steerable properties dominate the design of traditional filters, e.g., Gabor filters, and endow features the capability of dealing with spatial transformations. However, such excellent properties have not been well explored in the popular deep convolutional neural networks (DCNNs). In this paper, we propose a new deep model, termed Gabor Convolutional Networks (GCNs or Gabor CNNs), which incorporates Gabor filters into DCNNs to enhance the resistance of deep learned features to the orientation and scale changes. By only manipulating the basic element of DCNNs based on Gabor filters, i.e., the convolution operator, GCNs can be easily implemented and are compatible with any popular deep learning architecture. Experimental results demonstrate the super capability of our algorithm in recognizing objects, where the scale and rotation changes occur frequently. The proposed GCNs have much fewer learnable network parameters, and thus is easier to train with an endtoend pipeline. The source code will be here 1.<br/> &copy; 2018 IEEE.","Luan, Shangzhen and Zhang, Baochang and Zhou, Siyue and Chen, Chen and Han, Jungong and Yang, Wankou and Liu, Jianzhuang",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Bypassing system callsbased intrusion detection systems","Machine learning augments today's intrusion detection system (IDS) capability to cope with unknown malware. However, if an attacker gains partial knowledge about the IDS' classifier, he can create a modified version of his malware, which can evade detection. In this article we present an IDS on the basis of various classifiers using system calls, executed by the inspected code as features. We then present a camouflage algorithm that is used to modify malicious code to be classified as benign, while preserving the code's functionality, for decision tree and random forest classifiers. We also present transformations to the classifier's input, to prevent this camouflage - and a modified camouflage algorithm that overcomes those transformations. Our research shows that it is not enough to provide a decision tree based classifier with a large training set to counter malware. One must also be aware of the possibility that the classifier would be fooled by a camouflage algorithm, and try to counter such an attempt with techniques such as input transformation or training set updates.<br/> Copyright &copy; 2016 John Wiley & Sons, Ltd.","Rosenberg, Ishai and Gudes, Ehud",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Evading system-calls based intrusion detection systems","Machine-learning augments today&rsquo;s IDS capability to cope with unknown malware. However, if an attacker gains partial knowledge about the IDS&rsquo;s classifier, he can create a modified version of his malware, which can evade detection. In this article we present an IDS based on various classifiers using system calls executed by the inspected code as features. We then present a camouflage algorithm that is used to modify malicious code to be classified as benign, while preserving the code&rsquo;s functionality, for decision tree and random forest classifiers. We also present transformations to the classifier&rsquo;s input, to prevent this camouflage - and a modified camouflage algorithm that overcomes those transformations. Our research shows that it is not enough to provide a decision tree based classifier with a large training set to counter malware. One must also be aware of the possibility that the classifier would be fooled by a camouflage algorithm, and try to counter such an attempt with techniques such as input transformation or training set updates.<br/> &copy; Springer International Publishing AG 2016.","Rosenberg, Ishai and Gudes, Ehud",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Attacking and defending dynamic analysis system-calls based IDS","Machine-learning augments today&rsquo;s IDS capability to cope with unknown malware. However, if an attacker gains partial knowledge about the IDS&rsquo;s classifier, he can create a modified version of his malware, which can evade detection. In this article we present an IDS based on various classifiers using system calls executed by the inspected code as features. We then present a camouflage algorithm that is used to modify malicious code to be classified as benign, while preserving the code&rsquo;s functionality, for decision tree and random forest classifiers. We also present transformations to the classifier&rsquo;s input, to prevent this camouflage - and a modified camouflage algorithm that overcomes those transformations. Our research shows that it is not enough to provide a decision tree based classifier with a large training set to counter malware. One must also be aware of the possibility that the classifier would be fooled by a camouflage algorithm, and try to counter such an attempt with techniques such as input transformation or training set updates.<br/> &copy; IFIP International Federation for Information Processing 2016.","Rosenberg, Ishai and Gudes, Ehud",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Automation of Upgrade Process for Enterprise Resource Planning Systems","This paper presents a framework for semi-automatic process of enterprise resource planning (ERP) system upgrade. We suggest to change currently accepted practice of manual upgrade process when domain expert-programmer works through all localizations and transforms them manually to the new version of ERP system. The core idea for this framework is to induce the software code transformation patterns from completed upgrade projects and then to refine these patterns by using knowledge of ERP upgrade expert. These patterns lets us to increase productivity of upgrade process by improving automatic code alignment and annotation and by providing code transformation to the new version of ERP system. The price for these improvements is a requirement for upgrade expert to move from traditional 4/GL ERP programming language to stochastic meta-programming language which is used to describe code alignment and code transformation patterns. &copy; Springer-Verlag Berlin Heidelberg 2013.<br/>","Laukaitis, Algirdas",2013,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Efficient code generation for automatic parallelization and optimization","Graphical Design Tool for Parallel ProgramsSupercompilers look for the best execution order of the statement instances in the most compute intensive kernels. It has been extensively shown that the polyhedral model provides convenient abstractions to find and perform the useful program transformations. Nevertheless, the current polyhedral code generation algorithms lack for flexibility by adressing mainly unimodular or at least invertible transformation functions. Moreover, their complexity is challenging for large problems (with many statements). In this paper, we discuss a general transformation framework able to deal with non-unimodular, non-invertible functions. A completed and improved version of one of the best algorithms known so far is presented to actually perform the code generation. Experimental evidence proves the ability of our framework to handle real-life problems.<br/>","Bastoul, Cedric",2015,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Annotation-free and one-shot learning for instance segmentation of homogeneous object clusters","We propose a novel approach for instance segmentation given an image of homogeneous object cluster (HOC). Our learning approach is one-shot because a single video of an object instance is captured and it requires no human annotation. Our intuition is that images of homogeneous objects can be effectively synthesized based on structure and illumination priors derived from real images. A novel solver is proposed that iteratively maximizes our structured likelihood to generate realistic images of HOC. Illumination transformation scheme is applied to make the real and synthetic images share the same illumination condition. Extensive experiments and comparisons are performed to verify our method. We build a dataset consisting of pixel-level annotated images of HOC. The dataset and code will be released.<br/> &copy; 2018 International Joint Conferences on Artificial Intelligence. All right reserved.","Wu, Zheng and Chang, Ruiheng and Ma, Jiaxu and Lu, Cewu and Tang, Chi Keung",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Data change analysis based on function call path","In the software life cycle, software version changes may make the normal function of the original emerge questions, so the analysis of the affected domain data flow is of great significance for the localization of software defects. In this paper, the analysis method is based on the analysis of the regression test of the data change. First of all obtain the data changed point from comparing source code and the changed code, then static analyze the changed code to get the statements related to the change, and obtain the data flow path where the change data include the statement block tree according to the data change analysis and syntax analysis. Finally, the data change path is generated by combining the function call path, and then obtain the data change influence domain. The experiment results show that based on the modification of a function call trace data analysis is correct and ensure the integrity of the test, and this method is helpful to determine more test cases, improves the function of the test coverage, and provides help for developers to quickly deal with the software defects in the regression test.<br/> &copy; 2018, &copy; 2018 Informa UK Limited, trading as Taylor & Francis Group.","Yong, Cao and Yongmin, Mu and Meie, Shen",2018,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Impact of gamification on code review process - An experimental study","Researchers have supported the idea of gamification to enhance students&rsquo; interest in activities like code reviews, change management, knowledge management, issue tracking, etc. which might otherwise be repetitive and monotonous. We performed an experimental study consisting of nearly 180+ participants to measure the impact of gamification on code review process using 5 different code review tools, including one gamified code review instance from our extensible architectural framework. We assess the impact of gamification based on the code smells and bugs identified in a gamified and non-gamified environment as per code inspection report. Further, measurement and comparison of the quantity and usefulness of code review comments was done using machine learning techniques.<br/> &copy; 2017 ACM.","Khandelwal, Shivam and Sripada, Sai Krishna and Raghu Reddy, Y.",2017,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Information transformation and automated reasoning for automated compliance checking in construction","This paper presents a new approach for automated compliance checking in the construction domain. The approach utilizes semantic modeling, semantic Natural Language Processing (NLP) techniques (including text classification and information extraction), and logic reasoning to facilitate automated textual regulatory document analysis and processing for extracting requirements from these documents and formalizing these requirements in a computer-processable format. The approach involves developing a set of algorithms and combining them into one computational platform: (1) semantic machine-learning-based algorithms for text classification (TC); (2) hybrid syntactic-semantic rule-based algorithms for information extraction (IE); (3) semantic rule-based algorithms for information transformation (ITr); and (4) logic-based algorithms for compliance reasoning (CR). This paper focuses on presenting our algorithms for ITr. A semantic, logic-based representation for construction regulatory requirements is described. Semantic mapping rules and conflict resolution rules for transforming the extracted information into the representation are discussed. Our combined TC, IE and ITr algorithms were tested in extracting and formalizing quantitative requirements in the 2006 International Building Code, achieving 96% and 92% precision and recall, respectively. &copy; 2013 American Society of Civil Engineers.<br/>","Zhang, J. and El-Gohary, N.M.",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR8, CR10"
"Machine learning using R","Examine the latest technological advancements in building a scalable machine learning model with Big Data using R. This book shows you how to work with a machine learning algorithm and use it to build a ML model from raw data. All practical demonstrations will be explored in R, a powerful programming language and software environment for statistical computing and graphics. The various packages and methods available in R will be used to explain the topics. For every machine learning algorithm covered in this book, a 3-D approach of theory, case-study and practice will be given. And where appropriate, the mathematics will be explained through visualization in R. All the images are available in color and hi-res as part of the code download. This new paradigm of teaching machine learning will bring about a radical change in perception for many of those who think this subject is difficult to learn. Though theory sometimes looks difficult, especially when there is heavy mathematics involved, the seamless flow from the theoretical aspects to example-driven learning provided in this book makes it easy for someone to connect the dots.. What You&rsquo;ll Learn Use the model building process flow Apply theoretical aspects of machine learning Review industry-based cae studies Understand ML algorithms using R Build machine learning models using Apache Hadoop and Spark Who This Book is For Data scientists, data science professionals and researchers in academia who want to understand the nuances of machine learning approaches/algorithms along with ways to see them in practice using R. The book will also benefit the readers who want to understand the technology behind implementing a scalable machine learning model using Apache Hadoop, Hive, Pig and Spark.<br/> &copy; 2017 Karthik Ramasubramanian and Abhishek Singh.","Ramasubramanian, Karthik and Singh, Abhishek",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Deep auto-encoder based on supervised learning for damaged face reconstruction","Based on the reconstruction idea of auto-encoder (AE) and image reconstruction, we present a new idea that the classical auto-encoder can be polished up by supervised learning. We also present a novel supervised deep learning framework for damaged face reconstruction after analyzing the deep model structure. The proposed model is unlike the classical auto-encoder which is unsupervised learning. In this paper, the deep supervised auto-encoder model is illustrated, which has a set of &ldquo;progressive&rdquo; and &ldquo;interrelated&rdquo; learning strategies by multiple groups of supervised single-layer AE. In this structure, we define a Deep Supervised Network with the supervised auto-encoder which is trained to extract characteristic features from damaged images and reconstruct the corresponding similar facial images, and it improves the ability to express the feature code. Extensive experiment on AR database demonstrates that the proposed method can significantly improve the smoothness of the damaged face reconstruction under enormous illumination, expression change. Experiments show that the proposed method has good contribution and adaptability to the damaged face reconstruction.<br/> &copy; Springer Nature Singapore Pte Ltd. 2018.","Rui, Ting and Zhang, Sai and Zou, Junhua and Zhou, You and Tang, Jian",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Deep Reinforcement Learning for Surgical Gesture Segmentation and Classification","Recognition of surgical gesture is crucial for surgical skill assessment and efficient surgery training. Prior works on this task are based on either variant graphical models such as HMMs and CRFs, or deep learning models such as Recurrent Neural Networks and Temporal Convolutional Networks. Most of the current approaches usually suffer from over-segmentation and therefore low segment-level edit scores. In contrast, we present an essentially different methodology by modeling the task as a sequential decision-making process. An intelligent agent is trained using reinforcement learning with hierarchical features from a deep model. Temporal consistency is integrated into our action design and reward mechanism to reduce over-segmentation errors. Experiments on JIGSAWS dataset demonstrate that the proposed method performs better than state-of-the-art methods in terms of the edit score and on par in frame-wise accuracy. Our code will be released later.<br/> &copy; 2018, Springer Nature Switzerland AG.","Liu, Daochang and Jiang, Tingting",2018,"[""Engineering Village""]","Rejeitado: CR9, CR9","Rejeitado: CR9"
"Towards a blended learning using mobile devices, podcasts and QR codes in Algeria","Higher education in Algeria has witnessed significant reforms in its educational system with a growing number of students from year to year due to its young population and a dynamic transition in the integration of Information and Communication Technologies (ICTs). Algeria is gradually advancing in terms of telecoms and Internet. The fixed network is difficult to access. With mobile operators, Algeria is distinguished through the use of Mobile. In Algeria the mobile penetration rate stands at over 111% and 21% with 3G. Internet access remains inaccessible to our students. But almost all students have access to mobile technology. Indeed, learners now want to learn &ldquo;on the move&rdquo;. They are nomadic learners who learn in faculty, restaurant, library, before sleeping, around a coffee. But they also learn in communities (social networks) i.e they exchange them with unprecedented ease of information solutions to the problems and mutually explain what the professor said. With this Internet generation, we have to change our methods of teaching and learning to think fast and efficiently, with a minimum of organizational, logistical and above all loss of time. Mobile technology is increasingly being used to support blended learning. The satisfactory results of our previous research show that the use of mobile technology could enhance accessibility and communication in a blended learning course. We discuss the emerging trends that allow more involved learners, such as e-learning, m-learning, blended learning, the use of podcasts and using QR codes in Learning Management System (LMS) at the National Institute of Telecommunications and ICT (INTTIC). QR Code is still relatively new and still in its infancy in education. The use of QR codes in INTTIC-LMS can be placed in the context of mobile learning.<br/> &copy; 2017 Taylor & Francis Group, London.","Ghizlene, Soulimane and Belkacem, Kouninef and Mohamed, Djelti",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Wikipedia miner engine: A re-usable e-learning service based on a virtual MVC design pattern","E-Learning platforms are evolving from monolithic applications with a rigid structure that did not allowed for the exchange of tools or components to applications incorporating service orientation concepts as well as facilitating the dynamic discovery and assembling of e-learning services. Accordingly, the usage of support materials to provide additional guidance to students facilitates the comprehension of learning tasks. Wikipedia is one of the richest sources of human knowledge, encompassing a vast range of topics of all kinds of information, and content, which is in constant change due to its collaborative dynamic nature. The Wikipedia Miner provides a code that can parse a given document identifying main topics and link them to corresponding articles or short definitions from the Wikipedia content. In this paper, we discuss the realization of a reusable Wikipedia Miner service for the e-Learning Computational Cloud (eLC2) Platform designed with the J2EE technology and Service-Oriented (V-MVC) model excluding a direct link between the Model and the View. This allows enhancing the Controller as a middleware, removing the dependency and acting as a single point of contact. In the V-MVC design pattern, the Controller is modeled by the compound design pattern of the Enterprise Service Bus (ESB) supporting higher privacy of the business logic and higher re-usability Architecture standards. The eLC2 is also based on an original Virtual Model-View-Controller of application components. In this framework, Wikipedia Miner services were prototyped as an Application Engine that wraps the logic of the Wikipedia Miner API in order to re-use it for different types of applications. Particularly, we are focusing on two applications in order to demonstrate the usability of the proposed approach. The first application is the WikiGloss tool, which is based on a glossing approach to help learners of English-as-second-language with an extensive reading task. The second application is an Intelligent Hints service for a Task Management Environment which provides explanatory links from relevant Wikipedia articles related to topics of the e-Learning task. This allows re-use of the same problems in different task type modes such as lectures, exercises, and quizzes. &copy; 2012 The authors and IOS Press. All rights reserved.<br/>","Cortez, Ruth and Vazhenin, Alexander and Brine, John",2012,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"How to study programming on mobile touch devices - Interactive Python code exercises","Scaffolded learning tasks where programs are constructed from predefined code fragments by dragging and dropping them (i.e. Parsons problems) are well suited to mobile touch devices, but quite limited in their applicability. They do not adequately cater for different approaches to constructing a program. After studying solutions to automatically assessed programming exercises, we found out that many different solutions are composed of a relatively small set of mutually similar code lines. Thus, they can be constructed by using the drag-and-drop approach if only it was possible to edit some small parts of the predefined fragments. Based on this, we have designed and implemented a new exercise type for mobile devices that builds on Parsons problems and falls somewhere between their strict scaffolding and full-blown coding exercises. In these exercises, we can gradually fade the scaffolding and allow programs to be constructed more freely so as not to restrict thinking and limit creativity too much while still making sure we are able to deploy them to small-screen mobile devices. In addition to the new concept and the related implementation, we discuss other possibilities of how programming could be practiced on mobile devices. &copy; 2013 ACM.<br/>","Ihantola, Petri and Helminen, Juha and Karavirta, Ville",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Good change and bad change: An analysis perspective on software evolution","Software does change, and should change. Traditional industrial software systems often evolve over long periods of time with each new version forming a discreet milestone, while some new software systems involve constant adaptation to situations in the environment and therefore evolve continually. While necessary, software change can also be devastating, making the system difficult to change and maintain further. We believe that one promising way to manage and control change is to view an evolving system as a software product line where each version of the software is a product. Key to any successful software product line approach is a software architecture that supports variability management. Tools that can identify commonalities and differences among various releases are essential in collecting and managing the information on changed, added and deleted components. Equally important are tools that allow the architect to analyse the current status of the product line as well as its products from various perspectives, and to be able to detect and remove architectural violations that threaten the variability points and built-in flexibility. In this paper, we describe our current research on defining such a process and supporting tools for software evolution management based on product line concepts and apply it in a case study to a software testbed called TSAFE. We describe how we reverse engineer the actual architecture from the source code and how we develop new target architectures based on the reverse engineered one and the expected changes. We then described how we analyse the actual change across different implementations and visualize where the change actually occurred. We then describe how we determine if a particular implementation match the target architecture. The conclusion is that we have found that both these analysis techniques are particularly useful for analysing software evolution and complement each other.<br/> &copy; Springer International Publishing AG 2016.","Lindvall, Mikael and Becker, Martin and Tenev, Vasil and Duszynski, Slawomir and Hinchey, Mike",2016,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Improving automatic centralization by version separation","With today's importance of distributed applications, their verification and analysis are still challenging. They involve large combinational states, interactive network communications between peers, and concurrency. Although there are some dynamic analysis tools for analyzing the runtime behavior of a single-process application, they do not provide methods to analyze distributed applications as a whole, where multiple processes run simultaneously. Centralization is a general solution which transforms multi-process applications into a single-process one that can be directly analyzed by existing tools. In this paper, we improve the accuracy of centralization. Moreover, we extend it as a general framework for analyzing distributed applications with multiple versions. First, we formalize the version conflict problem and present a simple solution, and further propose an optimized solution to resolving class version conflicts during centralization. Our techniques enable sharing common code whenever possible while keeping the version space of each component application separate. Centralization issues like startup semantics and static field transformation are improved and discussed. We implement and apply our centralization tool to some network benchmarks. Experiments, where existing tools are used on the centralized application, prove the usefulness of our automatic centralization tool, showing that centralization enables these tools to analyze distributed applications with multiple versions. &copy; 2014 Information Processing Society of Japan.<br/>","Ma, Lei and Artho, Cyrille and Sato, Hiroyuki",2014,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Which method-stereotype changes are indicators of code smells?","A study of how method roles evolve during the lifetime of a software system is presented. Evolution is examined by analyzing when the stereotype of a method changes. Stereotypes provide a high-level categorization of a method's behavior and role, and also provide insight into how a method interacts with its environment and carries out tasks. The study covers 50 open-source systems and 6 closed-source systems. Results show that method behavior with respect to stereotype is highly stable and constant over time. Overall, out of all the history examined, only about 10% of changes to methods result in a change in their stereotype. Examples of methods that change stereotype are further examined. A select number of these types of changes are indicators of code smells.<br/> &copy; 2018 IEEE.","Decker, Michael J. and Newman, Christian D. and Dragan, Natalia and Collard, Michael L. and Maletic, Jonathan I. and Kraft, Nicholas A.",2018,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Machine Learning: A Bayesian and Optimization Perspective","This tutorial text gives a unifying perspective on machine learning by covering both probabilistic and deterministic approaches -which are based on optimization techniques - together with the Bayesian inference approach, whose essence lies in the use of a hierarchy of probabilistic models. The book presents the major machine learning methods as they have been developed in different disciplines, such as statistics, statistical and adaptive signal processing and computer science. Focusing on the physical reasoning behind the mathematics, all the various methods and techniques are explained in depth, supported by examples and problems, giving an invaluable resource to the student and researcher for understanding and applying machine learning concepts. The book builds carefully from the basic classical methods to the most recent trends, with chapters written to be as self-contained as possible, making the text suitable for different courses: pattern recognition, statistical/adaptive signal processing, statistical/Bayesian learning, as well as short courses on sparse modeling, deep learning, and probabilistic graphical models. &bull; All major classical techniques: Mean/Least-Squares regression and filtering, Kalman filtering, stochastic approximation and online learning, Bayesian classification, decision trees, logistic regression and boosting methods. &bull; The latest trends: Sparsity, convex analysis and optimization, online distributed algorithms, learning in RKH spaces, Bayesian inference, graphical and hidden Markov models, particle filtering, deep learning, dictionary learning and latent variables modeling. &bull; Case studies - protein folding prediction, optical character recognition, text authorship identification, fMRI data analysis, change point detection, hyperspectral image unmixing, target localization, channel equalization and echo cancellation, show how the theory can be applied. &bull; MATLAB code for all the main algorithms are available on an accompanying website, enabling the reader to experiment with the code.<br/> &copy; 2015 Elsevier Ltd. All rights reserved.","Theodoridis, Sergios",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical predictions for the thermal history, microstructure and hardness distributions at the HAZ during welding of low alloy steels","A phenomenological model to predict the multiphase diffusional decomposition of the austenite in low-alloy hypoeutectoid steels was adapted for welding conditions. The kinetics of phase transformations coupled with the heat transfer phenomena was numerically implemented using the Finite Volume Method (FVM) in a computational code. The model was applied to simulate the welding of a commercial type of low-alloy hypoeutectoid steel, making it possible to track the phase formations and to predict the volume fractions of ferrite, pearlite and bainite at the heat-affected zone (HAZ). The volume fraction of martensite was calculated using a novel kinetic model based on the optimization of the well-known Koistinen-Marburger model. Results were confronted with the predictions provided by the continuous cooling transformation (CCT) diagram for the investigated steel, allowing the use of the proposed methodology for the microstructure and hardness predictions at the HAZ of low-alloy hypoeutectoid steels.<br/>","Xavier, Carlos Roberto and Delgado, Horacio Guimaraes and De Castro, Jose Adilson and Ferreira, Alexandre Furtado",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Learning in the limit: A mutational and adaptive approach","The purpose of this work is to show the strong connection between learning in the limit and the second-order adaptive automaton. The connection is established using the mutating programs approach, in which any hypothesis can be used to start a learning process, and produces a correct final model following a step-by-step transformation of that hypothesis by a second-order adaptive automaton. Second-order adaptive automaton learner will be proved to acts as a learning in the limit one. &copy; 2013 Springer-Verlag Berlin Heidelberg.<br/>","Inojosa Da Silva Filho, Reginaldo and De Azevedo Da Rocha, Ricardo Luis and Gracini Guiraldelli, Ricardo Henrique",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Experiential Learning of Robotics Fundamentals Based on a Case Study of Robot-Assisted Stereotactic Neurosurgery","Robotics has been playing an important role in modern surgery, especially in procedures that require extreme precision, such as neurosurgery. This paper addresses the challenge of teaching robotics to undergraduate engineering students, through an experiential learning project of robotics fundamentals based on a case study of robot-assisted stereotactic neurosurgery. The project was integrated into the curriculum of a Biomedical Engineering and Electrical and Computer Engineering program, but can also be integrated in related courses. First, students are given a presentation on the planning and execution of a stereotactic neurosurgery procedure, with special attention being paid to the concepts involved, namely spatial transformations, kinematics, and trajectory planning. Students are then taught to use a robotics simulation tool for robot-assisted stereotactic neurosurgery. They are shown how this can be used as a specialized control application, providing direct feedback on the robot's motion in a neurosurgery scenario. They are then required to select a robotic manipulator, and to develop and implement its control code to make it perform as a robot assistant in this surgical procedure. Project efficacy was evaluated through student self-report data (with dedicated anonymous surveys) and through the impact on academic and pedagogical results (by means of statistical inference). The results of the student surveys show that the robotics simulator for stereotactic neurosurgery is well suited to its role as an experiential learning tool since it enhances the understanding and application of several robotics concepts in an appealing manner. The positive impact of the project learning experience is supported by a comparison to earlier years of student grades, pass rates, and feedback from an institutional survey.<br/> &copy; 2015 IEEE.","Faria, Carlos and Vale, Carolina and Machado, Toni and Erlhagen, Wolfram and Rito, Manuel and Monteiro, Sergio and Bicho, Estela",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Error-correcting output codes based on feature space transformation","The independency between each dichotomizer trained by coding matrix's bi-partition is the key to using errorcorrecting output codes (ECOC) to solve multiclass problems. Therefore, an error-correcting output codes method based on feature space transformation (FST) is proposed. Inspired by the ensemble learning theory, a third feature space dimension is introduced into the coding matrix. Then, different subspaces are obtained by feature space transformation based on different positive and negative subclasses, so that the diversity between different binary classifiers are promoted to make the classification performance better. The experiment results based on UCI datasets show that the codes based on FST are better than the original codes. Besides, the proposed method can be applied to any kind of coding matrix, and provides new thought to large dataset for its quick training time and simplicity.<br/> &copy;, 2015, Northeast University. All right reserved.","Lei, Lei and Wang, Xiao-Dan and Luo, Xi and Song, Ya-Fei and Xue, Ai-Jun",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Automatic calculation of process metrics and their bug prediction capabilities","Identifying fault-prone code parts is useful for the developers to help re-duce the time required for locating bugs. It is usually done by characterizing the already known bugs with certain kinds of metrics and building a predictive model from the data. For the characterization of bugs, software product and process metrics are the most popular ones. The calculation of product metrics is supported by many free and commercial software products. However, tools that are capable of computing process metrics are quite rare. In this study, we present a method of computing software process metrics in a graph database. We describe the schema of the database created and we present a way to readily get the process metrics from it. With this technique, process metrics can be calculated at the file, class and method levels. We used GitHub as the source of the change history and we selected 5 open-source Java projects for processing. To retrieve positional information about the classes and methods, we used SourceMeter, a static source code analyzer tool. We used Neo4j as the graph database engine, and its query language-cypher-to get the process metrics. We published the tools we created as open-source projects on GitHub. To demonstrate the utility of our tools, we selected 25 release versions of the 5 Java projects and calculated the process metrics for all of the source code elements (files, classes and methods) in these versions. Using our previous published bug database, we built bug databases for the selected projects that contain the computed process metrics and the corresponding bug numbers for files and classes. (We published these databases as an online appendix.) Then we applied 13 machine learning algorithms on the database we created to find out if it is feasible for bug prediction purposes. We achieved F-measure values on average of around 0.7 at the class level, and slightly better values of between 0.7 and 0.75 at the file level. The best performing algorithm was the RandomForest method for both cases.<br/>","Gyimesi, Peter",2017,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Robust Traffic Vehicle Lane Change Maneuver Recognition","The ability to recognize traffic vehicles' lane change maneuver lays the foundation for predicting their long-term trajectories in real-time, which is a key component for Advanced Driver Assistance Systems (ADAS) and autonomous automobiles. Learning-based approach is powerful and efficient, such approach has been used to solve maneuver recognition problems of the ego vehicles on conventional researches. However, since the parameters and driving states of the traffic vehicles are hardly observed by exteroceptive sensors, the performance of traditional methods cannot be guaranteed. In this paper, a novel approach using multi-class probability estimates and Bayesian inference model is proposed for traffic vehicle lane change maneuver recognition. The multi-class recognition problem is first decomposed into three binary problems under error correcting output codes (ECOC) framework. With probability estimates from the three binary classifiers, multi-class probability estimates are obtained through paired team comparisons. A sequence of the multi-class probability estimates are then fed into the Bayesian inference model. The Bayesian inference model views the input as sample of a random variable, and the output of the Bayesian inference model is used for the final recognition. A data set which is collected from a real-time driving simulation platform is used for the training of the binary classifiers. Typical samples are used to evaluate the performance of the proposed approach. The experimental results have demonstrated the improvement of robustness when using the proposed approach, and the approach is able to recognize lane change maneuver of the traffic vehicle with an average prediction horizon of 1.51 seconds.<br/> Copyright &copy; 2017 SAE International.","Sun, Hao and Deng, Weiwen and Su, Chen and Wu, Jian",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"VuRLE: Automatic vulnerability detection and repair by learning from examples","Vulnerability becomes a major threat to the security of many systems. Attackers can steal private information and perform harmful actions by exploiting unpatched vulnerabilities. Vulnerabilities often remain undetected for a long time as they may not affect typical systems&rsquo; functionalities. Furthermore, it is often difficult for a developer to fix a vulnerability correctly if he/she is not a security expert. To assist developers to deal with multiple types of vulnerabilities, we propose a new tool, called VuRLE, for automatic detection and repair of vulnerabilities. VuRLE (1) learns transformative edits and their contexts (i.e., code characterizing edit locations) from examples of vulnerable codes and their corresponding repaired codes; (2) clusters similar transformative edits; (3) extracts edit patterns and context patterns to create several repair templates for each cluster. VuRLE uses the context patterns to detect vulnerabilities, and customizes the corresponding edit patterns to repair them. We evaluate VuRLE on 279 vulnerabilities from 48 real-world applications. Under 10-fold cross validation, we compare VuRLE with another automatic repair tool, LASE. Our experiment shows that VuRLE successfully detects 183 out of 279 vulnerabilities, and repairs 101 of them, while LASE can only detect 58 vulnerabilities and repair 21 of them.<br/> &copy; 2017, Springer International Publishing AG.","Ma, Siqi and Thung, Ferdian and Lo, David and Sun, Cong and Deng, Robert H.",2017,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR9"
"Heat storage in direct-contact heat exchanger with phase change material","This paper describes the development and performance of a direct-contact heat exchanger using erythritol (melting point: 391 K) as a phase change material (PCM) and a heat transfer oil (HTO) for accelerating heat storage. A vertical cylinder with 200-mm inner diameter and 1000-mm height was used as the heat storage unit (HSU). A nozzle facing vertically downward was placed at the bottom of the HSU. We examined the effects of flowrate and inlet temperature of the HTO using three characteristic parameters of heat storage - difference between inlet and outlet HTO temperatures, temperature effectiveness, and heat storage rate. The temperature history of latent heat storage (LHS) showed three stages: sensible heat of solid PCM, latent heat of PCM, and sensible heat of liquid PCM. Further, the operating mechanism of the DCHEX was proposed to explain the results. The average heat storage rate during LHS was proportional to the increase in flowrate and inlet temperature of HTO. Thus, latent heat can be rapidly stored under large HTO flowrate and high inlet temperature in the DCHEX. &copy; 2012 Elsevier Ltd. All rights reserved.<br/>","Nomura, Takahiro and Tsubota, Masakatsu and Oya, Teppei and Okinaka, Noriyuki and Akiyama, Tomohiro",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Evolution of model clones in simulink","A growing and important area of Model-Based Development (MBD) is model evolution. Despite this, very little research on the evolution of Simulink models has been conducted. This is in contrast to the notable amount of research on UML models, which differ significantly from Simulink. Code clones and their evolution across system versions have been used to learn about source-code evolution. We postulate that the same idea can be applied to model clones and model evolution. In this paper, we explore this notion and apply it to Simulink models. We detect model clones in successive versions of MBD projects and, with a new tool, track the evolution of model clones with respect to their containing clone classes. When there is a change in classification of a model-clone, we investigate what specifically evolved in the model to cause this classification change.<br/> &copy; 2013 for the individual papers by the papers' authors.","Stephan, Matthew and Alalfi, Manar H. and Cordy, James R. and Stevenson, Andrew",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Phase Change Memory","In the late 1960s, Stanford Ovshinsky's (1922-2012) discovery of switching and phase change phenomena in chalcogenide materials seeded new possibilities in data storageunderstanding/application. Concomitantly, innovation in the materials and solid-state memory device research has led to phase change memory (PCM) as one of the potential candidates for future nonvolatile memory technology. PCM has the potential to combine DRAM-like features such as bit alteration, fast read and write, and good endurance and flash-like features such as nonvolatility using a simple device structure. Thus, introduction of PCM in the memory hierarchy would enable a seamless and versatile data exchange between the processor and storage. In this chapter, we explore important PCM materials and device learning in recent years, with a focus on how fundamental physics interact with device properties and the device scaling potential of PCM. We cover basic device operation, properties of the phase change material, potential for scaling to nanoscale dimensions, high-density memory structures, and promising applications of PCM.<br/> &copy; 2015 John Wiley and Sons Ltd.","Jeyasingh, Rakesh and Ahn, Ethan C. and Burc Eryilmaz, S. and Fong, Scott and Philip Wong, H.-S.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A model-driven framework to enhance the consistency of logical integrity constraints: Introducing integrity regression testing","Although the importance of models continuously grows in software development, common development approaches are less able to integrate the automatic management of model integrity into the development process. These critically important constraints may ensure the coherence of models in the evolution process to prevent manipulations that could violate defined constraints on a model. This paper proposes an integrity framework in the context of model-driven architecture to achieve sufficient structural code coverage at a higher program representation level than machine code. Our framework offers to propagate the modifications from a platform-independent specification to the corresponding test template model while keeping the consistency and integrity constraints after system evolution. To examine the efficiency of the proposed framework, a quantitative analysis plan is evaluated based on two experimental case studies. In addition, we propose coverage criteria for integrity regression testing (IRT), derived from logic coverage criteria that apply different conceptual levels of testing for the formulation of integrity requirements. The defined criteria for IRT reduce the inherent complexity and cost of verifying complex design changes in regression testing while keeping the fault detection capability with respect to the changes. The framework aims to keep pace with IRT in a formal way. The framework can solve a number of restricted outlooks in model integrity and some limiting factors of incremental maintenance and retesting. The framework satisfies several valuable quality attributes in software testing, such as safety percentage, precision, abstract fault detection performance measurable coverage level, and generality.<br/> &copy; 2018 John Wiley & Sons, Ltd.","Nooraei Abadeh, Maryam and Ajoudanian, Shohreh",2018,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Automatic detection of significant updates in regulatory documents","Regulations and legislations are regularly updated, which significantly burdens up the lawyers and compliance officers with a firehose of changes. However, not all changes are significant, and only a percentage of them are of legal importance. This percentage can certainly vary in different types of regulations. This paper focuses on automatic detection or ranking of meaningful legal changes, and presents a preliminary approach based on machine learning for the same, in the domain of Internal Revenue Code (IRC) related regulatory documents. Such system would provide the users with a means to quickly identify significant legal changes.<br/> &copy; 2017 The authors and IOS Press.","Asooja, Kartik and Foghlu, Oscar O. and Domhnaill, Breiffni O. and Marchin, George and McGrath, Sean",2017,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"An empirical study of software change classification with imbalance data-handling methods","Bug prediction in software code changes can help developers to find out and fix bugs immediately when they are introduced, thus to improve the effectiveness and validity of bug fixing. In data mining, this problem can be regarded as a change classification task. However, one of its key characteristics, ie, class-imbalance, holds back the performance of standard classification methods. In this paper, we consider a quantity of imbalance data-handling methods and extract a more comprehensive groups of change features, aiming to achieve better change classification performance. Two different types of imbalance data-handling methods, namely, resampling and ensemble learning methods, are employed. Especially, we explore the performance of their combination. To compare the performance of different imbalance data-handling methods, an experiment with 10 open source projects is conducted. Four classification methods, including J48, Na&iuml;ve Bayes, SMO, and Random Forest, are used as standard classifiers and as the base classifiers, respectively. Moreover, contribution of different groups of change features are evaluated. Experimental results show that imbalance data-handling methods can improve the performance of change classification and the combination methods, which take advantage of both ensemble learning and resampling, perform better than using ensemble learning methods or resampling methods individually. Of the studied imbalance data-handling methods, the combination of Bagging and random undersampling with J48 as the base classifier yields out better prediction results than those achieved by other methods. Additionally, of the collected change features, text vector features accounts for a larger proportion than others.<br/> &copy; 2018 John Wiley & Sons, Ltd.","Zhu, Xiaoyan and Niu, Binbin and Whitehead, E. James and Sun, Zhongbin",2018,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Finite element implementation of multi-pass fillet weld with phase changes","First, in this paper, a brief review of theoretical aspects of weld simulation and residual stress modelling using the finite element method (FEM) is presented. Thermo-elastic-plastic formulations using a von Mises yield criterion with nonlinear isotropic hardening has been employed. Residual stresses obtained from the analysis have been shown. The commercial FEM code ANSYS and a user created code were used for uncoupled thermalmechanical analysis. Second, the aim of this paper is to compare ANSYS capabilities extended by authors to model weld phenomena versus well known SYSWELD code. Element birth and death FEM technique was used to simulate the weld metal added to base metal due the welding process and to reset plastic history for molten portion of material. Goldak's double ellipsoid heat source was used to model welding heat source. The Leblond's model was used to simulate ferritic and bainitic phase transformations and Koistinen - Marburger model was used to simulate martensitic transformation. &copy; 2013 Published by Manufacturing Technology.<br/>","Novak, Pavol and Meko, Jozef and Zmindak, Milan",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Scalable graph hashing with feature transformation","Hashing has been widely used for approximate nearest neighbor (ANN) search in big data applications because of its low storage cost and fast retrieval speed. The goal of hashing is to map the data points from the original space into a binary-code space where the similarity (neighborhood structure) in the original space is preserved. By directly exploiting the similarity to guide the hashing code learning procedure, graph hashing has attracted much attention. However, most existing graph hashing methods cannot achieve satisfactory performance in real applications due to the high complexity for graph modeling. In this paper, we propose a novel method, called scalable graph hashing with feature transformation (SGH), for large-scale graph hashing. Through feature transformation, we can effectively approximate the whole graph without explicitly computing the similarity graph matrix, based on which a sequential learning method is proposed to learn the hash functions in a bit-wise manner. Experiments on two datasets with one million data points show that our SGH method can outperform the state-of-the-art methods in terms of both accuracy and scalability.<br/>","Jiang, Qing-Yuan and Li, Wu-Jun",2015,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Modeling of unstable behaviors of shape memory alloys during localization and propagation of phase transformation using a gradient-enhanced model","In this article, a multi-dimensional gradient-enhanced constitutive model of shape memory alloys is developed. The model is developed to capture unstable behaviors of shape memory alloys during both the forward and reverse phase transformations. Also, influence of loading history on the start of the phase transformation during both forward and reverse transformations is considered in the model by introducing new transformation limits and phase fraction formulations. The model is implemented in a finite element code, and using a numerical framework, effects of loading, boundary condition, inhomogeneous deformations, imperfection, and geometry on the unstable behaviors of different shape memory alloy samples during nucleation and phase transformation are investigated. The obtained results are compared with the available experimental and numerical results in the literature. The obtained results show that the gradient enhancement removes pathological localization effects which would typically result from the softening influence. In addition, the numerical study proves that the mode can capture the basic features of phase transformation front patterns and their evolution during transformations.<br/> &copy; SAGE Publications.","Badnava, Hojat and Kadkhodaei, Mahmoud and Mashayekhi, Mohammad",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Dispersion of changes in cloned and non-cloned code","Currently, the impacts of clones in software maintenance activities are being investigated by different researchers in different ways. Comparative stability analysis of cloned and non-cloned regions of a subject system is a well-known way of measuring the impacts where the hypothesis is that, the more a region is stable the less it is harmful for maintenance. Each of the existing stability measurement methods lacks to address one important characteristic, dispersion, of the changes happening in the cloned and non-cloned regions of software systems. Change dispersion of a particular region quantifies the extent to which the changes are scattered over that region. The intuition is that, more dispersed changes require more efforts to be spent in the maintenance phase. Measurement of Dispersion requires the extraction of method genealogies. In this paper, we have measured the dispersions of changes in cloned and non-cloned regions of several subject systems using a concurrent and robust framework for method genealogy extraction. We implemented the framework on Actor Architecture platform which facilitates coarse grained parallellism with asynchronous message passing capabilities. Our experimental results on 12 open-source subject systems written in three different programming languages (Java, C and C#) using two clone detection tools suggest that, the changes in cloned regions are more dispersed than the changes in non-cloned regions. Also, Type-3 clones exhibit more dispersion as compared to the Type-1 and Type-2 clones. The subject systems written in Java and C show higher dispersions as well as increased maintenance efforts as compared to the subject systems written in C#. &copy; 2012 IEEE.<br/>","Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.",2012,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8"
"Proposal for e-learning system with expandable and developable functions","Although e-learning systems are used by a wide number of learners and teachers, it is not necessarily true that all the functions they need are provided. In this study, we will discuss how to develop an e-learning system which will allow learners and teachers to create system functions and then share them by incorporating these functions into the system. E-learning systems are implemented through web applications. We will explain the reason why it is difficult to create these web applications and discuss a specification program which abstractly describes a common gateway interface (CGI) program, HTML and database access the elements of a web application. This specification program is described by an ET program, one based on the theory of equivalent transformation (ET) models, and expresses the web application&rsquo;s parallelism through the ET program framework. We will also explain how to incorporate a specification program into an e-learning system by transforming the program into an actual web application. Compared to conventional methods in which CGI programs, HTML and database access are directly scripted, the method, proposed in this study can clearly describe with a lower amount of code and, without requiring background, knowledge.<br/> &copy; 2015 ISSN 1349-4198.","Mabuchi, Hiroshi and Satoh, Toshiki",2015,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"The role of learning in construction technology transfer: A 'SCOT' perspective","Technology transfer (TT) has been given increasing importance since the formulation of the international code of conduct for technology transfer by the UNCTAD in 1985, and has become a preferred medium to bridge development gaps between developed and developing countries. Concomitantly, international joint ventures (IJVs) have been put forward as vehicles for change in the belief that contractors in developing countries can position themselves to receive technology from their developed counterparts. So far, TT has been studied through a variety of theoretical lenses. However, predominantly, the perspectives taken have assumed a linear process, viewing technology merely as an object, and effectively disregarding the multiple interactions involved in TT. In this paper, we argue that such perspectives only provide partial explanations of what construction technology entails, and how it is transferred between organisations. A counter-argument is put forward to view TT as a process of socio-technical interactions that is reliant on learning. Adopting the theoretical lens of the Social Construction of Technology (SCOT), we show how the SCOT framework allows for examining the socio-technical interactions between human actors and construction technology in TT. Specifically, we use the SCOT constructs of 'interpretative flexibility' and, 'closure and stabilisation' to reveal how learning is an integral process within the socio-technical interactions, which plays a critical role in TT between contractors in IJVs. Conclusions are drawn, highlighting the importance of studying TT as a system of socio-technical interactions on a construction project, in order to understand how learning plays a role in the process.<br/>","Oti-Sarpong, Kwadwo and Leiringer, Roine",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"CASPER: An efficient approach to detect anomalous code execution from unintended electronic device emissions","The CASPER system offers a lightweight, multi-disciplinary approach to detect the execution of anomalous code by monitoring the unintended electronic device emissions. Using commodity hardware and a combination of novel signal processing, machine learning, and program analysis techniques, we have demonstrated the ability to detect unknown code running on a device placed 12"" from the CASPER system by analyzing the devices RF emissions. Our innovations for the sensors subsystem include multi-antenna processing algorithms which allow us to extend range and extract signal features in the presence of background noise and interference encountered in realistic training and monitoring environments. In addition, robust feature estimation methods have been developed that allow detection of device operating conditions in the presence of varying clock frequency and other aspects that may change from device to device or from training to monitoring. Furthermore, a band-scan technique has been implemented to automatically identify suitable frequency bands for monitoring based on a set of metrics including received power, expected spectral feature content (based on loop length and clock frequency), kurtosis, and mode clustering. CASPER also includes an auto-labeling feature that is used to discover the signal processing features that provide the greatest information for detection without human intervention. The system additionally includes a framework for anomaly detection engines, currently populated with three engines based on n-grams, statistical frequency, and control flow. As we will describe, the combination of these engines reduces the ways in which an attacker can adapt in an attempt to hide from CASPER. We will describe the CASPER concept, components and technologies used, a summary of results to-date, and plans for further development. CASPER is an ongoing research project funded under the DARPA LADS program.<br/> &copy; 2018 SPIE.","Agrawal, Hira and Chen, Ray and Hollingsworth, Jeffrey K. and Hung, Christine and Izmailov, Rauf and Koshy, John and Liberti, Joe and Mesterharm, Chris and Morman, Josh and Panagos, Thimios and Pucci, Marc and Sebuktekin, Iil and Alexander, Scott and Tsang, Simon",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Assessment of subchannel code ASSERT-PV for prediction of critical heat flux in CANDU bundles","Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The recently-released ASSERT-PV 3.2 provides enhanced models for improved predictions of flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against five full-scale CANDU bundle experiments conducted in 1990s and in 2009 by Stern Laboratories (SL), using 28-, 37- and 43-element (CANFLEX) bundles. A total of 15 CHF test series with varying pressure-tube creep and/or bearing-pad height were analyzed. The SL experiments encompass the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for CANDU reactors. Code predictions of channel dryout power and axial and radial CHF locations were compared against measurements from the SL CHF tests to quantify the code prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. Furthermore, the sensitivity studies evaluated the contribution of each CHF model change or enhancement to the improvement in CHF prediction. Overall, the assessment demonstrated significant improvement in prediction of channel dryout power and axial and radial CHF locations in horizontal fuel channels containing CANDU bundles.<br/>","Rao, Y.F. and Cheng, Z. and Waddington, G.M.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Detecting heap-spraying code injection attacks in malicious web pages using runtime execution","The growing use of web services is increasing web browser attacks exponentially. Most attacks use a technique called heap spraying because of its high success rate. Heap spraying executes a malicious code without indicating the exact address of the code by copying it into many heap objects. For this reason, the attack has a high potential to succeed if only the vulnerability is exploited. Thus, attackers have recently begun using this technique because it is easy to use JavaScript to allocate the heap memory area. This paper proposes a novel technique that detects heap spraying attacks by executing a heap object in a real environment, irrespective of the version and patch status of the web browser. This runtime execution is used to detect various forms of heap spraying attacks, such as encoding and polymorphism. Heap objects are executed after being filtered on the basis of patterns of heap spraying attacks in order to reduce the overhead of the runtime execution. Patterns of heap spraying attacks are based on analysis of how an web browser accesses benign web sites. The heap objects are executed forcibly by changing the instruction register into the address of them after being loaded into memory. Thus, we can execute the malicious code without having to consider the version and patch status of the browser. An object is considered to contain a malicious code if the execution reaches a call instruction and then the instruction accesses the API of system libraries, such as kernel32.dll and ws 32.dll. To change registers and monitor execution flow, we used a debugger engine. A prototype, named HERAD(HEap spRAying Detector), is implemented and evaluated. In experiments, HERAD detects various forms of exploit code that an emulation cannot detect, and some heap spraying attacks that NOZZLE cannot detect. Although it has an execution overhead, HERAD produces a low number of false alarms. The processing time of several minutes is negligible because our research focuses on detecting heap spraying. This research can be applied to existing systems that collect malicious codes, such as Honeypot. Copyright &copy; 2012 The Institute of Electronics, Information and Communication Engineers.<br/>","Choi, Younghan and Kim, Hyoungchun and Lee, Donghoon",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A change guide method based on developers' interaction and past recommendation","In this paper, we propose a change guide method based on the past developers' activity that consists of read and write access records of artifacts. In our proposed method, we calculate candidates of next change recommendation considering the history of its recommendations. We define ""cumulative likelihood"" to enable the method to recommend the appropriate candidates when a change propagates more than one code elements. A case study using interaction history logs from 15 participants showed the improvement of the accuracy of the method-level change recommendation.<br/>","Yamamori, Akihiro and Kobayashi, Takashi",2016,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Detecting asynchrony and dephase change patterns by mining software repositories","Software maintenance accounts for the largest part of the costs of any program. During maintenance activities, developers implement changes (sometimes simultaneously) on artifacts in order to fix bugs and to implement new requirements. To reduce this part of the costs, previous work proposed approaches to identify the artifacts of programs that change together. These approaches analyze historical data, mined from version control systems, and report change patterns, which lead at the causes, consequences, and actors of the changes to source code files. They also introduce so-called change patterns that describe some typical change dependencies among files. In this paper, we introduce two novel change patterns: the asynchrony change pattern, corresponding to macro co-changes (MC), that is, of files that co-change within a large time interval (change periods) and the dephase change pattern, corresponding to dephase macro co-changes (DC), that is, MC that always happens with the same shifts in time. We present our approach, that we named Macocha, to identify these two change patterns in large programs. We use the k-nearest neighbor algorithm to group changes into change periods. We also use the Hamming distance to detect approximate occurrences of MC and DC. We apply Macocha and compare its performance in terms of precision and recall with UMLDiff (file stability) and association rules (co-changing files) on seven systems: ArgoUML, FreeBSD, JFreeChart, Openser, SIP, XalanC, and XercesC developed with three different languages (C, C++, and Java). These systems have a size ranging from 532 to 1693 files, and during the study period, they have undergone 1555 to 23,944 change commits. We use external information and static analysis to validate (approximate) MC and DC found by Macocha. Through our case study, we show the existence and usefulness of these novel change patterns to ease software maintenance and, potentially, reduce related costs. Copyright &copy; 2013 John Wiley &amp; Sons, Ltd.<br/>","Jaafar, Fehmi and Gueheneuc, Yann-Gael and Hamel, Sylvie and Antoniol, Giuliano",2014,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Predicting Software Maintenance Effort by Mining Software Project Reports Using Inter-Version Validation","Changes in the software are unavoidable due to an ever changing dynamic and active environment wherein expectations and requirements of the users tend to change rapidly. As a result, software needs to upgrade itself from its previous version to the next version in order to meet expectations of the user. The upgradation of the software is in terms of total number of Lines of Code (LOC) that might have been inserted, deleted or modified in moving from one version of software to the next. These changes are maintained in the change reports which constitute of the defect ID and defect description. Defect description describes the cause of defect which might have occurred in the previous version of the software due to which either new LOC needs to be inserted or existing LOC need to be deleted or modified. A lot of effort is required to correct the defects identified in software at the maintenance phase i.e., when software is delivered at the customers end. Thus, in this paper, we intend to predict maintenance effort by analyzing the defect reports using text mining techniques and thereafter developing the prediction models using suitable machine learning algorithms viz. Multi-Layer Perceptron (MLP), Radial-Basis Function (RBF) network and Decision Tree (DT). We have considered the changes between three successive versions of 'MMS' application package of Android operating system and have performed inter-version validation where the model predicted using the version 'v' is validated on the subsequent version i.e., 'v+1'. The performance of the model was evaluated using Receiver Operating Characteristics (ROC) analysis. The results indicated that the model predicted on 'MMS' 4.0 version using MLP algorithm has shown good results when validated on 'MMS' 4.1 version. On the other hand, the performance of RBF and DT algorithms has been consistently average in predicting the maintenance effort.<br/> &copy; 2016 World Scientific Publishing Company.","Jindal, Rajni and Malhotra, Ruchika and Jain, Abha",2016,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Computational intelligence from AI to BI to NI","This paper gives highlights of the history of the neural network field, stressing the fundamental ideas which have been in play. Early neural network research was motivated mainly by the goals of artificial intelligence (AI) and of functional neuroscience (biological intelligence, BI), but the field almost died due to frustrations articulated in the famous book Perceptrons by Minsky and Papert. When I found a way to overcome the difficulties by 1974, the community mindset was very resistant to change; it was not until 1987/1988 that the field was reborn in a spectacular way, leading to the organized communities now in place. Even then, it took many more years to establish crossdisciplinary research in the types of mathematical neural networks needed to really understand the kind of intelligence we see in the brain, and to address the most demanding engineering applications. Only through a new (albeit short-lived) funding initiative, funding crossdisciplinary teams of systems engineers and neuroscientists, were we able to fund the critical empirical demonstrations which put our old basic principle of ""deep learning"" firmly on the map in computer science. Progress has rightly been inhibited at times by legitimate concerns about the ""Terminator threat"" and other possible abuses of technology. This year, at SPIE, in the quantum computing track, we outline the next stage ahead of us in breaking out of the box, again and again, and rising to fundamental challenges and opportunities still ahead of us.<br/> &copy; 2015 COPYRIGHT SPIE.","Werbos, Paul J.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Automating the maintenance of nonfunctional system properties using demonstration-based model transformation","Domain-Specific Modeling Languages (DSMLs) are playing an increasingly significant role in software development. By raising the level of abstraction using notations that are representative of a specific domain, DSMLs allow the core essence of a problem to be separated from irrelevant accidental complexities, which are typically found at the implementation level in source code. In addition to modeling the functional aspects of a system, a number of nonfunctional properties (e.g., quality of service constraints and timing requirements) also need to be integrated into models in order to reach a complete specification of a system. This is particularly true for domains that have distributed real time and embedded needs. Given a base model with functional components, maintaining the nonfunctional properties that crosscut the base model has become an essential modeling task when using DSMLs. The task of maintaining nonfunctional properties in DSMLs is traditionally supported by manual model editing or by using model transformation languages. However, these approaches are challenging to use for those unfamiliar with the specific details of a modeling transformation language and the underlying metamodel of the domain, which presents a7 steep learning curve for many users. This paper presents a demonstration-based approach to automate the maintenance of nonfunctional properties in DSMLs. Instead of writing model transformation rules explicitly, users demonstrate how to apply the nonfunctional properties by directly editing the concrete model instances and simulating a single case of the maintenance process. By recording a user's operations, an inference engine analyzes the user's intention and generates generic model transformation patterns automatically, which can be refined by users and then reused to automate the same evolution and maintenance task in other models. Using this approach, users are able to automate the maintenance tasks without learning a complex model transformation language. In addition, because the demonstration is performed on model instances, users are isolated from the underlying abstract metamodel definitions. Our demonstration-based approach has been applied to several scenarios, such as auto scaling and model layout. The specific contribution in this paper is the application of the demonstration-based approach to capture crosscutting concerns representative of aspects at the modeling level. Several examples are presented across multiple modeling languages to demonstrate the benefits of our approach. &copy; 2013 John Wiley &amp; Sons, Ltd.<br/>","Sun, Yu and Gray, Jeff and Delamare, Romain and Baudry, Benoit and White, Jules",2013,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"Automatic CPU/GPU Generation of Multi-versioned OpenCL Kernels for C++ Scientific Applications","Parallelism has become one of the most extended paradigms used to improve performance. However, it forces software developers to adapt applications and coding mechanisms to exploit the available computing devices. Legacy source code needs to be re-written to take advantage of multi- core and many-core computing devices. Writing parallel applications in a traditional way is hard, expensive, and time consuming. Furthermore, there is often more than one possible transformation or optimization that can be applied to a single piece of legacy code. Therefore many parallel versions of the same original sequential code need to be considered. In this paper, we describe an automatic parallel source code generation workflow (REWORK) for parallel heterogeneous platforms. REWORK automatically identifies promising kernels on legacy C++ source code and generates multiple specific versions of kernels for improving C++ applications, selecting the most adequate version based on both static source code and target platform characteristics.<br/> &copy; 2016, Springer Science+Business Media New York.","Sotomayor, Rafael and Sanchez, Luis Miguel and Garcia Blas, Javier and Fernandez, Javier and Garcia, J. Daniel",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Discovering maintainability changes in large software systems*","In this paper we propose an approach to automatically discover meaningful changes to maintainability of applications developed using object oriented programming languages. Our approach consists of an algorithm that employs the values of several class-level software metrics that can be easily obtained using open source software. Based on these values, a score that illustrates the maintainability change between two versions of the system is calculated. We present relevant related work, together with the state of research regarding the link between software metrics and maintainability for object oriented systems. In order to validate the approach, we undertake a case study that covers the entire development history of the jEdit open source text editor. We consider 41 version pairs that are assessed for changes to maintainability. First, a manual tool assisted examination of the source code was performed, followed by calculating the Maintainability Index for each application version. In the last step, we apply the proposed approach and compare the findings with those of the manual examination as well as those obtained using the Maintainability Index. In the final section, we present the identified issues and propose future work to further fine tune the approach.<br/> &copy; 2017 Association for Computing Machinery.","Molnar, Arthur and Motogna, Simona",2017,"[""Engineering Village""]","Rejeitado: CR3","Rejeitado: CR3"
"Developing prediction models to assist software developers and support managers","A huge amount of historical information about the evolution of a software project is available in software repositories, namely bug repositories, source control repositories, archived communications, deployment logs, and code repositories. By mining the evolutionary history of a software, we have designed prediction models to assist software developers by predicting bug attributes like priority, severity, assignee and fix time. We have evaluated the uncertainty in the software in terms of entropy arises due to source code changes done in files of the software to fix different issues. To support software managers, we have designed prediction models to predict potential values of entropy and different issues, namely bugs, improvements in existing features (IMPs) and new features (NFs) over a long run. In this research work, we have developed mathematical models to assist software managers and developers in bug triag-ing, bug fixing and different software maintenance related tasks. Our work has been validated on issue and code change data of several open source projects, namely Eclipse, Open office, Mozilla and Apache.<br/> &copy; Springer International Publishing AG 2017.","Sharma, Meera and Tondon, Abhishek",2017,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"Paper: Togpu: Automatic source transformation from C++ to CUDA using Clang/LLVM","Parallel processing using GPUS provides substantial increases in algorithm performance across many disciplines. As a result serial algorithms are commonly translated to parallel algorithms written in CUDA or OpenCL. To perform this translation a user must first overcome various barriers to entry. These obstacles change depending on the user but in general may include learning to program using the chosen API, understanding the intricacies of parallel processing and optimization, and other issues such as the upkeep of two sets of code. Such barriers are experienced by both experts and novices alike. Leveraging the unique source to source transformation tools provided by Clang/LLVM we have created a tool to generate CUDA from C++. Such transformations reduce obstacles experienced in developing GPU software and can increase efficiency and revision speed regardless of experience. This manuscript details a new open source, cross platform tool, togpu, which performs source to source transformations from C++ to CUDA. We present experimentation results using common image processing algorithms. The tool lowers entrance barriers while preserving a singular code base and readability. Enhancing the GPU developer workflow through providing core tooling affords users immediate benefits-And facilitates further developments -To improve high performance, parallel computing.<br/> &copy; 2016 Society for Imaging Science and Technology.","Marangoni, Matthew and Wischgoll, Thomas",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Error-correcting output codes as a transformation from multi-class to multi-label prediction","In this paper, we reinterpret error-correcting output codes (ECOCs) as a framework for converting multi-class classification problems into multi-label prediction problems. Different well-known multi-label learning approaches can be mapped upon particular ways of dealing with the original multi-class problem. For example, the label powerset approach obviously constitutes the inverse transformation from multi-label back to multi-class, whereas binary relevance learning may be viewed as the conventional way of dealing with ECOCs, in which each classifier is learned independently of the others. Consequently, we evaluate whether alternative choices for solving the multi-label problem may result in improved performance. This question is interesting because it is not clear whether approaches that do not treat the bits of the code words independently have sufficient error-correcting properties. Our results indicate that a slight but consistent advantage can be obtained with the use of multi-label methods, in particular when longer codes are employed. &copy; 2012 Springer-Verlag Berlin Heidelberg.<br/>","Furnkranz, Johannes and Park, Sang-Hyeun",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Martensitic-Austenitic phase transformation of Ni-Ti SMAs: Thermal properties","The main scope of this work concerns the definition of the thermal conductivity temperature dependence of fully dense NiTi SMAs in the temperature range where the Martensitic-Austenitic phase transformation occurs. The methodology used to evaluate this thermal property is based on an experimental-numerical approach that requires the definition of the heat capacity temperature dependence and the knowledge of the latent heat of transformation. The experimental work is based on the capability to heat the cylindrical NiTi samples uniformly on one side and to impose a variety of initial heating rates ranging from 0.1 to 5 K/s. Laser radiant energy was used as the heating source and the temperature history of the top and bottom NiTi sample surfaces were recorded using thermocouples. The numerical code considered the sample as a solid with a constant density and with thermal properties that were dependent on temperature. The heat capacity and latent heat of transformation were defined on the basis of the thermal analysis data, while the convection heat exchange coefficient was estimated from knowledge of the experimental configuration, lateral sample surface and the temperature field of the gas surrounding the sample. The results indicated that the thermal conductivity generally increased with temperature, but a minimum in the temperature range defining the Martensitic-Austenitic transformation has been pointed out. The higher thermal conductivity value of the Austenite phase is correlated with its electronic structure. &copy; 2012 Elsevier Ltd. All rights reserved.","Zanotti, C. and Giuliani, P. and Chrysanthou, A.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An online course and teacher resource for residential building codes and above code construction methods","Community destruction and loss of life due to residential building code violations still occur too frequently and increasing code enforcement is often not possible due to lack of funds and resources. Teaching the International Residential Code (IRC) to college-level construction students is another way to encourage greater code compliance and enhance community resilience. In a national curriculum review of construction management, architecture, and civil engineering programs (2-year, 4-year, and graduate degrees, 950 in total), only seven percent provide courses with IRC related learning outcomes. A follow-up national survey to construction, architecture and civil engineering faculty suggests the barriers to teach codes are the lack of available resources and low cognitive student learning perceived in teaching about the IRC. In response to these findings an online course was developed. Students learn how codes will influence their professional careers, identifying the difference between prescriptive and performance based codes and communicating how codes relate to the performance of a structure. Student learning outcomes are created through multiple active learning methods. For example, house plans are distributed to students, and in a problem-based approach, students ""red line"" drawings to meet the IRC. In a case-based module, students identify solutions to grey-water systems that do not meet current local codes. Course modules were developed with an advisory committee including building code officials, architects, construction managers, disaster mitigation experts, and academic faculty. Advisory members anonymously submitted feedback for each module. Feedback was compiled, discussed and course content edited. This review-discuss-edit process was repeated until a final version was agreed upon with the advisory committee. The course and content is a free resource for educators. Over thirty modules, house plans and videos of industry professionals are embedded within. Modules can be delivered in a semester long course but can also stand-alone. Course link: canvas.instructure.com/courses/780681. &copy; American Society for Engineering Education, 2015.","Shealy, Tripp and Kiesling, Audra Ann and Smail, Timothy R.",2015,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Emerging topics in mining software repositories: Machine learning in software repositories and datasets","A software process is a set of related activities that culminates in the production of a software package: specification, design, implementation, testing, evolution into new versions, and maintenance. There are also other supporting activities such as configuration and change management, quality assurance, project management, evaluation of user experience, etc. Software repositories are infrastructures to support all these activities. They can be composed with several systems that include code change management, bug tracking, code review, build system, release binaries, wikis, forums, etc. This position paper on mining software repositories presents a review and a discussion of research in this field over the past decade. We also identify applied machine learning strategies, current working topics, and future challenges for the improvement of company decision-making systems. Machine learning is defined as the process of discovering patterns in data. It can be applied to software repositories, since every change is recorded as data. Companies can then use these patterns as the basis for their decision-making systems and for knowledge discovery.<br/> &copy; 2018, Springer-Verlag GmbH Germany, part of Springer Nature.","Guemes-Pena, Diego and Lopez-Nozal, Carlos and Marticorena-Sanchez, Raul and Maudes-Raedo, Jesus",2018,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Yet Another Intelligent Code-Generating System: A Flexible and Low-Cost Solution","Modern compilers apply various code transformation algorithms to improve the quality of the target code. However, a complex problem is to determine which transformation algorithms must be utilized. This is difficult because of three reasons: a number of transformation algorithms, various combination possibilities, and several configuration possibilities. Over the last few years, various intelligent systems were presented in the literature. The goal of these systems is to search for transformation algorithms and thus apply them to a certain program. This paper proposes a flexible, low-cost and intelligent system capable of identifying transformation algorithms for an input program, considering the program&rsquo;s specific features. This system is flexible for parameterization selection and has a low-computational cost. In addition, it has the capability to maximize the exploration of available computational resources. The system was implemented under the Low Level Virtual Machine infrastructure and the results indicate that it is capable of exceeding, up to 21.36%, performance reached by other systems. In addition, it achieved an average improvement of up to 17.72% over the most aggressive compiler optimization level of the Low Level Virtual Machine infrastructure.<br/> &copy; 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Filho, Joao Fabricio and Rodriguez, Luis Gustavo Araujo and da Silva, Anderson Faustino",2018,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8, CR10"
"A web-based system for error correction questions in programming exercise","In this chapter, the authors propose a system for generating error correction questions of programs, which are suitable for developing debugging skills in programming education. The system generates HTML files for answering questions and CGI programs for checking answers. They are deployed on a Web server, and learners read and answer questions on Web browsers. It provides an intuitive user interface; a learner can edit codes in place at the text. To make programs including errors for learning debugging, the authors analyze types of errors and define the processes of error injection as code transformation patterns. If learners can edit any codes freely, it is difficult to check all possible answers. Instead, the authors adopt a strategy to restrict editable points and possible answers from the educational view. A demonstration of error correction questions of the C language is available at http://ecq.tebasaki.jp/.<br/> &copy; 2015 by IGI Global. All rights reserved.","Hachisu, Yoshinari and Yoshida, Atsushi",2015,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Yazılım Davranışlarının Dinamik Olarak Değiştirilebilmesini Sağlayan Yeni Bir Yaklaşım","Nowadays software provided over web and mobile platforms can be frequently updated. W i t h updates the behavior of software changes. Certain behaviors can cause undesired damages. Therefore, the correc&not; tion of incorrect behavior that reaches customers is done via new up&not; dates. Fixing software's source code, testing these new changes, getting peer approval through code reviews, building a new version of the soft&not; ware and deploying it to the production environment can take hours. As a result, being able to correct any changes that may severely affect a lot of customers in the matter of seconds is an important need for software developers to preserve developers' prestige and customer satisfaction. I n this study, we propose an approach that enables developers to change software's behavior at run time and describe an implementation realiz&not; ing this approach. W i t h our approach, software updates can be deployed incrementally without making any code changes or they can be reverted completely within seconds. I n addition, our approach makes it possible to try out experimental features on small subsets of users in production environments.<br/>","Gerede, Cada Evren",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"MaLTeSQuE 2017 - IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with SANER 2017","The proceedings contain 6 papers. The topics discussed include: using source code metrics to predict change-prone web services: a case-study on eBay services; investigating code smell co-occurrences using association rule learning: a replicated study; using machine learning to design a flexible LOC counter; machine learning for finding bugs: an initial report; automatic feature selection by regularization to improve bug prediction accuracy; and hyperparameter optimization to improve bug prediction accuracy.",,2017,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Aggregating Association Rules to Improve Change Recommendation","As the complexity of software systems grows, it becomes increasingly difficult for developers to be aware of all the dependencies that exist between artifacts (e.g., files or methods) of a system. Change recommendation has been proposed as a technique to overcome this problem, as it suggests to a developer relevant source-code artifacts related to her changes. Association rule mining has shown promise in deriving such recommendations by uncovering relevant patterns in the system&rsquo;s change history. The strength of the mined association rules is captured using a variety of interestingness measures. However, state-of-the-art recommendation engines typically use only the rule with the highest interestingness value when more than one rule applies. In contrast, we argue that when multiple rules apply, this indicates collective evidence, and aggregating those rules (and their evidence) will lead to more accurate change recommendation. To investigate this hypothesis we conduct a large empirical study of 15 open source software systems and two systems from our industry partners. We evaluate association rule aggregation using four variants of the change history for each system studied, enabling us to compare two different levels of granularity in two different scenarios. Furthermore, we study 40 interestingness measures using the rules produced by two different mining algorithms. The results show that (1) between 13 and 90% of change recommendations can be improved by rule aggregation, (2) rule aggregation almost always improves change recommendation for both algorithms and all measures, and (3) fine-grained histories benefit more from rule aggregation.<br/> &copy; 2017, Springer Science+Business Media, LLC.","Rolfsnes, Thomas and Moonen, Leon and Alesio, Stefano Di and Behjati, Razieh and Binkley, Dave",2018,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Requirements Traceability Through Information Retrieval Using Dynamic Integration of Structural and Co-change Coupling","Requirement Traceability (RT) links correlate requirements to their corresponding source code and helps in better requirement understanding, reusability and other software maintenance activities. Since a major portion of software artifacts is in the form of text, for finding these links Information Retrieval (IR) techniques based on textual similarity are widely adopted for Requirement Traceability. But it is hard to find RT links when artifacts have less textual description. So, for finding these links indirectly non-textual techniques like structural information based, co-change history based, ownership based are used with IR. However, if the results of IR contain false positives, the combined approach may increase them further. So, instead of directly combining, this paper proposes an automatic technique for RT by first improving the IR approach and then combining it with the non-textual based techniques. Also, we present a new non-textual based technique based on weighted integration of structural coupling and change history based coupling of classes for retrieving indirect links. The results show that our proposed approach performs better than the existing methods which use coupling information complementary to IR.<br/> &copy; 2017, Springer Nature Singapore Pte Ltd.","Jyoti and Chhabra, Jitender Kumar",2017,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"HMETS-A simple and efficient hydrology model for teaching hydrological modelling, flow forecasting and climate change impacts","Hydrological models are commonly used to forecast streamflow and for climate change impact studies. There is a wide range of hydrology models using lumped conceptual approaches all the way to more physically based distributed algorithms. Most of these models come with a steep learning curve before they can be used efficiently by the end user, and they can be tricky to calibrate appropriately. Only a small number of hydrology models can be considered easy to set up and use, and even fewer provide their source code for easy modification to be tailored to individual needs. These drawbacks make it difficult to use these models in educational applications. The goal of this paper is to introduce a very simple, yet efficient, lumped-conceptual hydrological model designed to address the above problems. The MATLAB-based HMETS hydrological model is simple and can be easily and quickly set up on a new watershed, including automatic calibration using state of the art optimization algorithms. Despite its simplicity, the model has proved to perform well against two other lumped-conceptual hydrological models over 320 watersheds. HMETS obtained a median Nash-Sutcliffe Efficiency of 0.72 in validation, compared to 0.64 for MOHYSE (similar structure) and 0.77 for HSAMI (more complex structure). The model's source code is freely available and includes an optional simplified user interface. A climate change impacts simulation tool using the constant scaling downscaling method is also incorporated to the interface. HMETS has been tested in the Construction Engineering Final-Year Project for a group of 60 undergraduate students.<br/> &copy; 2017 TEMPUS Publications.","Martel, Jean-Luc and Demeester, Kenjy and Brissette, Francois and Poulin, Annie and Arsenault, Richard",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Wearable tech for halloween-The gemma MO's embedded python lets you change your code on the fly [Resources-Tools]","Halloween is approaching, and with it a global parade of costumes. So I thought this would be the perfect time to try out a new wearable microcontroller from Adafruit Industries: The Gemma M0. Adafruit has been putting out wearable microcontrollers for several years. These differ from conventional controllers, such as the Arduino Uno, in that the wearables are typically more compact and use pads with large through holes for input and output, instead of pins. These holes make it easy to sew boards to fabric or tie conductive thread to the pads. What makes the Gemma M0 particularly interesting is that it runs CircuitPython, Adafruit's modified version of the Python language designed for embedded devices. (At this point, I should note that Limor Fried, the founder of Adafruit, is a member of IEEE Spectrum's editorial advisory board, but she played no role in the origination of this article.<br/> &copy; 2017 IEEE.","Cass, Stephen",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"ACM International Conference Proceeding Series","The proceedings contain 26 papers. The topics discussed include: cognitive and contextual enterprise mobile computing; cognitive and contextual enterprise mobile computing; automatically detecting the up-to-date status of to-do comments in Java programs; energy and performance prediction of CUDA applications using dynamic regression models; SymTest : a framework for symbolic testing of embedded software; process edification for traceability in evolving architectures; architecting an extensible framework for gamifying software engineering concepts; divergence aware automated partitioning of OpenCL workloads; JDQL: a framework for java static analysis; explicating the relationships among subsystems; system maps: integrating knowledge system models; LogOpt: static feature extraction from source code for automated catch block logging prediction; lean transformation: adapting to the change, factors for success and lessons learnt during the journey; formal verification of avionics self adaptive software: a case study; learning's from developing a domain specific engineering environment for control systems; an approach for collaborative quality assessment of software; and integrating values into mobile software engineering.",,2016,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"An introduction to recommendation systems in software engineering","Software engineering is a knowledge-intensive activity that presents many information navigation challenges. Information spaces in software engineering include the source code and change history of the software, discussion lists and forums, issue databases, component technologies and their learning resources, and the development environment. The technical nature, size, and dynamicity of these information spaces motivate the development of a special class of applications to support developers: recommendation systems in software engineering (RSSEs), which are software applications that provide information items estimated to be valuable for a software engineering task in a given context. In this introduction, we review the characteristics of information spaces in software engineering, describe the unique aspects of RSSEs, present an overview of the issues and considerations involved in creating, evaluating, and using RSSEs, and present a general outlook on the current state of research and development in the field of recommendation systems for highly technical domains.<br/> &copy; Springer-Verlag Berlin Heidelberg 2014.","Robillard, Martin P. and Walker, Robert J.",2014,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"The design and implementation of OpenMP 4.5 and OpenACC backends for the RAJA C++ performance portability layer","Portability abstraction layers such as RAJA enable users to quickly change how a loop nest is executed with minimal modifications to high-level source code. Directive-based programming models such as OpenMP and OpenACC provide easy-to-use annotations on for-loops and regions which change the execution pattern of user code. Directive-based language backends for RAJA have previously been limited to few options due to multiplicative clauses creating version explosion. In this work, we introduce an updated implementation of two directive-based backends which helps mitigate the aforementioned version explosion problem by leveraging the C++ type system and template meta-programming concepts. We implement partial OpenMP 4.5 and OpenACC backends for the RAJA portability layer which can apply loop transformations and specify how loops should be executed. We evaluate our approach by analyzing compilation and runtime overhead for both backends using PGI 17.7 and IBM clang (OpenMP 4.5) on a collection of computation kernels.<br/> &copy; Springer International Publishing AG, part of Springer Nature 2018.","Killian, William and Scogland, Tom and Kunen, Adam and Cavazos, John",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Thermophysical properties of phase change emulsions prepared by D-phase emulsification","A phase change emulsion (PCE) is a mixture of fine particles of a phase change material (PCM) and an aqueous surfactant solution. PCEs are attracting attention as thermal media. They provide high-density thermal energy storage by utilizing the latent heat of the PCM, and high transportability because of their high fluidity. In this study, PCEs were prepared by D-phase emulsification, and their properties were evaluated. Two paraffin PCMs, n-hexadecane and n-octadecane, were dispersed inside the PCEs, and their mass fraction was varied. The PCEs exhibited similar particle size distributions, regardless of the type of dispersed PCM or its mass fraction. The viscosity of the PCE increased with increasing PCM mass fraction, in agreement with theoretical values. The latent heat of fusion and specific heat of the PCEs were evaluated using a temperature history method. Pump consumption rates were calculated from these results, and are compared with that of water.<br/> &copy; 2016 Elsevier Ltd.","Morimoto, Takashi and Togashi, Kenichi and Kumano, Hiroyuki and Hong, Hiki",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Clone evolution: A systematic review","Detection of code clones - similar or identical source code fragments - is of concern both to researchers and to practitioners. An analysis of the clone detection results for a single source code version provides a developer with information about a discrete state in the evolution of the software system. However, tracing clones across multiple source code versions permits a clone analysis to consider a temporal dimension. Such an analysis of clone evolution can be used to uncover the patterns and characteristics exhibited by clones as they evolve within a system. Developers can use the results of this analysis to understand the clones more completely, which may help them to manage the clones more effectively. Thus, studies of clone evolution serve a key role in understanding and addressing issues of cloning in software. In this paper, we present a systematic review of the literature on clone evolution. In particular, we present a detailed analysis of 30 relevant papers that we identified in accordance with our review protocol. The review results were organized to address three research questions. Through our answers to these questions, we present the methods that researchers have used to study clone evolution, the patterns that researchers have found evolving clones to exhibit, and the evidence that researchers have established regarding the extent of inconsistent change undergone by clones during software evolution. Overall, the review results indicate that whereas researchers have conducted several empirical studies of clone evolution, there are contradictions among the reported findings, particularly regarding the lifetimes of clone lineages and the consistency with which clones are changed during software evolution. We identify human-based empirical studies and classification of clone evolution patterns as two areas that are in particular need of further work. Copyright &copy; 2011 John Wiley &amp;Sons, Ltd.<br/>","Pate, Jeremy R. and Tairas, Robert and Kraft, Nicholas A.",2013,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Identifying and understanding header file hotspots in C/C++ build processes","Software developers rely on a fast build system to incrementally compile their source code changes and produce modified deliverables for testing and deployment. Header files, which tend to trigger slow rebuild processes, are most problematic if they also change frequently during the development process, and hence, need to be rebuilt often. In this paper, we propose an approach that analyzes the build dependency graph (i.e., the data structure used to determine the minimal list of commands that must be executed when a source code file is modified), and the change history of a software system to pinpoint header file hotspots&mdash;header files that change frequently and trigger long rebuild processes. Through a case study on the GLib, PostgreSQL, Qt, and Ruby systems, we show that our approach identifies header file hotspots that, if improved, will provide greater improvement to the total future build cost of a system than just focusing on the files that trigger the slowest rebuild processes, change the most frequently, or are used the most throughout the codebase. Furthermore, regression models built using architectural and code properties of source files can explain 32&ndash;57 % of these hotspots, identifying subsystems that are particularly hotspot-prone and would benefit the most from architectural refinement.<br/> &copy; 2015, Springer Science+Business Media New York.","McIntosh, Shane and Adams, Bram and Nagappan, Meiyappan and Hassan, Ahmed E.",2016,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Wikipedia Vandal Early Detection: From User Behavior to User Embedding","Wikipedia is the largest online encyclopedia that allows anyone to edit articles. In this paper, we propose the use of deep learning to detect vandals based on their edit history. In particular, we develop a multi-source long-short term memory network (M-LSTM) to model user behaviors by using a variety of user edit aspects as inputs, including the history of edit reversion information, edit page titles and categories. With M-LSTM, we can encode each user into a low dimensional real vector, called user embedding. Meanwhile, as a sequential model, M-LSTM updates the user embedding each time after the user commits a new edit. Thus, we can predict whether a user is benign or vandal dynamically based on the up-to-date user embedding. Furthermore, those user embeddings are crucial to discover collaborative vandals. Code and data related to this chapter are available at: https://bitbucket.org/bookcold/vandal_detection.<br/> &copy; 2017, Springer International Publishing AG.","Yuan, Shuhan and Zheng, Panpan and Wu, Xintao and Xiang, Yang",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"CLOCK-DWF: A write-history-aware page replacement algorithm for hybrid PCM and DRAM memory architectures","Phase change memory (PCM) has emerged as one of the most promising technologies to incorporate into the memory hierarchy of future computer systems. However, PCM has two critical weaknesses to substitute DRAM memory in its entirety. First, the number of write operations allowed to each PCM cell is limited. Second, write access time of PCM is about 6-10 times slower than that of DRAM. To cope with this situation, hybrid memory architectures that use a small amount of DRAM together with PCM have been suggested. In this paper, we present a new memory management technique for hybrid PCM and DRAM memory architecture that efficiently hides the slow write performance of PCM. Specifically, we aim to estimate future write references accurately and then absorb frequent memory writes into DRAM. To do this, we analyze the characteristics of memory write references and find two noticeable phenomena. First, using write history alone performs better than using both read and write history in estimating future write references. Second, the frequency characteristic is a better estimator than temporal locality in predicting future memory writes. Based on these two observations, we present a new page replacement algorithm called CLOCK-DWF (CLOCK with Dirty bits and Write Frequency) that significantly reduces the number of write operations that occur on PCM and also increases the lifespan of PCM memory.<br/> &copy; 2013 IEEE.","Lee, Soyoon and Bahn, Hyokyung and Noh, Sam H.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"What are the effects of history length and age on mining software change impact?","The goal of Software Change Impact Analysis is to identify artifacts (typically source-code files or individual methods therein) potentially affected by a change. Recently, there has been increased interest in mining software change impact based on evolutionary coupling. A particularly promising approach uses association rule mining to uncover potentially affected artifacts from patterns in the system&rsquo;s change history. Two main considerations when using this approach are the history length, the number of transactions from the change history used to identify the impact of a change, and history age, the number of transactions that have occurred since patterns were last mined from the history. Although history length and age can significantly affect the quality of mining results, few guidelines exist on how to best select appropriate values for these two parameters. In this paper, we empirically investigate the effects of history length and age on the quality of change impact analysis using mined evolutionary coupling. Specifically, we report on a series of systematic experiments using three state-of-the-art mining algorithms that involve the change histories of two large industrial systems and 17 large open source systems. In these experiments, we vary the length and age of the history used to mine software change impact, and assess how this affects precision and applicability. Results from the study are used to derive practical guidelines for choosing history length and age when applying association rule mining to conduct software change impact analysis.<br/> &copy; 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Moonen, Leon and Rolfsnes, Thomas and Binkley, Dave and Di Alesio, Stefano",2018,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Early prediction of merged code changes to prioritize reviewing tasks","Modern Code Review (MCR) has been widely used by open source and proprietary software projects. Inspecting code changes consumes reviewers much time and effort since they need to comprehend patches, and many reviewers are often assigned to review many code changes. Note that a code change might be eventually abandoned, which causes waste of time and effort. Thus, a tool that predicts early on whether a code change will be merged can help developers prioritize changes to inspect, accomplish more things given tight schedule, and not waste reviewing effort on low quality changes. In this paper, motivated by the above needs, we build a merged code change prediction tool. Our approach first extracts 34 features from code changes, which are grouped into 5 dimensions: code, file history, owner experience, collaboration network, and text. And then we leverage machine learning techniques such as random forest to build a prediction model. To evaluate the performance of our approach, we conduct experiments on three open source projects (i.e., Eclipse, LibreOffice, and OpenStack), containing a total of 166,215 code changes. Across three datasets, our approach statistically significantly improves random guess classifiers and two prediction models proposed by Jeong et al. (2009) and Gousios et al. (2014) in terms of several evaluation metrics. Besides, we also study the important features which distinguish merged code changes from abandoned ones.<br/> &copy; 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Fan, Yuanrui and Xia, Xin and Lo, David and Li, Shanping",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR9"
"Towards automatic learning of heuristics for mechanical transformations of procedural code","The current trends in next-generation exascale systems go towards integrating a wide range of specialized (co-)processors into traditional supercomputers. Due to the efficiency of heterogeneous systems in terms of Watts and FLOPS per surface unit, opening the access of heterogeneous platforms to a wider range of users is an important problem to be tackled. However, heterogeneous platforms limit the portability of the applications and increase development complexity due to the programming skills required. Program transformation can help make programming heterogeneous systems easier by defining a step-wise transformation process that translates a given initial code into a semantically equivalent final code, but adapted to a specific platform. Program transformation systems require the definition of efficient transformation strategies to tackle the combinatorial problem that emerges due to the large set of transformations applicable at each step of the process. In this paper we propose a machine learning-based approach to learn heuristics to define program transformation strategies. Our approach proposes a novel combination of reinforcement learning and classification methods to efficiently tackle the problems inherent to this type of systems. Preliminary results demonstrate the suitability of this approach.<br/> &copy; G. Vigueras, M. Carro, S. Tamarit & J. Mari&ntilde;o.","Vigueras, Guillermo and Carro, Manuel and Tamarit, Salvador and Marino, Julio",2017,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR9"
"Query expansion based on statistical learning from code changes","Thesaurus-based, code-related, and software-specific query expansion techniques are the main contributions in free-form query search. However, these techniques still could not put the most relevant query result in the first position because they lack the ability to infer the expansion words that represent the user needs based on a given query. In this paper, we discover that code changes can imply what users want and propose a novel query expansion technique with code changes (QECC). It exploits (changes, contexts) pairs from changed methods. On the basis of statistical learning from pairs, it can infer code changes for a given query. In this way, it expands a query with code changes and recommends the query results that meet actual needs perfectly. In addition, we implement InstaRec to perform QECC and evaluate it with 195 039 change commits from GitHub and our code tracker. The results show that QECC can improve the precision of 3 code search algorithms (ie, IR, Portfolio, and VF) by up to 52% to 62% and outperform the state-of-the-art query expansion techniques (ie, query expansion based on crowd knowledge and CodeHow) by 13% to 16% when the top 1 result is inspected.<br/> Copyright &copy; 2018 John Wiley & Sons, Ltd.","Huang, Qing and Yang, Yangrui and Zhan, Xue and Wan, Hongyan and Wu, Guoqing",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Evaluating QR code case studies using a mobile learning framework","The aim of this study was to evaluate the feasibility of Quick Response (QR) codes and mobile devices in the context of Finnish basic education. The feasibility was analyzed through a mobile learning framework, which includes the core characteristics of mobile learning. The study is part of a larger research where the aim is to develop a theoretical framework for mobile learning and mobile learning practices. QR codes were chosen in particular because teachers were interested in seeing how this relatively easy and versatile technology could be utilized in their classrooms. QR code implementations blended in and enriched the traditional teaching methods and classroom learning in a motivating and meaningful way. The core characteristics of mobile leaning were realized comparatively well. However, the study also indicated that the factors that should be added to the framework are the school curriculum, ICT integration strategies, teacher competencies, and technological, social and cultural change.<br/> &copy; 2014 IADIS.","Rikala, Jenni",2014,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Research of speech amplitude distribution based on hadamard transformation","In view of PCM (Pulse Code Modulation) of speech signal, this paper puts forward a method of speech processing based on hadamard matrix transformation to change the amplitude distribution of speech signal, which can reduce the standard deviation of speech signal. Experiments show that the hadamard matrix transformation algorithm can obviously reduce the amplitude range of speech signal. Speech signal standard deviation is reduced by 20% after the transformation. At the same time, speech quality after decoding is not decreased according to listening experimenter. The algorithm reduces amplitude range and standard deviation of the speech signal, which can code the speech signal with less bits, and compression efficiency can be further improved.<br/> &copy; ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2017.","Tu, Jingxue and Xu, Jingyun and Zhao, Xiaoqun",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"NESC Handbook, Premier Edition - Spanish version - A Discussion of the National Electrical Safety Code(R) plus COMENTARIOS en ESPAÑOL","Scope: The 2017 NESC&reg; Handbook, Premier Edition - Spanish version, is essential in Spanish speaking markets and assist in areas that need well-defined regulations related to electrical safety for power, energy transmission and distribution, and communications industries. Important to power and energy engineers, as well as to communications providers, such as internet, phone et al. The handbook includes text directly from Code in English which provides users an easy reference back to the code, ruling-by-ruling. It gives users insight into what lies behind the NESC's rules and how to apply them. The Handbook was developed for use at many levels in the electric and communication industries, including those involved in system design, construction, maintenance, inspection, standards development and worker training. The Handbook also discusses how the NESC Committee has developed the rules in the Code and responded to change proposals during the past 100 years. This allows users to understand how questions they may have were dealt with in the past. The Premier Edition includes: These are key points from the 2017 Handbook Edition: &bull; Revising the purpose rule to include only the safeguarding of persons and utility facilities and clarifying the application rules. &bull; Deleting unused definitions and adding definitions for communication and supply space. &bull; Revising the substation impenetrable fence requirements. &bull; Adding an exception to exempt underground cable grounding requirements from the 4 grounds in each mile rule under certain conditions. &bull; Revising and reorganizing the guy insulator placement rules along with eliminating the voltage transfer requirements associated with them. &bull; Requiring a 40 vertical clearance from communication cables in the communication space if a luminaire is not effectively grounded. &bull; Deleting the conductance requirement for underground insulating jacketed grounded neutral supply cables and revising the grounding and bonding rules for supply and communication cables in random separation installations. &bull; Revising and reorganizing the Grades of Construction Table 242-1 that will now include service drops. &bull; Revising the strength rules to require that all conductors be considered for damage due to Aeolian vibration. &bull; Revising the rules in Part 4 to align with changes made to 29 CFR by the Occupational Safety and Health Administration (OSHA).<br/> &copy; 2018 IEEE.",,2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Exploring Open Source for Machine Learning Problem on Diabetic Retinopathy","Open-source operating system, as well as its packages, is more powerful and secure than the proprietary sources. In the proprietary source, software source code is not easily available because it is secret; by contrast in the open-source operating system source code is easily available, so any programmer can change the code and implement their ideas and modify it because of its openness. Also, one major advantage is that we do not need to spend a huge amount of money on the software. So, in this paper, we used open-source software for coding purposes and looked at the data available on the UCI machine learning repository on the diabetic retinopathy.<br/> &copy; 2019, Springer Nature Singapore Pte Ltd.","Kumari, Archana and Choudhury, Tanupriya and Chitra Rajagopal, P.",2019,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"A fully-neoclassical finite-orbit-width version of the CQL3D Fokker-Planck code","The time-dependent bounce-averaged CQL3D flux-conservative finite-difference Fokker-Planck equation (FPE) solver has been upgraded to include finite-orbit-width (FOW) capabilities which are necessary for an accurate description of neoclassical transport, losses to the walls, and transfer of particles, momentum, and heat to the scrape-off layer. The FOW modifications are implemented in the formulation of the neutral beam source, collision operator, RF quasilinear diffusion operator, and in synthetic particle diagnostics. The collisional neoclassical radial transport appears naturally in the FOW version due to the orbit-averaging of local collision coefficients coupled with transformation coefficients from local (R, Z) coordinates along each guiding-center orbit to the corresponding midplane computational coordinates, where the FPE is solved. In a similar way, the local quasilinear RF diffusion terms give rise to additional radial transport of orbits. We note that the neoclassical results are obtained for 'full' orbits, not dependent on a common small orbit-width approximation. Results of validation tests for the FOW version are also presented.<br/> &copy; 2016 IOP Publishing Ltd.","Petrov, Yu.V. and Harvey, R.W.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"SMPLearner: learning to predict software maintainability","Accurate and practical software maintainability prediction enables organizations to effectively manage their maintenance resources and guide maintenance-related decision making. This paper presents SMPLearner, an automated learning-based approach to train maintainability predictors by harvesting the actual average maintenance effort computed from the code change history as well as employing a much richer set of 44 four-level hierarchical code metrics collected by static code analysis tools. We systematically evaluated SMPLearner on 150 observations partitioned from releases of eight large scale open source software systems. Our evaluation showed that SMPLearner not only outperformed the traditional 4-metric MI model but also the recent learning-based maintainability predictors constructed based on single Class-level metrics, demonstrating that single Class-level metrics were not sufficient for maintainability prediction.<br/> &copy; 2014, Springer Science+Business Media New York.","Zhang, Wei and Huang, LiGuo and Ng, Vincent and Ge, Jidong",2014,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Assigning change requests to software developers","The paper presents an approach to recommend a ranked list of expert developers to assist in the implementation of software change requests (e.g., bug reports and feature requests). An Information Retrieval (IR)-based concept location technique is first used to locate source code entities, e.g., files and classes, relevant to a given textual description of a change request. The previous commits from version control repositories of these entities are then mined for expert developers. The role of the IR method in selectively reducing the mining space is different from previous approaches that textually index past change requests and/or commits. The approach is evaluated on change requests from three open-source systems: ArgoUML, Eclipse, and KOffice, across a range of accuracy criteria. The results show that the overall accuracies of the correctly recommended developers are between 47 and 96% for bug reports, and between 43 and 60% for feature requests. Moreover, comparison results with two other recommendation alternatives show that the presented approach outperforms them with a substantial margin. Project leads or developers can use this approach in maintenance tasks immediately after the receipt of a change request in a free-form text. &copy; 2011 John Wiley &amp; Sons, Ltd.<br/>","Kagdi, Huzefa and Gethers, Malcom and Poshyvanyk, Denys and Hammad, Maen",2012,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Enhancing frequency based change proneness prediction method using artificial bee colony algorithm","In the field of software engineering, during the development of Object Oriented (OO) software, the knowledge of the classes which are more prone to changes in software is an important problem that arises nowadays. In order to solve this problem, several methods were introduced by predicting the changes in the software earlier. But those methods are not facilitating very good prediction result. This research work proposes a novel approach for predicting changes in software. Our proposed probabilistic approach uses the behavioral dependency generated from UML diagrams, as well as other code metrics such as time and trace events generated from source code. These measures combined with frequency of method calls and popularity can be used in automated manner to predict a change prone class. Thus all these five features (time, trace events, behavioral dependency, frequency and popularity) are obtained from our proposed work. Then, these features are given as the input to the ID3 (Interactive Dichotomizer version 3) decision tree algorithm for effectively classifying the classes, whether it predicts the change proneness or not. If a class is classified into prediction of change prone class, then the value of change proneness is also obtained by our work.<br/> &copy; Springer International Publishing Switzerland 2015.","Godara, Deepa and Singh, Rakesh Kumar",2015,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"An empirical study of the relation between strong change coupling and defects using history and social metrics in the apache aries project","Change coupling is an implicit relationship observed when artifacts change together during software evolution. The literature leverages change coupling analysis for several purposes. For example, researchers discovered that change coupling is associated with software defects and reveals relationships between software artifacts that cannot be found by scanning code or documentation. In this paper, we empirically investigate the strongest change couplings from the Apache Aries project to characterize and identify their impact in software development. We used historical and social metrics collected from commits and issue reports to build classification models to identify strong change couplings. Historical metrics were used because change coupling is a phenomenon associated with recurrent co-changes found in the software history. In turn, social metrics were used because developers often interact with each other in issue trackers to accomplish the tasks. Our classification models showed high accuracy, with 70-99% F-measure and 88-99% AUC. Using the same set of metrics, we also predicted the number of future defects for the artifacts involved in strong change couplings. More specifically, we were able to predict 45.7% of defects where these strong change couplings reoccurred in the post-release. These findings suggest that developers and projects managers should detect and monitor strong change couplings, because they can be associated with defects and tend to happen again in the subsequent release.<br/> &copy; IFIP International Federation for Information Processing 2015.","Wiese, Igor Scaliante and Kuroda, Rodrigo Takashi and Re, Reginaldo and Oliva, Gustavo Ansaldi and Gerosa, Marco Aurelio",2015,"[""Engineering Village""]","Aceito: CA5, CA6","Aceito: CA5, CA6"
"The case for adaptive change recommendation","As the complexity of a software system grows, it becomes increasingly difficult for developers to be aware of all the dependencies that exist between artifacts (e.g., files or methods) of the system. Change impact analysis helps to overcome this problem, as it recommends to a developer relevant source-code artifacts related to her current changes. Association rule mining has shown promise in determining change impact by uncovering relevant patterns in the system's change history. State-of-the-art change impact mining algorithms typically make use of a change history of tens of thousands of transactions. For efficiency, targeted association rule mining focuses on only those transactions potentially relevant to answering a particular query. However, even targeted algorithms must consider the complete set of relevant transactions in the history. This paper presents ATARI, a new adaptive approach to association rule mining that considers a dynamic selection of the relevant transactions. It can be viewed as a further constrained version of targeted association rule mining, in which as few as a single transaction might be considered when determining change impact. Our investigation of adaptive change impact mining empirically studies seven algorithm variants. We show that adaptive algorithms are viable, can be just as applicable as the start-of-the-art complete-history algorithms, and even outperform them for certain queries. However, more important than the direct comparison, our investigation lays necessary groundwork for the future study of adaptive techniques and their application to challenges such as the on-the-fly style of impact analysis that is needed at the GitHub-scale.<br/> &copy; 2018 IEEE.","Pugh, Sydney and Binkley, David and Moonen, Leon",2018,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"TrueGrid: Code the table, tabulate the data","Spreadsheet systems are live programming environments. Both the data and the code are right in front you, and if you edit either of them, the effects are immediately visible. Unfortunately, spreadsheets lack mechanisms for abstraction, such as classes, function definitions etc. Programming languages excel at abstraction, but most mainstream languages or integrated development environments (IDEs) do not support the interactive, live feedback loop of spreadsheets. As a result, exploring and testing of code is cumbersome and indirect. In this paper we propose a method to bring both worlds closer together, by juxtaposing ordinary code and spreadsheet-like grids in the IDE, called TrueGrid. Using TrueGrid spreadsheet cells can be programmed with a fully featured programming language. Spreadsheet users then may enjoy benefits of source code, including added abstractions, syntax highlighting, version control, etc. On the other hand, programmers may leverage the grid for interactive exploring and testing of code. We illustrate these benefits using a prototype implementation of True- Grid that runs in the browser and uses Javascript as a programming language.<br/> &copy; Springer International Publishing AG 2016.","Hermans, Felienne and van der Storm, Tijs",2016,"[""Engineering Village""]","Rejeitado: CR8","Rejeitado: CR8"
"The effects of thermistor linearization techniques on the T-history characterization of phase change materials","Phase Change Materials (PCMs) are increasingly being used in the area of energy sustainability. Thermal characterization is a prerequisite for any reliable utilization of these materials. Current characterization methods including the well-known T-history method depend on accurate temperature measurements. This paper investigates the impact of different thermistor linearization techniques on the temperature uncertainty in the T-history characterization of PCMs. Thermistor sensors and two linearization techniques were evaluated in terms of achievable temperature accuracy through consideration of both, non-linearity and self-heating errors. T-history measurements of RT21 (RUBITHERM<sup>&reg;</sup>GmbH) PCM were performed. Temperature measurement results on the RT21 sample suggest that the Serial-Parallel Resistor (SPR)<sup>1</sup>linearization technique gives better uncertainty (less than &plusmn;0.1&deg;C) in comparison with the Wheatstone Bridge (WB)<sup>1</sup>technique (up to &plusmn;1.5&deg;C). These results may considerably influence the usability of latent heat storage density of PCMs in the certain temperature range. They could also provide a solid base for the development of a T-history measuring device. &copy; 2012 Elsevier Ltd. All rights reserved.<br/>","Stankovic, Stanislava B. and Kyriacou, Panayiotis A.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Improved measurement technique for the characterization of organic and inorganic phase change materials using the T-history method","In the past decade, the interest in phase change materials (PCMs) has grown significantly due to their ability to store large amounts of thermal energy in relatively small temperature intervals. Accurate knowledge of thermo-physical properties is a prerequisite for any reliable utilization of these materials. The T-history method is widely used for the investigation of PCM. This paper presents an improved measurement technique for the characterization of PCM using the T-history method. The suggested improvements include the arrangements made in three different prospects: the experimental setup, data processing and data representation. T-history measurements of organic RT21 and inorganic SP22 A17 (RUBITHERM&reg; GmbH) PCM were performed. The applied arrangements resulted in the temperature accuracy of &plusmn;0.3. &deg;C and the reduction of uncertainty associated with heat stored/released between the cooling and heating measurements. The obtained results showed some important aspects of the T-history PCM investigation and could provide more effective design and development process of the thermal energy storage systems based on the investigated materials. &copy; 2013 Elsevier Ltd.<br/>","Stankovic, Stanislava B. and Kyriacou, Panayiotis A.",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Efficient image-aware version control systems using GPU","Version control is considered to be a vital component for supporting professional software development. While it has been widely used for textual artifacts, such as source code or documentation, little attention has been given to binary artifacts. This omission can place huge restrictions on projects in the game and media industries as they contain large amounts of binary data, such as images, videos, three-dimensional models, and animations, along with their source code. For these kinds of artifacts, existing strategies such as storing the file as a whole for each revision or saving conventional binary deltas consume significant storage space with duplicate data and, even worse, do not provide any understandable information on which modifications were made. As a response to this problem, this paper introduces a change-set model infrastructure to support version control of image artifacts using a specialized data structure. Additionally, our approach can deal with the maintenance of duplicate nearly identical images through a merge operation. Because of the amount of data that has to be processed, we designed our solution based on a parallel architecture, which permits a massively parallel approach to version control. The paper also compares our approach with some popular open-source version control systems, showing their repository growth in relation to ours as well as the time required to process image artifacts. Finally, we demonstrate that our architecture requires less storage space and runs much faster than current methods. Copyright &copy; 2015 John Wiley &amp; Sons, Ltd.<br/> Copyright &copy; 2015 John Wiley & Sons, Ltd.","da Silva Junior, Jose Ricardo and Clua, Esteban and Murta, Leonardo",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Using version control system to construct ownership architecture documentations","Ownership architecture was usually constructed by investigating the comments at the top of source files. That is, to associate developer names with source files is to examine the comments manually. If such documentation can be produced automatically, it will be more immediate to indicate the status of the project. This research focus on the logs in the version control system. The data within version control logs is in a regular form and information can be retrieved quickly. The importance of developers can also be estimated by the number of own files and frequency of making a change. In order to understand the system architecture, the directory structure of source code can be used to identify function components of the system essentially. The source files in a directory implement the same function component, and the owners of these source files can be considered a team. Using the documents, researcher can know the ownership architecture and more information about the status of the project. &copy; 2013 Springer-Verlag GmbH.<br/>","Huang, Po-Han and Yeh, Dowming and Lee, Wen-Tin",2013,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow","We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.<br/> &copy; 1995-2012 IEEE.","Wongsuphasawat, Kanit and Smilkov, Daniel and Wexler, James and Wilson, Jimbo and Mane, Dandelion and Fritz, Doug and Krishnan, Dilip and Viegas, Fernanda B. and Wattenberg, Martin",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Recommending program transformations: Automating repetitive software changes","Adding features and fixing bugs in software often require systematic edits which are similar but not identical changes to multiple code locations. Finding all relevant locations and making the correct edits is a tedious and error-prone process. This chapter presents several state-of-the art approaches to recommending program transformation in order to automate repetitive software changes. First, it discusses programming-by-demonstration (PBD) approaches that automate repetitive tasks by inferring a generalized action script from a user&rsquo;s recorded actions. Second, it presents edit location suggestion approaches that only recommend candidate edit locations but do not apply necessary code transformations. Finally, it describes program transformation approaches that take code examples or version histories as input, automatically identify candidate edit locations, and apply context awareness, customization program transformations to generate a new program version. In particular, this chapter describes two concrete example-based program transformation approaches in detail, Sydit and Lase. These two approaches are selected for an in-depth discussion, because they handle the issue of both recommending change locations and applying transformations, and they are specifically designed to update programs as opposed to regular text documents. The chapter is then concluded with open issues and challenges of recommending program transformations.<br/> &copy; Springer-Verlag Berlin Heidelberg 2014.","Kim, Miryung and Meng, Na",2014,"[""Engineering Village""]","Aceito: CA3, CA0","Aceito: CA3"
"Understanding systematic and collaborative code changes by mining evolutionary trajectory patterns","The life cycle of a large-scale software system can undergo many releases. Each release often involves hundreds or thousands of revisions committed by many developers over time. Many code changes are made in a systematic and collaborative way. However, such systematic and collaborative code changes are often undocumented and hidden in the evolution history of a software system. It is desirable to recover commonalities and associations among dispersed code changes in the evolutionary trajectory of a software system. In this paper, we present Summarizing Evolutionary Trajectory by Grouping and Aggregation (SETGA), an approach to summarizing historical commit records as trajectory patterns by grouping and aggregating relevant code changes committed over time. The SETGA extracts change operations from a series of commit records from version control systems. It then groups extracted change operations by their common properties from different dimensions such as change operation types, developers, and change locations. After that, SETGA aggregates relevant change operation groups by mining various associations among them. We implement SETGA and conduct an empirical study with 3 open-source systems. We investigate underlying evolution rules and problems that can be revealed by the identified patterns and analyze the evolution of trajectory patterns in different periods. The results show that SETGA can identify various types of trajectory patterns that are useful for software evolution management and quality assurance.<br/> Copyright &copy; 2017 John Wiley & Sons, Ltd.","Jiang, Qingtao and Peng, Xin and Wang, Hai and Xing, Zhenchang and Zhao, Wenyun",2017,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"What Strokes to Modify in the Painting? Code Changes Prediction for Object-Oriented Software","Software systems shall evolve to fulfill users&rsquo; increasingly various and sophisticated needs. As they become larger and more complex, the corresponding testing and maintenance have become a practical research challenge. In this paper, we employ an approach that can identify the change-proneness in the source code of new object-oriented software releases and predict the corresponding change sizes. We first define two metrics, namely Class Change Metric and Change Size Metric, to describe the features and sizes of code changes. A new software release may be based on several previous releases. Thus, we employ an Entropy Weight Method to calculate the best window size for determining the number of previous releases to use in the prediction of change-proneness in the new release. Based on a series of change evolution matrices, a code change prediction approach is proposed based on the Gauss Process Regression (GPR) algorithm. Experiments are conducted on 17 software systems collected from GitHub to evaluate our prediction approach. The results show that our approach outperforms three existing state-of-the-art approaches with significantly higher prediction accuracy.<br/> &copy; 2018, Springer Nature Switzerland AG.","Zhang, Dinan and Chen, Shizhan and He, Qiang and Feng, Zhiyong and Huang, Keman",2018,"[""Engineering Village""]","Aceito: CA5, CA0, CA6","Aceito: CA5"
"The impact of version control operations on the quality change of the source code","The number of software systems under development and maintenance is rapidly increasing. The quality of a system's source code tends to decrease during its lifetime which is a problem because maintaining low quality code consumes a big portion of the available efforts. In this research we investigated one aspect of code change, the version control commit operations (add, update, delete). We studied the impact of these operations on the maintainability of the code. We calculated the ISO/IEC 9126 quality attributes for thousands of revisions of an industrial and three open-source software systems. We also collected the cardinality of each version control operation type for every investigated revision. Based on these data, we identified that operation Add has a rather positive, while operation Update has a rather negative effect on the quality. On the other hand, for operation Delete we could not find a clear connection to quality change. &copy; 2014 Springer International Publishing.<br/>","Farago, Csaba and Hegedus, Peter and Ferenc, Rudolf",2014,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"A token oriented measurement method of source code similarity","In order to help teachers to identify plagiarism in student assignment submissions among students&rsquo; Source code quickly and accurately, this paper discusses a measurement method of Source code similarity. In the proposed algorithm, firstly, both of token oriented edit distance (TD) and token oriented length of longest common subsequence (TLCSLen) is calculated; secondly, considering the TD and TLCSLen, a similarity calculation formula is given to measure similarity of Source code; Thirdly, a dynamic and variable similarity threshold is set to determine whether there is plagiarism between Source codes, which ensure a relatively reasonable judgment of plagiarism. This method has been applied to the university's programming course work online submission system and online examination system. Practical application results show that this method can identify similar Source code timely, effectively and accurately. &copy; (2014) Trans Tech Publications, Switzerland.","Zhu, Hong Mei and Zhang, Liang and Sun, Wei and Sun, Yong Xiang",2014,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Predicting bugs in source code changes with incremental learning method","Software is constructed by a series of changes and each change has the risk to introduce bugs. Predicting the existence of bugs in source code changes could help developers detect and fix bugs immediately upon the completion of a change, which accelerates the bug fixing process and save the limited time and human resources effectively. However, because of altering nature in the underlying bug generation process, the concept used to depict the bugintroducing patterns is drifting, which makes it difficult to predict latent bugs of source code changes accurately, especially in the long-term prediction scenario. In order to deal with this problem, a feature-based incremental learning framework is proposed. It is comprised of three components: (1) an incremental discretization method, which is used to transform the quantitive features in the corpus incrementally, (2) an incremental feature selection method, which is always keeping a subset with the most informative features, and (3) an incremental classification algorithm, which updates the classifier dynamically and considers the current best subset of features during prediction. This proposed approach is evaluated on three famous open source systems, Eclipse, Mozilla and jedit. The results show that our approach performs better than the non-incremental method in dealing with concept drift, with the consideration of keeping the value of both precision and recall stable at a suitable level over time. We also implement a prototype with this learning framework and apply it to a real software development scenario. &copy; 2013 ACADEMY PUBLISHER.<br/>","Yuan, Zi and Yu, Lili and Liu, Chao and Zhang, Linghua",2013,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Connection between version control operations and quality change of the source code","Software erosion is a well-known phenomena, meaning that software quality is continuously decreasing due to the ever-ongoing modifications in the source code. In this research work we investigated this phenomena by studying the impact of version control commit operations (add, update, delete) on the quality of the code. We calculated the ISO/IEC 9126 quality attributes for thousands of revisions of an industrial and three open-source software systems with the help of the Columbus Quality Model. We also collected the cardinality of each version control operation type for every investigated revision. We performed Chisquared tests on contingency tables with rows of quality change and columns of version control operation commit types. We compared the results with random data as well. We identified that the relationship between the version control operations and quality change is quite strong. Great maintainability improvements are mostly caused by commits containing Add operation. Commits containing file updates only tend to have a negative impact on the quality. Deletions have a weak connection with quality, and we could not formulate a general statement.<br/>","Farago, Csaba and Hegedus, Peter and Vegh, adam Zoltan and Ferenc, Rudolf",2014,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Bug prediction for fine-grained source code changes","Software is constructed by a series of changes and each change has a risk of introducing bugs. Building bug prediction models for software changes can help developers know the existence of bugs immediately upon the completion of the change, which allows them to allocate more resources of testing and inspecting on the current risky changes, and to find and fix the introduced bugs timely. In this paper, we present a bug prediction model for fine-grained source code changes based on machine learning method, which takes a fine-grained source code change as a learning instance and a series of properties of the fine-grained change as features. This model has two desirable qualities: 1) Compared with previous research work that building bug prediction models for software changes at the file level or commit level (including one or more files), this model can predict bugs for changes at the statement level, which increases the granularity of prediction and thus reduces manual inspection efforts for developers. 2) This model can help developers or managers gain better knowledge on key factors of bug injection and provide guidance for software change of high quality. From the experiments on 8 famous open source projects, we observe that when using Random Forest as the classifier, the model proposed in this paper achieves the best performance, which can predict bugs for fine-grained source code changes with 78% precision, 71% recall, and 75% F-measure on average. Furthermore, among all the four feature groups (i.e. where, what, who, and when) defined in this paper, where is most influential, which has the strongest discriminative power in predicting bugs.<br/> Copyright &copy; 2013 by Knowledge Systems Institute Graduate School.","Yuan, Zi and Yu, Lili and Liu, Chao",2013,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Comparative study between two approaches using edit operations and code differences to detect past refactorings","Understanding which refactoring transformations were performed is in demand in modern software constructions. Traditionally, many researchers have been tackling understanding code changes with history data derived from version control systems. In those studies, problems of the traditional approach are pointed out, such as entanglement of multiple changes. To alleviate the problems, operation histories on IDEs' code editors are available as a new source of software evolution data nowadays. By replaying such histories, we can investigate past code changes in a fine-grained level. However, the prior studies did not provide enough evidence of their effectiveness for detecting refactoring transformations. This paper describes an experiment in which participants detect refactoring transformations performed by other participants after investigating the code changes with an operation-replay tool and diff tools. The results show that both approaches have their respective factors that pose misunderstanding and overlooking of refactoring transformations. Two negative factors on divided operations and generated compound operations were observed in the operation-based approach, whereas all the negative factors resulted from three problems on tangling, shadowing, and out-of-order of code changes in the difference-based approach. This paper also shows seven concrete examples of participants' mistakes in both approaches. These findings give us hints for improving existing tools for understanding code changes and detecting refactoring transformations.<br/> &copy; 2018 The Institute of Electronics, Information and Communication Engineers.","Omori, Takayuki and Maruyama, Katsuhisa",2018,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Unravel programming sessions with THRESHER: Identifying coherent and complete sets of fine-granular source code changes","Development teams benefit from version control systems, which manage shared access to code repositories and persist entire project histories for analysis or recovery. Such systems will be efficient if developers commit coherent and complete change sets. These best practices, however, are difficult to follow because multiple activities often interleave without notice and existing tools impede unraveling changes before committing them. We propose an interactive, graphical tool, called Thresher, that employs adaptable scripts to support developers to group and commit changes-especially for fine-granular change tracking where numerous changes are logged even in short programming sessions. We implemented our tool in Squeak/Smalltalk and derived a foundation of scripts from five refactoring sessions. We evaluated those scripts' precision and recall, which indicate a reduced manual effort because developers can focus on project-specific adjustments. Having such an interactive approach, they can easily intervene to accurately reconstruct activities and thus follow best practices.<br/> &copy; 2017, Japan Society for Software Science and Technology. All rights reserved.","Taeumel, Marcel and Platz, Stephanie and Steinert, Bastian and Hirschfeld, Robert and Masuhara, Hidehiko",2017,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Bug prediction method for fine-grained source code changes","Software changes constantly in its lifecycle to adapt to the changing requirements and environments. In order to predict whether each change will introduce any bugs timely, various bug prediction methods for software source code changes have been proposed by researchers. However, there are three deficiencies in existing methods: 1) The prediction granularities are limited at the coarse-grained levels (i.e. transaction or file levels); 2) As vector space model is used to represent software changes, abundant information in software repositories, such as program structure, natural language semantic and history information, can not be mined sufficiently; 3) Only short-time prediction is explored without considering the concept drift caused by new requirements, team restructuring or other external factors during the long time software evolution process. In order to overcome the shortcomings of existing methods, a bug prediction method for source code changes is proposed. It makes prediction for fine-grained (i.e. statement level) changes, which reduces the quality assurance cost effectively. By in-depth mining software repositories with static program analysis and natural language semantic inference technologies, feature sets of changes are constructed in four aspects (i.e. context, content, time, and developer) and key factors that lead to bug injection are revealed. Characteristics of concept drift in software evolution process are analyzed by using matrix of feature entropy difference, and an algorithm of adaptive window with concept reviewing is proposed to achieve stability of long-time prediction. Experiments on six famous open source projects demonstrate effectiveness of the proposed method.<br/> &copy; 2014 ISCAS.","Yuan, Zi and Yu, Li-Li and Liu, Chao",2014,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"The storyteller version control system tackling version control, code comments, and team learning","This demonstration shows the Storyteller version control system. The tool aims to change the way software developers learn by opening up for examination how they do their work. The tool has traditional version control functionality (branching and merging) but in addition it records how development work is done, organizes it, and allows it to be played back for others. Most importantly, the tool allows developers to tell stories about what they did and why. It captures and organizes institutional knowledge that would otherwise be lost.<br/>","Mahoney, Mark",2012,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Slicing fine-grained code change history","Change-aware development environments can automatically record fine-grained code changes on a program and allow programmers to replay the recorded changes in chronological order. However, since they do not always need to replay all the code changes to investigate how a particular entity of the program has been changed, they often eliminate several code changes of no interest by manually skipping them in replaying. This skipping action is an obstacle that makes many programmers hesitate when they use existing replaying tools. This paper proposes a slicing mechanism that automatically removes manually skipped code changes from the whole history of past code changes and extracts only those necessary to build a particular class member of a Java program. In this mechanism, fine-grained code changes are represented by edit operations recorded on the source code of a program and dependencies among edit operations are formalized. The paper also presents a running tool that slices the operation history and replays its resulting slices. With this tool, programmers can avoid replaying nonessential edit operations for the construction of class members that they want to understand. Experimental results show that the tool offered improvements over conventional replaying tools with respect to the reduction of the number of edit operations needed to be examined and over history filtering tools with respect to the accuracy of edit operations to be replayed.<br/> Copyright &copy; 2016 The Institute of Electronics, Information and Communication Engineers.","Maruyama, Katsuhisa and Omori, Takayuki and Hayashi, Shinpei",2016,"[""Engineering Village""]","Aceito: CA3","Aceito: CA3"
"Learning to accelerate symbolic execution via code transformation","Symbolic execution is an effective but expensive technique for automated test generation. Over the years, a large number of refined symbolic execution techniques have been proposed to improve its efficiency. However, the symbolic execution efficiency problem remains, and largely limits the application of symbolic execution in practice. Orthogonal to refined symbolic execution, in this paper we propose to accelerate symbolic execution through semantic-preserving code transformation on the target programs. During the initial stage of this direction, we adopt a particular code transformation, compiler optimization, which is initially proposed to accelerate program concrete execution by transforming the source program into another semantic-preserving target program with increased efficiency (e.g., faster or smaller). However, compiler optimizations are mostly designed to accelerate program concrete execution rather than symbolic execution. Recent work also reported that unified settings on compiler optimizations that can accelerate symbolic execution for any program do not exist at all. Therefore, in this work we propose a machine-learning based approach to tuning compiler optimizations to accelerate symbolic execution, whose results may also aid further design of specific code transformations for symbolic execution. In particular, the proposed approach LEO separates source-code functions and libraries through our program-splitter, and predicts individual compiler optimization (i.e., whether a type of code transformation is chosen) separately through analyzing the performance of existing symbolic execution. Finally, LEO applies symbolic execution on the code transformed by compiler optimization (through our local-optimizer). We conduct an empirical study on GNU Coreutils programs using the KLEE symbolic execution engine. The results show that LEO significantly accelerates symbolic execution, outperforming the default KLEE configurations (i.e., turning on/off all compiler optimizations) in various settings, e.g., with the default training/testing time, LEO achieves the highest line coverage in 50/68 programs, and its average improvement rate on all programs is 46.48%/88.92% in terms of line coverage compared with turning on/off all compiler optimizations.<br/> &copy; Junjie Chen, Wenxiang Hu, Lingming Zhang, Dan Hao, Sarfraz Khurshid, and Lu Zhang.","Chen, Junjie and Hu, Wenxiang and Zhang, Lingming and Hao, Dan and Khurshid, Sarfraz and Zhang, Lu",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Identifying change patterns in software history","Traditional algorithms for detecting differences in source code focus on differences between lines. As such, little can be learned about abstract changes that occur over time within a project. Structural differencing on the program's abstract syntax tree reveals changes at the syntactic level within code, which allows us to further process the differences to understand their meaning. We propose that grouping of changes by some metric of similarity, followed by pattern extraction via antiunification will allow us to identify patterns of change within a software project from the sequence of changes contained within a Version Control System (VCS). Tree similarity metrics such as a tree edit distance can be used to group changes in order to identify groupings that may represent a single class of change (e.g., adding a parameter to a function call). By applying antiunification within each group we are able to generalize from families of concrete changes to patterns of structural change. Studying patterns of change at the structural level, instead of line-by-line, allows us to gain insight into the evolution of software.<br/>","Dagit, Jason and Sottile, Matthew",2013,"[""Engineering Village""]","Aceito: CA1","Aceito: CA1"
"CodeAttention: translating source code to comments by exploiting the code constructs","Appropriate comments of code snippets provide insight for code functionality, which are helpful for program comprehension. However, due to the great cost of authoring with the comments, many code projects do not contain adequate comments. Automatic comment generation techniques have been proposed to generate comments from pieces of code in order to alleviate the human efforts in annotating the code.Most existing approaches attempt to exploit certain correlations (usually manually given) between code and generated comments, which could be easily violated if coding patterns change and hence the performance of comment generation declines. In addition, recent approaches ignore exploiting the code constructs and leveraging the code snippets like plain text. Furthermore, previous datasets are also too small to validate the methods and show their advantage. In this paper, we propose a new attention mechanism called CodeAttention to translate code to comments, which is able to utilize the code constructs, such as critical statements, symbols and keywords. By focusing on these specific points, CodeAttention could understand the semantic meanings of code better than previous methods. To verify our approach in wider coding patterns, we build a large dataset from open projects in GitHub. Experimental results in this large dataset demonstrate that the proposed method has better performance over existing approaches in both objective and subjective evaluation. We also perform ablation studies to determine effects of different parts in CodeAttention.<br/> &copy; 2018, Higher Education Press and Springer-Verlag GmbH Germany, part of Springer Nature.","Zheng, Wenhao and Zhou, Hongyu and Li, Ming and Wu, Jianxin",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Machine learning techniques for code smell detection: A Systematic literature review and meta-Analysis","Background: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of machine learning techniques represents an ever increasing research area. Objective: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how machine learning approaches have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of machine learning approaches in the field of code smells. Method: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far. Results: The analyses performed show that God Class, Long Method, Functional Decomposition, and Spaghetti Code have been heavily considered in the literature. DECISION TREES and SUPPORT VECTOR MACHINES are the most commonly used machine learning algorithms for code smell detection. Models based on a large set of independent variables have performed well. JRIP and RANDOM FOREST are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future. Conclusion: Based on our findings, we argue that there is still room for the improvement of machine learning techniques in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.<br/> &copy; 2019 Elsevier B.V.","Azeem, Muhammad Ilyas and Palomba, Fabio and Shi, Lin and Wang, Qing",2019,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018","The proceedings contain 27 papers. The topics discussed include: combining obfuscation and optimizations in the real world; obfuscating java programs by translating selected portions of bytecode to native libraries; enabling the continuous analysis of security vulnerabilities with VulData7; towards anticipation of architectural smells using link prediction techniques; periodic developer metrics in software defect prediction; which method-stereotype changes are indicators of code smells?; semantics-based code search using input/output examples; detecting evolutionary coupling using transitive association rules; the case for adaptive change recommendation; and on the use of machine learning techniques towards the design of cloud based automatic code clone validation tools.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Capturing software evolution and change through code repository smells","In the last years we have seen the rise and the fall of many version control systems. These systems collect a large amount of data spanning from the path of the files involved in changes to the exact text changed in every file. This data can be exploited to produce an overview about how the system changed over time and evolved. We have developed a tool, called VCS-Analyzer, to use this information, both for data retrieval and analysis tasks. Currently, VCS-Analyzer implements six different analyses: two based on source code for the computation of metrics and the detection of code smells, and four original analysis based on repositories metadata, which are based on the concepts of Repository Metrics and Code Repository Smells. In this paper, we describe one smell and two metrics we have defined for source code repositories analysis.<br/> &copy; Springer International Publishing Switzerland 2014.","Fontana, Francesca Arcelli and Rolla, Matteo and Zanoni, Marco",2014,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Evaluation of machine learning approaches for change-proneness prediction using code smells","In the field of technology, software is an essential driver of business and industry. Software undergoes changes due to maintenance activities initiated by bug fixing, improved documentation, and new requirements of users. In software, code smells are indicators of a system which may give maintenance problem in future. This paper evaluates six types of machine learning algorithms to predict change-proneness using code smells as predictors for various versions of four Java-coded applications. Two approaches are used: method 1-random undersampling is done before Feature selection; method 2-feature selection is done prior to random undersampling. This paper concludes that gene expression programming (GEP) gives maximum AUC value, whereas cascade correlation network (CCR), treeboost, and PNN\GRNN algorithms are among top algorithms to predict F-measure, precision, recall, and accuracy. Also, GOD and L_M code smells are good predictors of software change-proneness. Results show that method 1 outperforms method 2.<br/> &copy; Springer Nature Singapore Pte Ltd. 2017.","Kaur, Kamaldeep and Jain, Shilpi",2017,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Change Coupling Between Software Artifacts: Learning from Past Changes","While mining version control systems, researchers noticed that some artifacts frequently change together throughout software development. When a certain artifact co-changes repeatedly with another, we say that the former is change coupledto the latter. Researchers have found a series of applications for change coupling in software engineering. For instance, building on the idea that artifacts that changed together in the past are likely to change together in the future, researchers developed effective change prediction mechanisms. In this chapter, we describe the concept of change coupling in more detail and present different techniques to detect it. In particular, we provide ready-to-use code you can leverage as a starting step to detect change couplings in your own projects. In the last part of the chapter, we describe some of the main applications of change coupling analysis.<br/> &copy; 2015 Elsevier Inc. All rights reserved.","Oliva, Gustavo Ansaldi and Gerosa, Marco Aurelio",2015,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Mining A change history to quickly identify bug locations: A case study of the Eclipse project","In this study, we proposed an approach to mine a change history to improve the bug localization performance. The key idea is that a recently fixed file may be fixed in the near future. We used a combination of textual feature and mining the change history to recommend source code files that are likely to be fixed for a given bug report. First, we adopted the Vector Space Model (VSM) to find relevant source code files that are textually similar to the bug report. Second, we analyzed the change history to identify previously fixed files. We then estimated the fault proneness of these files. Finally, we combined the two scores, from textual similarity and fault proneness, for every source code file. We then recommend developers examine source code files with higher scores. We evaluated our approach based on 1,212 bug reports from the Eclipse Platform and Eclipse JDT. The experimental results show that our proposed approach can improve the bug localization performance and effectively identify buggy files. &copy; 2013 IEEE.<br/>","Tantithamthavorn, Chakkrit and Teekavanich, Rattamont and Ihara, Akinori and Matsumoto, Ken-Ichi",2013,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Using discriminative feature in software entities for relevance identification of code changes","Developers often bundle unrelated changes (eg, bug fix and feature addition) in a single commit and then submit a &ldquo;poor cohesive&rdquo; commit to version control system. Such a commit consists of multiple independent code changes and makes review of code changes harder. If the code changes before commit can be identified as related and unrelated ones, the &ldquo;cohesiveness&rdquo; of a commit can be guaranteed. Inspired by the effectiveness of machine learning techniques in classification field, we model the relevance identification of code changes as a binary classification problem (ie, related and unrelated changes) and propose discriminative feature in software entities to characterize the relevance of code changes. In particular, to quantify the discriminative feature, 21 coupling rules and 4 cochanged type relationships are elaborately extracted from software entities to construct related changes vector (RCV). Twenty-one coupling rules at granularities of class, attribute, and method can capture the relevance of code changes from structural coupling dimension, and 4 cochanged type relationships are defined to capture the change type combinations of software entities that may cause related changes. Based on RCV, machine learning algorithms are applied to identify the relevance of code changes. The experiment results show that probabilistic neural network and general regression neural network provide statistically significant improvements in accuracy of relevance identification of code changes over the other 4 machine learning algorithms. Related changes vector with 72 dimensions (RCV<inf>72</inf>) outperforms other 2RCVs with less dimensions.<br/> Copyright &copy; 2017 John Wiley & Sons, Ltd.","Huang, Yuan and Chen, Xiangping and Liu, Zhiyong and Luo, Xiaonan and Zheng, Zibin",2017,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Integrating conceptual and logical couplings for change impact analysis in software","The paper presents an approach that combines conceptual and evolutionary techniques to support change impact analysis in source code. Conceptual couplings capture the extent to which domain concepts and software artifacts are related to each other. This information is derived using Information Retrieval based analysis of textual software artifacts that are found in a single version of software (e.g., comments and identifiers in a single snapshot of source code). Evolutionary couplings capture the extent to which software artifacts were co-changed. This information is derived from analyzing patterns, relationships, and relevant information of source code changes mined from multiple versions in software repositories. The premise is that such combined methods provide improvements to the accuracy of impact sets compared to the two individual approaches. A rigorous empirical assessment on the changes of the open source systems Apache httpd, ArgoUML, iBatis, KOffice, and jEdit is also reported. The impact sets are evaluated at the file and method levels of granularity for all the software systems considered in the empirical evaluation. The results show that a combination of conceptual and evolutionary techniques, across several cut-off points and periods of history, provides statistically significant improvements in accuracy over either of the two techniques used independently. Improvements in F-measure values of up to 14% (from 3% to 17%) over the conceptual technique in ArgoUML at the method granularity, and up to 21% over the evolutionary technique in iBatis (from 9% to 30%) at the file granularity were reported. &copy; 2012 Springer Science+Business Media New York.<br/>","Kagdi, Huzefa and Gethers, Malcom and Poshyvanyk, Denys",2013,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Development history granularity transformations","Development histories can simplify some software engineering tasks, butdifferent tasks require different history granularities. For example, ahistory that includes every edit that resulted in compiling code is neededwhen searching for the cause of a regression, whereas a history that containsonly changes relevant to a feature is needed for understanding the evolutionof the feature. Unfortunately, today, both manual and automated historygeneration result in a single-granularity history. This paper introduces theconcept of multi-grained development history views and the architecture ofCodebase Manipulation, a tool that automatically records a fine-grainedhistory and manages its granularity by applying granularity transformations.<br/> &copy; 2015 IEEE.","Mulu, Kivanc and Swart, Luke and Brun, Yuriy and Ernst, Michael D.",2015,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Detecting program changes based on the edit history of source code","To detect the changes of a program, the line-based difference between two versions of the program have been frequently used. In a single commitment performed on a version control system, however, multiple changes are usually intermingled. In this case, it is troublesome to untangle them. The proposed method can detect such individual changes. For this, it restores snapshots of the program from the history of editing operations for the target program and compares information on class members that results from syntax analysis for respective snapshots. Experimental results with a running tool implementing are also shown.<br/>","Kitsu, Eijirou and Omori, Takayuki and Maruyama, Katsuhisa",2012,"[""Engineering Village""]","Aceito: CA1, CA0","Aceito: CA1"
"Variance of source code quality change caused by version control operations","Software maintenance consumes huge efforts. Its cost strongly depends on the quality of the source code: an easy-to-maintain code needs much less effort than the maintenance of a more problematic one. Based on experiences, the maintainability of the source code tends to decrease during its lifetime. However, in most of the cases, this decrease is not a smooth linear one, but there are larger and smaller ups and downs, and the net root of these changes generally results in a negative tendency. Detecting common development patterns which similarly influence the maintainability could help to stop or even turn back source code erosion. In this research the scale of the ups and downs are investigated, namely that which version control operations cause bigger and which smaller changes in the maintainability. We calculated the maintainability and collected the cardinality of each version control operation for every revision of four inspected software systems. With the help of these data we checked which version control operation causes higher absolute code quality changes and which lower. We found clear connection between version control operations and the variance of the maintainability changes. File Additions and file Deletions caused significantly higher variance in maintainability changes compared to file Updates. Commits containing higher number of operations - regardless of the type of the operation - caused higher variance in maintainability changes than those commits containing lower number of operations. As a practical conclusion, it is recommended to pay special attention to the quality of commits containing new file additions, e.g. with the help of a mandatory code review.<br/>","Farago, Csaba",2015,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Running ATLAS workloads within massively parallel distributed applications using Athena Multi-Process framework (AthenaMP)","AthenaMP is a multi-process version of the ATLAS reconstruction, simulation and data analysis framework Athena. By leveraging Linux fork and copy-on-write mechanisms, it allows for sharing of memory pages between event processors running on the same compute node with little to no change in the application code. Originally targeted to optimize the memory footprint of reconstruction jobs, AthenaMP has demonstrated that it can reduce the memory usage of certain configurations of ATLAS production jobs by a factor of 2. AthenaMP has also evolved to become the parallel event-processing core of the recently developed ATLAS infrastructure for fine-grained event processing (Event Service) which allows the running of AthenaMP inside massively parallel distributed applications on hundreds of compute nodes simultaneously. We present the architecture of AthenaMP, various strategies implemented by AthenaMP for scheduling workload to worker processes (for example: Shared Event Queue and Shared Distributor of Event Tokens) and the usage of AthenaMP in the diversity of ATLAS event processing workloads on various computing resources: Grid, opportunistic resources and HPC.<br/> &copy; Published under licence by IOP Publishing Ltd.","Calafiura, Paolo and Leggett, Charles and Seuster, Rolf and Tsulaia, Vakhtang and Van Gemmeren, Peter",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Implementation of dynamic matrix control algorithm using field programmable gate array: Preliminary results","This paper describes implementation of the Dynamic Matrix Control (DMC) algorithm performed on an Altera Field Programmable Gate Array (FPGA) with the Cyclone IV chip. The DMC algorithm is implemented in its analytical (explicit) version which requires computationally simple matrix and vector operations in real time, no on-line optimisation is necessary. The test-bench application is prepared for fast comparison between C and HDL versions of code. A large number of independent logic cells can provide multi-parallel operations to achieve very fast operations. As a result, the algorithm may be used for controlling very fast dynamic processes characterised by sampling periods of millisecond order. Preliminary results of real experiments are demonstrated. The discussed control structure provides possibility to fast change of algorithm.<br/> &copy; Springer International Publishing AG 2017.","Wojtulewicz, Andrzej",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Connecting hardware and software in a middle school engineering outreach effort-RTP","Recent years have seen tremendous growth in outreach programs aimed at bringing computer programming to children and young adults via in-class and extracurricular coding activities. Programs such as the Hour of Code and Girls who Code have introduced millions of young people to programming around the world. For this study, we explored how combining programming with interactive electronics hardware can create a more engaging and dynamic learning environment for some students than what programming alone can achieve. In this paper, we describe an electrical engineering outreach effort in collaboration with the technology and engineering teacher at a local middle school. Beginning with an introduction to programming via the Hour of Code, we progressed to lessons utilizing the Sparkfun Electronics Digital Sandbox, an Arduino-compatible microcontroller board with numerous built-in sensors and outputs. Under the guidance of both a Professor of electrical and computer engineering and their own technology teacher, the students learned about the relationship between electronics hardware and software via a series of hands-on activities that culminated in a final design project. To understand the experiences of the students who participated in these activities and develop insights into the relationship between hardware and software and students' learning outcomes, we administered a survey and conducted a focus group with the students. The students described an overall positive experience, and also appreciated the ability to connect coding with the interactivity provided by the microcontroller board. The students described deriving significant satisfaction out of relatively simple tasks like programming an LED light to blink or change color. The students also overwhelmingly felt that learning about the interconnections between hardware and software gave them an understanding and better appreciation of the complexity of the electronics and computer software they interact with on a daily basis. The students generally found the programming to be the most challenging part of the activity but also rewarding, but tended to indicate activities utilizing hardware as the most engaging activity they encountered. Overall, the results of this study suggest that combined hardware and software educational activities can engage a wide number of students, help students understand the interconnectedness of these areas, and create a positive learning environment. &copy; American Society for Engineering Education, 2016.","Salzman, Noah and Loo, Sin Ming",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A guide to A/B testing tools","A/B testing can help resolve many of their quandaries related to online marketing. A/B testing is a process in which we choose the best performing version of a webpage, by randomly displaying different versions of the site to visitors and assessing the performance of each variant against a desired metric. Visual editor is used to create the variants without having to write HTML/CSS code. Multivariate testing is useful when we want to change several page elements. Multipage testing is used to test changes to a multistep shopping process in an e-commerce scenario. Segmentation is helpful if we want to run tests on certain visitors or understand if variants perform differently for different visitor segments. If the tool is easy to use, nontechnical users can run the tests on an ongoing basis without much support from the IT department. The tools use either client-side or server-side scripts to serve the appropriate variation and track performance. If A/B testing tools can integrate with such underlying systems, we cannot only conduct more fine-grained A/B tests but also can deliver more personalized web experiences to our site visitors.<br/>","Kompella, Kashyap",2015,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Mock objects for testing java systems: Why and how developers use them, and how they evolve","When testing software artifacts that have several dependencies, one has the possibility of either instantiating these dependencies or using mock objects to simulate the dependencies&rsquo; expected behavior. Even though recent quantitative studies showed that mock objects are widely used both in open source and proprietary projects, scientific knowledge is still lacking on how and why practitioners use mocks. An empirical understanding of the situations where developers have (and have not) been applying mocks, as well as the impact of such decisions in terms of coupling and software evolution can be used to help practitioners adapt and improve their future usage. To this aim, we study the usage of mock objects in three OSS projects and one industrial system. More specifically, we manually analyze more than 2,000 mock usages. We then discuss our findings with developers from these systems, and identify practices, rationales, and challenges. These results are supported by a structured survey with more than 100 professionals. Finally, we manually analyze how the usage of mock objects in test code evolve over time as well as the impact of their usage on the coupling between test and production code. Our study reveals that the usage of mocks is highly dependent on the responsibility and the architectural concern of the class. Developers report to frequently mock dependencies that make testing difficult (e.g., infrastructure-related dependencies) and to not mock classes that encapsulate domain concepts/rules of the system. Among the key challenges, developers report that maintaining the behavior of the mock compatible with the behavior of original class is hard and that mocking increases the coupling between the test and the production code. Their perceptions are confirmed by our data, as we observed that mocks mostly exist since the very first version of the test class, and that they tend to stay there for its whole lifetime, and that changes in production code often force the test code to also change.<br/> &copy; 2018, The Author(s).","Spadini, Davide and Aniche, Mauricio and Bruntink, Magiel and Bacchelli, Alberto",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"A component buildup method for canard controlled projectiles","An engineering-level method has been applied for the estimation of the normalforce and the pitching-moment coefficients induced by canard controls (or wings) on body sections that feature change of diameter. These loads are missing in most component buildup cods. Adding them produces a more complete component buildup analysis. The validation of the procedure was done by calculating the longitudinal aerodynamic characteristics of three canard-projectile configurations, using the 1997 version of the USAF Missile Datcom code. The results show that the predicted normal-force coefficient of the canard unit is larger than test data. When the induced loads, calculated by the present method, are added as corrections to the results of the code, the agreement of the corrected predictions with test data greatly improves for all benchmarks.<br/>","Sigal, Asher",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Corrigendum to Implementation of tidal turbines in MIKE 3 and Delft3D models of Pentland Firth  Orkney waters [Ocean Coast. Manag. 147 (2017) 2136](S0964569117304192)(10.1016/j.ocecoaman.2017.04.015))","The authors regret that a software error caused incorrect predictions for the effects of tidal turbines in Delft3D. The predictions without turbines are unaffected, as are those from the MIKE 3 model. The overall conclusions of the article remain valid. Figs. 12&ndash;15 as published are incorrect. Replacements for Figs. 12&ndash;14 are presented here. Following this correction the differences in the effects of energy extraction between the two models are much smaller. As a result the discussion of these differences in Section 6 should be disregarded, and Fig. 15 is no longer required. The authors would like to apologise for any inconvenience caused. The version of the code for adding turbines to Delft3D that is publicly available has been corrected, and anybody using this for their own work is urged to download the latest version. [Figure presented] Fig. 12: (a) 400 turbines in the Inner Sound, viewed through the MIKE Zero GUI; (b) The same 400 turbines represented as porous plates for Delft3D. Higher values of the c<inf>loss</inf> parameter, shown by bluer colours, indicate plates with higher drag. [Figure presented] Fig. 13: Changes in mean current speeds over 28 days as a result of adding turbines. [Figure presented] Fig. 14: Change in mean bed stress magnitude over 28 days as a result of adding turbines, expressed as a proportion of the value without turbines.<br/> &copy; 2017 Elsevier Ltd","Waldman, S. and Baston, S. and Nemalidinne, R. and Chatzirodou, A. and Venugopal, V. and Side, J.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Improved mixed oxide fuel calculations with the evaluated nuclear data library JEFF-3.2","An overestimation of the keff values for mixed oxide (MOX) fuels was identified with Monte Carlo (TRIPOLI-4) and deterministic (APOLLO2) calculations based on the Joint Evaluated Fission and Fusion (JEFF) evaluated nuclear data library. The overestimation becomes sizeable with Pu aging, reaching a reactivity change of &Delta;&rho;&sime;+700 pcm for integral measurements carried out with MOX fuel containing a large amount of americium. This bias was observed for various critical configurations performed in the zeropower reactor EOLE of the Commissariat &agrave; l&rsquo;&eacute;nergie atomique et aux &egrave;nergies alternatives (CEA), Cadarache, France. The present work focuses on the improvements achieved with the new 239Pu and 241Am evaluated nuclear data files available in the latest version of the JEFF library (JEFF-3.2). The resolved resonance range of the plutonium evaluation was reevaluated at Oak Ridge National Laboratory (ORNL), Oak Ridge, Tennessee, with the SAMMY code in collaboration with CEA Cadarache. The resonance parameters of the americium evaluation were obtained with the REFIT code in collaboration with the research institutes Institute for Reference Materials and Measurements (IRMM), Geel, Belgium, and Institut de recherche sur les lois fondamentales de l&rsquo;Univers (Irfu), Saclay, France.<br/> &copy; 2018, American Nuclear Society. All rights reserved.","Noguere, G. and Bernard, D. and Blaise, P. and Bouland, O. and Leal, L. and Leconte, P. and Litaize, O. and Peneliau, Y. and Roque, B. and Santamarina, A. and Vidal, J.-F.",2016,"[""Engineering Village""]","Rejeitado: CR9, CR9","Rejeitado: CR9"
"Information Security - 15th International Conference, ISC 2012, Proceedings","The proceedings contain 24 papers. The topics discussed include: differential attacks on reduced RIPEMD-160; on optimal bounds of small inverse problems and approximate GCD problems with higher degree; domain-specific pseudonymous signatures for the German identity card; solutions for the storage problem of McEliece public and private keys on memory-constrained platforms; 100% connectivity for location aware code based KPD in clustered WSN: merging blocks; learning fine-grained structured input for memory corruption detection; dynamic anomaly detection for more trustworthy outsourced computation; an empirical study of dangerous behaviors in Firefox extensions; collaboration-preserving authenticated encryption for operational transformation systems; additively homomorphic encryption with a double decryption mechanism, revisited; and compliance checking for usage-constrained credentials in trust negotiation systems.",,2012,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Proceedings of the Annual International Symposium on Microarchitecture, MICRO","The proceedings contain 61 papers. The topics discussed include: dictionary sharing: an efficient cache compression scheme for compressed caches; pTask: a smart prefetching scheme for OS intensive applications; data-centric execution of speculative parallel programs; towards efficient server architecture for virtualized network function deployment: implications and implementations; bridging the I/O performance gap for big data workloads: a new NVDIMM-based approach; MIMD synchronization on SIMT architectures; KLAP: kernel launch aggregation and promotion for optimizing dynamic parallelism; cache-emulated register file: an integrated on-chip memory architecture for high performance GPGPUs; Zorua: a holistic approach to resource virtualization in GPUs; Cambricon-X: an accelerator for sparse neural networks; NEUTRAMS: neural network transformation and co-design under neuromorphic hardware constraints; continuous shape shifting: enabling loop co-optimization via near-free dynamic code rewriting; CrystalBall: statically analyzing runtime behavior via deep sequence learning; low-cost soft error resilience with unified data verification and fine-grained recovery for acoustic sensor based detection; improving energy efficiency of DRAM by exploiting half page row access; concise loads and stores: the case for an asymmetric compute-memory architecture for approximation; and the bunker cache for spatio-value approximation.",,2016,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"ParaTrough v1.0: Libreria en Modelica para Simulacion de Plantas Termosolares","This paper describes a Modelica-based library developed to the modeling and simulation of solar thermal plants with parabolic trough collectors. The Dymola 6.1 environment has been used. Unlike other commercial tools, the ParaTrough library is offered as a free open source tool, under Modelica License 2. Its modular code makes it easily extensible and modifiable to the requirements of each plant and process in particular. In its current version 1.0, this library can be used for modeling and simulating the solar resource and the heat transfer fluid without phase change. The models have been validated with real data of an operating plant. ParaTrough can be freely used by process analysts for one or more of the following cases: performance assessment, fault detection, exploring new operation modes and plant optimization. While other elements can be added in future extensions, this contribution covers a new specific application area of Modelica and in its current state it facilitates the operation and maintenance of parabolic trough power plants.<br/> &copy; 2016 CEA.","Cabrerizo, Juan A. Romera and Santos, Matilde",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"CFD study of added resistance and motion of DTC in short and long waves","In the present work, in-house CFD code naoe-FOAM-SJTU is used to investigate the added resistance and motion of DTC ship at Fr=0.052 and 0.138 in short and long head waves. The time history and Fourier series of resistance and motion is given. Validation against the experimental results shows that the computation agrees well with EFD data in high speed cases. Then, the time-averaged and the Fourier transform of hydrodynamic pressure on ship are analyzed. The time-averaged pressure shows the added resistance is mainly caused by the high pressure at the upper bow. The distribution of second harmonic pressure on the bow demonstrates the bow relative motion is closely related with the nonlinearity of resistance. It can be explained as the large bow relative motion induces the stronger non-linear change of instantaneous wetness at flare bow. The wave patterns are plotted and show that the angle which wave diverged from the ship increases as the ship speed decrease and wave length increase.<br/> &copy; 2018 ASME.","Liu, Cong and Chen, Gang and Wan, Decheng",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The Ndynamics packageNumerical analysis of dynamical systems and the fractal dimension of boundaries","A set of Maple routines is presented, fully compatible with the new releases of Maple (14 and higher). The package deals with the numerical evolution of dynamical systems and provide flexible plotting of the results. The package also brings an initial conditions generator, a numerical solver manager, and a focusing set of routines that allow for better analysis of the graphical display of the results. The novelty that the package presents an optional C interface is maintained. This allows for fast numerical integration, even for the totally inexperienced Maple user, without any C expertise being required. Finally, the package provides the routines to calculate the fractal dimension of boundaries (via box counting). New version program summary Program Title: Ndynamics Catalogue identifier: %Leave blank, supplied by Elsevier. Licensing provisions: no. Programming language: Maple, C. Computer: Intel(R) Core(TM) i3 CPU M330 @ 2.13&nbsp;GHz. Operating system: Windows 7. RAM: 3.0&nbsp;GB Keywords: Dynamical systems, Box counting, Fractal dimension, Symbolic computation, Differential equations, Maple. Classification: 4.3. Catalogue identifier of previous version: ADKH_v1_0. Journal reference of previous version: Comput. Phys. Commun. 119 (1999) 256. Does the new version supersede the previous version?: Yes. Nature of problem Computation and plotting of numerical solutions of dynamical systems and the determination of the fractal dimension of the boundaries. Solution method The default method of integration is a fifth-order Runge&ndash;Kutta scheme, but any method of integration present on the Maple system is available via an argument when calling the routine. A box counting&nbsp;[1] method is used to calculate the fractal dimension&nbsp;[2] of the boundaries. Reasons for the new version The Ndynamics package met a demand of our research community for a flexible and friendly environment for analyzing dynamical systems. All the user has to do is create his/her own Maple session, with the system to be studied, and use the commands on the package to (for instance) calculate the fractal dimension of a certain boundary, without knowing or worrying about a single line of C programming. So the package combines the flexibility and friendly aspect of Maple with the fast and robust numerical integration of the compiled (for example C) basin. The package is old, but the problems it was designed to dealt with are still there. Since Maple evolved, the package stopped working, and we felt compelled to produce this version, fully compatible with the latest version of Maple, to make it again available to the Maple user. Summary of revisions Deprecated Maple Packages and Commands: Paraphrasing the Maple in-built help files, &ldquo;Some Maple commands and packages are deprecated. A command (or package) is deprecated when its functionality has been replaced by an improved implementation. The newer command is said to supersede the older one, and use of the newer command is strongly recommended&rdquo;. So, we have examined our code to see if some of these occurrences could be dangerous for it. For example, the &ldquo;readlib&rdquo; command is unnecessary, and we have removed its occurrences from our code. We have checked and changed all the necessary commands in order for us to be safe in respect to danger from this source. Another change we had to make was related to the tools we have implemented in order to use the interface for performing the numerical integration in C, externally, via the use of the Maple command &ldquo;ssystem&rdquo;. In the past, we had used, for the external C integration, the DJGPP system. But now we present the package with (free) Borland distribution. The compilation and compiling commands are now slightly changed. For example, to compile only, we had used &ldquo;gcc-c&rdquo;; now, we use &ldquo;bcc32-c&rdquo;, etc. All this installation (Borland) is explained on a &ldquo;README&rdquo; file we are submitting here to help the potential user. Restrictions Besides the inherent restrictions of numerical integration methods, this version of the package only deals with systems of first-order differential equations. Unusual features This package provides user-friendly software tools for analyzing the character of a dynamical system, whether it displays chaotic behaviour, and so on. Options within the package allow the user to specify characteristics that separate the trajectories into families of curves. In conjunction with the facilities for altering the user's viewpoint, this provides a graphical interface for the speedy and easy identification of regions with interesting dynamics. An unusual characteristic of the package is its interface for performing the numerical integrations in C using a fifth-order Runge&ndash;Kutta method (default). This potentially improves the speed of the numerical integration by some orders of magnitude and, in cases where it is necessary to calculate thousands of graphs in regions of difficult integration, this feature is very desirable. Besides that tool, somewhat more experienced users can produce their own C integrator and, by using the commands available in the package, use it as the C integrator provided with the package as long as the new integrator manages the input and output in the same format as the default one does. Running time This depends strongly on the dynamical system. With an Intel<sup>&reg;</sup>&nbsp;Core&trade;&nbsp;i3 CPU M330 @ 2.13&nbsp;GHz, the integration of 50 graphs, for a system of two first-order equations, typically takes less than a second to run (with the C integration interface). Without the C interface, it takes a few seconds. In order to calculate the fractal dimension, where we typically use 10,000 points to integrate, using the C interface it takes from 20 to 30&nbsp;s. Without the C interface, it becomes really impractical, taking, sometimes, for the same case, almost an hour. For some cases, it takes many hours.<br/> &copy; 2012 Elsevier B.V.","Avellar, J. and Duarte, L.G.S. and da Mota, L.A.C.P. and de Melo, N. and Skea, J.E.F.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A portable, extensible and fast stochastic volatility model calibration using multi and many-core processors","Financial markets change precipitously, and on-demand pricing and risk models must be constantly recalibrated to reduce risk. However, certain classes of models are computationally intensive to robustly calibrate to intraday prices - stochastic volatility models being an archetypal example due to the non-convexity of the objective function. In order to accelerate this procedure through parallel implementation, financial application developers are faced with an ever growing plethora of low-level high-performance computing frameworks such as Open Multi-Processing, Open Computing Language, compute unified device architecture, or single instruction multiple data intrinsics, and forced to make a trade-off between performance versus the portability, flexibility, and modularity of the code required to facilitate rapid in-house model development and productionisation. This paper describes the acceleration of stochastic volatility model calibration on multi-core CPUs and graphics processing units (GPUs) using the Xcelerit platform. By adopting a simple programming model, the Xcelerit platform enables the application developer to write sequential, high-level C++ code, without concern for low-level high-performance computing frameworks. This platform provides the portability, flexibility, and modularity required by application developers. Speedups of up to 30x and 293x are respectively achieved on an Intel Xeon CPU and NVIDIA Tesla K40 GPU, compared with a sequential CPU implementation. The Xcelerit platform implementation is further shown to be equivalent in performance to a low-level compute unified device architecture version. Overall, we are able to reduce the entire calibration process time of the sequential implementation from 6189 to 183.8 and 17.8 s on the CPU and GPU, respectively, without requiring the developer to reimplement in low-level high-performance computing frameworks.<br/> Copyright &copy; 2015 John Wiley & Sons, Ltd.","Dixon, Matthew and Lotze, Jorg and Zubair, Mohammad",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Erratum to: Controlled-source electromagnetic monitoring of reservoir oil saturation using a novel borehole-to-surface configuration: CSEM monitoring of oil saturation (Geophysical Prospecting, (2015), 63, 6, (1468-1490), 10.1111/1365-2478.12322)","INTRODUCTION In the original manuscript, we presented figures of 3D controlled-source electromagnetic forward modelling results for sources, which extended vertically through a reservoir region. The original approach of modelling these sources and the 3D conductivity model led to inaccurate results with the secondary field approach of the code used. Below, we discuss the effects in more detail and show corrected results. Results obtained for &ldquo;conventional&rdquo; survey geometries, with (horizontal) sources and receivers at some distance from 3D conductivity anomalies, are correct. CORRECTIONS Figures and : In the original manuscript, data displayed in Figs (a) and (a) were extracted at the wrong position along a parallel profile with an offset of 1000&nbsp;m. The new plots displayed below show slightly higher absolute changes close to the reservoir for the electric fields, which are expected to be resolved under field conditions. 8 (Figure presented.) CSEM time-lapse responses for the same model and source configuration as in Fig. 7 but now for receivers located between surface and 1800&nbsp;m depth for a fixed frequency of 0.1&nbsp;Hz. (a) absolute differences; (b) relative differences (relative changes &lt; 1&nbsp;% are blanked). Isolines in (b) show log10 magnitudes of electric [V/(Am<sup>2</sup>)] and magnetic field [A/(Am<sup>2</sup>)] components for the initial state. Figures,,, and of our original paper were not correct. For the 3D simulations of the original manuscript, we used the 1D layered half-space without the reservoir as background model, and the reservoir was considered a 3D conductivity anomaly. For both source configurations under consideration, the vertical part was modelled as a set of 130 point-dipoles with a spacing of 10&nbsp;m in vertical direction. As the reservoir is located at 1200 m below surface with a thickness of 15&nbsp;m, 129 of the 130 point-dipoles were located above or below the reservoir, that is, within the 1D background structure. However, 1 of the point-dipoles was located within the reservoir, that is, within the 3D conductivity anomaly, which leads to inaccurate results with the 3D controlled-source electromagnetic modelling (CSEM) code for two reasons. (i) An anomalous cell at the source location leads to very strong secondary sources (the anomalous cell effectively behaves like a point source), and thus, fields near that source vary extremely rapidly, which cannot be modelled accurately with the finite difference (FD) discretisation used. (ii) Source coupling to the surrounding medium determines currents on the source. So therefore, by placing the source in the background medium in the original approach, the coupling between the source and the reservoir was not correctly modelled. As this only applied to 1 out of 130 (vertical dipoles) for the vertical source or even 180 dipoles for the horizontal&ndash;vertical (HV) source, it went unnoticed. 9 (Figure presented.) Vertical source: CSEM time-lapse (Ex) responses at surface using the 3D model shown in Fig. 6 for a resistivity change from 16&nbsp;&Omega;m (reduced oil saturation) to 0.6&nbsp;&Omega;m (100&nbsp;% brine-flushed) in the reservoir. The vertical dipole source is located at x = 5100&nbsp;m as indicated by the asterisk. Dipole lengths of 1150&nbsp;m ((2a) in Fig. 6) and 1300&nbsp;m ((2b) in Fig. 6). (a) Absolute differences, (b) relative differences (changes &lt; 1&nbsp;% suppressed). Isolines in (b) show log10 magnitudes of electric components [V/(Am<sup>2</sup>)] for the initial state. The black horizontal line above the top panel indicates the along-profile extent of reservoir. 10 (Figure presented.) CSEM time-lapse responses for the same model and source configurations as in Fig. but for receivers located at depth for a fixed frequency of 0.1&nbsp;Hz. (a) Absolute differences, (b) relative differences (changes &lt; 1&nbsp;% blanked). Isolines in (b) show log10 magnitudes of electric [V/(Am<sup>2</sup>)] and magnetic field [A/(Am<sup>2</sup>)] components for the initial state. 12 (Figure presented.) HV-source: CSEM time-lapse responses at surface using the 3D model shown in Fig. 6 for a resistivity change from 16 (reduced oil saturation) to 0.6&nbsp;&Omega;m (100&nbsp;% brine flushed) in the reservoir. The casing of the HV source ((3) in Fig. 6) is located at x = 5100&nbsp;m as indicated by the black line. (a) Absolute differences, (b) relative differences (changes &lt; 1&nbsp;% suppressed). The reservoir is located between 5000&nbsp;m and 6000&nbsp;m profile distance. 13 (Figure presented.) CSEM time-lapse responses for the same model and source configuration as in Fig. but for receivers located at depth for a fixed frequency of 0.1&nbsp;Hz. (a) absolute differences, (b) relative differences (changes &lt; 1&nbsp;% are suppressed). Isolines in (b) show log10 magnitudes of electric [V/(Am<sup>2</sup>)] and magnetic field [A/(Am<sup>2</sup>)] components for the initial state. Reconsidering the results in a different context, we found that even for the HV source, where source currents at reservoir depth are 1000 times smaller than currents at the top of the borehole and on the surface wires, errors in calculations of electric and magnetic fields can be severe over the entire modelling domain. Hence, our results shown in the original Figs &ndash; for the 1300 m-long vertical source and for the HV source using scaling factors s of 0.33 and 1.0 in the original Figs &ndash; were biased and wrong. The results for s = 2.0 were correct though, because the current strength was effectively 0 for all point-dipoles at depths &gt; 1165 m (including the anomalous 3D reservoir region). For the results shown in the new figures below, the reservoir was included in the 1D background model. The 3D region of anomalous conductivity comprises all cells at reservoir depth outside the reservoir. Consequently, all source dipoles including elements inside the reservoir reside in cells with conductivities identical to the 1D background model. In addition, the horizontal discretisation at and around the vertical part of the source was refined. For the new mesh, the horizontal edge length of cells was reduced to 20&nbsp;m at the source and the two neighbouring columns in all four directions. In the vertical direction, discretisation was kept the same. In general, cells measure 30&nbsp;m vertically. At the air&ndash;ground interface, as well as at reservoir depth &plusmn; 10&nbsp;m, vertical cell size is reduced to 5&nbsp;m with gradual change of cell heights to 30&nbsp;m following above and below. Modelling the HV source, we discretised the vertical part of the source as 10 m dipoles for depths between 0&nbsp;m and 1150&nbsp;m below surface. In the vicinity of the reservoir, we tested the original vertical discretisation of 10 m, as well as a refined source discretisation with 2 m dipole distance. In both cases, we ensured that the source position does not coincide with a vertical cell boundary. Horizontally, the vertical part of the source is centred in the model column. The results for the original and refined source discretisation were identical. Data for all scenarios were simulated with both staggering schemes implemented in the CSEM modelling code: (i) electric fields (E) defined as face normals and (ii) electric fields defined on cell edges (see, e.g., Streich). Comparison of both results showed excellent agreement for all electric and magnetic field components outside the zone of anomalous conductivity, which further supports reliability of the new modelling results. Deviations occurred at the boundaries of anomalous zones and resulted from differences of conductivity averaging schemes for the two staggering schemes during the calculation of secondary fields (see Streich). In the new modelling, the zone of anomalous conductivity has the same conductivity as cells above and below. Hence, at some lateral distance from the reservoir, electric and magnetic fields are expected to vary smoothly across the boundary of the anomalous region. Inspecting the modelling results for both staggering schemes, we could verify that horizontal electric fields at reservoir depth were varying smoothly and, thus, considered most accurate using staggering scheme with E defined on edges, whereas the vertical electric field required to use E defined on faces. Results shown in Figs,,, and were obtained with the optimal respective staggering scheme. Please note that the colour scale range for Figs (b) and (b) changed in comparison to the original version from +/&minus;&nbsp;100 % to +/&minus;&nbsp;10 %. DISCUSSION For the horizontal&ndash;vertical (HV) source (Figs. &ndash;), the corrected results indicate that measurements of the horizontal electric field Ex are sensitive to changes of conductivity at reservoir depth, but only if currents on the casing follow a source current distribution with a scaling factor of 0.33, corresponding to low frequencies and low resistivities. Absolute amplitude differences reach close to 10<sup>&minus;11</sup>&nbsp;V/(Am<sup>2</sup>) in Fig. (a), along with relative field changes of about 5% for frequencies &lt; 1&nbsp;Hz at 500 to 2000&nbsp;m distance to the source (Fig. b). Considering the results for scaling factors of 1.0 and 2.0, which are more representative for higher frequencies and lower resistivities of host rocks, sensitivities are below the resolution threshold under field conditions. As before, observable amplitude changes associated with a decrease of resistivity within the reservoir are even larger if sensors are deployed closer to the reservoir, in particular, the vertical electric field Ez (Fig.). Similar to the measurements of Ex, both absolute and relative changes are substantially smaller than those estimated in our original submission. The new results also indicate that measurements of magnetic fields both at surface and at depth do not provide measurable information on conductivity changes in the reservoir. Although sensitivities of the vertical dipole and the HV source and measurements of electric field components are not as large as previously stated for this particular scenario, such sources with vertical components are still essential to obtain sensitivity to deep targets such as hydrocarbon reservoirs. As before, we conclude that measurements of Ez are most promising. Better resolution may be obtained if background resistivities are higher or steel-cased boreholes can be energised at the bottom.<br/> &copy; 2017 European Association of Geoscientists & Engineers","Tietze, Kristina and Ritter, Oliver and Veeken, Paul",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Cache-oblivious matrix algorithms in the age of multicores and many cores","This article highlights the issue of upcoming wider single-instruction, multiple-data units as well as steadily increasing core counts on contemporary and future processor architectures. We present the recent port to and latest results of cache-oblivious algorithms and implementations of our TifaMMy code on four architectures: SGI's UltraViolet distributed shared-memory machine, Intel's latest x86 architecture code-named Sandy Bridge, AMD's new Bulldozer architecture, and Intel's future Many Integrated Core architecture. TifaMMy's matrix multiplication and LU decomposition routines have been adapted and tuned with regard to these architectures. Results are discussed and compared with vendors' architecture-specific and optimized libraries, Math Kernel Library and AMD Core Math Library, for both a standard C++ version with vectorization compiler switches and TifaMMy's highly optimized vector intrinsics version. We provide insights into architectural properties and comment on the feasibility of heterogeneous cores and accelerators, namely graphics processing units. Besides bare-metal performance, the test platforms' ease of use is analyzed in detail, and the portability of our approach to new and upcoming silicon is discussed with regard to required effort on code change abstraction levels.As a result, we demonstrate that because of its generic structure in terms of memory organization, TifaMMy executes with equally efficient performance on all four architectures as it automatically adapts itself to architectural parameters without losing performance against the Math Kernel Library and AMD Core Math Library, underlining its generic and cache-oblivious properties, as the porting effort was relatively low compared with that in other implementations.<br/> Copyright &copy; 2012 John Wiley & Sons, Ltd.","Heinecke, Alexander and Trinitis, Carsten",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Erratum: Cluster pattern analysis of energy deposition sites for the brachytherapy sources103Pd,125I,192Ir,137Cs and60Co (Physics in Medicine and Biology (2014) 59 (5531-5543))","Our aim in Villegas et al (2014) was to analyse the spatial distribution of energy deposition (ED) sites for several common radiation qualities used in brachytherapy. Track structure data were obtained through simulations with a modified version of the Monte Carlo (MC) code PENELOPE v.2008 (Fern&aacute;ndez-Varea et al 2012). The existence of a critical bug in this modified MC code has recently been pointed out (Villegas and Ahnesj&ouml; 2015) that overestimated the mean free path of the electrons. In this Corrigendum we present the corrected data for figures 14 in Villegas et al (2014) in the same order as in the published paper. In the original work a description of the procedure for scoring the distances from an energy deposition (ED) site to its ith nearest neighbours dNNi was given. From these, the frequency distribution of distances to the nearest neighbours f (dNNi) normalized per deposited energy - was derived for commonly used brachytherapy sources. In addition, a method for scoring clusters of EDs of different order (CO) based on a single parameter called the cluster distance dc was also described. These yielded the frequency of cluster order fdc (CO) normalized per deposited energy E In figure 1, the updated f (dNN) &lowast; distributions for the first nearest neighbour (NN) decrease with increasing distance for all radiation qualities; but the higher energy photon sources (<sup>192</sup>Ir,<sup>137</sup>Cs and<sup>60</sup>Co) have higher values for distances larger than 30 nm when compared to the lower energy sources (<sup>103</sup>Pd and<sup>125</sup>I). The fifth NN distributions present two peaks for the higher energy photon sources, one at 2 nm and the second at 200 nm. In contrast, the two lower photon energies decrease continuously. The ratios of the distributions show a clear separation between the lower energy photon sources (103Pd and 125I) and the higher energy photon sources (192Ir, 137Cs, and 60Co). The second peaks of the higher energy photon sources distributions coincide with the f (dNN)/&#1013; from a uniform random distribution of points as shown by the updated data in figure 2. The ratios in figure 2 show that the f (dNN)/&#1013; for 103Pd has no uniform random component. In figure 3, the updated f (dNN)/&#1013; for the first NN of the lowest photon energy (103Pd) does not change as dose decreases. Moreover, the frequency distributions are the same as the single track distribution which indicates that for this range of doses the analysed EDs belong to uncorrelated tracks, i.e. there are no inter-track interactions. For higher energy photon sources (e.g. 60Co), the random component of f (dNN)/&#1013; (observed in figure 1) deviates more from the single track f (dNN)/&#1013; as dose increases. For further discussion see Villegas et al (2014). In figure 4, the updated fdc (CO)/&#1013; decreases with increasing cluster order with a steeper slope for dc = 2 nm when compared to dc = 10 nm. For dc = 2 nm, the ratio between the lower photon energies with respect to 60Co is between 1.1 and 1.3, whereas the ratio for 192Ir is found between 0.97 and 1.15, for cluster orders between 2 and 14. For dc = 10, the ratios with respect to 60Co increase about 15 % overall. The ratio of fdc (CO)/&#1013; distributions from single tracks to the distributions from an ED density corresponding to a dose of 2 Gy varies from 0.87 to 1.11 for dc = 2 nm and from 0.89 to 1.11 for dc = 10 nm (corrigendum data for figure 5 in Villegas et al (2014)). Due to the fact that the corrected MC track data yields shorter tracks, the fdc (CO)/&#1013; distributions for dc = 40 nm were not calculated as this distance definition is unreasonably large. The results presented here indicate that the discussion and conclusions made in Villegas et al (2014) still hold true, despite the change in the magnitudes of the frequencies. Particularly, the corrected f (CO)/&#1013; indicate a larger variation between photon sources which further supports our clustering method yields adequate data for radiation quality characterization. &copy; 2016 Institute of Physics and Engineering in Medicine.",,2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Support for parallel computing in julia language","The purpose of this paper is a brief review of tools for parallel computing implemented in the current version of the Julia Language. Julia is a young promising language designed for scientific programming. Before describing directly the parallel programming capabilities of Julia we give a small overview of the main features of the language. We describe the main goals pursued by the authors of the language when it was created, and the ideology that they espoused. Separately discussed the scientific focus of Julia, features that make the language convenient for the needs of mathematical modelling, handling big data and intensive numerical computing. Separately discussed such a feature as Julia's multiple dispatch. This mechanism of language paying a lot of attention. Most built-in functions and operators using multiple dispatch. In the second part of the paper we turn to the description of parallel programming. Julia is under active development, so support for parallel computing will be expanded, and the existing mechanisms may change. However, now Julia provides enough capabilities for writing quite complex programs using parallel computing. The basis for concurrency are processes (parallelism based on threads is not yet available). We describe the basic functions and macros that allow you to parallelize the execution of the functions, cycles and separate blocks of code. As the basis for presentation used the official guide and also the experience obtained by the authors in the process of using language.<br/> &copy; Copyright 2017 for the individual papers by the papers' authors.","Kulyabov, Dmitry S. and Gevorkyan, Migran N. and Korolkova, Anna V. and Sevastianov, Leonid A.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2014","The proceedings contain 44 papers. The special focus in this conference is on Critical Systems, Rigorous Engineering of Autonomic Ensembles, Automata Learning, Formal Methods and Analysis, Model-Based Code Generators and Automata Learning in Practice. The topics include: Statistical abstraction boosts design and test efficiency of evolving critical systems; incremental syntactic-semantic reliability analysis of evolving structured workflows; domain-specific languages for enterprise systems; formalizing self-adaptive clouds with knowlang; towards performance-aware engineering of autonomic component ensembles; rigorous system design flow for autonomous systems; algorithms for inferring register automata; active learning of nondeterministic systems from an ioco perspective; fomal methods and analyses in software product line engineering; domain specific languages for managing feature models; deployment variability in delta-oriented models; coverage criteria for behavioural testing of software product lines; DSL implementation for model-based development of pumps; domain-specific code generator modeling; LNCS transactions on foundations for mastering change; formal methods for collective adaptive ensembles; current issues on model-based software quality assurance for mastering change and compositional model-based system design as a foundation for mastering change.",,2014,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Proceedings - International Computer Software and Applications Conference","The proceedings contain 158 papers. The topics discussed include: an empirical analysis on web service anti-pattern detection using a machine learning framework; DevOps improvements for reduced cycle times with integrated test optimizations for continuous integration; a framework for updating functionalities based on the MAPE loop mechanism; combining constraint solving with different MOEAs for configuring large software product lines: a case study; visualizing a tangled change for supporting its decomposition and commit construction; an assertion framework for mobile robotic programming with spatial reasoning; DistGear: a lightweight event-driven framework for developing distributed applications; a lightweight program dependence based approach to concurrent mutation analysis; SPESC: a specification language for smart contracts; an insight into the impact of dockerfile evolutionary trajectories on quality and latency; automatic detection of outdated comments during code changes; search-based efficient automated program repair using mutation and fault localization; identifying supplementary bug-fix commits; a generalized approach to verification condition generation; runtime verification of robots collision avoidance case study; formalization and verification of mobile systems calculus using the rewriting engine Maude; predicting the breakability of blocking bug pairs; model checking of embedded systems using RTCTL while generating timed Kripke structure; PERDICE: towards discovering software inefficiencies leading to cache misses and branch mispredictions; and loop invariant generation for non-monotone loop structures.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Deep convolutional inverse graphics network","This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [10]. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative tests of the model's efficacy at learning a 3D rendering engine for varied object classes including faces and chairs.<br/>","Kulkarni, Tejas D. and Whitney, William F. and Kohli, Pushmeet and Tenenbaum, Joshua B.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Textual Explanations for Self-Driving Vehicles","Deep neural perception and control networks have become key components of self-driving vehicles. User acceptance is likely to benefit from easy-to-interpret textual explanations which allow end-users to understand what triggered a particular behavior. Explanations may be triggered by the neural controller, namely introspective explanations, or informed by the neural controller&rsquo;s output, namely rationalizations. We propose a new approach to introspective explanations which consists of two parts. First, we use a visual (spatial) attention model to train a convolutional network end-to-end from images to the vehicle control commands, i.e.,&nbsp;acceleration and change of course. The controller&rsquo;s attention identifies image regions that potentially influence the network&rsquo;s output. Second, we use an attention-based video-to-text model to produce textual explanations of model actions. The attention maps of controller and explanation model are aligned so that explanations are grounded in the parts of the scene that mattered to the controller. We explore two approaches to attention alignment, strong- and weak-alignment. Finally, we explore a version of our model that generates rationalizations, and compare with introspective explanations on the same video segments. We evaluate these models on a novel driving dataset with ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-X) dataset. Code is available at https://github.com/JinkyuKimUCB/explainable-deep-driving.<br/> &copy; 2018, Springer Nature Switzerland AG.","Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"10th International Scientific and Practical Conference, 2015","The proceedings contain 37 papers. The special focus in this conference is on Computer Technologies, Modelling Sociotechnical Systems, Intellectual Decision Support Systems, Environmental Education and Sustainable Development Processes. The topics include: Latvian language as a code in different communication channels; computer programming aptitude test as a tool for reducing student attrition; instrument of determination and prediction of public opinion using IPTV statistic data; conditions for successful development of electronic commerce in Latvia; the myths about and solutions for an android OS controlled and secure environment; the influence of hidden neurons factor on neural network training quality assurance; modern approaches to reduce webpage load times; mathematical modelling of aquatic ecosystem; modeling of time dependent thermal process in sliding electrical microcontact; wireless sensor networks lifetime assessment model development; models of data and their processing for introductory courses of computer science; importance of data acquisition in problem based learning; structuration of courses at studying disciplines of programming; fuzzy multiple criteria decision making approach in environmental risk assessment; using the concept of fuzzy random events in the assessment and analysis of ecological risks; application of the ontology concept for the needs of theoretical mechanics; automatic transformation of relational database schema into owl ontologies; calculation temperature and pressure of the rotary engine; artificial neural networks and human brain; survey of improvement possibilities of learning and Niskanen classical model implementation of the office&rsquo;s operations evaluation.",,2015,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Experimental evaluation of a glass curtain wall of a tall building","Summary: The seismic demand parameters including the floor acceleration amplification (FAA) factors and the interstory drift ratios (IDRs) were acquired from the floor response in time history analysis of a tall building subjected to selected ground motions. The FAA factors determined in this way are larger than those given in most current code provisions, but the obtained IDRs are close to the values given in some code provisions. Imposing a series of in-plane pre-deformations to two glass curtain wall (CW) specimens mounted on a shaking table, the IDRs were reproduced and the FAA factors were satisfied through applications of computed floor spectra compatible motion time histories, whose peak accelerations corresponded to the FAA factors. The CW specimens performed well during the whole experimental program with almost no change in the fundamental frequencies. No visible damage was observed in the glass panels. The maximum stresses detected in each component of the CW system were smaller than the design strengths. The obtained component acceleration amplification factor approached 3.35, which is larger than the value given in the current code provisions. In conclusion, the performance of the studied CW system is seismically safe.<br/> &copy; 2016 John Wiley & Sons, Ltd.","Lu, Wensheng and Huang, Baofeng and Mosalam, Khalid M. and Chen, Shiming",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Comparison of photon organ and effective dose coefficients for PIMAL stylized phantom in bent positions in standard irradiation geometries","Computational phantoms with articulated arms and legs have been constructed to enable the estimation of radiation dose in different postures. Through a graphical user interface, the Phantom wIth Moving Arms and Legs (PIMAL) version 4.1.0 software can be employed to articulate the posture of a phantom and generate a corresponding input deck for the Monte Carlo N-Particle (MCNP) radiation transport code. In this work, photon fluence-to-dose coefficients were computed using PIMAL to compare organ and effective doses for a stylized phantom in the standard upright position with those for phantoms in realistic work postures. The articulated phantoms represent working positions including fully and half bent torsos with extended arms for both the male and female reference adults. Dose coefficients are compared for both the upright and bent positions across monoenergetic photon energies: 0.05, 0.1, 0.5, 1.0, and 5.0&nbsp;MeV. Additionally, the organ doses are compared across the International Commission on Radiological Protection&rsquo;s standard external radiation exposure geometries: antero-posterior, postero-anterior, left and right lateral, and isotropic (AP, PA, LLAT, RLAT, and ISO). For the AP and PA irradiation geometries, differences in organ doses compared to the upright phantom become more profound with increasing bending angles and have doses largely overestimated for all organs except the brain in AP and bladder in PA. In LLAT and RLAT irradiation geometries, energy deposition for organs is more likely to be underestimated compared to the upright phantom, with no overall change despite increased bending angle. The ISO source geometry did not cause a significant difference in absorbed organ dose between the different phantoms, regardless of position. Organ and effective fluence-to-dose coefficients are tabulated. In the AP geometry, the effective dose at the 45&deg; bent position is overestimated compared to the upright phantom below 1&nbsp;MeV by as much as 27% and 82% in the 90&deg; position. The effective dose in the 45&deg; bent position was comparable to that in the 90&deg; bent position for the LLAT and RLAT irradiation geometries. However, the upright phantom underestimates the effective dose to PIMAL in the LLAT and RLAT geometries by as much as 30% at 50&nbsp;keV.<br/> &copy; 2017, Springer-Verlag Berlin Heidelberg.","Dewji, Shaheen and Reed, K. Lisa and Hiller, Mauritius",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security, QRS 2018","The proceedings contain 50 papers. The topics discussed include: probabilistic sampling-based testing for accelerated reliability assessment; verifying stochastic behaviors of decentralized self-adaptive systems: a formal modeling and simulation based approach; machine learning to evaluate evolvability defects: code metrics thresholds for a given context; a method for predicting two-variable atomicity violations; cross-entropy: a new metric for software defect prediction; a security model for access control in graph-oriented databases; hypervisor-based sensitive data leakage detector; detecting errors in a humanoid robot; using crash frequency analysis to identify error-prone software technologies in multi-system monitoring; exploratory data analysis of fault injection campaigns; change-based test script maintenance for android apps; and how do defects hurt qualities? an empirical study on characterizing a software maintainability ontology in open source software.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Real-time infrared thermography at ASDEX upgrade","Infrared (IR) thermography is a widely used tool in fusion research to study the thermal load onto plasma-facing components. In present-day fusion experiments with short-pulse duration, off-line data analysis is still feasible. For devices with long-pulse duration and actively cooled plasma-facing components, IR thermography is a common tool for machine protection. In future fusion devices with long-pulse duration, online data evaluation of the thermography measurement for additional physics studies is required. Real-time' capable IR thermography was developed at ASDEX Upgrade. The feasibility of real-time thermography is discussed in this work. The evaluation process from raw data to evaluated temperature and heat flux is shown. The real-time version of the THEODOR code allows online calculation of the heat flux. Exploiting the possibility of the IR system to change the integration time during acquisition opens up the possibility to have automated thermography. The current status of the thermography system at ASDEX Upgrade and future developments for its improvement are discussed.<br/>","Sieglin, B. and Faitsch, M. and Herrmann, A. and Martinov, S. and Eich, T.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings","The proceedings contain 79 papers. The topics discussed include: software history under the lens: a study on why and how developers examine it; to fix or to learn? how production bias affects developers' information foraging during debugging; developers' perception of co-change patterns: an empirical study; when and why developers adopt and change software licenses; investigating naming convention adherence in java references; developing a model of loop actions by mining loop characteristics from a large code corpus; delta extraction: an abstraction technique to comprehend why two objects could be related; modeling changeset topics for feature location; four eyes are better than two: on the impact of code reviews on software quality; and a comparative study on the bug-proneness of different types of code clones.",,2015,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Effects of perforated baffle on reducing sloshing in rectangular tank: Experimental and numerical study","A liquid sloshing experimental rig driven by a wave-maker is designed and built to study liquid sloshing problems in a rectangular liquid tank with perforated baffle. A series of experiments are conducted in this experimental rig to estimate the free surface fluctuation and pressure distribution by changing external excitation frequency of the shaking table. An in-house CFD code is also used in this study to simulate the liquid sloshing in three-dimensional (3D) rectangular tank with perforated baffle. Good agreements of free surface elevation and pressure between the numerical results and the experimental data are obtained and presented. Spectral analysis of the time history of free surface elevation is conducted by using the fast Fourier transformation. &copy; 2013 Chinese Ocean Engineering Society and Springer-Verlag Berlin Heidelberg.<br/>","Xue, Mi-an and Lin, Peng-zhi and Zheng, Jin-hai and Ma, Yu-xiang and Yuan, Xiao-li and Nguyen, Viet-Thanh",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"ALM Tool Infrastructure with a Focus on DevOps Culture","At iNNOVA IT Solutions, with our team of more than 1300 IT professionals, we tailor platform-independent customized software and systems solutions. The ALM tool infrastructure has been continuously improved over the several years in order to maintain the work item traceability, define a single source of truth and increase communication and collaboration which is one of the key cultural aspects of DevOps. This paper describes how the tool infrastructure is set up and used in order to manage traceability between work items such as change requests, configuration items, technical documents, test cases, technical tasks and code; make use of the knowledge base and history during impact analysis; track the real time status of releases, change requests, sprints, test plans and all the related work items; generate formatted customer documentation in long term maintenance projects.<br/> &copy; 2018, Springer Nature Switzerland AG.","Akman, Suha and Aksuyek, Elif Berru and Kaynak, Onur",2018,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR9"
"Monte Carlo simulations of the relative biological effectiveness for DNA double strand breaks from 300 MeV u-1carbon-ion beams","Monte Carlo simulations are used to calculate the relative biological effectiveness (RBE) of 300 MeV u<sup>-1</sup>carbon-ion beams at different depths in a cylindrical water phantom of 10 cm radius and 30 cm long. RBE values for the induction of DNA double strand breaks (DSB), a biological endpoint closely related to cell inactivation, are estimated for monoenergetic and energy-modulated carbon ion beams. Individual contributions to the RBE from primary ions and secondary nuclear fragments are simulated separately. These simulations are based on a multi-scale modelling approach by first applying the FLUKA (version 2011.2.17) transport code to estimate the absorbed doses and fluence energy spectra, then using the MCDS (version 3.10A) damage code for DSB yields. The approach is efficient since it separates the non-stochastic dosimetry problem from the stochastic DNA damage problem. The MCDS code predicts the major trends of the DSB yields from detailed track structure simulations. It is found that, as depth is increasing, RBE values increase slowly from the entrance depth to the plateau region and change substantially in the Bragg peak region. RBE values reach their maxima at the distal edge of the Bragg peak. Beyond this edge, contributions to RBE are entirely from nuclear fragments. Maximum RBE values at the distal edges of the Bragg peak and the spread-out Bragg peak are, respectively, 3.0 and 2.8. The present approach has the flexibility to weight RBE contributions from different DSB classes, i.e. DSB0, DSB+ and DSB++.<br/> &copy; 2015 Institute of Physics and Engineering in Medicine.","Huang, Y.W. and Pan, C.Y. and Hsiao, Y.Y. and Chao, T.C. and Lee, C.C. and Tung, C.J.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Interpreting low-temperature thermochronology in magmatic terranes: Modeling and case studies from the Colorado Plateau","Exploring the complexities---and advantages---of interpreting low-temperature thermochronologic data in magmatic terranes is the principal theme of this work. Using simple analytical approximations as well as the finite-element code Pecube, we characterize the cooling age patterns inside and around plutons emplaced at upper and middle crustal levels and identify the advective and conductive scaling relationships that govern these patterns. We find that the resetting aureole width, the difference between reset and unreset cooling ages in country rocks, and the lag time between pluton crystallization age and pluton cooling age all scale with exhumation rate because this rate sets the advective timescale of cooling. Cooling age-elevation relationships in these steadily exhuming models have changes in slope that would masquerade as changes in exhumation or erosion rates in real datasets, if the thermal effects of the plutons were not accounted for. We also demonstrate the importance of considering the magmatic history of a region in field studies of the Colorado Plateau, where interpreting apatite (U-Th)/He data requires diagnosing significant inter- and intra-sample age variability. Prior to considering the thermal history of the region, we develop a new model for a common source of this age variability: excess He implantation from U and Th (i.e., eU) hosted in secondary grain boundary phases (GBPs), which can make very low eU apatites hundreds of percent 'too old'. Samples significantly affected by He implantation are not useful for thermal history interpretations, but this model does provide a diagnostic tool for discriminating these samples from those with useful age trends. Once the effects of GBPs have been accounted for, the remaining data from two different thermochronologic archives in the central Colorado Plateau provide a new perspective on the Cenozoic history of the region, which has a multiphase---and enigmatic---history of magmatism and erosion. We find that sandstones in the thermal aureoles around the Henry, La Sal, and Abajo mountains intrusive complexes were usefully primed by magmatic heating in the Oligocene to document the subsequent late Cenozoic history of the region more clearly than any other thermochronologic archive on the Plateau. These data document a stable Miocene landscape (erosion rates &lt;30 m/Ma) that rapidly exhumed ~1.5-2 km in the Plio-Pleistocene (~250-700 m/Ma no earlier than 5 Ma) in the Henry and Abajo mountains, and strongly suggest most of this erosion occurred in the last 3-2 Ma. The integration of the Colorado River ca. 6 Ma, which dropped regional base-level, is the principal driver of this erosion. It is likely, however, that a component of the rapid Pleistocene rock cooling is unique to the high mountains of the Colorado Plateau and reflects an increase in spring snow-melt discharge during glacial periods. Although apatite thermochronology results far from the Oligocene intrusive complexes cannot resolve this detailed Plio-Pleistocene history, they do constrain the onset of late Cenozoic erosion to no earlier than ~6 Ma. Moreover, apatite cooling ages from these rocks also document Oligocene cooling (ca. 25 Ma) that is contemporaneous with the emplacement of the laccoliths and the waning of the vigorous magmatic flare-up that swept through the southwestern USA ca. 40-25 Ma. Although the cooling ages are consistent with ~1 km of exhumation in the late Oligocene and early Miocene, as previous workers have suggested in the eastern Grand Canyon region, we demonstrate that a transient change in the geothermal gradient (peaking at ~50 ?C/Ma in the late Oligocene) driven by moderate mid-crustal magmatism can produce identical age patterns. Therefore, we re-interpret the mid-Cenozoic erosion event on the Colorado Plateau as primarily a change in the crustal thermal field, rather than an erosional event. This requires a more significant Laramide-age unroofing in parts of the central Plateau and perhaps a re-evaluation of the interpretations of Oligocene canyon cutting in the Grand Canyon region. (Abstract shortened by ProQuest.). ProQuest Subject Headings: Geology, Geochemistry.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.","Murray, Kendra Elizabeth",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The risk management of the owner in the bidding process under the 2013 edition of the valuation mode of bill of engineering quantity","Taking the perspective of the owners,we analyze the major valuation risk which is based on the theory of the project risk management under the version of 2013 code of valuation with bill quantity of construction works when the owner want to bid to identify the possible risk factors. And in this article we propose some specific measures of the risk control about the imperfect of construction drawing design,the quatity problems of tender documents,unbalanced quote of the bidders,the price changing of the building materials and equipments and various rates of change in order to achieve active control risk and avoid exceeding the cost and provide a theoretical basis for the owner (or the tender agent) to bid. &copy; (2014) Trans Tech Publications, Switzerland.","Guo, Zhang Lin and Jia, Rui Hong",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Compression of three-dimensional surfaces by means of chain coding","Studies for representing three-dimensional (3-D) objects are an important subject in different fields, such as computer vision, data compression, creation of virtual scenes, and others. Any surface can be seen as a 3-D object and studied as such. 3-D voxel-based surfaces are described using a 3-D tree structure known as an enclosing tree. A modified version of this structure is used to describe the surface. In order to describe the surfaces, we used the enclosing trees that are represented via the orthogonal direction change chain code. The representation obtained is compared with the original to verify if proper data compression is achieved.<br/> &copy; 2015 Society of Photo-Optical Instrumentation Engineers (SPIE).","Salazar, J. Miguel and Bribiesca, Ernesto",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Integrated methodology for the analysis of fuel integrity in high burnup fuel assemblies using CTF-UPVIs, Frapcon and Fraptran codes","Detailed fuel assembly description is increasingly important for Light Water Reactor (LWR) deterministic safety analysis in order to represent realistically the complexity of the high burn-up fuel designs. This need involves having the appropriate computer tools to simulate accurately the behavior of the fuel elements during the normal and accidental operational conditions. The subchannel analysis brings out a proper scale level for analyzing the phenomena occurring in the core during operation. Using this scale with thermo-mechanics and thermal-hydraulics analysis allows evaluating the safety parameters related to coolant and fuel rod properties. The sub-channel analysis defines the node mesh regarding the fuel pin and the coolant surrounding it. When using a thermal-hydraulic code at this level it can be observed the coolant and fuel behavior and how its properties change along the fuel assembly. Interesting parameters such as the Departure of Nucleate Boiling Ratio (DNBR), Critical Heat Flux (CHF), Critical Quality (CQ) or Peak Cladding Temperature (PCT) can be obtained for further safety analysis. On the other hand, thermo-mechanic codes at this scale are useful to calculate the fuel integrity and the hydrogen generation during the simulated transient. The fuel integrity can be evaluated using parameter like the cladding ballooning, elongation and stress, whereas the cladding corrosion and hydrogen generation are directly evaluated from the kinetics of the cladding metal electrochemical reaction. The objective of this work is to develop a procedure for evaluating the fuel integrity during normal operating conditions or during accidental transients using the sub-channel thermal-hydraulic code CTF-UPVIS (ISIRYM-UPV version of COBRA-TF (CTF) code) and the fuel performance codes FRAPCON/FRAPTRAN. The combination of detailed sub-channel thermal-hydraulics and fuel performance in a single methodology will increase the accuracy for the transient calculations as well as speeding up the simulation by means of automating the data exchange in a two-step process. The developed methodology will be tested in two scenarios: during the normal operating conditions and during a fast transient in a BWR reactor core. For both scenarios, the safety limits are calculated and presented in this work as an example of the capabilities of the developed tool.<br/> &copy; 2016 Association for Computing Machinery Inc. All Rights Reserved.","Hidalga, P. and Abarca, A. and Miro, R. and Verdu, G.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Effect of block valve and crack arrestor spacing on thermal radiation hazards associated with ignited rupture incidents for natural gas pipelines","A study was undertaken to evaluate crack arrestor and mainline block valve (MLBV) spacing distances beyond the limits defined in the 49 CFR Part 192 for Class 1 locations for the design of a 42-inch (1,067-mm) OD arctic pipeline. The study assessed whether an MLBV spacing longer than that required by 49 CFR Part 192 for Class 1 locations can provide a level of safety equivalent to that afforded by the spacing recommended in the code. This was accomplished by comparing the hazards in terms of the volume of natural gas released over time, the potential for damage to surrounding structures, and the life safety risk to personnel and the public. The analysis was performed using the software tool PIPESAFE (version 2.20.0), which was developed for a group of pipeline operators by Advantica Technology (now DNV-GL). A full transient analysis of the flow inside the pipeline and through the rupture opening was carried out with automatic shut-off valve (ASV) closures simulated as boundary condition changes at the locations of the valves triggered by the local transient pressure. Gas outflow rates were fed to a structured flame model that calculates the temperature distribution within the flame and the radiant energy emitted and uses the latter to determine the incident thermal radiation field in the area surrounding the rupture, the associated hazard areas and the accumulated thermal radiation dosage over time. These results were compiled into contour plots of thermal radiation intensity for different times; plots of the total area within specific contours of thermal radiation intensity for different times; and plots of the total area within specific contours of accumulated dosage. The dosage-Area curves facilitate a direct comparison of the various MLBV and crack arrestor spacing options considered within this study by providing a simple means to establish if the change in spacing causes a substantial change to the affected areas for dosages up to the limits associated with specific levels of lethality to humans and for piloted ignition of wooden structures. It was found that valve spacing has a strong effect on the time at which closure begins to affect the outflow rate. The decline in flow rate after valve closure had significant influence on the thermal radiation field, but these effects only occurred at a relatively late stage. Increasing fracture length led to considerable changes in the shape of the thermal radiation field, but the total footprint within which casualties might be expected in the event of an ignited rupture release and the severity of injuries within the footprint are unaffected by valve closure under the assumed conditions. Similarly, the damage potential to surrounding buildings was unaffected by valve spacing, indicating that increased valve spacing could be implemented in remote, low population density areas without affecting safety.<br/> &copy; Copyright 2016 by ASME.","Rothwell, Brian and Dessein, Thomas and Collard, Andy",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"4th International Conference on Internet Science, INSCI 2017 co-located with IFIN, DATA ECONOMY, DSI, and CONVERSATIONS 2017","The proceedings contain 18 papers. The special focus in this conference is on. The topics include: The Case for Collaborative Policy Experimentation Using Advanced Geospatial Data Analytics and Visualisation; an Engagement-Related Behaviour Change Approach for SavingFood in Greece; developing a Social Innovation Methodology in the Web 2.0 Era; code Hunting Games: A Mixed Reality Multiplayer Treasure Hunt Through a Conversational Interface; politician &ndash; An Imitation Game; Towards Open Domain Chatbots&mdash;A GRU Architecture for Data Driven Conversations; creating Dialogues Using Argumentation and Social Practices; an Overview of Open-Source Chatbots Social Skills; technology Adoption and Social Innovation: Assessing an Online Financial Awareness Platform; aalto Observatory on Digital Valuation Systems: A Position Paper; a Novel Lexicon-Based Approach in Determining Sentiment in Financial Data Using Learning Automata; a Hybrid Recommendation System Based on Density-Based Clustering; computing Platform for Virtual Economic Activities Index; data Based Stock Portfolio Construction Using Computational Intelligence; yourDataStories: Transparency and Corruption Fighting Through Data Interlinking and Visual Exploration; the Maker Movement and the Disruption of the Producer-Consumer Relation.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"An improved method to calculate the nonlinear rolling moment due to differential fin deflection of canard controlled missiles","An Improved Method to Calculate the Nonlinear Rolling Moment Due to Differential Fin Deflection of Canard Controlled Missiles has been developed. The method utilized a Computational Fluid Dynamics Data Base using the wing controlled Seasparrow as a baseline configuration. Improvements to the nonlinear rolling moment incorporated as a result of the new database include: a) accounting for fin interference as a function of angle of attack and Mach number, b) approximating the change in the leeward plane tail fin lateral center of pressure as a function of angle of attack and Mach number, and c) estimating the nonlinear change in rolling moment on the leeward plane tail fin as a function of angle of attack and Mach number. These improvements were incorporated into the 2013 version of the Aeroprediction Code to be released in 2014. Comparison of the improved method to existing approximate techniques and experimental data was made on several configurations. The improved method did a much better job in predicting the nonlinearities in roll moment due to differential fin deflection on all the configurations investigated than existing semiempirical codes, including the 2013 release of the Aeroprediction Code.<br/> &copy; 2015 by Aeroprediction, Inc.","Moore, F.G. and Moore, L.Y. and McGowan, Gregory",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"2013 1st International Workshop on Data Analysis Patterns in Software Engineering, DAPSE 2013 - Proceedings","The proceedings contain 13 papers. The topics discussed include: building statistical language models of code; commit graphs; concept to commit: a pattern designed to trace code changes from user requests to change implementation by analyzing mailing lists and code repositories; data analysis anti-patterns in empirical software engineering; effect size analysis; exploring software engineering data with formal concept analysis; extracting artifact lifecycle models from metadata history; measure what counts: an evaluation pattern for software data analysis; parametric classification over multiple samples; patterns for cleaning up bug data; patterns for extracting high level information from bug reports; structural and temporal patterns-based features; and the chunking pattern.",,2013,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Discrete element simulations of direct shear tests with particle angularity effect","This paper investigated the effect of the particle angularity in light of its importance in angular particle assemblies, using the discrete element method (DEM). A discrete element model with a general contact force law for arbitrarily shaped particles was developed, in which angular particles were modeled using convex polyhedra. Quasi-spherical polyhedral shapes with different vertexes were adopted to reflect the change of angularity. Four categories of assemblies with different angularities were generated. A series of direct shear tests performed on these assemblies were simulated at different vertical stresses. All numerical implementations were achieved using a modified version of the open source DEM code YADE. It was found that the macroscopic shear strength and dilatancy characteristics are in agreement with experimental and numerical results in the literature, indicating that the present numerical model is reasonable. Besides, the evolutions of coordination number, normal contact force distribution, and anisotropies of particle orientation and contact normal were investigated. The results show that the angularity plays a vital role in strengthening the interlocking of angular particles.<br/> &copy; 2015, Springer-Verlag Berlin Heidelberg.","Zhao, Shiwei and Zhou, Xiaowen and Liu, Wenhui",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC","The proceedings contain 54 papers. The topics discussed include: label management: keeping complex diagrams usable; who changed my annotation? an investigation into refitting freeform ink annotation; an evolutionary approach to determining hidden lines from a natural sketch; measuring perceived clutter in concept diagrams; evaluation of a modelling language for customer journeys; an empirical study of user perceived usefulness and preference of open learner model visualisations; learning programming from tutorials and code puzzles: children&#65533;s perceptions of value; coding, reading, and writing: integrated instruction in written language; visual discovery and model-driven explanation of time series patterns; diagnostic visualization for non-expert machine learning practitioners: a design study; supporting end-users in defining complex queries on evolving and domain-specific data models; Yestercode: improving code-change support in visual dataflow programming environments; declarative setup-free web application prototyping combining local and cloud datastores; a domain-specific visual modeling language for testing environment emulation; and trials and tribulations of developers of intelligent systems: a field study.",,2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The limited impact of individual developer data on software defect prediction","Previous research has provided evidence that a combination of static code metrics and software history metrics can be used to predict with surprising success which files in the next release of a large system will have the largest numbers of defects. In contrast, very little research exists to indicate whether information about individual developers can profitably be used to improve predictions. We investigate whether files in a large system that are modified by an individual developer consistently contain either more or fewer faults than the average of all files in the system. The goal of the investigation is to determine whether information about which particular developer modified a file is able to improve defect predictions. We also extend earlier research evaluating use of counts of the number of developers who modified a file as predictors of the file's future faultiness. We analyze change reports filed for three large systems, each containing 18 releases, with a combined total of nearly 4 million LOC and over 11,000 files. A buggy file ratio is defined for programmers, measuring the proportion of faulty files in Release R out of all files modified by the programmer in Release R-1. We assess the consistency of the buggy file ratio across releases for individual programmers both visually and within the context of a fault prediction model. Buggy file ratios for individual programmers often varied widely across all the releases that they participated in. A prediction model that takes account of the history of faulty files that were changed by individual developers shows improvement over the standard negative binomial model of less than 0.13% according to one measure, and no improvement at all according to another measure. In contrast, augmenting a standard model with counts of cumulative developers changing files in prior releases produced up to a 2% improvement in the percentage of faults detected in the top 20% of predicted faulty files. The cumulative number of developers interacting with a file can be a useful variable for defect prediction. However, the study indicates that adding information to a model about which particular developer modified a file is not likely to improve defect predictions. &copy; 2011 Springer Science+Business Media, LLC.<br/>","Bell, Robert M. and Ostrand, Thomas J. and Weyuker, Elaine J.",2013,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Technology, education and access: A 'fair go' for peoplewith disabilities","Australians are renowned for their cultural quirks, some of which include abbreviating everyday words such as 'breakfast' becoming 'brekky' and McDonalds becoming 'Maccas'. Australians also love sport and feature their own unique version of football, which to the untrained eye could be mistaken for a game whereby 36 players fight over a rugby-shaped ball for three hours while trying to kick the ball between two large posts. However, there's one aspect of Australian culture which effectively represents the core values of the nation - the importance of a 'fair go'. The concept of a 'fair go' is not about becoming a leader, achieving fame or fortune. Rather it's about the ability to achieve the everyday things in an environment that provides neither advantage or disadvantage. This could include having a regular income, a home, a good job and a family. For Dr. Scott Hollier, his early years suggested that he was destined for a fair go in life, growing up in a well-educated middle-income family in the hills area of Perth, Australia However when he was diagnosed with the Retinitis Pigmentosa eye condition at the age of five, it was a shock to his parents; they were told that Scott would go blind almost immediately, he would need to go to a 'special' school and his future career prospects were bleak. While each of these statements were cause for concern in itself, for Scott's parents the biggest issue was that it all represented the concern that Scott would not receive a fair go in life as a result of his disability. Scott's parents went and sought a second opinion about the diagnosis at which point it was discovered that there was no immediate need to change his schooling, and as the future was unwritten, the hope for a fair go was restored. As Scott progressed through primary and high school, he discovered a love for computers and video games, leading to his interest in Computer Science. While his initial exposure to the possibilities of technology supporting is disability came whit software Automated Mouth (SAM) software on the Commodore 64, it was a discussion in the comp.sys.cbm newsgroup that sparked Scott's interest in the potential of accessibility. Scott describes the experience in his memoir 'Outrunning the Night' as follows: ""One of the posts that I found particularly interesting was by someone asking how they could get their Commodore 64 on the Internet. From memory, this post led to a few responses, most of which were either critical of the author for making the ridiculous suggestion...or from others viewing it as a deliberate attempt to 'flame' the group. After a few posts, though, someone posted code in BASIC that would create a TCP/IP stack, and various primitive connectivity options to get it online. It was at this point that it dawned on me [if] the world could come together to get a computer from 1982 on the Internet, imagine the possibilities if similar efforts were put into using technologies to support people with disabilities."" It became clear to Scott at this point that the power of education and technology had the potential to ensure that people with disabilities could get a fair go if the world community came together to support it. After Scott completed his Computer Science degree he worked in the information technology industry for six years at which point he considered the implications for emerging consumer technologies and how beneficial it could be for employment. Scott undertook additional studies which ultimately led to studying a PhD in the field. Once completed, Scott moved to Sydney with his wife and two children to start a new position with a not-for-profit organisation in 2008. With rapid evolution of policy and consumer technologies including the release of the Web Content Accessibility Guidelines (WCAG) 2.0 in 2008, the first mainstream accessible touchscreen device with the iPhone 3GS in 2009 and the inclusion of accessibity features in popular operating systems including Windows MacOS, iOS and Android, Scott was able to focus his work on highlighting to people with disabilities that not only were previously expensive assistive technologies now built-in to popular everyday products, but people with disabilities had the same consumer choices as the general public. While Scott enjoyed supporting consumers with disabilities in embracing the benefits technology can provide, Scott also focused on supporting the ICT community in creating accessible content online. However, one challenge in seeking to actively participate in this space was the time zones in Australia for meeting with W3C working groups. With Scott returning to Perth in 2010 to establish a new office for his employer and more recently becoming an independent consultant and researcher, Scott has had the opportunity to become an active participant with the Research Questions Task Force (RQTF) despite the late-night teleconference calls. Scott encourages W3C and other organisations to considerhow to improve engagement with the region as the +8UTC time zone of Perth is the world's most populated time zone including Indonesia, Malaysia and China. Ultimately people in parts of the world outside of Europe and North America need to be given a fair go in terms of participation and Scott has volunteered to work with W3C and others to make improvements in this area. However, while issues of community participation remain, the challenges of people with disabilities continue to be addressed with the arrival of new technologies and platforms such as social media. Scott recalls a project called Sociability whereby he interviewed 49 people about their access to social media, and it became clear that for people with disabilities, such platforms genuinely provided a fair go. In some cases, it was about providing support to a hearing-impaired woman in socialising at parties with the help of Facebook, for a blind man it was about keeping up with industry knowledge using LinkedIn. Yet for another it was about not being able to drive and using social media to find cheap pizza coupons so dinner could be delivered. Moving forward, Scott considers that the arrival of digital assistants in the home and the Internet of Things (IoT) provides the next frontier of engagement whereby people with disabilities have one more interface option to communicate with devices. For example, if a person in a wheelchair can't reach the buttons on a microwave, or a blind person can't read the display on a washing machine, the ability to talk to devices or use digital assistants such as Google Home provides even more opportunities for people to get a fair go. The final point highlighted by Scott is that the opportunity of a fair go is only possible if people come together to make accessibility happen, With this in mind, Scott has highlighted the importance of people attending conferences such as W4A as they have effectively dedicated their lives and careers to support people with disabilities, while often people working in this research space do not always get the chance to see the practical outcomes of their work at the 'coal-face', Scott has offered his thanks both professionally and personally, to the hard work and dedication of people that have given him and people with disabilities all over the world the opportunity for a fair go.Copyright is held by the owner/author(s).<br/>","Hollier, Scott",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Evaluation of associated and non-associated flow metal plasticity; application for DC06 deep drawing steel","In this paper the capabilities of Associated Flow Rule (AFR) and non-AFR based finite element models for sheet metal forming simulations is investigated. In case of non-AFR, Hill's quadratic function used as plastic potential function, makes use of plastic strain ratios to determine the direction of effective plastic strain rate. In addition, the yield function uses direction dependent yield stress data. Therefore more accurate predictions are expected in terms of both yield stress and strain ratios at different orientations. We implemented a modified version of the non-associative flow rule originally developed by Stoughton [1] into the commercial finite element code ABAQUS by means of a user material subroutine UMAT. The main algorithm developed includes combined effects of isotropic and kinematic hardening [2]. This paper assumes proportional loading cases and therefore only isotropic hardening effect is considered. In our model the incremental change of plastic strain rate tensor is not equal to the incremental change of the compliance factor. The validity of the model is demonstrated by comparing stresses and strain ratios obtained from finite element simulations with experimentally determined values for deep drawing steel DC06. A critical comparison is made between numerical results obtained from AFR and non-AFR based models.&copy; (2012) Trans Tech Publications.<br/>","Safaei, Mohsen and Waele, Wim De and Zang, Shun-Lai",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"2013 International Conference on Information Engineering, ICIE 2013","The proceedings contain 192 papers. The special focus in this conference is on Information and Communication Engineering, Electronics Science, Technology, and Application, Computer science and Technology, ICT for Business and Management, and Information Engineering. The topics include: Bidirectional channel assignment for multiradio wireless mesh backhaul; application of neural networks in Taiwan train quali system performance evaluation; an evaluation of tourist attraction ranking methods; joint just noticeable distortion based stereo image watermarking method with self-recovery; decision fusion rule for dynamic large-scale wireless sensor networks; the research of black hole detection in AODV based on NS2; strumming pattern recognition from ukulele songs; shared optical infrastructure for precise time transfer; multicast management in OpenFlow network environment; a survey of code-based and social-based routing protocols in delay tolerant networks; learning strategies of domain ontology concepts from the web; domain-specific evolving network model for complex systems; a community partition algorithm for the network public opinion; spectral analysis of the moving system with multi-object based on V-system; the characteristics of APT attacks and strategies of countermeasure; research on serial assembling scheme for network coding in the internet; joint processing with local channel state information for heterogeneous networks; research on SVM-based securities time series; simulation and verification of high dynamic IF GPS signal; satellites selection method for high-dynamic vector GPS receiver; design and implementation of instant message system based on P2P in LAN; exploring the translation mode for scientific discourses under the framework of information dualism; extraction method for image region of interest based on visual attention model; analysis on the effect of channel transmission rate on communication efficiency; design of AMBA-compliant image scaler circuit for low bus bandwidth; design of improved 8 mm band multimode matching feed; overview of digital watermarking; design of the common framework of LIN low-level driver; image processing in research of the digital human; comparison of SVM and ANN classifier for mammogram classification using ICA features; note on scheduling with group technology and the effects of deterioration and learning; reliability analysis and development of microcontroller unit system; finding all approximate palindromes of the string; an improved algorithm of Cohen-Sutherland line clipping; an improved algorithm of the ID3 based on impact factors; computing semantic relatedness for domain entities from encyclopaedias of digital publishing resource; a new algorithm of generating a minimum spanning tree of a graph based on recursion; performance of adaptive proportional average delay index for scheduler in LTE-A systems; a method for compositing web services based on model-checking multiagent systems; survey on support vector machine algorithms; adaptive fuzzy de-interlacing algorithm with motion detection; a new SVM algorithm and selected application in sequence analysis; image resolution enhancement method using multiple interpolations; design and implementation of handheld wireless LAN analyzer system; a single-field deinterlacing method utilizing second-order derivative; analysis of computer image processing technology and pattern recognition technology; conjoint analysis of statistics and computer technology; application of computer information technology in library information management system; safety protection of computer network and multiport technology; research on partner selection in cloud services supply chain; research of CRB based on grey-relation analysis and its application in aviation fault diagnosis; detecting emotional diction in texts; research on entrepreneurial opportunity identification with technology acceptance model; on building an English website for Tujia traditional culture; enhancing bank reputation by centralizing bank debtor information system; research on promotion of grinding fineness on beneficiation indicators; about the transformation methods on the spectral moments of trees; design of secured U-health application middleware based on OTP; semiautomatic construction of lexicon for the recognition of restaurant attributes; building innovative design service model; developing the knowledge management platform in the APQP procedure; analysis on new enterprise marketing model in the internet age; research on the significance of enterprises human resource management in business process; analysis of tourism enterprises standardization and its long-term development; role of business English on business management; analysis of the relationship between corporate culture and performance management; music appreciation on the construction of enterprise culture; enterprise financial management innovation based on market economy; role of the motivation method in the physical education management of school; analysis on status and suggestion of competitive sports management system; application of humanistic management in sports club; influence of intangible cultural heritages on tourism; personalized management measures in educational reform; three-dimensional visualization technology in landscape design; management system of community sports in the new era; restriction of accounting computerization on financial management; applications of computer digital technology in sports competition; benign development of industry supply chain under financial system; enforcement of scientific management strategy in education; study of strategic enterprise management based on circular economy; role of human-based management in student; the research and implementation of bus monitoring card of fiber channel; legal sentencing as knowledge processing and design of college students personal physical health management system.",,2014,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Software engineering: The current practice","Software Engineering: The Current Practice teaches students basic software engineering skills and helps practitioners refresh their knowledge and explore recent developments in the field, including software changes and iterative processes of software development. After a historical overview and an introduction to software technology and models, the book discusses the software change and its phases, including concept location, impact analysis, refactoring, actualization, and verification. It then covers the most common iterative processes: agile, directed, and centralized processes. The text also journeys through the software life span from the initial development of software from scratch to the final stages that lead toward software closedown. For Professionals. The book gives programmers and software managers a unified view of the contemporary practice of software engineering. It shows how various developments fit together and fit into the contemporary software engineering mosaic. The knowledge gained from the book allows practitioners to evaluate and improve the software engineering processes in their projects. For Instructors. Instructors have several options for using this classroom-tested material. Designed to be run in conjunction with the lectures, ideas for student projects include open source programs that use Java or C++ and range in size from 50 to 500 thousand lines of code. These projects emphasize the role of developers in a classroom-tailored version of the directed iterative process (DIP). For Students. Students gain a real understanding of software engineering processes through the lectures and projects. They acquire hands-on experience with software of the size and quality comparable to that of industrial software. As is the case in the industry, students work in teams but have individual assignments and accountability.<br/> &copy; 2012 by Taylor & Francis Group, LLC. All rights reserved.","Rajlich, Vaclav",2016,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"23rd International Conference on Neural Information Processing, ICONIP 2016","The proceedings contain 296 papers. The special focus in this conference is on Applications, Computational, Cognitive Neurosciences, Theory and Algorithms. The topics include: Classifying human activities with temporal extension of random forest; unregistered bosniak classification with multi-phase convolutional neural networks; data analysis of correlation between project popularity and code change frequency; prediction of bank telemarketing with co-training of mixture-of-experts and MLP; android malware detection method based on function call graphs; fast color quantization via fuzzy clustering; topological order discovery via deep knowledge tracing; the effect of reward information on perceptual decision-making; an internal model of the human hand affects recognition of graspable tools; a framework for ontology based management of neural network as a service; bihemispheric cerebellar spiking network model to simulate acute VOR motor learning; speaker detection in audio stream via probabilistic prediction using generalized GEBI; attention estimation for input switch in scalable multi-display environments; fissionable deep neural network; on the singularity in deep neural networks; compressing word embeddings; self-organization on a sphere with application to topological ordering of Chinese characters; the ability of learning algorithms for fuzzy inference systems using vector quantization; data-based optimal tracking control of nonaffine nonlinear discrete-time systems and rule-based grass biomass classification for roadside fire risk assessment.",,2016,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"The application of ICASPA to critical and Safe mass calculations","ICASPA, an Improved Critical and Safe Parameters Algorithm, calculates the size of a fissile body for a given value of k-effective. ICASPA uses an iterative predictor/corrector algorithm in which the MONK9A code is used to derive a precise k-effective value for a given candidate body size; auxiliary equations are then used to adjust the body size. The process is repeated until MONK9A outputs the chosen target value of k-effective. EDF Energy operates eight nuclear power stations comprising one Pressurized Water Reactor (PWR) and seven Advanced Gas-cooled Reactor (AGR) stations. At these sites, the term ""fuel route"" is used to encompass all of the ex-reactor arrangements for the handling and storage of fuel. The criticality safety cases for these fuel routes are based on the use of geometrically safe arrangements for the required quantities of fuel. The allowed quantities and arrangements of fuel are safely sub-critical, even under the contingency of accidental moderator ingress. The fuel route safety cases consider all credible potential fault conditions, including events such as dropped fuel. Any such events might change the geometry of the fuel elements, but it is difficult to calculate exact k-effective values for realistic models of dropped fuel. This difficulty is overcome by the use of an alternative approach in which limiting safe masses are calculated for bounding (worst-case) rearrangements of fuel rods and moderators, using geometries similar to those covered by criticality handbook data. Where required, the resulting safe masses are used to set safety case limits and conditions. For example, some fire extinguishants, such as water and ABC dry powder, are also effective moderators. Hence, in some fuel route areas, administrative criticality controls are used to either prohibit or limit the permitted quantities of these materials. This paper describes the work that has been carried out to adopt ICASPA as the approved calculation method for these safe mass calculations. The work has been carried out within the company quality assurance requirements for nuclear safety related software. It has involved the development of a configuration controlled version of ICASPA and the local verification and validation of the resulting code. The benefits from the adoption of ICASPA are also presented. These include accuracy improvements and, because the method uses the MONK9A code for k-effective calculations, the ability to use more sophisticated models for limiting case geometries.<br/>","Putley, D. and Martin, J.S. and Henderson, M.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Analysis of turbofan performance under total pressure distortion at various operating points","Inlet distortion is an important consideration in fan performance. The focus of this paper is a series of high-fidelity time accurate Computational Fluid Dynamics (CFD) simulations of a multistage fan at choke, design, and near stall operating conditions. These investigate distortion transfer and generation as well as the underlying flow physics of these phenomena under different operating conditions. The simulations are performed on the full annulus of a 3 stage fan and are analyzed. The code used to carry out these simulations is a modified version of OVERFLOW 2.2. The inlet is specified as a 1/rev total pressure distortion. Analysis includes the phase and amplitude of total temperature and pressure distortion through each stage of the fan and blade loading. The total pressure distortion does not change in severity through the fan, but the peak pressure distortion rotates by as much as 45&deg; at the near stall point. This is due to a variation in the work input around the blades of the rotor. This variation is also responsible for the generation of total temperature distortion in the fan. The rotation of the total temperature distortion becomes more pronounced as the fan approaches stall, and the total temperature distortion levels increase. The amount of work performed by a single blade can vary by as much as 25% in the first stage at near stall. The variation in work becomes more pronounced as the fan approaches stall. The passage shock in the rotor blades moves nearly 20% of the blade chord in both the peak efficiency and near stall cases.<br/> Copyright &copy; 2015 by ASME.","Weston, David B. and Gorrell, Steven E. and Marshall, Matthew L. and Wallis, Carol V.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"EarSketch: An authentic, STEAM-based approach to computing education","Demand for computer scientists is robust, but the pipeline for producing them is not. US universities are only meeting about a third of demand for computer scientists, and recruiting a diverse student body is a struggle; the number of women in computer science has actually declined in the past decade. To help change the perception of the computing field, researchers at Georgia Institute of Technology developed EarSketch. EarSketch is an authentic STEAM (STEM + Arts) environment for teaching and learning programming (i.e. where learners are engaged in authentic practices both in computing and in the aesthetics of music remixing) aimed at increasing and broadening participation in computing. In the EarSketch environment, students write code to manipulate, or remix, musical samples. It is an integrated programming environment, digital audio workstation, curriculum, and audio loop library. EarSketch has already been piloted in multiple classroom environments, including Computer Science Principles (CSP) classes in Atlanta-area high schools, in summer workshops, as part of a MOOC music technology course, in undergraduate computing courses for non-majors, and in a graduate digital media course at Georgia Tech. EarSketch is unique from other STEAM projects in computing education in that it is authentic both from an artistic perspective and from a computing perspective. That is, students create music in popular, personally relevant styles and genres, like dubstep and techno, and also learn to code in an industry-relevant language, like Python or JavaScript, in a free, browser-based environment. In addition, the barriers to entry are kept low; no previous knowledge of music performance or composition is required to engage successfully with EarSketch. In this paper, we present a description of the EarSketch environment and curriculum. We also present an overview of the classroom environments in which EarSketch has been implemented to date, including professional development feedback, student artifacts, student engagement data, and student achievement. The authors believe that EarSketch has the potential to serve as an introductory programming unit for a variety of courses in both pre-college and college settings. Based on initial data, EarSketch is an effective method for teaching programming of musical content and is effective in improving motivation to succeed on computing problems. &copy; American Society for Engineering Education, 2016.","Moore, Roxanne and Edwards, Douglas and Freeman, Jason and Magerko, Brian and McKlin, Tom and Xambo, Anna",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Interactive topology optimization on hand-held devices","This paper presents an interactive topology optimization application designed for hand-held devices running iOS or Android. The TopOpt app solves the 2D minimum compliance problem with interactive control of load and support positions as well as volume fraction. Thus, it is possible to change the problem settings on the fly and watch the design evolve to a new optimum in real time. The use of an interactive app makes it extremely simple to learn and understand the influence of load-directions, support conditions and volume fraction. The topology optimization kernel is written in C# and the graphical user interface is developed using the game engine Unity3D. The underlying code is inspired by the publicly available 88 and 99 line Matlab codes for topology optimization but does not utilize any low-level linear algebra routines such as BLAS or LAPACK. The TopOpt App can be downloaded on iOS devices from the Apple App Store, at Google Play for the Android platform, and a web-version can be run from www.topopt.dtu.dk. &copy; 2012 Springer-Verlag.<br/>","Aage, Niels and Nobel-Jorgensen, Morten and Andreasen, Casper Schousboe and Sigmund, Ole",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Proceedings - Asia-Pacific Software Engineering Conference, APSEC","The proceedings contain 58 papers. The topics discussed include: task recommendation with developer social network in software crowdsourcing; EXPSOL: recommending online threads for exception-related bug reports; retrieving design pattern usage examples using domain matching; LibSift: automated detection of third-party libraries in Android applications; does the role matter? an investigation of the code quality of casual contributors in GitHub; a model checking based approach for containment checking of UML sequence diagrams; model driven software security architecture of systems-of-systems; analytical study of cognitive layered approach for understanding security requirements using problem domain ontology; a map of threats to validity of systematic literature reviews in software engineering; heterogeneous cross-company effort estimation through transfer learning; an algorithmic-based change effort estimation model for software development; achieving high code coverage in Android UI testing via automated widget exercising; testing android apps via guided gesture event generation; model-based API-call constraint checking for automotive control software; and minimalist qualitative models for model checking cyber-physical feature coordination.",,2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Effect of superficial velocity of air and riser cross sectional area on heat transfer characteristics of circulating fluidized bed","The present paper describes a numerical study on wall-to-bed heat transfer characteristics of circulating fluidized bed (CFB) risers of cross section 0.15 (m) &times;0.15 (m), 0.30 (m) &times; 0.30 (m), each of height 2.85 (m). 3-D CFD simulations for heat transfer characteristics were carried out under same operating conditions for heated portion (heater) of risers. For modeling and simulation, CFD code Ansys - Fluent version 13 was used. Modeling and meshing were done using ProE and Ansys ICEM CFD software, respectively. The wall of heater was maintained at the constant heat flux q"" = 1000 (W/m2). RNG k-&Epsilon; model was used for turbulence modeling. Gidaspow model for phase interaction was used for the simulation of two phase flow (air + sand mixture flow). Effect of increase in superficial velocity of air from 2.5 (m/s) to 4 (m/s) on heat transfer characteristics was studied experimentally and numerically for the CFB riser of riser of cross section 0.15 (m) x 0.15 (m). Effect of change in riser cross sectional area on heat transfer characteristics was also studied numerically when both the CFB risers were operated under same operating conditions. Results on heat transfer characteristics were obtained In terms of distribution of bed (air + sand mixture) temperature across the heater and local heat transfer coefficient along the height of the heater of the CFB risers. Results obtained through CFD simulations were compared with available experimental data which was obtained using available CFB setup of IIT Guwahati.<br/>","Patil, R.S. and Mahanta, P. and Pandey, M.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An evolutionary framework for culture and creativity: Selectionism versus communal exchange","Dawkins' replicator-based conception of evolution has led to widespread mis-application of selectionism across the social sciences because it does not address the paradox that inspired the theory of natural selection in the first place: how do organisms accumulate change when traits acquired over their lifetime are obliterated? This is addressed by von Neumann's concept of a self-replicating automaton (SRA). An SRA consists of a self-assembly code that is used in two distinct ways: (1) actively deciphered during development to construct a self-similar replicant, and (2) passively copied to the replicant to ensure that it can reproduce. Information that is acquired over a lifetime is not transmitted to offspring, whereas information that is inherited during copying is transmitted. In cultural evolution there is no mechanism for discarding acquired change. Acquired change can accumulate orders of magnitude faster than, and quickly overwhelm, inherited change due to differential replication of variants in response to selection. This prohibits a selectionist but not an evolutionary framework for culture and the creative processes that fuel it. Recent work on the origin of life suggests that early life evolved through a non-Darwinian process referred to as communal exchange that does not involve a self-assembly code, and that natural selection emerged from this more haphazard, ancestral evolutionary process. It is proposed that communal exchange provides an evolutionary framework for culture that enables specification of cognitive features necessary for a (real or artificial) society to evolve culture. This is supported by a computational model of cultural evolution and a conceptual network based program for documenting material cultural history, and it is consistent with high levels of human cooperation. &copy; 2012 Elsevier B.V. All rights reserved.","Gabora, Liane",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Investigation on the aerodynamic performance of an annular exhaust system for a small turboshaft engine","An annular exhaust system design for being used in the bench testing of MTR390-E turboshaft engine has been performed at ITP. The exhaust system is aimed at improving the aerodynamic performance at high power compared with an existing exhaust system used in the previous version of the engine. The exhaust cone emulates to some extend the exhaust system in the helicopter and it is comprised of outer and inner cones supported by three struts. The CFD commercial code FLUENT is used to investigate the aerodynamic performance of the baseline design and to optimise the inner and outer cone angles in the new design based on 2D axisymmetric models. Representative radial exit turbine conditions and far field conditions are imposed in the model comprising the exhaust cones plus a large external domain. Two outer and inner cone angles and two inner cone lengths are analysed at low and high power conditions. The aerodynamic performance of the exhaust shows high sensitivity to the inlet flow angle which varies up to 30/40 between the high and low power conditions In all the simulated cases a large separation region is generated after the inner cone. Due to the high swirling flow the separation bubble behind the plug growths downstream hence reducing the effective flow exit area compared with the geometry area and reducing the pressure recovery downstream once the flow has been separated from the inner cone. Although all cases show similar qualitative behaviour, the best case based on the computed figures of merit (i.e., lowest total pressure loss) is chosen for the new design In order to further optimise the behaviour of the exhaust at high power, in the new design the three struts are aligned with the flow angle at high power conditions (struts were axially oriented in the baseline design) and the resulting geometry is analysed by 3D CFD simulations. As expected, the orientation of the struts has a dramatic impact in the aerodynamic behaviour of the exhaust. The new design shows an improvement of 29% in pressure recovery at high power compared with the baseline configuration, although it shows a degradation of 12% at low power. Both the baseline and the new exhaust systems are tested with the real engine in the test bench. The general aerodynamic performance of the new design is compared with the CFD simulation. As a consequence of the design change an important modification in the aerodynamic behaviour of the exhaust is obtained impacting the whole engine performance. Therefore a new performance model of the exhaust system is proposed to be implemented in the whole engine performance model in order to accurately simulate the behaviour of the engine coupled with the new exhaust. &copy; 2013 ASME.<br/>","De La Calzada, Pedro and Parra, Jorge and Minguez, Belen",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"AODV Routing Protocol Modification with Dqueue(dqAODV) and Optimization with Neural Network for VANET in City Scenario","Vehicular ad hoc network (VANET) is considered as a sub-set of mobile ad hoc network (MANET). VANET can provide road safety by generating collision warning messages before a collision takes place, lane change assistance; can provide efficient traffic system by introducing cooperation among vehicles; and can also improves in infotainment applications like cooperative file accessing, accessing internet, viewing movies etc. It provides smart Transportation System i.e., wireless ad-hoc communication among vehicles and vehicle to roadside equipments. VANET communication broadly distinguished in two types; 1) vehicle to vehicle interaction, 2) vehicle to infrastructure interaction. The main objective of VANET is to provide safe, secure and automated traffic system. For this automated traffic techniques, there are several types of routing protocols has been developed. MANET routing protocols are not equally applicable in VANET. In the recent past Roy and his group has proposed several study in VANET transmission in [1-3]. In this study, we propose a modified AODV routing protocol in the context of VANET with the help of dqueue introduction into the RREQ header. Recently Saha et al [4] has reported the results showing the nature of modified AODV obtained from the rudimentary version of their simulation code. It is mainly based on packet delivery throughput. It shows greater in-throughput information of packet transmission compare to original AODV. Hence our proposal has less overhead and greater performance routing algorithm compared to conventional AODV. In this study, we propose and implement in the NCTUns-6.0 simulator, the neural network based modified dqueue AODV (dqAODV) routing protocol considering Power, TTL, Node distance and Payload parameter to find the optimal route from the source station (vehicle) to the destination station in VANET communications. The detail simulation techniques with result and output will be presented in the conference.<br/> &copy; Owned by the authors, published by EDP Sciences, 2016.","Saha, Soumen and Roy, Utpal and Sinha, D.D.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Work in progress: Teaching design theory and mastercam in a hybrid flipped classroom environment","The revision of the junior-level Computer Aided Design and Manufacturing course MFGE3316 was driven by three forces: ABET, keeping current on an ever-changing software program, and fostering classroom discussion. At an ABET outcomes annual review, the consensus opinion was that the students could effectively design but they needed more practice to better recognize the concepts of engineering design theory. This weakness led to a push for students to practice more with designing and design theory before their senior design courses. Traditionally, MFGE3316 lectured on design theory, fundamentals of CAD/CAM systems, and CNC code generation by CAD/CAM software using a combined class/lab time. The Mastercam software is important for preparing students for industry, but was taking significant classroom time and resources. The revised course pedagogy is a hybrid flipped classroom environment to shift instruction of software use out of the classroom, but the instructor did not have the time or resources to create and continually update video content on how to use Mastercam. Instead, the instructor assigned an ""e-text"" (SolidProfesor account) for the course. The videos from the e-text are assigned to be watched before coming to class for that topic. During class the instructor does a short overview, leads discussion, and then the students work on the lab. Previously, a significant portion of the lab and class time was devoted to lecturing on software use. This change in pedagogy has allowed more time for in-class discussion and in-class design exercises. As well, the change in presentation style has resulted in more rapid understanding of Mastercam, as evidenced by the semester week in which the class completes the labs. The use of the e-text has also assisted the instructor with keeping class content up-to-date for each new version of the software without having to personally create new videos. The effectiveness of the additional time spent on design theory was assessed with the beginning of semester and end of semester engineering design self-efficacy survey instrument. This instrument was administered to determine if the course and time spent on design had an effect on the students' engineering design self-efficacy.<br/> &copy; American Society for Engineering Education, 2017.","Talley, Austin and Talley, Kimberly Grau",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Scientific Research - 4th International Conference, S-BPM ONE 2012, Proceedings","The proceedings contain 14 papers. The topics discussed include: the subject-oriented approach to software design and the abstract state machines method; ad-hoc adaptation of subject-oriented business processes at runtime to support organizational learning; an approach towards subject-oriented access control; building a conceptual roadmap for systemic change - a novel approach to change management in expert organizations in health care; e-learning support for business process modeling: linking modeling language concepts to general modeling concepts and vice versa; from subject-phase model based process specifications to an executable workflow; modeling business objectives for business process management; stakeholder-driven collaborative modeling of subject-oriented business processes; and using S-BPM for PLC code generation and extension of subject-oriented methodology to all layers of modern control systems.",,2012,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Fast reactor design using the advanced reactor modeling interface","The Advanced Reactor Modeling Interface (ARMI) code system has been developed at TerraPower to enable rapid and robust core design. ARMI is a modular modeling framework that loosely couples nuclear reactor simulations to provide high-fidelity system analysis in a highly automated fashion. Using a unified description of the reactor as input, a wide variety of independent modules run sequentially within ARMI. Some directly calculate results, while others write inputs for external simulation tools, execute them, and then process the results and update the state of the ARMI model. By using a standardized framework, a single design change, such as the modification of the fuel pin diameter, is seamlessly translated to every module involved in the full analysis; bypassing errorprone multi-analyst, multi-code approaches. Incorporating global flux and depletion solvers, subchannel thermalhydraulics codes, pin-level power and flux reconstruction methods, detailed fuel cycle and history tracking systems, finite element-based fuel performance coupling, reactivity coefficient generation, SASSYS-1/SAS4A transient modeling, control rod worth routines, and multi-objective optimization engines, ARMI allows ""one click"" steady-state and transient assessments throughout the reactor lifetime by a single user. This capability allows a user to work on the full-system design iterations required for reactor performance optimizations that has traditionally required the close attention of a multidisciplinary team. Through the ARMI framework, a single user can quickly explore a design concept and then consult the multi-disciplinary team for model validation and design improvements. This system is in full production use for reactor design at TerraPower, and some of its capabilities are demonstrated in this paper by looking at how design perturbations in fast reactor core assemblies affect steady-state performance at equilibrium as well as transient performance. Additionally, the pin-power profile is examined in the high flux gradient portion of the core to show the impact of the perturbations on pin peaking factors. Copyright &copy; 2013 by ASME.<br/>","Cheatham, Jesse and Truong, Bao and Touran, Nicholas and Latta, Ryan and Reed, Mark and Petroski, Robert",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"DEM simulation of direct shear tests considering particle angularity effect","Particle shape has a significant effect on the microscopic and macroscopic mechanical behavior of granular assemblies. This paper focuses on effects of particle angularity on the mechanical behavior of granular materials and evolution of the contact force anisotropy during direct shear tests. To this end, a discrete polyhedral element model with a general contact force law for arbitrarily shaped bodies is carried out. Particle angularity is defined in terms of the sphericity and the number of vertexes of a polyhedron. Four groups of assemblies with different particle angularities consist of non-cohesive mono-sized reasonably symmetric polyhedral particles. Direct shear tests are simulated using a modified version of the open source DEM code YADE. The results show that: the granular assembly has increasing shear strength and dilatation as particle angularity increases; the effect of angularity on the shear strength and dilative behavior of assemblies is more significant as the vertical loading is larger; the anisotropy of normal contact force increases at the start, then decreasing to remain a relatively stable value during shearing; and a larger change of the anisotropy of normal contact force after shearing is corresponding to a larger angularity.<br/> &copy;, 2015, Academia Sinica. All right reserved.","Zhao, Shi-Wei and Zhou, Xiao-Wen and Liu, Wen-Hui and Liu, Pan",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"CWSNET: An object-oriented toolkit for water distribution system simulations","In the field of water distribution systems the EPANET 2 toolkit is considered nowadays the industry standard for hydraulic modelling. Unfortunately, the design and programming model of EPANET 2 have some limitations that make any attempt to extend its hydraulic solver, add new functionalities or improve performance difficult to achieve and time consuming. A new software toolkit for water distribution system modelling, CWSNet, is presented. CWSNet is developed in C++ using the object-oriented programming model. The aim is to deliver an open-source substitute for EPANET 2 that obtains numerically comparable results while providing similar or better performance, a higher degree of extensibility, as well as backward compatibility where possible. The idea behind this project is to simplify development and testing of new hydraulic elements (specific types of valves, pumps, etc) and computational algorithms (pressure-driven approaches, etc.) by keeping logically independent parts of the code separate. This also allows the performance and accuracy of new computational methods as well as the use of advanced programming techniques (multi-threading, OpenMP, GPGPU, etc) to be studied without the need for extensive code refactoring. The basic version of CWSNet gives numerically the same results as EPANET 2 for various networks while allowing the following: (a) to change the topology of the network at runtime; (b) to run different simulations of the same network or different networks in parallel (thread-safe); (c) to easily change the mathematical model and other particulars behind the hydraulic simulation engine; (d) to allow a high degree of customisation of the output of an extended period simulation. The CWSNet software capabilities are demonstrated using several examples. The results obtained demonstrate the effectiveness and efficiency of the proposed approach. &copy; 2011 ASCE.<br/>","Guidolin, M. and Burovskiy, P. and Kapelan, Z. and Savic, D.A.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"1st International Conference on Advanced Informatics for Computing Research, ICAICR 2017","The proceedings contain 32 papers. The special focus in this conference is on Advanced Informatics for Computing Research. The topics include: Fuzzy based efficient mechanism for URL assignment in dynamic web crawler; towards filtering of SMS spam messages using machine learning based technique; intelligent computing methods in language processing by brain; classification algorithms for prediction of lumbar spine pathologies; keyword based identification of thrust area using mapreduce for knowledge discovery; an efficient genetic algorithm for fuzzy community detection in social network; priority based service broker policy for fog computing environment; software remodularization by estimating structural and conceptual relations among classes and using hierarchical clustering; requirements traceability through information retrieval using dynamic integration of structural and co-change coupling; bilingual code-mixing in Indian social media texts for Hindi and English; performance evaluation and comparative study of color image segmentation algorithm; electroencephalography based analysis of emotions among Indian film viewers; fuel assembly height measurements at the nuclear power plant unit active zone; a novel approach to segment nucleus of uterine cervix pap smear cells using watershed segmentation; parametric study of various direction of arrival estimation techniques; deep CNN-based method for segmenting lung fields in digital chest radiographs; quality assessment of a job portal system designed using bout design pattern; analyzing factors affecting the performance of data mining tools; stable feature selection with privacy preserving data mining algorithm and an efficient routing protocol for DTN.",,2017,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Characterization with microturbulence simulations of the zero particle flux condition in case of a TCV discharge showing toroidal rotation reversal","In view of the stabilization effect of sheared plasma rotation on microturbulence, it is important to study the intrinsic rotation that develops in tokamaks that present negligible external toroidal torque, like ITER. Remarkable observations have been made on TCV, analysing discharges without NBI injection, as reported in [A. Bortolon et al. 2006 Phys. Rev. Lett. 97] and exhibiting a rotation inversion occurring in conjunction with a relatively small change in the plasma density. We focus in particular on a limited L-mode TCV shot published in [B. P. Duval et al. 2008 Phys. Plasmas 15], that shows a rotation reversal during a density ramp up. In view of performing a momentum transport analysis on this TCV shot, some constraints have to be considered to reduce the uncertainty on the experimental parameters. One useful constraint is the zero particle flux condition, resulting from the absence of direct particle fuelling to the plasma core. In this work, a preliminary study of the reconstruction of the zero particle flux hyper-surface in the physical parameters space is presented, taking into account the effect of the main impurity (carbon) and beginning to explore the effect of collisions, in order to find a subset of this hyper-surface within the experimental error bars. The analysis is done performing gyrokinetic simulations with the local (flux-tube) version of the Eulerian code GENE [Jenko et al 2000 Phys. Plasmas 7 1904], computing the fluxes with a Quasi-Linear model, according to [E. Fable et al. 2010 PPCF 52], and validating the QL results with Non-Linear simulations in a subset of cases.<br/> &copy; Published under licence by IOP Publishing Ltd.","Mariani, A. and Merlo, G. and Brunner, S. and Merle, A. and Sauter, O. and Gorler, T. and Jenko, F. and Told, D.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Low complexity time-concatenated turbo equalization for block transmission without guard interval: Part 1-The concept","This paper proposes a novel time-concatenated turbo equalization technique, chained turbo equalization (CHATUE), that allows block transmission systems to eliminate the guard interval (GI), while achieving excellent performance. The proposedCHATUEalgorithm connects turbo equalizers neighboring in time, so that they exchange information about their inter-block-interference components in the form of a posteriori log-likelihood ratio. The latest version of the low complexity sub-optimal turbo equalization technique for block-wise single carrier transmission, frequency domain soft cancellation and minimum mean squared error, is fully exploited in developing the CHATUE algorithm. Results of extrinsic information transfer chart analysis as well as a series of bit-error rate (BER) simulations show that excellent performances can be achieved without imposing heavy computational burden in multipath-rich (quasi-static) block Rayleigh fading channels. It is shown that, if the information bit-rate is kept identical (because it may be unpreferable for the industry to change the frame structure), the CHATUE algorithm achieves lower BER than that with block transmission with GI, because lower rate (strong) code for error protection can be used by utilizing the time-duration made available by eliminating the GI. In addition, by combining the proposed structure with a simple rate-1 doped accumulator, further BER improvement exhibiting clear turbo cliff can be achieved. A sister paper (a Part-2 paper) applies the proposed CHATUE algorithm to single carrier frequency division multiple access systems Hui et al. (Wirel Pers Commun, 2011). &copy; The Author(s) 2012.","Anwar, Khoirul and Matsumoto, Tad",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"2013 International Forum on Materials Science and Industrial Technology, IFMSIT 2013","The proceedings contain 264 papers. The special focus in this conference is on Materials Science and Industrial Technology. The topics include: Enzymatic actions of acid cellulase on cotton fabrics; lumping kinetics of hydrodesulfurization for crude longkou shale oil; effect of double elements Co-doping on nano-TiO<inf>2</inf> photocatalysis of rhodamine B solution; evaluation of empirical kinetics models of athermal martensite transformation in plain carbon and low alloy steels; preparation and characterization of super pure coal; preparation and characterization of stable TiO<inf>2</inf> colloid composing of nearly monodispersive TiO<inf>2</inf> nanocrystallite; molecular orbitals and the wave equation; a review for granular media; review of thyristor junction temperature calculation methods; effect of MPBM on crosslinking reaction of tetrafluoroethylene-propylene copolymer/DCP; studies on an ecofriendly oxidative degradation system for phenol; the degradation of decabromodiphenyl ether in the supercritical fluid; a soil water characteristic curve model considering urea concentration; solid-phase extraction of salidroside by electrospun nanofibers; study on water stability of warm mix drainage asphalt with sasobit; the study of surface longitudinal crack on SPA-H steel slab by CSP; mesitylene-assisted formation of large pore SBA-15 microspheres; intercalation of Na-montmorillonite with ionic liquids; preparation of gold nanoparticles; a comparative study of the selective oxidation of NH3 to N2 over transition metal catalysts; applications of FRP material in bridge structures; the design of self-service intelligent stereo garage; forming stability analysis of surface flexible rolling; a video-based theodolite simulation system; injection moulding simulation analysis of handle shell; the key stamping process analysis and mold design; the method of optimal phasor measurement unit placement; output displacement analysis of symmetric four-bar mechanism with right angle flexible hinge; the force state analysis of the service door on an airliner; cavity tool path optimization in high-speed milling based on NX CAM; the mobile oilfield map based on SVG and inforamtion integration; the brittleness source identification of electric grid system; the function of the materials science in industrial design; applications of 3D stratum visualization CAD in geological exploration; simulation research on the performance of vehicle suspension system; the effectively method of detecting network traffic anomaly; iterative construct of reversible network based on cascade operation; rapidly construct the network teaching environment for training; microblog social network analysis based on network group behavior; hierarchical mobile IPv6 based handoff optimization scheme; analysis and optimization of mobile IPv6 handoff technology; variable multi-channel high frequency signal acquisition system; research on pattern classification of SVM-based gait signal; recent advances in preserving privacy data mining; research on a positioning application by RTK technique and with goGPS; design of temperature controlling system for CCD based on DSP; numerical simulation of chaotic laser secure communication; hardware design of code transmitter and monitor station; research on automatic line of wheeled robot based on MCU; study on dynamics of small tracked steering on ramp; research on propagation characteristics of SAW based on ANSYS; a new way for handwritten numeral recognition; the empirical analysis of function-call graph; chaos synchronization of the modified sprott E system; a 3D model watermarking algorithm; application of fuzzy mathematical method in insulator state inspection; image fusion with sparse representation; discussions on securities software expert system MA and RSI; sequences and series of functions on fractal space; on the perron root of nonnegative matrices; an data replication and deletion algorithm for web objects; the research on Chinese automatic segmentation; the importance of exhibition design in modern society; anonymity query method of outsourced database; an empirical study on factors influencing Taiwan's future basis of securities market; influence of modeling methods for housing price forecasting; VISC application in securities technology analysis teaching; models research of enterprise logistics cost control; research on the current situation of the efficiency evaluation of logistics; E-commerce business process modeling and verification based on Pi calculus; the development strategy of tourism informatization in small and medium-sized cities; the construction enterprises culture building based on knowledge management; history, current situation and development prospect of the TRIZ; content-based medical image retrieval system for color endoscopic images; effect of salidroside on cardiac functional recovery; characterization for staling of Chinese rolls; study on the changes of urea content during brewing of Chinese Huangjiu; analysis and evaluation of the health care systems; progress of microbial removal of arsenic from wastewater; assessment on industrial land-use fitness in a resource based town; fitness evaluation for residential land in a mountainous town; the application and development of EM technology in municipal sewage treatment and introduction of new methods for insulator contaminant detection.",,2013,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"New elements in old tools: Nucleosynthesis in N-body codes","The main scientific goal of this project is to unravel for the first time the stellar assembly history of our Galaxy using an accurate dynamical and chemical approach. Of particular importance is to identify the early dissolution events of primordial stellar clusters and the overall impact on the chemical evolution of the Galaxy. We will do this by adding detailed nucleosynthesis models into the current state-of-the-art N-body code NBODY6. In this way we will obtain accurate predictions for the chemical evolution of the Galaxy and the evolution of stellar clusters, particularly the change of surface abundances from non-standard evolution, aspects which will be a novel implementation in the field. &copy; Copyright owned by the author(s) under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike Licence.<br/>","Loyola, Guido R.I. Moyano and Hurley, Jarrod R.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Dynamic seismic response analysis of nuclear storage tank based on fluid-structure coupling method","The storage tanks in nuclear facilities has a significant impact on the safety of the reactor and the radiation shielding, so its mechanical property analysis has been widely concerned in the field of engineering and scientific research. Meanwhile, the storage tank is usually filled with gas and liquid medium. In the presence of external disturbances (such as external force, displacement, earthquake etc.), the position and structure of the vessel changes, that lead to changing of the gas-liquid interface. This characteristic can make the storage tank system as a tightly fluid-structure coupling system. In this paper, a storage tank which stored radioactive gas liquid medium is choosing to study such fluid-structure coupling system phenomenon, and a typical dynamic seismic condition is assumed. A two-way fluid-structure coupling method is used with CFD (Computational Fluid Dynamics) and FEM (Finite Element Method) numerical method. The study considered interaction between structure and two phase turbulent fluid. In FEM calculation, the time history seismic acceleration load is applied to the support of tank, and the flow loading coming from fluid medium is applied to the wall of tank which is send from CFD code. Then, the structure displacement which is calculate by FEM is transferred to CFD code. In CFD calculation, multiphase fluid numerical model is applied to simulate the flow characteristics of gas-water two phase fluid, and the turbulent properties are also considered in the calculation. Mesh deformation method is used to simulate the displacement of flow passage boundary which is send by FEM code. After CFD calculation, flow loading is transferred to the tank wall of FEM code again. Such loop of FEM and CFD calculation continues to go on with the seismic time history, the response characteristics of the tank will be solved. In order to evaluate the difference between the above method and the traditional analysis method. An independent calculation used added mass approach is carrying out, in which the effect of steady state water is applied to the wall of the vessel, and this load will not change with the earthquake. All others load and constraint mode are same with the above method. According to the two-way fluid-structure coupling analysis, the detailed characteristics of liquid free surface distribution and structural response of the vessel are obtained. The results show that the response vibration amplitude of the tank structure increases with the earthquake, and the response is mainly affected by the liquid sloshing. According to comparative analysis, the advantages of coupling method are proved. The method from this study can be used for the same type of analysis.<br/> &copy; Copyright 2017 ASME.","Weng, Yu and Liu, Lang and Jiang, Yang and Gu, Hongfang and Wang, Haijun",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Uncertainty analysis of the tru-burning thorium-fueled rbwr using generalized perturbation theory","The RBWR-TR is a thorium-based reduced moderation BWR (RBWR) with a high transuranic (TRU) consumption rate. It is charged with LWR TRU and thorium, and it recycles all actinides an unlimited number of times while discharging only fission products and trace amounts of actinides through reprocessing losses. This design is a variant of the Hitachi RBWR-TB2, which arranges its fuel in a hexagonal lattice, axially segregates seed and blanket regions, and fits within an existing ABWR pressure vessel. The RBWR-TR eliminates the internal axial blanket, eliminates absorbers from the upper reflector, and uses thorium rather than depleted uranium as the fertile makeup fuel. This design has been previously shown to perform comparably to the RBWR-TB2 in terms of TRU consumption rate and burnup, while providing significantly larger margin against critical heat flux. This study examines the uncertainty in key neutronics parameters due to nuclear data uncertainty. As most of the fissions are induced by epithermal neutrons and since the reactor uses higher actinides as well as thorium and 233U, the cross sections have significantly more uncertainty than in typical LWRs. The sensitivity of the multiplication factor (keff) to the cross sections of many actinides is quantified using a modified version of Serpent 2.1.19 [1]. Serpent [2] is a Monte Carlo code which uses delta tracking to speed up the simulation of reactors; in this modified version, cross sections are artificially inflated to sample more collision, and collisions are rejected to preserve a ""fair game."" The impact of these rejected collisions is then propagated to the multiplication factor using generalized perturbation theory [3]. Covariance matrices are retrieved for the ENDF/B-VII.1 library [4], and used to collapse the sensitivity vectors to an uncertainty on the multiplication factor. The simulation is repeated for several reactor configurations (for example, with a reduced flow rate, and with control rods inserted), and the difference in keff sensitivity is used to assess the uncertainty associated with the change (the uncertainty in the void feedback and the control rod worth). The uncertainty in the RBWR-TR is found to be dominated by the epithermal fission cross section for 233U in reference conditions, although when the spectrum hardens, the uncertainty in fast capture cross sections of 232Th becomes dominant.<br/> Copyright &copy; 2017 ASME.","Bogetic, Sandra and Gorman, Phillip and Aufiero, Manuele and Fratoni, Massimiliano and Greenspan, Ehud and Vujic, Jasmina",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Mean composite fire severity metrics computed with google earth engine offer improved accuracy and expanded mapping potential","Landsat-based fire severity datasets are an invaluable resource for monitoring and research purposes. These gridded fire severity datasets are generally produced with pre- and post-fire imagery to estimate the degree of fire-induced ecological change. Here, we introduce methods to produce three Landsat-based fire severity metrics using the Google Earth Engine (GEE) platform: The delta normalized burn ratio (dNBR), the relativized delta normalized burn ratio (RdNBR), and the relativized burn ratio (RBR). Our methods do not rely on time-consuming a priori scene selection but instead use a mean compositing approach in which all valid pixels (e.g., cloud-free) over a pre-specified date range (pre- and post-fire) are stacked and the mean value for each pixel over each stack is used to produce the resulting fire severity datasets. This approach demonstrates that fire severity datasets can be produced with relative ease and speed compared to the standard approach in which one pre-fire and one post-fire scene are judiciously identified and used to produce fire severity datasets. We also validate the GEE-derived fire severity metrics using field-based fire severity plots for 18 fires in the western United States. These validations are compared to Landsat-based fire severity datasets produced using only one pre- and post-fire scene, which has been the standard approach in producing such datasets since their inception. Results indicate that the GEE-derived fire severity datasets generally show improved validation statistics compared to parallel versions in which only one pre-fire and one post-fire scene are used, though some of the improvements in some validations are more or less negligible. We provide code and a sample geospatial fire history layer to produce dNBR, RdNBR, and RBR for the 18 fires we evaluated. Although our approach requires that a geospatial fire history layer (i.e., fire perimeters) be produced independently and prior to applying our methods, we suggest that our GEE methodology can reasonably be implemented on hundreds to thousands of fires, thereby increasing opportunities for fire severity monitoring and research across the globe.<br/> &copy; 2018 by the authors.","Parks, Sean A. and Holsinger, Lisa M. and Voss, Morgan A. and Loehman, Rachel A. and Robinson, Nathaniel P.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"2012 6th International Workshop on Software Clones, IWSC 2012 - Proceedings","The proceedings contain 24 papers. The topics discussed include: an accurate estimation of the Levenshtein distance using metric trees and Manhattan distance; a novel approach based on formal methods for clone detection; claims and beliefs about code clones: do we agree as a community? a survey; clone detection using rolling hashing, suffix trees and dagification: a case study; dispersion of changes in cloned and non-cloned code; java bytecode clone detection via relaxation on code fingerprint and semantic web reasoning; mining object-oriented design models for detecting identical design structures; safe clone-based refactoring through stereotype identification and iso-generation; a case study on applying clone technology to an industrial application framework; industrial application of clone change management system; a common conceptual model for clone detection results; and filtering clones for individual user based on machine learning analysis.",,2012,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Adoption and use of Java generics","Support for generic programming was added to the Java language in 2004, representing perhaps the most significant change to one of the most widely used programming languages today. Researchers and language designers anticipated this addition would relieve many long-standing problems plaguing developers, but surprisingly, no one has yet measured how generics have been adopted and used in practice. In this paper, we report on the first empirical investigation into how Java generics have been integrated into open source software by automatically mining the history of 40 popular open source Java programs, traversing more than 650 million lines of code in the process. We evaluate five hypotheses and research questions about how Java developers use generics. For example, our results suggest that generics sometimes reduce the number of type casts and that generics are usually adopted by a single champion in a project, rather than all committers. We also offer insights into why some features may be adopted sooner and others features may be held back. &copy; 2012 Springer Science+Business Media New York.<br/>","Parnin, Chris and Bird, Christian and Murphy-Hill, Emerson",2013,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR11"
"Analyzing the significance of process metrics for TTC software defect prediction","In the existing studies on software prediction, the most proposed methods are usually assessed over the public datasets like NASA metrics data repository, which include a combination of code metrics merely. Obviously, the process metric is also one of the key factors that affect the defect-proneness of software modules. In this paper, life-cycle based management process metrics set and history change process metrics set have been proposed based on the characteristics of development process. In order to analyze the importance of these different metrics for predicting defects in aerospace tracking telemetry and control (TT&amp;C) software, an improved PSO optimized support vector machine algorithm (PSO-SVM) has been presented and took into application. The experiment results over the actual TT&amp;C projects suggest that the prediction performance can be significance improved if the 2 kinds of process metrics are included in the model.<br/> &copy; 2014 IEEE.","Xia, Ye and Yan, Guoying and Zhang, Huiying",2014,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Simulation of cross-flow vortex-induced vibration of single tube based on two-way fluid-solid interaction method","The fatigue damage and lift force caused by vortex induced vibration occur very often in the core of the Pressurized Water Reactor (PWR) [1] It is extremely complex to illustrate the mechanism of vibration which induced by Cross-flow. With the spacer grids and wings, the flow direction which in axial direction at the inlet will change and create swirls, so there are many flow directions in the nuclear fuel component. Assumed the tube endure cross-flow only in this article to simplify the fluid model. Most researchers in this field often ignore the displacement of structure induced by the cross flow because the value is so small that not enough to change the fluid region. In truth conditions, the motion of the cylinder caused the wake oscillation and strengthen the vortex shedding, in turn, the vortex shedding will aggravate the vibration amplitude. According that, one way FSI (Fluid Solid Interaction) can't capture the influence from the cylinder vibration. In this article, Two-way FSI method was executed to get the vibration in time history in order to get the random vibration induced by the cross flow more close to the actual project. Using Finite Volume Method to discrete the fluid control equation and finite element method to discrete structure control equation combined with moving mesh technology. An interface between the fluid region and the structure region was created to transfer the fluid force and the structure displacement. Coupling CFD code and CSD(Computational Solid Dynamics) code to solve the differential equation and obtain the displacement of the cylinder in time history. A Fast Fourier Transfer (FFT) has been done to get the vibration frequency. An Analysis of the vortex shedding frequency and vibration frequency to find the correlation between the vortex shedding and the vibration frequency has been done. A modal analysis for the cylinder without water has been done to get the natural frequency. Results shows the cylinder has different response to the vortex shedding at different position of the cylinder in the same condition. There are more works need to be done aim to get the vibration mechanism in tandem tube and parallel tube to get clearly mechanism of vortex induced vibration in nuclear fuel assembly. The research of the vortex induced vibration in this article is a key to get on the follow research in more tubes array in different methods.<br/> &copy; Copyright 2017 ASME.","Zengzeng, Wang and Tao, Lu and Bo, Liu",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Defragmentation and mashup: Ludic mashup as a design approach","The history of technological progress has involved a repeated application of abstraction, of encapsulation, specialization and composition. Film, for example, has moved from a specialized field of equipment and concepts only available to trained professionals, into a field which has been commoditized and composited, and made available to almost everyone with basic equipment. New media has become more modular and thus passes into the hands of users who rely less on crafting from scratch and rely more on pre-built, readymade components that can be assembled. This ""pulling together"", i.e. this ""mashup"" or ""remix"" approach is already trivially true in the field of games in the modding community, which may introduce new 3D models, images, music or even new code blocks which change behaviors. These are very important, but signal a future move toward more sophisticated, pre-packaged modular blocks which players might assemble on their own in a more controlled manner. This might include swappable A.I. algorithms, interchangeable in-game weapons, interoperable ""rulesets"" and other key game entities that are normally thought of as being integral to a specific, single game. While mashup, assemblage and perhaps actor-network-theory has highlighted the ways in which a game played in context is more than the sum of its parts, this paper looks to the future of game design, in which players can assemble (on-the-fly) a set of game components. Such a situation is a defragmenting of ready-made ludic chunks, resulting in unpredictable and chaotic games created by players, and forces designers to consider their role less as a creator of a game in toto, but also as designers of interoperable ludic components.<br/> &copy; 2013 Lenhart & Digital Games Research Association DiGRA.","Lenhart, Isaac",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"IEEE International Symposium on Information Theory - Proceedings","The proceedings contain 603 papers. The topics discussed include: some gabidulin codes cannot be list decoded efficiently at any radius; state-dependent multiple-access channels with partially cribbing encoders; cooperative multiple access channels with oblivious encoders; on the cost and benefit of cooperation; rate-compatible spatially-coupled LDPC code ensembles with nearly-regular degree distributions; the multi-step peg and ace constrained PEG algorithms can design the LDPC codes with better cycle-connectivity; spatially-coupled split-component codes with bounded-distance component decoding; learning immune-defectives graph through group tests; blind identification of an unknown interleaved convolutional code; the likelihood decoder: error exponents and mismatch; exact asymptotics for the random coding error probability; achievable rates and exponents for asynchronous communication with ml decoding; and quickest change detection and Kullback-Leibler divergence for two-state hidden Markov models.",,2015,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Self-adaptive global best harmony search algorithm applied to reactor core fuel management optimization","The aim of this work is to apply the new developed optimization algorithm, Self-adaptive Global best Harmony Search (SGHS), for PWRs fuel management optimization. SGHS algorithm has some modifications in comparison with basic Harmony Search (HS) and Global-best Harmony Search (GHS) algorithms such as dynamically change of parameters. For the demonstration of SGHS ability to find an optimal configuration of fuel assemblies, basic Harmony Search (HS) and Global-best Harmony Search (GHS) algorithms also have been developed and investigated. For this purpose, Self-adaptive Global best Harmony Search Nodal Expansion package (SGHSNE) has been developed implementing HS, GHS and SGHS optimization algorithms for the fuel management operation of nuclear reactor cores. This package uses developed average current nodal expansion code which solves the multi group diffusion equation by employment of first and second orders of Nodal Expansion Method (NEM) for two dimensional, hexagonal and rectangular geometries, respectively, by one node per a FA. Loading pattern optimization was performed using SGHSNE package for some test cases to present the SGHS algorithm capability in converging to near optimal loading pattern. Results indicate that the convergence rate and reliability of the SGHS method are quite promising and practically, SGHS improves the quality of loading pattern optimization results relative to HS and GHS algorithms. As a result, it has the potential to be used in the other nuclear engineering optimization problems. &copy; 2013 Elsevier Ltd. All rights reserved.<br/>","Poursalehi, N. and Zolfaghari, A. and Minuchehr, A. and Valavi, K.",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"UAPD: Predicting Urban Anomalies from Spatial-Temporal Data","Urban city environments face the challenge of disturbances, which can create inconveniences for its citizens. These require timely detection and resolution, and more importantly timely preparedness on the part of city officials. We term these disturbances as anomalies, and pose the problem statement: if it is possible to also predict these anomalous events (proactive), and not just detect (reactive). While significant effort has been made in detecting anomalies in existing urban data, the prediction of future urban anomalies is much less well studied and understood. In this work, we formalize the future anomaly prediction problem in urban environments, such that those can be addressed in a more efficient and effective manner. We develop the Urban Anomaly PreDiction (UAPD) framework, which addresses a number of challenges, including the dynamic, spatial varieties of different categories of anomalies. Given the urban anomaly data to date, UAPD first detects the change point of each type of anomalies in the temporal dimension and then uses a tensor decomposition model to decouple the interrelations between the spatial and categorical dimensions. Finally, UAPD applies an autoregression method to predict which categories of anomalies will happen at each region in the future. We conduct extensive experiments in two urban environments, namely New York City and Pittsburgh. Experimental results demonstrate that UAPD outperforms alternative baselines across various settings, including different region and time-frame scales, as well as diverse categories of anomalies. Code related to this chapter is available at: https://bitbucket.org/xianwu9/uapd.<br/> &copy; 2017, Springer International Publishing AG.","Wu, Xian and Dong, Yuxiao and Huang, Chao and Xu, Jian and Wang, Dong and Chawla, Nitesh V.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Pattern formation of multi mobile robots using arm gesture action","In this paper, we get the arm gesture from Compass, Gyroscope and Ac- celerometer, and use fuzzy identification algorithm to analyze the arm gesture actions to control the formation of multiple mobile robots. The mobile robots move on a grid plane. The supervised computer determines the planning motion path of each mobile robot using the A*search algorithm with collision avoidance in consideration. In case of possible collision, robot with smaller ID code has higher priority to move. Five pattern formations are developed in our applications. The mobile robot contains a controller module, three IR sensor modules, a wireless RF module, and two DC servomotors. The mobile robot can detect signals from reective IR sensor module, decide the cross points of the aisle, receive the command from the supervised computer, and transmit the status of environment to the supervised computer via wireless RF interface. In the experimental results, we show that the mobile robots can receive the pattern formation command from the supervised computer, and then change the pattern formation on the motion platform without any collision. &copy; 2014 ICIC International.","Hsia, Kuo-Hsien and Li, Bo-Yi and Su, Kuo-Lan",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Drought influence on forest plantations in Zululand, South Africa, using MODIS time series and climate data","South Africa has a long history of recurrent droughts that have adversely affected its economic performance. The recent 2015 drought has been declared the most serious in 26 years and impaired key agricultural sectors including the forestry sector. Research on the forests' responses to drought is therefore essential for management planning and monitoring. The effects of the latest drought on the forests in South Africa have not been studied and are uncertain. The study reported here addresses this gap by using Moderate Resolution Imaging Spectroradiometer (MODIS)-derived normalized difference vegetation index (NDVI) and precipitation data retrieved and processed using the JavaScript code editor in the Google Earth Engine (GEE) and the corresponding normalized difference infrared index (NDII), Palmer drought severity index (PDSI), and El Ni&ntilde;o time series data for KwaMbonambi, northern Zululand, between 2002 and 2016. The NDVI and NDII time series were decomposed using the Breaks for Additive Seasonal and Trend (BFAST) method to establish the trend and seasonal variation. Multiple linear regression and Mann-Kendall tests were applied to determine the association of the NDVI and NDII with the climate variables. Plantation trees displayed high NDVI values (0.74-0.78) from 2002 to 2013; then, they decreased sharply to 0.64 in 2015. The Mann-Kendall trend test confirmed a negative significant (p = 0.000353) trend between 2014 and 2015. This pattern was associated with a precipitation deficit and low NDII values during a strong El Ni&ntilde;o phase. The PDSI (-2.6) values indicated severe drought conditions. The greening decreased in 2015, with some forest remnants showing resistance, implying that the tree species had varying sensitivity to drought. We found that the plantation trees suffered drought stress during 2015, although it seems that the trees began to recover, as the NDVI signals rose in 2016. Overall, these results demonstrated the effective use of the NDVI- and NDII-derived MODIS data coupled with climatic variables to provide insights into the influence of drought on plantation trees in the study area.<br/> &copy; 2018 by the authors.","Xulu, Sifiso and Peerbhay, Kabir and Gebreslasie, Michael and Ismail, Riyad",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Dynamics of content quality in collaborative knowledge production","We explore the dynamics of user performance in collaborative knowledge production by studying the quality of answers to questions posted on Stack Exchange. We propose four indicators of answer quality: answer length, the number of code lines and hyperlinks to external web content it contains, and whether it is accepted by the asker as the most helpful answer to the question. Analyzing millions of answers posted over the period from 2008 to 2014, we uncover regular short-term and long-term changes in quality. In the short-term, quality deteriorates over the course of a single session, with each successive answer becoming shorter, with fewer code lines and links, and less likely to be accepted. In contrast, performance improves over the long-term, with more experienced users producing higher quality answers. These trends are not a consequence of data heterogeneity, but rather have a behavioral origin. Our findings highlight the complex interplay between short-term deterioration in performance, potentially due to mental fatigue or attention depletion, and long-term performance improvement due to learning and skill acquisition, and its impact on the quality of user-generated content.<br/> &copy; Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Ferrara, Emilio and Alipoufard, Nazanin and Burghardt, Keith and Gopal, Chiranth and Lerman, Kristina",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Dynamic and speculative polyhedral parallelization using compiler-generated skeletons","We propose a framework based on an original generation and use of algorithmic skeletons, and dedicated to speculative parallelization of scientific nested loop kernels, able to apply at run-time polyhedral transformations to the target code in order to exhibit parallelism and data locality. Parallel code generation is achieved almost at no cost by using binary algorithmic skeletons that are generated at compile-time, and that embed the original code and operations devoted to instantiate a polyhedral parallelizing transformation and to verify the speculations on dependences. The skeletons are patched at run-time to generate the executable code. The run-time process includes a transformation selection guided by online profiling phases on short samples, using an instrumented version of the code. During this phase, the accessed memory addresses are used to compute on-the-fly dependence distance vectors, and are also interpolated to build a predictor of the forthcoming accesses. Interpolating functions and distance vectors are then employed for dependence analysis to select a parallelizing transformation that, if the prediction is correct, does not induce any rollback during execution. In order to ensure that the rollback time overhead stays low, the code is executed in successive slices of the outermost original loop of the nest. Each slice can be either a parallel version which instantiates a skeleton, a sequential original version, or an instrumented version. Moreover, such slicing of the execution provides the opportunity of transforming differently the code to adapt to the observed execution phases, by patching differently one of the pre-built skeletons. The framework has been implemented with extensions of the LLVM compiler and an x86-64 runtime system. Significant speed-ups are shown on a set of benchmarks that could not have been handled efficiently by a compiler. &copy; 2013 Springer Science+Business Media New York.<br/>","Jimborean, Alexandra and Clauss, Philippe and Dollinger, Jean-Francois and Loechner, Vincent and Martinez Caamano, Juan Manuel",2014,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"2nd International Conference on Real-Time Intelligent Systems, RTIS 2017","The proceedings contain 46 papers. The special focus in this conference is on Real-Time Intelligent Systems. The topics include: A New Vision for Multilingual Architecture; using Continuous Hopfield Neural Network for Choice Architecture of Probabilistic Self-Organizing Map; the Hybrid Framework for Multi-objective Evolutionary Optimization Based on Harmony Search Algorithm; new Prior Model for Bayesian Neural Networks Learning and Application to Classification of Tissues in Mammographic Images; Multilayer Perceptron: NSGA II for a New Multi-objective Learning Method for Training and Model Complexity; a New Quasi-Cyclic Majority Logic Codes Constructed from Disjoint Difference Sets by Genetic Algorithm; prediction of Coordinate Measuring Machines Geometric Errors by Measuring a Ball Step-Gauge; hierarchical Load Balancing Strategy in Cloud Environment; Impact of Hybrid Virtualization Using VM and Container on Live Migration and Cloud Performance; Applying Data Analytics and Cumulative Accuracy Profile (CAP) Approach in Real-Time Maintenance of Instructional Design Models; ioT Interoperability Architectures: Comparative Study; collaborative and Communicative Logistics Flows Management Using the Internet of Things; performance Analysis of Internet of Things Application Layer Protocol; simulation Automation of Wireless Network on Opnet Modeler; Smart SDN Policy Management Based VPN Multipoint; towards Smart Software Defined Wireless Network for Quality of Service Management; transformation of High Level Specification Towards nesC Code; Flexible Mobile Network Service Chaining in an NFV Environment: IMS Use Case; Performance Analysis of the Vertical Handover Across Wifi/3G Networks Based on IEEE 802.21; a New Encryption Scheme to Perform Smart Computations on Encrypted Cloud Big Data; a Meta-model for Real-Time Embedded Systems.<br/>",,2019,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Airfoil optimization for wind turbine application","An airfoil optimization method for wind turbine applications that controls the loss in performance due to leading edge contamination is developed and tested. The method uses the class-shape-transformation technique to parametrize the airfoil geometry and uses an adjusted version of the panel code XFOIL to calculate the aerodynamic performance. To find optimal airfoil shapes, the derivative-free Covariance Matrix Adaptation Evolution Strategy is used in combination with an adaptive penalty function. The method is tested for the design of airfoils for the outer part of a megawatt-class wind turbine rotor blade, and the results are compared with airfoils from Delft University. It is found that the method is able to automatically create airfoils with equal or improved performance compared with the Delft designs. For the tested application, the adjustments performed to the XFOIL code improve the maximum lift, post stall, and the overall drag predictions.<br/> Copyright &copy; 2018 John Wiley & Sons, Ltd.","Hansen, T.H.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Software regression and migration assistance using dynamic instrumentation","Companies and organizations use the legacy software for decades to serve various purposes. During this journey, the software system travels through several change requests and amendments of functionalities due to the changing nature of business and other requirements. As a result, different methodologies and implementations employed over the time are often not at all documented. So, modifying or migrating those software systems become difficult due to lack of technical knowledge about their behavior. This difficulty is even more when there is no Subject-Matter Expert (SME). Here, we propose a technique to verify the unchanged functionalities of untouched modules of the modified application by comparing with the older version of the application. Sometimes, the number of functional behaviors become irrelevant as they are no longer required by the business. However, significantly large portions of legacy applications continue executing, untouched by any modification or customization, to serve tiny yet critical purposes. Stakeholders also remain reluctant to cleanup or migrate because only for finding out the active part or functionals scope of the application is very tedious and consumes lot of effort due to lack of knowledge or documentation. Here, we have devised a mechanism to assist the migration specialists to identify the active part of an application, associated files, and data used by the active code that help in building the new one with similar functionalities. We can also assist the performance engineer by detecting the resource leakage in the application.<br/> &copy; 2019, Springer Nature Singapore Pte Ltd.","Chatterjee, Nachiketa and Chakrabarti, Amlan and Das, Partha Pratim",2019,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Corrigendum to “Relationships between human activity and biodiversity in Europe at the national scale: Spatial density of human activity as a core driver of biodiversity erosion” (Ecological Indicators (2018) 90 (356–365), (S1470160X18301559) (10.1016/j.ecolind.2018.03.010))","This corrigendum aims to correct the first three columns of Table 3 in Gosselin and Callois (2018), which were based on an erroneous program to calculate marginal Leave-one-out Information Criteria (LOOIC). Table 1 in the present paper replaces this previous Table 3. We discuss why the new results are more in agreement with the other results of Gosselin and Callois (2018) and why the marginal version of the LOOIC appears better than the conditional version. Statistical methods: Here, the statistical models are the same as the ones in Gosselin and Callois (2018). They are Bayesian models which incorporate country random effects when observations were repeated in each country (for the two State Indicators and the Response Indicator), and which have a beta-binomial probability distribution for the two State Indicators and a zero-inflated beta distribution for the Response Inidcator and the two Pressure indicators. The zero-inflated beta distribution is a direct sub-product of the zero-inflated cumulative beta distribution that Herpigny and Gosselin (2015; MTUnlimited2 version) proposed to analyze plant cover class data. Indeed, we used statistical methods with a priori relevant probabilistic properties relative to our data. Based on the lessons learned on partly similar data from Gosselin (2015), we paid careful attention to the potential over-dispersion of data as well as to the inclusion of random country effects. We compared the models with the Leave-one-out Information Criterion (LOOIC) developed by Vehtari et al. (2016). For State and Response indicators, we used the marginal version of the LOOIC &ndash; i.e. the LOOIC integrated over the random effects &ndash; for reasons discussed by Millar (2009, 2017), and because the country random effect introduced a number of parameters which was of the same order of magnitude as the number of observations. We calculated the marginal versions of the LOOIC for 10,000 parameter values randomly drawn from the MCMC output by taking the mean of probabilities of the observed data grouped by country over 1,000 random draws of the country random effect. The original code in Gosselin and Callois (2018) was erroneous in that it did not take into account the dependence between observations in the same country (i.e. the mean probability of each observation was taken instead of the mean probability of groups of obervations of each country). The new calculations used here correct this previous error. Results: Results of the new calculations are presented in Table 1. Discussion: The new corrected values of the marginal LOOIC in Table 3 do not change any of the practical conclusions in Gosselin and Callois (2018). However, they are more coherent with the levels of significance of the estimators (Tables 4 to 7 in Gosselin and Callois, 2018) than were the previous values obtained (Table 3 in Gosselin and Callois, 2018). Indeed, in the previous table, the levels of significance of the most significant effects of explanatory variables were overall less significant for the Proportion of Extinct and Threatened species (Tables 4 and 5 in Gosselin and Callois, 2018) than for the proportion of Sealed area or the dynamics of this proportion (Tables 6 and 7 in Gosselin and Callois, 2018). Yet, previous differences in LOOIC values of the null model had indicated stronger differences for Extinct and Threatened species with respect to the dynamics in the proportion of Sealed areas (Table 3 in Gosselin and Callois, 2018). The new model comparison results presented herein in Table 1 establish an ordering that is more in line with the significance of the effects. This behavior is in agreement with McQuarrie and Tsai's (1999) results; for orthogonal regressions, they showed a direct link between the statistical significance of the effects of models and their Akaike Information Criterion (AIC), a criterion close to LOOIC for such simple settings. Our settings are not very different from orthogonal regressions since we mostly chose models with not too much correlated explanatory variables. The fact that the new marginal LOOIC values are more coherent with the levels of significance of the estimators than were the previous values obtained is therefore welcome. In the case of the SC model for the proportion of Threatened species (Table 1), the results for marginal LOOIC shown in Table 1 are also more satisfactory than the conditional LOOIC results presented in Table SM1 (Supplementary material): given the two significant estimators involved in the model (Table 5 in Gosselin and Callois, 2018), marginal LOOIC indicates that the SC model is better than the Null model, qualitatively in agreement with McQuarrie and Tsai (1999)&rsquo;s results, while conditional LOOIC indicates that it is not as good as the Null model.<br/> &copy; 2018 Elsevier Ltd","Gosselin, Frederic and Callois, Jean-Marc",2019,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Veriabs: Verification by abstraction","VeriAbs verifies C programs by transforming them to abstract programs. The transformation replaces loops in the original code by abstract loops of small known bounds. Bounded model checkers can then be used to prove properties over such programs. To perform such a transformation, VeriAbs implements (i) a static value analysis to compute loop invariants, (ii) abstract acceleration and output abstraction for numerical loops, (iii) a novel array witness selection for loops that iterate over arrays, and (iv) an iterative refinement using an enhanced k-induction technique. To find errors, VeriAbs computes bounds of the original loops and then checks for errors within those bounds. VeriAbs can thus prove properties and find errors using bounded model checking. It uses the C Bounded Model Checker (CBMC) version 5.4 with MiniSat version 2.2.<br/> &copy; Springer-Verlag GmbH Germany 2017.","Chimdyalwar, Bharti and Darke, Priyanka and Chauhan, Avriti and Shah, Punit and Kumar, Shrawan and Venkatesh, R.",2017,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Dependency-based automatic parallelization of java applications","There are billions of lines of sequential code inside nowadays software which do not benefit from the parallelism available in modern multicore architectures. Transforming legacy sequential code into a parallel version of the same programs is a complex and cumbersome task. Trying to perform such transformation automatically and without the intervention of a developer has been a striking research objective for a long time. This work proposes an elegant way of achieving such a goal. By targeting a task-based runtime which manages execution using a task dependency graph, we developed a translator for sequential JAVA code which generates a highly parallel version of the same program. The translation process interprets the AST nodes for signatures such as read-write access, execution-flow modifications, among others and generates a set of dependencies between executable tasks. This process has been applied to well known problems, such as the recursive Fibonacci and FFT algorithms, resulting in versions capable of maximizing resource usage. For the case of two CPU bounded applications we were able to obtain 10.97x and 9.0x speedup on a 12 core machine.<br/> &copy; Springer International Publishing Switzerland 2014.","Rafael, Joao and Correia, Ivo and Fonseca, Alcides and Cabral, Bruno",2014,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"J3Model: A novel framework for improved Modified Condition/Decision Coverage analysis","In the real-time systems and safety critical domains, software quality assurance adheres to protocols such as DO-178C standard. Regarding these issues, concolic testing generates test cases that can attain high coverage using an augmented approach based on Modified Condition/Decision Coverage (MC/DC). In this paper, we propose a framework to compute MC/DC percentage for test case generation. To achieve an increase in MC/DC, we transform the input Java program, J, into its transformed version, J&prime;, using Java Program Code Transformer (JPCT). Then, we use JCUTE tool to generate test cases. At last, we use Java Coverage Analyzer (JCA) to compute MC/DC percentage. The Java program code transformer adds additional empty nested if-else conditional statements for each decision that causes variation in MC/DC percentage. In later step, these extra conditional statements get stripped-off. This approach resolves some of the bottleneck issues associated with traditional concolic testers. In our experimental study, we have experimented with forty Java programs. We have computed the difference of MC/DC%, for both the scenarios (i.e. with code transformation and without code transformation). Our approach (i.e. with code transformation achieves) 24.09% average increase in MC/DC% over the traditional approach (i.e. without code transformation).<br/> &copy; 2016 Elsevier B.V.","Godboley, Sangharatna and Dutta, Arpita and Mohapatra, Durga Prasad and Mall, Rajib",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"IWoR 2016 - Proceedings of the 1st International Workshop on Software Refactoring, co-located with ASE 2016","The proceedings contain 7 papers. The topics discussed include: refactoring for software architecture smells; empirical evaluation of code smells in open source projects: preliminary results; measuring refactoring benefits: a survey of the evidence; graph-based approach for detecting impure refactoring from version commits; refactoring verification using model transformation; automated translation among EPSILON languages for performance-driven UML Software model refactoring; and full application of the extract interface refactoring: conceptual structures in the hands of master students.<br/>",,2016,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Stochastic hyperfine interactions modeling libraryVersion 2","The stochastic hyperfine interactions modeling library (SHIML) provides a set of routines to assist in the development and application of stochastic models of hyperfine interactions. The library provides routines written in the C programming language that (1) read a text description of a model for fluctuating hyperfine fields, (2) set up the Blume matrix, upon which the evolution operator of the system depends, and (3)&nbsp;find the eigenvalues and eigenvectors of the Blume matrix so that theoretical spectra of experimental techniques that measure hyperfine interactions can be calculated. The optimized vector and matrix operations of the BLAS and LAPACK libraries are utilized. The original version of SHIML constructed and solved Blume matrices for methods that measure hyperfine interactions of nuclear probes in a single spin state. Version 2 provides additional support for methods that measure interactions on two different spin states such as M&ouml;ssbauer spectroscopy and nuclear resonant scattering of synchrotron radiation. Example codes are provided to illustrate the use of SHIML to (1) generate perturbed angular correlation spectra for the special case of polycrystalline samples when anisotropy terms of higher order than A<inf>22</inf>can be neglected and (2) generate M&ouml;ssbauer spectra for polycrystalline samples for pure dipole or pure quadrupole transitions. New version program summary Program title: SHIML Catalogue identifier: AEIF_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIF_v2_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU General Public License version 3 with supplemental citation provision No. of lines in distributed program, including test data, etc.: 88510 No. of bytes in distributed program, including test data, etc.: 3311047 Distribution format: tar.gz Programming language: C. Computer: Any. Operating system: LINUX, OS X. RAM: Variable Catalogue identifier of previous version: AEIF_v1_0 Journal reference of previous version: Comput. Phys. Comm. 182(2011)1061 Classification: 7.4. External routines: TAPP [1], BLAS [2], a C-interface to BLAS [3], and LAPACK [4]. Additionally, GSL [3] is needed to compile the example code that simulates M&ouml;ssbauer spectra. Does the new version supersede the previous version?: No Nature of problem: In condensed matter systems, hyperfine methods such as nuclear magnetic resonance (NMR), M&ouml;ssbauer effect (ME), muon spin rotation (&mu;SR), and perturbed angular correlation spectroscopy (PAC) measure electromagnetic fields due to electronic and magnetic structure within Angstroms of nuclear probes through the hyperfine interaction. When interactions fluctuate at rates comparable to the time scale of a hyperfine method, there is a loss in signal coherence, and spectra in the time domain are damped while spectra in the frequency domain are broadened. The degree of damping or broadening can be used to determine fluctuation rates, provided that theoretical expressions for spectra can be derived for relevant physical models of the fluctuations. SHIML provides routines to help researchers quickly develop code to incorporate stochastic models of fluctuating hyperfine interactions in calculations of hyperfine spectra. Solution method: Calculations are based on the method for modeling stochastic hyperfine interactions for PAC by Winkler and Gerdau [5]. The method is extended to include other hyperfine methods following the work of Dattagupta [6]. The code provides routines for reading model information from text files, allowing researchers to develop new models quickly without the need to modify computer code for each new model to be considered. Reasons for new version: The original version of the library provided support only for those methods that measure hyperfine interactions on one spin state of the nuclear probe. As such, it excluded important hyperfine methods that measure the interactions on two spin states such as M&ouml;ssbauer spectroscopy and nuclear resonant scattering of synchrotron radiation. The present version of SHIML provides the necessary support for such double spin-state methods while maintaining backward compatibility for code already developed using the original version. Summary of revisions: Routines now check that values representing nuclear spins are positive integers or positive half-integers. Additional utility functions are provided to make it easier for code developers to calculate Hamiltonians of electric quadrupole interactions. A correction was made to the portion of code responsible for calculating the Blume matrix of single spin-state methods; however, this change will not alter results obtained from single spin-state simulations using version 1 of the library. The remaining revisions support calculations for double spin-state methods. (1) Model-file syntax is expanded in order to allow users to specify different hyperfine interactions for ground and excited spin states and to input isomer shifts. (2) New routines for initialization and for Blume-matrix calculations are included for the double spin-state case. (3) New example code is provided to illustrate how SHIML can be used to simulate M&ouml;ssbauer spectra of polycrystalline samples for pure dipole or pure quadrupole transitions; background information about the M&ouml;ssbauer examples can be found in Ref. [7]. Finally, updated software documentation is included in a User's Guide as a PDF file in the code distribution. Running time: Variable References: [1] M. O. Zacate, The Adjustable Parameter Package, Technical Report 2, CINSAM Grant 2006-R7 (unpublished); available for download at http://tapp.nku.edu/.[2] L. S. Blackford et al., ACM Trans. Math. Soft. 28 (2002) 135; J. Dongarra, International Journal of High Performance Applications and Supercomputing 16 (2002) 1; http://www.netlib.org/blas/.[3] M. Galassi et al., GNU Scientific Library Reference Manual, third edition (2009); available for download at http://www.gnu.org/software/gsl/.[4] E. Anderson et al., LAPACK Users&rsquo; Guide, third ed. (Society for Industrial and Applied Mathematics, Philadelphia, PA, 1999); http://www.netlib.org/lapack/.[5] H. Winkler, E. Gerdau, Z. Phys. 262 (1973) 363.[6] S. Dattagupta, Hyperfine Interact. 11 (1981) 77.[7] M. O. Zacate, W. E. Evenson, Hyperfine Interact. 231 (2015) 143.<br/> &copy; 2015 Elsevier B.V.","Zacate, Matthew O. and Evenson, William E.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"32nd european conference on objectoriented programming (ECOOP 2018)","The proceedings contain 25 papers. The topics discussed include: fault-tolerant distributed reactive programming; theory and practice of coroutines with snapshots; a characteristic study of parameterized unit tests in .NET open source projects; learning to accelerate symbolic execution via code transformation; accelerating dynamically-typed languages on heterogeneous platforms using guards optimization; automating object transformations for dynamic software updating via online execution synthesis; LEGATO: an at-most-once analysis with applications to dynamic configuration updates; and efficient reflection string analysis via graph coloring.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Electronic Proceedings in Theoretical Computer Science, EPTCS","The proceedings contain 5 papers. The topics discussed include: a tutorial on using Dafny to construct verified software; comparing MapReduce and pipeline implementations for counting triangles; towards a semantics-aware code transformation toolchain for heterogeneous systems; towards automatic learning of heuristics for mechanical transformations of procedural code; and an introduction to liquid Haskell.",,2017,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Static compilation analysis for host-accelerator communication optimization","We present an automatic, static program transformation that schedules and generates efficient memory transfers between a computer host and its hardware accelerator, addressing a well-known performance bottleneck. Our automatic approach uses two simple heuristics: to perform transfers to the accelerator as early as possible and to delay transfers back from the accelerator as late as possible. We implemented this transformation as a middle-end compilation pass in the pips /Par4All compiler. In the generated code, redundant communications due to data reuse between kernel executions are avoided. Instructions that initiate transfers are scheduled effectively at compile-time. We present experimental results obtained with the Polybench 2.0, some Rodinia benchmarks, and with a real numerical simulation. We obtain an average speedup of 4 to 5 when compared to a nai&die;ve parallelization using a modern gpu with Par4All, hmpp, and pgi, and 3.5 when compared to an OpenMP version using a 12-core multiprocessor. &copy; 2013 Springer-Verlag.<br/>","Amini, Mehdi and Coelho, Fabien and Irigoin, Francois and Keryell, Ronan",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"An automatic method for assessing the versions affected by a vulnerability","Vulnerability data sources are used by academics to build models, and by industry and government to assess compliance. Errors in such data sources therefore not only are threats to validity in scientific studies, but also might cause organizations, which rely on retro versions of software, to lose compliance. In this work, we propose an automated method to determine the code evidence for the presence of vulnerabilities in retro software versions. The method scans the code base of each retro version of software for the code evidence to determine whether a retro version is vulnerable or not. It identifies the lines of code that were changed to fix vulnerabilities. If an earlier version contains these deleted lines, it is highly likely that this version is vulnerable. To show the scalability of the method we performed a large scale experiments on Chrome and Firefox (spanning 7,236 vulnerable files and approximately 9,800 vulnerabilities) on the National Vulnerability Database (NVD). The elimination of spurious vulnerability claims (e.g. entries to a vulnerability database such as NVD) found by our method may change the conclusions of studies on the prevalence of foundational vulnerabilities.<br/> &copy; 2015, Springer Science+Business Media New York.","Nguyen, Viet Hung and Dashevskyi, Stanislav and Massacci, Fabio",2016,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"30th IFIP International Conference on Testing Software and Systems, ICTSS 2018","The proceedings contain 14 papers. The special focus in this conference is on Testing Software and Systems. The topics include: Neural networks as artificial specifications; combining model learning and data analysis to generate models of component-based systems; deriving tests with guaranteed fault coverage for finite state machines with timeouts; from ontologies to input models for combinatorial testing; validation of transformation from abstract state machine models to C++ code; security testing for chatbots; JMCTest: Automatically testing inter-method contracts in Java; testing ambient assisted living solutions with simulations; Generating OCL constraints from test case schemas for testing model behavior; Test derivation for SDN-enabled switches: A logic circuit based approach; an energy aware testing framework for smart-spaces; c++11/14 mutation operators based on common fault patterns.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Enriching model execution with feedback to support testing of semantic conformance between models and requirements: Design and evaluation of feedback automation architecture","Model Driven Development (MDD) has traditionally been used to support model transformations and code generation. While plenty of techniques and tools are available to support modeling and transformations, tool support for checking the model quality in terms of semantic conformance with respect to the domain requirements is largely absent. In this work we present a model verification and validation approach based on model-driven feedback generation in a model-to-code transformation. The transformation is achieved using a single click. The generated output of the transformation is a compiled code which is achieved by a single click. This also serves as a rapid prototyping instrument that allows simulating a model (the terms prototyping and simulation are thus used interchangeably in the paper). The proposed feedback incorporation method in the generated prototype allows linking event execution in the generated code to its causes in the model used as input for the generation. The goal of the feedback is twofold: (1) to assist a modeler in validating semantic conformance of a model with respect to a domain to be engineered; (2) to support the learning perspective of less experienced modelers (such as students or junior analysts in their early career) by allowing them to detect modeling errors that result from the misinterpreted use of modeling language constructs. Within this work we focus on conceptual and platform independent models (PIM) that make use of two prominent UML diagrams - a class diagram (for modeling the structure of a system) and multiple interacting statecharts (for modeling a system's dynamic behavior). The tool has been used in the context of teaching a requirements analysis and modeling course at KU Leuven. The proposed feedback generation technique has been constantly validated by means of ""usability"" evaluations, and demonstrates a high level of self-reported utility of the feedback. Additionally, the findings of our experimental studies also show a significant positive impact of feedback-enabled rapid prototyping method on semantic validation capabilities of novices. Despite our focus on specific diagramming techniques, the principles of the approach presented in this work can be used to support educational feedback automation for a broader spectrum of diagram types in the context of MDD and simulation.<br/> Copyright &copy; 2016 by SCITEPRESS - Science and Technology Publications.","Sedrakyan, Gayane and Snoeck, Monique",2016,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Influence apart from Adoption: How Interaction between Programming and Scientific Practices Shapes Modes of Inquiry in Four Oceanography Teams","Scientists have been producing and sharing code for decades. Code work done by scientists spans simulation, data processing, analysis, visualization, communication, and data stewardship. Robust instrumentation generates data beyond the scale and comprehension of individuals in their current tools, requiring new approaches to automation and collaboration. Sophisticated web frameworks enable more interactive web portals for displaying data or simulation results to various stakeholders. Educational initiatives that target scientists learning to program are increasingly available, and increasingly reach enrollment limits. Given this context, how is programming in science changing? I argue that changes enacted in scientific programming practices are intended to lead to code that is not only exists and runs for a long time, but continues to be understandable, usable, and extensible. My dissertation examines the interaction between coding practices and scientific inquiry. Although deliberate change is oriented toward this goal, a particular tool or protocol does not require sustained use by a group to have impact on the work practices of that group. In this dissertation, I develop an alternative conceptual framework for reflecting on the goals and outcomes of change. My findings are based on over 300 hours of observation of a total of 46 scientists from four different oceanography groups. Of the 46 scientists, 21 comprise the core study participants, doing code work at graduate, post-graduate, and faculty levels. Of the four oceanography groups I studied, two focus on simulation and two on observational data analysis. All engaged in deliberate, reflective change of their programming skills and practices. In collecting and analysing qualitative data, I focused on ""code work"" in a broader sense, rather than referring to ""scripting,"" ""high-performance computing,"" ""scientific software engineering,"" ""data science,"" or other more specific terms that imply particular working environments and aesthetics. Maintaining an inclusive scope allowed me to not only draw parallels between these practices, but also to consider ways in which they intersect and influence one another. Particular practices or philosophies can be pervasive through all layers of code work, from maintaining a co-authored LaTeX-typeset manuscript in GitHub, to ""adding biology"" to a well-established model, to implementing an automated test suite for an analytic pipeline that generates daily results and images for a web endpoint. I propose a conceptual framework of change and use stories from my qualitative study to illustrate its components and dynamics. This framework defines relationships between (1) the working environment, which is subject to deliberate change; (2) the perfect world, which directs that change; and the (3) moment of flux, which constitutes taking action to bring about a change and its immediate outcome. The working environment combines resources that are technical (e.g., iPython Notebook, Google search), cognitive (e.g., looking at many small charts encoding information in a familiar and consistent way to aid quick understanding), and social (e.g., a shared office with frequent ""hey, how do you [do a particular tricky thing]?""). The working environment is subject to change, including changes in not only the technical components (e.g., tools) but also cognitive (e.g., skills) and social (e.g., communication practices and language). This change is informed and directed by a collective imagination of a perfect world: the moving target to which possible modifications to the current way of working can be compared. The moment of flux when a scientist elects to pursue deliberate change requires both momentum and opportunity, which can arise in the wake of a breakdown of the prior approach, in the space created by embarking on a new project, or through an energizing workshop or group event. These situations allow the exploration of options that are already in the awareness or intention. Actually making the ``leap'' of deliberate change to integrate an unfamiliar component or learn a new skill is associated with uncertainty and the possibility of disappointment or failure. The conceptual framework I propose creates optimistic vocabulary for reflecting upon these changes. As projects involve more people, longer time spans, and more ambitious collaboration between disciplines, understanding how coding practices influence scientific inquiry is increasingly important. The discussion of ""best practices"" in open science encourages the sharing of negative results and disappointing data as a top priority. This call for reflection on failure must be extended to include code work. With data as well as with code sharing, repeated ""best practices"" are not sufficient to inspire change, even for those scientists who openly feel they ""should"" do it. I present qualitative findings that demonstrate concrete ways to deliver interpersonal rewards in the wake of particular efforts not panning out as well as hoped or intended. ProQuest Subject Headings: Computer science, Physical oceanography.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.","Kuksenok, Kateryna",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Photoactivation of Mutant Isocitrate Dehydrogenase 2 Reveals Rapid Cancer-Associated Metabolic and Epigenetic Changes","Isocitrate dehydrogenase is mutated at a key active site arginine residue (Arg172 in IDH2) in many cancers, leading to the synthesis of the oncometabolite (R)-2-hydroxyglutarate (2HG). To investigate the early events following acquisition of this mutation in mammalian cells we created a photoactivatable version of IDH2(R172K), in which K172 is replaced with a photocaged lysine (PCK), via genetic code expansion. Illumination of cells expressing this mutant protein led to a rapid increase in the levels of 2HG, with 2HG levels reaching those measured in patient tumor samples, within 8 h. 2HG accumulation is closely followed by a global decrease in 5-hydroxymethylcytosine (5-hmC) in DNA, demonstrating that perturbations in epigenetic DNA base modifications are an early consequence of mutant IDH2 in cells. Our results provide a paradigm for rapidly and synchronously uncloaking diverse oncogenic mutations in live cells to reveal the sequence of events through which they may ultimately cause transformation.<br/> &copy; 2016 American Chemical Society.","Walker, Olivia S. and Elsasser, Simon J. and Mahesh, Mohan and Bachman, Martin and Balasubramanian, Shankar and Chin, Jason W.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"CEUR Workshop Proceedings","The proceedings contain 18 papers. The topics discussed include: machine translation within one language as a paraphrasing technique; employing evolutionary algorithms for classification of astrophysical spectra; transformation of pipeline stage algorithms to event-driven code; acting and Bayesian reinforcement structure learning of partially observable environment; smart and easy object tracking; control of depth-sensing camera via plane of interaction; extracting product data from e-shops; an improved algorithm for ancestral gene order reconstruction; deviations prediction in timetables based on AVL data; neuropeptide recognition by machine learning methods; fairytale child chatbot; towards automation of ontology analysis reporting; and contemplating efficiency of the root file format for data intensive simulations in particle physics.",,2014,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Natural supervised hashing","Among learning-based hashing methods, supervised hashing tries to find hash codes which preserve semantic similarities of original data. Recent years have witnessed much efforts devoted to design objective functions and optimization methods for supervised hashing learning, in order to improve search accuracy and reduce training cost. In this paper, we propose a very straightforward supervised hashing algorithm and demonstrate its superiority over several state-of-the-art methods. The key idea of our approach is to treat label vectors as binary codes and to learn target codes which have similar structure to label vectors. To circumvent direct optimization on large n &times; n Gram matrices, we identify an inner-product-preserving transformation and use it to bring close label vectors and hash codes without changing the structure. The optimization process is very efficient and scales well. In our experiment, training 16-bit and 96-bit code on NUS-WIDE cost respectively only 3 and 6 minutes.<br/>","Liu, Qi and Lu, Hongtao",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"International conference on Software Technologies: Applications and Foundations, STAF 2017","The proceedings contain 37 papers. The special focus in this conference is on Software Technologies: Applications and Foundations. The topics include: On the Need for Temporal Model Repositories; on the Need for Artifact Models in Model-Driven Systems Engineering Projects; cognifying Model-Driven Software Engineering; non-human Modelers: Challenges and Roadmap for Reusable Self-explanation; Some Narrow and Broad Challenges in MDD; modelling by the People, for the People; from Building Systems Right to Building Right Systems: A Generic Architecture and Its Model Based Realization; the Tool Generation Challenge for Executable Domain-Specific Modeling Languages; toward Product Lines of Mathematical Models for Software Model Management; Introduction of an OpenCL-Based Model Transformation Engine; model-Driven Interaction Design for Social Robots; towards Integration of Context-Based and Scenario-Based Development; (An Example for) Formally Modeling Robot Behavior with UML and OCL; Synthesizing Executable PLC Code for Robots from Scenario-Based GR(1) Specifications; evaluating a Graph Query Language for Human-Robot Interaction Data in Smart Environments; a Simulation Framework to Analyze Knowledge Exchange Strategies in Distributed Self-adaptive Systems; Workshop in OCL and Textual Modelling: Report on Recent Trends and Panel Discussions; improving Incremental and Bidirectional Evaluation with an Explicit Propagation Graph; Translating UML-RSDS OCL to ANSI C; Mapping USE Specifications into Spec#; collaborative Modelling with Version Control; Deterministic Lazy Mutable OCL Collections; Step 0: An Idea for Automatic OCL Benchmark Generation; SICOMORo-CM: Development of Trustworthy Systems via Models and Advanced Tools; Developer-Centric Knowledge Mining from Large Open-Source Software Repositories (CROSSMINER); technical Obsolescence Management Strategies for Safety-Related Software for Airborne Systems; loose Graph Simulations.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"ICSOFT-PT 2015 - 10th International Conference on Software Paradigm Trends, Proceedings; Part of 10th International Joint Conference on Software Technologies, ICSOFT 2015","The proceedings contain 17 papers. The topics discussed include: SuperMod - a model-driven tool that combines version control and software product line engineering; semantic version management based on formal certification; Java-meets Eclipse - an IDE for teaching Java following the object-later approach; systematic identification of information flows from requirements to support privacy impact assessments; model checking to improve precision of design pattern instances identification in OO systems; transformation from R-UML to R-TNCES: new formal solution for verification of flexible control systems; a tool for management of knowledge dispersed throughout multiple references; a pi-calculus-based approach for the verification of UML2 sequence diagrams; on a-posteriori integration of Ecore models and hand-written Java code; OCL for rich domain models implementation - an incremental aspect based solution; and novel approach for computing skyline services with fuzzy consistent model for QoS- based service composition.",,2015,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"The flame graph","AN EVERYDAY PROBLEM in our industry is understanding how software is consuming resources, particularly CPUs. What exactly is consuming how much, and how did this change since the last software version? These questions can be answered using software profilers - tools that help direct developers to optimize their code and operators to tune their environment. The output of profilers can be verbose, however, making it laborious to study and comprehend. The flame graph provides a new visualization for profiler output and can make for much faster comprehension, reducing the time for root cause analysis. Copyright held by author.<br/>","Gregg, Brendan",2016,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Implicit coupling approach for simulation of charring carbon ablators","This study demonstrates that coupling of a material thermal response code and a flow solver with nonequilibrium gas/surface interaction for simulation of charring carbon ablators can be performed using an implicit approach. The material thermal response code used in this study is the three-dimensional version of Fully Implicit Ablation and Thermal response program, which predicts charring material thermal response and shape change on hypersonic space vehicles. The flow code solves the reacting Navier-Stokes equations using Data Parallel Line Relaxation method. Coupling between the material response and flow codes is performed by solving the surface mass balance in flow solver and the surface energy balance in material response code. Thus, the material surface recession is predicted in flow code, and the surface temperature and pyrolysis gas injection rate are computed in material response code. It is demonstrated that the time-lagged explicit approach is sufficient for simulations at low surface heating conditions, in which the surface ablation rate is not a strong function of the surface temperature. At elevated surface heating conditions, the implicit approach has to be taken, because the carbon ablation rate becomes a stiff function of the surface temperature, and thus the explicit approach appears to be inappropriate resulting in severe numerical oscillations of predicted surface temperature. Implicit coupling for simulation of arc-jet models is performed, and the predictions are compared with measured data. Implicit coupling for trajectory based simulation of Stardust fore-body heat shield is also conducted. The predicted stagnation point total recession is compared with that predicted using the chemical equilibrium surface assumption.<br/>","Chen, Yih-Kanq and Gokcen, Tahir",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"18th Asia Simulation Conference, AsiaSim 2018","The proceedings contain 45 papers. The special focus in this conference is on Asia Simulation. The topics include: Deep Dissimilarity Measure for Trajectory Analysis; Performance Comparison of Eulerian Kinetic Vlasov Code Between Xeon Phi KNL and Xeon Broadwell; heterogeneous Scalable Multi-languages Optimization via Simulation; smart Simulation Cloud (Simulation Cloud 2.0)&mdash;The Newly Development of Simulation Cloud; a Semantic Composition Framework for Simulation Model Service; dynamic Optimization of Two-Coil Power-Transfer System Using L-Section Matching Network for Magnetically Coupled Intrabody Communication; demand and Supply Model for the Natural Gas Supply Chain in Colombia; Deep-Learning-Based Storage-Allocation Approach to Improve the AMHS Throughput Capacity in a Semiconductor Fabrication Facility; research on the Cooperative Behavior in Cloud Manufacturing; simulation Credibility Evaluation Based on Multi-source Data Fusion; Particle in Cell Simulation to Study the Charging and Evolution of Wake Structure of LEO Spacecraft; Wise-Use of Sediment for River Restoration: Numerical Approach via HJBQVI; calculation of Extreme Precipitation Threshold by Percentile Method Based on Box-Cox Transformation; OpenPTDS Dataset: Pedestrian Trajectories in Crowded Scenarios; description and Analysis of Cognitive Processes in Ground Control Using a Mutual Belief-Based Team Cognitive Model; a Credibility Assessment Method for Training Simulations from the View of Training Effectiveness; digital Twin-Based Energy Modeling of Industrial Robots; Dyna-Q Algorithm for Path Planning of Quadrotor UAVs; Boarding Stations Inferring Based on Bus GPS and IC Data; acoustic Properties of Resonators Using Deployable Cylinders; a Method of Parameter Calibration with Hybrid Uncertainty; iterative Unbiased Conversion Measurement Kalman Filter with Interactive Multi-model Algorithm for Target Tracking.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Modeling Techniques of Storage Modules with PCM Microcapsules: Case Study","A preliminary study on the increased thermal capacity in thermal energy storage concrete solids with inclusions of phase change materials is presented here. Particularly, the change in the thermal behavior of such composites, by varying the inclusions percentage, is evaluated. An ad hoc hygrothermal finite-element code has been developed, able to evaluate the nonlinear concrete behavior during the phase changes of phase change materials (PCM), subjected to a load history of the test plant designed by ENEA (Italian National Agency for New Technologies, Energy and Sustainable Economic Development). Transient thermal analyses have been conducted, assuming a homogeneous distribution of phase change materials within the cementitious matrix. Hygro-thermo-mechanical models have been developed to evaluate the heat storage capacity of the composite material, and its change in mechanical strength has been analytically and numerically investigated, both at environmental temperature and during heating. The adopted code takes into account a homogenized composite, having a uniform distribution of PCM within the cementitious matrix and characterized by homogenized hygrothermal properties.<br/> &copy; 2017 American Society of Civil Engineers.","Mazzucco, G. and Xotta, G. and Salomoni, V.A. and Majorana, C.E. and Giannuzzi, G.M. and Miliozzi, A.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Gradient-enhanced model and its micromorphic regularization for simulation of Luders-like bands in shape memory alloys","Shape memory alloys, notably NiTi, often exhibit softening pseudoelastic response that results in formation and propagation of L&uuml;ders-like bands upon loading, for instance, in uniaxial tension. A common approach to modelling softening and strain localization is to resort to gradient-enhanced formulations that are capable of restoring well-posedness of the boundary-value problem. This approach is also followed in the present paper by introducing a gradient-enhancement into a simple one-dimensional model of pseudoelasticity. In order to facilitate computational treatment, a micromorphic-type regularization of the gradient-enhanced model is subsequently performed. The formulation employs the incremental energy minimization framework that is combined with the augmented Lagrangian treatment of the resulting non-smooth minimization problem. A thermomechanically coupled model is also formulated and implemented in a finite-element code. The effect of the loading rate on the localization pattern in a NiTi wire under tension is studied, and the features predicted by the model show a good agreement with the experimental observations. Additionally, an analytical solution is provided for a propagating interface (macroscopic transformation front) both for the gradient-enhanced model and for its micromorphic version.<br/> &copy; 2017","Rezaee Hajidehi, Mohsen and Stupkiewicz, Stanislaw",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Variability abstractions: Trading precision for speed in family-based analyses","Family-based (lifted) data-flow analysis for Software Product Lines (SPLs) is capable of analyzing all valid products (variants) without generating any of them explicitly. It takes as input only the common code base, which encodes all variants of a SPL, and produces analysis results corresponding to all variants. However, the computational cost of the lifted analysis still depends inherently on the number of variants (which is exponential in the number of features, in the worst case). For a large number of features, the lifted analysis may be too costly or even infeasible. In this paper, we introduce variability abstractions defined as Galois connections and use abstract interpretation as a formal method for the calculational-based derivation of approximate (abstracted) lifted analyses of SPL programs, which are sound by construction. Moreover, given an abstraction we define a syntactic transformation that translates any SPL program into an abstracted version of it, such that the analysis of the abstracted SPL coincides with the corresponding abstracted analysis of the original SPL. We implement the transformation in a tool, that works on Object-Oriented Java program families, and evaluate the practicality of this approach on three Java SPL benchmarks.<br/> &copy; Aleksandar S. Dimovski, Claus Brabrand, and Andrzej Wasowski;.","Dimovski, Aleksandar S. and Brabrand, Claus and Wasowski, Andrzej",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Proceedings of 2017 11th International Conference on Intelligent Systems and Control, ISCO 2017","The proceedings contain 94 papers. The topics discussed include: two degree of freedom controller optimization using GA for shell and tube heat exchanger; code review analysis of software system using machine learning techniques; a low cost wireless sensor system for monitoring the air handling unit of the university building; revamped market-basket analysis using in-memory computation framework; an improved face recognition method using local binary pattern method; fast-converging MPPT technique for photovoltaic system using dsPIC controller; an approach for classification using simple CART algorithm in Weka; a full band adaptive harmonic model based speaker identity transformation using radial basis function; a new chaotic attractor from Rucklidge system and its application in secured communication using OFDM; EPPN: extended prime product number based wormhole detection scheme for MANETs; load balancing and position based adaptive clustering scheme for effective data communication in WBAN healthcare monitoring systems; PAPR reduction in OFDM systems using higher order prediction filter; and impact of length and thickness of active region on radiated output power of InP/InGaAsP laser.",,2017,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"On the current AISC approach to stability analysis and design of steel structures","In 2005, the American Institute of Steel Construction (AISC) stability analysis and design requirements changed significantly compared to procedures required prior to 2005. The most significant change related to stability analysis and design is the requirement to include geometric imperfections in the calculation of the required strength. Direct Analysis Method (DAM) is currently the recommended code method, while a modified version of the traditional Effectiveness Length Method (ELM) is now referred to as an alternative method of design. The critical changes appeared in the 13<sup>th</sup>edition (AISC, 2005) and continued to the present 14<sup>th</sup>edition (AISC, 2011). The objectives of this paper are to: 1) provide an overview of the rationale behind the code change that took place in 2005 and remained in the current specifications and 2) present the current features of ELM and DAM methods. A case study is presented to assess the differences in structural response when DAM and ELM methods are used. It was shown that DAM predicts higher demand on beams and columns of the structural system at the lower levels, but the difference in demand between DAM and ELM methods decreases at the upper levels of the structural system. This is due to the variation of effective length factor required in ELM for compression members, from top levels to bottom levels of the system. DAM, however, permits the use of an effective length factor of 1.0.<br/> &copy; 2017 JUST. All Rights Reserved.","Mohamed, Osama",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"NCAR Release of CAM-SE in CESM2.0: A Reformulation of the Spectral Element Dynamical Core in Dry-Mass Vertical Coordinates With Comprehensive Treatment of Condensates and Energy","It is the purpose of this paper to provide a comprehensive documentation of the new NCAR (National Center for Atmospheric Research) version of the spectral element (SE) dynamical core as part of the Community Earth System Model (CESM2.0) release. This version differs from previous releases of the SE dynamical core in several ways. Most notably the hybrid sigma vertical coordinate is based on dry air mass, the condensates are dynamically active in the thermodynamic and momentum equations (also referred to as condensate loading), and the continuous equations of motion conserve a more comprehensive total energy that includes condensates. Not related to the vertical coordinate change, the hyperviscosity operators and the vertical remapping algorithms have been modified. The code base has been significantly reduced, sped up, and cleaned up as part of integrating SE as a dynamical core in the CAM (Community Atmosphere Model) repository rather than importing the SE dynamical core from High-Order Methods Modeling environment as an external code.<br/> &copy;2018. The Authors.","Lauritzen, P.H. and Nair, R.D. and Herrington, A.R. and Callaghan, P. and Goldhaber, S. and Dennis, J.M. and Bacmeister, J.T. and Eaton, B.E. and Zarzycki, C.M. and Taylor, Mark A. and Ullrich, P.A. and Dubos, T. and Gettelman, A. and Neale, R.B. and Dobbins, B. and Reed, K.A. and Hannay, C. and Medeiros, B. and Benedict, J.J. and Tribbia, J.J.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"1st International Symposium on Mobile Internet Security, MobiSec 2016","The proceedings contain 15 papers. The special focus in this conference is on Mobile Internet Security. The topics include: The cryptanalysis of WPA &amp; WPA2 using the parallel-computing with GPUs; a policy management system based on multi-dimensional attribution label; access control for cross-border transfer of sensor data; security analysis oriented physical components modeling in quantum key distribution; a novel hybrid architecture for high speed regular expression matching; mobile security assurance for automotive software through archimate; a generalized data inconsistency detection model for wireless sensor networks; building a frame-based cyber security learning game; Personal identification using time and frequency domain features of ECG lead-I; a secure color-code key exchange protocol for mobile chat application; Design and implementation of SPEEX speech technology on ARM processor; a discrete wavelet transformation based fast and secure transmission of images for group communication; an automated graph based approach to risk assessment for computer networks with mobile components.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Legato: An at-most-once analysis with applications to dynamic configuration updates","Modern software increasingly relies on external resources whose location or content can change during program execution. Examples of such resources include remote network hosts, database entries, dynamically updated configuration options, etc. Long running, adaptable programs must handle these changes gracefully and correctly. Dealing with all possible resource update scenarios is difficult to get right, especially if, as is common, external resources can be modified without prior warning by code and/or users outside of the application&rsquo;s direct control. If a resource unexpectedly changes during a computation, an application may observe multiple, inconsistent states of the resource, leading to incorrect program behavior. This paper presents a sound and precise static analysis, Legato, that verifies programs correctly handle changes in external resources. Our analysis ensures that every value computed by an application reflects a single, consistent version of every external resource&rsquo;s state. Although consistent computation in the presence of concurrent resource updates is fundamentally a concurrency issue, our analysis relies on the novel at-most-once condition to avoid explicitly reasoning about concurrency. The at-most-once condition requires that all values depend on at most one access of each resource. Our analysis is flow-, field-, and context-sensitive. It scales to real-world Java programs while producing a moderate number of false positives. We applied Legato to 10 applications with dynamically updated configurations, and found several non-trivial consistency bugs.<br/> &copy; John Toman and Dan Grossman.","Toman, John and Grossman, Dan",2018,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"A thermal parametric study of non-evaporative spray cooling process","Ordinary water spray cooling is connected with very high temperatures where heat transfer during evaporation plays a key role. However, during cooling without phase change, the behaviour of the spray cooling parameters is rarely considered. The purpose of this paper is to study the influence of spray hydrodynamic parameters on heat transfer without liquid phase change during the cooling of an aluminium 3003-H18 plate at a temperature of 92 &deg;C. First of all, the flow rate was varied from 0.497 up to 1 l/min. Then, the inlet pressure varied from 0.7 to 2.1 bars. The influence of nozzle-to-target distance is also tested since the simulations were carried out in a wide height range, from 100 mm to 505 mm. The present simulation was achieved using the version 5.2 of COMSOL Multiphysics code.<br/> &copy; The Authors, published by EDP Sciences, 2018.","Otmani, Abdessalam and Mzad, Hocine and Bey, Kamel",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Monk10: Burnup credit capability","MONK&reg; is a Monte Carlo code for nuclear criticality and reactor physics analyses. It has a proven track record of application to the whole of the nuclear fuel cycle and is well established in the UK criticality community. Furthermore it is increasingly being used for reactor physics analysis (as described at ICNC 2011), which makes it an ideal tool for burn-up credit (BUC) calculations. Throughout the paper, example calculations based on a PWR are presented to illustrate the capabilities of the MONK10 code. In order to account for the spatial dependence of material burn-up it has in the past been necessary to design models with multiple regions and materials specifically to allow material burn-up to vary spatially. This is very labour intensive and difficult to change at a later stage. A new code version, MONK10, was released last year which includes the facility to allow a burn-up (BU) mesh to be superimposed on an existing model in order to account for the spatial dependence of the burn-up. This facility is used to consider the effect of radial position of a fuel element in a PWR core on BUC. Additionally, a thermal hydraulics (TH) mesh can be used to specify region dependent temperature. This, coupled with the fact that MONK10 also incorporates an on-the-fly Doppler broadening methodology facilitates the modelling of spatially dependent temperatures for the different components. A TH mesh is used to superimpose a temperature profile on a PWR based model and the effect of this on BUC is considered. The burn-up modelling in MONK has been benchmarked against the ANSWERS WIMS deterministic reactor physics code. Once the burn-up calculation has been completed and the depleted fuel compositions determined the spent fuel compositions can be transferred into a model of a storage facility or transport flask in order to perform burn-up credit analysis. The initial model is usually described as the donor model and the latter model as the receiver model. This transfer is carried out using the COWL option which allows the specification of a material in the receiver model based on the material's composition in a given BU mesh cell from the donor model. This allows compositions and densities to be transferred and also allows user specified adjustments to be made. For example, this could include omitting the fission products in order to estimate their contribution to burn-up credit and provide an actinide-only analysis. The effect of excluding appropriate nuclides is presented. An example of how the ANSWERS SPRUCE code can be used to quantify uncertainty in a BUC calculation is also presented.<br/>","Shepherd, Max and Davies, Nigel and Richards, Simon and Smith, Paul N. and Philpott, Will and Baker, Chris and Hiles, Richard and Hanlon, Dave",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Evaluation strategies for monadic computations","Monads have become a powerful tool for structuring effectful computations in functional programming, because they make the order of effects explicit. When translating pure code to a monadic version, we need to specify evaluation order explicitly. Two standard translations give call-by-value and call-by-name semantics. The resulting programs have different structure and types, which makes revisiting the choice difficult. In this paper, we translate pure code to monadic using an additional operation malias that abstracts out the evaluation strategy. The malias operation is based on computational comonads; we use a categorical framework to specify the laws that are required to hold about the operation. For any monad, we show implementations of malias that give call-by-value and call-by-name semantics. Although we do not give call-by-need semantics for all monads, we show how to turn certain monads into an extended monad with call-by-need semantics, which partly answers an open question. Moreover, using our unified translation, it is possible to change the evaluation strategy of functional code translated to the monadic form without changing its structure or types.<br/>","Petricek, Tomas",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Case-based instruction of 'how computer works' courses for high school students","The concept of binary, which is the basis of how computer works, is often taught in Taiwan's high school computer courses as the introductory unit of the class 'Introduction to Computers'. In the past, teachers used to lecture through binary concepts such as the decimal binary conversion, ASCII code transformation, and several traditional courses. However, most students did not know why they needed to learn binary concepts, and seemed less interested in learning. To enhance the effectiveness of learning, we incorporated some examples corresponding to each binary concept to link students' life experiences to the concepts in order to make learning meaningful. In this paper, we will share our teaching materials for the binary concept from the course 'Introduction to Computers'. The lesson starts from how computer stores values, text or image data to introduce basic concepts of binary data interpretation and hex conversion. Afterwards, a combination of simple logic gates adder instance is introduced to guide students to learn how to perform a binary compute operation. Finally, software CPU simulator is covered to allow students to learn and visualize how computer uses binary data and basic operations to perform some specific compute functions.<br/> &copy; 2015 IEEE.","Wang, Ting-Chung",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"1st International Conference on Advanced Intelligent System and Informatics, AISI 2015","The proceedings contain 46 papers. The special focus in this conference is on Intelligent Systems and Informatics. The topics include: Automatic rules generation approach for data cleaning in medical applications; action classification using weighted directional wavelet LBP histograms; markerless tracking for augmented reality using different classifiers; an experimental comparison between seven classification algorithms for activity recognition; machine learning based classification approach for predicting students performance in blended learning; detection of dead stained microscopic cells based on color intensity and contrast; building numbers with rods; telepresence robot using microsoft kinect sensor and video glasses; video foreground object extraction using an intermittent flash; enhanced region growing segmentation for CT liver images; a multi-objective genetic algorithm for community detection in multidimensional social network; creativity in the era of social networking; OCR system for poor quality images using chain-code representation; human thermal face extraction based on superpixel technique; unsupervised brain MRI tumor segmentation with deformation-based feature; face sketch synthesis and recognition based on linear regression transformation and multi-classifier technique; a new learning strategy for complex-valued neural networks using particle swarm optimization; abdominal CT liver parenchyma segmentation based on particle swarm optimization; grey wolf optimizer and case-based reasoning model for water quality assessment; new rough set attribute reduction algorithm based on grey wolf optimization; solving manufacturing cell design problems using a shuffled frog leaping algorithm and a fish detection approach based on BAT algorithm.",,2016,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"14th International Work-Conference on Artificial Neural Networks, IWANN 2017","The proceedings contain 127 papers. The special focus in this conference is on Artificial Neural Networks. The topics include: A parallel swarm library based on functional programming; a parallel island approach to multiobjective feature selection for brain computer interfaces; deep belief networks and multiobjective feature selection for BCI with multiresolution analysis; an intelligent multi-objective genetic algorithm using self organizing map; solving scheduling problems with genetic algorithms using a priority encoding scheme; tuning of clustering search based metaheuristic by cross-validated racing approach; a transformation approach towards big data multilabel decision trees; analysis of electroreception with temporal code-driven stimulation; a novel technique to estimate biological parameters in an epidemiology problem; deep learning using EEG data in time and frequency domains for sleep stage classification; application of an eye tracker over facility layout problem to minimize user fatigue; active sensing in human activity recognition; searching the sky for neural networks; non-linear least mean squares prediction based on non-Gaussian mixtures; synchronized multi-chain mixture of independent component analyzers; pooling spike neural network for acceleration of global illumination rendering; automatic tool for optic disc and cup detection on retinal fundus images; uncertainty analysis of ANN based spectral analysis using monte Carlo method; using deep learning for image similarity in product matching; enhanced similarity measure for sparse subspace clustering method; neural network-based simultaneous estimation of actuator and sensor faults.",,2017,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"A correlated-polaron electronic propagator: Open electronic dynamics beyond the Born-Oppenheimer approximation","In this work, we develop an approach to treat correlated many-electron dynamics, dressed by the presence of a finite-temperature harmonic bath. Our theory combines a small polaron transformation with the second-order time-convolutionless master equation and includes both electronic and system-bath correlations on equal footing. Our theory is based on the ab initio Hamiltonian, and is thus well-defined apart from any phenomenological choice of basis states or electronic system-bath coupling model. The equation-of-motion for the density matrix we derive includes non-Markovian and non-perturbative bath effects and can be used to simulate environmentally broadened electronic spectra and dissipative dynamics, which are subjects of recent interest. The theory also goes beyond the adiabatic Born-Oppenheimer approximation, but with computational cost scaling such as the Born-Oppenheimer approach. Example propagations with a developmental code are performed, demonstrating the treatment of electron-correlation in absorption spectra, vibronic structure, and decay in an open system. An untransformed version of the theory is also presented to treat more general baths and larger systems. &copy; 2012 American Institute of Physics.<br/>","Parkhill, John A. and Markovich, Thomas and Tempel, David G. and Aspuru-Guzik, Alan",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Metallo-Thermo-Mechanical Coupling in Quenching","This chapter summarizes the computer simulation on the macroscopic feature of the quenching process carried out by the present author. The history of this area up to the year 2000 is introduced in the first part since there have been so many works in the twenty-first century. A governing theory termed metallo-thermo-mechanics is presented. This theory is relevant to describing coupled temperature, metallic structure, and mechanical fields, which are predominant parameters in the course of some processes involving phase transformation and related theories of the kinetics of phase transformation and inelastic constitutive relations. Based on the theory, the computer simulation code COSMAP recently developed by the author is summarized in addition to the material database MATEQ, which must be utilized for the simulation. The next four sections treat the results of simulation on practical quenching processes such as Jominy end quenching, carburized quenching, induction hardening, and laser hardening, as well as quenching of Japanese swords. The final section briefly introduces the results of a benchmark project conducted by our group in Japan, where some results obtained by quenching simulation solvers are presented and discussed. &copy; 2014 Elsevier Ltd All rights reserved.<br/>","Inoue, T.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Implementation of two-group interfacial are a transport in a one-dimensional computational environment","A simplified one-dimensional computer code has been written which uses the two-fluid model to predict two-phase flows in large diameter vertical channels. Simplifying assumptions have been used to eliminate the energy equation and simplify the momentum equations. This code, written using MATLAB, uses a two-group momentum approach with two-group interfacial area transport to dynamically predict the development of the flow. This provides the ability to evaluate the interfacial area transport equation within a similar computational environment to that used in best estimate thermal-hydraulics analysis codes used in the nuclear industry and allows the evaluation of the performance of the transport equation, constitutive relations, bubble coalescence and breakup models, and other models using the same framework in which they will be used. The current version is limited to systems without phase change, i.e. where boiling and condensation are not significant. While the force-balance approach to interfacial drag has been used in the analyses presented, the code also includes a two-group drift-flux correlation and the implementation of the drag-coefficient approach recommended by Ishii and Hibiki [5] as options within the code, allowing the comparison of various interfacial drag approaches. The code has been used to simulate the experiments performed by Shen et al [18] in pipes with diameter of 0.2 m and length of 26.0 m. The code was able to duplicate the characteristic axial void fraction profile caused by the transition between the bubbly and cap-bubbly flow regimes in the experimental facility, showing that the two-group approach can be applied within the framework of existing best estimate codes. Further improvements to the accuracy of the code are expected if the bubble coalescence and breakup models are benchmarked based on a wider database and using the new approach to compute the flow development.<br/>","Schlegel, J.P. and Hibiki, T. and Ishii, M. and Shen, X. and Appathurai, S.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"17th European Conference on Genetic Programming, EuroGP 2014","The proceedings contain 22 papers. The special focus in this conference is on Genetic programming. The topics include: Higher order functions for kernel regression; a GP-GPU ensemble learning system for handling large datasets; learning dynamical systems using standard symbolic regression; semantic crossover based on the partial derivative error; a multi-dimensional genetic programming approach for multi-class classification problems; generalisation enhancement via input space transformation; on diversity, teaming, and hierarchical policies; genetically improved CUDA C++ software; measuring mutation operators&rsquo; exploration-exploitation behaviour and long-term biases; exploring the search space of hardware / software embedded systems by means of GP; enhancing branch-and-bound algorithms for order acceptance and scheduling with genetic programming; using genetic improvement and code transplants to specialise a C++ program to a problem class; ESAGP - a semantic GP framework based on alignment in the error space; building a stage 1 computer aided detector for breast cancer using genetic programming; NEAT, there&rsquo;s no bloat; the best things don&rsquo;t always come in small packages; asynchronous evolution by reference-based evaluation; behavioral search drivers for genetic programing; cartesian genetic programming: why no bloat? and on evolution of multi-category pattern classifiers suitable for embedded systems.",,2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"SLE 2016 - Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering, co-located with SPLASH 2016","The proceedings contain 23 papers. The topics discussed include: parsing and reflective printing, bidirectionally; taming context-sensitive languages with principled stateful parsing; efficient development of consistent projectional editors using grammar cells; experiences of Models@run-time with EMF and CDO; runtime support for rule-based access-control evaluation through model-transformation; object-oriented design pattern for DSL program monitoring; execution framework of the GEMOC studio; language design and implementation for the domain of coding conventions; BSML-mbeddr: integrating semantically configurable state-machine models in a C programming environment; adding uncertainty and units to quantity types in software models; FRaMED: full-fledge role modeling editor; towards a universal code formatter through machine learning; the IDE portability problem and its solution in Monto; principled syntactic code completion using placeholders; automated testing support for reactive domain-specific modelling languages; symbolic execution of high-level transformations; and raincode assembler compiler.",,2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Safety transformations: Sound and complete?","Safety transformations transform unsafe original software into safe software that, in contrast to the unsafe version, detects if its execution was incorrect due to execution errors. Especially transformations based on arithmetic codes such as an AN- or ANB-code apply complex and error-prone transformations, while at the same time aiming for safety- or mission-critical applications. Testing and error injection are used so far to ensure correctness and error detection capabilities. But both are incomplete and might miss errors that change functionality or reduce error detection rates. Our research provides tools for a complete analysis of AN-encoding safety transformations. This paper presents our analysis tools and results for the AN-encoded operations. While we were able to demonstrate functional correctness, we discovered bugs that prevent propagation of errors almost completely for AN-encoded divisions and reduce propagation significantly for logical bitwise operations. &copy; 2013 Springer-Verlag.<br/>","Schiffel, Ute",2013,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"49th Annual Convention of Computer Society of India: Emerging ICT for Bridging the Future, CSI 2014","The proceedings contain 70 papers. The special focus in this conference is on Machine Learning, Network and Information security, Data Mining, Data Engineering and Soft Computing. The topics include: A non-local means filtering algorithm for restoration of rician distributed MRI; a probabilistic based multi-label classification method using partial information; design and utilization of bounding box in human detection and activity identification; recognition and tracking of occluded objects in dynamic scenes; design of fuzzy logic controller to drive autopilot altitude in landing phase; a discrete particle swarm optimization based clustering algorithm for wireless sensor networks; a short survey on teaching learning based optimization; linear array optimization using teaching learning based optimization; multilevel thresholding in image segmentation using swarm algorithms; efficient data aggregation approaches over cloud in wireless sensor networks; sensor controlled sanitizer door knob with scan technique; detection of black hole attack using code division security method; network quality estimation - error protection and fault localization in router based network; the probabilistic encryption algorithm using linear transformation; network management framework for network forensic analysis; a new approach for data hiding with LSB steganography; designing energy-aware adaptive routing for wireless sensor networks; dynamic recurrent FLANN based adaptive model for forecasting of stock indices; effect of mahalanobis distance on time series classification using shapelets; diabetic retinal exudates detection using extreme learning machine and a memory efficient algorithm with enhance preprocessing technique for web usage mining.",,2015,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"FEVER: An approach to analyze feature-oriented changes and artefact co-evolution in highly configurable systems","The evolution of highly configurable systems is known to be a challenging task. Thorough understanding of configuration options their relationships, and their implementation in various types of artefacts (variability model, mapping, and implementation) is required to avoid compilation errors, invalid products, or dead code. Recent studies focusing on co-evolution of artefacts detailed feature-oriented change scenarios, describing how related artefacts might change over time. However, relying on manual analysis of commits, such work do not provide the means to obtain quantitative information on the frequency of described scenarios nor information on the exhaustiveness of the presented scenarios for the evolution of a large scale system. In this work, we propose FEVER and its instantiation for the Linux kernel. FEVER extracts detailed information on changes in variability models (KConfig files), assets (preprocessor based C code), and mappings (Makefiles). We apply this methodology to the Linux kernel and build a dataset comprised of 15 releases of the kernel history. We performed an evaluation of the FEVER approach by manually inspecting the data and compared it with commits in the system&rsquo;s history. The evaluation shows that FEVER accurately captures feature related changes for more than 85% of the 810 manually inspected commits. We use the collected data to reflect on occurrences of co-evolution in practice. Our analysis shows that complex co-evolution scenarios occur in every studied release but are not among the most frequent change scenarios, as they only occur for 8 to 13% of the evolving features. Moreover, only a minority of developers working on a given release will make changes to all artefacts related to a feature (between 10% and 13% of authors). While our conclusions are derived from observations on the evolution of the Linux kernel, we believe that they may have implications for tool developers as well as guide further research in the field of co-evolution of artefacts.<br/> &copy; 2017, The Author(s).","Dintzner, Nicolas and van Deursen, Arie and Pinzger, Martin",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"IEEE International Conference on Program Comprehension","The proceedings contain 41 papers. The topics discussed include: software engineers' information seeking behavior in change impact analysis-an interview study; variability through the eyes of the programmer; meaningful identifier names: the case of single-letter variables; effects of variable names on comprehension: an empirical study; exploiting type hints in method argument names to improve lightweight type inference; binary code clone detection across architectures and compiling configurations; identifying code clones having high possibilities of containing bugs; smells are sensitive to developers! on the efficiency of (un)guided customized detection; on the uniqueness of code redundancies; RepDroid: an automated tool for android application repackaging detection; NetDroid: summarizing network behavior of Android apps for network code maintenance; an exploratory study on the relationship between changes and refactoring; developer-related factors in change prediction: an empirical assessment; analyzing user comments on YouTube coding tutorial videos; bug localization with combination of deep learning and information retrieval; and bug report enrichment with application of automated fixer recommendation.",,2017,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"A study on SiC cladding option for light water reactor fuel","In this study, the public version of FEMAXI code was modified for silicon carbide (SiC)composite cladding Light Water Reactor (LWR) fuel analysis by implementing SiC material property data. A sample calculation was conducted using a short LWR fuel rod and the results were compared to those of zircaloy cladding fuel. The apparent differences in material properties of SiC and zircaloy are the thermal conductivity and stiffness. Though the thermal conductivity of SiC is excellent in a non-irradi at edcondition, it degrades appreciably when irradiated, which can cause higher temperature in the fuel pellet, more thermal expansion, and more release of fission gas when compared with the zircaloy cladding fuel. While the fuel gap is closed earlier (at 5 GWd/t for the sample case) for the zircaloy cladding fuel, the same initial fuel gap of the SiC cladding fuel is not closed until 18GWd/t due to high stiffness of SiC. The radial dimensional change of the SiC cladding is very small as SiC doesn't shrink into the pellet. Because no experi-mental data are available so far for SiC composite cladding fuel, sensitivity calculations are used to explore the implementation of the SiC cladding model.<br/>","Choi, Hangbok and Gutierrez, Oscar",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Proceedings of the International Conference on Compilers, Architectures and Synthesis for Embedded Systems, CASES 2016","The proceedings contain 21 papers. The topics discussed include: ILP-based modulo scheduling for high-level synthesis; handling large data sets for high-performance embedded applications in heterogeneous systems-on-chip; theoretical foundations for workload modeling with implications on power optimization; thermal-driven resource allocation and application mapping for complex many core systems; runtime management of adaptive MPSoCs for graceful degradation; towards the design of fault-tolerant mixed-criticality systems on multicores; COMET: communication-optimised multi-threaded error-detection technique; neural network transformation and co-design under neuromorphic hardware constraints; cambricon: an instruction set architecture for neural networks; RRAM based learning acceleration; a real-time digital-microfluidic platform for epigenetics; LOCUS: low-power customizable many-core architecture for wearables; D-PUF: an intrinsically reconfigurable DRAM PUF for device authentication in embedded systems; hybrid network-on-chip architectures for accelerating deep learning kernels on heterogeneous Manycore platforms; matrix multiplication beyond auto-tuning: rewrite-based GPU code generation; and a jump-target identification method for multi-architecture static binary translation.",,2016,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Fluid and gyrokinetic modelling of particle transport in plasmas with hollow density profiles","Hollow density profiles occur in connection with pellet fuelling and L to H transitions. A positive density gradient could potentially stabilize the turbulence or change the relation between convective and diffusive fluxes, thereby reducing the turbulent transport of particles towards the center, making the fuelling scheme inefficient. In the present work, the particle transport driven by ITG/TE mode turbulence in regions of hollow density profiles is studied by fluid as well as gyrokinetic simulations. The fluid model used, an extended version of the Weiland transport model, Extended Drift Wave Model (EDWM), incorporates an arbitrary number of ion species in a multi-fluid description, and an extended wavelength spectrum. The fluid model, which is fast and hence suitable for use in predictive simulations, is compared to gyrokinetic simulations using the code GENE. Typical tokamak parameters are used based on the Cyclone Base Case. Parameter scans in key plasma parameters like plasma &beta;, R/L<inf>T</inf>, and magnetic shear are investigated. It is found that &beta; in particular has a stabilizing effect in the negative R/L<inf>n</inf>region, both nonlinear GENE and EDWM show a decrease in inward flux for negative R/L<inf>n</inf>and a change of direction from inward to outward for positive R/L<inf>n</inf>. This might have serious consequences for pellet fuelling of high &beta; plasmas.<br/> &copy; Published under licence by IOP Publishing Ltd.","Tegnered, D. and Oberparleiter, M. and Nordman, H. and Strand, P.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An adapted yield criterion for the evolution of subsequent yield surfaces","In numerical analysis of sheet metal forming processes, the anisotropic material behaviour is often modelled with isotropic work hardening and an average Lankford coefficient. In contrast, experimental observations show an evolution of the Lankford coefficients, which can be associated with a yield surface change due to kinematic and distortional hardening. Commonly, extensive efforts are carried out to describe these phenomena. In this paper an isotropic material model based on the Yld2000-2d criterion is adapted with an evolving yield exponent in order to change the yield surface shape. The yield exponent is linked to the accumulative plastic strain. This change has the effect of a rotating yield surface normal. As the normal is directly related to the Lankford coefficient, the change can be used to model the evolution of the Lankford coefficient during yielding. The paper will focus on the numerical implementation of the adapted material model for the FE-code LS-Dyna, mpi-version R7.1.2-d. A recently introduced identification scheme [1] is used to obtain the parameters for the evolving yield surface and will be briefly described for the proposed model. The suitability for numerical analysis will be discussed for deep drawing processes in general. Efforts for material characterization and modelling will be compared to other common yield surface descriptions. Besides experimental efforts and achieved accuracy, the potential of flexibility in material models and the risk of ambiguity during identification are of major interest in this paper.<br/> &copy; Published under licence by IOP Publishing Ltd.","Kusters, N. and Brosius, A.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"2016 21st International Conference on Methods and Models in Automation and Robotics, MMAR 2016","The proceedings contain 224 papers. The topics discussed include: solvability of reactions and inverse dynamics problem for complex kinematic chains; ArchGenTool: a system-independent collaborative tool for robotic architecture design; signal fusion of changes in the inductive loop impedance components for vehicle axle detection; implementation of non-zero initial conditions for multi-notch FIR filter using Raspberry Pi; real time localization system with extended Kalman filter for indoor applications; linearization by generalized input-output injections on homogeneous time scales; computation of final dimension initial conditions and inputs for given outputs of differential-algebraic systems with delay; optimal sensor selection for model identification in iterative learning control of spatio-temporal systems; design of iterative learning control schemes for systems with sector-bounded nonlinearities; transformation of a fuzzy interpreted Petri net diagram into structured text code; estimation of the blood volume in pneumatically controlled ventricular assist device by vision sensor and image processing technique; and comparison of real-time industrial process control solutions: glass melting case study.",,2016,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European So ftware Engineering Conference and Symposium on the Foundations of So ftware Engineering","The proceedings contain 122 papers. The topics discussed include: Testing multithreaded programs via thread speed control; data race detection on compressed traces; using finite-state models for log differencing; identifying impactful service system problems via log analysis; learning to sample: exploiting similarities across environments to learn performance models for configurable systems; scalability-first pointer analysis with self-tuning context-sensitivity; code vectors: understanding programs through embedded abstracted symbolic traces; what makes a code change easier to review: an empirical investigation on code change reviewability; and the impact of regular expression denial of service (ReDoS) in practice: an empirical study at the ecosystem scale.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Development of an in-core fuel management tool for boiling water reactors","The in-core fuel management of a nuclear reactor is a challenging task due to the virtually infinite number of loading patterns one could theoretically adopt. The ROSA (Reloading Optimization by Simulated Annealing) code is an optimization tool that has been successfully used in the last two decades to facilitate the core design of several Pressurized Water Reactors (PWRs). It is designed to perform a stochastic search for an optimal Loading Pattern (LP) using a simulated annealing algorithm. This corresponds to performing a depletion calculation for each one of the hundreds of thousands of unique LPs generated during the stochastic search. Therefore, speed is one of the most important requirements that the solvers used by the depletion tool must fulfill. ROSA's depletion analysis tool makes use of a particularly fast nodal method (known as the kernel method) for the evaluation of the power distribution associated with a particular LP. One of the strongest assumptions behind the kernel method is that the neutron migration length does not change considerably between the point where a neutron is generated and the point where the same neutron is absorbed. Although strong, this assumption is quite compatible with the neutronic characteristics of PWRs cores. In this paper we give an overview of the work done in order to develop a version of ROSA capable of performing the core design of Boiling Water Reactors (BWRs). We focus the discussion on the development of the depletion analysis tool by outlining the modifications of the kernel methods implemented in order to make the solver accurate for BWR cores. An improvement of the definition of the transport kernel is necessary to take the strong anisotropies characterizing the neutronic problem into account. These anisotropies arise due to the presence of strong changes in the moderator density and due to the presence of control blades. Furthermore, we are going to discuss how the boundary conditions are adopted by the neutronic solver in the form of albedos, which are used as a calibration parameter to tune the solution using the information obtained from an external license code. We then analyze the accuracy of ROSA-BWR when predicting power distributions and safety parameters by benchmarking it against the core follow calculations obtained by using the license code MICROBURN-B2 for three cycles of the Browns Ferry 3 nuclear power plant. This benchmark is also discussed in terms of computational performances and potential gains achievable through parallelization. The final section of the paper contains a description of the optimization capabilities that are currently implemented in ROSA-BWR and an overview of the preliminary applications of the code to real life core design studies.<br/>","Gilli, Luca and Wakker, Pieter H. and Elder, Brian R.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Impact of a hollow density profile on turbulent particle fluxes: Gyrokinetic and fluid simulations","Hollow density profiles may occur in connection with pellet fuelling and L to H transitions. A positive density gradient could potentially stabilize the turbulence or change the relation between convective and diffusive fluxes, thereby reducing the turbulent transport of particles towards the center, making the pellet fuelling scheme inefficient. In the present work, the particle transport driven by Ion Temperature Gradient/Trapped Electron (ITG/TE) mode turbulence in hollow density profiles is studied by fluid as well as gyrokinetic simulations. The fluid model used, an extended version of the Weiland transport model, Extended Drift Wave Model (EDWM), incorporates an arbitrary number of ion species in a multi-fluid description and an extended wavelength spectrum. The fluid model, which is fast and hence suitable for use in predictive simulations, is compared to gyrokinetic simulations using the code GENE. Typical tokamak parameters are used based on the Cyclone Base Case. Parameter scans in key plasma parameters like plasma &beta;, R/L<inf>T</inf>, and magnetic shear are investigated. In addition, the effects of a fast species are studied and global ITG simulations in a simplified physics description are performed in order to investigate nonlocal effects. It is found that &beta; in particular, has a stabilizing effect in the negative R/L<inf>n</inf>region. Both nonlinear GENE and EDWM simulations show a decrease in inward flux for negative R/L<inf>n</inf>and a change in the direction from inward to outward for positive R/L<inf>n</inf>. Moreover, the addition of fast particles was shown to decrease the inward main ion particle flux in the positive gradient region further. This might have serious consequences for pellet fuelling of high &beta; plasmas. Additionally, the heat flux in global ITG turbulence simulations indicates that nonlocal effects can play a different role from usual in connection with pellet fuelling.<br/> &copy; 2017 Author(s).","Tegnered, D. and Oberparleiter, M. and Strand, P. and Nordman, H.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"How do developers react to RESTful API evolution?","With the rapid adoption of REpresentational State Transfer (REST), more software organizations expose their applications as RESTful web APIs and client code developers integrate RESTful APIs into their applications. When web APIs evolve, the client code developers have to update their applications to incorporate the API changes accordingly. However client code developers often encounter challenges during the migration and API providers have little knowledge of how client code developers react to the API changes. In this paper, we investigate the changes among subsequent versions of APIs and classify the identified changes to understand how the RESTful web APIs evolve. We study the on-line discussion from developers to the API changes by analyzing the StackOverflow questions. Through an empirical study, we identify 21 change types and 7 of them are new compared with existing studies. We find that a larger portion of RESTful web API elements are changed between versions compared with Java APIs and WSDL services. Moreover, our results show that adding new methods in the new version causes more questions and views from developers. However the deleted methods draw more relevant discussions. In general, our results provide valuable insights of RESTful web API evolution and help service providers understand how their consumers react to the API changes in order to improve the practice of evolving the service APIs.<br/> &copy; Springer-Verlag Berlin Heidelberg 2014.","Wang, Shaohua and Keivanloo, Iman and Zou, Ying",2014,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"An empirical study on the impact of AspectJ on software evolvability","Since its inception in 1996, aspect-oriented programming (AOP) has been believed to reduce the effort required to maintain software systems by replacing cross-cutting code with aspects. However, little convincing empirical evidence exists to support this claim, while several studies suggest that AOP brings new obstacles to maintainability. This paper discusses two controlled experiments conducted to evaluate the impact of AspectJ (the most mature and popular aspect-oriented programming language) versus Java on software evolvability. We consider evolvability as the ease with which a software system can be updated to fulfill new requirements. Since a minor language was compared to the mainstream, the experiments were designed so as to anticipate that the participants were much more experienced in one of the treatments. The first experiment was performed on 35 student subjects who were asked to comprehend either Java or AspectJ implementation of the same system, and perform the corresponding comprehension tasks. Participants of both groups achieved a high rate of correct answers without a statistically significant difference between the groups. Nevertheless, the Java group significantly outperformed the AspectJ group with respect to the average completion time. In the second experiment, 24 student subjects were asked to implement (in a non-invasive way) two extension scenarios to the system that they had already known. Each subject evolved either the Java version using Java or the AspectJ version using AspectJ. We found out that a typical AspectJ programmer needs significantly fewer atomic changes to implement the change scenarios than a typical Java programmer, but we did not observe a significant difference in completion time. The overall result indicates that AspectJ has a different effect on two sub-characteristics of the evolvability: understandability and changeability. While AspectJ decreases the former, it improves one aspect of the latter.<br/> &copy; 2017, The Author(s).","Przybylek, Adam",2018,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10, CR8"
"The hydrogen issue in the initial operation of a filtered containment venting system","This study evaluated the hydrogen issue in the initial operation of a filtered containment venting system (FCVS). We calculated the volumetric concentration of hydrogen, steam, and air in the postulated FCVS connected with the OPR 1000, as a target nuclear power plant, under a station blackout using the MELCOR computer code (version 1.8.6). A large amount of steam and a flammable mixture generated during a severe accident are immediately released from the containment building to the FCVS when the pressure in the containment building approaches a set value. The constituent ratio of the flammable mixture of hydrogen, steam, and air can change due to the different thermal-hydraulic conditions between those due to a severe accident in the containment building and the initial condition in the FCVS. The volumetric concentration of hydrogen was 6% in the containment building just before the operation of the FCVS. It increased up to 9% in the FCVS vessel during the early operation, and steam condensation occurred simultaneously. The atmospheric condition including steam, hydrogen, and air in the FCVS can enter the combustion zone in the Shapiro diagram.<br/>","Na, Young Su and Cho, Song-Won and Ha, Kwang Soon",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Compiler Construction - 22nd International Conference, CC 2013, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2013, Proceedings","The proceedings contain 13 papers. The topics discussed include: optimal register allocation in polynomial time; optimal and heuristic global code motion for minimal spilling; efficient and effective handling of exceptions in java points-to analysis; an incremental points-to analysis with CFL-reachability; FESA: fold- and expand-based shape analysis; simple and efficient construction of static single assignment form; PolyGLoT: a polyhedral loop transformation framework for a graphical dataflow language; architecture-independent dynamic information flow tracking; automatic generation of program affinity policies using machine learning; compiler-guided identification of critical sections in parallel code; refactoring MATLAB; and on LR parsing with selective delays.",,2013,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Improving of a circuit checkability and trustworthiness of data processing results in LUT-based FPGA components of safety-related systems","The possibility of improving on attributes of a solution which are traditionally opposed each other is proved. For digital components of safety-related systems, the method of improving on attributes of a checkability of the circuit and trustworthiness of the results calculated on FPGA with the LUT-oriented architecture is offered. The method is directed to improving of the ready project by a choice of the version of a program code without change of the hardware decision. Versions of LUT memory programming and a set of faults on which these versions exert impact are generated. Faults of shorts between adjacent address inputs of LUT are considered. Operation of the circuit is simulated on all versions and on all set of faults. The method selects the versions providing increase in a checkability of the circuit in a normal mode and trustworthiness of results in emergency mode of the safety-related systems.<br/>","Drozd, Oleksandr and Drozd, Miroslav and Martynyuk, Oleksandr and Kuznietsov, Mykola",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Verification of new floating capabilities in FAST v8","FAST v8 is the latest release of the National Renewable Energy Laboratory&rsquo;s wind turbine aero-hydro-servo-elastic simulation software, with several new capabilities and major changes from the previous version. FAST has been significantly altered to improve the simulator&rsquo;s modularity and to include new functionalities in the form of modules in the FAST v8 framework. This paper focuses on the improvements made for the modeling of floating offshore wind systems. The most significant change was to the hydrodynamic load calculation algorithms, which are embedded in the HydroDyn module. HydroDyn is now capable of applying strip-theory (via an extension of Morison&rsquo;s equation) at the member level for user-defined geometries. Users may now use a strip-theory-only approach for applying the hydrodynamic loads, as well as the previous potential-flow (radiation/diffraction) approach and a hybrid combination of both methods (radiation/diffraction and the drag component of Morison&rsquo;s equation). Second-order hydrodynamic implementations in both the wave kinematics used by the strip-theory solution and the wave-excitation loads in the potential-flow solution were also added to HydroDyn. The new floating capabilities were verified through a direct code-to-code comparison. We conducted a series of simulations of the International Energy Agency Wind Task 30 Offshore Code Comparison Collaboration Continuation (OC4) floating semisubmersible model and compared the wind turbine response predicted by FAST v8, the corresponding FAST v7 results, and results from other participants in the OC4 project. We found good agreement between FAST v7 and FAST v8 when using the linear radiation/diffraction modeling approach. The strip-theory-based approach inherently differs from the radiation/diffraction approach used in FAST v7 and we identified and characterized the differences. Enabling the second-order effects significantly improved the agreement between FAST v8 and the other OC4 participants.<br/> &copy; 2015, American Institute of Aeronautics and Astronautics Inc. All rights reserved.","Wendt, Fabian and Robertson, Amy and Jonkman, Jason and Hayman, Greg",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Facilitating the specification of wsmo ontology using model-driven development","A semantic web service extends the capabilities of a web service by associating a semantic description of the web service in order to enable better search, discovery, selection, composition and integration. Semantically rich language such as Web Service Modeling Ontology (WSMO) has been created in order to provide a mechanism for describing the semantics of semantic web services. Unfortunately, for the common developer the learning curve for such languages can be steep, providing a barrier for adoption and widespread use. In this paper, we present a Model- Driven Architecture (MDA) approach for facilitating the specification of WSMO ontology through the use of meta-model and UML profile for modelling semantic web services; we concentrate our efforts to develop a transformation approach based on MDA to automate the generation of code by translating XMI specifications (e.g. XML encodings of UML) into equivalent WSMO specifications using Atlas Transformation Language (ATL) transformations. &copy;2014 Inderscience Enterprises Ltd.<br/>","Bensaber, Djamel Amar",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Selecting and scaling methods of bi-directional horizontal ground motion records using BdMA spectrum","Bi-directional ground motions with two orthogonal horizontal values are required to predict seismic response of high-rise and complicated buildings using time history analysis. By reviewing current methods of selecting and scaling bi-directional ground motion records, it points out shortcomings of the Chinese codes in this aspect. In addition, based on the bi-directional maximum acceleration (BdMA) spectrum, the BdMA method of selecting and scaling ground motion records was developed. Nonlinear time history analyses were performed for a representative building subjected to bi-directional ground motions selected using the BdMA method and the one used in the current code in China. The differences of predicting the structural seismic response using these two methods were compared. Further the reliability of seismic response prediction based on the current Chinese code was evaluated. The results indicate that, the BdMA method produces a larger structural seismic response than the Chinese code method. The former one has 7% and 15% larger results than the latter for force and displacement prediction respectively. With the change of types of ground motions controlling seismic hazard, the method in China code has different levels of hazard compared to the BdMA method. Using uniform performance index to control the structural seismic performance, the ultimate structural seismic safety for these two methods is different. The BdMA method of selecting and scaling bi-directional ground motions is recommended.<br/> &copy; 2017, Editorial Office of Journal of Building Structures. All right reserved.","Wang, Min and Li, Wenjie and Zhu, Aiping and Fu, Jianping",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Using random error correcting codes in near-collision attacks on generic hash-functions","In this paper we consider the problem of finding nearcollisions with Hamming distance bounded by r in generic n-bit hash functions. In 2011, Lamberger and Rijmen proposed a modified version of Pollard&rsquo;s rho method, and in 2012 Leurent improved this memoryless algorithm by using any available memory to store chain endpoints. Both algorithms use a perfect error correcting code to change near-collisions into full-collisions, but such codes are rare and have very small distance. In this paper we propose using randomly chosen linear codes, whose decoding can be made efficient by using some of the available memory to store error-correction tables. Compared to Leurent&rsquo;s algorithm, we experimentally verified an improvement ratio of about 3 in a small example with n = 160 and r = 33 which we implemented on a single PC, and mathematically predicted a significant improvement ratio of about 730 in a larger example with n = 1024 and r = 100, using 2<sup>40</sup>memory.<br/> &copy; Springer International Publishing Switzerland 2014.","Polak, Inna and Shamir, Adi",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Towards a unified heterogeneous development model in android","The advent of emergent SoCs and MPSocs opens a new era on The small mobile devices (Smartphones, Tablets,.) in Terms of computing capabilities and applications To be addressed. The efficient use of such devices, including The parallel power, is still a challenge for general purpose programmers due To The very high learning curve demanding very specific knowledge of The devices. While some efforts are currently being made, mainly in The scientific scope, The scenario is still quite far from being The desirable for non-scientific applications where very few of Them Take advantage of The parallel capabilities of The devices. We propose Paralldroid (Framework for Parallelism in Android), a parallel development framework oriented To general purpose programmers for standard mobile devices. Paralldroid presents a programming model That unifies The different programming models of Android. The user just implements a Java application and introduces a set of Paralldroid annotations in The sections of code To be optimized. The Paralldroid system automatically generates The native C, OpenCL or Renderscript code for The annotated section. The Paralldroid Transformation model involves source-To-source Transformations and skeletal programming. &copy; 2014 Springer-Verlag Berlin Heidelberg.<br/>","Acosta, Alejandro and Almeida, Francisc",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Bridging a gap: in search of an analytical tool capturing teachers perceptions of their own teaching","Computing and computers are introduced in school as important examples of technology, sometimes as a subject matter of their own, and sometimes they are used as tools for other subjects. All in all, one might even say that learning about computing and computers is part of learning about technology. Lately, many countries have implemented programming in their curricula as a means to address society&rsquo;s dependence on, and need for programming knowledge and code. Programming is a fairly new school subject without educational traditions and, due to the rapid technological development, in constant change. This means that most programming teachers must decide for themselves what and how to teach. In this study, programming teachers&rsquo; teaching is studied. With the aim of exploring the connection/possible gap between teacher&rsquo;s intentions and the teacher&rsquo;s instructional practice, an expansion of the conceptual apparatus of phenomenography and variation theory is tested. In the article, phenomenography and variation theory and the suggested supplementary theoretical tool (Georg Henrik von Wright&rsquo;s model of logic of events) are briefly presented and then deployed upon one selected case. Findings reveal that teachers&rsquo; intentions (reflected in their actions) include an emphasis (of teachers&rsquo; side) on the importance of balancing theory and practice, using different learning strategies, encouraging learning by trial-and-error and fostering collaboration between students for a deeper understanding of concepts. In conclusion, logic of events interpretations proves to be useful as a complementary tool to the conceptual apparatus of phenomenography.<br/> &copy; 2016, Springer Science+Business Media Dordrecht.","Rolandsson, Lennart and Skogh, Inga-Britt and Mannikko Barbutiu, Sirkku",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Design of in-service repair welding procedures for operating pipelines: Critical assessment of variables affecting restraint level and heat-affected zone microstructures of vintage pipelines","In order to maintain pipeline operation during repair and maintenance work, operators typically install branch (i.e. hot-tap) and repair fittings (i.e. sleeves) onto flowing pipelines. In-service welding procedures must be designed for these installations per code requirement. Welding induced cracking during the installation of pressure containing repair fittings is a major concern when welding onto flowing pipelines. Repair fitting dimensions influence cooling rates and restraint conditions. A combination of high stress and brittle microstructures formed during the rapid cooling of high carbon equivalent vintage pipeline steel can create conditions that promote the formation of cracks. CSA Z662 and API 1104 specify essential variables (requirements) that aim to mitigate risk of cracking by qualifying the weld procedure to equal or more severe conditions than expected in the field. These essential variables can include material carbon equivalent, cooling rate, and level of restraint limitations to be applied during qualification of the weld procedure. This paper will focus on the creation of a safe welding procedure by prewelding assessment of the phase transformations that occur during welding on liquid product vintage pipelines and modelling the influence of readily quantifiable variables on the level of restraint induced by repair fittings. Finite element analysis (FEA) was utilized to study the thermal history of simulated in-service weld heat affected zones to approximate the stress and strain magnitudes (level of restraint) at the fillet weld toe of simulated sleeve repairs. Thermal analysis was conducted on various weld bead geometries to simulate the effects of cooling rates and tempering. To aid in the design of a safe weld procedure, two continuous cooling transformation (CCT) diagrams were constructed from a vintage 1960s API 5L X52 pipe with a carbon equivalent of 0.51% (CEN and IIW). This enabled the selection of optimal welding parameters that produced desirable HAZ microstructures. The modeling of restraint level accounted for the thermal expansion and contraction of a multipass fillet weld sequence on various pipe and sleeve thicknesses. The sleeve-on-pipe configuration was compared to the plate-on-plate configuration. Sleeve wall thickness was varied from 1 to 7 times the pipe wall thickness to account for any possible instances where a very thick fitting, such as emergency fittings (e.g. STOPPLE), may be installed on a thin pipeline. Test welds were completed on the 1960s vintage pipeline steel with a high volume water flow loop to simulate operating conditions. The heat affected zone hardness values correlated well with those predicted by the FEA and CCT results.<br/> &copy; Copyright 2016 by ASME.","Guest, Stuart and Dyck, Jason and Egbewande, Afolabi and MacKenzie, Robert and Sadowski, Mark",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Online structural health monitoring and parameter estimation for vibrating active cantilever beams using low-priced microcontrollers","This paper presents a structural health monitoring and parameter estimation system for vibrating active cantilever beams using low-cost embedded computing hardware. The actuator input and the measured position are used in an augmented nonlinear model to observe the dynamic states and parameters of the beam by the continuous-discrete extended Kalman filter (EKF). The presence of undesirable structural change is detected by variations of the first resonance estimate computed from the observed equivalent mass, stiffness, damping, and voltage-force conversion coefficients. A fault signal is generated upon its departure from a predetermined nominal tolerance band. The algorithm is implemented using automatically generated and deployed machine code on an electronics prototyping platform, featuring an economically feasible 8-bit microcontroller unit (MCU). The validation experiments demonstrate the viability of the proposed system to detect sudden or gradual mechanical changes in real-time, while the functionality on low-cost miniaturized hardware suggests a strong potential for mass-production and structural integration. The modest computing power of the microcontroller and automated code generation designates the proposed system only for very flexible structures, with a first dominant resonant frequency under 4 Hz; however, a code-optimized version certainly allows much stiffer structures or more complicated models on the same hardware.<br/> &copy; 2015 Gergely Tak&aacute;cs et al.","Takacs, Gergely and Vachalek, Jan and Rohal'-Ilkiv, Boris",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Experimental and crash-worthiness optimization of end-capped conical tubes under quasi-static and dynamic loading","In the present study, the energy absorption capacity of thin-walled end-capped conical geometries is taken into consideration and the collapse of the absorbers under different loading types and strengths is investigated. First, the manual spinning method is utilized in order to manufacture the specimen. The manufacturing geometry quality of the parts is then evaluated. Next, quasi-static load-deflection tests are employed to investigate the collapse process as well as the calculation of energy absorption for conical tubes. Drop experiments are carried out using a free flight drop tower on the conical tubes to obtain the acceleration time-history of the hammer. The time history of hammer velocity change during its collision with the energy absorber, the mean collapse load of the absorber and absorbed energy are calculated. The explicit FE code Abaqus/explicit is employed and validated using dynamic experimental data. Finally, a multi-objective optimization method is used to find the tube geometry which has the maximum energy absorption and specific energy absorption. The results show that the impactor's velocity, as well as the geometrical characteristics of the end-capped conical tubes, have significant effects on the energy absorption and specific energy absorption capacity.<br/> &copy; 2018 Taylor & Francis Group, LLC","Rahi, Abbas",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Mathematical modeling of steel heat treating using CCT diagrams","Mathematical modeling is a powerful tool to design, control and optimize heat-treating processes. However, the complex interactions occurring between the thermal, microstructural, and stress fields inside the part during those processes (which must be taken into account in detailed modeling work) precludes its use in many instances-especially in a production environment. Thus, it is desirable to find methodologies that can speed up the simulations while maintaining the mathematical model close to reality. In this work, the evolution of the microstructural field was estimated from fraction-transformed-temperature correlations derived directly from a published continuous-cooling-transformation (CCT) diagram, which uses the cooling rate at 750&deg;C as the x-axis. This approach ""softens"" the coupling between the thermal and fraction-transformed fields resulting in an efficient algorithm. The thermal field evolution was computed using standard procedures embedded in the commercially available code Abaqus, whereas empirical equations describing the fraction transformed-temperature relationships were programmed through user subroutines. The mathematical model was validated by comparing measured and model-predicted thermal response and final microstructure. In the experiments, austenitized AISI 4140 steel cylindrical probes (0.5-in. diameter &times; 2-in. length) were cooled in: (1) still air, and (2) a fluidized bed reactor, both at room temperature. The thermal response was measured during the cooling process by inserting two thermocouples: one at the geometrical center of the probe and the other near the probe surface, at mid-length. The latter was input to a code developed in-house to estimate the surface heat flux history, which constitutes the active boundary condition for the direct heat conduction problem and was, in turn, fed to the computational model. Once heat treated, the probes were prepared for metallographic observation using standard techniques. The results indicate that the proposed methodology can be used for predicting the thermo-microstructural evolution during a heat-treating process.<br/> &copy; 2012 by ASTM International.","Hernandez-Morales, B. and Tellez-Martinez, J.S. and Duenas-Perez, A.M. and Diaz-Cruz, M.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Bridging the gap between ABET outcomes and industry expectations - A case study on software engineering course","The role of ABET accreditation system is quite significant in providing guidance towards program and course design. The program outcomes encompassing through knowledge, skill and attitude play an important role towards competency development of the students. To improve the employability quotient of these fresh graduates, the alignment of respective outcomes along with the base lined expectations of IT industry is needed. In this context Software Engineering (SE) plays an important role in the transformation journey of the graduates to become entry level developers. It also helps to bridge the gap between academia and IT industry so that these new hires are productive as soon as possible. In this paper, the expectations of an IT industry from the new hires is described in conjunction with ABET educational outcomes highlighting pedagogical aspects that enable students develop these abilities. SE course is used as vehicle and an approach is presented integrating software code of ethics (recommended by ACM - IEEE), SE principles, pedagogical aspects and assessment instruments. Subsequently experiences from our corporate learning environment are highlighted.<br/> &copy; 2014 IEEE.","Seshagiri, Saradhi and Goteti, Lns Prakash",2014,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Time-dependent damage characteristics of steel-concrete composite structures under rarely occurred earthquake","For steel-concrete composite structure, the dynamic elastic-plastic time history analysis method is utilized to obtain the structural dynamic response under the rarely occurred earthquake. The seismic damage time-dependent model, which integrates global method and weighting combination method, respectively, is utilized to study the structural damage time-dependent characteristics under the rarely occurred earthquake. Considering that the parameters such as the stiffness and the damping are constants in the calculation but have the variable characteristics in the whole service period, they are treated as pseudo variables. A typical steel-concrete composite sample is studied. The results show that the damage has significantly time-dependent characteristics under rare earthquake and maybe exceeds the performance limits specified in the code. The stiffness change can cause the changes of lateral resistance force and earthquake conduction ability, thus leading to the significant fluctuations of the time-dependent damage. The influence of lateral resistance force on damage plays a dominant role. The more severe the earthquake, the greater the effect of earthquake conduction ability on damage. This can lead to more significant fluctuation of damage and vibration shapes. Comparing the time-dependent damage values and the design values without considering material time-dependent characteristics, it can be found that the material time-dependent characteristics have an important effect on the accuracy of the seismic design and evaluation.<br/> &copy; 2017, Editorial Department of Journal of Southeast University. All right reserved.","Wang, Ying and Wang, Ruozhu and Li, Zhaoxia",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Optimal power allocation in DS-CDMA with adaptive SIC technique","This paper proposes an optimal power allocation in direct sequence-code division multiple access (DS-CDMA) system. The objective is to minimize total transmit power, while simultaneously meeting the certain sum channel capacity (data transmission rate) and outage probability constraints on Rayleigh fading channel. Then a weighted correlator with an adaptive successive interference cancelation (SIC) scheme is developed using neural network (NN) for an improvement in receiver performance. A closed mathematical form of joint probability of error (JPOE) is derived. This determines the number of active users' interfering effect that needs to be canceled in order to achieve a desired bit error rate (BER) value. Mathematical analysis shows that better receiver performance can be achieved through large change in weight up-gradation (w) for the strong users with a particular change in learning rate (&eta;). Simulation results in terms of sum capacity as well as weak user's (users with poor channel gain) capacity, outage probability and BER performance duly support the effectiveness of the proposed scheme over the existing works. &copy; 2013 Springer Science+Business Media New York.<br/>","Maity, Santi P. and Hati, Sumanta and Maji, Chinmay",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Multi-point seismic motions based on focal mechanism and considering local site multi-layer soil effect: Theory and program implementation","In this paper, a framework for generating multi-point earthquake motion of target filed in China, basing on focal mechanism and taking account of the spatial variability of soil properties, is given and proposed. The main contents include: (1) the transfer function of site including multiple soil layers is es-tablished based on random vibration theory, (2) based on the improved bedrock spectrum applicable to the target site in China, the variability among different site condition is reflected by establishing the transfer function of local site, (3) non-flat factor of site surface is considered on the basis of previous literature, (4) explicit expression phase-angle change by the filtration of multiple soil layers is also given for the convenience of the subsequent program code. Then, according to the theory framework, the visual program MEMS V2011.6 (Multi-support Earthquake Motions Simulation Version 2011.6) is developed and run successfully. The specific functions of the program involves the focal parameters assignment, inputting soil parameters, calculating and displaying transfer function of site soil with multi-layer, adjusting non-stationary parameters and frequency, generating multi-point earthquake motion histories, verifying the coherence of spatial seismic motions, fitting code-specified spectrum for bridge, building and electronic facility. In addition, based on a bridge example, the multi-point seismic motions of the target field are generated using the program MEMS V2011.6, and the sensibility analysis of the generated ground motions to local site effect and epicentral distance is analyzed as well. The content in this paper involve theory and practicability, and can provide directly reference for engineering.<br/>","Liu, Guo-Huan and Lu, Xin-Zheng and Guo, Wei",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Video quality assessment based on the improved LSTM model","With the development of computer and network technologies, video service and content will continue to dominate while comparing to all other applications. It is particularly important to build a real-time and effective video quality assessment system. Video content, network status, viewing environment and so on will affect the end user quality of experience (QoE). In this work, we evaluate the prediction accuracy and code running time of Support Vector Machine (SVM), Decision Tree (DT) and Long Short Term Memory (LSTM). Moreover, we try to further improve the prediction accuracy from two aspects. One is to introduce some new input features to change the characteristic parameters. The other is to combine the LSTM with traditional machine learning algorithms. Experimental results show that the QoE prediction accuracy can be improved with the increased characteristic parameters. It is worth mentioning that the prediction accuracy can be increased by 8% with the improved LSTM algorithm.<br/> &copy; Springer International Publishing AG 2017.","Bao, Qiuxia and Huang, Ruochen and Wei, Xin",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Selection and ranking of optimal routes through genetic algorithm in a cognitive routing system for mobile ad-hoc network","Genetic algorithm can be used for proper selection and ranking of all possible variable route addresses in mobile ad-hoc network (MANET). Ranking is based upon priority code of the links. A priority code is calculated by respective routing protocol, which depends on different parameters and metrics. A node can change its position and new nodes may join the MANET, so genetic algorithm can better estimate such kind of variations through its crossover and mutation genetic operators. Genetic algorithm is especially useful in cases of novel cognitive routing for MANET. Cognition in MANET is either based upon learning automata method as in some wireless sensor networks or specialized cognitive neural networks. Ranking of optimal links in MANET after a regular interval through genetic algorithm enhance the performance of cognitive routing. It help in proper selection of desired routing protocol for a given set of network conditions. &copy; 2012 IEEE.<br/>","Afridi, Muhammad Ishaq",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Variation factors in the design and analysis of replicated controlled experiments: Three (dis)similar studies on inspections versus unit testing","In formal experiments on software engineering, the number of factors that may impact an outcome is very high. Some factors are controlled and change by design, while others are are either unforeseen or due to chance. This paper aims to explore how context factors change in a series of formal experiments and to identify implications for experimentation and replication practices to enable learning from experimentation. We analyze three experiments on code inspections and structural unit testing. The first two experiments use the same experimental design and instrumentation (replication), while the third, conducted by different researchers, replaces the programs and adapts defect detection methods accordingly (reproduction). Experimental procedures and location also differ between the experiments. Contrary to expectations, there are significant differences between the original experiment and the replication, as well as compared to the reproduction. Some of the differences are due to factors other than the ones designed to vary between experiments, indicating the sensitivity to context factors in software engineering experimentation. In aggregate, the analysis indicates that reducing the complexity of software engineering experiments should be considered by researchers who want to obtain reliable and repeatable empirical measures.<br/> &copy; 2013, Springer Science+Business Media New York.","Runeson, Per and Stefik, Andreas and Andrews, Anneliese",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Effect of loading modes on the limit behaviors for pipe bends under combined loads","Pipe bends are widely used in industrial piping and pipelines. Because of their flexibility properties, they can accommodate thermal expansions and absorb other externally induced loads. Under severe loading conditions, bends exhibit significant cross-section ovalization, associated with strains well beyond the elastic limit and fail because of excessive cross-section ovalization. The past references provide several studies on the ovalization induced loading capacity change. The main aim of the study is to establish the effects of load history on the calculated failure load, when a bend is subjected to a combination loading of in-plane bending, torsion and internal pressure. The results of the investigation show that geometric nonlinearity is a significant consideration when calculating plastic failure loads of pipe bends subject to combined loads. The limit load is path independent of the loading sequence in the cases of bending and torsion combinations. In the cases of pressure and moment combinations the proportional and pressure-moment load cases exhibit significant geometric strengthening, whereas the moment- pressure load case exhibits a little. The results according to the ASME Boiler and Pressure Vessel Code may show overall conservative in all loading conditions.<br/> Copyright &copy; 2016 by ASME.","Li, Jian and Zhou, Changyu and He, Xiaohua",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Influence of different seismic shear force coefficient control methods on seismic performance of a super-tall building","The seismic shear force coefficient is the key parameter of structural seismic design. There are clear specifications for the minimum seismic shear force coefficient in Chinese seismic design code. When this requirement cannot be satisfied, it is required to adjust the design base shear force and story shear force to meet the limitation. If the gap between the target design base shear force and the actual base shear force is too large, it is required to change the structural arrangement or the structural system. Engineering practice shows that if using the method of changing structural arrangement, the amount of construction materials will be too large for a super-tall building whose vibration period is very long and the minimum seismic shear force coefficient is hard to meet the requirement. A super-tall building using three control methods of seismic shear force coefficient was studied. The seismic performance and amount of construction materials were compared and analyzed through elastic analysis and elasto-plastic analysis. The model A, whose structural stiffness was adjusted to meet the seismic shear force coefficient limit, has the largest structural stiffness and highest cost. The model B, whose designed base shear force was adjusted to meet the minimum seismic shear force and the maximum story drift, has lower cost and smaller structural stiffness than the model A. The model C, whose designed base shear was adjusted to meet the minimum seismic shear force has lower cost and smaller structural stiffness than the model B. After large numbers of nonlinear time history analysis, it is found that all the models can meet the displacement limitations in the code. The displacement response of the model B and model C are close. The displacement response of the model A is a bit smaller than the other models.<br/>","Liu, Bin and Qi, Wuhui and Ye, Lieping and Lu, Xinzheng and Chen, Binlei",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Two-dimensional ablation and thermal response analyses for mars science laboratory heat shield","This paper examines transient simulations performed to predict in-depth thermal response and surface recession of the proposed heat shield material for the Mars Science Laboratory entry capsule, that is, phenolic impregnated carbon ablator. The finite volume material response code used in this paper solves the time-dependent governing equations, including energy conservation and a three-component decomposition model, with a surface energy-balance condition and a moving grid system to predict shape change due to surface recession. The predicted in-depth thermal response of heat shield material generally agrees well with the thermocouple data under various arcjet conditions. Also, two-dimensional computations using aerothermal environment for Mars entry (derived from a proposed three-sigma trajectory) are performed around the heat shield shoulder region, where high heating occurs as the result of angle of attack. Parametric studies are conducted to examine the effects of carbon-fiber orientation, material properties, and surface recession on heat shield bondline temperature history. It is proved that the fiber orientation configuration of the baseline heat shield has the lowest maximum bondline temperature.<br/> Copyright Clearance Center, Inc.","Chen, Yih-Kanq and Gokcen, Tahir and Edquist, Karl T.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Steamfront detection using time-lapse micro-gravity inversion","Objectives/Scope: PDO 4D gravity inversion is aimed at detecting lateral steam chest movement within a highly fractured carbonate oil reservoir. The PDO field uses Thermally Assisted Gas-Oil Gravity Drainage (TA-GOGD) method to produce the low API oil. The shallow reservoir rests at about 300 meters below the surface. Methods/Procedures/Process: We use regularized least-squares inversion code to invert for reservoir 4D density changes between base and repeat. Base gravity survey is carried out in 2013 and two repeat surveys in 2015 and 2016. Two methods of gravity surveying were deployed in 2013, relative and absolute gravity surveys, to acquire a total of 302 points per each method. Only relative surveys were acquired for both 2015 and 2016 repeat surveys. Relative surveys used a CG5 gravimeters, while the absolute gravity survey used the A10 Micro-g LaCoste specialized instrument to measure absolute gravity at each point. In this study the calculated 4D gravity signal is based on the difference between the 2013 Absolute base survey and the Relative repeats in 2015 and 2016. Gravity survey data is known to suffer from different types of noise, the highest of which is the gravity overprint caused by shallow water aquifer water level changes on the reservoir gravity signal, and secondly, any changes in surface elevation. Using water level monitoring wells we estimated shallow aquifer signal and removed from the observed gravity data values prior to inversion. Surface elevation changes are tracked using InSAR satellite data. Also, observed gravity data is pre-conditioned by removing outliers and de-bias the data to improve inversion stability. Results/Observations/Conclusions: After the removal of the shallow aquifer gravity signal, the observed 4D gravity anomaly is found to vary between 21&mu;Gal to -35 &mu;Gal between 2016-2013, which is relatively comparable with an estimated forward 4D gravity signal calculated from history matched reservoir models for the same periods of time (2013-2015 &amp; 2013-2016). The Least Squares inversion is carried out assuming a single reservoir layer of a certain (heated) thickness, inverting the 4D gravity signal to estimate the density changes at the reservoir for the periods 2013-2015 and 2013-2016. The inversion results, with the shallow aquifer compensated, do not show dramatic difference with the non compensated inversion. Assuming a thickness of the steamed/heated zone 100m, the estimated change in density varies between -29 kg/m3 to 12 kg/m3. We find the largest negative anomaly to correspond with the steam injection area. We concludefrom this work that 4D gravity signal is detectable, which is the result from reservoir density changes caused by steam injection. A steam chest outline can be estimated by applying a density cut-off. Also, we conclude from this study that surface elevation changes error contribution in this field is negligible, and a single elevation survey combined with time-lapse, InSAR satellite elevation changes is sufficient for both base and repeat survey.<br/> Copyright 2017, Society of Petroleum Engineers.","Al-Lazki, Ali and Al-Ismaili, Ali and Stammeijer, Johannes and Hamm, Mark",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Toward enhanced forming simulation of woven fabrics using a coupled non-orthogonal hypoelastic constitutive model, integrated with a new wrinkling onset criterion","One of the advantages of woven fabrics over unidirectional prepregs is their superior formability thanks to the large shear deformation capability. There exists, however, a limit on the shear deformation of woven fabrics, namely the wrinkling. Applying tension to delay wrinkling during forming processes, a consequence of the inherent coupling in woven fabrics, is widely known to the industry. Yet, inherent coupling - change in the effective material properties of a given direction of the fabric due to the applied deformation in other directions - has not been fully understood and implemented in the forming simulations of fabric reinforcements. Coupling should be incorporated in numerical optimization routines to accurately predict the deformation of the material under complex forming set-ups, and more importantly to predict a realistic yarn tension level that can suppress wrinkles. Towards this goal, the present study proposes and implements a new coupled non-orthogonal model which predicts not only the stress-strain path, but also the critical point (shear wrinkling) of the woven fabrics, under combined loading conditions similar to the draping processes. Furthermore, the study reveals that the concept of inherent coupling raises a new issue in fabric forming simulations; the load history dependency of the fabric. Accordingly, the constitutive model has been enhanced to a hypoelastic form to capture the load path dependency of the forming material. Finally, the constitutive model has been integrated with a newly developed analytical fabric instability criterion by authors to account for the occurrence of wrinkling, based on the fabric properties and the level of tension applied during forming. To show its application, the model has been implemented in ABAQUS via a UMAT code to predict the stress and strain fields as well as the onset of wrinkling under large shear deformations.<br/> &copy; 2017 by DEStech Publications, Inc.","Kashani, M. Haghi and Hejazi, M. and Hosseini, A. and Sassani, F. and Ko, F. and Milani, A.S.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The performance assessment of a multi-span, box girder reinforced concrete bridge with and without seismic isolation","This paper investigates earthquake performance of a multi-span, box girder R/C bridge with and without isolation. Bridges and viaducts are important structures since they must be functional immediately and remain stable after an earthquake. For this reason, seismic isolations can be used to decrease the effects of earthquake loads on bridges. Seismic isolation on a bridge elongates the period of structure, thus the inertia and the earthquake forces are decreased without a significant change in the stiffness of the structure. Among various types of seismic isolators used in engineering practice, lead rubber bearing (LRB) was used in this study. Performance assessment was applied to the un-isolated and isolated bridge to make some comparisons, and the benefits of isolation were discussed. Linear elastic method, nonlinear static method (pushover analysis) and nonlinear time history analysis, which are defined in Turkish Earthquake Code (TEC) 2007, were used for performance assessment. &copy; Civil-Comp Press, 2012.<br/>","Borekci, M. and Altun, M. and Arslan, G.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Long term, interrelated interventions to increase women's participation in STEM in the Netherlands","Traditionally, the Netherlands lag behind other countries in terms of the percentage of girls opting for Science, Technology, Engineering, Mathematics (STEM)-study programs (Eurostat, 2009; OECD, 2003). In international research a number of determinants for the under-representation of girls/women in STEM have been appointed, including girls? lower self-concepts, non-stimulating learning environments, lack of female role models, stereotyped associations in society about girls/women and STEM, fertility/lifestyle factors, and career preferences of girls and women (e.g., [1-3]). Since the early 1980s, VHTO (www.vhto.nl, WiTEC partner), has been building up knowledge and experience about the participation of girls and women in STEM. VHTO has been deploying this expertise in the whole chain of education ? from primary to higher education ? through the labour market in the Netherlands. In the course of time it has proved to be essential to commit interrelated interventions for a long period of time, instead of ad hoc or short term single actions.Implementation strategies are directed towards change in educational policy (i.e., awareness and gender inclusiveness), professional development for staff and teachers, career exploration opportunities and offering counter stereotypes (e.g., support of role models, Girls day and girls code events) at critical junctions/moments of choice in the girls? (school) careers, parents? involvement, and dissemination. Statistics show that in the course more girls (and boys) opted for STEM subjects and higher STEM education, especially in pre-university secondary schools. We will show how policies regarding the under-representation of girls and women in STEM (specifically in education) have developed over the years in the Netherlands and other countries. We will introduce a policy compass that can support institutions to choose appropriate policies and to set up an adequate action plan.<br/>","Booij, C. and Jansen, E.J.M. and Van Schaik, E.J.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Software engineering ethics education: Incorporating critical pedagogy into student outreach projects","The difficulties inherent in the nature of software as an intangible object pose problems for specifying its needs, predicting overall behavior or impact on users, and therefore on defining the ethical questions that are involved in software development. Whereas software engineering drew from older engineering disciplines for process and practice development, culminating in the IEEE/ACM Professional Code in 1999, the topic of Software Engineering Ethics is entwined with Computer Science, and developments in Computer and Information Ethics. Contemporary issues in engineering ethics such as globalization have raised questions for software engineers about computer crime, civil liberties, open access, digital divide, etc. Similarly, computer-related ethics is becoming increasingly important for engineering ethics because of the dominance of computers in modern engineering practice. This is not to say that software engineers should consider everything, but the diversity of ethical issues presents a challenge to the approach of accumulating resources that many ethicists maintain can be overcome by developing critical thinking skills as part of technical training courses. This chapter explores critical pedagogies in the context of student outreach activities such as service learning projects and considers their potential in broadening software engineering ethics education. The practical emphasis in critical pedagogy can allow students to link specific software design decisions and ethical positions, which can perhaps transform both student and teacher into persons more curious about their individual contribution to the public good and more conscious of their agency to change the conditions around them. After all, they share with everyone else a basic human desire to survive and flourish.<br/> &copy; 2015, IGI Global.","Kadoda, Gada",2015,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Ergodic Quotients in Analysis of Dynamical Systems","The analysis of dynamical systems typically taught in introductory graduate courses focuses on analysis of trajectories: curves in the state space, parametrized by time. While the trajectory-based approach is successful in analyzing exceptional solutions, e.g., stable and unstable manifolds attached to fixed points, it is a poor choice for recognizing coarse patterns in state spaces, e.g., vortices in fluid-like flows. An alternative is the operator-theoretic description of dynamical systems. This doctoral project studied the Koopman operator, which describes how functions, or observables, change as the dynamical system evolves in time. The main interest was in identification of invariant sets in the state space, where behavior of the system is uniform ''on average''. The arrangement of invariant sets is connected to the ergodic quotient: a subset of a sequence space where trajectories are described using values of invariant functions along them. By endowing the ergodic quotient with a Sobolev space metric structure, we are able to identify coherent sets: invariant subsets in the state space that possess a complete set of continuous invariant functions. These coherent sets correspond to continuous segments in the ergodic quotient. The ergodic quotient analysis is model-free: it requires only data obtained by simulation or through experiments. As a proof-of-concept, we implemented the algorithm as a numerical code that approximates the ergodic quotient using trajectory averages of a function basis on the state space. The geometry of the ergodic quotient is analyzed with help of a machine-learning algorithm, Diffusion Maps, which enables computational identification of continuous segments and, consequently, partitioning of the state space into coherent sets. To demonstrate effectiveness, several examples of dynamical systems were analyzed, ranging from iterated maps with mixed state spaces to forced fluid-like flows. The results obtained by the ergodic quotient analysis match the results of previous analyses of the systems. To conclude, further research directions are proposed, based on the results of the doctoral project. ProQuest Subject Headings: Mathematics, Mechanical engineering, Applied mathematics.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.","Budisic, Marko",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An analysis of the feasibility and benefits of GPU/multicore acceleration of the Weather Research and Forecasting model","There is a growing need for ever more accurate climate and weather simulations to be delivered in shorter timescales, in particular, to guard against severe weather events such as hurricanes and heavy rainfall. Due to climate change, the severity and frequency of such events - and thus the economic impact - are set to rise dramatically. Hardware acceleration using graphics processing units (GPUs) or Field-Programmable Gate Arrays (FPGAs) could potentially result in much reduced run times or higher accuracy simulations. In this paper, we present the results of a study of the Weather Research and Forecasting (WRF) model undertaken in order to assess if GPU and multicore acceleration of this type of numerical weather prediction (NWP) code is both feasible and worthwhile. The focus of this paper is on acceleration of code running on a single compute node through offloading of parts of the code to an accelerator such as a GPU. The governing equations set of the WRF model is based on the compressible, non-hydrostatic atmospheric motion with multi-physics processes. We put this work into context by discussing its more general applicability to multi-physics fluid dynamics codes: in many fluid dynamics codes, the numerical schemes of the advection terms are based on finite differences between neighboring cells, similar to the WRF code. For fluid systems including multi-physics processes, there are many calls to these advection routines. This class of numerical codes will benefit from hardware acceleration. We studied the performance of the original code of the WRF model and proposed a simple model for comparing multicore CPU and GPU performance. Based on the results of extensive profiling of representative WRF runs, we focused on the acceleration of the scalar advection module. We discuss the implementation of this module as a data-parallel kernel in both OpenCL and OpenMP. We show that our data-parallel kernel version of the scalar advection module runs up to seven times faster on the GPU compared with the original code on the CPU. However, as the data transfer cost between GPU and CPU is very high (as shown by our analysis), there is only a small speed-up (two times) for the fully integrated code. We show that it would be possible to offset the data transfer cost through GPU acceleration of a larger portion of the dynamics code. In order to carry out this research, we also developed an extensible software system for integrating OpenCL code into large Fortran code bases such as WRF. This is one of the main contributions of our work. We discuss the system to show how it allows the replacement of the sections of the original codebase with their OpenCL counterparts with minimal changes - literally only a few lines - to the original code. Our final assessment is that, even with the current system architectures, accelerating WRF - and hence also other, similar types of multi-physics fluid dynamics codes - with a factor of up to five times is definitely an achievable goal. Accelerating multi-physics fluid dynamics codes including NWP codes is vital for its application to weather forecasting, environmental pollution warning, and emergency response to the dispersion of hazardous materials. Implementing hardware acceleration capability for fluid dynamics and NWP codes is a prerequisite for up-to-date and future computer architectures.<br/> &copy; Copyright 2015 John Wiley & Sons, Ltd.","Vanderbauwhede, Wim and Takemi, Tetsuya",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Image segmentation through fuzzy clustering: A survey","In modern years, image processing is a vast area for research. Image segmentation is the most popular part of image processing which divides the image into number of segments to analyze the better quality of image. It is used to detect objects and boundaries in images. Main goal of image segmentation is to change the representation of image into the more meaningful regions. Image segmentation results in a set of segments that covers the whole image or curves that are extracted from the image. In this paper, different image segmentation techniques and algorithms are presented, and clustering is one of the techniques that is used for segmentation. Fuzzy c-means clustering (FCM) algorithm is presented in this paper for image segmentation. On the basis of literature reviewed, several problems are analyzed in previously FCM, and the problems have been overcome by modifying the objective function of the previously FCM, and spatial information is incorporated in objective function of FCM. Fuzzy c-mean clustering is also known as soft clustering. The techniques that are explained in this survey are segmentation of the noisy medicinal images along spatial probability, histogram-based FCM, improved version of fuzzy c-means (IFCM), fuzzy possibilistic c-means (FPCM), possibilistic c-means (PCM), and possibilistic fuzzy c-means (PFCM) algorithms are to be explained in further sections on the basis of literature review. Moreover, several recent works on fuzzy c-means using clustering till 2017 are presented in this survey.<br/> &copy; Springer Nature Singapore Pte Ltd. 2019.","Jain, Rashi and Sharma, Rama Shankar",2019,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"PARFUME modeling status update","The recent developments in PARFUME have increased the flexibility of the code as a fuel performance simulator. PARFUME now has the option to replace the SiC layer with ZrC to explore the effects of a possible material change. Also, PARFUME can capture the multidimensional behavior of different particle types and sizes over a broad range of both normal and accident conditions. Finally, it has provided a way to explore fuel particle behavior via response surfaces to identify operating conditions that may be adverse to fuel behavior to feed into larger system codes. Currently, PARFUME development is focused on the addition of a cylindrical geometry option similar to the prismatic fuel geometry option. In addition, an initially defective SiC model has been added to the code and is currently being validated. These models will be included in a future version of the code to assist in fuel performance modeling applications.<br/>","Skerjanc, William F. and Maki, John T. and Petti, David A.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An experience of using numerical methods in experimental investigations of TsAGI","Description of EWT (Electronic Wind Tunnel) code is done. This code is used in TsAGI from 1996. It permits to solve stationary (RANS) and non-stationary (URANS) Navier-Stokes equations with Reynolds-averaging. A possibility to simulate large-scale vortices (LES) is realized. Special boundary conditions, such as ""wind tunnel start"", ""permeable walls"" (perforated and slotted), ""threadmill"" and ""plenum chamber walls"" are discussed in details. It is mentioned that code effectively uses chimera-type grids based on original ""connect"" technology. Practical aspects are described. It is shown that grid templates with special blocks for model and wind tunnel parts are prepared in advance. It permits to change model in EWT operatively. Important algorithm for grid rebuilding, in the case of changing the model incidence and slip angles, is also developed and works reliable. Initially (1996), the code was adapted for conditions of T-128 wind tunnel (TsAGI). Later on (2006), the version for ETW (European Transonic Windtunnel) appeared. Special code for T-104 wind tunnel (TsAGI) with simulation of ""moving runway"" and open test section has been developed after that (2008). Examples of results obtained in the frame of code integration to experimental cycle of different wind tunnels are presented. Important tasks are solved: 1) wall interference prediction; 2) systematic mistakes such as support influence etc. estimation; 3) creation of optimal wind tunnel parts. Picture bellow shows result of penishe height problem solution during mounting the half model in T-128 TsAGI. Streamlines around ""free"" model and halfmodel in wind tunnel are compared.<br/>","Bosnyakov, S.M. and Gorbushin, A.R. and Mikhaylov, S.V. and Neyland, V. Ya",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A fast and efficient python library for interfacing with the Biological Magnetic Resonance Data Bank","Background: The Biological Magnetic Resonance Data Bank (BMRB) is a public repository of Nuclear Magnetic Resonance (NMR) spectroscopic data of biological macromolecules. It is an important resource for many researchers using NMR to study structural, biophysical, and biochemical properties of biological macromolecules. It is primarily maintained and accessed in a flat file ASCII format known as NMR-STAR. While the format is human readable, the size of most BMRB entries makes computer readability and explicit representation a practical requirement for almost any rigorous systematic analysis. Results: To aid in the use of this public resource, we have developed a package called nmrstarlib in the popular open-source programming language Python. The nmrstarlib's implementation is very efficient, both in design and execution. The library has facilities for reading and writing both NMR-STAR version 2.1 and 3.1 formatted files, parsing them into usable Python dictionary- and list-based data structures, making access and manipulation of the experimental data very natural within Python programs (i.e. ""saveframe"" and ""loop"" records represented as individual Python dictionary data structures). Another major advantage of this design is that data stored in original NMR-STAR can be easily converted into its equivalent JavaScript Object Notation (JSON) format, a lightweight data interchange format, facilitating data access and manipulation using Python and any other programming language that implements a JSON parser/generator (i.e., all popular programming languages). We have also developed tools to visualize assigned chemical shift values and to convert between NMR-STAR and JSONized NMR-STAR formatted files. Full API Reference Documentation, User Guide and Tutorial with code examples are also available. We have tested this new library on all current BMRB entries: 100% of all entries are parsed without any errors for both NMR-STAR version 2.1 and version 3.1 formatted files. We also compared our software to three currently available Python libraries for parsing NMR-STAR formatted files: PyStarLib, NMRPyStar, and PyNMRSTAR. Conclusions: The nmrstarlib package is a simple, fast, and efficient library for accessing data from the BMRB. The library provides an intuitive dictionary-based interface with which Python programs can read, edit, and write NMR-STAR formatted files and their equivalent JSONized NMR-STAR files. The nmrstarlib package can be used as a library for accessing and manipulating data stored in NMR-STAR files and as a command-line tool to convert from NMR-STAR file format into its equivalent JSON file format and vice versa, and to visualize chemical shift values. Furthermore, the nmrstarlib implementation provides a guide for effectively JSONizing other older scientific formats, improving the FAIRness of data in these formats.<br/> &copy; 2017 The Author(s).","Smelter, Andrey and Astra, Morgan and Moseley, Hunter N.B.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Transient, spatially varied groundwater recharge modeling","The objective of this work is to integrate field data and modeling tools in producing temporally and spatially varying groundwater recharge in a pilot watershed in North Okanagan, Canada. The recharge modeling is undertaken by using the Richards equation based finite element code (HYDRUS-1D), ArcGIS&trade;, ROSETTA, in situ observations of soil temperature and soil moisture, and a long-term gridded climate data. The public version of HYDUS-1D and another version with detailed freezing and thawing module are first used to simulate soil temperature, snow pack, and soil moisture over a one year experimental period. Statistical analysis of the results show both versions of HYDRUS-1D reproduce observed variables to the same degree. After evaluating model performance using field data and ROSETTA derived soil hydraulic parameters, the HYDRUS-1D code is coupled with ArcGIS&trade; to produce spatially and temporally varying recharge maps throughout the Deep Creek watershed. Temporal and spatial analysis of 25 years daily recharge results at various representative points across the study watershed reveal significant temporal and spatial variations; average recharge estimated at 77.8 &plusmn; 50.8 mm/year. Previous studies in the Okanagan Basin used Hydrologic Evaluation of Landfill Performance without any attempt of model performance evaluation, notwithstanding its inherent limitations. Thus, climate change impact results from this previous study and similar others, such as Jyrkama and Sykes (2007), need to be interpreted with caution. &copy;2013. American Geophysical Union. All Rights Reserved.<br/>","Assefa, Kibreab Amare and Woodbury, Allan D.",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An extensible, multi-model software library for simulating crop growth and development","The availability of different crop models and of a variety of techniques to evaluate their behaviour led to a change of paradigm in crop models use. Modellers are now looking beyond the idea of groups of users and developers grounded on a specific model, and international initiatives focusing on model improvement basing on intercomparison and knowledge sharing are recently catalysing the attention of the international community. Also, the analysis under environmental conditions of no adaptation by crops, such as the ones of climate change scenarios, demand for extension of crop models to account for extreme events, diseases and pests impact, difficult to implement into legacy code. The Crop Models Library (CropML) is a framework-independent MS .NET software component where different pure (e.g., WOFOST, CropSyst, WARM), hybrid and new modelling solutions for crop growth and development are implemented following a fine level of granularity, according to a high level software architecture. CropML can be extended by third parties and is distributed at no cost with a software development kit, including documentation of code and algorithms and sample applications. CropML provides modellers with an environment favouring the hybridization of models with parts from others, the evolution of existing approaches, and the possibility of analysing and easily comparing diverse modelling solutions. As an example, a new generation of SUCROS-type models has been developed and included in the component. Comparison of the standard and of the new version of the WOFOST model carried out using data from rice field experiments revealed an increase in accuracy and robustness with less than half of the parameters used by the standard version of the model. These results support the idea that high-level technology for models formalization can favour the development of the models themselves.<br/>","Confalonieri, R. and Bregaglio, S. and Stella, T. and Negrini, G. and Acutis, M. and Donatelli, M.",2012,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Cross-Layer Software Dependability on Unreliable Hardware","To enable reliable embedded systems, it is imperative to leverage the compiler and system software for joint optimization of functional correctness (i.e., vulnerability indexes) and timing correctness (i.e., deadline misses). This paper considers the optimization of the reliability-timing (RT) penalty, defined as a linear combination of the vulnerability and deadline misses. We propose a cross-layer approach to achieve reliable code generation and execution at compilation and system software layers for embedded systems. This is enabled by the concept of generating multiple versions for given application functions, with diverse performance and reliability tradeoffs, by exploiting different reliability-guided compilation options. As the execution time of a function is not fixed, the selection of the versions depends upon the execution behavior of the previous functions. Based on the reliability and execution time profiling of these versions, our reliability-driven system software decides the prioritization of the functions for determining their execution order and employs dynamic version selection to dynamically select a suitable version of a function. Specifically, our scheme builds a schedule table offline to optimize the RT penalty, and uses this table at run time to select suitable versions for the subsequent functions. A complex real-world application of 'secure video and audio processing' composed of various functions is evaluated for reliable code generation and execution.<br/> &copy; 1968-2012 IEEE.","Rehman, Semeen and Chen, Kuan-Hsun and Kriebel, Florian and Toma, Anas and Shafique, Muhammad and Chen, Jian-Jia and Henkel, Jorg",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Exploiting Earth observation data pools for urban analysis: The TEP URBAN project","Large amounts of Earth observation (EO) data have been collected to date, to increase even more rapidly with the upcoming Sentinel data. All this data contains unprecedented information, yet it is hard to retrieve, especially for nonremote sensing specialists. As we live in an urban era, with more than 50% of the world population living in cities, urban studies can especially benefit from the EO data. Information is needed for sustainable development of cities, for the understanding of urban growth patterns or for studying the threats of natural hazards or climate change. Bridging this gap between the technology-driven EO sector and the information needs of environmental science, planning, and policy is the driver behind the TEP-Urban project. Modern information technology functionalities and services are tested and implemented in the Urban Thematic Exploitation Platform (U-TEP). The platform enables interested users to easily exploit and generate thematic information on the status and development of the environment based on EO data and technologies. The beta version of the web platform contains value added basic earth observation data, global thematic data sets, and tools to derive user specific indicators and metrics. The code is open source and the architecture of the platform allows adding of new data sets and tools. These functionalities and concepts support the four basic use scenarios of the U-TEP platform: explore existing thematic content; task individual on-demand analyses; develop, deploy and offer your own content or application; and, learn more about innovative data sets and methods.<br/> &copy; 2017 SPIE.","Heldens, W. and Esch, T. and Asamer, H. and Boettcher, M. and Brito, F. and Hirner, A. and Marconcini, M. and Mathot, E. and Metz, A. and Permana, H. and Zeidler, J. and Balhar, J. and Kuchar, S. and Soukop, T. and Stankek, F.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Mapping afforestation and forest biomass using time-series Landsat stacks","Satellite data can adequately capture forest dynamics over larger areas. Firstly, the Landsat ground surface reflectance (GSR) images from 1974 to 2013 were collected and processed based on 6S atmospheric transfer code and a relative reflectance normalization algorithm. Subsequently, we developed a vegetation change tracking method to reconstruct the forest change history (afforestation and deforestation) from the dense time-series Landsat GSR images, and the afforestation age was successfully retrieved from the Landsat time-series stacks in the last forty years and shown to be consistent with the surveyed tree ages. Then, the above ground biomass (AGB) regression models were greatly improved by integrating the simple ratio vegetation index (SR) and tree age. Finally, the forest AGB images were mapped at eight epochs from 1985 to 2013 using SR and afforestation age. The total forest AGB in six counties of Yulin District increased by 20.8 G kg, from 5.8 G kg in 1986 to 26.6 G kg in 2013, a total increase of 360%. For the forest area, the forest AGB density increased from 15.72 t/ha in 1986 to 44.53 t/ha in 2013, with an annual rate of about 1 t/ha. The results present a noticeable carbon increment for the planted artificial forest in Yulin District over the last four decades.<br/> &copy; 2014 SPIE.","Liu, Liangyun and Peng, Dailiang and Wang, Zhihui and Hu, Yong",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Automating Live Update for Generic Server Programs","The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage the adoption of live update. This paper presents Mutable Checkpoint-Restart (MCR), a new live update solution for generic (multiprocess and multithreaded) server programs written in C. Compared to prior solutions, MCR can support arbitrary software updates and automate most of the common live update operations. The key idea is to allow the running version to safely reach a quiescent state and then allow the new version to restart as similarly to a fresh program initialization as possible, relying on existing code paths to automatically restore the old program threads and reinitialize a relevant portion of the program data structures. To transfer the remaining data structures, MCR relies on a combination of precise and conservative garbage collection techniques to trace all the global pointers and apply the required state transformations on the fly. Experimental results on popular server programs (Apache httpd, nginx, OpenSSH and vsftpd) confirm that our techniques can effectively automate problems previously deemed difficult at the cost of negligible performance overhead (2 percent on average) and moderate memory overhead (3.9\times on average, without optimizations).<br/> &copy; 1976-2012 IEEE.","Giuffrida, Cristiano and Iorgulescu, Clin and Tamburrelli, Giordano and Tanenbaum, Andrew S.",2017,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Scaling agile mechatronics: An industrial case study","The automotive industry is currently in a state of rapid change. The traditional mechanical industry has, forced by electronic revolution and global threats of climate change, transformed into a computerized electromechanical industry. A hybrid or electric car of 2013 can have, in the order of 100 electronic control units, running gigabytes of code, working together in a complex network within the car as well as being connected to networks in the world outside. This exponential increase of software has posed new challenges for the R&amp;D organizations. In many cases the commonly used method of requirement engineering towards external suppliers in a waterfall process has shown to be unmanageable. Part of the solution has been to introduce more in-house software development and the new standardized platform for embedded software, AUTOSAR. During the past few years, Volvo Cars has focused on techniques and processes for continuous integration of embedded software for active safety, body functions, and motor and hybrid technology. The feedback times for ECU system test have decreased from months to, in the best cases, hours. Domain-specific languages (DSL), for both software and physical models, have been used to great extent when developing in-house embedded software at Volvo Cars. The main reasons are the close connection with mechatronic systems (motors, powertrain, servos, etc.), the advantage of having domain experts (not necessarily software experts) developing control software, and the facilitated reuse of algorithms. Model-driven engineering also provides a method for agile development and early learning in projects where hardware and mechanics usually are available only late. Model-based testing of the software is performed, both as pure simulation (MIL) and in hardware-in-the-loop (HIL) rigs, before it is deployed in real cars. This testing is currently being automated for several rigs, as part of the continuous integration strategy. The progress is, however, not without challenges. Details of the work split with Tier 1 suppliers, using the young AUTOSAR standard, and the efficiency of AUTOSAR code are still open problems. Another challenge is to manage the complex model framework required for virtual verification when applied on system level and numerous DSLs have to be executed together.<br/> &copy; 2014 Springer International Publishing Switzerland. All rights reserved.","Lantz, Jonn and Eliasson, Ulf",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A common backend for hardware acceleration on FPGA","Field Programmable Gate Arrays (FPGAs) are configurable integrated circuits able to provide a good trade-off in terms of performance, power consumption, and flexibility with respect to other architectures, like CPUs, GPUs and ASICs. The main drawback in using FPGAs, however, is their steep learning curve. An emerging solution to this problem is to write algorithms in a Domain Specific Language (DSL) and to let the DSL compiler generate efficient code targeting FPGAs. This work proposes FROST, a unified backend that enables different DSL compilers to target FPGA architectures. Differently from other code generation frameworks targeting FPGA, FROST exploits a scheduling co-language that enables users to have full control over which optimizations to apply in order to generate efficient code (e.g. loop pipelining, array partitioning, vectorization). At first, FROST analyzes and manipulates the input Abstract Syntax Tree (AST) in order to apply FPGA-oriented transformations and optimizations, then generates a C/C++ implementation suitable for High-Level Synthesis (HLS) tools. Finally, the output of HLS phase is synthesized and implemented on the target FPGA using Xilinx SDAccel toolchain. The experimental results show a speedup up of 15 with respect to O3-optimized implementations of the same algorithms on CPU.<br/> &copy; 2017 IEEE.","Del Sozzo, Emanuele and Baghdadi, Riyadh and Amarasinghe, Saman and Santambrogio, Marco D.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Reusable Specification Templates for Defining Dynamic Semantics of DSLs","In the context of Model Driven Engineering (MDE), the dynamic (execution) semantics of domain specific languages (DSLs) is usually not specified explicitly and stays (hard)coded in model transformations and code generation. This poses challenges such as learning, debugging, understanding, maintaining, and updating a DSL. Facing the lack of supporting tools for specifying the dynamic semantics of DSLs (or programming languages in general), we propose to specify the architecture and the detailed design of the software that implements the DSL, rather than requirements for the behavior expected from DSL programs. To compose such a specification we use specification templates that capture software design solutions typical for the (application) domain of the DSL. As a result, on the one hand, our approach allows for an explicit and clear definition of the dynamic semantics of a DSL, supports separation of concerns and reuse of typical design solutions. On the other hand, we do not introduce (yet another) specification formalism, but we base our approach on an existing formalism and apply its extensive tool support for verification and validation to the dynamic semantics of a DSL.<br/> &copy; 2017 IEEE.","Tikhonova, Ulyana",2017,"[""Engineering Village""]","Rejeitado: CR9, CR8","Rejeitado: CR9"
"Towards automatic complex feature engineering","Feature engineering is one of the most difficult and time-consuming tasks in data mining projects, and requires strong expert knowledge. Existing feature engineering techniques tend to use limited numbers of simple feature transformation methods and validate on simple datasets (small volume, simple structure), obviously limiting the benefits of feature engineering. In this paper, we propose a general Automatic Feature Engineering Machine framework (AFEM for short), which defines families of complex features and introduces them one family at a time (block bottom-up). We show that this framework covers most of the existing features used in the literature and allows us to efficiently generate complex feature families: in particular, local time, social network and representation-based families for relational and graph datasets, as well as composition of features. We validate our approach on two large realistic competitions datasets and a recommendation system task with social network. In the first two tasks, AFEM automatically reached ranks 15 and 12 compared to human teams; in the last task, it achieved 1.5% regression error reduction, compared to best results in the literature. Furthermore, in the context of big data and web applications, by balancing computation time and number of features/performance, in one case, we could reduce 2/3 computation time with only 0.2% AUC performance loss. Our code is publicly available on GitHub (https://github.com/TjuJianyu/AFEM).<br/> &copy; Springer Nature Switzerland AG 2018.","Zhang, Jianyu and Fogelman-Soulie, Francoise and Largeron, Christine",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A meta-model for DEVS: Designed following Model Driven Engineering specifications","In this paper we give a state-of-art of DEVS components interoperability, and we propose a meta-model for classic DEVS formalism, designed following a Model-Driven Engineering philosophy. After glancing at the existing related works, we explain in a step-by-step way how our meta-model is built, starting from the formal definition of DEVS formalism. As the hardest steps when defining a DEVS Platform-Independent Model (PIM) are the definition of the states and the definition of the DEVS functions, we particularly focus on those concepts and we propose a way to describe them in a simple and platform-independent way. UML class diagrams were chosen to represent this meta-model. Not only can this meta-model be useful to generate DEVS PIMs but it can also be seen as a powerful tool to improve interoperability between DEVS models (and in a larger way discrete-event models, via model-to-model transformations) and to provide automatic code generation towards DEVS simulators (model-to-text transformations). As this meta-model is not a final version but rather a starting point, we tried to make it as modular and upgradable as possible.<br/>","Garredu, Stephane and Vittori, Evelyne and Santucci, Jean-Francois and Bisgambiglia, Paul-Antoine",2012,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Management of the quality of teaching at Universities - A course for teachers at the Technical University of Kosice developed within the operational programme 'Education'","The aim of this contribution is to provide the basic information concerning the course 'Management of the Quality of Teaching at Universities'. This course was prepared within the framework of the operational programme Education 'Package of Innovative Elements for the Transformation of Education at TUKE' (code ITMS 26110230018). In recent weeks we finished the implementation of the pilot course that was focused to the development of professional skills of teachers and to the enhancement of quality education at TUKE (Technical University of Kosice). &copy; 2012 IEEE.<br/>","Blasko, Michal and Raschman, Pavel",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Complex inference in neural circuits with probabilistic population codes and topic models","Recent experiments have demonstrated that humans and animals typically reason probabilistically about their environment. This ability requires a neural code that represents probability distributions and neural circuits that are capable of implementing the operations of probabilistic inference. The proposed probabilistic population coding (PPC) framework provides a statistically efficient neural representation of probability distributions that is both broadly consistent with physiological measurements and capable of implementing some of the basic operations of probabilistic inference in a biologically plausible way. However, these experiments and the corresponding neural models have largely focused on simple (tractable) probabilistic computations such as cue combination, coordinate transformations, and decision making. As a result it remains unclear how to generalize this framework to more complex probabilistic computations. Here we address this short coming by showing that a very general approximate inference algorithm known as Variational Bayesian Expectation Maximization can be naturally implemented within the linear PPC framework. We apply this approach to a generic problem faced by any given layer of cortex, namely the identification of latent causes of complex mixtures of spikes. We identify a formal equivalent between this spike pattern demixing problem and topic models used for document classification, in particular Latent Dirichlet Allocation (LDA). We then construct a neural network implementation of variational inference and learning for LDA that utilizes a linear PPC. This network relies critically on two non-linear operations: divisive normalization and super-linear facilitation, both of which are ubiquitously observed in neural circuits. We also demonstrate how online learning can be achieved using a variation of Hebb's rule and describe an extension of this work which allows us to deal with time varying and correlated latent causes.<br/>","Beck, Jeff and Heller, Katherine and Pouget, Alexandre",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Are social network sites the future of engineering design education?","This paper presents how online social network sites (SNSs) are being used by students in distributed engineering design teams to support design activities; and its implications for the future of design education. Ethnographic studies of a Global Design Project (GDP) were conducted from 2015-2017 to collect information on the growing use of SNSs by students. Team diaries were kept, systematically recording observations, and students reported their personal experiences in reports. Nvivo 11 was utilised to code data and make conclusions on team&rsquo;s collaborative behaviour, and their successes and failures with the technologies used. This study has revealed that students of the GDP have made a change in the way they collaborate by means of SNSs. Evidence shows that students are able to utilise the functionality of SNSs to support the design process, design activities and design thinking. The growth of SNSs within academia and industry suggest that students will need to utilise the technology or at least the functionalities of SNSs in the future. It is important to question how future engineering design education might be delivered and how social network site functionality can be best used.<br/> &copy; 2018 Institution of Engineering Designers The Design Society. All Rights Reserved.","Brisco, Ross and Whitfield, Robert and Grierson, Hilary",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Survey of static software defect prediction","Static software defect prediction is an active research topic in the domain of software engineering data mining. The phases of the study include designing novel code or process metrics to characterize the faults in the program modules, constructing software defect prediction model based on the training data gathered after mining software historical repositories, using the trained model to predict potential defect-proneness of program modules. The research on software defect prediction can optimize the allocation of testing resources and improve the quality of software. This paper offers a systematic survey of existing research achievements of the domestic and foreign researchers in recent years. First, a research framework is proposed and three key factors (i.e., metrics, model construction approaches, and issues in datasets) influencing the performance of defect prediction are identified. Next, existing research achievements in these three key factors are discussed in sequence. Then, the existing achievements on a special defect prediction issues (i.e., code change based defect prediction) are summarized. Finally a perspective of the future work in this research area is discussed.<br/> &copy; Copyright 2016, Institute of Software, the Chinese Academy of Sciences. All rights reserved.","Chen, Xiang and Gu, Qing and Liu, Wang-Shu and Liu, Shu-Long and Ni, Chao",2016,"[""Engineering Village""]","Rejeitado: CR7","Rejeitado: CR7"
"Multi-feature extraction and selection in writer-independent off-line signature verification","Some of the fundamental problems faced in the design of signature verification (SV) systems include the potentially large number of input features and users, the limited number of reference signatures for training, the high intra-personal variability among signatures, and the lack of forgeries as counterexamples. In this paper, a new approach for feature selection is proposed for writer-independent (WI) off-line SV. First, one or more preexisting techniques are employed to extract features at different scales. Multiple feature extraction increases the diversity of information produced from signature images, allowing to produce signature representations that mitigate intra-personal variability. Dichotomy transformation is then applied in the resulting feature space to allow for WI classification. This alleviates the challenges of designing off-line SV systems with a limited number of reference signatures from a large number of users. Finally, boosting feature selection is used to design low-cost classifiers that automatically select relevant features while training. Using this global WI feature selection approach allows to explore and select from large feature sets based on knowledge of a population of users. Experiments performed with real-world SV data comprised of random, simple, and skilled forgeries indicate that the proposed approach provides a high level of performance when extended shadow code and directional probability density function features are extracted at multiple scales. Comparing simulation results to those of off-line SV systems found in literature confirms the viability of the new approach, even when few reference signatures are available. Moreover, it provides an efficient framework for designing a wide range of biometric systems from limited samples with few or no counterexamples, but where new training samples emerge during operations. &copy; 2011 Springer-Verlag.<br/>","Rivard, Dominique and Granger, Eric and Sabourin, Robert",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Evaluating the conventional wisdom in clone removal: A genealogy-based empirical study","Clone management has drawn immense interest from the research community in recent years. It is recognized that a deep understanding of how code clones change and are refactored is necessary for devising effective clone management tools and techniques. This paper presents an empirical study based on the clone genealogies from a significant number of releases of six software systems, to characterize the patterns of clone change and removal in evolving software systems. With a blend of qualitative analysis, quantitative analysis and statistical tests of significance, we address a number of research questions. Our findings reveal insights into the removal of individual clone fragments and provide empirical evidence in support of conventional clone evolution wisdom. The results can be used to devise informed clone management tools and techniques. Copyright 2013 ACM.<br/>","Zibran, Minhaz F. and Saha, Ripon K. and Roy, Chanchal K. and Schneider, Kevin A.",2013,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Quantifying rates of landscape evolution and tectonic processes by thermochronology and numerical modeling of crustal heat transport using PECUBE","PECUBE is a three-dimensional thermal-kinematic code capable of solving the heat production-diffusion-advection equation under a temporally varying surface boundary condition. It was initially developed to assess the effects of time-varying surface topography (relief) on low-temperature thermochronological datasets. Thermochronometric ages are predicted by tracking the time-temperature histories of rock-particles ending up at the surface and by combining these with various age-prediction models. In the decade since its inception, the PECUBE code has been under continuous development as its use became wider and addressed different tectonic-geomorphic problems. This paper describes several major recent improvements in the code, including its integration with an inverse-modeling package based on the Neighborhood Algorithm, the incorporation of fault-controlled kinematics, several different ways to address topographic and drainage change through time, the ability to predict subsurface (tunnel or borehole) data, prediction of detrital thermochronology data and a method to compare these with observations, and the coupling with landscape-evolution (or surface-process) models. Each new development is described together with one or several applications, so that the reader and potential user can clearly assess and make use of the capabilities of PECUBE. We end with describing some developments that are currently underway or should take place in the foreseeable future. &copy; 2012 Elsevier B.V.<br/>","Braun, Jean and van der Beek, Peter and Valla, Pierre and Robert, Xavier and Herman, Frederic and Glotzbach, Christoph and Pedersen, Vivi and Perry, Claire and Simon-Labric, Thibaud and Prigent, Cecile",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Lunar micro rover design for exploration through virtual reality tele-operation","A micro rover, code-named Moonraker, was developed to demonstrate the feasibility of 10kg-class lunar rover missions. Requirements were established based on the Google Lunar X-Prize mission guidelines in order to effectively evaluate the prototype. A 4-wheel skid steer configuration was determined to be effective to reduce mass, maximize regolith traversability, and fit within realistic restrictions on the rover&rsquo;s envelope by utilizing the top corners of the volume. A static, hyperbolic mirror-based omnidirectional camera was selected in order to provide full 360<sup>&deg;</sup>views around the rover, eliminating the need for a pan/tilt mechanism and motors. A front mounted, motorless MEMS laser scanner was selected for similar mass reduction qualities. A virtual reality interface is used to allow one operator to intuitively change focus between various narrow targets of interest within the wide set of fused data available from these sensors. Lab tests were conducted on the mobility system, as well as field tests at three locations in Japan and Mauna Kea. Moonraker was successfully teleoperated to travel over 900m up and down a peak with slopes of up to 15&deg;. These tests demonstrate the rover&rsquo;s capability to traverse across lunar regolith and gather sufficient data for effective situational awareness and near real-time tele-operation.<br/> &copy; Springer International Publishing Switzerland 2015.","Britton, Nathan and Yoshida, Kazuya and Walker, John and Nagatani, Keiji and Taylor, Graeme and Dauphin, Loic",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Developing a summer bridge course for improving retention in engineering","This paper outlines the details of a summer bridge, project-based, cooperative, introduction to engineering pilot course developed and successfully implemented at Spelman College in an effort to increase the retention rate of students to be enrolled in its dual-degree engineering program. The course aims to expose incoming students of any STEM discipline to a broad array of practical and theoretical engineering principles for the purpose of helping students make informed decisions about pursuing engineering as a study major prior to the start of their freshman year. To satisfy this objective, the cross-disciplinary course that was developed is based on completing a software-driven, electro-mechanical engineering project that, at various times and to various extents, calls upon students to function in the capacity of an electrical engineer, a mechanical engineer, a technician, a mathematician, a computer scientist, a researcher and a communicator of technical material. In so doing, the students gain insight about how engineers combine knowledge from these diverse disciplines to solve a real problem-in this case, constructing and characterizing a 2-DOF, servoed laser system used to trace arbitrary patterns against a wall. Using an ""inverted curriculum"" approach that by-passes the first two years of the classic engineering curriculum, the course immerses the students directly into an engineering design project in an attempt to capture, as closely as possible, the end-goal of engineering training while providing a window to the challenges and gratifications of engineering both in practice and in an R&amp;D setting. It was observed that, far from having the feared effect of driving the students to disinterest, challenging the students with a difficult curriculum of technical concepts to be used to solve a non-trivial but well-defined and tangible problem elicited high interest and thoughtful evaluations and re-evaluations of engineering as a study major. Details of the course, which involves building a circuit from a schematic, developing code for a multi-core microcontroller, learning and applying the concept of pulse-width-modulation to control servo motors and developing the required mathematical coordinate transformations to successfully control the orientation of the laser are discussed. &copy; American Society for Engineering Education, 2013.<br/>","Volcy, Jerry and Sidbury, Carmen",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Metalearning to choose the level of analysis in nested data: A case study on error detection in foreign trade statistics","Traditionally, a single model is developed for a data mining task. As more data is being collected at a more detailed level, organizations are becoming more interested in having specific models for distinct parts of data (e.g. customer segments). From the business perspective, data can be divided naturally into different dimensions. Each of these dimensions is usually hierarchically organized (e.g. country, city, zip code), which means that, when developing a model for a given part of the problem (e.g. a zip code) the training data may be collected at different levels of this nested hierarchy (e.g. the same zip code, the city and the country it is located in). Selecting different levels of granularity may change the performance of the whole process, so the question is which level to use for a given part. We propose a metalearning model which recommends a level of granularity for the training data to learn the model that is expected to obtain the best performance. We apply decision tree and random forest algorithms for metalearning. At the base level, our experiment uses results obtained by outlier detection methods on the problem of detecting errors in foreign trade transactions. The results show that using metalearning help finding the best level of granularity.<br/> &copy; 2015 IEEE.","Zarmehri, Mohammad Nozari and Soares, Carlos",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Automatic improvement of graph based image segmentation","Automatic Design of Algorithms through Evolution (ADATE) is a system for fully automatic programming that has the ability to either generate algorithms from scratch or improve existing ones. In this paper, we employ ADATE to improve a standard image processing algorithm, namely graph based segmentation (GBS), which has emerged as one of the very most popular methods for image segmentation, that is partitioning an image into regions. The key contribution of the paper is to show that a proven and well-known computer vision code is easy to improve through automatic programming. This may presage a change to the entire field of computer vision where automatic programming becomes a routine way of improving standard as well as state-of-the art image processing and pattern analysis algorithms. GBS was mostly chosen as case study to investigate how useful the ADATE automatic programming system may be in computer vision. Numerous other algorithms in the field could have been chosen instead. &copy; 2012 Springer-Verlag.<br/>","Vu, Huyen and Olsson, Roland",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The effects of climate model similarity on probabilistic climate projections and the implications for local, risk-based adaptation planning","Approaches for probability density function (pdf) development of future climate often assume that different climate models provide independent information, despite model similarities that stem from a common genealogy (models with shared code or developed at the same institution). Here we use an ensemble of projections from the Coupled Model Intercomparison Project Phase 5 to develop probabilistic climate information, with and without an accounting of intermodel correlations, for seven regions across the United States. We then use the pdfs to estimate midcentury climate-related risks to a water utility in one of the regions. We show that the variance of climate changes is underestimated across all regions if model correlations are ignored, and in some cases, the mean change shifts as well. When coupled with impact models of the hydrology and infrastructure of a water utility, the underestimated likelihood of large climate changes significantly alters the quantification of risk for water shortages by midcentury.<br/> &copy; 2015. American Geophysical Union. All Rights Reserved.","Steinschneider, Scott and McCrary, Rachel and Mearns, Linda O. and Brown, Casey",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Empowering Professional Teaching in Engineering: Sustaining the Scholarship of Teaching","Each one of us has views about education, how discipline should function, how individuals learn, how they should be motivated, what intelligence is, and the structures (content and subjects) of the curriculum. Perhaps the most important beliefs that (beginning) teachers bring with them are their notions about what constitutes ""good teaching"". The scholarship of teaching requires that (beginning) teachers should examine (evaluate) these views in the light of knowledge currently available about the curriculum and instruction, and decide their future actions on the basis of that analysis. Such evaluations are best undertaken when classrooms are treated as laboratories of inquiry (research) where teachers establish what works best for them. Two instructor centred and two learner centred philosophies of knowledge, curriculum and instruction are used to discern the fundamental (basic) questions that engineering educators should answer in respect of their own beliefs and practice. They point to a series of classroom activities that will enable them to challenge their own beliefs, and at the same time affirm, develop, or change their philosophies of knowledge, curriculum and instruction.<br/> Copyright &copy; 2018 by Morgan & Claypool.","Heywood, John",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Tracing your maintenance work - A cross-project validation of an automated classification dictionary for commit messages","A commit message is a description of a change in a Version Control System (VCS). Besides the actual description of the change, it can also serve as an indicator for the purpose of the change, e.g. a change to refactor code might be accompanied by a commit message in the form of ""Refactored class XY to improve readability"". We would label the change in our example a perfective change, according to maintenance literature. This simplified example shows how it is possible to classify a change by its commit message. However, commit messages are unstructured, textual data and efforts to automatically label changes into categories like perfective have only been applied to a small set of projects within the same company or the same community. In this work, we present a cross-project evaluated and valid mapping of changes to the code base and their purpose that is usable without any customization on any open-source project. We provide further the Eclipse Plug-In Subcat which allows for a comfortable analysis of projects from within Eclipse. By using Subcat, we are able to automatically assess if a commit to the code was e.g. a bug fix or a refactoring. This information is very useful for e.g. developer profiling or locating bad smells in modules. &copy; 2012 Springer-Verlag Berlin Heidelberg.<br/>","Mauczka, Andreas and Huber, Markus and Schanes, Christian and Schramm, Wolfgang and Bernhart, Mario and Grechenig, Thomas",2012,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Tracing your maintenance work  a cross-project validation of an automated classification dictionary for commit messages","A commit message is a description of a change in a Version Control System (VCS). Besides the actual description of the change, it can also serve as an indicator for the purpose of the change, e.g. a change to refactor code might be accompanied by a commit message in the form of &ldquo;Refactored class XY to improve readability&rdquo;. We would label the change in our example a perfective change, according to maintenance literature. This simplified example shows how it is possible to classify a change by its commit message. However, commit messages are unstructured, textual data and efforts to automatically label changes into categories like perfective have only been applied to a small set of projects within the same company or the same community. In this work, we present a cross-project evaluated and valid mapping of changes to the code base and their purpose that is usable without any customization on any open-source project. We provide further the Eclipse Plug-In Subcat which allows for a comfortable analysis of projects from within Eclipse. By using Subcat, we are able to automatically assess if a commit to the code was e.g. a bug fix or a refactoring. This information is very useful for e.g. developer profiling or locating bad smells in modules.<br/> &copy; Springer-Verlag Berlin Heidelberg 2012.","Mauczka, Andreas and Huber, Markus and Schanes, Christian and Schramm, Wolfgang and Bernhart, Mario and Grechenig, Thomas",2012,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"Customized performance evaluation approach for Indian green buildings","The green building movement in India is lacking an important link: ensuring that the design intent of such buildings is actually realized. This paper undertakes an exploratory investigation to develop and test a customized building performance evaluation (BPE) approach (I-BPE framework) for the Indian context. As academia is considered to be an initial primary outlet of BPE, a survey of experts is conducted to investigate the drivers and barriers for implementing BPE-based methods in educational curricula. The I-BPE approach is tested in a case study building to gain insights for refining the underlying methods and processes for conducting further BPE studies in the context of India. The expert survey reveals the lack of trained people for teaching BPE as a key challenge to its adoption, implying that trained people are needed as much as frameworks. To enable widespread adoption of I-BPE in India, what will be necessary is a new cadre of building performance evaluators who can be trained (or up-skilled) through formal or continuing education. This will need to be driven by both policy (energy code) and market transformation (&lsquo;green&rsquo; rating systems). A series of delivery routes are suggested to enable rapid and deeper learning.<br/> &copy; 2018, &copy; 2018 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Gupta, Rajat and Gregg, Matt and Manu, Sanyogita and Vaidya, Prasad and Dixit, Maaz",2019,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Impact of compiler phase ordering when targeting GPUs","Research in compiler pass phase ordering (i.e., selection of compiler analysis/transformation passes and their order of execution) has been mostly performed in the context of CPUs and, in a small number of cases, FPGAs. In this paper we present experiments regarding compiler pass phase ordering specialization of OpenCL kernels targeting NVIDIA GPUs using Clang/LLVM 3.9 and the libclc OpenCL library. More specifically, we analyze the impact of using specialized compiler phase orders on the performance of 15 PolyBench/GPU OpenCL benchmarks. In addition, we analyze the final NVIDIA PTX assembly code generated by the different compilation flows in order to identify the main reasons for the cases with significant performance improvements. Using specialized compiler phase orders, we were able to achieve performance improvements over the CUDA version and OpenCL compiled with the NVIDIA driver. Compared to CUDA, we were able to achieve geometric mean improvements of 1.54&times; (up to 5.48&times;). Compared to the OpenCL driver version, we were able to achieve geometric mean improvements of 1.65&times; (up to 5.70&times;).<br/> &copy; Springer International Publishing AG, part of Springer Nature 2018.","Nobre, Ricardo and Reis, Luis and Cardoso, Joao M. P.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"SPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git","In this demo, we show how an effective and application agnostic way of curating SPARQL queries can be achieved by leveraging Git-based architectures. Often, SPARQL queries are hard-coded into Linked Data consuming applications. This tight coupling poses issues in code maintainability, since these queries are prone to change to adapt to new situations; and query reuse, since queries that might be useful in other applications remain inaccessible. In order to enable decoupling, version control, availability and accessibility of SPARQL queries, we propose SPARQL2Git, an interface for editing, curating and storing SPARQL queries that uses cloud based Git repositories (such as GitHub) as a backend. We describe the query management capabilities of SPARQL2Git, its convenience for SPARQL users that lack Git knowledge, and its combination with grlc to easily generate Linked Data APIs.<br/> &copy; Springer International Publishing AG 2017.","Merono-Penuela, Albert and Hoekstra, Rinke",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Using runtime state analysis to decide applicability of dynamic software updates","Updating application code while it is running is a popular approach to the dynamic software update problem. But in many cases the behavior of the updated application bears side effects of the update in the form of a runtime phenomena that breaks application state assumptions leading to unwanted complications. We present a runtime state analysis system, Genrih, that enhances a dynamic system update solution and automatically decides if the state transformation functions of a DSU solution are sufficient for the given update. Genrih analyzes the atomic changes in the updated code compared to the already running version and based on these changes automatically determines whether updating the system's runtime state will lead to the observable runtime phenomena. The designed system does not break the update procedure, but observes the state and produces notifications for enhanced analysis and crash management. The practical evaluation shows that the designed system imposes acceptable overhead and can help the developer be aware of several kinds of runtime phenomena.<br/> Copyright &copy; 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","elajev, Oleg and Gregersen, Allan",2017,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Nontarget Vision Sensor for Remote Measurement of Bridge Dynamic Response","Displacements of railroad bridges under trainloads need to be closely monitored, but conventional displacement sensors have limitations for use in the field. This paper presents a new vision-based sensor system developed for remote measurement of structural dynamic displacements without requiring a specially installed target-marker panel. By implementing a robust object-search algorithm, the displacement can be accurately measured by tracking existing bridge surface features from a remote distance. The accuracy of measured dynamic displacements was first evaluated using a shaking table test. Then field tests were carried out on two railroad bridges subjected to freight trainloads traveling at various speeds. Measurements were taken remotely during the daytime and also at night from different distances with and without a target panel. Through comparison with a conventional contact-type displacement sensor, the high accuracy of the proposed nontarget remote-sensor system was demonstrated in the realistic field environments. From the measured displacement time histories, frequency-domain characteristics associated with the train-bridge systems were further analyzed, confirming the capability of the vision system in measuring high-frequency components. By targeting existing features on a structure without requiring a target panel installed on a fixed location of the structure, the vision sensor system developed in this study provides the flexibility to easily change displacement measurement locations, in addition to other advantages, such as easy setup and no need to access the structure.<br/> &copy; 2015 American Society of Civil Engineers.","Feng, Maria Q. and Fukuda, Yoshio and Feng, Dongming and Mizuta, Masato",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Compressive sensing multiple description image coding with hybrid sampling","A Compressive Sensing(CS) multiple description coding scheme with hybrid sampling was proposed to improve the coding efficiency of the traditional CS coding system and to maintain the ability of resisting packet loss. In the scheme, both 2-D Discrete Cosine Transformation(DCT) matrix and sub-Gaussian matrix were used to measure the image signal simultaneously. Then, a Golomb code and its improved version were used to encode for the resulted measurements, respectively. As a result, the 2-D DCT measurement bit streams with complete code words and the Gaussian measurement bit streams with incomplete code words were obtained respectively. In the decoder, these incomplete code words could be decoded successfully with a Maximum A posteriori Probability (MAP) estimator, and the deficient code words could be estimated by the relevance between 2-D DCT and Gaussian measurements. Finally, these decoded measurements were grouped together again to reconstruct the image signal by solving a 1-norm optimization problem. Experimental results on both natural and remote sensing images show that the Peak Signal to Noise Ratio(PSNRs) of the images reconstructed by proposed method can be superior to that of traditional CS coding scheme by 2~4 dB at different packet loss rates, meanwhile, it has a robust resisting packet loss ability.<br/>","Wang, Liang-Jun and Shi, Guang-Ming and Li, Fu and Shi, Si-Qi",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Comparison of ultra-rapid orbit prediction strategies for GPS, GLONASS, galileo and BeiDou","Currently, ultra-rapid orbits play an important role in the high-speed development of global navigation satellite system (GNSS) real-time applications. This contribution focuses on the impact of the fitting arc length of observed orbits and solar radiation pressure (SRP) on the orbit prediction performance for GPS, GLONASS, Galileo and BeiDou. One full year&rsquo;s precise ephemerides during 2015 were used as fitted observed orbits and then as references to be compared with predicted orbits, together with known earth rotation parameters. The full nine-parameter Empirical Center for Orbit Determination in Europe (CODE) Orbit Model (ECOM) and its reduced version were chosen in our study. The arc lengths of observed fitted orbits that showed the smallest weighted root mean squares (WRMSs) and medians of the orbit differences after a Helmert transformation fell between 40 and 45 h for GPS and GLONASS and between 42 and 48 h for Galileo, while the WRMS values and medians become flat after a 42 h arc length for BeiDou. The stability of the Helmert transformation and SRP parameters also confirmed the similar optimal arc lengths. The range around 42-45 h is suggested to be the optimal arc length interval of the fitted observed orbits for the multi-GNSS joint solution of ultra-rapid orbits.<br/> &copy; 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Geng, Tao and Zhang, Peng and Wang, Wei and Xie, Xin",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Dynamic service versioning: Network-based approach (DSV-N)","The version of a service describes the service functionality, guiding client on the details for accessing the service. Service versioning requires optimal strategies to appropriately manage versions of the service which result from changes during service life cycle. However, there is no standard for handling service versions which leads to the difficulties in tracing changes, measuring their impact as well as managing multiple services concurrently without any backward compatibility issues. Sometime these services need to be modified as per the client&rsquo;s prerequisites which result in a new version of the existing service, and changes done in the services may or may not be backward compatible. If changes done in the services are not backward compatible, then it can create compatibility issues in the client side code which is using these service features. The problem aggregates even more when a product requires customization for its clients with minor differences in each version. This results in deploying multiple versions of the service for each one of them. This work describes DSV-N (Dynamic Service Versioning-Network-Based Approach) to handle issues related to change management of the service. DSV-N is also capable of handling both backward and incompatible changes. This paper also extends the functionality of dynamic service dispatching by using it with service versioning so that the versions do not need to reside in the memory permanently except the system bus which will execute the appropriate version at run time. Another advantage of using DSV-N is that multiple service versions are required to be bound with the system bus merely (which is the address of all the service versions for the client). DSV-N automates the process of replicating identical modules in different versions of the service with the help of component file in which version id(s) is(are) prefixed to each module name. DSV-N also provides backward compatibility because the appropriate version that needs to be executed will be resolved at run time. Another advantage DSV-N provides is that the component file needs to be parsed only if the service modules of the component file are modified or only for the first request.<br/> &copy; Springer Nature Singapore Pte Ltd. 2019.","Jauhari, Prashank and Kumar, Sachin and Mal, Chiranjeev and Marwaha, Preeti and Preveen, Anu",2019,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A metaphone based chaotic searchable encryption algorithm for border management","In this paper, we consider a use case for national border control and management involving the assurance of privacy and protection of personally identifiable information (PII) in a shared multi-tenant environment, i.e. the cloud. A fuzzy searchable encryption scheme is applied on a watch list of names which are used as indexes for the identification files that are in their turn encrypted and stored on the cloud. Two propositions are described and tested in this paper. The first entails the application of a chaotic fuzzy searchable encryption scheme directly on the use case and its subsequent verification on a number of phonetics synonyms for each name. In the second version, a metaphone based chaotic fuzzy transformation method is used to perform a secure search and query. In this latter case, the fuzzy transformation is performed in two stages: the first stage is the application of the metaphone algorithm which maps all the words pronounced in the same way to a single code and the second stage is the application of the chaotic Local Sensitive Hashing (LSH) to the code words. In both the first and second propositions, amplification of the LSH is also performed which permits controlled fuzziness and ranking of the results. Extensive tests are performed and experimental results show that the proposed scheme can be used for secure searchable identification files and a privacy preserving scheme on the cloud.<br/> Copyright &copy; 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","Awad, Abir and Lee, Brian",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Simplifying translation validation via model extrapolation","We revisit our case study on the NASA's Voyager space mission to automatically discover its behaviour by means of model transformation and automata learning. We investigate the conformance of three structurally different types of specification of the case study: (1) a formal specification given in ASSL, (2) a derived implementation in Java, and (3) two behavioral models, one derived from the ASSL specification and one learned from the Java implementation. This way we show that Behavioural Mining, that extracts directly analyzable behavioural models from other artifacts (specifications or code) is a practicable and very simple way to obtain a process-oriented description of third-party systems. As the learning technique can be tailored to different abstraction levels according what behavioural primitives we decide to observe, we show and discuss different alternative learned models. This process oriented description is directly amenable to formal verification, as we show here by means of model checking. &copy; 2013 - Society for Design and Process Science.<br/>","Howar, Falk and Margaria, Tiziana and Wagner, Christian",2013,"[""Engineering Village""]","Rejeitado: CR12, CR9","Rejeitado: CR12"
"FE modeling of the cooling and tempering steps of bimetallic rolling mill rolls","Numerical simulations enable the analysis of the stress and strain histories of bimetallic rolling mill rolls. The history of rolling mill rolls is simulated by thermo-mechanical metallurgical finite element code while considering two steps: post-casting cooling and subsequent tempering heat treatment. The model requires a notably large set of material parameters. For different phases and temperatures, Young modulus, yield limit and tangent plastic modulus are determined through compression tests. Rupture stresses and strains are obtained by tensile tests. Thermo-physical parameters are measured by such experimental methods as dilatometry, DSC (Differential Scanning Calorimetry) and Laser Flash methods. Such parameters as the transformation plasticity coefficients for the ferrite, pearlite and martensite phases are identified through an inverse method. From the simulation results, the profile of the stresses evolution at different critical times is presented. An analysis of the potential damage is proposed by comparing the predicted axial stress with rupture stresses. The perspective of the Ghosh and McClintock damage criteria is also investigated.<br/> &copy; 2015, Springer-Verlag France.","Neira Torres, Ingrid and Gilles, Gaetan and Tchoufang Tchuindjang, Jerome and Flores, Paulo and Lecomte-Beckers, Jacqueline and Habraken, Anne Marie",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A satellite relative motion model including j2and j3via Vinti's intermediary","Vinti's potential is revisited for analytical propagation of the main satellite problem, this time in the context of relative motion. A particular version of Vinti's spheroidal method is chosen that is valid for arbitrary elliptical orbits, encapsulating J<inf>2</inf>, J<inf>3</inf>, and approximately two thirds of J<inf>4</inf>in an orbit propagation theory without resorting to perturbation methods. As a child of Vinti's solution, the proposed relative motion model inherits these properties. Furthermore, the problem is solved in oblate spheroidal elements, leading to large regions of validity for the linearization approximation. After offering several enhancements to Vinti's solution, including boosts in accuracy and removal of some singularities, the proposed model is derived and subsequently reformulated so that Vinti's solution is piecewise differentiable. While the model is valid for the critical inclination and nonsingular in the element space, singularities remain in the linear transformation from ECI coordinates to spheroidal elements when the eccentricity nears zero or for nearly circular equatorial orbits. The new state transition matrix is evaluated against numerical solutions including the J<inf>2</inf>through J<inf>5</inf>terms for a wide range of chief orbits and separation distances. The solution is also compared with side-by-side simulations of the original Gim-Alfriend state transition matrix, which considers the J<inf>2</inf>perturbation. Code for computing the resulting state transition matrix and associated reference frame and coordinate transformations is provided online as supplementary material.<br/>","Biria, Ashley D. and Russell, Ryan P.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"State of the art of dynamic software updating in Java","The dynamic software updating system JRebel from Zeroturnaround has proven to be an efficient mean to improve developer productivity, as it allows developers to change the code of their applications while developing and testing them. Hence, developers no longer have to go through the tedious cycle of serializing application state, halting execution, redeploy the binary, restarting, and de-serializing state before they can test the effect of a code change. However, the current version of JRebel has its limits, as it does not support all kinds of code changes. In this paper, we compare the three most comprehensive dynamic updating systems developed for Java to date. Together, these systems provide comprehensive support for changing class definitions of live objects, including adding, removing and moving fields, methods, classes and interfaces anywhere in the inheritance hierarchy. We then investigate the effects of dynamic updating by performing a dynamic updating experiment on five consecutive revisions of the classical arcade game Breakout using the dynamic updating system Gosh! (Prior to the acquisition by zeroturnaround.com known as Javeleon.). Based on the result of this experiment we show that dynamic updating of class definitions for live objects may under some circumstances result in different run-time behavior than would be observed after a cold restart of the upgraded application. Finally, we conclude by discussing the implication of integrating the dynamic updating model of Gosh! with JRebel. The successful integration of these two systems will set a new standard for dynamic software updating in Java.<br/> &copy; Springer-Verlag Berlin Heidelberg 2014.","Gregersen, Allan Raundahl and Rasmussen, Michael and Jorgensen, Bo Norregaard",2014,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Reply to comment by Anel on Most computational hydrology is not reproducible, so is it really science?","In this article, we reply to a comment made on our previous commentary regarding reproducibility in computational hydrology. Software licensing and version control of code are important technical aspects of making code and workflows of scientific experiments open and reproducible. However, in our view, it is the cultural change that is the greatest challenge to overcome to achieve reproducible scientific research in computational hydrology. We believe that from changing the culture and attitude among hydrological scientists, details will evolve to cover more (technical) aspects over time.<br/> &copy; 2017. American Geophysical Union. All Rights Reserved.","Hutton, Christopher and Wagener, Thorsten and Freer, Jim and Han, Dawei and Duffy, Chris and Arheimer, Berit",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Bug patterns detection from android apps","Android has become the most popular OS because of its user-friendly environment, free-ware licensing and thousands of available applications. It is an open source for contributors and developers. The challenging problem in Android apps is to handle the bugs those are generated because of code segment (code constructs) written by developers to fix the reported bug. so code change management is also as critical task, as bug tracking. We have investigated all available previous history of Android bug reports and code changes to identify bug introducing changes. Apply the chi square test to observe the buggy construct. This study will help the reviewers, contributors, developers and quality assurance testers to concentrate and take special care while making or accepting changes to those constructs where it is most likely to induce a bug, which will lead to improve the quality of services provided by Android platform, and ultimately will get more satisfied user.<br/> &copy; Springer Nature Singapore Pte Ltd. 2018.","Ramay, Waheed Yousuf and Akbar, Arslan and Sajjad, Muhammad",2018,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Erasure correction-based CSMA/CA","It is well known that the performance of carrier sense multiple access with collision avoidance (CSMA/CA) is poor when the number of users increases, because of collisions. In this paper, we consider a modified version of CSMA/CA based on erasure codes at the packet level, which significantly reduces the complexity of the decoding and does not require any change in the underlying physical layer. In order to improve the performance, we use non-binary maximum distance separable (MDS) codes. We give analytical derivation of the global goodput and show that there is a trade-off between the code parameters and the length of the contention window in order to maximize the global goodput for a given number of users.<br/> &copy; 2017, Institut Mines-T&eacute;l&eacute;com and Springer-Verlag France SAS.","Tortelier, Patrick and Le Ruyet, Didier",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Introduction to devOps with chocolate, LEGO and scrum game","Discover a role-based simulation game designed to introduce DevOps in a very unusual way. Working with LEGO and chocolate, using avatars, personas, and role cards, you will gain an understanding of the Dev and Ops roles as well as their interdependencies. Throughout the game, players go through a range of emotions and learn to expand the boundaries of individual roles, acquire T-shaped skills, and grow the Scrum-team circle to include Operations. The game combines ideas from &ldquo;The Phoenix Project&rdquo; with the experience gained from real-life challenges, encountered by development and operations teams in many organizations. Security vulnerabilities, environments patching, deployment code freeze, development and operations silos - the game helps simulate an end-to-end product delivery process and visualize the bottlenecks in the value delivery flow. Introduction to DevOps with Chocolate, LEGO and Scrum Game engages all five senses to maximize learning effectiveness and in three sprints takes players through a gamified DevOps transformation journey. What You Will Learn Play the Chocolate, LEGO and Scrum role-simulation game Gain knowledge of DevOps and how to apply the game to it See how this game illustrates the DevOps cycle as a case study Who This Book Is For Programmers or system admins/project managers who are new to DevOps. DevOps trainers and Agile Coaches who are interested in offering a collaborative and engaging learning experience to their teams.<br/> &copy; 2017 by Dana Pylayeva.","Pylayeva, Dana",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Lightweight online software updates for C applications","It is obvious that software after its initial deployment changes over time statically or dynamically. Some software systems such as servers and safety-critical systems cannot allow for static code changes-fixing and rebooting a running program. Dynamic software updates (DSU) make it possible to change executing program code without rebooting or stopping. DSU systems can be used to modify applications on the fly, thus saving the programmer's time and using computing resources more productively. This paper presents a lightweight dynamic updating system which replaces currently running software using dynamic linking. This update system implements software updates in the same process memory space. Thus, the new version of software is required to be loaded into the shard memory of the existing process's virtual memory space. The state of the old version (i.e., local and global variables) is transferred to the new one. The experimental results indicate that the proposed updating system have the potential to become an effective tool for C applications.<br/>","Kim, Dong Kwan",2013,"[""Engineering Village""]","Rejeitado: CR10, CR12","Rejeitado: CR10"
"Local Transformed Features for Epileptic Seizure Detection in EEG Signal","Epilepsy is a well known neurological disorder characterized by the presence of recurrent seizures. Electroencephalograms (EEGs) record electrical activity in the brain and are used to detect epilepsy. Traditional EEG analysis methods for epileptic seizure detection are time-consuming, which has led to the recent proposal of several automated seizure detection frameworks. Feature extraction and classification are two important steps in this procedure. Feature extraction focuses on finding the informative features that could be used in the classification step for correct decision making; therefore, proposing some effective feature extraction techniques for seizure detection is of great significance. This paper introduces two novel feature extraction techniques: local centroid pattern (LCP) and one-dimensional local ternary pattern (1D-LTP) for seizure detection in EEG signal. Both the techniques are computationally simple and easy to implement. In both the techniques, the histograms are formed in the first step using the transformation code and then these histogram-based feature vectors are fed into a classifier in the second step. The performance of the proposed techniques was evaluated through 10-fold cross-validation tested on the benchmark dataset. Different machine learning classifiers were used for the classification. The experimental results show that LCP and 1D-LTP achieved the highest accuracy of 100% for the classification between normal and seizure EEG signals with the artificial neural network classifier. Nine different experimental cases have been tested. The results achieved for different experimental cases were higher than the results of some existing techniques in the literature. The experimental results indicate that LCP and 1D-LTP could be effective feature extraction techniques for seizure detection.<br/> &copy; 2017, Taiwanese Society of Biomedical Engineering.","Jaiswal, Abeg Kumar and Banka, Haider",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Reactive transport modeling of subsurface flow constructed wetlands using the HYDRUS wetland module","Constructed wetlands (CWs) are engineered water treatment systems designed to remove various types of contaminants. A large number of processes simultaneously contribute to water quality improvement in CWs. During the last decade, there has been a wide interest in the understanding of complex ""constructed wetland"" systems, including the development of numerical process-based models describing these systems. A number of process-based numerical models for subsurface flow (SSF) CWs have been developed during the last few years; however, most of them are either in an early stage of development or are available only in-house. The HYDRUS wetland module is the only implementation of a CW model that is currently publicly available. Version 2 of the HYDRUS wetland module includes two biokinetic model formulations simulating reactive transport in CWs: CW2D and CWM1. In CW2D, aerobic and anoxic transformation and degradation processes for organic matter, N, and P are considered, whereas in CWM1, aerobic, anoxic, and anaerobic processes for organic matter, N, and S are taken into account. We simulated horizontal flow CWs using both biokinetic models. Compared with the CWM1 implementation in the RETRASO code, the HYDRUS implementation was able to simulate fixed biomass, which is of high importance for obtaining realistic predictions for the treatment efficiency of CWs. We also compared simulation results for horizontal flow CWs obtained using both CW2D and CWM1 modules that showed that CWM1 produces more reasonable results because it also considers anaerobic degradation processes. The influence of wetland plants on the simulation results was also investigated. Simulated biomass profiles in the filter were completely different when considering O<inf>2</inf>release from roots, thus indicating the importance of considering plant effects. &copy; Soil Science Society of America.<br/>","Langergraber, Gunter and imunek, Jirka",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Open issues in model transformations for multimodal applications","Multimodal human&ndash;computer interaction refers to the interaction with the virtual and physical environment through natural modes of communication. Multimodal input user interfaces have significant role in different domains, for example industrial plants or hospitals, also they have implications for accessibility. To develop multimodal applications in formally rigorous settings, software developer teams may use tools or a software development kit to increase the efficiency and the quality of the resulted software artifacts. Such a technique is performing the design with software modeling and applying model transformations to generate well-defined components of the software. Furthermore, representation-bridging communication is a discipline of cognitive infocommunications, where the sensory information transferred to the receiver entity is filtered and/or converted. Whenever such approaches are used, the challenges associated with the modeling of information requirements, user capabilities and cross-model interactions are compounded and further increase the need for formal design and verification tools. Applying model transformations is a way to support this activity. Communication-intensive solutions often require complex methods, i.e. significant model transformation efforts between the different representations. Important semantic information should be preserved and not misinterpreted in a complex model transformations. Therefore, methods are required to verify that the semantics used during the application generation and analysis are indeed preserved across the transformation. As a case in point, such a model transformation could yield embedded code for a given type of electronic driver assistant system based on a high-level characterizations of the information to be transferred and the driver&rsquo;s cognitive capabilities. Later, a multimodal interactions expert could easily modify those characterizations on demand, and regenerate a modified version of the software without having to know about the low-level details of the embedded platform. This paper provides a strong motivation regarding the necessity of methods to support verification and validation of model transformations supporting multimodal application development and cognitive infocommunications. As the main result of the paper, we compile a list of open issues in the field of verification/validation of model transformations, and link those issues to the development of multimodal interfaces. Through its discussions, the paper makes the point that the design practices behind multimodal interfaces could strongly benefit from the use of formal modeling techniques in general, and model transformation approaches in particular.<br/> &copy; 2015, OpenInterface Association.","Lengyel, Laszlo and Charaf, Hassan",2015,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Investigation of the effects of soluble boron tracking on coupled CTF / NEM, LWR simulations","The primary objective of this study is to evaluate the effects of introducing a boron tracking capability to the COBRA-TF / NEM code coupling. The Pennsylvania State University (PSU) versions of COBRA-TF - CTF, and Nodal Expansion Method (NEM) codes are utilized. Previous implementations of the CTF / NEM coupled code had no capability to model soluble boron feedback effects due to boron transport. This study builds upon the validation and qualification efforts of the boron tracking model implementation in CTF by modeling the boron feedback calculated by the CTF boron tracking model in NEM. The core model chosen for this study is the Purdue MOX/UO<inf>2</inf>core model used in the 2007 OECD/NRC code benchmark study. Following the implementation of an explicit online coupling scheme and accompanying k-search routine, the newly coupled CTF / NEM code version with boron tracking is compared to prior results of the non-boron tracking CTF / NEM code version at steady-state hot full power and hot zero power conditions. It was found that the boron tracking model exhibited little influence on the hot zero power result as expected due to a smaller heat flux, which does not significantly change the moderator density and boron concentration as the moderator travels up the axial core length. Meanwhile the boron tracking model had a much greater impact on the hot full power results, predicting the critical inlet boron concentration to be 9.9 ppm below the non-boron tracking result due to greater and more rapid changes in boron concentration corresponding to the reduction in moderator density from being more rapidly heated.<br/>","Biery, M. and Avramova, M. and Ivanov, K.",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR10, CR9"
"A set of patterns for concurrent and parallel programming teaching","The use of key parallel-programming patterns has proved to be extremely helpful for mastering difficult concurrent and parallel programming concepts and the associated syntactical constructs. The method suggested here consists of a substantial change of more traditional teaching and learning approaches to teach programming. According to our approach, students are first introduced to concurrency problems through a selected set of preliminar program code-patterns. Each pattern also has a series of tests with selected samples to enable students to discover the most common cases that cause problems and then the solutions to be applied. In addition, this paper presents the results obtained from an informal assessment realized by the students of a course on concurrent and real-time programming that belongs to the computer engineering (CE) degree. The obtained results show that students feel now to be more actively involved in lectures, practical lessons, and thus students make better use of their time and gain a better understanding of concurrency topics that would not have been considered possible before the proposed method was implemented at our University.<br/> &copy; Springer International Publishing AG, part of Springer Nature 2018.","Capel, Manuel I. and Tomeu, Antonio J. and Salguero, Alberto G.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Adaptive Online Sequential ELM for Concept Drift Tackling","A machine learning method needs to adapt to over time changes in the environment. Such changes are known as concept drift. In this paper, we propose concept drift tackling method as an enhancement of Online Sequential Extreme Learning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by adding adaptive capability for classification and regression problem. The scheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier scheme that works well to handle real drift, virtual drift, and hybrid drift. The AOS-ELM also works well for sudden drift and recurrent context change type. The scheme is a simple unified method implemented in simple lines of code. We evaluated AOS-ELM on regression and classification problem by using concept drift public data set (SEA and STAGGER) and other public data sets such as MNIST, USPS, and IDS. Experiments show that our method gives higher kappa value compared to the multiclassifier ELM ensemble. Even though AOS-ELM in practice does not need hidden nodes increase, we address some issues related to the increasing of the hidden nodes such as error condition and rank values. We propose taking the rank of the pseudoinverse matrix as an indicator parameter to detect ""underfitting"" condition.<br/> &copy; 2016 Arif Budiman et al.","Budiman, Arif and Fanany, Mohamad Ivan and Basaruddin, Chan",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An experience report on (Auto-)tuning of mesh-based PDE solvers on shared memory systems","With the advent of manycore systems, shared memory parallelisation has gained importance in high performance computing. Once a code is decomposed into tasks or parallel regions, it becomes crucial to identify reasonable grain sizes, i.e. minimum problem sizes per task that make the algorithm expose a high concurrency at low overhead. Many papers do not detail what reasonable task sizes are, and consider their findings craftsmanship not worth discussion. We have implemented an autotuning algorithm, a machine learning approach, for a project developing a hyperbolic equation system solver. Autotuning here is important as the grid and task workload are multifaceted and change frequently during runtime. In this paper, we summarise our lessons learned. We infer tweaks and idioms for general autotuning algorithms and we clarify that such a approach does not free users completely from grain size awareness.<br/> &copy; Springer International Publishing AG, part of Springer Nature 2018.","Charrier, Dominic E. and Weinzierl, Tobias",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"There and back again: Can you compile that snapshot?","A broken snapshot represents a snapshot from a project's change history that cannot be compiled. Broken snapshots can have significant implications for researchers, as they could hinder any analysis of the past project history that requires code to be compiled. Noticeably, while some broken snapshots may be observable in change history repositories (e.g., no longer available dependencies), some of them may not necessarily happen during the actual development. In this paper, we systematically study the compilability of broken snapshots in 219 395 snapshots belonging to 100 Java projects from the Apache Software Foundation, all relying on Maven as an automated build tool. We investigated broken snapshots from 2 different perspectives: (1) how frequently they happen and (2) likely causes behind them. The empirical results indicate that broken snapshots occur in most (96%) of the projects we studied and that they are mainly due to problems related to the resolution of dependencies. On average, only 38% of the change history of the analyzed systems is currently successfully compilable.<br/> Copyright &copy; 2016 John Wiley & Sons, Ltd.","Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and DiPenta, Massimiliano and Oliveto, Rocco and DeLucia, Andrea and Poshyvanyk, Denys",2017,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Relationship between design and defects for software in evolution","Successful prediction of defects at an early stage is one of the main goals of software quality assurance. Having an indicator of the severity of defect occurrence may bring further benefit to allocation of testing resources. Code churn in terms of modified lines of code was found to be a good indicators of bugs. However, that metric does not reveal the structural changes of the code design. The idea of this project is to analyze the relationship between the evolution of object oriented software metrics and the appearance of defects. To achieve this, the absolute and relative differences between the initial version of a class and its current version were calculated for each metrics as indicators of design change. Then, the correlation between these differences and the number of defects were analyzed. Our case study showed that certain metrics have no influence on defect occurrence, while several of them exhibit moderate level of correlation. In addition, we concluded that the relative differences were inappropriate indicator for determining the relationship.<br/> &copy; Copyright 2017 by the paper's authors.","Mileti, Matija and Vukui, Monika and Maus, Goran and Grbac, Tihana Galinac",2017,"[""Engineering Village""]","Rejeitado: CR5","Rejeitado: CR5"
"15th International Conference on Enterprise Information Systems, ICEIS 2013","The proceedings contain 40 papers. The special focus in this conference is on Databases, Information Systems Integration, Artificial Intelligence, Decision Support Systems, Information Systems Analysis and Specification. The topics include: An evaluation of multi-way joins for relational database systems; applying semantic web tools and techniques to the textile traceability; estimating sufficient sample sizes for approximate decision support queries; a data-driven prediction framework for analyzing and monitoring business process performances; an overview of experimental studies on software inspection process; optimizing power, heating, and cooling capacity on a decision-guided energy investment framework; business intelligence for improving supply chain risk management; Bayesian prediction of fault-proneness of agile-developed object-oriented system; foundation for fine-grained security and DRM control based on a service call graph context identification; capturing semiotic and social factors of organizational evolution; the change impact analysis in BPM based software applications; reengineering of object-oriented software into aspect-oriented ones supported by class models; re-learning of business process models from legacy system using incremental process mining; an analysis of the usage of a micro blogging system; cloud-based collaborative business services provision; an architecture for health information exchange in pervasive healthcare environment; dealing with usability in model-driven development method; self-service classroom capture generating interactive multivideo objects; from gaps to transformation paths in enterprise architecture planning; an automated architectural evaluation approach based on metadata and code analysis and blueprint of a semantic business process-aware enterprise information architecture.",,2014,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Mission-type education programs with smart device facilitating LBS","The assessment of Korean students' academic achievement was in higher rank among OECD member countries. However, in terms of spontaneity, interest and confidence, they are in lower rank due to the oppressive schooling system. To raise the preference for study and develop creativity, the new kind of mission-type education programs using smart devices and ICT are drawing attention. For example, Ins-Edu Institute obtained a patent on learning programs using iPads and students on field trips were each given a mission with a QR code and instructed to take pictures, write reports and edit photos using the smart device and send them via SMS. At the 5<sup>th</sup> AEMM Meeting, the Institute of APEC Collaborative Education handed out iPads with task information and questions for a tour around the city We analyzed the content and system of these apps, which perform a role to cultivate students' creativity and interest.","Dong, Uk Im and Lee, Jong Oh",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Selecting a climate model subset to optimise key ensemble properties","End users studying impacts and risks caused by human-induced climate change are often presented with large multi-model ensembles of climate projections whose composition and size are arbitrarily determined. An efficient and versatile method that finds a subset which maintains certain key properties from the full ensemble is needed, but very little work has been done in this area. Therefore, users typically make their own somewhat subjective subset choices and commonly use the equally weighted model mean as a best estimate. However, different climate model simulations cannot necessarily be regarded as independent estimates due to the presence of duplicated code and shared development history. &lt;br&gt;&lt;br&gt; Here, we present an efficient and flexible tool that makes better use of the ensemble as a whole by finding a subset with improved mean performance compared to the multi-model mean while at the same time maintaining the spread and addressing the problem of model interdependence. Out-of-sample skill and reliability are demonstrated using model-as-truth experiments. This approach is illustrated with one set of optimisation criteria but we also highlight the flexibility of cost functions, depending on the focus of different users. The technique is useful for a range of applications that, for example, minimise present-day bias to obtain an accurate ensemble mean, reduce dependence in ensemble spread, maximise future spread, ensure good performance of individual models in an ensemble, reduce the ensemble size while maintaining important ensemble characteristics, or optimise several of these at the same time. As in any calibration exercise, the final ensemble is sensitive to the metric, observational product, and pre-processing steps used.<br/> &copy; Author(s) 2018.","Herger, Nadja and Abramowitz, Gab and Knutti, Reto and Angelil, Oliver and Lehmann, Karsten and Sanderson, Benjamin M.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"One-Way Wave Equation Migration at Scale on GPUs Using Directive Based Programming","One-Way Wave Equation Migration (OWEM) is a depth migration algorithm used for seismic imaging. A parallel version of this algorithm is widely implemented using MPI. Heterogenous architectures that use GPUs have become popular in the Top 500 because of their performance/power ratio. In this paper, we discuss the methodology and code transformations used to port OWEM to GPUs using OpenACC, along with the code changes needed for scaling the application up to 18,400 GPUs (more than 98%) of the Titan leadership class supercomputer at Oak Ridget National Laboratory. For the individual OpenACC kernels, we achieved an average of 3X speedup on a test dataset using one GPU as compared with an 8-core Intel Sandy Bridge CPU. The application was then run at large scale on the Titan supercomputer achieving a peak of 1.2 petaflops using an average of 5.5 megawatts. After porting the application to GPUs, we discuss how we dealt with other challenges of running at scale such as the application becoming more I/O bound and prone to silent errors. We believe this work will serve as valuable proof that directive-based programming models are a viable option for scaling HPC applications to heterogenous architectures.<br/> &copy; 2017 IEEE.","Mehta, Kshitij and Hugues, Maxime and Hernandez, Oscar and Bernholdt, David E. and Calandra, Henri",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A multipurpose framework for model-based reuse-oriented software integration synthesis","Systems are increasingly built by reusing and integrating existing software. This paper presents the preliminary version of a multipurpose framework for software integration synthesis. The objective is to provide both researchers and practitioners with an easily accessible environment that, integrating different kinds of software synthesizers, permit to perform different kinds of analyses, verification, model-to-model and model-to-code transformations, all oriented to the reuse and the integration of existing, possibly third-party, software.<br/>","Perucci, Alexander and Autili, Marco and Tivoli, Massimo",2017,"[""Engineering Village""]","Aceito: CA3","Aceito: CA3"
"Boxlib with tiling: An adaptive mesh refinement software framework","In this paper we introduce a block-structured adaptive mesh refinement software framework that incorporates tiling, a well-known loop transformation. Because the multiscale, multiphysics codes built in BoxLib are designed to solve complex systems at high resolution, performance on current and next generation architectures is essential. With the expectation of many more cores per node on next generation architectures, the ability to effectively utilize threads within a node is essential, and the current model for parallelization will not be sufficient. We describe a new version of BoxLib in which the tiling constructs are embedded so that BoxLib-based applications can easily realize expected performance gains without extra effort on the part of the application developer. We also discuss a path forward to enable future versions of BoxLib to take advantage of NUMA-aware optimizations using the TiDA portable library.<br/> &copy; 2016 Society for Industrial and Applied Mathematics.","Zhang, Weiqun and Almgren, Ann and Day, Marcus and Nguyen, Tan and Shalf, John and Unat, Didem",2016,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Interleaver with high dimensional encoding principle using hybrid group search optimizer","Handling high dimensional data and an effective error correcting code with low complexity are the two main challenges in the communication system. In the first paper, we focused on the second problem of error correcting code and introduced a turbo encoder with an interleaver which uses a hybrid meta-heuristic search algorithm by combining renowned Genetic Algorithm (GA) and Group Search Optimizer (GSO) in the name of hybrid GSO (HGSO) and that performs the high dimensional data transmission. Such a high dimensional data has transformed to low dimensional data by introducing a new high dimension interleaver in the conventional design in which data transformation and optimal pattern generation has taken place using the recent meta-heuristic search algorithm. The performance of the proposed system has compared with the existing GSO, GA, FA, ABC and random interleaver design in terms of signal to noise ratio (SNR), BER and FER and also the computation time is computed for each component of both existing and proposed system. Finally, the experimental results shows that the proposed high dimension interleaver design using HGSO approach performs well than any other methods.<br/> &copy; 2017 IEEE.","Deshmukh, Rutuja Abhishek and Panat, A.R.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Interactive shadow removal and ground truth for variable scene categories","We present an interactive, robust and high quality method for fast shadow removal. To perform detection we use an on-the-fly learning approach guided by two rough user inputs for the pixels of the shadow and the lit area. From this we derive a fusion image that magnifies shadow boundary intensity change due to illumination variation. After detection, we perform shadow removal by registering the penumbra to a normalised frame which allows us to efficiently estimate non-uniform shadow illumination changes, resulting in accurate and robust removal. We also present the first reliable, validated and multi-scene category ground truth for shadow removal algorithms which overcomes limitations in existing data sets-such as inconsistencies between shadow and shadow-free images and limited variations of shadows. Using our data, we perform the most thorough comparison of state of the art shadow removal methods to date. Our algorithm outperforms the state of the art, and we supply our P-code and evaluation data and scripts to encourage future open comparisons.<br/> &copy; 2014. The copyright of this document resides with its authors.","Gong, Han and Cosker, Darren",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An extendible design exploration tool for supporting approximate computing techniques","Today, Approximate Computing represents one of the most important breakthrough in many scientific research areas. It exploits the inherent tolerance property of algorithms in order to perform approximate computations that have an acceptable accuracy and, at the same time, better performance than original versions. Previous work in the literature demonstrated the effectiveness of the trade-off between accuracy and performance, such as energy consumption, time and occupied area for integrated circuits, and many approximate computing methodologies were proposed. Unfortunately, introduced approaches fit in specific application domains and a general and systematic methodology to automatically define approximate algorithms is still an open challenge. Taking into account previous considerations, we make a key contribution in this direction by introducing a novel Approximate Computing methodology. Indeed, in this paper, we present an extended version of idea IDEA, a design exploration tool, which makes use of a source-to-source manipulation tool in order to apply code transformations that approximate the computation of a C/C++ algorithm. IDEA searches for configurations of a given algorithm that are characterized by an approximation error which is less than a user-defined threshold. Aiming at show the efficacy of IDEA, we configure it to find approximate variants of an algorithm by applying rounding techniques on numeric operations and we optimize two image processing algorithms, reducing the hardware overhead of their equivalent implementations.<br/> &copy; 2016 IEEE.","Barbareschi, Mario and Iannucci, Federico and Mazzeo, Antonino",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Tookoormuse hinnangu tapsustamine mikroproduktiivsuse profiili abil","We investigate a phenomenon we call micro-productivity decrease, which is expected to be found in most development or maintenance projects and has a specific profile that depends on the project, the development model, and the team. Micro-productivity decrease refers to the observation that the cumulative effort to implement a series of changes is larger than the effort that would be needed if we made the same modification in only one step. The reason for the difference is that the same sections of code are usually modified more than once in the series of (sometimes imperfect) atomic changes. Hence, we suggest that effort estimation methods based on atomic change estimations should incorporate these profiles when being applied to larger modification tasks. We verify the concept on industrial development projects with our metrics-based machine learning models extended with statistical data. We show that the calculated Micro-Productivity Profile for these projects could be used for effort estimation of larger tasks with more accuracy than a naive atomic change-oriented estimation.<br/>","Toth, Gabriella and Vegh, Adam Zoltan and Beszedes, Arpad and Schrettner, Lajos and Gergely, Tamas and Gyimothy, Tibor",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"A BPMN-based design and maintenance framework for ETL processes","Business Intelligence (BI) applications require the design, implementation, and maintenance of processes that extract, transform, and load suitable data for analysis. The development of these processes (known as ETL) is an inherently complex problem that is typically costly and time consuming. In a previous work, the authors have proposed a vendor-independent language for reducing the design complexity due to disparate ETL languages tailored to specific design tools with steep learning curves. Nevertheless, the designer still faces two major issues during the development of ETL processes: (i) how to implement the designed processes in an executable language, and (ii) how to maintain the implementation when the organization data infrastructure evolves. In this paper, the authors propose a model-driven framework that provides automatic code generation capability and ameliorate maintenance support of our ETL language. They present a set of model-to-text transformations able to produce code for different ETL commercial tools as well as model-to-model transformations that automatically update the ETL models with the aim of supporting the maintenance of the generated code according to data source evolution. A demonstration using an example is conducted as an initial validation to show that the framework covering modeling, code generation and maintenance could be used in practice. Copyright &copy; 2013, IGI Global.<br/>","El Akkaoui, Zineb and Zimanyi, Esteban and Mazon, Jose-Norberto and Trujillo, Juan",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Moriarty: Improving 'time to market' in big data and artificial intelligence applications","The objective of this paper is to present the Moriarty framework and show one use case of the recommendation of entertainment events. Moriarty is a tool that can generate Big Data near real-Time analytics solutions (Streaming Analytics). This new tool makes possible the collaboration among the data scientist and the software engineer. Through Moriarty, they join forces for the rapid generation of new software solutions. The data scientist works with algorithms and data transformations using a visual interface, while the software engineer works with the idea of services to be invoked. The underlying idea is that a user can build projects of Artificial Intelligence and Data Analytics without having to make any line of code. The main power of the tool is to reduce the 'time to market' in an application which embeds complex algorithms of Artificial Intelligence. It is based on different Artificial Intelligence algorithms (like Deep Learning, Natural Language Processing and Semantic Web) and Big Datamodules (Spark as a distributed data engine and access to NoSQL databases). Moriarty is divided into several layers; its core is a BPMN engine, which executes the processing and defines data analytics process, called workflows. Each workflow is defined by the standard BPMN model and is linked to a set of reusable functions or Artificial Intelligence algorithms written following a service-oriented architecture. An example of service presented is a recommendation application of restaurants, concerts, entertainment and events in general, where information is collected from social networks and websites, is processed by Natural Language Processingalgorithms and finally introduced into a graph database.<br/> &copy; 2016 WIT Press.","Pena, P. and Del Hoyo, R. and Vea-Murguia, J. and Rodrigalvarez, V. and Calvo, J.I. and Martin, J.M.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An open-source object-graph-mapping framework for Neo4j and Scala: Renesca","The usage and application of graph databases is increasing. Many research problems are based on understanding relationships between data entities. This is where graph databases are powerful. Nevertheless, software developers model and think in object-oriented software. Combining both approaches leads to a paradigm mismatch. This mismatch can be addressed by using object graph mappers (OGM). OGM adapt graph databases for object-oriented code, to relieve the developer. Most graph database access frameworks only support table based result outputs. This defeats one of the strongest purposes of using graph databases. In order to harness both the power of graph databases and object-oriented modeling (e.g. type-safety, inheritance, etc.) we propose an open-source framework with two libraries: (1) renesca, which is a graph database driver providing graph-query-results and change tracking. (2) renesca-magic, a macro-based ER-modeling domain specific language (DSL). Both were tested in a graph-based application and lead to dramatic improvements in code size (factor 10) and extensibility of the code, with no significant effect on performance.<br/> &copy; IFIP International Federation for Information Processing 2016.","Dietze, Felix and Karoff, Johannes and Valdez, Andre Calero and Ziefle, Martina and Greven, Christoph and Schroeder, Ulrik",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Online interactive educational system for submission and evaluation of programming assignments","Nowadays in educational institutions teachers provide problem statement to students and students submit their solution in accordance with problem statement provided. Grading of these solutions done manually. It is a time-consuming task. To find out the correctness of the solution they required to run multiple test cases. Also, teachers have to ensure that each computer in the lab has editor and compiler to run the code. This paper will provide guidelines for system interface to teachers in which teachers are able to create new programming assignments and assign it to a particular class. Also, this system helps teachers for grading of programming assignments. It will provide a detailed report of class for each programming assignments. It will also reduce the overhead of installing the software required to edit and run the code. This the system provide an easy interface to students to edit and to compile programs. It will also provide the detailed analysis of evaluated assignments.<br/> &copy; 2017 IEEE.","Varat, Abhishek and Vetal, Mayur and Bawadkar, Pooja and Shinde, Shubham and Naik, Varsha",2018,"[""Engineering Village""]","Rejeitado: CR9, CR10","Rejeitado: CR10"
"Preparation and enhanced thermal performance of novel (solid to gel) form-stable eutectic PCM modified by nano-graphene platelets","This study presents the development of form-stable eutectic mixtures, modified with nanoscale structures for enhanced thermal performance. These additives may result in the next generation of phase change materials (PCMs) for thermal energy storage systems. An appropriate gelling or thickening agent (2-hydroxypropyl ether cellulose) is introduced so that the PCM will lose its fluidity, become form-stable, and the liquid leakage problem will be overcome. Nano-graphene platelets (NGPs) are added in order to enhance the thermal properties and overall heat transfer. Differential scanning calorimetry (DSC) was carried out for the thermal analysis of the PCMs. The paper experimentally studied in detail the enhanced thermo-physical properties required for stimulating and modelling the PCM in energy storage applications such as specific heat, thermal diffusivity, thermal conductivity, enthalpy, and density. The principle of the T-history method was applied using a parallel plate heating/cooling guarded plate apparatus to determine the true phase transition temperatures of bulk PCM. The supercooling of the enhanced shape stable mixture was found to be less than 0.1 &deg;C. The thermal reliability test indicated that the enhanced form-stable eutectic mixture had reliable thermal performance over a postulated lifetime of 80 years. As a result, the developed form stable PCM eutectic mixture is a promising material for thermal energy storage.<br/> &copy; 2017","Saeed, Rami M. and Schlegel, J.P. and Castano, C. and Sawafta, R.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Causes of architecture changes: An empirical study through the communication in OSS mailing lists","Understanding the causes of architecture changes allows us to devise means to prevent architecture knowledge vaporization and architecture degeneration. But the causes are not always known, especially in open source software (OSS) development. This makes it very hard to understand the underlying reasons for the architecture changes and design appropriate modifications. Architecture information is communicated in development mailing lists of OSS projects. To explore the possibility of identifying and understanding the causes of architecture changes, we conducted an empirical study to analyze architecture information (i.e., architectural threads) communicated in the development mailing lists of two popular OSS projects: Hibernate and ArgoUML, verified architecture changes with source code, and identified the causes of architecture changes from the communicated architecture information. The main findings of this study are: (1) architecture information communicated in OSS mailing lists does lead to architecture changes in code; (2) the major cause for architecture changes in both Hibernate and ArgoUML is preventative changes. (3) more than 45% of architecture changes in both projects happened before the first stable version was released, which indicates that the architectures of the investigated OSS projects are relatively stable after the first stable release.<br/>","Ding, Wei and Liang, Peng and Tang, Antony and Van Vliet, Hans",2015,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Understanding the causes of architecture changes using OSS mailing lists","The causes of architecture changes can tell about why architecture changes, and this knowledge can be captured to prevent architecture knowledge vaporization and architecture degeneration. But the causes are not always known, especially in open source software (OSS) development. This makes it very hard to understand the underlying reasons for the architecture changes and design appropriate modifications. Architecture information is communicated in development mailing lists of OSS projects. To explore the possibility of identifying and understanding the causes of architecture changes, we conducted an empirical study to analyze architecture information (i.e. architectural threads) communicated in the development mailing lists of two popular OSS projects: Hibernate and ArgoUML, verified architecture changes with source code, and identified the causes of architecture changes from the communicated architecture information. The main findings of this study are: (1) architecture information communicated in OSS mailing lists does lead to architecture changes in code; (2) the major cause for architecture changes in both Hibernate and ArgoUML is preventative changes, and the causes of architecture changes are further classified to functional requirement, external quality requirement, and internal quality requirement using the coding techniques of grounded theory; (3) more than 45% of architecture changes in both projects happened before the first stable version was released.<br/> &copy; 2015 World Scientific Publishing Company.","Ding, Wei and Liang, Peng and Tang, Antony and Van Vliet, Hans",2015,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Large-scale analysis of neuroimaging data on commercial clouds with content-aware resource allocation strategies","The combined use of mice that have genetic mutations (transgenic mouse models) of human pathology and advanced neuroimaging methods (such as magnetic resonance imaging) has the potential to radically change how we approach disease understanding, diagnosis and treatment. Morphological changes occurring in the brain of transgenic animals as a result of the interaction between environment and genotype can be assessed using advanced image analysis methods, an effort described as 'mouse brain phenotyping'. However, the computational methods involved in the analysis of high-resolution brain images are demanding. While running such analysis on local clusters is possible, not all users have access to such infrastructure and even for those that do, having additional computational capacity can be beneficial (e.g. to meet sudden high throughput demands). In this paper we use a commercial cloud platform for brain neuroimaging and analysis. We achieve a registration-based multi-atlas, multi-template anatomical segmentation, normally a lengthy-in-time effort, within a few hours. Naturally, performing such analyses on the cloud entails a monetary cost, and it is worthwhile identifying strategies that can allocate resources intelligently. In our context a critical aspect is the identification of how long each job will take. We propose a method that estimates the complexity of an image-processing task, a registration, using statistical moments and shape descriptors of the image content. We use this information to learn and predict the completion time of a registration. The proposed approach is easy to deploy, and could serve as an alternative for laboratories that may require instant access to large high-performance-computing infrastructures. To facilitate adoption from the community we publicly release the source code.<br/> &copy; SAGE Publications.","Minervini, Massimo and Rusu, Cristian and Damiano, Mario and Tucci, Valter and Bifone, Angelo and Gozzi, Alessandro and Tsaftaris, Sotirios A.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"CLCMiner: Detecting Cross-Language Clones without Intermediates","The proliferation of diverse kinds of programming languages and platforms makes it a common need to have the same functionality implemented in different languages for different platforms, such as Java for Android applications and C# forWindows phone applications. Although versions of code written in different languages appear syntactically quite different from each other, they are intended to implement the same software and typically contain many code snippets that implement similar functionalities, which we call cross-language clones. When the version of code in one language evolves according to changing functionality requirements and/or bug fixes, its cross-language clones may also need be changed to maintain consistent implementations for the same functionality. Thus, it is needed to have automated ways to locate and track cross-language clones within the evolving software. In the literature, approaches for detecting cross-language clones are only for languages that share a common intermediate language (such as the .NET language family) because they are built on techniques for detecting single-language clones. To extend the capability of cross-language clone detection to more diverse kinds of languages, we propose a novel automated approach, CLCMiner, without the need of an intermediate language. It mines such clones from revision histories, based on our assumption that revisions to different versions of code implemented in different languages may naturally reflect how programmers change cross-language clones in practice, and that similarities among the revisions (referred to as clones in diffs or diff clones) may indicate actual similar code. We have implemented a prototype and applied it to ten open source projects implementations in both Java and C#. The reported clones that occur in revision histories are of high precisions (89% on average) and recalls (95% on average). Compared with token-based code clone detection tools that can treat code as plain texts, our tool can detect significantly more cross-language clones. All the evaluation results demonstrate the feasibility of revision-history based techniques for detecting cross-language clones without intermediates and point to promising future work.<br/> &copy; 2017 The Institute of Electronics, Information and Communication Engineers.","Cheng, Xiao and Peng, Zhiming and Jiang, Lingxiao and Zhong, Hao and Yu, Haibo and Zhao, Jianjun",2017,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Teaching introductory web development using scrimba: An interactive and cooperative development tool","New software applications may influence the way we teach. Scrimba facilitates live stream coding, ease of sharing code and an interesting new video format. What looks like a video is actually an audio stream combined with a dynamic code display. The viewer hears the audio while the code is generated as if it were a normal video tutorial. However, as the format is audio combined with text, the viewer may at any time stop the ""video"" and edit the code. How can educators use Scrimba to activate students and engage them in cooperative activities? We have investigated different use cases using Scrimba in two introductory web development courses. After an initial pilot, we evaluated the use of Scrimba in a course with 200 students. Data was collected through a survey (N=107) and semi-structured interviews with ten students. The students provided multiple reasons why the live stream of the coding in a classroom was useful. They also saw the value of being able to watch something that looks like a video, but with the possibility of jumping into the code and start to build upon it. The main finding, however, was how the ease of sharing code within a classroom setting created new opportunities. The entire class could engage in debugging activities, they could display multiple solutions for each other and they could create cooperative assignments. These are not new activities, but the activities were enhanced because of the reduced time in order to be able to cooperate and interact. We argue that the students became live coding participants and not only spectators through the introduction of a new software application. We further discuss these findings in the context of blended learning. Our findings should be relevant and interesting for anyone involved in teaching computer programming topics, and especially within web development.<br/> &copy; The Authors, 2018. All Rights Reserved.","Lauvas, Per and Gonzalez, Rolando",2018,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Dynamically testing GUIs using ant colony optimization","In this paper we introduce a dynamic GUI test generator that incorporates ant colony optimization. We created two ant systems for generating tests. Our first ant system implements the normal ant colony optimization algorithm in order to traverse the GUI and find good event sequences. Our second ant system, called AntQ, implements the antq algorithm that incorporates Q-Learning, which is a behavioral reinforcement learning technique. Both systems use the same fitness function in order to determine good paths through the GUI. Our fitness function looks at the amount of change in the GUI state that each event causes. Events that have a larger impact on the GUI state will be favored in future tests. We compared our two ant systems to random selection. We ran experiments on six subject applications and report on the code coverage and fault finding abilities of all three algorithms.<br/> &copy; 2015 IEEE.","Carino, Santo and Andrews, James H.",2015,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Wind turbine pitch change simulation with helicoidal vortex model","The vortex model has been extended to account for large changes in power, such as occur with abrupt changes in blade pitch. The wake is treated as a flexible spring, extending from the rotor to the far-field (Trefftz plane), along which the pitch varies according to the convected power history at the rotor. Results of test cases are compared with experimental data available from the Tjaereborg and NREL experiments, indicating that the code gives not only correct power levels asymptotically, but also predicts the transient overshoots and undershoots and recovery time accurately at a very low cost. Copyright &copy; 2012 by ASME.<br/>","Chattot, Jean-Jacques and Braaten, Mark E.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Examining the stability of logging statements","Logging statements (embedded in the source code) produce logs that assist in understanding system behavior, monitoring choke-points and debugging. Prior work showcases the importance of logging statements in operating, understanding and improving software systems. The wide dependence on logs has lead to a new market of log processing and management tools. However, logs are often unstable, i.e., the logging statements that generate logs are often changed without the consideration of other stakeholders, causing sudden failures of log processing tools and increasing the maintenance costs of such tools. We examine the stability of logging statements in four open source applications namely: Liferay, ActiveMQ, Camel and CloudStack. We find that 20&ndash;45% of their logging statements change throughout their lifetime. The median number of days between the introduction of a logging statement and the first change to that statement is between 1 and 17 in our studied applications. These numbers show that in order to reduce maintenance effort, developers of log processing tools must be careful when selecting the logging statements on which their tools depend. In order to effectively mitigate the issues that are caused by unstable logging statements, we make an important first step towards determining whether a logging statement is likely to remain unchanged in the future. First, we use a random forest classifier to determine whether a just-introduced logging statement will change in the future, based solely on metrics that are calculated when it is introduced. Second, we examine whether a long-lived logging statement is likely to change based on its change history. We leverage Cox proportional hazards models (Cox models) to determine the change risk of long-lived logging statements in the source code. Through our case study on four open source applications, we show that our random forest classifier achieves a 83&ndash;91% precision, a 65&ndash;85% recall and a 0.95&ndash;0.96 AUC. We find that file ownership, developer experience, log density and SLOC are important metrics in our studied projects for determining the stability of logging statements in both our random forest classifiers and Cox models. Developers can use our approach to determine the risk of a logging statement changing in their own projects, to construct more robust log processing tools, by ensuring that these tools depend on logs that are generated by more stable logging statements.<br/> &copy; 2017, Springer Science+Business Media New York.","Kabinna, Suhas and Bezemer, Cor-Paul and Shang, Weiyi and Syer, Mark D and Hassan, Ahmed E",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Isotopic, ecological and technological investigations of the land snail record","In the ever-evolving landscape of the natural world, change is the only constant. Investigating how life accommodates that change can provide valuable insights into the biological, ecological and geological history of our planet. The fossil record is replete with examples of organisms which failed to survive in the wake of ongoing environmental change. However, for as many organisms as succumbed to extinction, there are just as many that not only survived, but thrived. Adaptability, in the face of ecological adversity, is the key to evolutionary success. This trait is no more evident than in the life history of terrestrial gastropods. This dissertation examines how land snail strategies for survival may reveal valuable paleoenvironmental and paleoecological information and how that information might be more effectively and efficiently managed using mobile technology. The results of stable isotopic analyses of modern and ancient land snail shells from two Algerian archaeological sites and experiments demonstrating the effects of insect and non-insect herbivory on leaf tissue combined with data management techniques exploring the use of Augmented Reality (AR) and Quick Response (QR) Code 2D barcode technology in laboratory research show how dynamic life can be, even when you live it ""in the slow lane."". ProQuest Subject Headings: Geochemistry, Paleoecology, Information technology.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.","Faber, Meredith L.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Avances en Ingenieria de Software a Nivel Iberoamericano, CIbSE 2018","The proceedings contain 50 papers. The topics discussed include: a taste of the software industry perception of technical debt and its management in Brazil; knowledge management in agile testing teams: a survey; gender gap in computing: a preliminary empirical study; an exploratory study of academic architectural tactics and patterns in microservices: a systematic literature review; software measurement and estimation: a case study in the financial services industry; towards a developer's reputation model based on source code features; privacy by design in software engineering: a systematic mapping study; challenges in system of systems development: a systematic mapping; an ontology-based approach to identify developers in a software ecosystem based on their skillset; towards using task similarity to recommend stack overflow posts; improving the decision-making support in context-aware applications: the case of an adaptive virtual education learning management system; S&yacute;ntixi - a generative approach to dynamic fusion of software components; towards a simulation-based project monitoring and control learning approach; evaluation of source code in ACM ICPC style programming and training competitions; applying transformation templates to diversify user interfaces generated by model-driven engineering; and MoWeb - a mobile: modeling and generation of the communication of mobile apps with their functions in the cloud.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Design relativistic charged particle beam transportation channels","This paper contains results of development new version (2016) of program for channels design high-energy beams of charged particles. The program includes application package modeling the dynamics of charged particles in the channel, operational tools to change the channel parameters, channel optimization tools and processing output beam parameters with graphic and digital presentation of its key features. The MATLAB (Scilab) was used as programming tools, allows to make the source code modular, compact and scalable. New object-oriented graphical user interface provides an interactive assembly of new or modernization of previously developed channel - selection and arrangement of its elements, as well as the installation and the variation of their parameters. The relational database, which is part of the new version of program, providing additional functionality to the designer. It is intended for storage of the current development, and to preserve the previously completed projects, as well as other useful designer related information. A multi-output of all the main parameters of the beam at the output, as well as anywhere in the channel. In this case, the developer has the ability to interactively search and setting the optimum mode of operation channel.<br/> Copyright &copy; 2017 CC-BY-3.0 and by the respective authors","Averyanov, G.P. and Budkin, V.A. and Osadchuk, I.O.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017","The proceedings contain 68 papers. The topics discussed include: semantically enhanced software traceability using deep learning techniques; can latent topics in source code predict missing architectural tactics?; preventing defects: the impact of traceability completeness on software quality; imprecise matching of requirements specifications for software services using fuzzy logic; analyzing APIs documentation and code to detect directive defects; detecting user story information in developer-client conversations to generate extractive summaries; keyword search for building service-based systems; clone refactoring with lambda expressions; automated refactoring of legacy Java software to default methods; using cohesion and coupling for software remodularization: is it enough?; recommending and localizing change requests for mobile apps based on user reviews; machine learning-based detection of open source license exceptions; supporting change impact analysis using a recommendation system: an industrial case study in a safety-critical context; becoming agile: a grounded theory of agile transitions in practice; from diversity by numbers to diversity as process: supporting inclusiveness in software development teams with brainstorming; and process aspects and social dynamics of contemporary code review: insights from open source development and industrial practice at Microsoft.",,2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"2018 IEEE 1st International Workshop on Mining and Analyzing Interaction Histories, MAINT 2018 - Proceedings","The proceedings contain 4 papers. The topics discussed include: the cost-benefit analysis of usage data in RobotStudio; CodeCAM: capturing programmer's reaction during coding session; privacy preservation in interaction history on integrated development environments; and integrating source code search into Git client for effective retrieving of change history.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS","The proceedings contain 212 papers. The topics discussed include: utilizing cluster quality in hierarchical clustering for analogy-based software effort estimation; unconventional service engineering: toward a new paradigm of service engineering for empowering senior citizens in city platform as a service; classification and metrics for replay tools; SuperedgeRank algorithm and its application for core technology identification; performance analysis of CDN-P2P networks based on processer-sharing queues; a test language for avionics system; application of CART decision tree combined with PCA algorithm in intrusion detection; a performance comparison of two versatile frequency transformation approach in texture image retrieval; intrusion detection for engineering vehicles under the transmission line based on deep learning; detecting injection vulnerabilities in executable codes with concolic execution; an abstract method linearization for detecting source code plagiarism in object-oriented environment; adaptive life-cycle control system for overhead transmission lines using forecasting models; intelligent model of decision support system of distributed generation integration; machine learning for predictive maintenance of industrial machines using IoT sensor data; ECG based authentication for remote patient monitoring in IoT by wavelets and template matching; and a model-driven deployment approach for scaling distributed software architectures on a cloud computing platform.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR9"
"11th International Andrei Ershov Memorial Conference on Perspectives of System Informatics, PSI 2017","The proceedings contain 31 papers. The special focus in this conference is on Perspectives of System Informatics. The topics include: Lightweight non-intrusive virtual machine introspection; a distributed approach to coreference resolution in multiagent text analysis for ontology population; a framework for dynamical construction of software components; A transformation-based approach to developing high-performance GPU programs; domain engineering the Magnolia way; approximating event system abstractions by covering their states and transitions; implementing the symbolic method of verification in the C-light project; highlights of the rice-shapiro theorem in computable topology; a memory model for deductively verifying linux kernel modules; a human-in-the-loop perspective for safety assessment in robotic applications; indexing of hierarchically organized spatial-temporal data using dynamic regular octrees; An approach to the validation of XML documents based on the model driven architecture and the object constraint language; compositional relational programming with name projection and compositional synthesis; whaleProver: First-order intuitionistic theorem prover based on the inverse method; distributed in situ processing of big raster data in the cloud; statistical approach to increase source code completion accuracy; using the subject area ontology for automating learning processes and scientific investigation; Runtime specialization of postgreSQL query executor; MicroTESK: A tool for constrained random test program generation for microprocessors; Enriching textual Xtext-DSLs with a graphical GEF-based editor; multi-level static analysis for finding error patterns and defects in source code; Towards automated static verification of GNU C programs; domain specific semantic validation of schema.org annotations; pipelined bottom-up evaluation of datalog programs: The push method; PosDB: A distributed column-store engine.<br/>",,2018,"[""Engineering Village""]","Rejeitado: CR4","Rejeitado: CR4"
"A Design Methodology for Developing Resilient Cloud Services","Cloud computing is emerging as a new paradigm that aims at delivering computing as a utility. For the cloud computing paradigm to be fully adopted and effectively used, it is critical that the security mechanisms are robust and resilient to malicious faults and attacks. Security in cloud computing is of major concern and a challenging research problem since it involves many interdependent tasks including application layer firewalls, configuration management, alert monitoring and analysis, source code analysis, and user identity management. It is widely accepted that we cannot build software and computing systems that are free from vulnerabilities and cannot be penetrated or attacked. Therefore, it is widely accepted that cyber resilient techniques are the most promising solutions to mitigate cyberattacks and change the game to advantage the defender over the attacker. Moving Target Defense (MTD) has been proposed as a mechanism to make it extremely difficult for an attacker to exploit existing vulnerabilities by varying the attack surface of the execution environment. By continuously changing the environment (e.g., software versions, programming language, operating system, connectivity, etc.), we can shift the attack surface and, consequently, evade attacks. In this chapter we present a methodology for designing resilient cloud services that is based on the following capabilities: Redundancy, Diversity, Shuffling, and Autonomic Management. Redundancy is used to tolerate attacks if any redundant version or resource is compromised. The diversity is to use to avoid the software monoculture problem where one attack vector can successfully attack many instances of the same software module. Shuffling is needed to randomly change the execution environment and is achieved by ""hot"" shuffling of multiple functionally equivalent, behaviorally different software versions (code implementation) at runtime (e.g., the software task can have multiple versions where each version can be a different algorithm implemented in different programming language running on different computing systems). We also present our experimental results and evaluation of the RCS design methodology. We have implemented the applications on an IBM blade server with four blades, where each blade has 24 cores and can run several virtual machines. Our experimental results show that our environment is resilient against attacks with less than 7% in overhead time.<br/> &copy; 2017 Elsevier Inc. All rights reserved.","Tunc, Cihan and Hariri, Salim and Battou, Abdella",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Improving the user experience of the rCUDA remote GPU virtualization framework","Graphics processing units (GPUs) are being increasingly embraced by the high-performance computing community as an effective way to reduce execution time by accelerating parts of their applications. remote CUDA (rCUDA) was recently introduced as a software solution to address the high acquisition costs and energy consumption of GPUs that constrain further adoption of this technology. Specifically, rCUDA is a middleware that allows a reduced number of GPUs to be transparently shared among the nodes in a cluster. Although the initial prototype versions of rCUDA demonstrated its functionality, they also revealed concerns with respect to usability, performance, and support for new CUDA features. In response, in this paper, we present a new rCUDA version that (1) improves usability by including a new component that allows an automatic transformation of any CUDA source code so that it conforms to the needs of the rCUDA framework, (2) consistently features low overhead when using remote GPUs thanks to an improved new communication architecture, and (3) supports multithreaded applications and CUDA libraries. As a result, for any CUDA-compatible program, rCUDA now allows the use of remote GPUs within a cluster with low overhead, so that a single application running in one node can use all GPUs available across the cluster, thereby extending the single-node capability of CUDA.<br/> &copy; 2014 John Wiley & Sons, Ltd.","Reano, Carlos and Silla, Federico and Castello, Adrian and Pena, Antonio J. and Mayo, Rafael and Quintana-Orti, Enrique S. and Duato, Jose",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Sensor fault identification based on Error-Correcting Output Codes method","In this paper, we proposed a method within the framework of Error-Correcting Output Codes (ECOC) to solve sensor fault feature extraction and online identification problem. Time and frequency domain signal features are selected as the initial fault characteristics, and we enhance the separability of initial characteristics by a nonlinear transformation based on the probabilistic confidence of the first ECOC outputs. Then construct the second ECOC to classify the fault state taking the feature extracted from the first ECOC as the input. We compared the accuracy and efficiency with this method between different ECOC coding strategies. The results indicate that some certain coding combination show better results on efficiency and accuracy, being able to meet the demand of sensor online fault identification. And generalization experiment on UCI Machine Learning Repository show that this method can also be promoted to other problems.<sup>1</sup><br/> &copy; 2015 IEEE.","Zhou, Rui and Chen, Jie and Deng, Fang",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Residential energy efficiency standards in Australia: where to next?","Increasing the energy and carbon efficiency of homes has been at the forefront of international climate change mitigation policy. In Australia, recent policy action led to the introduction of minimum energy efficiency standards for new homes within the Building Code of Australia in 2003, with subsequent stringency increases in 2006 and 2010. Although not yet reflecting international best regulatory practice, these standards represent substantial progress in addressing the energy and carbon emission impact of new homes, yet there are a number of energy policy challenges that highlight the need for further change. This paper documents the history of house energy standards in Australia and examines the post-occupancy evidence of that policy outcome. The paper examines international and domestic issues pointing to a possible future direction for Australian house energy regulation, highlighting the key drivers for change. In particular, we investigate the concepts of net zero carbon and net zero energy homes which have recently been adopted internationally, examining the technical and economic evidence that would support such a policy position in Australia.<br/> &copy; 2015, Springer Science+Business Media Dordrecht.","Berry, Stephen and Marker, Tony",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Application of Group Method of Data Handling model for software maintainability prediction using object oriented systems","Object-oriented methodology has emerged as most prominent in software industry for application development. Maintenance phase begins once the product is delivered and by software maintainability we mean the ease with which existing software could be modified during maintenance phase. We can improve and control software maintainability if we can predict it in the early phases of software life cycle using design metrics. Predicting the maintainability of any software has become critical with the increasing importance of software maintenance. Many authors have practiced and proved theoretical validation followed by empirical evaluation using statistical and experimental techniques for evaluating the relevance of any given metrics suite using many models. In this paper, we have presented an empirical study to evaluate the effectiveness of novel technique called Group Method of Data Handling (GMDH) for the prediction of maintainability over other models. Although many metrics have been proposed in the literature, software design metrics suite proposed by Chidamber et al. and revised by Li et al. have been selected for this study. Two web-based customized softwares developed using C# Language have been used for empirical study. Source code of old and new versions for both applications were collected and analysed against modifications made in every class. The changes were counted in terms of number of lines added, deleted or modified in the classes belonging to new version with respect to the classes of old version. Finally values of metrics were combined with ""change"" in order to generate data points. Hence, in this study an attempt has been made to evaluate and examine the effectiveness of prediction models for the purpose of software maintainability using real life web based projects. Three models using Feed Forward 3-Layer Back Propagation Network (FF3LBPN), General Regression Neural Network (GRNN) and GMDH are developed and performance of GMDH is compared against two others i.e. FF3LBPN and GRNN. With the aid of this empirical analysis, we can safely suggest that software professionals can use OO metric suite to predict the maintainability of software using GMDH technique with least error and best precision in an object oriented paradigm. &copy; 2014 The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.<br/>","Malhotra, Ruchika and Chug, Anuradha",2014,"[""Engineering Village""]","Rejeitado: CR9, CR7","Rejeitado: CR9"
"How to Evaluate Software Architectures: Tutorial on Practical Insights on Architecture Evaluation Projects with Industrial Customers","Thorough and continuous architecting is the key to overall success in software engineering, and architecture evaluation is a crucial part of it. This tutorial presents a pragmatic architecture evaluation approach and insights gained from its application in more than 75 projects with industrial customers in the past decade. It presents context factors, empirical data, and example cases, as well as lessons learned on mitigating the risk of change through architecture evaluation. By providing comprehensive answers to many typical questions and discussing more frequent mistakes and lessons learned, the tutorial allows the audience to not only learn how to conduct architecture evaluations and interpret its results, but also to become aware of risks such as false conclusions, manipulating data, and unsound lines of argument. It equips the audience to become confident in assessing quantitative measurement results and recognize when it is better to rely on qualitative expertise. The target audience includes both practitioners and researchers. By demonstrating its impact and providing clear guidelines, data, and examples, it encourages practitioners to conduct architecture evaluations. At the same time, it offers researchers insights into industrial architecture evaluations, which serve as the basis for guiding research in this area and will inspire future research directions. Both groups will get an overview of the foundations and history of architecture evaluation. The tutorial covers the following important aspects of architecture evaluation &bull; Architecture drivers: types of drivers, importance of drivers, elicitation of drivers, documentation of drivers as architecture scenarios &bull; Solution Adequacy Check: check whether an architecture is adequate for its drivers, what are risks, assumptions, tradeoffs (based on ATAM (architecture tradeoff analysis method) and enhanced with other techniques for increasing confidence of results) &bull; Documentation Quality Check: How adequate is the documentation of an architecture to be understandable and to serve its purposes? &bull; Architecture Compliance Check: How to check whether an intended architecture is consistently reflected in the source code? For all aspects, pragmatic methodical support is provided in the tutorial and all checks are well integrated in the overall architecture evaluation method.<br/> &copy; 2018 IEEE.","Naab, Matthias and Rost, Dominik",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Intelligent GPGPU classification in volume visualization: A framework based on error-correcting output codes","In volume visualization, the definition of the regions of interest is inherently an iterative trial-and-error process finding out the best parameters to classify and render the final image. Generally, the user requires a lot of expertise to analyze and edit these parameters through multi-dimensional transfer functions. In this paper, we present a framework of intelligent methods to label on-demand multiple regions of interest. These methods can be split into a two-level GPU-based labelling algorithm that computes in time of rendering a set of labelled structures using the Machine Learning Error-Correcting Output Codes (ECOC) framework. In a pre-processing step, ECOC trains a set of Adaboost binary classifiers from a reduced pre-labelled data set. Then, at the testing stage, each classifier is independently applied on the features of a set of unlabelled samples and combined to perform multi-class labelling. We also propose an alternative representation of these classifiers that allows to highly parallelize the testing stage. To exploit that parallelism we implemented the testing stage in GPU-OpenCL. The empirical results on different data sets for several volume structures shows high computational performance and classification accuracy.<br/> &copy; 2011 The Author(s).","Escalera, S. and Puig, A. and Amoros, O. and Salamo, M.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"GREENTHUMB: Superoptimizer construction framework","Developing an optimizing compiler backend remains a laborious process, especially for nontraditional ISAs that have been appearing recently. Superoptimization sidesteps the need for many code transformations by searching for the most optimal instruction sequence semantically equivalent to the original code fragment. Even though superoptimization discovers the best machine-specific code optimizations, it has yet to become widely-used. We propose GREENTHUMB, an extensible framework that reduces the cost of constructing superoptimizers and provides a fast search algorithm that can be reused for any ISA, exploiting the unique strengths of enumerative, stochastic, and symbolic (SAT-solver-based) search algorithms. To extend GREENTHUMB to a new ISA, it is only necessary to implement an emulator for the ISA and provide some ISA-specific search utility functions.<br/> &copy; 2016 ACM.","Phothilimthana, Phitchaya Mangpo and Thakur, Aditya and Bodik, Rastislav and Dhurjati, Dinakar",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Design considerations of synaptic device for neuromorphic computing","Hardware implementation of neuromorphic computing is attractive as a computing paradigm beyond the conventional digital Boolean computing. Recently, two-terminal emerging memory devices that show electrically-triggered resistance modulation have been proposed as synaptic devices for neuromorphic computing. The synaptic device candidates include phase change memory (PCM), resistive RAM (RRAM) and conductive bridge RAM (CBRAM), etc. In this paper, we discuss the general design considerations of synaptic devices for plasticity and learning. As a rule of thumb for performance metrics assessment, an ideal synaptic device should have characteristics such as dimension, energy consumption, operation frequency, dynamic range, etc. that are scalable to biological systems with comparable complexity. &copy; 2014 IEEE.<br/>","Yu, Shimeng and Kuzum, Duygu and Wong, H.-S. Philip",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"XB+-tree: A novel index for PCM/DRAM-Based hybrid memory","Phase Change Memory (PCM) has emerged as a new kind of future memories that can be used as an alternative of DRAM. PCM has a number of special properties such as non-volatility, high density, read/write asymmetry, and byte addressability. Specially, PCM has higher write latency than DRAM but has comparable read latency with DRAM. This makes it difficult to directly replace DRAM with PCM in current memory hierarchy. Thus, in this paper, we propose to construct hybrid memory architecture that involves both PCM and DRAM, which is a practical and feasible way to utilize PCM. Such hybrid memory architecture introduces many new issues for database researches, as existing algorithms have to be revised to be suitable for hybrid memory. In this paper, we study the indexing issue on PCM/DRAM-based hybrid memory and propose an improved version of the B+-tree called XB+-tree (eXtended B+- tree). The key idea of the XB+-tree is to detect the read/write tendency of the nodes in the tree index and organize write-intensive nodes on PCM while putting read-intensive nodes on DRAM. We propose a new node management and migration algorithm in the XB+-tree to effectively move nodes between DRAM and PCM. With this mechanism, we can reduce the read and write operations on PCM and improve the overall performance. We conduct trace-driven experiments and compare our proposal with three existing indices including the B+-tree, the OB+-tree (B+-tree with the overflow scheme), and the CB+-tree. The results in terms of PCM read/write count and run time suggest the efficiency of our proposal.<br/> &copy; Springer International Publishing AG 2016.","Li, Lu and Jin, Peiquan and Yang, Chengcheng and Wan, Shouhong and Yue, Lihua",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Nonwovens with thermal storage properties based on paraffin-modified polypropylene fibres","A series of nonwovens with various mechanical and thermal properties was prepared by a textile technique based on melt-spun continuous PP fibres modified with paraffin as a phase change material (PCM). The PCM is not encapsulated; it forms a structure like ""islands in the sea"" in the PP fibres. This permits the addition of a larger amount of the active substance to the fibre than in the encapsulated version. The nonwovens made of such fibres retained high thermal resistance. Paraffin was added to the PP fibre in amounts of 10 - 30 wt%; 20 wt% appeared to be best in terms of thermal properties and processability. To prevent fibre to fibre sticking at elevated temperatures, the nonwovens were made of a blend of paraffin-modified and standard PP fibres in variable proportions. The thermal activity determined by the kind of paraffin used was estimated to be in the range of 30 - 60 &deg;C. The fibre heat accumulation capacity stemming from the phase transition in the PCM was in the range of 3.6 - 19.4 kJ/m<sup>2</sup>, at a thermal regulation factor (TRF) from 0.8 to 0.4.<br/>","Tomaszewski, Waclaw and Twarowska-Schmidt, Krystyna and Moraczewski, Andrzej and Kudra, Michal and Szadkowski, Marek and Palys, Beata",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An Optimal Page-Level Power Management Strategy in PCMDRAM Hybrid Memory","The new emergence of data-intensive applications raises a huge requirement on the capacity of memory. However, an obvious increase of memory makes the corresponding energy consumption unacceptable in practice. To address this question, any effort only to reduce the energy consumption of dynamic random access memory (DRAM) is ineffective. Recently, combining DRAM and phase change memory (PCM) to construct a hybrid main memory is recognized as a promising solution to distinctly reduce the energy consumption. In this paper, we propose a new page-level energy management strategy to optimize the energy consumption of the hybrid main memory. The proposed strategy records pages&rsquo; local and global access information by a new data structure, and then classifies pages by the access history, at last adaptively places PCM or DRAM pages according to the memory characteristics and remaps the migrated pages. Our experimental results show that our strategy can achieve 9.4 % of energy saving and 9.6 % of performance improvement at most compared with APG and PDRAM, which were proposed respectively by conferences RACS&rsquo;12 and DAC&rsquo;09.<br/> &copy; 2015, Springer Science+Business Media New York.","Zhang, Jinbao and Liao, Xiaofei and Jin, Hai and Liu, Dong and Lin, Li and Zhao, Kao",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Climate model genealogy: Generation CMIP5 and how we got there","A new ensemble of climate models is becoming available and provides the basis for climate change projections. Here, we show a first analysis indicating that the models in the new ensemble agree better with observations than those in older ones and that the poorest models have been eliminated. Most models are strongly tied to their predecessors, and some also exchange ideas and code with other models, thus supporting an earlier hypothesis that the models in the new ensemble are neither independent of each other nor independent of the earlier generation. On the basis of one atmosphere model, we show how statistical methods can identify similarities between model versions and complement process understanding in characterizing how and why a model has changed. We argue that the interdependence of models complicates the interpretation of multimodel ensembles but largely goes unnoticed. &copy;2013 American Geophysical Union. All Rights Reserved.<br/>","Knutti, Reto and Masson, David and Gettelman, Andrew",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Improved airfoil polar predictions with data-driven boundary-layer closure relations","The accuracy of airfoil polar predictions is limited by the usage of imperfect turbulence models. Can machine-learning improve this situation? Will airfoil polars teach the effect of turbulence on skin-friction? We try to answer these questions by refining turbulence treatment in the Rfoil code: boundary layer closure relations are learned from airfoil polar data. Two turbulent closure relations, for skin friction and energy shape factor, are parametrized with a class-shape transformation. An experimental database is then used to define code inaccuracy measures that are minimized with an interior point gradient algorithm. Results show that airfoil polars contain exploitable information about turbulent phenomena. Inferred closures agree with direct numerical simulation results of skin friction and the new code predicts drag more accurately. Maximum lift remains under-predicted but Rfoil maintains its robustness and suitability for optimization of wind energy airfoils.<br/> &copy; Published under licence by IOP Publishing Ltd.","De Oliveira, Gael and Pereira, Ricardo and Timmer, Nando and Van Rooij, Ruud",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A Survey on Cross-Project Software Defect Prediction Methods","Software defect prediction firstly analyzes and mines software historical repositories to extract program modules and label them. It secondly designs novel metrics, which have strong correlation with defects, based on the analysis on code complexity or development process. Then it uses these metrics to measure these program modules. It finally uses a specific machine learning algorithm to construct software defect prediction models, which are trained on these datasets. Therefore software defect prediction can optimize the software testing resource allocation by identifying the potential defect modules in advance. However in real software development, a project, which needs defect prediction, maybe a new project or it maybe has less training data. A simple solution is directly using training data from other projects to construct the model. However application domain, development process, used programming language, developer experience of different projects may be not same. This will cause the distribution of corresponding datasets to be large and result in the poor performance of defect prediction. Therefore, how to effectively transfer the knowledge of the source project to build a defect prediction model for the target project has attracted the attention of researchers, and this problem is called cross-project defect prediction (CPDP).We conduct a comprehensive survey on this topic and classify existing methods into three categories: supervised learning based methods, unsupervised learning based methods, and semi-supervised learning based methods. In particular, the supervised learning based methods will use the modules of the source project to construct the model. These methods can be further classified into two categories: homogeneous cross-project defect learning and heterogeneous cross-project defect prediction based on whether the source project and the target project use the same metric set. For the former, researchers design novel methods by using metric value transformation, instance selection and weight setting, feature mapping and selection, ensemble learning, class imbalance learning. For the latter, the issue is more challenging and researchers design novel methods by using feature mapping and canonical correlation analysis. The unsupervised learning based methods will attempt to make a prediction on the modules of the target project immediately. The assumption of these methods is that the metric value of defective modules has the tendency to be higher than the metric value of non-defective modules. Researchers design novel methods by using cluster algorithms. The semi-supervised learning based methods will use the modules of the source project and some labeled programs in the target project together to construct the model. These methods try to improve the performance of CPDP by identifying some representative program modules in the target project and label them manually. Researchers design novel methods by using ensemble learning and TrAdaBoost. We summarize and comment the existing research work for each category in sequence. Then we analyze the commonly used performance metrics and benchmarks in empirical studies in CPDP for other researchers to better design empirical studies. Finally we conclude this paper and discuss some potentially future research work from four dimensions: dataset gathering, dataset preprocessing, CPDP model construction and evaluation, and CPDP model application.<br/> &copy; 2018, Science Press. All right reserved.","Chen, Xiang and Wang, Li-Ping and Gu, Qing and Wang, Zan and Ni, Chao and Liu, Wang-Shu and Wang, Qiu-Ping",2018,"[""Engineering Village""]","Aceito: CA5, CA6","Aceito: CA5"
"Transforming SQLITE to run on a bare PC","SQLITE is a popular small open-source database management system with many versions that run on popular platforms. However, there is currently no version of the SQLITE application that runs on a bare PC. Since a bare PC does not provide any form of operating system (or kernel) support, bare PC applications need to be completely self-contained with their own interfaces to the hardware. Such applications are characterized by small code size, and have inherent security and performance advantages due to the absence of a conventional operating system. We describe a general transformation approach that can be used to transform the SQLITE application to a lean SQLITE application that runs on a bare PC. We present the current state of this work and identify several important issues that need further research.<br/>","Okafor, Uzo and Karne, Ramesh K. and Wijesinha, Alexander L. and Rawal, Bharat S.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Fast feature extraction method for faults detection system","The feature extraction based on machine learning is significant in the detection system. The boundary information, the circumference and the area are the essential features in the identification and the classification of flaws. In order to get those information, this paper proposed a novel algorithm to get the boundary information using the boundary tracking, and to make each flaw independent by establishing a balanced binary search tree for data storage. By scanning the image and the image boundaries based on binarization transformation, there is no need to fill the region, nor need to use the chain code to count the number of regions and the boundary information. According to the established balanced binary search tree, we can calculate the number of the pixel of the area of each fault, the edge information of the boundary, and the circumference. The algorithm has the advantages of fast speed, less computation, better noise suppression and accurate results.<br/> &copy; ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018.","Wang, Hongmin and Zhu, Xiaohui and Niu, Xiangyong and Xue, Ping",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Using qualitative data analysis to measure user experience in a serious game for premed students","The University of Texas Transformation in Medical Education Portal (UT TIME Portal) is a game-based learning platform for select premed students, with a particular emphasis on communication and professionalism. In addition to quantitative data on system usage and user performance, the UT TIME Portal generates rich sets of qualitative data collected through discussion board posts and pre- and post- surveys. Using NVivo 10&rsquo;s built-in tools, our team used this qualitative data to measure game experience outcomes in many ways by building and testing out hypotheses about our user experience design. The ability to tag, code and organize themes to then be analyzed in the context of quantitative data generated by the UT TIME Portal adds an important dimension to understanding the user experience and generates insights not possible to glean from quantitative data alone.<br/> &copy; Springer International Publishing Switzerland 2016.","Zielke, Marjorie A. and Zakhidov, Djakhangir and Jacob, Daniel and Lenox, Sean",2016,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Neural acceleration for generalpurpose approximate programs","As improvements in per-transistor speed and energy efficiency diminish, radical departures from conventional approaches are needed to continue improvements in the performance and energy efficiency of general-purpose processors. One such departure is approximate computing, where error in computation is acceptable and the traditional robust digital abstraction of near-perfect accuracy is relaxed. Conventional techniques in energy-efficient computing navigate a design space defined by the two dimensions of performance and energy, and traditionally trade one for the other. General-purpose approximate computing explores a third dimension-error-and trades the accuracy of computation for gains in both energy and performance. Techniques to harvest large savings from small errors have proven elusive. This paper describes a new approach that uses machine learning-based transformations to accelerate approximation-tolerant programs. The core idea is to train a learning model how an approximable region of code-code that can produce imprecise but acceptable results-behaves and replace the original code region with an efficient computation of the learned model. We use neural networks to learn code behavior and approximate it. We describe the Parrot algorithmic transformation, which leverages a simple programmer annotation (""approximable"") to transform a code region from a von Neumann model to a neural model. After the learning phase, the compiler replaces the original code with an invocation of a low-power accelerator called a neural processing unit (NPU). The NPU is tightly coupled to the processor pipeline to permit profitable acceleration even when small regions of code are transformed. Offloading approximable code regions to NPUs is faster and more energy efficient than executing the original code. For a set of diverse applications, NPU acceleration provides whole-application speedup of 2.3&times; and energy savings of 3.0&times; on average with average quality loss of at most 9.6%. NPUs form a new class of accelerators and show that significant gains in both performance and efficiency are achievable when the traditional abstraction of near-perfect accuracy is relaxed in general-purpose computing.<br/> &copy; 2015 ACM.","Esmaeilzadeh, Hadi and Sampson, Adrian and Ceze, Luis and Burger, Doug",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Automatic optimization of stream programs via source program operator graph transformations","Distributed data stream processing is a data analysis paradigm where massive amounts of data produced by various sources are analyzed online within real-time constraints. Execution performance of a stream program/query executed on such middleware is largely dependent on the ability of the programmer to fine tune the program to match the topology of the stream processing system. However, manual fine tuning of a stream program is a very difficult, error prone process that demands huge amounts of programmer time and expertise which are expensive to obtain. We describe an automated process for stream program performance optimization that uses semantic preserving automatic code transformation to improve stream processing job performance. We first identify the structure of the input program and represent the program structure in a Directed Acyclic Graph. We transform the graph using the concepts of Tri-OP Transformation and Bi-Op Transformation. The resulting sample program space is pruned using both empirical as well as profiling information to obtain a ranked list of sample programs which have higher performance compared to their parent program. We successfully implemented this methodology on a prototype stream program performance optimization mechanism called Hirundo. The mechanism has been developed for optimizing SPADE programs which run on System S stream processing run-time. Using five real world applications (called VWAP, CDR, Twitter, Apnoea, and Bargain) we show the effectiveness of our approach. Hirundo was able to identify a 31.1 times higher performance version of the CDR application within seven minutes time on a cluster of 4 nodes. &copy; 2013 Springer Science+Business Media New York.<br/>","Dayarathna, Miyuru and Suzumura, Toyotaro",2013,"[""Engineering Village""]","Rejeitado: CR10","Rejeitado: CR10"
"Numerical simulation of an U-channel component tailored tempering process considering the effect of stress on the shift of TTT curves","A numerical model of the tailored hot stamping process was developed in the framework of the commercial FE code Forge&trade; and accurately calibrated in order to take into account the influence of applied stress and strain on the phase transformation kinetics. The calibration was carried out by introducing in the numerical model data on the shift of the TTT curves due to applied stress and the transformation plasticity coefficients, which were obtained through an extensive dilatometric analysis. The numerical model was validated through a laboratory-scale hot-formed U-channel produced using a segmented die with local heating and cooling zones. The predicted distribution of Vickers hardness and evolution of microstructure given by the numerical model was compared with the experimental results to show the significant predictive improvements introduced by considering the influence of the transformation plasticity and deformation history on the phase transformation kinetics. &copy; (2014) Trans Tech Publications, Switzerland.","Tang, B.T. and Wang, Q.L. and Bruschi, S.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Proceedings - 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN-W 2016","The proceedings contain 60 papers. The topics discussed include: use of similarity measure to suggest the existence of duplicate user stories in the Srum process; code change history and software vulnerabilities; D-MBTDD: an approach for reusing test artefacts in evolving system; comparing detection capabilities of antivirus products: an empirical study with different versions of products from the same vendors; an application of unsupervised fraud detection to passenger name records; MimeoDroid: large scale dynamic app analysis on cloned devices via machine learning classifiers; error monitoring for legacy mission-critical systems; classifying virtual machine managers by overhead; hunting killer tasks for cloud system through behavior pattern learning; ground control to major faults: towards a fault tolerant and adaptive sdn control network; experience with 3 SDN controllers in an enterprise setting; NetCo: reliable routing with unreliable routers; an architecture for semi-automatic collaborative malware analysis for CIs; quantification and analysis of interdependency in cyber-physical systems; and on the feasibility of distinguishing between process disturbances and intrusions in process control systems using multivariate statistical process control.",,2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"An investigation of the fault-proneness of clone evolutionary patterns","Two identical or similar code fragments form a clone pair. Previous studies have identified cloning as a risky practice. Therefore, a developer needs to be aware of any clone pairs in order to properly propagate any changes between clones. A clone pair may experience many changes during the creation and maintenance of a software system. A change can either maintain or remove the similarity between clones in a clone pair. If a change maintains the similarity between clones, the clone pair is left in a consistent state. When a change makes the clones no longer similar, the clone pair is left in an inconsistent state. The set of states and changes experienced by clone pairs over time form an evolution history known as a clone genealogy. In this paper, we examine clone genealogies to identify fault-prone &ldquo;patterns&rdquo; of states and changes. We explore the use of clone genealogy information in fault prediction. We conduct a quasi-experiment with four long-lived software systems (i.e., Apache Ant, ArgoUML, JEdit, Maven) and identify clones using the NiCad and iClones clone detection tools. Overall, we find that the size of the clone can impact the fault-proneness of a clone pair. However, there is no clear impact of the time interval between changes to a clone pair on the fault-proneness of the clone pair. We also discover that adding clone genealogy information can increase the explanatory power of fault prediction models.<br/> &copy; 2017, Springer Science+Business Media New York.","Barbour, Liliane and An, Le and Khomh, Foutse and Zou, Ying and Wang, Shaohua",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Seismic Evaluation of a Multitower Connected Building by Using Three Software Programs with Experimental Verification","Shanghai International Design Center (SHIDC) is a hybrid structure of steel frame and reinforced concrete core tube (SF-RCC). It is a building of unequal height two-tower system and the story lateral stiffness of two towers is different, which may result in the torsion effect. To fully evaluate structural behaviors of SHIDC under earthquakes, NosaCAD, ABAQUS, and Perform-3D, which are widely applied for nonlinear structure analysis, were used to perform elastoplastic time history analyses. Numerical results were compared with those of shake table testing. NosaCAD has function modules for transforming the nonlinear analysis model to Perform-3D and ABAQUS. These models were used in ABAQUS or Perform-3D directly. With the model transformation, seismic performances of SHIDC were fully investigated. Analyses have shown that the maximum interstory drift can satisfy the limits specified in Chinese code and the failure sequence of structural members was reasonable. It meant that the earthquake input energy can be well dissipated. The structure keeps in an undamaged state under frequent earthquakes and it does not collapse under rare earthquakes; therefore, the seismic design target is satisfied. The integrated use of multisoftware with the validation of shake table testing provides confidence for a safe design of such a complex structure.<br/> &copy; 2016 Deyuan Zhou et al.","Zhou, Deyuan and Guo, Changtuan and Wu, Xiaohan and Zhang, Bo",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Modeling the injection of carbon dioxide and nitrogen into a methane hydrate reservoir and the subsequent production of methane gas on the North slope of Alaska","HydrateResSim (HRS) is an open-source finite-difference reservoir simulation code capable of simulating the behavior of gas hydrate in porous media. The original version of HRS was developed to simulate pure methane hydrates, and the relationship between equilibrium temperature and pressure is given by a simple, 1-D regression expression. In this work, we have modified HydrateResSim to allow for the formation and dissociation of gas hydrates made from gas mixtures. This modification allows one to model the ConocoPhillips Ignik Sikumi #1 field test performed in early 2012 on the Alaska North Slope. The Ignik Sikumi #1 test is the first field-based demonstration of gas production through the injection of a mixture of carbon dioxide and nitrogen gases into a methane hydrate reservoir and thereby sequestering the greenhouse gas CO<inf>2</inf> into hydrate form. The primary change to the HRS software is the added capability of modeling a ternary mixture consisting of CH<inf>4</inf> + CO<inf>2</inf> + N<inf>2</inf> instead of only one hydrate guest molecule (CH<inf>4</inf>), therefore the new software is called Mix3HydrateResSim. This Mix3HydrateResSim upgrade to the software was accomplished by adding primary variables (for the concentrations of CO<inf>2</inf> and N<inf>2</inf>), governing equations (for the mass balances of CO<inf>2</inf> and N<inf>2</inf>), and phase equilibrium data. The phase equilibrium data in Mix3HydrateResSim is given as an input table [1] obtained using a statistical mechanical method developed in our research group called the cell potential method. An additional phase state describing a two-phase Gas-Hydrate (GsH) system was added to consider the possibility of converting all available free water to form hydrate with injected gas. Using Mix3HydrateResSim, a methane hydrate reservoir with coexisting pure-CH<inf>4</inf>-hydrate and aqueous phases at 7.0 MPa and 5.5&deg;C was modeled after the conditions of the Ignik Sikumi #1 test: (i) 14-day injection of CO<inf>2</inf> and N<inf>2</inf> followed by (ii) 30-day production of CH<inf>4</inf> (by depressurization of the well). During the injection phase, the injection well is modeled as a fixed-condition boundary maintained as a gas phase (23% CO<inf>2</inf> + 77% N<inf>2</inf>) at 9.65 MPa and 5.5 &deg;C. Initially, there is an increase in the saturation of hydrate indicating the formation of secondary hydrate due to the injected gas and the available free water. There is also a slight increase in the temperature due to the exothermic reaction of hydrate formation. As the hydrate becomes saturated with the injected gases it releases CH<inf>4</inf>. After the initial 14 days of injection, a mixture of the three gases was produced through depressurization. This was modeled by maintaining the well as a fixed-state boundary at the bottom-hole pressure. The amount of CH<inf>4</inf> released from the hydrate phase during the injection and production phases and the amount of CO<inf>2</inf> and N<inf>2</inf> gases sequestered as hydrates have been examined in this study. A model-based history-matching of the gas flow rates from the ConocoPhillips field test will be conducted to validate the code.<br/> &copy; Copyright 2013, Unconventional Resources Technology Conference (URTeC)","Garapati, Nagasree and McGuire, Patrick and Anderson, Brian J.",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Periodic developer metrics in software defect prediction","Defect prediction studies have proposed several data-driven approaches, and recently, this field has put more emphasis on whether the people factor is associated software defects. Developer metrics can capture experience, code ownership, coding skills and techniques, and commit activities. These metrics have so far been measured at a specified snapshot of the codebase although developer's knowledge on a source module could change over time. In this paper, we propose to measure periodic developer experience with regard to contextual knowledge on files and directories. We extract periodic experience metrics capturing the previous activities of developers on source files and investigate the explanatory effect of these metrics on defects. We also use activity-based (churn) metrics to observe the performance of both metric types on defect prediction. We used two large-scale open source projects, Lucene and Jackrabbit, for model evaluation. We calculate periodic developer experience metrics and churn metrics at two granularity levels: File level and commit level. We build the models using five popular machine learning algorithms in defect prediction literature. The models with the two best performing algorithms are assessed in terms of Precision, Recall, False Positive Rate, and F-measure. The set of metrics that explains software defects the best is also identified using correlation-based feature selection method. Results show that periodic developer experience metrics extracted at file level are good merits for defect prediction, accompanied with churn. When there is not enough data to extract the contextual knowledge of developers on source files, churn metrics play an important role on defect prediction.<br/> &copy; 2018 IEEE.","Ozcan Kini, Seldag and Tosun, Ayse",2018,"[""Engineering Village""]","Rejeitado: CR11","Rejeitado: CR11"
"An empirical study of faults in late propagation clone genealogies","Two similar code segments, or clones, form a clone pair within a software system. The changes to the clones over time create a clone evolution history. In this work, we study late propagation, a specific pattern of clone evolution. In late propagation, one clone in a clone pair is modified, causing the clone pair to diverge. The code segments are then reconciled in a later commit. Existing work has established late propagation as a clone evolution pattern and suggested that the pattern is related to a high number of faults. In this study, we examine the characteristics of late propagation in three long-lived software systems using the Simian (Simon Harris, Victoria, Australia, http://www.harukizaemon.com/simian), CCFinder, and NiCad (Software Technology Laboratory, Queen's University, Kingston, ON, Canada) clone detection tools. We define eight types of late propagation and compare them to other forms of clone evolution. Our results not only verify that late propagation is more harmful to software systems but also establish that some specific types of late propagations are more harmful than others. Specifically, two types are most risky: (1) when a clone experiences diverging changes and then a reconciling change without any modification to the other clone in a clone pair; and (2) when two clones undergo a diverging modification followed by a reconciling change that modifies both the clones in a clone pair. We also observe that the reconciliation in the former case is more prone to faults than in the latter case. We determine that the size of the clones experiencing late propagation has an effect on the fault proneness of specific types of late propagation genealogies. Lastly, we cannot report a correlation between the delay of the propagation of changes and its faults, as the fault proneness of each delay period is system dependent. Copyright &copy; 2013 John Wiley &amp; Sons, Ltd.<br/>","Barbour, Liliane and Khomh, Foutse and Zou, Ying",2013,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR9"
"Suspiciously structured entropy: Wavelet decomposition of software entropy reveals symptoms of malware in the energy spectrum","Sophisticated malware authors can sneak hidden malicious code into portable executable files, and this code can be hard to detect, especially if it is encrypted or compressed. However, when an executable file shifts between native code, encrypted or compressed code, and padding, there are corresponding shifts in the file's representation as an entropy signal. In this paper, we develop a method for automatically quantifying the extent to which the patterned variations in a file's entropy signal makes it ""suspicious."" A corpus of n = 39,968 portable executable files were studied, 50% of which were malicious. Each portable executable file was represented as an entropy stream, where each value in the entropy stream describes the amount of entropy at a particular locations in the file. Wavelet transforms were then applied to this entropy signal in order to extract the amount of entropie energy at multiple scales of code resolution. Based on this entropie energy spectrum, we derive a Suspiciously Structured Entropie Change Score (SSECS), a single scalar feature which quantifies the extent to which a given file's entropie energy spectrum makes the file suspicious as possible malware. We found that, based on SSECS alone, it was possible to predict with 68.7% accuracy whether a file in this corpus was malicious or legitimate (a 18.7% gain over random guessing). Moreover, we found that SSECS contains predictive information not contained in mean entropy alone. Thus, we argue that SSECS could be a useful single feature for machine learning models which attempt to identify malware based on millions of file features.<br/> &copy; 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Wojnowicz, Michael and Chisholm, Glenn and Wolff, Matt",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Incremental dynamic analysis of sdof by using nonlinear earthquake accelerograms based on modified inverse Fourier transform","In evaluation of structures, performing nonlinear response of model over a time analysis or incremental dynamic analysis needs more time. Hence, it can be beneficial if the event history report is carried out with long time steps without loss of accuracy. This study includes a method to simplify of accelerograms meant on the change of their Fourier reports. So, the Fourier Spectrum of the accelerogram is initially determined. Next applying a PC code generation, the similar Inverse Fourier Convert is computed utilizing a comparatively large time stage, depending on the structure&rsquo;s times; that is ordinarily five to ten times bigger than primary accelerogram&rsquo;s duration stage to generate the visible accelerogram. This application from the simplified accelerogram apparently takes much less time. Results indicates that the analysis time can be reduced up to 80 % by using the proposed method. While the maximum response shows an error of merely five to ten percent, about the sort of structure and the characteristics of the records used.<br/> &copy; JVE INTERNATIONAL LTD.","Faroughi, Alireza and Hosseini, Mahmood",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Design and analysis of online micro-course of Garden Architecture design based on CPC model","In the network information age, micro-course is a brand-new teaching method which integrates conciseness, vividness, visualizability and distinctiveness. It is also a means of educational informationization. Especially in architecture design course, micro-course has been effectively applied. Based on CPC model, online micro-course of Garden Architecture Design was constructed again by combining patent 2D code technology in this paper. The design and analysis of online micro-course of Garden Architecture Design based on CPC model can basically achieve classroom flipping, promote equal interaction between teachers and students, enhance communications among classmates, feedback and evaluate teaching effect in time, promote students' learning interest, change students' learning attitude and boost their academic performance so as to reach the expected teaching objective.<br/>","Liu, Lili",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events","Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https://github.com/eracah/hur-detect.<br/> &copy; 2017 Neural information processing systems foundation. All rights reserved.","Racah, Evan and Beckham, Christopher and Maharaj, Tegan and Kahou, Samira Ebrahimi and Prabhat and Pal, Christopher",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"2013 International Conference on Sensors, Mechatronics and Automation, ICSMA 2013","The proceedings contain 245 papers. The special focus in this conference is on Sensors, Mechatronics and Automation. The topics include: A novel method to dicing anodically bonded silicon glass MEMS wafers based on UV laser technique; a microfluidic device for sample pretreatment by laminar flow extraction; mechanical properties and microstructure analysis of aluminum and steel dissimilar material weld joint; experimental study on axial tensile performance of low volume fraction of ternary hybrid fiber reinforced concrete; mechanical properties of nanocrystalline coatings prepared by thermal spraying process; preparation and ascertainment of ingredient contents of uranium ore sandstone standard substance; study on corrosion behavior of A3 carbon steel in the manicipal sludge; study on effect of high oxygen treatment on storage quality and anti-browning of fresh-cut apples; study on wall slip in pipeline flow of dense paste; adsorption characteristic research of sediments of lake baiyangdian; high temperature deformation behavior and dynamic recrystallization law of 33Mn2V steel for oil well tube; finite element analysis of 1-3-2 piezoelectric composite resonance frequency characteristics; study on novel relaxor ferroelectric single crystal PMNT/epoxy composite transducer array; influence of sulfur on the long-term durability of SOFC cathode; distribution function for nonlinear potential gradient high pressure plasma; analysis of copper wires short circuited melted mark; a new approach for autonomous robot obstacle avoidance using PSD infrared sensor combined with digital compass; design and build of a nine-axis digital output gyroscope-based percutaneous puncture ultrasonic navigation system; construction principle and application test of 220kV self-healing optical voltage sensor; a survey on QoS-aware secure data transmission in multimedia sensor networks; the influence of implantable actuators on residual hearing; wireless sensor networks for structural healthy monitoring applications-a design proposal; dynamic estimation of water hyacinth area by fusing data from satellite and GPS sensors; optimal ecological restoration of degraded wetland ecosystem by using satellite sensors; PZT based ultrasonic wave force detecting sensor; research on routing protocol for large-scale wireless sensor networks; research on wireless MAC protocols oriented to smart clothing; sensor node identifier resolution of IP-based and non-IP-based WSN; study on the conductivity of DMEM by PASCO sensors; study on transient temperature generator and dynamic compensation technology; substation equipment temperature monitoring system based on heterogeneous wireless sensor network; tamper detection in RFID tags using zero-watermarks; tuning characteristics of DFB diode laser and its application to TDLAS gas sensor design; theoretical modeling and error analysis for reflection structure fiber optical current transformer; the calibration techniques of paper basis weight sensor; modeling of collaborative context-awareness for smart home; sensor fault diagnosis based on SOFM neural network; the design of a basketball goal automatic detector; numerical analysis of the effects caused by ground reflection on aperture coupling; acoustic wave resonance of stimulated brillouin scattering in a single mode fiber ring; rules of harmonic longitudinal propagation in various voltage classes; study on the multi-optical theodolites positioning method based on function restriction; weight size determined variable step size LMS method for identifying under damped systems; a fuzzy curve detection method based on lateral acceleration; research and performance analysis on pedestrian detection based on optimized hog and SVM; a new method for calculating the volume of debris flow using GIS and RS technology; design of thermocouple verification data records table based on labview; development of a transducer for dental implant stability measurement; temperature influence on real-time ultrasonic detection of multi-planetary gear transmission; application of frequency-voltage converter chip in the vehicle data acquisition; multi-point temperature monitoring system for the LNG storage tank; research on the impact of CNC servo dynamic characteristic for the on-line detection precision; weak signal detection and application in electronic experiment based on virtual instrument; design of a constant probing force system for profilometer; research of film thickness online measurement system; a new method based on lyapunov exponent to determine the threshold of chaotic systems; application of barcode information and acquisition system for the machining process technology; capability assessment of satellite communication system based on multi-granular modeling; research on discharge signals feature extraction of sealing packaging food existing pinholes based on EEMD; dynamic displacement response characteristics of a solenoid actuator for electromagnetic separation; error processing techniques of integral velocity based on frequency spectrum forecasting; FH-FSK underwater acoustic control and communication system based on LDPC code; monitoring system for plant growth environment based on zigbee/RS485; feature extraction of the small leakage diagnosis of oil pipeline based on acoustic signal; pseudo two-hop distributed consensus with noise; research on MWD mud pulse signal waveform identification algorithm; ultralow frequency standard vibration calibration system based on DSP; analysis on graduate employment based on data warehouse; air quality evaluation based on local normalized image contrast; inexact distributed reconstruction via alternating direction method; reasoning method of remote sensing imagery based on topological transformation; extraction of the palm vein texture features based on Gabor wavelet transforms; facial expression recognition based on monogenic binary coding; HVS-based low bit-rate image compression; a novel method of macro block motion prediction using particle filter; an effective multi-feature extraction method for manually plotted well-log curve line; an image segmentation method of underwater targets based on active contour model; removal of glass reflection using TV-retinex model; recognition method of cotton blind stinkbug hazard level based on image processing; stereoscopic images enhancement based on edge sharpening of wavelet coefficients; the bistatic MIMO imaging algorithm based on waveform diversity; the algal blooms prediction in the lakes based on remote sensing information; the image information recognition of large structures based on CAD model characteristics; topological equivalence in multi-scale remote sensing image segmentation; study on regression model of measuring weld position; the change and management for surveying achievement of real estate using GIS techno logy; intelligent video surveillance system based on sound source localization technology; decision method of optimal X-ray digital imaging parameters; gesture acquisition and tracking with kinect under complex background; a novel approach of edge detection based on vector morphology; edge detection of color images based on improved morphological gradient operators; the filtering and streamline of three-dimensional point-cloud data; compressive strength analysis of MWD microchip tracer; analysis of mechanical fatigue behavior for MEMS structures; feature based parametric design of piston reliability; numerical study on effect of wall on upwelling flow; influence of technical parameters on contact pressure in straight bevel gear meshing; modeling of electrostatic chuck and simulation of electrostatic force; research on finite element static analysis in mechanical vehicle structure design; research on vault plate installation technology of LNG storage tank; seal optimization technology research of large LNG storage tank dome gas lift; study on elevator drive system dynamics simulation of rail transport conveyer; multiple instance space description method in the application research of complex product conceptual design; design of multi-mode PID controller and application in time-delay process; remote household appliance control system based on internet; the simulation investigation of matlab in intelligent control course; optimal power management of final load and electrolyser in a solar hydrogen power generation system; design of absorbing wave maker control system based on trio controller; design of a chaos phenomenon exhibition system; dynamic modeling of a new version of cylindrical planetary gear used for a robotic arm; modular design based on design rules; study on the automatic spraying system of dust suppressant and speed adaptive algorithms; research and design of shortwave and ultra-short wave SDR engineering in urban emergency system; design of the embedded control system of laser engraving machine; research of IOT in the application of wisdom agricultural; research of RAM on the all-electronic computer interlocking system; the research on embedded database system of wireless network; drive characteristic analysis and test system design for water hydraulic artificial muscle; a type of equipment TV goniometer angle deviation signal simulation research; design of a new dual-core intelligent PLC for Ethernet communication; design and implementation of an automatic hydrological telemetry system; improvement and design of single coil vibrating-wire sensor signal acquisition system based on STM32; application of three-dimensional fine geological modeling in complex fault-block reservoir with low permeability; decentralised sanitation and energy reuse in the built environment; the empirical research on industry informatization; manufacturing defects and safety technology of automobile product; research on SSCR technology to reduce NOx emissions for diesel engines; a new technique to improve estimation of position for serial robots.",,2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Do Planning and Design Policies and Procedures Matter In Microclimate Management and Urban Heat Mitigation?","In this research, I developed a method for analyzing how urban form affects urban microclimate and how planning and design policies shape urban form. I scrutinized the policies, contexts, and implementation procedures of urban redevelopment projects in two cities in the Denver metropolitan area. Both the Belmar (located in Lakewood, Colorado) and 29th Street Mall (located in Boulder, Colorado) projects were conventional indoor malls developed in the 1960s, that declined in the 1990s, and were redeveloped in the early 2000s to create mixed-use walkable urban centers. The zoning approaches (Belmar used Form-based code, 29th Street Mall used Euclidian), design guidelines, and local politics of these two projects were significantly different in ways that resulted in different built environments after redevelopment. My research aim is to explore how these differences can potentially impact urban climate systems with positive or negative influences on climate variables such as wind, ambient temperature and mean radiant temperature. My research answers two research questions: (1) to what extent are different zoning approaches (Euclidian and Form-based) capable of mitigating urban heat? (2) To what extent are planning contexts, including local politics, important in developing a climate responsive project? I found that a series of variables affected the process of planning and design in these sites. The findings show that the choices made in the development management affected the microclimate of both sites. The built form of Belmar is more effective in heat mitigation and creating a more comfortable temperature. Based on the results of both my microclimate simulations and the policy analysis, I identified five main themes in the development management of both sites that control microclimate outcomes and show why Belmar ultimately was a better project. These themes, which are also relevant for other environmental objectives, are: (1) urban vision, (2) land use and building form controls, (3) design guidelines, (4) public financing, and (5) condemnation/ownership factors. These five policy themes I have identified explain how a combination of context and choice variations affect the quality of built environments. Although many regulations did not intentionally address microclimate issues, elements that were considered for improving walkability contributed to heat mitigation as well. The simulation of policy and form variations showed that the built environment of Belmar has been more successful in mitigating urban heat. Conflicts and a complex planning history in Boulder led to a very slow and ineffective review process that created a less climate responsive built environment. ProQuest Subject Headings: Urban planning, Geographic information science and geodesy, Climate change.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.","Heris, Mehdi Pourpeikari",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Interpretation of non-isothermal testing data based on the numerical simulation","The goal of this paper is demonstration of an approach to enhancing reliability of quantitative interpretation of well tests data. The new approach comprises joint interpretation of transient temperature, pressure and flow rate data taking into account flow rate history before the test. It provides additional information about near-wellbore zone, flow profiles and hydrodynamic properties of a multilayer reservoir. The transient model for solving the coupled thermal-hydraulic problems in the reservoir, formation, and wellbore is described in the paper. It takes into account heat convection and conduction, Joule- Thomson, adiabatic effects, thermal effect caused by degassing as well as mixing of fluids in the well, heat transfer between wellbore fluid and formation, frictional heat release and thermal effect of fluid compression or expansion. This model allows simulating flow behind casing, crossflows during shut-in and different properties of the near-wellbore zone. Transient temperature and pressure fields, and temperature profiles along the wellbore are simulated for a given flow rate history. The model parameters are determined by solving the inverse problem based on a comparison of simulated temperature data with measurements in the wellbore. Application of new approach to quantitative interpretation of well test data and developed transient numerical code are demonstrated on the example of field data from the Bashneft - Polus oil well. The data from production logging during flow at different choke sizes and transient station measurements of temperature and pressure after change of regimes are available and simulated with the developed code. Standard Pressure Transient Analysis (PTA) was used for pressure and flow rate data interpretation, while available temperature measurements were simulated with a numerical code developed for solving the coupled thermal-hydraulic multiphase problems by Bashkir State University and Schlumberger Moscow Research center. The paper demonstrates the results of quantitative interpretation of the field data and sensitivity study of the model parameters to uncertainties in measurements, flow rate history, thermal properties and other input data.<br/> Copyright 2015, Society of Petroleum Engineers.","Valiullin, Rim and Ramazanov, Ayrat and Khabirov, Timur and Sadretdinov, Alexander and Kotlyar, Lev and Sidorova, Mariya and Fedorov, Vyacheslav and Salimgareeva, Elmira",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Tutorial on state variable based plasticity: An Abaqus UHARD subroutine","Since plasticity is path dependent, it is necessary to properly take into account the deformation, strain rate and temperature history in applications such as crash worthiness and ballistics simulations. To accurately model the evolution of the yield stress, the incremental (differential) update from a previous converged time step is required instead of a closed form expression that relates flow stress to plastic strain. Elastoviscoplastic models that make use of state variables better capture the physical phenomenon of a perceived lag between a change in strain rate or temperature and the subsequent stress response. It is impossible to capture this when making use of a closed form expression or data table based method. One model that makes use of an evolving state variable is the Mechanical Threshold Stress (MTS) model. In this paper, the implementation of the MTS model into an Abaqus user hardening (UHARD) subroutine is discussed and the code is included. The aim is not to improve on the current knowledge of the model, but to illustrate the ease with which a state variable based plasticity model can be implemented and used instead of an empirical (closed form expression) or data table based method. The MTS model is compared to the Johnson-Cook plasticity model which takes the form of a simple closed form expression relating yield stress to plastic strain as a function of temperature and strain rate. The model parameters are calibrated using isothermal, constant strain rate experimental data and then used to predict the stress response for a strain rate jump test and a temperature change test.<br/> &copy;SACAM 2012.","Van Rensburg, Jansen G.J. and Kok, S.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Coronal heating by the partial relaxation of twisted loops","Context. Relaxation theory offers a straightforward method for estimating the energy that is released when continual convective driving causes a magnetic field to become unstable. Thus, an upper limit to the heating caused by ensembles of nanoflaring coronal loops can be calculated and checked against the level of heating required to maintain observed coronal temperatures (T &gsim; 10<sup>6</sup>K). Aims. We present new results obtained from nonlinear magnetohydrodynamic (MHD) simulations of idealised coronal loops. All of the initial loop configurations discussed are known to be linearly kink unstable. The purpose of this work is to determine whether or not the simulation results agree with Taylor relaxation, which will require a modified version of relaxation theory applicable to unbounded field configurations. In addition, we show for two cases how the relaxation process unfolds. Methods. A three-dimensional (3D) MHD Lagrangian-remap code is used to simulate the evolution of a line-tied cylindrical coronal loop model. This model comprises three concentric layers surrounded by a potential envelope; hence, being twisted locally, each loop configuration is distinguished by a piecewise-constant current profile, featuring three parameters. Initially, all configurations carry zero-net-current fields and are in ideally unstable equilibrium. The simulation results are compared with the predictions of helicity-conserving relaxation theory. Results. For all simulations, the change in helicity is no more than 2% of the initial value; also, the numerical helicities match the analytically-determined values. Magnetic energy dissipation predominantly occurs via shock heating associated with magnetic reconnection in distributed current sheets. The energy release and final field profiles produced by the numerical simulations are in agreement with the predictions given by a new model of partial relaxation theory: the relaxed field is close to a linear force free state; however, the extent of the relaxation region is limited, while the loop undergoes some radial expansion. Conclusions. The results presented here support the use of partial relaxation theory, specifically, when calculating the heating-event distributions produced by ensembles of kink-unstable loops. The energy release increases with relaxation radius; but, once the loop has expanded by more than 50%, further expansion yields little more energy. We conclude that the relaxation methodology may be used for coronal heating studies. &copy; 2013 ESO.<br/>","Bareford, M.R. and Hood, A.W. and Browning, P.K.",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Vdiff: A program differencing algorithm for Verilog hardware description language","During code review tasks, comparing two versions of a hardware design description using existing program differencing tools such as diff is inherently limited because these tools implicitly assume sequential execution semantics, while hardware description languages are designed to model concurrent computation. We designed a position-independent differencing algorithm to robustly handle language constructs whose relative orderings do not matter. This paper presents Vdiff, an instantiation of this position-independent differencing algorithm for Verilog HDL. To help programmers reason about the differences at a high-level, Vdiff outputs syntactic differences in terms of Verilog-specific change types. The quantitative evaluation of Vdiff on two open source hardware design projects shows that Vdiff is very accurate, with overall 96.8 % precision and 97.3 % recall when using manually classified differences as a basis of comparison. Vdiff is also fast and scalable-it processes the entire revision history of nine open projects all under 5.35 minutes. We conducted a user study with eight hardware design experts to understand how the program differences identified by the experts match Vdiff's output. The study results show that Vdiff's output is better aligned with the experts' classification of Verilog changes than an existing textual program differencing tool. &copy; 2012 Springer Science+Business Media, LLC.<br/>","Duley, Adam and Spandikow, Chris and Kim, Miryung",2012,"[""Engineering Village""]","Rejeitado: CR12","Rejeitado: CR12"
"Mathews' diagram and Euclid's line - Fifty years ago -","Making the science and technology of computer music comprehensible to musicians and composers who had little or no background therein was a part of Max Mathews' genius. In this presentation I will show how a simple diagram led to the essential understanding of Claude Shannon's sampling theorem, which in turn opened up a conceptual path to composing music for loudspeakers that had nothing to do with wires, cables and electronic devices, but led to learning how to program a computer- to write code. The change from device-determined output (analog) to program-determined output (digital) was a major change in paradigm that led to my realization of an integral sound spatialization system that would have been impossible for me to achieve in any other medium. Along the way, the discovery of FM Synthesis provided not only a means of creating diverse spectra but coupled with a ratio from Euclid's Elements produced an unusual and productive connection between spectral space and pitch space and a path that leads &mellip;? Copyright:<br/> &copy; 2014 Curtis Roads.","Chowning, John",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical analysis of the counter-intuitive dynamic behavior of the elastic-plastic pin-ended beams under impulsive loading with regard to linear hardening effects","The counter-intuitive behavior where the permanent deflection of elastic-plastic beam come to rest in the opposite direction of the impulsive loading, normally appears and disappears abruptly, in certain small ranges of loading and structural parameters. One of the most important issues in the study of this phenomenon is the determination of the influence of different parameters. This work is aimed to study the effects of hardening in counter-intuitive dynamic behavior of elastic-plastic pin-ended beams under impulsive loading. This has been done by developing the proposed Galerkin numerical model and presenting a novel algorithm. The Galerkin method as well as the commercial finite element code ANSYS/LS-DYNA is applied to study this phenomenon. In order to account for the hardening effects in Galerkin method, a new algorithm is proposed. The time history curves for mid-span of the beam is studied in detail and the region of the occurrence of the counter-intuitive behavior is determined. Furthermore, using the finite element software, energy diagrams of the beam are also derived. It has been found that the counter-intuitive behavior is a phenomenon, which is very sensitive to loading, therefore it may appear with a little change in the amount of loading. The results also show that although both methods predict one continuous region of loading for the occurrence of this phenomenon in elastic-perfectly plastic beams, still there are two continuous distinct regions of loading, when considering the hardening effects, for this phenomenon. In addition, this anomalous behavior would occur in the proper ratios of kinetic to internal energy and when considering the linear hardening effects, the possibility of the occurrence of the counter-intuitive behavior exists in a wider domain of energy ratio.<br/> &copy; IMechE 2018.","Shams Alizadeh, Mehdi and Heidari Shirazi, Kourosh and Moradi, Shapour and Sedighi, Hamid Mohammad",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Multiple Lower BAC offenders: Characteristics and response to remedial interventions","Background: In recent years, there has been increasing attention to &ldquo;lower BAC&rdquo; drinking drivers, typically those whose blood alcohol content (BAC) is under the legal limits defined in criminal law. In 2009, legislation was enacted in Ontario, Canada that enabled police to issue roadside license suspensions to individuals caught driving with BAC between 0.05% and 0.08%, known as the &ldquo;warn range&rdquo;. Multiple warn range (MWR) offenders are required to attend the Back on Track (BOT) remedial measures program. This study aimed to provide: (1) a preliminary characterization of MWR drivers charged under warn range legislation; and (2) an initial assessment of outcomes associated with BOT participation among MWR offenders. Methods: A subsample of 727 MWR offenders was drawn from program records, and compared to samples of 3597 first-time Criminal Code (CC) offenders (those caught driving with a BAC of 0.08% or higher) and 359 second-time CC offenders. To provide an initial assessment of outcomes associated with BOT participation, another subsample consisted of 394 MWR participants from whom pre- and post-workshop questionnaires were collected and successfully matched using probabilistic matching processes. Results: Similarities in demographic profile and driving history between MWR and first-time CC participants were apparent. MWR offenders scored higher on risk of problem drinking and drink-driving recidivism than either of the CC offender groups. Second-time CC offenders scored higher on these measures than first-time CC offenders. Following BOT participation, MWR participants demonstrated positive change including improved knowledge of and intentions to avoid drink-driving. Conclusions: MWR offenders share a similar demographic profile to that of first-time CC offenders and they report significantly higher risk of problem drinking and recidivism. MWR offenders may include high-functioning problem drinkers who are likely to continue drink-driving and who may escalate to a CC drink-driving offense. Like CC offenders, MWR offenders benefited from BOT participation.<br/> &copy; 2018","Wickens, Christine M. and Flam-Zalcman, Rosely and Stoduto, Gina and Docherty, Chloe and Thomas, Rita K. and Watson, Tara Marie and Matheson, Justin and Mehra, Kamna and Mann, Robert E.",2018,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical simulation of the aerodynamic behavior of high velocity trains under synthetic crosswinds of different shear and turbulence characteristics","A numerical simulation of the aerodynamic behavior of high-speed trains under synthetic crosswinds at a 90&deg; yaw angle is presented. The train geometry is the aerodynamic train model (ATM). Flow description based on numerical simulations is obtained using large eddy simulation (LES) and the commercial code ANSYS-Fluent V14.5. A crosswind whose averaged velocity and turbulence characteristics change with distance to the ground is imposed. Turbulent fluctuations that vary temporally and spatially are simulated with TurbSim code. The crosswind boundary condition is calculated for the distance the train runs during a simulation period. The inlet streamwise velocity boundary condition is generated using Taylor's frozen turbulence hypothesis. The model gives a time history of the force and moments acting on the train; this includes averaged values, standard deviations and extreme values. Of particular interest are the spectra of the forces and moments, and the admittance spectra. For comparison, results obtained with LES and a uniform wind velocity fluctuating in time, and results obtained with Reynolds averaged Navier Stokes equations (RANS), and the averaged wind conditions, are also presented.<br/> &copy; Civil-Comp Press, 2014.","Garcia, J. and Munoz, J. and Jimenez, A. and Migoya, E. and Crespo, A.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Schema Evolution Survival Guide for Tables: Avoid Rigid Childhood and Youre En Route to a Quiet Life","In this paper, we study the factors that relate to the survival of a table in the context of schema evolution in open-source software. We study the history of the schema of eight open-source software projects that include relational databases and extract patterns related to the survival or death of their tables. Our study shows that the probability of a table with a wide schema (i.e., a large number of attributes) being removed is systematically lower than average. Activity and duration are related to survival too. Rigid tables, without any change to their schema, are more likely to be removed than tables that sustain changes. Durations of dead and survival tables demonstrate a mirror image: dead tables&rsquo; durations are mostly short, whereas survivor tables gravitate toward higher durations. Our findings are mostly summarized by a pattern, which we call electrolysis pattern, due to its diagrammatic representation, stating that dead and survivor tables live quite different lives: tables typically die shortly after birth, with short durations and mostly no updates, whereas survivors mostly live quiet lives with few updates&mdash;except for a small group of tables with high update ratios that are characterized by high durations and survival. Equally important is the evidence that schema evolution suffers from the antagonism of gravitation to rigidity, i.e., the tendency to minimize evolution as much as possible in order to minimize the resulting impact to the surrounding code. Several factors contribute to this observation: the absence of long durations in removed tables, the low percentage of tables whose schema size is scaled up or down, and the low numbers of tables with a high rate of updates, contrasted to the high numbers of tables with zero or few updates. We complement our findings with explanations and recommendations to developers.<br/> &copy; 2017, Springer-Verlag GmbH Germany.","Vassiliadis, Panos and Zarras, Apostolos V.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Bowl Shape Design Optimization for Engine-Out PM Reduction in Heavy Duty Diesel Engine","This paper shows development challenges for 6 liter heavy duty off-road diesel engines to meet the Tier4 final emission regulations with a base diesel engine compliant with Tier4 interim emission regulations. Even if an after-treatment system helps to reduce emissions, quite amount of particulate matters (PM) reduction is still necessary since a diesel particulate filter (DPF) system is supposed to be excluded in Tier4 final diesel engine. The objective of this research is to see if the base engine has a feasibility to meet Tier4 final emission regulations by a change of piston bowl geometry without DPF. Quite amount of PM can be reduced by piston bowl geometry because piston bowl geometry is a very important part that enhances air and fuel mixing process that help the combustion process. Local air to fuel ratio and mixing rate could be improved by manipulating the gas motions of in-cylinder and PM could be reduced In this study, multidimensional CFD code KIVA-3V coupled with detailed chemical kinetics is used to perform combustion simulations. The validation between engine experiment results and simulation results was established at first. Then, with visualized in-cylinder images analysis, geometrical parameters were changed from the conceptual ULPC (Ultra Low Particulate matter Combustion) piston bowl geometry. After several iterations of analysis and geometrical parameter changes, main geometrical parameters were finally found and optimized piston bowl geometry was obtained. To prove the simulation results, experiments were performed with installation of final version of combustion bowl geometry in C1-8 mode which is one of main test cycle to meet Tier4 final emission regulations. Finally, Tier4 final emission regulations were met with changing the piston bowl geometry without DPF. Copyright &copy; 2015 SAE International.","Lee, Jongyoon and Lee, Sangyul and Kim, Jungho and Kim, Duksang",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A peculiar of star in the Local Group galaxy IC 1613","Context. Results from the theory of radiatively driven winds are nowadays incorporated in stellar evolutionary and population synthesis models, and are used in our interpretation of the observations of the deep Universe. Yet, the theory has been confirmed only until Small Magellanic Cloud metallicities. Observations and analyses of O-stars at lower metallicities are difficult, but much needed to prove the theory. Aims. We have observed GHV-62024, an O6.5 IIIf star in the low-metallicity galaxy IC 1613 (Z &asymp; 0.15 Z<inf>&odot;</inf>) to study its evolution and wind. According to a previous preliminary analysis that was subject to significant restrictions this star could challenge the radiatively driven wind theory at low metallicities. Here we present a complete analysis of this star. Methods. Our observations were obtained with VIMOS at VLT, at R &asymp; 2000 and covered approximately between 4000 and 7000 A&ring;. The observations were analysed using the latest version of the model atmosphere code FASTWIND, which includes the possibility of calculating the N iii spectrum. Results. We obtain the stellar parameters and conclude that the star follows the average wind momentum-luminosity relationship (WLR) expected for its metallicity, but with a high value for the exponent of the wind velocity law, &beta;. Comparing this with values of other stars in the literature, we suggest that this high value may be reached because GHV-62024 could be a fast rotator seen at a low inclination angle. We also suggest that this could favour the appearance of the spectral ""f""-characterictics. While the derived &beta; value does not change by adopting a lower wind terminal velocity, we show that a wrong V<inf>&infin;</inf>has a clear impact on the position of the star in the WLR diagram. The N and He abundances are very high, consistent with strong CNO mixing that could have been caused by the fast rotation, although we cannot discard a different origin with present data. Stellar evolutionary model predictions are consistent with the star being still a fast rotator. We find again the well-known mass-discrepancy for this star. Conclusions. We conclude that the star follows the WLR expected for its metallicity. The results are consistent with GHV-62024 being a fast rotator seen close to pole-on, strongly contaminated at the surface with CNO products and with a wind structure altered by the fast rotation but without modifying the global WLR. We suggest that this could be a general property of fast rotators. &copy; 2012 ESO.<br/>","Herrero, A. and Garcia, M. and Puls, J. and Uytterhoeven, K. and Najarro, F. and Lennon, D.J. and Rivero-Gonzalez, J.G.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A review of seismic hazard assessment studies and hazard description in the building codes for Egypt","Reduction of damage in earthquake-prone areas requires modern building codes that should be continuously updated to reflect the improvement in our understanding of the physical effects of earthquake ground shaking on buildings and the increase in the quality and amount of seismological and tectonic studies, among other factors. This work reviews the published seismic hazard assessments available for Egypt as well as the seismic actions included in the building codes, in order to show the state-of-the-art of the seismic hazard assessment studies for the country. The review includes the history and development of seismic hazard assessments and the adoption of seismic building codes in Egypt. All the previous studies were analyzed in order to conclude that a new seismic hazard assessment according to the state-of-the-art is desirable, as well as a change in the hazard description for the actual Egyptian building code.<br/> &copy; 2015, Akad&eacute;miai Kiad&oacute;.","Sawires, Rashad and Pelaez, Jose A. and Fat-Helbary, Raafat E. and Ibrahim, Hamza A.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Multidimensional spectral hashing","With the growing availability of very large image databases, there has been a surge of interest in methods based on ""semantic hashing"", i.e. compact binary codes of data-points so that the Hamming distance between codewords correlates with similarity. In reviewing and comparing existing methods, we show that their relative performance can change drastically depending on the definition of ground-truth neighbors. Motivated by this finding, we propose a new formulation for learning binary codes which seeks to reconstruct the affinity between datapoints, rather than their distances. We show that this criterion is intractable to solve exactly, but a spectral relaxation gives an algorithm where the bits correspond to thresholded eigenvectors of the affinity matrix, and as the number of datapoints goes to infinity these eigenvectors converge to eigenfunctions of Laplace-Beltrami operators, similar to the recently proposed Spectral Hashing (SH) method. Unlike SH whose performance may degrade as the number of bits increases, the optimal code using our formulation is guaranteed to faithfully reproduce the affinities as the number of bits increases. We show that the number of eigenfunctions needed may increase exponentially with dimension, but introduce a ""kernel trick"" to allow us to compute with an exponentially large number of bits but using only memory and computation that grows linearly with dimension. Experiments shows that MDSH outperforms the state-of-the art, especially in the challenging regime of small distance thresholds. &copy; 2012 Springer-Verlag.<br/>","Weiss, Yair and Fergus, Rob and Torralba, Antonio",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Numerical analysis of dynamic response and in-site monitoring of wind power tower considering the influence of the fluctuating wind load","Based on the finite element model, rotor-cabin-tower, of wind power tower, dynamic response under the storm load is analyzed, The stress, displacement and internal force of the wind power tower are calculated based on the calculation method of regulation and time-history. Change rules of mechanical characteristics of wind power tower influenced by the worst wind direction, and field monitoring under the common wind load are analyzed. The results show that the stress, displacement and internal force of dynamic analysis are greater than that calculated by regulation. The maximum stress value is smaller than the minimum permissible stress, however the displacement at the top of wind power tower is greater than allowable deformation in code. The maximum shear and bending moment influenced by the worst wind direction of the storm increased 91% and 106%. Simulation results have a good agreement with the monitoring results, and the monitoring results are greater than the simulation results.<br/>","Huang, Shuai and Song, Bo and He, Wenshan and Wei, Wei",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Modeling the hot-dense plasma of the solar interior in and out of thermal equilibrium","The developments in helioseismology ensure a wealth of studies in solar physics. In particular, with the high precision of the observations of helioseismology, a high-quality solar model is mandated, since even the tiny deviations between a model and the real Sun can be detected. One crucial ingredient of any solar model is the thermodynamics of hot-dense plasmas, in particular the equation of state. This has motivated efforts to develop sophisticated theoretical equations of state (EOS). It is important to realize that for the conditions of solar-interior plasmas, there are no terrestrial laboratory experiments; the only observational constraints come from helioseismology. Among the most successful EOS is so called OPAL EOS, which is part of the Opacity Project at Livermore. It is based on an activity expansion of the quantum plasma, and realized in the so-called ""physical picture"". One of its main competitor is the so called MHD EOS, which is part of the international Opacity Project (OP), a non-classified multi-country consortium. The approach of MHD is via the so-called ""chemical picture"". Since OPAL is the most accurate equation of state so far, there has been a call for a public-domain version of it. However, the OPAL code remains proprietary, and its ""emulation"" makes sense. An additional reason for such a project is that the results form OPAL can only be accessed via tables generated by the OPAL team. Their users do not have the flexibility to change the chemical composition from their end. The earlier MHD-based OPAL emulator worked well with its modifications of the MHD equation of state, which is the Planck-Larkin partition function and its corresponding scattering terms. With this modification, MHD can serve as a OPAL emulator with all the flexibility and accessibility. However, to build a really user-friendly OPAL emulator one should consider CEFF-based OPAL emulator. CEFF itself is already widely used practical EOS which can be easily implemented within any solar model code. In the present work we have carried the technique of the MHD-based OPAL emulator to the CEFF-based OPAL emulator and successfully accomplished this goal. At the same time, we went beyond the earlier work by adding more terms. In particular, the previous MHD-based OPAL emulator was restricted to the approximation of a hydrogen plasma; our work is extended to the more realistic H-He mixture. In a separate part of the present work, we have examined a non-equilibrium effect in the solar interior. The effect is located in the zone of the sharp transition between the differential rotation in the convection zone and the solid-sphere rotation in the radiation zone beneath it. This transition was discovered by helioseismology in the 1980s, and the transition zone is called the solar tachocline. The tachocline is subject to strong shear and in many theories of the solar dynamo it plays important role. Being inspired by the well-known Soret effect, which states the mass diffusion drive by a temperature gradient, we have examined if there could also be mass diffusion by a shear flow. If such an effect were to exist, it would have potential applications to the solar tachocline and dynamo. We have run a so-called reverse non-equilibrium molecular dynamics (RNEMD) simulation. As a test, we first confirmed the Soret effect by the simulation, then we tested for a shear- driven analogous effect. As a result, we did not see the shear-driven Soret effect in our simulation. We do observe the normal Soret effect due to the temperature gradient caused by the numerical scheme we used. ProQuest Subject Headings: Astrophysics, Plasma physics.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.","Lin, Hsiao-Hsuan",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The influence on products yield and feedstock conversion of feedstock injection position along the industrial riser","The fluid catalytic cracking (FCC) process is at the heart of a modern refinery oriented toward maximum gasoline and diesel production. Within the entire refinery process, this process offers the greatest potential for increasing profitability; even a small improvement in the gasoline yield it implies a substantial economical profit when dealing with a production of millions of barrels of gasoline a day. There are several articles published in the last two decades focusing the attention on 2-D and 3-D computational fluid dynamic models of the industrial riser of a circulating fluidized bed. Nevertheless, there are few research works published in the literature that include studies on how the localization of feedstock along the riser affects the yield products. A 3D hydrodynamic model coupled with a 12 lump kinetic model is presented in this work. Four different injection points in an FCC industrial riser were considered in order to evaluate the hydrodynamic behavior and their effect in the gas oil conversion and products yield. The equations were solved numerically by finite volume method using the Eulerian-Eulerian approach and a commercial CFD code, CFX version 14.0. Appropriate functions were implemented in the model via user defined functions considering the heterogeneous kinetics and catalyst deactivation. The results from the model were validated against the experimental industrial results and it was found that the conversion of gas oil and the production yield significantly change with the feedstock localization.<br/> Copyright &copy; 2015, AIDIC Servizi S.r.l.","Alvarez-Castro, Helver Crispiniano and Armellini, Victor and Mori, Milton and Martignoni, Waldir Pedro and Ocone, Raffaella",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Parts-based geophysical inversion with application to water flooding interface detection and geological facies detection","I built parts-based and manifold based mathematical learning model for the geophysical inverse problem and I applied this approach to two problems. One is related to the detection of the oil-water encroachment front during the water flooding of an oil reservoir. In this application, I propose a new 4D inversion approach based on the Gauss-Newton approach to invert time-lapse cross-well resistance data. The goal of this study is to image the position of the oil-water encroachment front in a heterogeneous clayey sand reservoir. This approach is based on explicitly connecting the change of resistivity to the petrophysical properties controlling the position of the front (porosity and permeability) and to the saturation of the water phase through a petrophysical resistivity model accounting for bulk and surface conductivity contributions and saturation. The distributions of the permeability and porosity are also inverted using the time-lapse resistivity data in order to better reconstruct the position of the oil water encroachment front. In our synthetic test case, we get a better position of the front with the by-products of porosity and permeability inferences near the flow trajectory and close to the wells. The numerical simulations show that the position of the front is recovered well but the distribution of the recovered porosity and permeability is only fair. A comparison with a commercial code based on a classical Gauss-Newton approach with no information provided by the two-phase flow model fails to recover the position of the front. The new approach could be also used for the time-lapse monitoring of various processes in both geothermal fields and oil and gas reservoirs using a combination of geophysical methods. A paper has been published in Geophysical Journal International on this topic and I am the first author of this paper. The second application is related to the detection of geological facies boundaries and their deforation to satisfy to geophysica data and prior distributions. We pose the geophysical inverse problem in terms of Gaussian random fields with mean functions controlled by petrophysical relationships and covariance functions controlled by a prior geological cross-section, including the definition of spatial boundaries for the geological facies. The petrophysical relationship problem is formulated as a regression problem upon each facies. The inversion is performed in a Bayesian framework. We demonstrate the usefulness of this strategy using a first synthetic case study, performing a joint inversion of gravity and galvanometric resistivity data with the stations all located at the ground surface. The joint inversion is used to recover the density and resistivity distributions of the subsurface. In a second step, we consider the possibility that the facies boundaries are deformable and their shapes are inverted as well. We use the level set approach to deform the facies boundaries preserving prior topological properties of the facies throughout the inversion. With the additional help of prior facies petrophysical relationships, topological characteristic of each facies, we make posterior inference about multiple geophysical tomograms based on their corresponding geophysical data misfits. The result of the inversion technique is encouraging when applied to a second synthetic case study, showing that we can recover the heterogeneities inside the facies, the mean values for the petrophysical properties, and, to some extent, the facies boundaries. A paper has been submitted to Geophysics on this topic and I am the first author of this paper. During this thesis, I also worked on the time lapse inversion problem of gravity data in collaboration with Marios Karaoulis and a paper was published in Geophysical Journal international on this topic. I also worked on the time-lapse inversion of cross-well geophysical data (seismic and resistivity) using both a structural approach named the cross-gradient approach and a petrophysical approach. A paper was published in Geophysics on this topic. ProQuest Subject Headings: Geophysics, Geology, Statistics.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.","Zhang, Junwei",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Seismic finite element analysis of an existing old concrete structure by using multifiber beams: Introduction of an adaptive pushover method","The study of the seismic vulnerability of existing structures is an important issue. Many researches have been developed in order to investigate the structural behavior of these structures, and extract the basic informations needed to establish retrofitting guidelines in order to reduce the seismic risk to acceptable levels. The most accurate analysis procedure for the structures subjected to strong ground motions is the time-history analysis. This method is time-consuming though for application in all practical purposes. The necessity for faster methods that would ensure a reliable structural assessment or design of structures subjected to seismic loading led to the pushover analysis. Pushover analysis is a non-linear static analysis based on the assumption that structures oscillate predominantly in the first mode or in the lower modes of vibration during a seismic event. The present work deals with seismic vulnerability assessment of an old existing reinforced concrete structure - Perret tower - located in Grenoble, France. After a brief description of the structure in exam, a preliminary computation of the mass of the building and the definition of every existing section are performed. A simplified 3D numerical model is carried out using a finite element code based on multifiber beams approach. Firstly, a non-linear temporal dynamic analysis is performed, then a conventional and adaptive pushover analysis is carried out. The results obtained of the studied cases are then compared: it is observed that the conventional pushover analysis should be adjusted in order to take into account the change of dynamic characteristics due to the formation of plastic mechanisms. Finally, the tower critical levels in term of damage are highlighted.<br/> &copy; 2017 National Technical University of Athens. All rights reserved.","Omar, A. and Grange, S. and Dufour, F.",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The simulation-game controversy: What is a ludic simulation?","Games use the same base technology and design strategy as do simulations, but add a few items to the mixture. Understanding this gives 'new' (read borrowed) tools for game creation and testing. The idea that simulations are implementations of a model, for instance, leads to a focus on the model rather than the code when designing a game. Similarly, the verification/validation pair used in simulations can be extended by adding playtesting for games, thus giving an educational game (for example) viable, demonstrable educational characteristics as well as playable (and thus engaging and motivating) characteristics. Productive work on improving games for specific purposes (serious games) can be advanced if the authors can agree on a common terminology and concept set (Shaw &amp; Gaines, 1989), and if games can be seen as a valuable extension of a simulation that has specific characteristics that make them useful in specific circumstances. The idea of 'fun' is often thought of as the enemy of 'learning' in educational literature, and this needs to change if progress on serious and educational games is to be made. This paper will describe the hierarchy of computer simulation objects within which ludic simulations can be understood. Copyright &copy; 2013, IGI Global.<br/>","Parker, J.R. and Becker, Katrin",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Heuristic approach in structural strength evaluation and retrofitting of existing buildings in seismic areas","The migration of the population, to developed areas, lead to birth of overcrowded areas with claim for residences, financial, administrative, cultural and industrial facilities. The density augmentation of the population required sophisticated solutions from the authorities to ensure adequate environmental state suited for the 21<sup>th</sup>century. Due to the huge demand of the facilities significant number of buildings must be strengthened and modified to be suitable to the new demands. This change claims increase of surfaces and storey addition. This trend leads to the hazard of the new urban agglomerations versus natural causes as flood, fire and earthquake. The present paper is devoted to structures which were built before the implementation of any code or obsolete ones for the seismic design. Strengthening and retrofitting of existent buildings is a major interest of the actual engineering effort to provide the suitable structures to the new demands and to be stable under seismic loads. Each seismic event is accompanied by damages that better design and execution would have prevented. In general, severe earthquakes are followed by intense research of the causes which damaged structures. In some cases the code is changed due to the short comes which were observed. This means that each experiment including earthquakes is followed by a learning process that provides new solutions, use of new materials and new design strategies. This process must be a heuristic process according the abilities of evaluation and implementation.<br/>","Gluck, N. and Farhat, R. and Eid, R. and Tzadka, U.",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The CIRA effort toward a fast aerothermal design environment tool","This paper deals with the CIRA effort undertaken to develop an in-house tool aiming at the assessment of the aerothermodynamic loading environment expected by hypersonic vehicles during re-entry. This tool couples proper orthogonal decomposition techniques and a Kriging interpolator to provide vehicle aerothermal loading conditions, starting from a limited number of design results available for the vehicle. Indeed, the code inputs are data coming from CFD design results previously obtained at different flight conditions of the vehicle re-entry corridor. Thus, the tool allows computing new flowfield distributions around the vehicle and on its surface simply asking for the desired Mach, Reynolds numbers, vehicle attitude and flap settings, thus avoiding time consuming and cost expensive CFD analyses. Furthermore, the main advantage of the developed tool is its flexibility: Efficient, fast, adaptable at each trajectory and vehicle's shape change, and hopefully reliable, due to the physics-based interpolation methods. Examples of tool application are provided in the paper considering the USV3 vehicle design. For instance, time history of USV3 surface map for pressure and skin friction coefficient are presented, provided that such design results are useful to identify aerothermal loads. A cross-validation of the results, are also shown.<br/> Copyright &copy; 2014 by the International Astronautical Federation.","Cinquegrana, Davide and Pezzella, Giuseppe and Catalano, Pietro and De Stefano Fumo, Mario",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Maps as geomedial action spaces: considering the shift from logocentric to egocentric engagements","This paper considers some significant questions in geography and cognate fields about the roles of maps in the information age. Most maps are now digital products, offering immersive environments for user involvement. The increasingly networked digital distribution of geographic information in consumer-orientated cartographic representations leads to substantial changes how people individually and collaboratively experience and produce space and place. This article focuses on the ongoing metamorphosis arising through geobrowsing, the media-based flexible production of geographic knowledge through interactive maps. Drawing on work in media studies influenced by the so-called spatial turn&mdash;the rediscovering of geography-related questions in the social sciences and humanities, after modernism&rsquo;s claimed prioritization of time and history (Soja in Postmodern Geographies. The reassertion of space in critical social theory, London, 1989; Jameson in Postmodernism, or, the cultural logic of late capitalism, Duke University Press, Durham, 1991)&mdash;this paper develops a theoretical framework built on the dynamic networked geomedial action spaces concept to understand the changing roles of information age maps as imagined materialist spaces for the experience and production of space&mdash;ultimately a medial turn. Following this concept, maps change from offering static and non-interactive frames of geographic reference for the production of space and place and as geomedia support a veritable infinity of interactive and map-based activities. Geobrowsing facilitates some new modes of geographic interactions that move from logocentric engagements with static maps to egocentric dynamic interactions with code-based elements of geomedial action spaces. Google Earth and similar geomedia facilitate maps that become intrinsic to a growing number of social action spaces and alter the experience and production of space and place.<br/> &copy; 2015, Springer Science+Business Media Dordrecht.","Abend, Pablo and Harvey, Francis",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Findings from 16 years of auditing pipeline integrity management systems","A Pipeline Integrity Management System (PIMS) is a comprehensive, systematic, and integrated set of arrangements implemented by an operator to assess, mitigate, and manage pipeline risk. Over the past 16 years, Penspen have performed over 30 PIMS audits of pipeline operators internationally. This paper presents the collated findings from these audits, and examines the common areas in which operators have fallen short of best practice. The paper concludes with a series of recommendations based on the findings, which can be adopted by operators to improve their PIMS arrangements and practices. Penspen's standardized 17-element PIMS Model takes a holistic view of pipeline integrity. The audits, which are based on the Model, assess the adequacy and effectiveness of operators' management systems and arrangements in keeping risks to people, the environment, and to the business to acceptable levels, given the anticipated pipeline operating conditions and taking into account the pipeline's history and current status. Starting at the 'top level' of a PIMS, the audits consider the adequacy of operators' pipeline policies, objectives, and performance metrics, and how these are subject to monitoring, review, and audit. The audits look at the organization responsible for managing the integrity of pipelines, and examine how all those with a role to play in the wider PIMS work together to this end. The numerous activities that take place during a pipeline's lifecycle are investigated, to assess how the risk assessment results are used to determine the control and mitigation measures to be implemented during the pipeline's design, construction, handover, commissioning, operation, inspection and maintenance, and how the operator ensures the effectiveness of these measures. The audits also study those 'supporting' processes and systems which play an important part in pipeline integrity management, including procurement, emergency response and recovery, incident investigation, change control, document and data management, and legal and code compliance. The collated results from the 30+ audits reveal that while operators typically have good control systems in place for the project stages of the pipeline lifecycle, controls for the operational stages have been found to be less robust. In terms of management and organization, operators can fail to recognize how many different individuals and teams have a role to play in the management of pipeline integrity. Furthermore, while operators often have good corporate systems in place for change control, emergency response, and risk assessment, such systems may not take into account pipeline-specific risks or requirements. Operators can tend to focus on pipeline safety and/or environmental-related risks, when through holistic assessment it can be shown that risks associated with production interruptions will tend to drive actions in practice.<br/> &copy; Copyright 2016 by ASME.","Hodgson, Oliver J. and Keen, Dennis W. J. and Toft, Malcolm",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Radon induced hyperplasia: Effective adaptation reducing the local doses in the bronchial epithelium","There is experimental and histological evidence that chronic irritation and cell death may cause hyperplasia in the exposed tissue. As the heterogeneous deposition of inhaled radon progeny results in high local doses at the peak of the bronchial bifurcations, it was proposed earlier that hyperplasia occurs in these deposition hot spots upon chronic radon exposure. The objective of the present study is to quantify how the induction of basal cell hyperplasia modulates the microdosimetric consequences of a given radon exposure. For this purpose, computational epithelium models were constructed with spherical cell nuclei of six different cell types based on histological data. Basal cell hyperplasia was modelled by epithelium models with additional basal cells and increased epithelium thickness. Microdosimetry for alpha-particles was performed by an own-developed Monte-Carlo code. Results show that the average tissue dose, and the average hit number and dose of basal cells decrease by the increase of the measure of hyperplasia. Hit and dose distribution reveal that the induction of hyperplasia may result in a basal cell pool which is shielded from alpha-radiation. It highlights that the exposure history affects the microdosimetric consequences of a present exposure, while the biological and health effects may also depend on previous exposures. The induction of hyperplasia can be considered as a radioadaptive response at the tissue level. Such an adaptation of the tissue challenges the validity of the application of the dose and dose rate effectiveness factor from a mechanistic point of view. As the location of radiosensitive target cells may change due to previous exposures, dosimetry models considering the tissue geometry characteristic of normal conditions may be inappropriate for dose estimation in case of protracted exposures. As internal exposures are frequently chronic, such changes in tissue geometry may be highly relevant for other incorporated radionuclides.<br/> &copy; 2016 IOP Publishing Ltd.","Madas, Balazs G.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Historic desmond building retrofit-a case study of the seismic retrofit of a non-ductile concrete building in the los angeles area","According to a Los Angeles Times report in Oct. 2013 (Los Angeles Times, 2013), by the most conservative estimate, as many as 50 old concrete buildings in the city of Los Angeles would be destroyed in a major earthquake, exposing thousands to injury or death. To address this concern, the City of Los Angeles recently proposed instituting what are arguably the most ambitious seismic safety regulations in California's history; regulations that would require building owners to retrofit thousands of building deemed to be at risk of collapse during a major earthquake. Non-ductile concrete buildings built before 1980 were singled out as one of two particularly vulnerable structural types requiring attention. This paper presents a case study of seismically retrofitting a historic concrete building located in Los Angeles, California. Designed in 1916, the building originally housed the Willy's Overland Company car dealership and assembly plant, which later became a Desmond's department store warehouse. After the retrofit, the five-story warehouse will be transformed into high quality, creative office space with a ground floor cafe and the addition of a 7,000-square-foot sixth floor that will bring the total size of the building to 82,000 square feet. The retrofit project is currently under construction with full occupancy expected in the summer of 2015. When it is complete, it will be the first of many potential renovations of historic properties located in the South Park district for creative office use. This case study presents an innovative but rigorous approach taken to enable the addition of a story to the existing building by not exceeding the gravity load and lateral force change triggers in Chapter 34 of the 2011 Los Angeles City Building Code (2011 LABC) necessitating retrofit. Although demonstrated to not be required by code, a seismic retrofit was nevertheless instituted; this while continuing to ensure that the changes in force effects in retained existing structural members did not exceed code retrofit triggers. All structural modifications made since the building was originally constructed using very low strength concrete were considered in the evaluations and retrofit design.<br/> &copy; 2015 ASCE and ATC.","Jiang, Z. and Sarkisian, M. and Mathias, N. and Garai, R. and Lyrenmann, J.",2015,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"FPGA implementation issues of a two-pole adaptive notch filter for GPS/Galileo receivers","In the field of satellite navigation technologies, and in particular for Safety-of-Life (SoL) applications, great attention is given to rejection of interference. In particular, continuous wave (CW) signals represent one of the most common types of interference. A properly designed adaptive notch filter should be employed in order to remove it. Such an algorithm has been widely exploited in literature, with many variations. Most of these studies are however mainly focused on the floating point precision model, and only a few of them really consider the potential issues of a real implementation. The objective of this paper is to discuss the issues arising from fixed-point modeling and FPGA implementation of an adaptive notch filter. Previous works showed that in a complex base-band front-end, one CW interfering signal can be eliminated by means of an IIR one-pole adaptive notch filter. For the case of more than one CW, two or more filtering cells can be used in cascade. Here, a two-pole adaptive notch filter is considered. A distortion analysis of the filter output is conducted in the paper by means of Cross-Ambiguity-Function (CAF) and Delay-Locked-Loop (DLL) S-curve evaluation, showing a great distortion of the ranging code after the filtering operation. Thus, to reduce such a distortion, the proposed solution consists in constraining the zeros of the filter at staying on the unit circle in a new and simpler way with respect to existing methods. A run-time change of the algorithm parameters is also performed, improving convergence speed. The VHDL architecture is then discussed. Pipelining and delaying the parameter updating are applied to increase clock frequency. The post Place&amp;Route (P&amp;R) results of the improved version of the two-pole adaptive notch filter on a Virtex 5 show a clock frequency of 47 MHz with an occupied area of 96.4 KGE. Tests with a fully software receiver support the validity of the proposed solution.<br/>","Gamba, Micaela Troglia and Falletti, Emanuela and Rovelli, Davide and Tuozzi, Alberto",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Achieving the InsideOutside Coupling During Network Simulation of Isothermal Drying of a Porous Medium in a Turbulent Flow","The problem of isothermal (slow) drying of a porous medium in the form of a square bluff body resting on a flat surface while being exposed to turbulent air flow is investigated in this paper. The porous medium is represented by a pore-network model consisting of a network of volume-less nodes connected to each other via narrow throats. As the air flows past the water-saturated network, the water evaporates in the network and the evaporated water vapors diffuse through the air inside the network to advect away in the outside domain. In the history of such simulations coupling the &lsquo;outside&ndash;inside&rsquo; processes during drying, this paper investigates, for the first time, the effect of turbulent flow on drying of a porous medium after coupling the outside-the-network flow and transport with the inside-the-network drying and liquid redistribution. Also, a commercial software package is used for the first time to obtain the outside flow. The water redistribution inside the network is predicted by the invasion-percolation algorithm after assuming the dominance of capillary forces over viscous and gravity forces. The outside velocity field is first obtained using the k- &Epsilon; model in ANSYS Fluent package. Then, the velocity field is used in a finite-volume-based code in order to solve for mass transfer outside the network using the regular convective-diffusive species transport equation. The drying mechanism and vapor transport inside the pore network is coupled with the outside mass-transfer simulation by considering a flux-balancing condition at the top surface of the network. After achieving grid independence and demonstrating the suitability of the no-slip boundary condition for the network top, the effects of the outside-flow Reynolds number and different turbulent flow models on several global drying parameters such as evaporation or drying rate, cumulative drying time, and top-surface saturation were studied. The resistance to vapor transport within the network was observed to dominate the enhanced vapor transport outside the network through the use of turbulent flow. Though the effect of variations in the network throat-size distribution upon saturation distribution within the network was found to be significant, its effect on the drying rate evolution was minimal. The concentration boundary layer thickness, which is employed to set the mass-transfer boundary condition in pore-network simulations and has been hitherto taken as constant in the literature, was found to change not only with position on the network top, but was also found to decline with time due to the drying of the network top.<br/> &copy; 2016, Springer Science+Business Media Dordrecht.","Beyhaghi, Saman and Xu, Zhenyu and Pillai, Krishna M.",2016,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Teaching teachers to think like engineers using NetLogo","This paper provides a view of 22 K12 teachers' expectations versus the actuality of immersion into an engineering education computer science (CS) project during a Math/Science Partnership (MSP) grant called RAMPED, which was a 16-day, yearlong MSP grant. The CS session using NetLogo was selected for focused examination. NetLogo is a multi-agent simulator that uses the educational Logo programming language and was designed for classroom modeling experience. The research question for the study was, ""How do K12 teachers view their skill set of using computer science in their classrooms before, during, and after professional development (PD)?"" RAMPED participants spent a total of three days immersed in using NetLogo as a vehicle for learning fundamental computer science principles and engineering applications for K12 classrooms. The authors used a social constructivism approach and examined K12 teacher NetLogo usage in and out of the classroom. The authors also collected the data via K12 teacher surveys and informal interviews. Findings show that the teachers self-reported high expectations of their skillset as well as easy assimilation of NetLogo (but not CS) into their classroom teaching. On a scale from 0 to 5, where 0 is not at all skillful and 5 is extremely skillful, survey pretest results show over 50% at a 3, 4, or 5. Posttest survey results show over 90% at a 3, 4, or 5. After the summer session, NetLogo was useful to 95% of K12 teachers. After an academic year NetLogo follow-up session over 75% of the K12 teachers were satisfied with instruction and support. Over 85% of teachers believed that the workshop ""stretched teacher thinking into their classrooms."" Teachers' qualitative comments are included for triangulation. Conclusions include that intense K12 teacher exposure to engineering CS topics (e.g. 24 hours total of a larger PD) is not enough to truly enact meaningful classroom changes (although the teachers did create new activities). Additional support for meaningful classroom change and K12 teacher confidence is necessary. In general, K12 teachers need (and asked for) support in the form of ready to use lessons and documents (e.g. additional activities) along with leader presence to support them in trying their self-created plans situated within the NGSS standards. The actuality of working with NetLogo (and changing functions and code) to present STEM concepts/topics was both invigorating (it was new for the K12 teachers) and frustrating (it was often hard for the K12 teachers to see connections to content) as teachers moved through expectations and actuality. Implications include planning for structured K12 teacher academic year support in implementing CS topics for sustainability in classrooms.<br/> &copy; American Society for Engineering Education, 2017.","Burrows, Andrea Carneal and Borowczak, Mike",2017,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Title: VI in PA - Managing a threat to the certainty of the act 2 closure process","Pennsylvania's Land Recycling Program under Act 21 encourages the voluntary cleanup and reuse of contaminated commercial and industrial sites. Three cornerstones of this Program are: Uniform cleanup standards Liability relief Standardized reviews and time limits Vapor intrusion creates challenges to elements of the Act 2 Program and the overall goal of encouraging land re-use through certainty and efficiency in investigation and clean-up. This presentation will explain: 1) the legal and technical origins of these challenges; 2) how the Pennsylvania Department of Environmental Protection (""DEP"") is addressing these challenges; and 3) practical implications and approaches for managing vapor intrusion in Pennsylvania under Act 2. Act 2 was not written with the vapor pathway of exposure in mind. For example, under Section 303(e), Statewide Health Attainment: Vapor intrusion (""VI"") exposures are not considered in development of Statewide Health Medium Specific Concentrations (MSCs) under Section 303 For groundwater, attainment of Statewide Health Standards (""SHS"") is demonstrated at the point of compliance, defined as the property boundary at time of discovery of contamination A potential for vapor intrusion may exist within a site that has achieved attainment of SHS at the property boundary A deed acknowledgement or environmental covenant is not required as to contamination that remains in groundwater if the SHS for groundwater is met for the contaminant at the property boundary. Therefore, the Act 2 statute itself, the source of authority for the Act 2 program, has a hole through which vapor intrusion could pass, especially as to buildings located on a source site where SHS standards for groundwater are used to demonstrate compliance with the statute. Despite the statutory hole, DEP has taken steps to address vapor intrusion for entire sites seeking to demonstrate attainment with SHS, e.g.: ? Through publishing guidance: Land Recycling Program Technical Guidance Manual, Section IV.A.4, Vapor Intrusion into Buildings from Groundwater and Soil Under the Act 2 Statewide Health Standard, Jan. 24, 2004 (""2004 SHS VI Guidance""). ? Through rulemaking: 25 Pa. Code &sect; 250.312, which states, ""The final [SHS] report must include, as appropriate, an assessment of the vapor intrusion pathway."" ? Through a current effort to update the 2004 SHS VI Guidance. The 2004 SHS VI Guidance is currently being revised for, among other reasons, replacement of outdated screening levels. Some screening values in the 2004 SHS VI Guidance for key contaminants, such as TCE, exceed comparable values used by the United States Environmental Protection Agency (""USEPA"") and almost all other states that have such values, by several orders of magnitude.2 Under Act 2, DEP generally applies somewhat less conservative target risk levels than USEPA does in setting its screening values (one additional cancer in 100,000 under Act 2 versus one additional cancer in a million), but the updated screening values in the DEP's revised SHS VI Guidance are very likely to be orders of magnitude lower than the values in the 2004 version.3 DEP is currently struggling to develop the revised SHS VI Guidance in the face of the potentially dramatic drops in screening values, the variability that is increasingly associated with vapor intrusion sampling and results, and the need for efficiency and certainty for the Act 2 program to remain as effective as it has been in encouraging property re-use and re-development.4 Undeveloped parts of properties and the potential that future buildings will be configured differently than existing structures (for example, as to subgrade floors or parking garages) especially create challenges for a program that has come to be known for the stream-lined nature of its property assessment and the reliability of its releases of liability. Looming large is what impact change, in the form of guidance that directs a more robust evaluation of the vapor intrusion pathway, and most especially, much lower screen values, will have on the efficiency and certainly that have characterized Act 2 to date.5 In short, vapor intrusion creates risks for both remediators and developers under Act 2. Those risks include that releases of liability using SHS standards may be harder to obtain and may be re-opened, if not by DEP - which is not likely to deliberately undermine the certainty of its successful program -Then by third parties who seek protection, further remediation (or mitigation), or other relief where vapor intrusion is left unaddressed or arises as an issue in the future. Close evaluation of existing sign-offs for sites with remaining volatile contamination and common sense approaches to mitigating vapor risk are needed to achieve the predictability and confidence required to move forward with development in the face of the inherent uncertainty currently associated with VI assessments.<br/>","Roe, Christopher M. and Davis, Karen H. and Cutler, Adam H.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The Declaratron, semantic specification for scientific computation using MathML","We introduce the Declaratron, a system which takes a declarative approach to specifying mathematically based scientific computation. This uses displayable mathematical notation (Content MathML) and is both executable and semantically well defined. We combine domain specific representations of physical science (e.g. CML, Chemical Markup Language), MathML formulae and computational specifications (DeXML) to create executable documents which include scientific data and mathematical formulae. These documents preserve the provenance of the data used, and build tight semantic links between components of mathematical formulae and domain objects-in effect grounding the mathematical semantics in the scientific domain. The Declaratron takes these specifications and i) carries out entity resolution and decoration to prepare for computation ii) uses a MathML execution engine to run calculations over the revised tree iii) outputs domain objects and the complete document to give both results and an encapsulated history of the computation. A short description of a case study is given to illustrate how the system can be used. Many scientific problems require frequent change of the mathematical functional form and the Declaratron provides this without requiring changes to code. Additionally, it supports reproducible science, machine indexing and semantic search of computations, makes implicit assumptions visible, and separates domain knowledge from computational techniques. We believe that the Declaratron could replace much conventional procedural code in science.<br/>","Murray-Rust, Dave and Murray-Rust, Peter",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Desain study of Pb-Bi cooled fast reactors with natural uranium as fuel cycle input using special shuffling strategy in radial direction","Design study of Pb-Bi cooled fast reactors with natural uranium as fuel cycle input using special radial shuffling strategy has been performed. The reactors utilizes UN-PUN as fuel, Eutectic Pb-Bi as coolant, and can be operated without refueling for 10 years in each batch. Reactor design optimization is performed to utilize natural uranium as fuel cycle input. This reactor subdivided into 6 regions with equal volume in radial directions. The natural uranium is initially put in region 1, and after one cycle of 10 years of burn-up it is shifted to region 2 and the region 1 is filled by fresh natural uranium fuel. This concept is basically applied to all regions. The calculation has been done by using SRAC-Citation system code and JENDL-3.2 library. The effective multiplication factor change increases monotonously during 10 years reactor operation time. There is significant power distribution change in the central part of the core during the BOC and the EOC. It is larger than that in the case of modified CANDLE case which use axial direction burning region move. The burnup level of fuel is slowly grows during the first 15 years but then grow fastly in the rest of burnup history. This pattern is a little bit different from the case of modified CANDLE burnup scheme in Axial direction in which the slow growing burnup period is relatively longer almost half of the burnup history. &copy; (2013) Trans Tech Publications, Switzerland.","Su'ud, Zaki and Irka, Feriska H. and Imam, T.Taufiq and Sekimoto, H. and Sidik, P.",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Polarimetric Scattering and SAR Information Retrieval","Taking an innovative look at Synthetic Aperture Radar (SAR), this practical reference fully covers new developments in SAR and its various methodologies and enables readers to interpret SAR imagery. An essential reference on polarimetric Synthetic Aperture Radar (SAR), this book uses scattering theory and radiative transfer theory as a basis for its treatment of topics. It is organized to include theoretical scattering models and SAR data analysis techniques, and presents cutting-edge research on theoretical modelling of terrain surface. The book includes quantitative approaches for remote sensing, such as the analysis of the Mueller matrix solution of random media, mono-static and bistatic SAR image simulation. It also covers new parameters for unsupervised surface classification, DEM inversion, change detection from multi-temporal SAR images, reconstruction of building objects from multi-aspect SAR images, and polarimetric pulse echoes from multi-layering scatter media. Structured to encourage methodical learning, earlier chapters cover core material, whilst later sections involve more advanced new topics which are important for researchers. The final chapter completes the book as a reference by covering SAR interferometry, a core topic in the remote sensing community. Features theoretical scattering models and SAR data analysis techniques. Explains the simulation of SAR images for mono- and bi-static radars, covering both qualitative and quantitative information retrieval Chapter topics include: theoretical scattering models; SAR data analysis and processing techniques; and theoretical quantitative simulation reconstruction and inversion techniques. Structured to enable both academic learning and independent study, laying down the foundations first of all before advancing to more complex topics. Experienced author team presents mathematical derivations and figures so that they are easy for readers to understand. Pitched at graduate-level students in electrical engineering, physics, earth and space sciences, as well as researchers. MATLAB code available for readers to run their own routines. An invaluable reference for research scientists, engineers and scientists working on polarimetric SAR hardware and software, Application developers of SAR and polarimetric SAR, remote sensing specialists working with SAR data - using ESA. &copy; 2013 John Wiley &amp; Sons Singapore Pte. Ltd.<br/>","Jin, Ya-Qiu and Xu, Feng",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A numerical steady state and dynamic study in a data center using calibrated fan curves for CRACS and servers","This study presents the results of a detailed parametric study for a data center that is air cooled using a set of four CRAC units in a cold/hot aisle raised floor configuration. The fans of the CRAC units and the servers are calibrated using their practical characteristics fan curves. A commercial CFD code is utilized for this purpose in which the buoyancy forces are taken into account. The k-epsilon model and the Boussinesq approximation are used to model the turbulent airflow and the buoyancy effect, respectively. A dynamic model is developed to take into account the changes in flow rates and power dissipation in the data center environment. The current dynamic model does not take into account the thermal mass of the CRAC units or the servers. The effect of the CRAC fan speed, instantaneous change in power dissipation, tiles perforation ratio, and servers fan speeds on the total flow rate in the room and the inlet temperatures of the racks are investigated. In the transient model, we investigate the effect of different CRAC failure scenarios on the time history of the temperatures and the flow pattern in the data center. Time constants and safe time are estimated from this study. A fundamental understanding of the effect of different data center entities on the flow and the temperatures is developed. Interesting flow patterns are observed in the case of different CRAC failures that could be used to recommend general design guidelines. &copy; 2014 ASME.<br/>","Alkharabsheh, Sami A. and Sammakia, Bahgat and Shrivastava, Saurabh and Ellsworth, Michael and David, Milnes and Schmidt, Roger",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"The GAIA theory: From Lovelock to Margulis. from a homeostatic to a cognitive autopoietic worldview","This work consists of two parts. The first presents the state of art concerning the history and the reception by the scientific community of the Gaia hypothesis introduced in the 1970s and which evolved, in time, into theory and quasi-science, i.e., Earth system science. The original Gaia supposes that the temperature, oxidation state, acidity and certain aspect of rocks and waters are at any time kept constant and that this homeostasis is maintained by active biogeochemical feedback processes (first-order cybernetics) operated automatically and unconsciously by the biota. In turn, the probability of life's event and its survival should be linked to processes regulated by the second thermodynamic principle, in its own dynamical equilibrium. This consists in maintaining the organisms at a low level of entropy, through energy-dissipative leakage into the surrounding environment. Life and the environment are so closely coupled that evolution concerns Gaia, and not the organisms or the environment taken separately. Since the end of 1980s, Lynn Margulis, Lovelock's longstanding co-author, proposed replacing Gaia's homeostatic nature with an autopoietic and evolutionary one that is connected to second-order cybernetic processes. Margulis arrived at the Gaian paradigm shift, mainly based on her authority in the field of the microcosmos. This included symbiogenetic processes concerning the birth and evolution of microbiotic organisms at the planetary level, which led to the construction of macroorganisms and their properties that stabilize the environment. A close relationship between symbiogenetic and autopoietic theory (the latter proposed by Maturana and Varela in Autopoiesis and Cognition: the Realization of the Living, D. Reidel Publishing Co., Dordecht 1980) is represented by the fact that both theories refer primarily to the epigenetic-cytoplasmatic mechanisms in cellular constitution and evolution, and only secondarily to the established, DNA-mediated genetic code. It is the consequent lack of primeval genetic information that requires that both theories postulate the existence of cognitive-intentional properties of the living matter (Luisi in Springer 90(2):49-59, 2003) in the construction of the cell and of multicellular organisms. Conversely, traditional theory treats biological organizations as an epiphenomenon, that is, a result of casual processes leading to the constitution of genetic material responsible of cell constitution, duplication and sometime mutation-recombination for new phenotypic forms. The second part of this work consists of more speculative comments about some important articulations of the Gaian construct, in particular: (a) The apparent lack of information on the chemical-physical nature of living and inert matter and on their possible interaction in the construction of organisms and environment. (b) The substantial weakness in the descriptive processes leading to the auto-organization of the two terrestrial matrices (organisms and environment) that is Lovelock's engineeristic and physiological automatisms without consciousness and Margulis' cognitive symbiogenetic processes operating at elementary matter. Both hypotheses have been scantly accepted by established science. The latter appears to privilege the theory of spontaneous and istantaneous cooperative phenomenon between elementary particles, at the base of the change from chaos to order and from one ordered state to another, both in physical and living realm. (c) Finally, the substantial underevaluation, operated by Lovelock in his holistic approach to the study of planet Earth, of the role played by the physical phenomenon of the distance interaction between quantum objects, leading to their entanglement. Such phenomena, apparently spontaneous, istantaneous and mediated by quantum field, have questioned the same objective nature of reality. Recently, Su&die;sskind (interviews with P. Byrne, Scientific American, June 2011) noticed that the entanglement phenomenon allows obtaining the knowledge of everything about a composite system without knowing its singular parts: a possible form of holistic approach to planet Earth, well distinct from those proposed by Lovelock and Margulis on solely cybernetic basis. &copy; 2012 The Author(s).<br/>","Onori, Luciano and Visconti, Guido",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Urban and landscape changes through historical maps: The Real Sitio of Aranjuez (1775-2005), a case study","When determining the evolution of a territory or town over time, comparing historical maps with contemporary maps is indispensable. In this study, we applied the methodology of georectification to compare historical maps with current orthophotos from 2005. We propose colour and lines code as useful tools for the analysis of the urban and landscape changes that the town has undergone since the 18th century, and we graphically reconstruct certain former heritage items that no longer exist. For example, these techniques are applied to the Real Sitio de Aranjuez (Spain) using the two most important historical maps: the 1775 Domingo de Aguirre map, which shows the full extent of the royal site for the first time, and the 1835 General Town Plan, which is the most characteristic of available 19th-century maps, as it displays the consolidated historical town. Next, using two rectified rasters and the orthophoto, we overlay a grid of nine 1. &times;. 1. km squares, allowing us to ""see the town and its territory"" at three moments in history: 1775, 1835 and 2005. Thus, we obtain formal and dimensional information allowing analysis of the evolution of the territory, urban area and historic buildings. Among the many applications of this methodology in the fields of urban development and monumental-heritage conservation, we propose the graphical reconstruction of three urban elements that no longer exist. We determined that graphical reconstruction, in conjunction with traditional historical research, provides the greatest benefits for recreating an historical landscape. These methodologies will aid in the development of long-range management strategies and facilitate the assessment of threats posed by anthropogenic activities and environmental change to preserve the landscape heritage. &copy; 2013 Elsevier Ltd.<br/>","San-Antonio-Gomez, C. and Velilla, C. and Manzano-Agugliaro, F.",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Study of Tectonic Tremor in Depth: Triggering Stress Observation and Model of the Triggering Mechanism","Non-volcanic tremor (NVT) has been discovered in recent years due to advances in seismic instruments and increased density of seismic networks. The NVT is a special kind of seismic signal indicative of the physical conditions and the failure mechanism on the source on the fault where NVT occurs. The detection methods used and the sensitivity of them relies on the density, distance and instrumentation of the station network available. How accurately the tremor is identified in different regions varies greatly among different studies. Therefore, there has not been study that rigorously documents tectonic tremors in different regions under limited methods and data. Meanwhile, many incidences of NVTs are observed during or after small but significant strain change induced by teleseismic, regional or local earthquake. The understanding of the triggering mechanisms critical for tremor remains unclear. In addition, characteristics of the triggering of NVT in different regions are rarely compared because of the short time frame after the discovery of the triggered NVTs. We first explore tectonic tremor based on observations to learn about its triggering, frequency of occurrence, location and spectral characteristics. Then, we numerically model the triggering of instability on the estimated tremor-source, under assumptions fine-tuned according to previous studies (Thomas et al., 2009; Miyazawa et al., 2005; Hill, 2008; Ito, 2009; Rubinstein et al., 2007; Peng and Chao, 2008). The onset of the slip reveals that how and when the external loading triggers tremor. It also holds the information to the background stress conditions under which tremor source starts with. We observe and detect tremor in two regions: Anza and Cholame, along San Jacinto Fault (SJF) and San Andreas Fault (SAF) respectively. These two sections of the faults, relative to general fault zone on which general earthquakes occur, are considered transition zones where slip of slow rates occurs. Slip events including NVT occur on these sections have slower slip rates than that of the general earthquakes (Rubin, 2008; Ide, 2008). In Azna region, we use envelope and waveform cross-correlation to detect tremor. We investigate the stress required to trigger tremor and tremor spectrum using continuous broadband seismograms from 11 stations located near Anza, California. We examine 44 Mw&ge;7.4 teleseismic events between 2001 and 2011, in addition to one regional earthquake of smaller-magnitude, the 2009 Mw 6.5 Gulf of California earthquake, because it induced extremely high strain at Anza. The result suggests that not only the amplitude of the induced strain, but also the period of the incoming surface wave, may control triggering of tremor near Anza. In addition, we find that the transient-shear stress (17--35 kPa) required to trigger tremor along the SJF at Anza is distinctly higher than what has been reported for the well-studied SAF (Gulihem et al. 2010). We model slip initiation using the analytical solution of rate-and-state friction. We verify the correctness of this method by comparing the results with that from the dynamic model, implemented using the Multi-Dimensional Spectral Boundary Integral Code (MDSBI) written by Eric M. Dunham from Sanford University. We find that the analytical result is consistent with that of the dynamic model. We set up a patch model with which the source stress and frictional conditions best resemble the current estimates of the tremor source. The frictional regime of this patch is rate-weakening. The initial normal and shear stress, and friction parameters are suggested by previous observations of tectonic tremors both in this and other studies (Brown et al., 2005; Shelly et al., 2006; Miyazawa, 2008; Ben-Zion, 2012). Our dynamic loading first consists of simple harmonic stress change with fixed periods, simplifying the transient stress history to resemble teleseismic earthquakes. We tested the period and amplitude of such periodic loading. We find that the period of the transient shear stress is less important relative to the amplitude. The triggering depends mainly on the ratio between amplitude of the shear stress loading and the background normal stress. We define a range of ratio indicative of the occurrence of the triggering. We later test the triggering of the instability using the shear stress history from 44 large teleseismic earthquakes (data equivalent to those used in Chapter 1). With the constraints of these observations, we find that the background normal stress should be in the range of &sim;400-700 kPa. The background normal stress suggested agrees with the common hypothesis that the tremor source is under low normal stress. In addition, our results provide a first estimation of the background normal stress with numerical method. We also demonstrate how our model find constrains on the background physical stress or frictional conditions, with several true incidences that transient shear stress triggers or not-triggers tremor. (Abstract shortened by UMI.). ProQuest Subject Headings: Geophysics.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.","Wang, Tien-Huei",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Modelling of seismic wave propagation in 2d","From the seismic hazard map of Albania the designers of structures take the peak ground acceleration (PGA) values on ground type A (rock). Based on these values and the type of the ground is formed the response spectra which serves as input for seismic analysis of structures. Recent measurements and studies of different authors have emphasized that for the ground classification accepted in EC8 response spectrum values in some cases do not match with reality. So it is necessary, especially when we built on soft soils is important to do a detailed analysis for propagation and amplification of seismic waves. These analyses are even more important if for the calculation of the structure will be used seismic dynamic analysis in which time history of acceleration is used as input. The article gives for a real example the wave propagation modelling made with ""Plaxis 2D"" software. The choice of soil parameters, damping coefficients, boundary modelling, finite element size, the integration constants are chosen based on the literature and 1D solutions. After all parameters are calibrated in the model solution output are obtained time histories of acceleration and corresponding spectra. Obtained spectra are compared with spectra generated by Euro Code 8 procedures and have been seen that they change, so is therefore recommended that for seismic analysis of the structure to use values taken from the study.<br/> &copy; 2014, Editura Cefin. All right reserved.","Dervishaj, Arben and Paci, Ervin and Cullufi, Hektor",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"A fully coupled multiphase multicomponent flow and geomechanics model for enhanced coalbed-methane recovery and CO2storage","Enhanced coalbed-methane (ECBM) recovery by the injection of CO<inf>2</inf>and/or N<inf>2</inf>is an attractive method for recovering additional natural gas resources, while at the same time sequestering CO<inf>2</inf>in the subsurface. For the naturally fractured coalbed-methane (CBM) reservoirs, the coupled fluid-flow and geomechanics effects involving both the effective-stress effect and the matrix shrinkage/swelling, are crucial to simulate the permeability change and; thus gas migration during primary or enhanced CBM recovery. In this work, a fully coupled multiphase multicomponent flow and geomechanics model is developed. The coupling effects are modeled by introducing a set of elaborate geomechanical equations, which can provide more fundamental understanding about the solid deformation and give a more accurate permeability/porosity prediction over the existing analytical models. In addition, the fluid-flow model in our study is fully compositional; considering both multicomponent gas dissolution and water volatility. To obtain accurate gas solubility in the aqueous phase, the Peng-Robinson equation of state (EOS) is modified according to the suggestions of S&oslash;reide and Whitson (1992). An extended Langmuir isotherm is used to describe the adsorption/desorption behavior of the multicomponent gas to/from the coal surface. With a fully implicit finite-difference method, we develop: a 3D, multiphase, multicomponent, dual-porosity CBM/ECBM research code that is fully compositional and has fully coupled fluid flow and geomechanics. It has been partially validated and verified by comparison against other simulators such as GEM, Eclipse, and Coalgas. We then perform a series of simulations/investigations with our research code. First, history matching of Alberta flue-gas-injection micropilot data is performed to test the permeability model. The commonly used uniaxial-strain and constant-overburden-stress assumptions for analytical permeability models are then assessed. Finally, the coupling effects of fluid flow and geomechanics are investigated, and the impact of different mixed CO<inf>2</inf>/N<inf>2</inf>injection scenarios is explored for bothmethane (CH<inf>4</inf>) production and CO<inf>2</inf>sequestration. Copyright &copy; 2013 Society of Petroleum Engineers.<br/>","Wei, Zhijie and Zhang, Dongxiao",2013,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Analysis of BWR OPRM plant data and detection algorithms with DSSPP","All U.S. BWRs are required to have licensed stability solutions that satisfy General Design Criteria (GDC) 10 and 12 of 10 CFR 50 Appendix A. Implemented solutions are either detect and suppress or preventive in nature. Detection and suppression of power oscillations is accomplished by specialized hardware and software such as the Oscillation Power Range Monitor (OPRM) utilized in Option III and Detect and Suppress Solution - Confirmation Density (DSS-CD) stability Long-Term Solutions (LTSs). The detection algorithms are designed to recognize a Thermal-Hydraulic Instability (THI) event and initiate control rod insertion before the power oscillations increase much higher above the noise level that may threaten the fuel integrity. Option III is the most widely used long-term stability solution in the US and has more than 200 reactor years of operational history. DSS-CD represents an evolutionary step from the stability LTS Option III and its licensed domain envelopes the Maximum Extended Load Line Limit Analysis Plus (MELLLA +) domain. In order to enhance the capability to investigate the sensitivity of key parameters of stability detection algorithms, GEH has developed a new engineering analysis code, namely DSSPP (Detect and Suppress Solution Post Processor), which is introduced in this paper. The DSSPP analysis tool represents a major advancement in the method for diagnosing the design of stability detection algorithms that enables designers to perform parametric studies of the key parameters relevant for THI events and to fine tune these system parameters such that a potential spurious scram might be avoided. Demonstrations of DSSPPs application are also presented in this paper utilizing actual plant THI data. A BWR/6 plant had a plant transient that included unplanned recirculation pump transfer from fast to slow speed resulting in about 100% to &sim;40% rated power decrease and about 99% to &sim;30% rated core flow decrease. As the feedwater temperature is reduced to equilibrium conditions, the power increased from about &sim;40% to about &sim;60% with little change inflow. A THI event developed and subsequently, an OPRM initiated scram occurred with Option III.<br/>","Yang, J. and Vedovi, J. and Chung, A.K. and Zino, J.F.",2012,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"Modeling transient multiphase flow and mold top surface behavior in steel continuous casting","This thesis develops, validates and applies a system of computational models to investigate transient multiphase turbulent flow physics in the mold region and mold top surface behavior during continuous casting of steel slabs. Each model can be used independently or combined together as a comprehensive system to gain insights into the inter-related multiphase fluid dynamics in the caster mold during both quasi-steady-state and essentially transient events in practical casting operations. Argon gas commonly is injected into the liquid metal stream through porous refractory walls in many metallurgical processes. Modeling multiphase flows in caster molds is of great significance to understanding of inclusion transport and defect formation mechanisms and to improving the quality of the final products. To better understand the gas injection process, a new model is developed to investigate gas permeating through heated upper tundish nozzle (UTN) porous refractory, including the effects of nozzle geometry, gas thermal expansion, temperature-dependent gas viscosity, and possible gas leakages into unsealed joints. Furthermore, a procedure to predict initial bubble size is established. Two (semi-) analytical models, a stopper-position- and a gate-position- based model, predict liquid steel flow rate histories during the transient events and serve as a first step of this comprehensive model system. Argon-steel two-phase flow during a transient ""declogging"" event with multiple stopper-rod movements is simulated. The flow rate history during stopper rod movements is obtained from the analytical model, and the hot argon flow rate calculated using the porous-flow model and the initial bubble size estimated. Nail board experiments are also conducted to measure steel surface velocities and mold level profiles. A correlation for surface velocity prediction is proposed based on previous modeling results and validated by another set of measurements using a sub-meniscus velocity control (SVC) device. To further understand particle transport and deposition in wall bounded turbulent flows, direct numerical simulations (DNS) are performed in the continuous phase and a Lagrangian particle tracking algorithm was developed into the in-house code, CU-FLOW, to investigate dispersion and deposition of particles with different Stokes numbers in a square duct flow with and without the effect of imposed magnetic field. A new free-surface tracking model with a moving-grid technique is developed and integrated in the commercial computational fluid dynamics (CFD) package of ANSYS Fluent (v14.5) based on its dynamic mesh feature, which naturally combines with multiphase flow models. This model is validated and adopted to simulate the dynamic responses of mold top surface to flow rate variations in the SEN subject to the upstream actuator position change. The complete model system is applied to investigate the effects of slide-gate dithering on transient single- and multi- phase flows in the caster mold. Mold sloshing is identified by both plant experiments and numerical simulations when the dithering frequency matches with the mold natural frequency determined by its geometry. Mechanism for the liquid steel flow variation to activate this standing wave (mold sloshing) is discussed. Multiphase flow pattern and top surface evolution under a low-frequency dithering trial is studied via numerical simulations. Mold level fluctuations are computed from the dithering simulations are compared in favor with the measurements. Up to this point, the model system has been demonstrated a powerful computational tool to resolve complicated multiphase flows during essentially transient events in continuous steel casting subjected to flow rate (both liquid steel and argon gas) variations. ProQuest Subject Headings: Mechanical engineering, Mechanics, Applied mathematics.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.","Liu, Rui",2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"
"4th International Conference on Frontiers of Manufacturing Science and Measuring Technology, ICFMM 2014","The proceedings contain 487 papers. The special focus in this Conference is on Frontiers of Manufacturing Science and Measuring Technology. The topics include: Analysis of factors influencing the performance of Q460 steel; development and application of aluminium-lithium alloy; corrosion resistance of X52 pipeline steel at sea mud zone; experimental study on the burning behavior of automotive internal decorating materials; mechanical properties analysis and application of metal matrix composites; side effects on enhancing chemical herbicides to control proposal; study on ball milling of TiH<inf>2</inf> and application in energetic materials; study on features and application of typical smart materials; texture evolution of friction-stir-welded 5A30 aluminum alloy plate; the range distribution of Yb ions implanted in 6H silicon carbide; the analysis of temperature field of concrete beam of hydration heat; double sided explosive cladding of stainless steel and regular steel; the influence of particle diameter on analytical results; 90W off-line high power factor correction module design; cavitation effect to the hydraulic piston pump flow pulsation; design and analysis of a novel micro flying head in mass storage system; design of a type of lift coating machine; design of for feel simulator of battleplan; development of the assistant positioning device of parallel machine vice; effect of the rudder on the vibration frequency of wind turbine; hardware design of automotive parking heater remoter and receiver; hydraulic excavator boom lightweight design; numerical investigation of tip clearance flow in an axial compressor cascade; parametric design of tapered end mills with variable pitch for improving stability; research on attitude singularity problem of small tail-sitter aircraft; research on typical vibration accelerations of wind wheel changing with yaw angle; simulation and optimization design of gasoline engine exhaust muffler; simulation study on a parallel hybrid system of hydraulic excavator; the design of the fixture for the thin wall cone parts; the forced response analysis of heavy duty vehicle's cab; UML based design approach for storage system of nano-satellite; modal analysis based on finite element jaw crusher rotor; parameter identification of a plastic damage model; stress forming of fine metal wire based on ultrasonic vibration; modeling and predicting surface roughness for the grinding process; design, fabrication and evaluation of a multifunctional jacket with optoelectronic effects; a control system on soil temperature; a novel optical fiber displacement measurement system; a method of correcting brightness gradient in thermal infrared image; the intelligent traffic monitoring system based on Beidou research and design; the resulting toolpath of turning a special surface on CNC lathe; vehicle trajectory prediction based on road recognition; an approach for detecting illegal load in wireless power transfer system; analysis and research of Rogowski coil current sensing technology; approach to design digital meter for monitoring armored vehicle engine; design for speed measuring of railway crossing based on track circuit; design of automatic milking controller; design of plunger limit switch testing system based on virtual instrument technology; empirical likelihood based testing for polynomial regression models; finite element analysis of capacitance sensor; hybrid base and parallel hybrid electric vehicle control strategy; modeling analysis and experimental study of circular piezoelectric unimorph actuator; research on data acquisition technology of wind turbine monitoring system; test and analysis of dynamic characteristics of reinforced concrete arch bridge; the design of communication system for wearable health monitoring equipment; the development of an automatic ultrasonic non-destructive testing system; static characteristic test of high temperature pressure sensor; the measurement of concrete temperature distribution based on VB; a fast pose estimation method of four-rotor aircraft; effect of vehicle speed on contact pressure and current collecting quality; computer modal analysis of the two-dimensional precision turntable; the localization method of ALV based on lateral dynamics; a class of difference inequality with three iterative summation; a new method for code optimization; a new rough set model and its application; an improved decision strategy of network routing based on ant colony algorithm; computer network security strategy research; face recognition image processing design research; mobile learning mode based on 3G; prediction of road congestion level based on Bayes algorithm; research of text recognition based on natural scene; simulation of clouds dynamics based on the change of sight distance; speaker recognition techniques; the SWOT analysis of the application of cloud computing to archives informatization; weakness and improvement of an efficient key agreement protocol; image decomposition and in painting based on Besov and Hilbert-Sobolev space; research on design of network curriculum; optimal power flow solution using the harmony search algorithm; getting the health management information system building; cross-cultural awareness in college english teaching; the research on the core system of athletics; on the design and development of micro-course; a traffic capacity model of lane occupation; design and implementation of storage management system based on Java; material selection and management based on cybernetics; the urban and rural public transit demand analysis in Hohhot; computer database system in the application of information management and theoretical framework for innovation design with optimised customization.",,2014,"[""Engineering Village""]","Rejeitado: CR9","Rejeitado: CR9"