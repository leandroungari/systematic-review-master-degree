@inproceedings{20122115050758,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Performance-reliability tradeoff analysis for multithreaded applications},
journal = {Proceedings -Design, Automation and Test in Europe, DATE},
author = {Oz, Isil and Topcuoglu, Haluk Rahmi and Kandemir, Mahmut and Tosun, Oguz},
year = {2012},
pages = {893 - 898},
issn = {15301591},
address = {Dresden, Germany},
abstract = {Modern architectures become more susceptible to transient errors with the scale down of circuits. This makes reliability an increasingly critical concern in computer systems. In general, there is a tradeoff between system reliability and performance of multithreaded applications running on multicore architectures. In this paper, we conduct a performance-reliability analysis for different parallel versions of three data-intensive applications including FFT, Jacobi Kernel, andWater Simulation. We measure the performance of these programs by counting execution clock cycles, while the system reliability is measured by Thread Vulnerability Factor (TVF) which is a recently-proposed metric. TVF measures the vulnerability of a thread to hardware faults at a high level. We carry out experiments by executing parallel implementations on multicore architectures and collect data about the performance and vulnerability. Our experimental evaluation indicates that the choice is clear for FFT application and Jacobi Kernel. Transpose algorithm for FFT application results in less than 5% performance loss while the vulnerability increases by 20% compared to binary-exchange algorithm. Unrolled Jacobi code reduces execution time up to 50% with no significant change on vulnerability values. However, the tradeoff is more interesting for Water Simulation where nsquared version reduces the vulnerability values significantly by worsening the performance with similar rates compared to faster but more vulnerable spatial version. &copy; 2012 EDAA.<br/>},
key = {Reliability analysis},
keywords = {Fast Fourier transforms;Parallel algorithms;Parallel architectures;Software architecture;},
note = {Data-intensive application;Experimental evaluation;Multi-threaded application;Multicore architectures;Parallel implementations;Performance reliability;Reliable Parallel;Thread vulnerabilities;},
} 


@article{20182905549322,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Do Planning and Design Policies and Procedures Matter In Microclimate Management and Urban Heat Mitigation?},
journal = {ProQuest Dissertations and Theses Global},
author = {Heris, Mehdi Pourpeikari},
year = {2018},
abstract = {In this research, I developed a method for analyzing how urban form affects urban microclimate and how planning and design policies shape urban form. I scrutinized the policies, contexts, and implementation procedures of urban redevelopment projects in two cities in the Denver metropolitan area. Both the Belmar (located in Lakewood, Colorado) and 29th Street Mall (located in Boulder, Colorado) projects were conventional indoor malls developed in the 1960s, that declined in the 1990s, and were redeveloped in the early 2000s to create mixed-use walkable urban centers. The zoning approaches (Belmar used Form-based code, 29th Street Mall used Euclidian), design guidelines, and local politics of these two projects were significantly different in ways that resulted in different built environments after redevelopment. My research aim is to explore how these differences can potentially impact urban climate systems with positive or negative influences on climate variables such as wind, ambient temperature and mean radiant temperature. My research answers two research questions: (1) to what extent are different zoning approaches (Euclidian and Form-based) capable of mitigating urban heat? (2) To what extent are planning contexts, including local politics, important in developing a climate responsive project? I found that a series of variables affected the process of planning and design in these sites. The findings show that the choices made in the development management affected the microclimate of both sites. The built form of Belmar is more effective in heat mitigation and creating a more comfortable temperature. Based on the results of both my microclimate simulations and the policy analysis, I identified five main themes in the development management of both sites that control microclimate outcomes and show why Belmar ultimately was a better project. These themes, which are also relevant for other environmental objectives, are: (1) urban vision, (2) land use and building form controls, (3) design guidelines, (4) public financing, and (5) condemnation/ownership factors. These five policy themes I have identified explain how a combination of context and choice variations affect the quality of built environments. Although many regulations did not intentionally address microclimate issues, elements that were considered for improving walkability contributed to heat mitigation as well. The simulation of policy and form variations showed that the built environment of Belmar has been more successful in mitigating urban heat. Conflicts and a complex planning history in Boulder led to a very slow and ineffective review process that created a less climate responsive built environment. ProQuest Subject Headings: Urban planning, Geographic information science and geodesy, Climate change.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.},
key = {Climate change},
keywords = {Design;Geodesy;Geographic information systems;Land use;Urban planning;Zoning;},
note = {Comfortable temperature;Development management;Environmental objectives;Geographic information science;Mean radiant temperature;Planning and design;Research questions;Urban redevelopment projects;},
} 


@inproceedings{20160902040342,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Interpretation of non-isothermal testing data based on the numerical simulation},
journal = {Society of Petroleum Engineers - SPE Russian Petroleum Technology Conference},
author = {Valiullin, Rim and Ramazanov, Ayrat and Khabirov, Timur and Sadretdinov, Alexander and Kotlyar, Lev and Sidorova, Mariya and Fedorov, Vyacheslav and Salimgareeva, Elmira},
year = {2015},
address = {Moscow, Russia},
abstract = {The goal of this paper is demonstration of an approach to enhancing reliability of quantitative interpretation of well tests data. The new approach comprises joint interpretation of transient temperature, pressure and flow rate data taking into account flow rate history before the test. It provides additional information about near-wellbore zone, flow profiles and hydrodynamic properties of a multilayer reservoir. The transient model for solving the coupled thermal-hydraulic problems in the reservoir, formation, and wellbore is described in the paper. It takes into account heat convection and conduction, Joule- Thomson, adiabatic effects, thermal effect caused by degassing as well as mixing of fluids in the well, heat transfer between wellbore fluid and formation, frictional heat release and thermal effect of fluid compression or expansion. This model allows simulating flow behind casing, crossflows during shut-in and different properties of the near-wellbore zone. Transient temperature and pressure fields, and temperature profiles along the wellbore are simulated for a given flow rate history. The model parameters are determined by solving the inverse problem based on a comparison of simulated temperature data with measurements in the wellbore. Application of new approach to quantitative interpretation of well test data and developed transient numerical code are demonstrated on the example of field data from the Bashneft - Polus oil well. The data from production logging during flow at different choke sizes and transient station measurements of temperature and pressure after change of regimes are available and simulated with the developed code. Standard Pressure Transient Analysis (PTA) was used for pressure and flow rate data interpretation, while available temperature measurements were simulated with a numerical code developed for solving the coupled thermal-hydraulic multiphase problems by Bashkir State University and Schlumberger Moscow Research center. The paper demonstrates the results of quantitative interpretation of the field data and sensitivity study of the model parameters to uncertainties in measurements, flow rate history, thermal properties and other input data.<br/> Copyright 2015, Society of Petroleum Engineers.},
key = {Flow rate},
keywords = {Boreholes;Codes (symbols);Computational fluid dynamics;Heat convection;Inverse problems;Oil field equipment;Oil well logging;Oil well testing;Oil wells;Parameter estimation;Petroleum reservoir evaluation;Problem solving;Temperature;Temperature measurement;Transient analysis;Uncertainty analysis;Well logging;},
note = {Convection and conduction;Hydrodynamic properties;Multilayer reservoirs;Pressure transient analysis;Quantitative interpretation;Temperature and pressures;Thermal-hydraulic problem;Transient temperature;},
} 


@inproceedings{20150800552383,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Tutorial on state variable based plasticity: An Abaqus UHARD subroutine},
journal = {8th South African Conference on Computational and Applied Mechanics, SACAM 2012 - Conference Proceedings},
author = {Van Rensburg, Jansen G.J. and Kok, S.},
year = {2012},
pages = {158 - 165},
address = {Johannesburg, South africa},
abstract = {Since plasticity is path dependent, it is necessary to properly take into account the deformation, strain rate and temperature history in applications such as crash worthiness and ballistics simulations. To accurately model the evolution of the yield stress, the incremental (differential) update from a previous converged time step is required instead of a closed form expression that relates flow stress to plastic strain. Elastoviscoplastic models that make use of state variables better capture the physical phenomenon of a perceived lag between a change in strain rate or temperature and the subsequent stress response. It is impossible to capture this when making use of a closed form expression or data table based method. One model that makes use of an evolving state variable is the Mechanical Threshold Stress (MTS) model. In this paper, the implementation of the MTS model into an Abaqus user hardening (UHARD) subroutine is discussed and the code is included. The aim is not to improve on the current knowledge of the model, but to illustrate the ease with which a state variable based plasticity model can be implemented and used instead of an empirical (closed form expression) or data table based method. The MTS model is compared to the Johnson-Cook plasticity model which takes the form of a simple closed form expression relating yield stress to plastic strain as a function of temperature and strain rate. The model parameters are calibrated using isothermal, constant strain rate experimental data and then used to predict the stress response for a strain rate jump test and a temperature change test.<br/> &copy;SACAM 2012.},
key = {Strain rate},
keywords = {Plastic deformation;Plasticity;Stresses;Subroutines;Yield stress;},
note = {Closed-form expression;Constant strain rate;Elasto-viscoplastic models;Johnson-Cook plasticities;Mechanical threshold stress;On state;Uhard;User subroutine;},
} 


@article{20130515972197,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Coronal heating by the partial relaxation of twisted loops},
journal = {Astronomy and Astrophysics},
author = {Bareford, M.R. and Hood, A.W. and Browning, P.K.},
volume = {550},
year = {2013},
issn = {00046361},
abstract = {Context. Relaxation theory offers a straightforward method for estimating the energy that is released when continual convective driving causes a magnetic field to become unstable. Thus, an upper limit to the heating caused by ensembles of nanoflaring coronal loops can be calculated and checked against the level of heating required to maintain observed coronal temperatures (T &gsim; 10<sup>6</sup>K). Aims. We present new results obtained from nonlinear magnetohydrodynamic (MHD) simulations of idealised coronal loops. All of the initial loop configurations discussed are known to be linearly kink unstable. The purpose of this work is to determine whether or not the simulation results agree with Taylor relaxation, which will require a modified version of relaxation theory applicable to unbounded field configurations. In addition, we show for two cases how the relaxation process unfolds. Methods. A three-dimensional (3D) MHD Lagrangian-remap code is used to simulate the evolution of a line-tied cylindrical coronal loop model. This model comprises three concentric layers surrounded by a potential envelope; hence, being twisted locally, each loop configuration is distinguished by a piecewise-constant current profile, featuring three parameters. Initially, all configurations carry zero-net-current fields and are in ideally unstable equilibrium. The simulation results are compared with the predictions of helicity-conserving relaxation theory. Results. For all simulations, the change in helicity is no more than 2% of the initial value; also, the numerical helicities match the analytically-determined values. Magnetic energy dissipation predominantly occurs via shock heating associated with magnetic reconnection in distributed current sheets. The energy release and final field profiles produced by the numerical simulations are in agreement with the predictions given by a new model of partial relaxation theory: the relaxed field is close to a linear force free state; however, the extent of the relaxation region is limited, while the loop undergoes some radial expansion. Conclusions. The results presented here support the use of partial relaxation theory, specifically, when calculating the heating-event distributions produced by ensembles of kink-unstable loops. The energy release increases with relaxation radius; but, once the loop has expanded by more than 50%, further expansion yields little more energy. We conclude that the relaxation methodology may be used for coronal heating studies. &copy; 2013 ESO.<br/>},
key = {Magnetoplasma},
keywords = {Aerodynamic heating;Energy dissipation;Magnetic fields;Magnetohydrodynamics;Plasma (human);Plasma diagnostics;Stability;Sun;},
note = {Coronal temperatures;Magnetic reconnections;Magnetohydrodynamic simulations;Piece-wise constants;Straight-forward method;Sun : corona;Threedimensional (3-d);Unstable equilibriums;},
URL = {http://dx.doi.org/10.1051/0004-6361/201219725},
} 


@article{20123515372435,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Vdiff: A program differencing algorithm for Verilog hardware description language},
journal = {Automated Software Engineering},
author = {Duley, Adam and Spandikow, Chris and Kim, Miryung},
volume = {19},
number = {4},
year = {2012},
pages = {459 - 490},
issn = {09288910},
abstract = {During code review tasks, comparing two versions of a hardware design description using existing program differencing tools such as diff is inherently limited because these tools implicitly assume sequential execution semantics, while hardware description languages are designed to model concurrent computation. We designed a position-independent differencing algorithm to robustly handle language constructs whose relative orderings do not matter. This paper presents Vdiff, an instantiation of this position-independent differencing algorithm for Verilog HDL. To help programmers reason about the differences at a high-level, Vdiff outputs syntactic differences in terms of Verilog-specific change types. The quantitative evaluation of Vdiff on two open source hardware design projects shows that Vdiff is very accurate, with overall 96.8 % precision and 97.3 % recall when using manually classified differences as a basis of comparison. Vdiff is also fast and scalable-it processes the entire revision history of nine open projects all under 5.35 minutes. We conducted a user study with eight hardware design experts to understand how the program differences identified by the experts match Vdiff's output. The study results show that Vdiff's output is better aligned with the experts' classification of Verilog changes than an existing textual program differencing tool. &copy; 2012 Springer Science+Business Media, LLC.<br/>},
key = {Computer hardware description languages},
keywords = {Hardware;Open source software;Open systems;Semantics;},
note = {Concurrent computation;Differencing algorithm;Open-source hardwares;Program differencing;Quantitative evaluation;Sequential execution;Software Evolution;Verilog hardware description languages;},
URL = {http://dx.doi.org/10.1007/s10515-012-0107-6},
} 


@inproceedings{20144600206114,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mathews' diagram and Euclid's line - Fifty years ago -},
journal = {Proceedings - 40th International Computer Music Conference, ICMC 2014 and 11th Sound and Music Computing Conference, SMC 2014 - Music Technology Meets Philosophy: From Digital Echos to Virtual Ethos},
author = {Chowning, John},
year = {2014},
pages = {7 - 12},
address = {Athens, Greece},
abstract = {Making the science and technology of computer music comprehensible to musicians and composers who had little or no background therein was a part of Max Mathews' genius. In this presentation I will show how a simple diagram led to the essential understanding of Claude Shannon's sampling theorem, which in turn opened up a conceptual path to composing music for loudspeakers that had nothing to do with wires, cables and electronic devices, but led to learning how to program a computer- to write code. The change from device-determined output (analog) to program-determined output (digital) was a major change in paradigm that led to my realization of an integral sound spatialization system that would have been impossible for me to achieve in any other medium. Along the way, the discovery of FM Synthesis provided not only a means of creating diverse spectra but coupled with a ratio from Euclid's Elements produced an unusual and productive connection between spectral space and pitch space and a path that leads &mellip;? Copyright:<br/> &copy; 2014 Curtis Roads.},
key = {Computer music},
keywords = {Computation theory;},
note = {Electronic device;Science and Technology;Shannon's sampling theorems;Sound spatialization;Spectral spaces;},
} 


@inproceedings{20173504099656,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dot-product based preference preserved hashing for fast collaborative filtering},
journal = {IEEE International Conference on Communications},
author = {Zhang, Yan and Yang, Guowu and Hu, Lin and Wen, Hong and Wu, Jinsong},
year = {2017},
issn = {15503607},
address = {Paris, France},
abstract = {Recommendation is widely used to deal with information overloading by suggesting items based on historical information of users. One of the most popular recommendation techniques is matrix factorization (MF), in which the preferences of users are estimated by dot products of their real latent factors between users and items. Although MF can achieve high recommendation accuracy, it suffers from efficiency issues when making preferences ranking in real space. Hash retrieval technique can be applied to recommender systems to speed up preferences ranking. Due to the existence of discrete constraints in learning hash codes, it is possible to exploit a two-stage learning procedure according to most existing methods. This two-stage procedure consists of relaxed optimization by discarding discrete constraints and subsequent binary quantization. However, existing methods have not been able to well handle the change of dot product arising from quantization. To this end, we propose a dot-product based preference preserved hashing method, which quantizes both norm and cosine similarity in dot product respectively. We also design an algorithm to optimize the bit length for norm quantization. Based on the evaluation to several datasets, the proposed framework shows consistent superiority to the competing baselines even though only using shorter binary code. &copy; 2017 IEEE.},
URL = {http://dx.doi.org/10.1109/ICC.2017.7996544},
} 


@article{20181605032286,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Numerical analysis of the counter-intuitive dynamic behavior of the elastic-plastic pin-ended beams under impulsive loading with regard to linear hardening effects},
journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
author = {Shams Alizadeh, Mehdi and Heidari Shirazi, Kourosh and Moradi, Shapour and Sedighi, Hamid Mohammad},
volume = {232},
number = {24},
year = {2018},
pages = {4588 - 4600},
issn = {09544062},
abstract = {The counter-intuitive behavior where the permanent deflection of elastic-plastic beam come to rest in the opposite direction of the impulsive loading, normally appears and disappears abruptly, in certain small ranges of loading and structural parameters. One of the most important issues in the study of this phenomenon is the determination of the influence of different parameters. This work is aimed to study the effects of hardening in counter-intuitive dynamic behavior of elastic-plastic pin-ended beams under impulsive loading. This has been done by developing the proposed Galerkin numerical model and presenting a novel algorithm. The Galerkin method as well as the commercial finite element code ANSYS/LS-DYNA is applied to study this phenomenon. In order to account for the hardening effects in Galerkin method, a new algorithm is proposed. The time history curves for mid-span of the beam is studied in detail and the region of the occurrence of the counter-intuitive behavior is determined. Furthermore, using the finite element software, energy diagrams of the beam are also derived. It has been found that the counter-intuitive behavior is a phenomenon, which is very sensitive to loading, therefore it may appear with a little change in the amount of loading. The results also show that although both methods predict one continuous region of loading for the occurrence of this phenomenon in elastic-perfectly plastic beams, still there are two continuous distinct regions of loading, when considering the hardening effects, for this phenomenon. In addition, this anomalous behavior would occur in the proper ratios of kinetic to internal energy and when considering the linear hardening effects, the possibility of the occurrence of the counter-intuitive behavior exists in a wider domain of energy ratio.<br/> &copy; IMechE 2018.},
key = {Loading},
keywords = {Elastoplasticity;Finite element method;Galerkin methods;Hardening;},
note = {Anomalous response;Counter-intuitive behavior;Elastic-plastic analysis;Impulsive loading;Kinematic hardening;},
URL = {http://dx.doi.org/10.1177/0954406217753456},
} 


@inproceedings{20163702786193,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Batch model for batched timestamps data analysis with application to the SSA disability program},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
author = {Yue, Qingqi and Yuan, Ao and Che, Xuan and Huynh, Minh and Zhou, Chunxiao},
volume = {13-17-August-2016},
year = {2016},
pages = {343 - 352},
address = {San Francisco, CA, United states},
abstract = {The Office of Disability Adjudication and Review (ODAR) is responsible for holding hearings, issuing decisions, and reviewing appeals as part of the Social Security Administration's disability determining process. In order to control and process cases, the ODAR has established a Case Processing and Management System (CPMS) to record management information since December 2003. The CPMS provides a detailed case status history for each case. Due to the large number of appeal requests and limited resources, the number of pending claims at ODAR was over one million cases by March 31, 2015. Our National Institutes of Health (NIH) team collaborated with SSA and developed a Case Status Change Model (CSCM) project to meet the ODAR's urgent need of reducing backlogs and improve hearings and appeals process. One of the key issues in our CSCM project is to estimate the expected service time and its variation for each case status code. The challenge is that the systems recorded job departure times may not be the true job finished times. As the CPMS timestamps data of case status codes showed apparent batch patterns, we proposed a batch model and applied the constrained least squares method to estimate the mean service times and the variances. We also proposed a batch search algorithm to determine the optimal batch partition, as no batch partition was given in the real data. Simulation studies were conducted to evaluate the performance of the proposed methods. Finally, we applied the method to analyze a real CPMS data from ODAR/SSA.<br/> &copy; 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
key = {Batch data processing},
keywords = {Application programs;Data handling;Data mining;Information management;Least squares approximations;},
note = {Constrained least squares;Information matrix;Management systems;Service time;Time stamps;},
URL = {http://dx.doi.org/10.1145/2939672.2939706},
} 


@article{20142917943529,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Application of a Fuzzy Feasibility Bayesian Probabilistic Estimation of supply chain backorder aging, unfilled backorders, and customer wait time using stochastic simulation with Markov blankets},
journal = {Expert Systems with Applications},
author = {Rodger, James A.},
volume = {41},
number = {16},
year = {2014},
pages = {7005 - 7022},
issn = {09574174},
abstract = {Because supply chains are complex systems prone to uncertainty, statistical analysis is a useful tool for capturing their dynamics. Using data on acquisition history and data from case study reports, we used regression analysis to predict backorder aging using National Item Identification Numbers (NIINs) as unique identifiers. More than 56,000 NIINs were identified and used in the analysis. Bayesian analysis was then used to further investigate the NIIN component variables. The results indicated that it is statistically feasible to predict whether an individual NIIN has the propensity to become a backordered item. This paper describes the structure of a Bayesian network from a real-world supply chain data set and then determines a posterior probability distribution for backorders using a stochastic simulation based on Markov blankets. Fuzzy clustering was used to produce a funnel diagram that demonstrates that the Acquisition Advice Code, Acquisition Method Suffix Code, Acquisition Method Code, and Controlled Inventory Item Code backorder performance metric of a trigger group dimension may change dramatically with variations in administrative lead time, production lead time, unit price, quantity ordered, and stock. Triggers must be updated regularly and smoothly to keep up with the changing state of the supply chain backorder trigger clusters of market sensitiveness, collaborative process integration, information drivers, and flexibility. &copy; 2014 Elsevier Ltd. All rights reserved.<br/>},
key = {Fuzzy logic},
keywords = {Artificial intelligence;Barium compounds;Bayesian networks;Codes (symbols);Decision support systems;Probability distributions;Regression analysis;Stochastic models;Stochastic systems;Supply chains;Uncertainty analysis;},
note = {Backorders;Collaborative process;Identification number;Markov Blankets;Performance metrices;Probabilistic estimation;Stochastic simulations;Unique identifiers;},
URL = {http://dx.doi.org/10.1016/j.eswa.2014.05.012},
} 


@inproceedings{20181004887179,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Most common fixes students use to improve the correctness of their programs},
journal = {Proceedings - Frontiers in Education Conference, FIE},
author = {De Souza, Draylson Micael and Kolling, Michael and Barbosa, Ellen Francine},
volume = {2017-October},
year = {2017},
pages = {1 - 9},
issn = {15394565},
address = {Indianapolis, IN, United states},
abstract = {Teach students how to program is the main goal of most introductory CS courses. In fact, programming is one of the basic skills a professional in CS should have. However, there are many difficulties students face when they are learning how to program and, consequently, it is common introductory programming courses have high dropout rates. The purpose of this paper is to identify and discuss the most common fixes students use to improve the correctness of their programs. The findings can be useful to help students to produce more correct programs and highlight issues about possible difficulties they are having. To do so, we used the BLACKBOX data collection, which stores the actions of the BLUEJ programming environment users. The main idea was to observe the modifications students did in their source codes that made a failed JUNIT test case become succeeded. The results suggest the majority of fixes students use in their source codes are related either to the change of expressions or to the restructuring of code, reflecting difficulties in logic and problem solving among students.<br/> &copy; 2017 IEEE.},
key = {Students},
keywords = {Codes (symbols);Computer programming;Problem solving;Teaching;},
note = {Black boxes;Data collection;Introductory programming course;Programming environment;Source codes;Test case;},
URL = {http://dx.doi.org/10.1109/FIE.2017.8190524},
} 


@article{20181104913029,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Multiple Lower BAC offenders: Characteristics and response to remedial interventions},
journal = {Accident Analysis and Prevention},
author = {Wickens, Christine M. and Flam-Zalcman, Rosely and Stoduto, Gina and Docherty, Chloe and Thomas, Rita K. and Watson, Tara Marie and Matheson, Justin and Mehra, Kamna and Mann, Robert E.},
volume = {115},
year = {2018},
pages = {110 - 117},
issn = {00014575},
abstract = {Background: In recent years, there has been increasing attention to &ldquo;lower BAC&rdquo; drinking drivers, typically those whose blood alcohol content (BAC) is under the legal limits defined in criminal law. In 2009, legislation was enacted in Ontario, Canada that enabled police to issue roadside license suspensions to individuals caught driving with BAC between 0.05% and 0.08%, known as the &ldquo;warn range&rdquo;. Multiple warn range (MWR) offenders are required to attend the Back on Track (BOT) remedial measures program. This study aimed to provide: (1) a preliminary characterization of MWR drivers charged under warn range legislation; and (2) an initial assessment of outcomes associated with BOT participation among MWR offenders. Methods: A subsample of 727 MWR offenders was drawn from program records, and compared to samples of 3597 first-time Criminal Code (CC) offenders (those caught driving with a BAC of 0.08% or higher) and 359 second-time CC offenders. To provide an initial assessment of outcomes associated with BOT participation, another subsample consisted of 394 MWR participants from whom pre- and post-workshop questionnaires were collected and successfully matched using probabilistic matching processes. Results: Similarities in demographic profile and driving history between MWR and first-time CC participants were apparent. MWR offenders scored higher on risk of problem drinking and drink-driving recidivism than either of the CC offender groups. Second-time CC offenders scored higher on these measures than first-time CC offenders. Following BOT participation, MWR participants demonstrated positive change including improved knowledge of and intentions to avoid drink-driving. Conclusions: MWR offenders share a similar demographic profile to that of first-time CC offenders and they report significantly higher risk of problem drinking and recidivism. MWR offenders may include high-functioning problem drinkers who are likely to continue drink-driving and who may escalate to a CC drink-driving offense. Like CC offenders, MWR offenders benefited from BOT participation.<br/> &copy; 2018},
key = {Alcoholic beverages},
keywords = {Blood;Crime;Population statistics;Surveys;},
note = {Blood alcohol contents;Blood-alcohol concentration;Demographic profile;Drink driving;Initial assessment;Probabilistic matching;Recidivism;Remedial measures;},
URL = {http://dx.doi.org/10.1016/j.aap.2018.02.019},
} 


@inproceedings{20161702277034,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Numerical simulation of the aerodynamic behavior of high velocity trains under synthetic crosswinds of different shear and turbulence characteristics},
journal = {Civil-Comp Proceedings},
author = {Garcia, J. and Munoz, J. and Jimenez, A. and Migoya, E. and Crespo, A.},
volume = {104},
year = {2014},
issn = {17593433},
abstract = {A numerical simulation of the aerodynamic behavior of high-speed trains under synthetic crosswinds at a 90&deg; yaw angle is presented. The train geometry is the aerodynamic train model (ATM). Flow description based on numerical simulations is obtained using large eddy simulation (LES) and the commercial code ANSYS-Fluent V14.5. A crosswind whose averaged velocity and turbulence characteristics change with distance to the ground is imposed. Turbulent fluctuations that vary temporally and spatially are simulated with TurbSim code. The crosswind boundary condition is calculated for the distance the train runs during a simulation period. The inlet streamwise velocity boundary condition is generated using Taylor's frozen turbulence hypothesis. The model gives a time history of the force and moments acting on the train; this includes averaged values, standard deviations and extreme values. Of particular interest are the spectra of the forces and moments, and the admittance spectra. For comparison, results obtained with LES and a uniform wind velocity fluctuating in time, and results obtained with Reynolds averaged Navier Stokes equations (RANS), and the averaged wind conditions, are also presented.<br/> &copy; Civil-Comp Press, 2014.},
key = {Navier Stokes equations},
keywords = {Aerodynamics;Atmospheric turbulence;Boundary conditions;Computational fluid dynamics;Large eddy simulation;Numerical models;Railroad cars;Railroad transportation;Railroads;Shear flow;Velocity;},
note = {Cross wind;Frozen-turbulence hypothesis;High speed train (HST);Reynolds Averaged Navier-Stokes Equations;Stream-wise velocities;Train aerodynamics;Turbulence characteristics;Turbulent fluctuation;},
} 


@inproceedings{20144200110494,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A replication case study to measure the architectural quality of a commercial system},
journal = {International Symposium on Empirical Software Engineering and Measurement},
author = {Reimanis, Derek and Izurieta, Clemente and Luhr, Rachael and Xiao, Lu and Cai, Yuanfang and Rudy, Gabe},
year = {2014},
pages = {IEEE Software; Microsoft Research; Politecnico di Torino; Telecom Italia JOL (Joint Open Lab); Telecom Italia Lab - },
issn = {19493770},
address = {Torino, Italy},
abstract = {Context: Long-term software management decisions are directly impacted by the quality of the software's architecture. Goal: Herein, we present a replication case study where structural information about a commercial software system is used in conjunction with bug-related change frequencies to measure and predict architecture quality. Method: Metrics describing history and structure were gathered and then correlated with future bug-related issues; the worst of which were visualized and presented to developers. Results: We identified dependencies between components that change together even though they belong to different architectural modules, and as a consequence are more prone to bugs. We validated these dependencies by presenting our results back to the developers. The developers did not identify any of these dependencies as unexpected, but rather architectural necessities. Conclusions: This replication study adds to the knowledge base of CLIO (a tool that detects architectural degradations) by incorporating a new programming language (C++) and by externally replicating a previous case study on a separate commercial code base. Additionally, we provide lessons learned and suggestions for future applications of CLIO.<br/> &copy; 2014 Authors.},
key = {C++ (programming language)},
keywords = {Knowledge based systems;Software engineering;Static analysis;},
note = {Architectural quality;Commercial software systems;grime;modularity violations;replication;Software management;Structural information;Technical debts;},
URL = {http://dx.doi.org/10.1145/2652524.2652581},
} 


@article{20174704446895,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Schema Evolution Survival Guide for Tables: Avoid Rigid Childhood and Youre En Route to a Quiet Life},
journal = {Journal on Data Semantics},
author = {Vassiliadis, Panos and Zarras, Apostolos V.},
volume = {6},
number = {4},
year = {2017},
pages = {221 - 241},
issn = {18612032},
abstract = {In this paper, we study the factors that relate to the survival of a table in the context of schema evolution in open-source software. We study the history of the schema of eight open-source software projects that include relational databases and extract patterns related to the survival or death of their tables. Our study shows that the probability of a table with a wide schema (i.e., a large number of attributes) being removed is systematically lower than average. Activity and duration are related to survival too. Rigid tables, without any change to their schema, are more likely to be removed than tables that sustain changes. Durations of dead and survival tables demonstrate a mirror image: dead tables&rsquo; durations are mostly short, whereas survivor tables gravitate toward higher durations. Our findings are mostly summarized by a pattern, which we call electrolysis pattern, due to its diagrammatic representation, stating that dead and survivor tables live quite different lives: tables typically die shortly after birth, with short durations and mostly no updates, whereas survivors mostly live quiet lives with few updates&mdash;except for a small group of tables with high update ratios that are characterized by high durations and survival. Equally important is the evidence that schema evolution suffers from the antagonism of gravitation to rigidity, i.e., the tendency to minimize evolution as much as possible in order to minimize the resulting impact to the surrounding code. Several factors contribute to this observation: the absence of long durations in removed tables, the low percentage of tables whose schema size is scaled up or down, and the low numbers of tables with a high rate of updates, contrasted to the high numbers of tables with zero or few updates. We complement our findings with explanations and recommendations to developers.<br/> &copy; 2017, Springer-Verlag GmbH Germany.},
key = {Open source software},
keywords = {Open systems;},
note = {Diagrammatic representations;High rate;Long duration;Mirror images;Open source software projects;Relational Database;Schema evolution;Short durations;},
URL = {http://dx.doi.org/10.1007/s13740-017-0083-x},
} 


@inproceedings{20175104566581,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ACM International Conference Proceeding Series},
journal = {ACM International Conference Proceeding Series},
volume = {Part F132090},
year = {2017},
pages = {ACM SIGCHI; Stanford University - },
address = {Stanford, CA, United states},
abstract = {The proceedings contain 20 papers. The topics discussed include: sustaining making in the era of accountability: STEM integration using e-textiles materials in a high school physics class; from classroom-making to functional-making: a study in the development of making literacy; equity-oriented STEM-rich making among youth from historically marginalized communities; how time gets used in afterschool maker programs; using assessment tools at an after school youth maker program; fidgeting with fabrication: students with ADHD making tools to focus; physical making online: a study of children's maker websites; teaching practices for making e-textiles in high school computing classrooms; Roll It Wall: developing a framework for evaluating practices of learning; framing MakerSpace communities; learning by fixing and designing problems: developing a reconstruction kit for debugging e-textiles; towards fostering the maker mindset: a case study of Koby's engagement in making and tinkering program with squishy circuits; dancing robots: a collaboration between elementary school and university engineering students; app making for pro-social and environmental change at an equity-oriented Makeathon; designing for rightful presence in stem-rich making; connecting space and narrative in culturally responsive making in ARIS with indigenous youth; the role of computational thinking in making: how beginning youth makers encounter &amp; appropriate ct practices in making; texture, buttons, sound and code: the role of modal preference in the formation of leadership identities; and MakerSpaces as figured worlds.<br/>},
} 


@inproceedings{20153201106417,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Bowl Shape Design Optimization for Engine-Out PM Reduction in Heavy Duty Diesel Engine},
journal = {SAE Technical Papers},
author = {Lee, Jongyoon and Lee, Sangyul and Kim, Jungho and Kim, Duksang},
volume = {2015-April},
number = {April},
year = {2015},
pages = {AVL; Continental; et al.; FEV; Fiat Chrysler Automobiles; IAV Automotive Engineering - },
address = {Detroit, MI, United states},
abstract = {This paper shows development challenges for 6 liter heavy duty off-road diesel engines to meet the Tier4 final emission regulations with a base diesel engine compliant with Tier4 interim emission regulations. Even if an after-treatment system helps to reduce emissions, quite amount of particulate matters (PM) reduction is still necessary since a diesel particulate filter (DPF) system is supposed to be excluded in Tier4 final diesel engine. The objective of this research is to see if the base engine has a feasibility to meet Tier4 final emission regulations by a change of piston bowl geometry without DPF. Quite amount of PM can be reduced by piston bowl geometry because piston bowl geometry is a very important part that enhances air and fuel mixing process that help the combustion process. Local air to fuel ratio and mixing rate could be improved by manipulating the gas motions of in-cylinder and PM could be reduced In this study, multidimensional CFD code KIVA-3V coupled with detailed chemical kinetics is used to perform combustion simulations. The validation between engine experiment results and simulation results was established at first. Then, with visualized in-cylinder images analysis, geometrical parameters were changed from the conceptual ULPC (Ultra Low Particulate matter Combustion) piston bowl geometry. After several iterations of analysis and geometrical parameter changes, main geometrical parameters were finally found and optimized piston bowl geometry was obtained. To prove the simulation results, experiments were performed with installation of final version of combustion bowl geometry in C1-8 mode which is one of main test cycle to meet Tier4 final emission regulations. Finally, Tier4 final emission regulations were met with changing the piston bowl geometry without DPF. Copyright &copy; 2015 SAE International.},
URL = {http://dx.doi.org/10.4271/2015-01-0789},
} 


@article{20122715219702,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A peculiar of star in the Local Group galaxy IC 1613},
journal = {Astronomy and Astrophysics},
author = {Herrero, A. and Garcia, M. and Puls, J. and Uytterhoeven, K. and Najarro, F. and Lennon, D.J. and Rivero-Gonzalez, J.G.},
volume = {543},
year = {2012},
issn = {00046361},
abstract = {Context. Results from the theory of radiatively driven winds are nowadays incorporated in stellar evolutionary and population synthesis models, and are used in our interpretation of the observations of the deep Universe. Yet, the theory has been confirmed only until Small Magellanic Cloud metallicities. Observations and analyses of O-stars at lower metallicities are difficult, but much needed to prove the theory. Aims. We have observed GHV-62024, an O6.5 IIIf star in the low-metallicity galaxy IC 1613 (Z &asymp; 0.15 Z<inf>&odot;</inf>) to study its evolution and wind. According to a previous preliminary analysis that was subject to significant restrictions this star could challenge the radiatively driven wind theory at low metallicities. Here we present a complete analysis of this star. Methods. Our observations were obtained with VIMOS at VLT, at R &asymp; 2000 and covered approximately between 4000 and 7000 A&ring;. The observations were analysed using the latest version of the model atmosphere code FASTWIND, which includes the possibility of calculating the N iii spectrum. Results. We obtain the stellar parameters and conclude that the star follows the average wind momentum-luminosity relationship (WLR) expected for its metallicity, but with a high value for the exponent of the wind velocity law, &beta;. Comparing this with values of other stars in the literature, we suggest that this high value may be reached because GHV-62024 could be a fast rotator seen at a low inclination angle. We also suggest that this could favour the appearance of the spectral "f"-characterictics. While the derived &beta; value does not change by adopting a lower wind terminal velocity, we show that a wrong V<inf>&infin;</inf>has a clear impact on the position of the star in the WLR diagram. The N and He abundances are very high, consistent with strong CNO mixing that could have been caused by the fast rotation, although we cannot discard a different origin with present data. Stellar evolutionary model predictions are consistent with the star being still a fast rotator. We find again the well-known mass-discrepancy for this star. Conclusions. We conclude that the star follows the WLR expected for its metallicity. The results are consistent with GHV-62024 being a fast rotator seen close to pole-on, strongly contaminated at the surface with CNO products and with a wind structure altered by the fast rotation but without modifying the global WLR. We suggest that this could be a general property of fast rotators. &copy; 2012 ESO.<br/>},
key = {Stars},
keywords = {Astrophysics;Galaxies;Integrated circuits;Metals;Timing circuits;},
note = {Galaxies: individuals;Stars: early-type;Stars: evolution;Stars: mass loss;Stars: Rotation;Stars:fundamental parameters;},
URL = {http://dx.doi.org/10.1051/0004-6361/201118383},
} 


@article{20162302465246,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A review of seismic hazard assessment studies and hazard description in the building codes for Egypt},
journal = {Acta Geodaetica et Geophysica},
author = {Sawires, Rashad and Pelaez, Jose A. and Fat-Helbary, Raafat E. and Ibrahim, Hamza A.},
volume = {51},
number = {2},
year = {2016},
pages = {151 - 180},
issn = {22135812},
abstract = {Reduction of damage in earthquake-prone areas requires modern building codes that should be continuously updated to reflect the improvement in our understanding of the physical effects of earthquake ground shaking on buildings and the increase in the quality and amount of seismological and tectonic studies, among other factors. This work reviews the published seismic hazard assessments available for Egypt as well as the seismic actions included in the building codes, in order to show the state-of-the-art of the seismic hazard assessment studies for the country. The review includes the history and development of seismic hazard assessments and the adoption of seismic building codes in Egypt. All the previous studies were analyzed in order to conclude that a new seismic hazard assessment according to the state-of-the-art is desirable, as well as a change in the hazard description for the actual Egyptian building code.<br/> &copy; 2015, Akad&eacute;miai Kiad&oacute;.},
key = {Hazards},
keywords = {Building codes;Buildings;Codes (symbols);Earthquakes;Reviews;Seismic response;},
note = {Egypt;Ground-shaking;Modern buildings;Physical effects;Seismic action;Seismic hazard assessment;Seismic hazards;State of the art;},
URL = {http://dx.doi.org/10.1007/s40328-015-0117-5},
} 


@inproceedings{20124415618925,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Multidimensional spectral hashing},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Weiss, Yair and Fergus, Rob and Torralba, Antonio},
volume = {7576 LNCS},
number = {PART 5},
year = {2012},
pages = {340 - 353},
issn = {03029743},
address = {Florence, Italy},
abstract = {With the growing availability of very large image databases, there has been a surge of interest in methods based on "semantic hashing", i.e. compact binary codes of data-points so that the Hamming distance between codewords correlates with similarity. In reviewing and comparing existing methods, we show that their relative performance can change drastically depending on the definition of ground-truth neighbors. Motivated by this finding, we propose a new formulation for learning binary codes which seeks to reconstruct the affinity between datapoints, rather than their distances. We show that this criterion is intractable to solve exactly, but a spectral relaxation gives an algorithm where the bits correspond to thresholded eigenvectors of the affinity matrix, and as the number of datapoints goes to infinity these eigenvectors converge to eigenfunctions of Laplace-Beltrami operators, similar to the recently proposed Spectral Hashing (SH) method. Unlike SH whose performance may degrade as the number of bits increases, the optimal code using our formulation is guaranteed to faithfully reproduce the affinities as the number of bits increases. We show that the number of eigenfunctions needed may increase exponentially with dimension, but introduce a "kernel trick" to allow us to compute with an exponentially large number of bits but using only memory and computation that grows linearly with dimension. Experiments shows that MDSH outperforms the state-of-the art, especially in the challenging regime of small distance thresholds. &copy; 2012 Springer-Verlag.<br/>},
key = {Eigenvalues and eigenfunctions},
keywords = {Binary codes;Computer vision;Hamming distance;Optimal systems;Semantics;},
note = {Affinity matrix;Kernel trick;Laplace-Beltrami operator;Large image database;Optimal codes;Relative performance;Spectral relaxations;State of the art;},
URL = {http://dx.doi.org/10.1007/978-3-642-33715-4_25},
} 


@article{20123715424855,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Numerical analysis of dynamic response and in-site monitoring of wind power tower considering the influence of the fluctuating wind load},
journal = {Tumu Gongcheng Xuebao/China Civil Engineering Journal},
author = {Huang, Shuai and Song, Bo and He, Wenshan and Wei, Wei},
volume = {45},
number = {SUPPL.1},
year = {2012},
pages = {102 - 106},
issn = {1000131X},
abstract = {Based on the finite element model, rotor-cabin-tower, of wind power tower, dynamic response under the storm load is analyzed, The stress, displacement and internal force of the wind power tower are calculated based on the calculation method of regulation and time-history. Change rules of mechanical characteristics of wind power tower influenced by the worst wind direction, and field monitoring under the common wind load are analyzed. The results show that the stress, displacement and internal force of dynamic analysis are greater than that calculated by regulation. The maximum stress value is smaller than the minimum permissible stress, however the displacement at the top of wind power tower is greater than allowable deformation in code. The maximum shear and bending moment influenced by the worst wind direction of the storm increased 91% and 106%. Simulation results have a good agreement with the monitoring results, and the monitoring results are greater than the simulation results.<br/>},
key = {Finite element method},
keywords = {Aerodynamic loads;Dynamic response;Storms;Towers;Wind power;Wind stress;},
note = {Field monitoring;Fluctuating wind;Mechanical characteristics;Monitoring results;Power towers;Site monitoring;Storm loads;Wind directions;},
} 


@inproceedings{20161502230986,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Individual detection-tracking-recognition using depth activity images},
journal = {2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence, URAI 2015},
author = {Jalal, Ahmad and Kamal, Shaharyar and Kim, Daijin},
year = {2015},
pages = {450 - 455},
address = {Goyang City, Korea, Republic of},
abstract = {In this paper, a depth camera-based novel approach for human activity recognition is presented using robust depth silhouettes context features and advanced Hidden Markov Models (HMMs). During HAR framework, at first, depth maps are processed to identify human silhouettes from noisy background by considering frame differentiation constraints of human body motion and compute depth silhouette area for each activity to track human movements in a scene. From the depth silhouettes context features, temporal frames information are computed for intensity differentiation measurements, depth history features are used to store gradient orientation change in overall activity sequence and motion difference features are extracted for regional motion identification. Then, these features are processed by Principal component analysis for dimension reduction and k-mean clustering for code generation to make better activity representation. Finally, we proposed a new way to model, train and recognize different activities using advanced HMM. Experimental results show superior recognition rate, resulting up to the mean recognition of 57.69% over the state of the art methods using IM-DailyDepthActivity dataset. In addition, MSRAction3D dataset also showed some promising results.<br/> &copy; 2015 IEEE.},
key = {Hidden Markov models},
keywords = {Ambient intelligence;Artificial intelligence;Cameras;Clustering algorithms;Intelligent robots;Pattern recognition;Principal component analysis;},
note = {Activity representation;advanced HMM;Depth camera;Gradient orientations;Hidden markov models (HMMs);Human activity recognition;Spatio temporal features;State-of-the-art methods;},
URL = {http://dx.doi.org/10.1109/URAI.2015.7358903},
} 


@inproceedings{20161702306580,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Synchronization algorithm for RFID-reader and its implementation},
journal = {Proceedings - 2015 Advances in Wireless and Optical Communications, RTUWO 2015},
author = {Martens, Olev and Liimets, Aivar and Kuusik, Alar},
year = {2015},
pages = {82 - 85},
address = {Riga, Latvia},
abstract = {An algorithm for synchronization of a decoder of a RFID receiver has been proposed, implemented and evaluated, by experimental setup. The solution can be used for UHF RFID (e.g. ISO18000-6 and EPC gen 2) readers using FM0, bi-phase or differential Manchester decoding. Goal of the proposed solution is the improved decoding of the received data in the presence of noise (so by using less power or working at longer distance) by innovative synchronization techniques. Synchronization problem (and challenge) is caused by non-precise timing of the binary signals sent by simple passive RFID-tags. Hardware prototype has been developed for testing and evaluating of the RFID-reader synchronization algorithm for decoding of the binary information. The developed algorithm has been implemented as C/C++ code for DSP and can be evaluated, benchmarked and developed further, with real-world experimental data log files acquired with the developed hardware setup (also on PC). The proposed algorithm is based on the finding of the best correlation of the expected binary preamble (header) waveform of the communication packet with corresponding part of the received noisy signal, as used in other known solutions, with variation of the possibly expected timing parameters, e.g by using matched correlation (FIR) filter-banks. The idea of the proposed solution is to use additionally to maximum correlation of the received signal with the reference preamble waveform also the quadrature version of the reference (derived from the ideal preamble) signal, as the quadrature correlation has at the best match clear zero value and has change around this zero point. So using this additional Q correlation channel gives the opportunity to find the exact timing base (bit duration) more precisely. The proposed algorithm and the results of the evaluation of the solution with real-world data are described. Also, potential future developments of the solution are discussed.<br/> &copy; 2015 IEEE.},
key = {Synchronization},
keywords = {C++ (programming language);Decoding;Hardware;Matched filters;Optical communication;},
note = {Binary information;Communication packets;Hardware prototype;Maximum correlations;Passive RFID tags;Synchronization algorithm;Synchronization problem;Synchronization technique;},
URL = {http://dx.doi.org/10.1109/RTUWO.2015.7365725},
} 


@article{20154001339758,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modeling the hot-dense plasma of the solar interior in and out of thermal equilibrium},
journal = {ProQuest Dissertations and Theses Global},
author = {Lin, Hsiao-Hsuan},
year = {2012},
abstract = {The developments in helioseismology ensure a wealth of studies in solar physics. In particular, with the high precision of the observations of helioseismology, a high-quality solar model is mandated, since even the tiny deviations between a model and the real Sun can be detected. One crucial ingredient of any solar model is the thermodynamics of hot-dense plasmas, in particular the equation of state. This has motivated efforts to develop sophisticated theoretical equations of state (EOS). It is important to realize that for the conditions of solar-interior plasmas, there are no terrestrial laboratory experiments; the only observational constraints come from helioseismology. Among the most successful EOS is so called OPAL EOS, which is part of the Opacity Project at Livermore. It is based on an activity expansion of the quantum plasma, and realized in the so-called "physical picture". One of its main competitor is the so called MHD EOS, which is part of the international Opacity Project (OP), a non-classified multi-country consortium. The approach of MHD is via the so-called "chemical picture". Since OPAL is the most accurate equation of state so far, there has been a call for a public-domain version of it. However, the OPAL code remains proprietary, and its "emulation" makes sense. An additional reason for such a project is that the results form OPAL can only be accessed via tables generated by the OPAL team. Their users do not have the flexibility to change the chemical composition from their end. The earlier MHD-based OPAL emulator worked well with its modifications of the MHD equation of state, which is the Planck-Larkin partition function and its corresponding scattering terms. With this modification, MHD can serve as a OPAL emulator with all the flexibility and accessibility. However, to build a really user-friendly OPAL emulator one should consider CEFF-based OPAL emulator. CEFF itself is already widely used practical EOS which can be easily implemented within any solar model code. In the present work we have carried the technique of the MHD-based OPAL emulator to the CEFF-based OPAL emulator and successfully accomplished this goal. At the same time, we went beyond the earlier work by adding more terms. In particular, the previous MHD-based OPAL emulator was restricted to the approximation of a hydrogen plasma; our work is extended to the more realistic H-He mixture. In a separate part of the present work, we have examined a non-equilibrium effect in the solar interior. The effect is located in the zone of the sharp transition between the differential rotation in the convection zone and the solid-sphere rotation in the radiation zone beneath it. This transition was discovered by helioseismology in the 1980s, and the transition zone is called the solar tachocline. The tachocline is subject to strong shear and in many theories of the solar dynamo it plays important role. Being inspired by the well-known Soret effect, which states the mass diffusion drive by a temperature gradient, we have examined if there could also be mass diffusion by a shear flow. If such an effect were to exist, it would have potential applications to the solar tachocline and dynamo. We have run a so-called reverse non-equilibrium molecular dynamics (RNEMD) simulation. As a test, we first confirmed the Soret effect by the simulation, then we tested for a shear- driven analogous effect. As a result, we did not see the shear-driven Soret effect in our simulation. We do observe the normal Soret effect due to the temperature gradient caused by the numerical scheme we used. ProQuest Subject Headings: Astrophysics, Plasma physics.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.},
key = {Magnetohydrodynamics},
keywords = {Astrophysics;Diffusion in solids;Equations of state;Interactive devices;Molecular dynamics;Opacity;Shear flow;Solar equipment;Thermal gradients;},
note = {Chemical compositions;Differential rotation;Non equilibrium molecular dynamic (NEMD);Non-equilibrium effects;Partition functions;Terrestrial laboratories;Theoretical equation;Thermal equilibriums;},
} 


@article{20154501504658,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The influence on products yield and feedstock conversion of feedstock injection position along the industrial riser},
journal = {Chemical Engineering Transactions},
author = {Alvarez-Castro, Helver Crispiniano and Armellini, Victor and Mori, Milton and Martignoni, Waldir Pedro and Ocone, Raffaella},
volume = {43},
year = {2015},
pages = {1597 - 1602},
issn = {22839216},
abstract = {The fluid catalytic cracking (FCC) process is at the heart of a modern refinery oriented toward maximum gasoline and diesel production. Within the entire refinery process, this process offers the greatest potential for increasing profitability; even a small improvement in the gasoline yield it implies a substantial economical profit when dealing with a production of millions of barrels of gasoline a day. There are several articles published in the last two decades focusing the attention on 2-D and 3-D computational fluid dynamic models of the industrial riser of a circulating fluidized bed. Nevertheless, there are few research works published in the literature that include studies on how the localization of feedstock along the riser affects the yield products. A 3D hydrodynamic model coupled with a 12 lump kinetic model is presented in this work. Four different injection points in an FCC industrial riser were considered in order to evaluate the hydrodynamic behavior and their effect in the gas oil conversion and products yield. The equations were solved numerically by finite volume method using the Eulerian-Eulerian approach and a commercial CFD code, CFX version 14.0. Appropriate functions were implemented in the model via user defined functions considering the heterogeneous kinetics and catalyst deactivation. The results from the model were validated against the experimental industrial results and it was found that the conversion of gas oil and the production yield significantly change with the feedstock localization.<br/> Copyright &copy; 2015, AIDIC Servizi S.r.l.},
key = {Fluid catalytic cracking},
keywords = {Catalyst deactivation;Computational fluid dynamics;Feedstocks;Finite volume method;Fluidized bed process;Fluidized beds;Gas oils;Gasoline;Hydrodynamics;Profitability;Refining;},
note = {Circulating fluidized bed;Eulerian-Eulerian approach;Fluid catalytic cracking(FCC);Heterogeneous kinetics;Hydrodynamic behavior;Hydrodynamic model;Lump kinetic models;User Defined Functions;},
URL = {http://dx.doi.org/10.3303/CET1543267},
} 


@article{20155101707672,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Parts-based geophysical inversion with application to water flooding interface detection and geological facies detection},
journal = {ProQuest Dissertations and Theses Global},
author = {Zhang, Junwei},
year = {2015},
abstract = {I built parts-based and manifold based mathematical learning model for the geophysical inverse problem and I applied this approach to two problems. One is related to the detection of the oil-water encroachment front during the water flooding of an oil reservoir. In this application, I propose a new 4D inversion approach based on the Gauss-Newton approach to invert time-lapse cross-well resistance data. The goal of this study is to image the position of the oil-water encroachment front in a heterogeneous clayey sand reservoir. This approach is based on explicitly connecting the change of resistivity to the petrophysical properties controlling the position of the front (porosity and permeability) and to the saturation of the water phase through a petrophysical resistivity model accounting for bulk and surface conductivity contributions and saturation. The distributions of the permeability and porosity are also inverted using the time-lapse resistivity data in order to better reconstruct the position of the oil water encroachment front. In our synthetic test case, we get a better position of the front with the by-products of porosity and permeability inferences near the flow trajectory and close to the wells. The numerical simulations show that the position of the front is recovered well but the distribution of the recovered porosity and permeability is only fair. A comparison with a commercial code based on a classical Gauss-Newton approach with no information provided by the two-phase flow model fails to recover the position of the front. The new approach could be also used for the time-lapse monitoring of various processes in both geothermal fields and oil and gas reservoirs using a combination of geophysical methods. A paper has been published in Geophysical Journal International on this topic and I am the first author of this paper. The second application is related to the detection of geological facies boundaries and their deforation to satisfy to geophysica data and prior distributions. We pose the geophysical inverse problem in terms of Gaussian random fields with mean functions controlled by petrophysical relationships and covariance functions controlled by a prior geological cross-section, including the definition of spatial boundaries for the geological facies. The petrophysical relationship problem is formulated as a regression problem upon each facies. The inversion is performed in a Bayesian framework. We demonstrate the usefulness of this strategy using a first synthetic case study, performing a joint inversion of gravity and galvanometric resistivity data with the stations all located at the ground surface. The joint inversion is used to recover the density and resistivity distributions of the subsurface. In a second step, we consider the possibility that the facies boundaries are deformable and their shapes are inverted as well. We use the level set approach to deform the facies boundaries preserving prior topological properties of the facies throughout the inversion. With the additional help of prior facies petrophysical relationships, topological characteristic of each facies, we make posterior inference about multiple geophysical tomograms based on their corresponding geophysical data misfits. The result of the inversion technique is encouraging when applied to a second synthetic case study, showing that we can recover the heterogeneities inside the facies, the mean values for the petrophysical properties, and, to some extent, the facies boundaries. A paper has been submitted to Geophysics on this topic and I am the first author of this paper. During this thesis, I also worked on the time lapse inversion problem of gravity data in collaboration with Marios Karaoulis and a paper was published in Geophysical Journal international on this topic. I also worked on the time-lapse inversion of cross-well geophysical data (seismic and resistivity) using both a structural approach named the cross-gradient approach and a petrophysical approach. A paper was published in Geophysics on this topic. ProQuest Subject Headings: Geophysics, Geology, Statistics.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.},
key = {Inverse problems},
keywords = {Floods;Gaussian distribution;Geology;Geophysics;Geothermal fields;Low permeability reservoirs;Oil well flooding;Petroleum reservoir evaluation;Phase interfaces;Porosity;Recovery;Reservoirs (water);Statistics;Topology;Two phase flow;},
note = {Gaussian random fields;Geophysical inverse problems;Permeability and porosities;Petrophysical properties;Petrophysical relationship;Resistivity distributions;Topological characteristics;Topological properties;},
} 


@inproceedings{20180904832703,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Seismic finite element analysis of an existing old concrete structure by using multifiber beams: Introduction of an adaptive pushover method},
journal = {COMPDYN 2017 - Proceedings of the 6th International Conference on Computational Methods in Structural Dynamics and Earthquake Engineering},
author = {Omar, A. and Grange, S. and Dufour, F.},
volume = {2},
year = {2017},
pages = {3491 - 3505},
address = {Rhodes Island, Greece},
abstract = {The study of the seismic vulnerability of existing structures is an important issue. Many researches have been developed in order to investigate the structural behavior of these structures, and extract the basic informations needed to establish retrofitting guidelines in order to reduce the seismic risk to acceptable levels. The most accurate analysis procedure for the structures subjected to strong ground motions is the time-history analysis. This method is time-consuming though for application in all practical purposes. The necessity for faster methods that would ensure a reliable structural assessment or design of structures subjected to seismic loading led to the pushover analysis. Pushover analysis is a non-linear static analysis based on the assumption that structures oscillate predominantly in the first mode or in the lower modes of vibration during a seismic event. The present work deals with seismic vulnerability assessment of an old existing reinforced concrete structure - Perret tower - located in Grenoble, France. After a brief description of the structure in exam, a preliminary computation of the mass of the building and the definition of every existing section are performed. A simplified 3D numerical model is carried out using a finite element code based on multifiber beams approach. Firstly, a non-linear temporal dynamic analysis is performed, then a conventional and adaptive pushover analysis is carried out. The results obtained of the studied cases are then compared: it is observed that the conventional pushover analysis should be adjusted in order to take into account the change of dynamic characteristics due to the formation of plastic mechanisms. Finally, the tower critical levels in term of damage are highlighted.<br/> &copy; 2017 National Technical University of Athens. All rights reserved.},
key = {Finite element method},
keywords = {Computational methods;Concrete buildings;Concrete construction;Dynamic analysis;Engineering geology;Modal analysis;Reinforced concrete;Seismic design;Seismology;Static analysis;Structural analysis;Structural dynamics;Vibration analysis;},
note = {3-D numerical modeling;Ambient vibrations;Dynamic characteristics;Existing reinforced concrete;Non-linear dynamics;Non-linear static analysis;Push-over analysis;Structural assessments;},
URL = {http://dx.doi.org/10.7712/120117.5660.18747},
} 


@inproceedings{20181104892529,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Seismic finite element analysis of an existing old concrete structure by using multifiber beams: Introduction of an adaptive pushover method},
journal = {COMPDYN 2017 - Proceedings of the 6th International Conference on Computational Methods in Structural Dynamics and Earthquake Engineering},
author = {Omar, A. and Grange, S. and Dufour, F.},
volume = {2},
year = {2017},
pages = {3491 - 3505},
address = {Rhodes Island, Greece},
abstract = {The study of the seismic vulnerability of existing structures is an important issue. Many researches have been developed in order to investigate the structural behavior of these structures, and extract the basic informations needed to establish retrofitting guidelines in order to reduce the seismic risk to acceptable levels. The most accurate analysis procedure for the structures subjected to strong ground motions is the time-history analysis. This method is time-consuming though for application in all practical purposes. The necessity for faster methods that would ensure a reliable structural assessment or design of structures subjected to seismic loading led to the pushover analysis. Pushover analysis is a non-linear static analysis based on the assumption that structures oscillate predominantly in the first mode or in the lower modes of vibration during a seismic event. The present work deals with seismic vulnerability assessment of an old existing reinforced concrete structure - Perret tower - located in Grenoble, France. After a brief description of the structure in exam, a preliminary computation of the mass of the building and the definition of every existing section are performed. A simplified 3D numerical model is carried out using a finite element code based on multifiber beams approach. Firstly, a non-linear temporal dynamic analysis is performed, then a conventional and adaptive pushover analysis is carried out. The results obtained of the studied cases are then compared: It is observed that the conventional pushover analysis should be adjusted in order to take into account the change of dynamic characteristics due to the formation of plastic mechanisms. Finally, the tower critical levels in term of damage are highlighted.<br/> &copy; 2017 ECCOMAS Proceedia. All Rights Reserved.},
key = {Finite element method},
keywords = {Computational methods;Concrete buildings;Concrete construction;Dynamic analysis;Engineering geology;Modal analysis;Reinforced concrete;Seismic design;Seismology;Static analysis;Structural analysis;Structural dynamics;Vibration analysis;},
note = {3-D numerical modeling;Ambient vibrations;Dynamic characteristics;Existing reinforced concrete;Non-linear dynamics;Non-linear static analysis;Push-over analysis;Structural assessments;},
} 


@article{20134716990607,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The simulation-game controversy: What is a ludic simulation?},
journal = {International Journal of Gaming and Computer-Mediated Simulations},
author = {Parker, J.R. and Becker, Katrin},
volume = {5},
number = {1},
year = {2013},
pages = {1 - 12},
issn = {19423888},
abstract = {Games use the same base technology and design strategy as do simulations, but add a few items to the mixture. Understanding this gives 'new' (read borrowed) tools for game creation and testing. The idea that simulations are implementations of a model, for instance, leads to a focus on the model rather than the code when designing a game. Similarly, the verification/validation pair used in simulations can be extended by adding playtesting for games, thus giving an educational game (for example) viable, demonstrable educational characteristics as well as playable (and thus engaging and motivating) characteristics. Productive work on improving games for specific purposes (serious games) can be advanced if the authors can agree on a common terminology and concept set (Shaw &amp; Gaines, 1989), and if games can be seen as a valuable extension of a simulation that has specific characteristics that make them useful in specific circumstances. The idea of 'fun' is often thought of as the enemy of 'learning' in educational literature, and this needs to change if progress on serious and educational games is to be made. This paper will describe the hierarchy of computer simulation objects within which ludic simulations can be understood. Copyright &copy; 2013, IGI Global.<br/>},
key = {Serious games},
keywords = {Computer applications;Computer simulation;},
note = {Digital Objects;Educational game;Games;Ludic Simulation;Simulation games;Technology and designs;},
URL = {http://dx.doi.org/10.4018/jgcms.2013010101},
} 


@inproceedings{20141817645760,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Heuristic approach in structural strength evaluation and retrofitting of existing buildings in seismic areas},
journal = {ECCOMAS Thematic Conference - COMPDYN 2013: 4th International Conference on Computational Methods in Structural Dynamics and Earthquake Engineering, Proceedings - An IACM Special Interest Conference},
author = {Gluck, N. and Farhat, R. and Eid, R. and Tzadka, U.},
year = {2013},
pages = {2454 - 2465},
address = {Kos Island, Greece},
abstract = {The migration of the population, to developed areas, lead to birth of overcrowded areas with claim for residences, financial, administrative, cultural and industrial facilities. The density augmentation of the population required sophisticated solutions from the authorities to ensure adequate environmental state suited for the 21<sup>th</sup>century. Due to the huge demand of the facilities significant number of buildings must be strengthened and modified to be suitable to the new demands. This change claims increase of surfaces and storey addition. This trend leads to the hazard of the new urban agglomerations versus natural causes as flood, fire and earthquake. The present paper is devoted to structures which were built before the implementation of any code or obsolete ones for the seismic design. Strengthening and retrofitting of existent buildings is a major interest of the actual engineering effort to provide the suitable structures to the new demands and to be stable under seismic loads. Each seismic event is accompanied by damages that better design and execution would have prevented. In general, severe earthquakes are followed by intense research of the causes which damaged structures. In some cases the code is changed due to the short comes which were observed. This means that each experiment including earthquakes is followed by a learning process that provides new solutions, use of new materials and new design strategies. This process must be a heuristic process according the abilities of evaluation and implementation.<br/>},
key = {Seismic design},
keywords = {Codes (symbols);Computational methods;Earthquakes;Engineering geology;Heuristic methods;Reinforced concrete;Retrofitting;Structural analysis;Structural dynamics;Structures (built objects);},
note = {Codes;Density augmentation;Environmental state;Heuristic evaluation;Industrial facilities;Nonlinear FEM;Structural strength;Urban agglomerations;},
} 


@inproceedings{20153101097998,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The CIRA effort toward a fast aerothermal design environment tool},
journal = {Proceedings of the International Astronautical Congress, IAC},
author = {Cinquegrana, Davide and Pezzella, Giuseppe and Catalano, Pietro and De Stefano Fumo, Mario},
volume = {10},
year = {2014},
pages = {7036 - 7046},
issn = {00741795},
address = {Toronto, ON, Canada},
abstract = {This paper deals with the CIRA effort undertaken to develop an in-house tool aiming at the assessment of the aerothermodynamic loading environment expected by hypersonic vehicles during re-entry. This tool couples proper orthogonal decomposition techniques and a Kriging interpolator to provide vehicle aerothermal loading conditions, starting from a limited number of design results available for the vehicle. Indeed, the code inputs are data coming from CFD design results previously obtained at different flight conditions of the vehicle re-entry corridor. Thus, the tool allows computing new flowfield distributions around the vehicle and on its surface simply asking for the desired Mach, Reynolds numbers, vehicle attitude and flap settings, thus avoiding time consuming and cost expensive CFD analyses. Furthermore, the main advantage of the developed tool is its flexibility: Efficient, fast, adaptable at each trajectory and vehicle's shape change, and hopefully reliable, due to the physics-based interpolation methods. Examples of tool application are provided in the paper considering the USV3 vehicle design. For instance, time history of USV3 surface map for pressure and skin friction coefficient are presented, provided that such design results are useful to identify aerothermal loads. A cross-validation of the results, are also shown.<br/> Copyright &copy; 2014 by the International Astronautical Federation.},
key = {Hypersonic vehicles},
keywords = {Data handling;Friction;Hypersonic aerodynamics;Interpolation;Principal component analysis;Reynolds number;Thermodynamics;},
note = {Design environment;Flight conditions;Interpolation method;Loading condition;Loading environment;Proper orthogonal decomposition techniques;Skin friction coefficient;Tool applications;},
} 


@article{20154301433340,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Maps as geomedial action spaces: considering the shift from logocentric to egocentric engagements},
journal = {GeoJournal},
author = {Abend, Pablo and Harvey, Francis},
volume = {82},
number = {1},
year = {2017},
pages = {171 - 183},
issn = {03432521},
abstract = {This paper considers some significant questions in geography and cognate fields about the roles of maps in the information age. Most maps are now digital products, offering immersive environments for user involvement. The increasingly networked digital distribution of geographic information in consumer-orientated cartographic representations leads to substantial changes how people individually and collaboratively experience and produce space and place. This article focuses on the ongoing metamorphosis arising through geobrowsing, the media-based flexible production of geographic knowledge through interactive maps. Drawing on work in media studies influenced by the so-called spatial turn&mdash;the rediscovering of geography-related questions in the social sciences and humanities, after modernism&rsquo;s claimed prioritization of time and history (Soja in Postmodern Geographies. The reassertion of space in critical social theory, London, 1989; Jameson in Postmodernism, or, the cultural logic of late capitalism, Duke University Press, Durham, 1991)&mdash;this paper develops a theoretical framework built on the dynamic networked geomedial action spaces concept to understand the changing roles of information age maps as imagined materialist spaces for the experience and production of space&mdash;ultimately a medial turn. Following this concept, maps change from offering static and non-interactive frames of geographic reference for the production of space and place and as geomedia support a veritable infinity of interactive and map-based activities. Geobrowsing facilitates some new modes of geographic interactions that move from logocentric engagements with static maps to egocentric dynamic interactions with code-based elements of geomedial action spaces. Google Earth and similar geomedia facilitate maps that become intrinsic to a growing number of social action spaces and alter the experience and production of space and place.<br/> &copy; 2015, Springer Science+Business Media Dordrecht.},
key = {Earth (planet)},
keywords = {Earth sciences;Planning;},
note = {Geobrowsing;Geomedia;Media studies;Neogeography;Online maps;Spatial turns;User interaction;},
URL = {http://dx.doi.org/10.1007/s10708-015-9673-z},
} 


@inproceedings{20164803066048,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Findings from 16 years of auditing pipeline integrity management systems},
journal = {Proceedings of the Biennial International Pipeline Conference, IPC},
author = {Hodgson, Oliver J. and Keen, Dennis W. J. and Toft, Malcolm},
volume = {2},
year = {2016},
pages = {Pipeline Division - },
address = {Calgary, AB, Canada},
abstract = {A Pipeline Integrity Management System (PIMS) is a comprehensive, systematic, and integrated set of arrangements implemented by an operator to assess, mitigate, and manage pipeline risk. Over the past 16 years, Penspen have performed over 30 PIMS audits of pipeline operators internationally. This paper presents the collated findings from these audits, and examines the common areas in which operators have fallen short of best practice. The paper concludes with a series of recommendations based on the findings, which can be adopted by operators to improve their PIMS arrangements and practices. Penspen's standardized 17-element PIMS Model takes a holistic view of pipeline integrity. The audits, which are based on the Model, assess the adequacy and effectiveness of operators' management systems and arrangements in keeping risks to people, the environment, and to the business to acceptable levels, given the anticipated pipeline operating conditions and taking into account the pipeline's history and current status. Starting at the 'top level' of a PIMS, the audits consider the adequacy of operators' pipeline policies, objectives, and performance metrics, and how these are subject to monitoring, review, and audit. The audits look at the organization responsible for managing the integrity of pipelines, and examine how all those with a role to play in the wider PIMS work together to this end. The numerous activities that take place during a pipeline's lifecycle are investigated, to assess how the risk assessment results are used to determine the control and mitigation measures to be implemented during the pipeline's design, construction, handover, commissioning, operation, inspection and maintenance, and how the operator ensures the effectiveness of these measures. The audits also study those 'supporting' processes and systems which play an important part in pipeline integrity management, including procurement, emergency response and recovery, incident investigation, change control, document and data management, and legal and code compliance. The collated results from the 30+ audits reveal that while operators typically have good control systems in place for the project stages of the pipeline lifecycle, controls for the operational stages have been found to be less robust. In terms of management and organization, operators can fail to recognize how many different individuals and teams have a role to play in the management of pipeline integrity. Furthermore, while operators often have good corporate systems in place for change control, emergency response, and risk assessment, such systems may not take into account pipeline-specific risks or requirements. Operators can tend to focus on pipeline safety and/or environmental-related risks, when through holistic assessment it can be shown that risks associated with production interruptions will tend to drive actions in practice.<br/> &copy; Copyright 2016 by ASME.},
key = {Information management},
keywords = {Compliance control;Digital storage;Emergency services;Environmental management;Human resource management;Life cycle;Patient monitoring;Pipelines;Project management;Risk assessment;Structural integrity;},
note = {Emergency response;Incident investigation;Inspection and maintenance;Mitigation measures;Operating condition;Performance metrics;Pipeline integrity management;Pipeline integrity management systems;},
URL = {http://dx.doi.org/10.1115/IPC2016-64397},
} 


@article{20164803052157,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Radon induced hyperplasia: Effective adaptation reducing the local doses in the bronchial epithelium},
journal = {Journal of Radiological Protection},
author = {Madas, Balazs G.},
volume = {36},
number = {3},
year = {2016},
pages = {653 - 666},
issn = {09524746},
abstract = {There is experimental and histological evidence that chronic irritation and cell death may cause hyperplasia in the exposed tissue. As the heterogeneous deposition of inhaled radon progeny results in high local doses at the peak of the bronchial bifurcations, it was proposed earlier that hyperplasia occurs in these deposition hot spots upon chronic radon exposure. The objective of the present study is to quantify how the induction of basal cell hyperplasia modulates the microdosimetric consequences of a given radon exposure. For this purpose, computational epithelium models were constructed with spherical cell nuclei of six different cell types based on histological data. Basal cell hyperplasia was modelled by epithelium models with additional basal cells and increased epithelium thickness. Microdosimetry for alpha-particles was performed by an own-developed Monte-Carlo code. Results show that the average tissue dose, and the average hit number and dose of basal cells decrease by the increase of the measure of hyperplasia. Hit and dose distribution reveal that the induction of hyperplasia may result in a basal cell pool which is shielded from alpha-radiation. It highlights that the exposure history affects the microdosimetric consequences of a present exposure, while the biological and health effects may also depend on previous exposures. The induction of hyperplasia can be considered as a radioadaptive response at the tissue level. Such an adaptation of the tissue challenges the validity of the application of the dose and dose rate effectiveness factor from a mechanistic point of view. As the location of radiosensitive target cells may change due to previous exposures, dosimetry models considering the tissue geometry characteristic of normal conditions may be inappropriate for dose estimation in case of protracted exposures. As internal exposures are frequently chronic, such changes in tissue geometry may be highly relevant for other incorporated radionuclides.<br/> &copy; 2016 IOP Publishing Ltd.},
key = {Tissue},
keywords = {Alpha particles;Cell death;Deposition;Polymethyl methacrylates;Radon;},
note = {Adaptive response;Dose distributions;Effectiveness factor;Histological data;hyperplasia;Internal exposure;Micro-dosimetry;Radon exposure;},
URL = {http://dx.doi.org/10.1088/0952-4746/36/3/653},
} 


@inproceedings{20160701950064,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Historic desmond building retrofit-a case study of the seismic retrofit of a non-ductile concrete building in the los angeles area},
journal = {Improving the Seismic Performance of Existing Buildings and other Structures 2015 - Proceedings of the 2nd ATC and SEI Conference on Improving the Seismic Performance of Existing Buildings and Other Structures},
author = {Jiang, Z. and Sarkisian, M. and Mathias, N. and Garai, R. and Lyrenmann, J.},
year = {2015},
pages = {173 - 184},
address = {San Francisco, CA, United states},
abstract = {According to a Los Angeles Times report in Oct. 2013 (Los Angeles Times, 2013), by the most conservative estimate, as many as 50 old concrete buildings in the city of Los Angeles would be destroyed in a major earthquake, exposing thousands to injury or death. To address this concern, the City of Los Angeles recently proposed instituting what are arguably the most ambitious seismic safety regulations in California's history; regulations that would require building owners to retrofit thousands of building deemed to be at risk of collapse during a major earthquake. Non-ductile concrete buildings built before 1980 were singled out as one of two particularly vulnerable structural types requiring attention. This paper presents a case study of seismically retrofitting a historic concrete building located in Los Angeles, California. Designed in 1916, the building originally housed the Willy's Overland Company car dealership and assembly plant, which later became a Desmond's department store warehouse. After the retrofit, the five-story warehouse will be transformed into high quality, creative office space with a ground floor cafe and the addition of a 7,000-square-foot sixth floor that will bring the total size of the building to 82,000 square feet. The retrofit project is currently under construction with full occupancy expected in the summer of 2015. When it is complete, it will be the first of many potential renovations of historic properties located in the South Park district for creative office use. This case study presents an innovative but rigorous approach taken to enable the addition of a story to the existing building by not exceeding the gravity load and lateral force change triggers in Chapter 34 of the 2011 Los Angeles City Building Code (2011 LABC) necessitating retrofit. Although demonstrated to not be required by code, a seismic retrofit was nevertheless instituted; this while continuing to ensure that the changes in force effects in retained existing structural members did not exceed code retrofit triggers. All structural modifications made since the building was originally constructed using very low strength concrete were considered in the evaluations and retrofit design.<br/> &copy; 2015 ASCE and ATC.},
key = {Earthquakes},
keywords = {Codes (symbols);Concrete buildings;Concretes;Ductility;Floors;Office buildings;Retrofitting;Seismic waves;Warehouses;},
note = {Building retrofits;Historic concrete;Los Angeles Times;Low strength concrete;Retrofit project;Rigorous approach;Seismic retrofits;Structural modifications;},
} 


@inproceedings{20132816483308,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {FPGA implementation issues of a two-pole adaptive notch filter for GPS/Galileo receivers},
journal = {25th International Technical Meeting of the Satellite Division of the Institute of Navigation 2012, ION GNSS 2012},
author = {Gamba, Micaela Troglia and Falletti, Emanuela and Rovelli, Davide and Tuozzi, Alberto},
volume = {5},
year = {2012},
pages = {3549 - 3557},
address = {Nashville, TN, United states},
abstract = {In the field of satellite navigation technologies, and in particular for Safety-of-Life (SoL) applications, great attention is given to rejection of interference. In particular, continuous wave (CW) signals represent one of the most common types of interference. A properly designed adaptive notch filter should be employed in order to remove it. Such an algorithm has been widely exploited in literature, with many variations. Most of these studies are however mainly focused on the floating point precision model, and only a few of them really consider the potential issues of a real implementation. The objective of this paper is to discuss the issues arising from fixed-point modeling and FPGA implementation of an adaptive notch filter. Previous works showed that in a complex base-band front-end, one CW interfering signal can be eliminated by means of an IIR one-pole adaptive notch filter. For the case of more than one CW, two or more filtering cells can be used in cascade. Here, a two-pole adaptive notch filter is considered. A distortion analysis of the filter output is conducted in the paper by means of Cross-Ambiguity-Function (CAF) and Delay-Locked-Loop (DLL) S-curve evaluation, showing a great distortion of the ranging code after the filtering operation. Thus, to reduce such a distortion, the proposed solution consists in constraining the zeros of the filter at staying on the unit circle in a new and simpler way with respect to existing methods. A run-time change of the algorithm parameters is also performed, improving convergence speed. The VHDL architecture is then discussed. Pipelining and delaying the parameter updating are applied to increase clock frequency. The post Place&amp;Route (P&amp;R) results of the improved version of the two-pole adaptive notch filter on a Virtex 5 show a clock frequency of 47 MHz with an occupied area of 96.4 KGE. Tests with a fully software receiver support the validity of the proposed solution.<br/>},
key = {Notch filters},
keywords = {Adaptive filtering;Adaptive filters;Bandpass filters;Clocks;Delay lock loops;Digital arithmetic;Field programmable gate arrays (FPGA);Global positioning system;IIR filters;Navigation;Poles;Satellites;Software testing;Sols;},
note = {Adaptive notch filters;Algorithm parameters;Continuous-wave signals;Cross Ambiguity Function (CAF);Distortion analysis;Filtering operations;FPGA implementations;Satellite navigation;},
} 


@article{20163402734779,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Achieving the InsideOutside Coupling During Network Simulation of Isothermal Drying of a Porous Medium in a Turbulent Flow},
journal = {Transport in Porous Media},
author = {Beyhaghi, Saman and Xu, Zhenyu and Pillai, Krishna M.},
volume = {114},
number = {3},
year = {2016},
pages = {823 - 842},
issn = {01693913},
abstract = {The problem of isothermal (slow) drying of a porous medium in the form of a square bluff body resting on a flat surface while being exposed to turbulent air flow is investigated in this paper. The porous medium is represented by a pore-network model consisting of a network of volume-less nodes connected to each other via narrow throats. As the air flows past the water-saturated network, the water evaporates in the network and the evaporated water vapors diffuse through the air inside the network to advect away in the outside domain. In the history of such simulations coupling the &lsquo;outside&ndash;inside&rsquo; processes during drying, this paper investigates, for the first time, the effect of turbulent flow on drying of a porous medium after coupling the outside-the-network flow and transport with the inside-the-network drying and liquid redistribution. Also, a commercial software package is used for the first time to obtain the outside flow. The water redistribution inside the network is predicted by the invasion-percolation algorithm after assuming the dominance of capillary forces over viscous and gravity forces. The outside velocity field is first obtained using the k- &Epsilon; model in ANSYS Fluent package. Then, the velocity field is used in a finite-volume-based code in order to solve for mass transfer outside the network using the regular convective-diffusive species transport equation. The drying mechanism and vapor transport inside the pore network is coupled with the outside mass-transfer simulation by considering a flux-balancing condition at the top surface of the network. After achieving grid independence and demonstrating the suitability of the no-slip boundary condition for the network top, the effects of the outside-flow Reynolds number and different turbulent flow models on several global drying parameters such as evaporation or drying rate, cumulative drying time, and top-surface saturation were studied. The resistance to vapor transport within the network was observed to dominate the enhanced vapor transport outside the network through the use of turbulent flow. Though the effect of variations in the network throat-size distribution upon saturation distribution within the network was found to be significant, its effect on the drying rate evolution was minimal. The concentration boundary layer thickness, which is employed to set the mass-transfer boundary condition in pore-network simulations and has been hitherto taken as constant in the literature, was found to change not only with position on the network top, but was also found to decline with time due to the drying of the network top.<br/> &copy; 2016, Springer Science+Business Media Dordrecht.},
key = {Air},
keywords = {Atmospheric thermodynamics;Boundary conditions;Boundary layer flow;Boundary layers;Drying;Gravitation;Isotherms;Mass transfer;Percolation (fluids);Percolation (solid state);Porous materials;Reynolds number;Solvents;Turbulent flow;Velocity;},
note = {Bluff body;Concentration boundary layer;Invasion percolation;Mass transfer simulation;No-slip boundary conditions;Pore-network modeling;Saturation distribution;Water redistribution;},
URL = {http://dx.doi.org/10.1007/s11242-016-0746-3},
} 


@inproceedings{20174104247873,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Teaching teachers to think like engineers using NetLogo},
journal = {ASEE Annual Conference and Exposition, Conference Proceedings},
author = {Burrows, Andrea Carneal and Borowczak, Mike},
volume = {2017-June},
year = {2017},
issn = {21535965},
address = {Columbus, OH, United states},
abstract = {This paper provides a view of 22 K12 teachers' expectations versus the actuality of immersion into an engineering education computer science (CS) project during a Math/Science Partnership (MSP) grant called RAMPED, which was a 16-day, yearlong MSP grant. The CS session using NetLogo was selected for focused examination. NetLogo is a multi-agent simulator that uses the educational Logo programming language and was designed for classroom modeling experience. The research question for the study was, "How do K12 teachers view their skill set of using computer science in their classrooms before, during, and after professional development (PD)?" RAMPED participants spent a total of three days immersed in using NetLogo as a vehicle for learning fundamental computer science principles and engineering applications for K12 classrooms. The authors used a social constructivism approach and examined K12 teacher NetLogo usage in and out of the classroom. The authors also collected the data via K12 teacher surveys and informal interviews. Findings show that the teachers self-reported high expectations of their skillset as well as easy assimilation of NetLogo (but not CS) into their classroom teaching. On a scale from 0 to 5, where 0 is not at all skillful and 5 is extremely skillful, survey pretest results show over 50% at a 3, 4, or 5. Posttest survey results show over 90% at a 3, 4, or 5. After the summer session, NetLogo was useful to 95% of K12 teachers. After an academic year NetLogo follow-up session over 75% of the K12 teachers were satisfied with instruction and support. Over 85% of teachers believed that the workshop "stretched teacher thinking into their classrooms." Teachers' qualitative comments are included for triangulation. Conclusions include that intense K12 teacher exposure to engineering CS topics (e.g. 24 hours total of a larger PD) is not enough to truly enact meaningful classroom changes (although the teachers did create new activities). Additional support for meaningful classroom change and K12 teacher confidence is necessary. In general, K12 teachers need (and asked for) support in the form of ready to use lessons and documents (e.g. additional activities) along with leader presence to support them in trying their self-created plans situated within the NGSS standards. The actuality of working with NetLogo (and changing functions and code) to present STEM concepts/topics was both invigorating (it was new for the K12 teachers) and frustrating (it was often hard for the K12 teachers to see connections to content) as teachers moved through expectations and actuality. Implications include planning for structured K12 teacher academic year support in implementing CS topics for sustainability in classrooms.<br/> &copy; American Society for Engineering Education, 2017.},
key = {Teaching},
keywords = {Computer science;Education computing;Engineering education;Modeling languages;Multi agent systems;STEM (science, technology, engineering and mathematics);Surveys;},
note = {Classroom teaching;Computer Science Education;Engineering applications;K-12 teachers;Pre-service teacher education;Professional development;Research questions;Social constructivism;},
} 


@inproceedings{20170903403900,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Title: VI in PA - Managing a threat to the certainty of the act 2 closure process},
journal = {A and WMA Specialty Conference on Vapor Intrusion, Remediation, and Site Closure 2014 - The Drive to Site Closure: Critical Sustainability Concerns and Key Technical Issues},
author = {Roe, Christopher M. and Davis, Karen H. and Cutler, Adam H.},
year = {2014},
pages = {24 - 35},
address = {Philadelphia, PA, United states},
abstract = {Pennsylvania's Land Recycling Program under Act 21 encourages the voluntary cleanup and reuse of contaminated commercial and industrial sites. Three cornerstones of this Program are: Uniform cleanup standards Liability relief Standardized reviews and time limits Vapor intrusion creates challenges to elements of the Act 2 Program and the overall goal of encouraging land re-use through certainty and efficiency in investigation and clean-up. This presentation will explain: 1) the legal and technical origins of these challenges; 2) how the Pennsylvania Department of Environmental Protection ("DEP") is addressing these challenges; and 3) practical implications and approaches for managing vapor intrusion in Pennsylvania under Act 2. Act 2 was not written with the vapor pathway of exposure in mind. For example, under Section 303(e), Statewide Health Attainment: Vapor intrusion ("VI") exposures are not considered in development of Statewide Health Medium Specific Concentrations (MSCs) under Section 303 For groundwater, attainment of Statewide Health Standards ("SHS") is demonstrated at the point of compliance, defined as the property boundary at time of discovery of contamination A potential for vapor intrusion may exist within a site that has achieved attainment of SHS at the property boundary A deed acknowledgement or environmental covenant is not required as to contamination that remains in groundwater if the SHS for groundwater is met for the contaminant at the property boundary. Therefore, the Act 2 statute itself, the source of authority for the Act 2 program, has a hole through which vapor intrusion could pass, especially as to buildings located on a source site where SHS standards for groundwater are used to demonstrate compliance with the statute. Despite the statutory hole, DEP has taken steps to address vapor intrusion for entire sites seeking to demonstrate attainment with SHS, e.g.: ? Through publishing guidance: Land Recycling Program Technical Guidance Manual, Section IV.A.4, Vapor Intrusion into Buildings from Groundwater and Soil Under the Act 2 Statewide Health Standard, Jan. 24, 2004 ("2004 SHS VI Guidance"). ? Through rulemaking: 25 Pa. Code &sect; 250.312, which states, "The final [SHS] report must include, as appropriate, an assessment of the vapor intrusion pathway." ? Through a current effort to update the 2004 SHS VI Guidance. The 2004 SHS VI Guidance is currently being revised for, among other reasons, replacement of outdated screening levels. Some screening values in the 2004 SHS VI Guidance for key contaminants, such as TCE, exceed comparable values used by the United States Environmental Protection Agency ("USEPA") and almost all other states that have such values, by several orders of magnitude.2 Under Act 2, DEP generally applies somewhat less conservative target risk levels than USEPA does in setting its screening values (one additional cancer in 100,000 under Act 2 versus one additional cancer in a million), but the updated screening values in the DEP's revised SHS VI Guidance are very likely to be orders of magnitude lower than the values in the 2004 version.3 DEP is currently struggling to develop the revised SHS VI Guidance in the face of the potentially dramatic drops in screening values, the variability that is increasingly associated with vapor intrusion sampling and results, and the need for efficiency and certainty for the Act 2 program to remain as effective as it has been in encouraging property re-use and re-development.4 Undeveloped parts of properties and the potential that future buildings will be configured differently than existing structures (for example, as to subgrade floors or parking garages) especially create challenges for a program that has come to be known for the stream-lined nature of its property assessment and the reliability of its releases of liability. Looming large is what impact change, in the form of guidance that directs a more robust evaluation of the vapor intrusion pathway, and most especially, much lower screen values, will have on the efficiency and certainly that have characterized Act 2 to date.5 In short, vapor intrusion creates risks for both remediators and developers under Act 2. Those risks include that releases of liability using SHS standards may be harder to obtain and may be re-opened, if not by DEP - which is not likely to deliberately undermine the certainty of its successful program -Then by third parties who seek protection, further remediation (or mitigation), or other relief where vapor intrusion is left unaddressed or arises as an issue in the future. Close evaluation of existing sign-offs for sites with remaining volatile contamination and common sense approaches to mitigating vapor risk are needed to achieve the predictability and confidence required to move forward with development in the face of the inherent uncertainty currently associated with VI assessments.<br/>},
key = {Land reclamation},
keywords = {Computer software reusability;Contamination;Diseases;Efficiency;Environmental Protection Agency;Environmental regulations;Garages (parking);Groundwater;Groundwater pollution;Impurities;Recycling;Regulatory compliance;Risk assessment;Sustainable development;},
note = {Existing structure;Orders of magnitude;Pennsylvania department of environmental protections;Property assessment;Specific concentration;Technical guidances;United States environmental protection agency;Vapor intrusion pathway;},
} 


@inproceedings{20151200655089,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The Declaratron, semantic specification for scientific computation using MathML},
journal = {CEUR Workshop Proceedings},
author = {Murray-Rust, Dave and Murray-Rust, Peter},
volume = {1010},
year = {2013},
issn = {16130073},
address = {Bath, United kingdom},
abstract = {We introduce the Declaratron, a system which takes a declarative approach to specifying mathematically based scientific computation. This uses displayable mathematical notation (Content MathML) and is both executable and semantically well defined. We combine domain specific representations of physical science (e.g. CML, Chemical Markup Language), MathML formulae and computational specifications (DeXML) to create executable documents which include scientific data and mathematical formulae. These documents preserve the provenance of the data used, and build tight semantic links between components of mathematical formulae and domain objects-in effect grounding the mathematical semantics in the scientific domain. The Declaratron takes these specifications and i) carries out entity resolution and decoration to prepare for computation ii) uses a MathML execution engine to run calculations over the revised tree iii) outputs domain objects and the complete document to give both results and an encapsulated history of the computation. A short description of a case study is given to illustrate how the system can be used. Many scientific problems require frequent change of the mathematical functional form and the Declaratron provides this without requiring changes to code. Additionally, it supports reproducible science, machine indexing and semantic search of computations, makes implicit assumptions visible, and separates domain knowledge from computational techniques. We believe that the Declaratron could replace much conventional procedural code in science.<br/>},
key = {Computation theory},
keywords = {Computer programming languages;Computer systems programming;Semantics;Specifications;User interfaces;XML;},
note = {Chemical markup languages;Computational specifications;Computational technique;Mathematical formulas;Mathematical notations;Mathematical semantics;Scientific computation;Semantic specification;},
} 


@inproceedings{20134116835110,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Desain study of Pb-Bi cooled fast reactors with natural uranium as fuel cycle input using special shuffling strategy in radial direction},
journal = {Advanced Materials Research},
author = {Su'ud, Zaki and Irka, Feriska H. and Imam, T.Taufiq and Sekimoto, H. and Sidik, P.},
volume = {772},
year = {2013},
pages = {530 - 535},
issn = {10226680},
address = {Singapore},
abstract = {Design study of Pb-Bi cooled fast reactors with natural uranium as fuel cycle input using special radial shuffling strategy has been performed. The reactors utilizes UN-PUN as fuel, Eutectic Pb-Bi as coolant, and can be operated without refueling for 10 years in each batch. Reactor design optimization is performed to utilize natural uranium as fuel cycle input. This reactor subdivided into 6 regions with equal volume in radial directions. The natural uranium is initially put in region 1, and after one cycle of 10 years of burn-up it is shifted to region 2 and the region 1 is filled by fresh natural uranium fuel. This concept is basically applied to all regions. The calculation has been done by using SRAC-Citation system code and JENDL-3.2 library. The effective multiplication factor change increases monotonously during 10 years reactor operation time. There is significant power distribution change in the central part of the core during the BOC and the EOC. It is larger than that in the case of modified CANDLE case which use axial direction burning region move. The burnup level of fuel is slowly grows during the first 15 years but then grow fastly in the rest of burnup history. This pattern is a little bit different from the case of modified CANDLE burnup scheme in Axial direction in which the slow growing burnup period is relatively longer almost half of the burnup history. &copy; (2013) Trans Tech Publications, Switzerland.},
key = {Lead},
keywords = {Batch reactors;Fast reactors;Fuels;Nuclear fuels;Optimization;Uranium;},
note = {Candle burn-up schemes;Effective multiplication factor;Fuel cycle;Long life;Nitride fuel;Power distributions;Radial direction;Radial shuffling;},
URL = {http://dx.doi.org/10.4028/www.scientific.net/AMR.772.530},
} 


@book{20182705416062,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Polarimetric Scattering and SAR Information Retrieval},
journal = {Polarimetric Scattering and SAR Information Retrieval},
author = {Jin, Ya-Qiu and Xu, Feng},
year = {2013},
abstract = {Taking an innovative look at Synthetic Aperture Radar (SAR), this practical reference fully covers new developments in SAR and its various methodologies and enables readers to interpret SAR imagery. An essential reference on polarimetric Synthetic Aperture Radar (SAR), this book uses scattering theory and radiative transfer theory as a basis for its treatment of topics. It is organized to include theoretical scattering models and SAR data analysis techniques, and presents cutting-edge research on theoretical modelling of terrain surface. The book includes quantitative approaches for remote sensing, such as the analysis of the Mueller matrix solution of random media, mono-static and bistatic SAR image simulation. It also covers new parameters for unsupervised surface classification, DEM inversion, change detection from multi-temporal SAR images, reconstruction of building objects from multi-aspect SAR images, and polarimetric pulse echoes from multi-layering scatter media. Structured to encourage methodical learning, earlier chapters cover core material, whilst later sections involve more advanced new topics which are important for researchers. The final chapter completes the book as a reference by covering SAR interferometry, a core topic in the remote sensing community. Features theoretical scattering models and SAR data analysis techniques. Explains the simulation of SAR images for mono- and bi-static radars, covering both qualitative and quantitative information retrieval Chapter topics include: theoretical scattering models; SAR data analysis and processing techniques; and theoretical quantitative simulation reconstruction and inversion techniques. Structured to enable both academic learning and independent study, laying down the foundations first of all before advancing to more complex topics. Experienced author team presents mathematical derivations and figures so that they are easy for readers to understand. Pitched at graduate-level students in electrical engineering, physics, earth and space sciences, as well as researchers. MATLAB code available for readers to run their own routines. An invaluable reference for research scientists, engineers and scientists working on polarimetric SAR hardware and software, Application developers of SAR and polarimetric SAR, remote sensing specialists working with SAR data - using ESA. &copy; 2013 John Wiley &amp; Sons Singapore Pte. Ltd.<br/>},
key = {Radar imaging},
keywords = {Application programs;Coremaking;Data handling;Earth (planet);Image analysis;Image reconstruction;Information retrieval;MATLAB;Matrix algebra;Polarimeters;Remote sensing;Space optics;Students;Synthetic aperture radar;},
note = {Mathematical derivation;Mueller matrix solution;Multi-temporal SAR images;Polarimetric scattering;Polarimetric synthetic aperture radars;Quantitative information;Quantitative simulation;Radiative transfer theory;},
URL = {http://dx.doi.org/10.1002/9781118188149},
} 


@inproceedings{20141017425072,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A numerical steady state and dynamic study in a data center using calibrated fan curves for CRACS and servers},
journal = {ASME 2013 International Technical Conference and Exhibition on Packaging and Integration of Electronic and Photonic Microsystems, InterPACK 2013},
author = {Alkharabsheh, Sami A. and Sammakia, Bahgat and Shrivastava, Saurabh and Ellsworth, Michael and David, Milnes and Schmidt, Roger},
volume = {2},
year = {2013},
pages = {Electronic and Photonic Packaging Division - },
address = {Burlingame, CA, United states},
abstract = {This study presents the results of a detailed parametric study for a data center that is air cooled using a set of four CRAC units in a cold/hot aisle raised floor configuration. The fans of the CRAC units and the servers are calibrated using their practical characteristics fan curves. A commercial CFD code is utilized for this purpose in which the buoyancy forces are taken into account. The k-epsilon model and the Boussinesq approximation are used to model the turbulent airflow and the buoyancy effect, respectively. A dynamic model is developed to take into account the changes in flow rates and power dissipation in the data center environment. The current dynamic model does not take into account the thermal mass of the CRAC units or the servers. The effect of the CRAC fan speed, instantaneous change in power dissipation, tiles perforation ratio, and servers fan speeds on the total flow rate in the room and the inlet temperatures of the racks are investigated. In the transient model, we investigate the effect of different CRAC failure scenarios on the time history of the temperatures and the flow pattern in the data center. Time constants and safe time are estimated from this study. A fundamental understanding of the effect of different data center entities on the flow and the temperatures is developed. Interesting flow patterns are observed in the case of different CRAC failures that could be used to recommend general design guidelines. &copy; 2014 ASME.<br/>},
key = {Information management},
keywords = {Air;Buoyancy;Computational fluid dynamics;Dynamic models;Electric losses;Energy efficiency;Flow patterns;Microsystems;},
note = {Boussinesq approximations;Buoyancy effect;Buoyancy forces;Current dynamics;Failure scenarios;Inlet temperature;K epsilon models;Parametric study;},
URL = {http://dx.doi.org/10.1115/IPACK2013-73217},
} 


@article{20131516194153,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The GAIA theory: From Lovelock to Margulis. from a homeostatic to a cognitive autopoietic worldview},
journal = {Rendiconti Lincei},
author = {Onori, Luciano and Visconti, Guido},
volume = {23},
number = {4},
year = {2012},
pages = {375 - 386},
issn = {20374631},
abstract = {This work consists of two parts. The first presents the state of art concerning the history and the reception by the scientific community of the Gaia hypothesis introduced in the 1970s and which evolved, in time, into theory and quasi-science, i.e., Earth system science. The original Gaia supposes that the temperature, oxidation state, acidity and certain aspect of rocks and waters are at any time kept constant and that this homeostasis is maintained by active biogeochemical feedback processes (first-order cybernetics) operated automatically and unconsciously by the biota. In turn, the probability of life's event and its survival should be linked to processes regulated by the second thermodynamic principle, in its own dynamical equilibrium. This consists in maintaining the organisms at a low level of entropy, through energy-dissipative leakage into the surrounding environment. Life and the environment are so closely coupled that evolution concerns Gaia, and not the organisms or the environment taken separately. Since the end of 1980s, Lynn Margulis, Lovelock's longstanding co-author, proposed replacing Gaia's homeostatic nature with an autopoietic and evolutionary one that is connected to second-order cybernetic processes. Margulis arrived at the Gaian paradigm shift, mainly based on her authority in the field of the microcosmos. This included symbiogenetic processes concerning the birth and evolution of microbiotic organisms at the planetary level, which led to the construction of macroorganisms and their properties that stabilize the environment. A close relationship between symbiogenetic and autopoietic theory (the latter proposed by Maturana and Varela in Autopoiesis and Cognition: the Realization of the Living, D. Reidel Publishing Co., Dordecht 1980) is represented by the fact that both theories refer primarily to the epigenetic-cytoplasmatic mechanisms in cellular constitution and evolution, and only secondarily to the established, DNA-mediated genetic code. It is the consequent lack of primeval genetic information that requires that both theories postulate the existence of cognitive-intentional properties of the living matter (Luisi in Springer 90(2):49-59, 2003) in the construction of the cell and of multicellular organisms. Conversely, traditional theory treats biological organizations as an epiphenomenon, that is, a result of casual processes leading to the constitution of genetic material responsible of cell constitution, duplication and sometime mutation-recombination for new phenotypic forms. The second part of this work consists of more speculative comments about some important articulations of the Gaian construct, in particular: (a) The apparent lack of information on the chemical-physical nature of living and inert matter and on their possible interaction in the construction of organisms and environment. (b) The substantial weakness in the descriptive processes leading to the auto-organization of the two terrestrial matrices (organisms and environment) that is Lovelock's engineeristic and physiological automatisms without consciousness and Margulis' cognitive symbiogenetic processes operating at elementary matter. Both hypotheses have been scantly accepted by established science. The latter appears to privilege the theory of spontaneous and istantaneous cooperative phenomenon between elementary particles, at the base of the change from chaos to order and from one ordered state to another, both in physical and living realm. (c) Finally, the substantial underevaluation, operated by Lovelock in his holistic approach to the study of planet Earth, of the role played by the physical phenomenon of the distance interaction between quantum objects, leading to their entanglement. Such phenomena, apparently spontaneous, istantaneous and mediated by quantum field, have questioned the same objective nature of reality. Recently, Su&die;sskind (interviews with P. Byrne, Scientific American, June 2011) noticed that the entanglement phenomenon allows obtaining the knowledge of everything about a composite system without knowing its singular parts: a possible form of holistic approach to planet Earth, well distinct from those proposed by Lovelock and Margulis on solely cybernetic basis. &copy; 2012 The Author(s).<br/>},
key = {Chaos theory},
keywords = {C (programming language);Earth (planet);Gene encoding;Geology;Geophysics;Large scale systems;Physics;Psychophysiology;Quantum entanglement;Social sciences;},
note = {Biological organization;Cooperative phenomenon;Geophysiology;Global environment;Homeostasis;Multicellular organisms;Origin of life;Surrounding environment;},
URL = {http://dx.doi.org/10.1007/s12210-012-0187-z},
} 


@article{20140117153458,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Urban and landscape changes through historical maps: The Real Sitio of Aranjuez (1775-2005), a case study},
journal = {Computers, Environment and Urban Systems},
author = {San-Antonio-Gomez, C. and Velilla, C. and Manzano-Agugliaro, F.},
volume = {44},
year = {2014},
pages = {47 - 58},
issn = {01989715},
abstract = {When determining the evolution of a territory or town over time, comparing historical maps with contemporary maps is indispensable. In this study, we applied the methodology of georectification to compare historical maps with current orthophotos from 2005. We propose colour and lines code as useful tools for the analysis of the urban and landscape changes that the town has undergone since the 18th century, and we graphically reconstruct certain former heritage items that no longer exist. For example, these techniques are applied to the Real Sitio de Aranjuez (Spain) using the two most important historical maps: the 1775 Domingo de Aguirre map, which shows the full extent of the royal site for the first time, and the 1835 General Town Plan, which is the most characteristic of available 19th-century maps, as it displays the consolidated historical town. Next, using two rectified rasters and the orthophoto, we overlay a grid of nine 1. &times;. 1. km squares, allowing us to "see the town and its territory" at three moments in history: 1775, 1835 and 2005. Thus, we obtain formal and dimensional information allowing analysis of the evolution of the territory, urban area and historic buildings. Among the many applications of this methodology in the fields of urban development and monumental-heritage conservation, we propose the graphical reconstruction of three urban elements that no longer exist. We determined that graphical reconstruction, in conjunction with traditional historical research, provides the greatest benefits for recreating an historical landscape. These methodologies will aid in the development of long-range management strategies and facilitate the assessment of threats posed by anthropogenic activities and environmental change to preserve the landscape heritage. &copy; 2013 Elsevier Ltd.<br/>},
key = {Geomorphology},
keywords = {Color codes;Urban growth;},
note = {Heritage conservation;Historical maps;Planimetric accuracy;Rectification;Urban development;},
URL = {http://dx.doi.org/10.1016/j.compenvurbsys.2013.12.001},
} 


@article{20153101085050,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Study of Tectonic Tremor in Depth: Triggering Stress Observation and Model of the Triggering Mechanism},
journal = {ProQuest Dissertations and Theses Global},
author = {Wang, Tien-Huei},
year = {2014},
abstract = {Non-volcanic tremor (NVT) has been discovered in recent years due to advances in seismic instruments and increased density of seismic networks. The NVT is a special kind of seismic signal indicative of the physical conditions and the failure mechanism on the source on the fault where NVT occurs. The detection methods used and the sensitivity of them relies on the density, distance and instrumentation of the station network available. How accurately the tremor is identified in different regions varies greatly among different studies. Therefore, there has not been study that rigorously documents tectonic tremors in different regions under limited methods and data. Meanwhile, many incidences of NVTs are observed during or after small but significant strain change induced by teleseismic, regional or local earthquake. The understanding of the triggering mechanisms critical for tremor remains unclear. In addition, characteristics of the triggering of NVT in different regions are rarely compared because of the short time frame after the discovery of the triggered NVTs. We first explore tectonic tremor based on observations to learn about its triggering, frequency of occurrence, location and spectral characteristics. Then, we numerically model the triggering of instability on the estimated tremor-source, under assumptions fine-tuned according to previous studies (Thomas et al., 2009; Miyazawa et al., 2005; Hill, 2008; Ito, 2009; Rubinstein et al., 2007; Peng and Chao, 2008). The onset of the slip reveals that how and when the external loading triggers tremor. It also holds the information to the background stress conditions under which tremor source starts with. We observe and detect tremor in two regions: Anza and Cholame, along San Jacinto Fault (SJF) and San Andreas Fault (SAF) respectively. These two sections of the faults, relative to general fault zone on which general earthquakes occur, are considered transition zones where slip of slow rates occurs. Slip events including NVT occur on these sections have slower slip rates than that of the general earthquakes (Rubin, 2008; Ide, 2008). In Azna region, we use envelope and waveform cross-correlation to detect tremor. We investigate the stress required to trigger tremor and tremor spectrum using continuous broadband seismograms from 11 stations located near Anza, California. We examine 44 Mw&ge;7.4 teleseismic events between 2001 and 2011, in addition to one regional earthquake of smaller-magnitude, the 2009 Mw 6.5 Gulf of California earthquake, because it induced extremely high strain at Anza. The result suggests that not only the amplitude of the induced strain, but also the period of the incoming surface wave, may control triggering of tremor near Anza. In addition, we find that the transient-shear stress (17--35 kPa) required to trigger tremor along the SJF at Anza is distinctly higher than what has been reported for the well-studied SAF (Gulihem et al. 2010). We model slip initiation using the analytical solution of rate-and-state friction. We verify the correctness of this method by comparing the results with that from the dynamic model, implemented using the Multi-Dimensional Spectral Boundary Integral Code (MDSBI) written by Eric M. Dunham from Sanford University. We find that the analytical result is consistent with that of the dynamic model. We set up a patch model with which the source stress and frictional conditions best resemble the current estimates of the tremor source. The frictional regime of this patch is rate-weakening. The initial normal and shear stress, and friction parameters are suggested by previous observations of tectonic tremors both in this and other studies (Brown et al., 2005; Shelly et al., 2006; Miyazawa, 2008; Ben-Zion, 2012). Our dynamic loading first consists of simple harmonic stress change with fixed periods, simplifying the transient stress history to resemble teleseismic earthquakes. We tested the period and amplitude of such periodic loading. We find that the period of the transient shear stress is less important relative to the amplitude. The triggering depends mainly on the ratio between amplitude of the shear stress loading and the background normal stress. We define a range of ratio indicative of the occurrence of the triggering. We later test the triggering of the instability using the shear stress history from 44 large teleseismic earthquakes (data equivalent to those used in Chapter 1). With the constraints of these observations, we find that the background normal stress should be in the range of &sim;400-700 kPa. The background normal stress suggested agrees with the common hypothesis that the tremor source is under low normal stress. In addition, our results provide a first estimation of the background normal stress with numerical method. We also demonstrate how our model find constrains on the background physical stress or frictional conditions, with several true incidences that transient shear stress triggers or not-triggers tremor. (Abstract shortened by UMI.). ProQuest Subject Headings: Geophysics.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.},
key = {Earthquakes},
keywords = {Dynamic loads;Dynamic models;Friction;Geophysics;Numerical methods;Shear stress;Strike-slip faults;Surface waves;},
note = {Broadband seismograms;Non-volcanic tremors;Normal and shear stress;Rate and state friction;Spectral characteristics;Teleseismic earthquakes;Triggering mechanism;Waveform cross correlation;},
} 


@article{20152100869388,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modelling of seismic wave propagation in 2d},
journal = {Romanian Review Precision Mechanics, Optics and Mechatronics},
author = {Dervishaj, Arben and Paci, Ervin and Cullufi, Hektor},
number = {45},
year = {2014},
pages = {120 - 124},
issn = {15845982},
abstract = {From the seismic hazard map of Albania the designers of structures take the peak ground acceleration (PGA) values on ground type A (rock). Based on these values and the type of the ground is formed the response spectra which serves as input for seismic analysis of structures. Recent measurements and studies of different authors have emphasized that for the ground classification accepted in EC8 response spectrum values in some cases do not match with reality. So it is necessary, especially when we built on soft soils is important to do a detailed analysis for propagation and amplification of seismic waves. These analyses are even more important if for the calculation of the structure will be used seismic dynamic analysis in which time history of acceleration is used as input. The article gives for a real example the wave propagation modelling made with "Plaxis 2D" software. The choice of soil parameters, damping coefficients, boundary modelling, finite element size, the integration constants are chosen based on the literature and 1D solutions. After all parameters are calibrated in the model solution output are obtained time histories of acceleration and corresponding spectra. Obtained spectra are compared with spectra generated by Euro Code 8 procedures and have been seen that they change, so is therefore recommended that for seismic analysis of the structure to use values taken from the study.<br/> &copy; 2014, Editura Cefin. All right reserved.},
key = {Seismology},
keywords = {Seismic waves;Wave propagation;},
note = {2-D model;Damping coefficients;Integration constants;Peak ground acceleration;Response spectra;Seismic analysis;Seismic hazard maps;Soil parameters;},
} 


@article{20132516426227,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A fully coupled multiphase multicomponent flow and geomechanics model for enhanced coalbed-methane recovery and CO2storage},
journal = {SPE Journal},
author = {Wei, Zhijie and Zhang, Dongxiao},
volume = {18},
number = {3},
year = {2013},
pages = {448 - 467},
issn = {1086055X},
abstract = {Enhanced coalbed-methane (ECBM) recovery by the injection of CO<inf>2</inf>and/or N<inf>2</inf>is an attractive method for recovering additional natural gas resources, while at the same time sequestering CO<inf>2</inf>in the subsurface. For the naturally fractured coalbed-methane (CBM) reservoirs, the coupled fluid-flow and geomechanics effects involving both the effective-stress effect and the matrix shrinkage/swelling, are crucial to simulate the permeability change and; thus gas migration during primary or enhanced CBM recovery. In this work, a fully coupled multiphase multicomponent flow and geomechanics model is developed. The coupling effects are modeled by introducing a set of elaborate geomechanical equations, which can provide more fundamental understanding about the solid deformation and give a more accurate permeability/porosity prediction over the existing analytical models. In addition, the fluid-flow model in our study is fully compositional; considering both multicomponent gas dissolution and water volatility. To obtain accurate gas solubility in the aqueous phase, the Peng-Robinson equation of state (EOS) is modified according to the suggestions of S&oslash;reide and Whitson (1992). An extended Langmuir isotherm is used to describe the adsorption/desorption behavior of the multicomponent gas to/from the coal surface. With a fully implicit finite-difference method, we develop: a 3D, multiphase, multicomponent, dual-porosity CBM/ECBM research code that is fully compositional and has fully coupled fluid flow and geomechanics. It has been partially validated and verified by comparison against other simulators such as GEM, Eclipse, and Coalgas. We then perform a series of simulations/investigations with our research code. First, history matching of Alberta flue-gas-injection micropilot data is performed to test the permeability model. The commonly used uniaxial-strain and constant-overburden-stress assumptions for analytical permeability models are then assessed. Finally, the coupling effects of fluid flow and geomechanics are investigated, and the impact of different mixed CO<inf>2</inf>/N<inf>2</inf>injection scenarios is explored for bothmethane (CH<inf>4</inf>) production and CO<inf>2</inf>sequestration. Copyright &copy; 2013 Society of Petroleum Engineers.<br/>},
key = {Methane},
keywords = {Carbon dioxide;Coal bed methane;Coal deposits;Digital storage;Energy resources;Equations of state of gases;Finite difference method;Firedamp;Geomechanics;Isotherms;Multiphase flow;Natural gas;Petroleum reservoir engineering;Petroleum reservoirs;Porosity;Recovery;},
note = {Adsorption/desorption;Effective stress effects;Enhanced coalbed methane recoveries;Fluid flow modeling;Fully implicit finite differences;Multiphase multicomponent flows;Natural gas resources;Peng-Robinson equation of state;},
URL = {http://dx.doi.org/10.2118/163078-PA},
} 


@inproceedings{20124715677284,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Analysis of BWR OPRM plant data and detection algorithms with DSSPP},
journal = {International Congress on Advances in Nuclear Power Plants 2012, ICAPP 2012},
author = {Yang, J. and Vedovi, J. and Chung, A.K. and Zino, J.F.},
volume = {4},
year = {2012},
pages = {2370 - 2375},
address = {Chicago, IL, United states},
abstract = {All U.S. BWRs are required to have licensed stability solutions that satisfy General Design Criteria (GDC) 10 and 12 of 10 CFR 50 Appendix A. Implemented solutions are either detect and suppress or preventive in nature. Detection and suppression of power oscillations is accomplished by specialized hardware and software such as the Oscillation Power Range Monitor (OPRM) utilized in Option III and Detect and Suppress Solution - Confirmation Density (DSS-CD) stability Long-Term Solutions (LTSs). The detection algorithms are designed to recognize a Thermal-Hydraulic Instability (THI) event and initiate control rod insertion before the power oscillations increase much higher above the noise level that may threaten the fuel integrity. Option III is the most widely used long-term stability solution in the US and has more than 200 reactor years of operational history. DSS-CD represents an evolutionary step from the stability LTS Option III and its licensed domain envelopes the Maximum Extended Load Line Limit Analysis Plus (MELLLA +) domain. In order to enhance the capability to investigate the sensitivity of key parameters of stability detection algorithms, GEH has developed a new engineering analysis code, namely DSSPP (Detect and Suppress Solution Post Processor), which is introduced in this paper. The DSSPP analysis tool represents a major advancement in the method for diagnosing the design of stability detection algorithms that enables designers to perform parametric studies of the key parameters relevant for THI events and to fine tune these system parameters such that a potential spurious scram might be avoided. Demonstrations of DSSPPs application are also presented in this paper utilizing actual plant THI data. A BWR/6 plant had a plant transient that included unplanned recirculation pump transfer from fast to slow speed resulting in about 100% to &sim;40% rated power decrease and about 99% to &sim;30% rated core flow decrease. As the feedwater temperature is reduced to equilibrium conditions, the power increased from about &sim;40% to about &sim;60% with little change inflow. A THI event developed and subsequently, an OPRM initiated scram occurred with Option III.<br/>},
key = {Boiling water reactors},
keywords = {Nuclear energy;Nuclear fuels;Nuclear power plants;Parameter estimation;Signal detection;Stability criteria;},
note = {Detection algorithm;Engineering analysis;Equilibrium conditions;General design criteria;Long term stability;Long-term solutions;Power oscillations;Specialized hardware;},
} 


@article{20153801282058,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modeling transient multiphase flow and mold top surface behavior in steel continuous casting},
journal = {ProQuest Dissertations and Theses Global},
author = {Liu, Rui},
year = {2014},
abstract = {This thesis develops, validates and applies a system of computational models to investigate transient multiphase turbulent flow physics in the mold region and mold top surface behavior during continuous casting of steel slabs. Each model can be used independently or combined together as a comprehensive system to gain insights into the inter-related multiphase fluid dynamics in the caster mold during both quasi-steady-state and essentially transient events in practical casting operations. Argon gas commonly is injected into the liquid metal stream through porous refractory walls in many metallurgical processes. Modeling multiphase flows in caster molds is of great significance to understanding of inclusion transport and defect formation mechanisms and to improving the quality of the final products. To better understand the gas injection process, a new model is developed to investigate gas permeating through heated upper tundish nozzle (UTN) porous refractory, including the effects of nozzle geometry, gas thermal expansion, temperature-dependent gas viscosity, and possible gas leakages into unsealed joints. Furthermore, a procedure to predict initial bubble size is established. Two (semi-) analytical models, a stopper-position- and a gate-position- based model, predict liquid steel flow rate histories during the transient events and serve as a first step of this comprehensive model system. Argon-steel two-phase flow during a transient "declogging" event with multiple stopper-rod movements is simulated. The flow rate history during stopper rod movements is obtained from the analytical model, and the hot argon flow rate calculated using the porous-flow model and the initial bubble size estimated. Nail board experiments are also conducted to measure steel surface velocities and mold level profiles. A correlation for surface velocity prediction is proposed based on previous modeling results and validated by another set of measurements using a sub-meniscus velocity control (SVC) device. To further understand particle transport and deposition in wall bounded turbulent flows, direct numerical simulations (DNS) are performed in the continuous phase and a Lagrangian particle tracking algorithm was developed into the in-house code, CU-FLOW, to investigate dispersion and deposition of particles with different Stokes numbers in a square duct flow with and without the effect of imposed magnetic field. A new free-surface tracking model with a moving-grid technique is developed and integrated in the commercial computational fluid dynamics (CFD) package of ANSYS Fluent (v14.5) based on its dynamic mesh feature, which naturally combines with multiphase flow models. This model is validated and adopted to simulate the dynamic responses of mold top surface to flow rate variations in the SEN subject to the upstream actuator position change. The complete model system is applied to investigate the effects of slide-gate dithering on transient single- and multi- phase flows in the caster mold. Mold sloshing is identified by both plant experiments and numerical simulations when the dithering frequency matches with the mold natural frequency determined by its geometry. Mechanism for the liquid steel flow variation to activate this standing wave (mold sloshing) is discussed. Multiphase flow pattern and top surface evolution under a low-frequency dithering trial is studied via numerical simulations. Mold level fluctuations are computed from the dithering simulations are compared in favor with the measurements. Up to this point, the model system has been demonstrated a powerful computational tool to resolve complicated multiphase flows during essentially transient events in continuous steel casting subjected to flow rate (both liquid steel and argon gas) variations. ProQuest Subject Headings: Mechanical engineering, Mechanics, Applied mathematics.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.},
key = {Two phase flow},
keywords = {Analytical models;Argon;Computational fluid dynamics;Computational methods;Continuous casting;Deposition;Flow patterns;Flow rate;Forecasting;Gases;Heating;Liquid sloshing;Mechanical engineering;Mechanics;Mechanisms;Molds;Nozzles;Numerical models;Refractory materials;Refractory metals;Slab mills;Steel castings;Thermal expansion;Turbulent flow;},
note = {Applied mathematics;Continuous casting of steels;Continuous steel casting;Lagrangian particle tracking;Multi-phase flow models;Particle transport and depositions;Steel continuous castings;Wall-bounded turbulent flows;},
} 


@article{20144500161273,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {3D numerical modeling using smoothed particle hydrodynamics of flow-like landslide propagation triggered by the 2008 Wenchuan earthquake},
journal = {Engineering Geology},
author = {Dai, Zili and Huang, Yu and Cheng, Hualin and Xu, Qiang},
volume = {180},
year = {2014},
pages = {21 - 33},
issn = {00137952},
abstract = {The extremely strong Wenchuan earthquake triggered thousands of landslides in Sichuan Province, China. Flow-like landslides, such as the Tangjiashan, Wangjiayan, and Donghekou landslides were among the most destructive, causing many casualties and serious economic damage. It is therefore important to identify the flow mechanisms and investigate the specific characteristics of these flow-like landslides. This paper presents a three-dimensional model based on smoothed particle hydrodynamics (SPH) to simulate rapid landslide motion across 3D terrain. The Navier-Stokes equations in a CFD framework are used as governing equations, and artificial viscosity is incorporated into the pressure terms in the momentum equation to dissipate energy to avoid numerical oscillation and particle penetration, thus improving the stability of the numerical results. A non-Newtonian fluid model, the Bingham model, has proven suitable for describing the relationship between the shear strain rate and the shear stress in highly deformed soil materials. In the proposed model it is used as the constitutive model to describe the fluidization characteristics of flow-like landslides combined with the Mohr-Coulomb yield criterion. The model incorporates a no-slip boundary condition, to consider the effect of a solid boundary on slope movement. Ghost particles are created and assigned an artificial velocity. The viscous force caused by the solid boundary is calculated using the relative velocities between the fluid and the boundary particles. Open Multiprocessing (OpenMP), an API for multi-platform shared-memory parallel programming, is used to improve the efficiency of the SPH code running. To show the validity of the proposed approach, a benchmark problem of 3D dam break was simulated. The calculated distances of the surge front at different times agree well with the test results. Numerical modeling of the propagation of the Tangjiashan, Wangjiayan, and Donghekou landslides was performed by the application of SPH models to real flow-like landslides. The whole flow processes of these flow-like landslides across the 3D terrain are represented. The landslides change direction, split or join in, and spread or contract in their flow path in response to the local topography. Time-history curves of the velocity and displacement were obtained to analyze the movement characteristics of the landslide mass. The shapes of the deposition zones after slide occurrence were investigated. Comparisons of the SPH simulated geometry and the surveyed landslide configurations for the Tangjiashan, Wangjiayan and Donghekou landslides were conducted, and show a high degree of similarity. This indicates that the proposed 3D SPH model can accurately represent the evolution of the final slide shape. The prediction of the fluidization characteristics of earthquake-induced flow-like landslides can notably reduce sudden loss of life, as it provides a means for mapping hazardous areas, for estimating the hazard intensity, and for identification and design of appropriate protective measures.<br/> &copy; 2014 Elsevier B.V.},
key = {Hydrodynamics},
keywords = {Application programming interfaces (API);Computational fluid dynamics;Earthquakes;Elastohydrodynamic lubrication;Fluidization;Hazards;Landslides;Navier Stokes equations;Non Newtonian flow;Non Newtonian liquids;Numerical models;Parallel programming;Shear strain;Shear stress;Strain rate;Viscous flow;},
note = {Fluidization characteristics;Hazard Assessment;Mohr Coulomb yield criterion;No-slip boundary conditions;Shared memory parallel programming;Smoothed particle hydrodynamics;Three-dimensional numerical simulations;Wenchuan Earthquake;},
URL = {http://dx.doi.org/10.1016/j.enggeo.2014.03.018},
} 


@inproceedings{20143618135491,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {4th International Conference on Frontiers of Manufacturing Science and Measuring Technology, ICFMM 2014},
journal = {Applied Mechanics and Materials},
volume = {599-601},
year = {2014},
pages = {Academy Publisher; CEIS; et al; INDERSCIENCE Publishers; TMS; Trans Tech publications inc. - },
issn = {16609336},
address = {Guilin, China},
abstract = {The proceedings contain 487 papers. The special focus in this Conference is on Frontiers of Manufacturing Science and Measuring Technology. The topics include: Analysis of factors influencing the performance of Q460 steel; development and application of aluminium-lithium alloy; corrosion resistance of X52 pipeline steel at sea mud zone; experimental study on the burning behavior of automotive internal decorating materials; mechanical properties analysis and application of metal matrix composites; side effects on enhancing chemical herbicides to control proposal; study on ball milling of TiH<inf>2</inf> and application in energetic materials; study on features and application of typical smart materials; texture evolution of friction-stir-welded 5A30 aluminum alloy plate; the range distribution of Yb ions implanted in 6H silicon carbide; the analysis of temperature field of concrete beam of hydration heat; double sided explosive cladding of stainless steel and regular steel; the influence of particle diameter on analytical results; 90W off-line high power factor correction module design; cavitation effect to the hydraulic piston pump flow pulsation; design and analysis of a novel micro flying head in mass storage system; design of a type of lift coating machine; design of for feel simulator of battleplan; development of the assistant positioning device of parallel machine vice; effect of the rudder on the vibration frequency of wind turbine; hardware design of automotive parking heater remoter and receiver; hydraulic excavator boom lightweight design; numerical investigation of tip clearance flow in an axial compressor cascade; parametric design of tapered end mills with variable pitch for improving stability; research on attitude singularity problem of small tail-sitter aircraft; research on typical vibration accelerations of wind wheel changing with yaw angle; simulation and optimization design of gasoline engine exhaust muffler; simulation study on a parallel hybrid system of hydraulic excavator; the design of the fixture for the thin wall cone parts; the forced response analysis of heavy duty vehicle's cab; UML based design approach for storage system of nano-satellite; modal analysis based on finite element jaw crusher rotor; parameter identification of a plastic damage model; stress forming of fine metal wire based on ultrasonic vibration; modeling and predicting surface roughness for the grinding process; design, fabrication and evaluation of a multifunctional jacket with optoelectronic effects; a control system on soil temperature; a novel optical fiber displacement measurement system; a method of correcting brightness gradient in thermal infrared image; the intelligent traffic monitoring system based on Beidou research and design; the resulting toolpath of turning a special surface on CNC lathe; vehicle trajectory prediction based on road recognition; an approach for detecting illegal load in wireless power transfer system; analysis and research of Rogowski coil current sensing technology; approach to design digital meter for monitoring armored vehicle engine; design for speed measuring of railway crossing based on track circuit; design of automatic milking controller; design of plunger limit switch testing system based on virtual instrument technology; empirical likelihood based testing for polynomial regression models; finite element analysis of capacitance sensor; hybrid base and parallel hybrid electric vehicle control strategy; modeling analysis and experimental study of circular piezoelectric unimorph actuator; research on data acquisition technology of wind turbine monitoring system; test and analysis of dynamic characteristics of reinforced concrete arch bridge; the design of communication system for wearable health monitoring equipment; the development of an automatic ultrasonic non-destructive testing system; static characteristic test of high temperature pressure sensor; the measurement of concrete temperature distribution based on VB; a fast pose estimation method of four-rotor aircraft; effect of vehicle speed on contact pressure and current collecting quality; computer modal analysis of the two-dimensional precision turntable; the localization method of ALV based on lateral dynamics; a class of difference inequality with three iterative summation; a new method for code optimization; a new rough set model and its application; an improved decision strategy of network routing based on ant colony algorithm; computer network security strategy research; face recognition image processing design research; mobile learning mode based on 3G; prediction of road congestion level based on Bayes algorithm; research of text recognition based on natural scene; simulation of clouds dynamics based on the change of sight distance; speaker recognition techniques; the SWOT analysis of the application of cloud computing to archives informatization; weakness and improvement of an efficient key agreement protocol; image decomposition and in painting based on Besov and Hilbert-Sobolev space; research on design of network curriculum; optimal power flow solution using the harmony search algorithm; getting the health management information system building; cross-cultural awareness in college english teaching; the research on the core system of athletics; on the design and development of micro-course; a traffic capacity model of lane occupation; design and implementation of storage management system based on Java; material selection and management based on cybernetics; the urban and rural public transit demand analysis in Hohhot; computer database system in the application of information management and theoretical framework for innovation design with optimised customization.},
} 



