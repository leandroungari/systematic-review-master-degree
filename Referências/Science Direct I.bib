@article{HUCK20161485,
title = "Source Transformation of C++ Codes for Compatibility with Operator Overloading",
journal = "Procedia Computer Science",
volume = "80",
pages = "1485 - 1496",
year = "2016",
note = "International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2016.05.470",
url = "http://www.sciencedirect.com/science/article/pii/S1877050916309553",
author = "Alexander Hück and Jean Utke and Christian Bischof",
keywords = "C++, Type Change, Static Analysis, Source Transformation, Operator Overloading, Algorithmic Differentiation",
abstract = "In C++, new features and semantics can be added to an existing software package without sweeping code changes by introducing a user-defined type using operator overloading. This approach is used, for example, to add capabilities such as algorithmic differentiation. However, the introduction of operator overloading can cause a multitude of compilation errors. In a previous paper, we identified code constructs that cause a violation of the C++ language standard after a type change, and a tool called OO-Lint based on the Clang compiler that identifies these code constructs with lint-like messages. In this paper, we present an extension of this work that automatically transforms such problematic code constructs in order to make an existing code base compatible with a semantic augmentation through operator overloading. We applied our tool to the CFD software OpenFOAM and detected and transformed 23 instances of problematic code constructs in 160,000 lines of code. A significant amount of these root causes are included up to 425 times in other files causing a tremendous compiler error amplification. In addition, we show the significance of our work with a case study of the evolution of the ice flow modeling software ISSM, comparing a recent version which was manually type changed with a legacy version. The recent version shows no signs of problematic code constructs. In contrast, our tool detected and transformed a remarkable amount of issues in the legacy version that previously had to be manually located and fixed."
}
@article{GODBOLEY20171,
title = "J3 Model: A novel framework for improved Modified Condition/Decision Coverage analysis",
journal = "Computer Standards & Interfaces",
volume = "50",
pages = "1 - 17",
year = "2017",
issn = "0920-5489",
doi = "https://doi.org/10.1016/j.csi.2016.09.006",
url = "http://www.sciencedirect.com/science/article/pii/S0920548916300770",
author = "Sangharatna Godboley and Arpita Dutta and Durga Prasad Mohapatra and Rajib Mall",
keywords = "MC/DC, CONCOLIC Testing, JPCT, JCUTE, JCA",
abstract = "In the real-time systems and safety critical domains, software quality assurance adheres to protocols such as DO-178C standard. Regarding these issues, concolic testing generates test cases that can attain high coverage using an augmented approach based on Modified Condition/Decision Coverage (MC/DC). In this paper, we propose a framework to compute MC/DC percentage for test case generation. To achieve an increase in MC/DC, we transform the input Java program, J, into its transformed version, J′, using Java Program Code Transformer (JPCT). Then, we use JCUTE tool to generate test cases. At last, we use Java Coverage Analyzer (JCA) to compute MC/DC percentage. The Java program code transformer adds additional empty nested if-else conditional statements for each decision that causes variation in MC/DC percentage. In later step, these extra conditional statements get stripped-off. This approach resolves some of the bottleneck issues associated with traditional concolic testers. In our experimental study, we have experimented with forty Java programs. We have computed the difference of MC/DC%, for both the scenarios (i.e. with code transformation and without code transformation). Our approach (i.e. with code transformation achieves) 24.09% average increase in MC/DC% over the traditional approach (i.e. without code transformation)."
}
@article{FROESEFISCHER2018,
title = "GRASP2018—A Fortran 95 version of the General Relativistic Atomic Structure Package",
journal = "Computer Physics Communications",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2018.10.032",
url = "http://www.sciencedirect.com/science/article/pii/S0010465518303928",
author = "C. Froese Fischer and G. Gaigalas and P. Jönsson and J. Bieroń",
keywords = "Atomic structure calculations, Configuration interaction, Correlation, Energy levels, Isotope shift, Multiconfiguration Dirac–Hartree–Fock, Relativistic effects in atoms, Transition probabilities",
abstract = "The present Grasp2018 is an updated Fortran 95 version of the recommended block versions of programs from Grasp2K Version 1_1 for large-scale calculations Jönsson et al. (2013). MPI programs are included so that all major tasks can be executed using parallel computers. Tools have been added that simplify the generation of configuration state function expansions for the multireference single- and double computational model. Names of programs have been changed to accurately reflect the task performed by the code. Modifications to the relativistic self-consistent field program have been made that, in some instances, greatly reduce the number of iterations needed for determining the requested eigenvalues and the memory required. Changes have been made to the relativistic configuration interaction program to substantially cut down on the time for constructing the Hamiltonian matrix for configuration state function expansions based on large orbital sets. In the case of a finite nucleus the grid points have been changed so that the first non-zero point is Z-dependent as for the point nucleus. A number of tools have been developed to generate LaTeX tables of eigenvalue composition, energies, transition data and lifetimes. Tools for plotting and analyzing computed properties along an iso-electronic sequence have also been added. A number of minor errors have been corrected. A detailed manual is included that describes different aspects of the package as well as the steps needed in order to produce reliable results.
Program summary
Program Title:Grasp2018 Program Files doi:http://dx.doi.org/10.17632/x574wpp2vg.1 Licensing provisions: MIT license Programming language: Fortran 95. Nature of problem: Prediction of atomic properties – atomic energy levels, isotope shifts, oscillator strengths, radiative decay rates, hyperfine structure parameters, specific mass shift parameters, Zeeman effects – using a multiconfiguration Dirac–Hartree–Fock approach. Solution method: The computational method is the same as in the previous Grasp2K [1,2] version except that only the latest recommended versions of certain routines are included. Restrictions: All calculations are for bound state solutions. Instead of relying on packing algorithms for specifying arguments of arrays of integrals, orbitals are designated by a “short integer” requiring one byte of memory for a maximum of 127 orbitals. The tables of reduced coefficients of fractional parentage used in this version are limited to sub-shells with j≤9∕2 [3]; occupied sub-shells with j>9∕2 are, therefore, restricted to a maximum of two electrons. Some other parameters, such as the maximum number of orbitals are determined in a parameter_def_M.f90 file that can be modified prior to compile time. Unusual features: Parallel versions are available for several applications. References•[[1]] P. Jönsson, X. He, C. Froese Fischer, and I. P. Grant, Comput. Phys. Commun. 176, 597 (2007).•[[2]] P. Jönsson, G. Gaigalas, J. Bieroń, C. Froese Fischer, and I. P. Grant, Comput. Phys. Commun. 184, 2197 (2013).•[[3]] G. Gaigalas, S. Fritzsche, Z. Rudzikas, Atomic Data and Nuclear Data Tables 76, 235 (2000)."
}
@article{YIN2017278,
title = "Sparse representation over discriminative dictionary for stereo matching",
journal = "Pattern Recognition",
volume = "71",
pages = "278 - 289",
year = "2017",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2017.06.015",
url = "http://www.sciencedirect.com/science/article/pii/S0031320317302364",
author = "Jihao Yin and Hongmei Zhu and Ding Yuan and Tianfan Xue",
keywords = "Computer vision, Stereo matching, Data-driven, Sparse coding, Dictionary learning",
abstract = "We propose a novel data-driven matching cost for dense correspondence based on sparse theory. The ability of sparse coding to selectively express the sources of influence on stereo images allows us to learn a discriminative dictionary. The dictionary learning process is incorporated with discriminative learning and weighted sparse coding to enhance the discrimination of sparse coefficients and weaken the influence of radiometric changes. Then, the sparse representations over the learned discriminative dictionary are utilized to measure the dissimilarity between image patches. Semi-global cost aggregation and postprocessings are finally enforced to further improve the matching accuracy. Extensive experimental comparisons demonstrate that: the proposed matching cost outperforms traditional matching costs, the discriminative dictionary learning model is more suitable than previous dictionary learning models for stereo matching, and the proposed stereo method ranks the third place on the Middlebury benchmark v3 in quarter resolution up to the submitting, and achieves the best accuracy on 30 classic stereo images."
}
@article{DAMOUCHE201547,
title = "Transformation of a PID Controller for Numerical Accuracy",
journal = "Electronic Notes in Theoretical Computer Science",
volume = "317",
pages = "47 - 54",
year = "2015",
note = "The Seventh and Eighth International Workshops on Numerical Software Verification (NSV)",
issn = "1571-0661",
doi = "https://doi.org/10.1016/j.entcs.2015.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S157106611500047X",
author = "N. Damouche and M. Martel and A. Chapoutot",
keywords = "Numerical Accuracy, Semantics-Based Program Transformation, Floating-Point Arithmetic, Validation of Numerical Programs",
abstract = "Numerical programs performing floating-point computations are very sensitive to the way formulas are written. Several techniques have been proposed concerning the transformation of expressions in order to improve their accuracy and now we aim at going a step further by automatically transforming larger pieces of code containing several assignments and control structures. This article presents a case study in this direction. We consider a PID controller and we transform its code in order to improve its accuracy. The experimental data obtained when we compare the different versions of the code (which are mathematically equivalent) show that those transformations have a significant impact on the accuracy of the computations."
}
@article{HELMI2018464,
title = "Multi-step ahead time series forecasting via sparse coding and dictionary based techniques",
journal = "Applied Soft Computing",
volume = "69",
pages = "464 - 474",
year = "2018",
issn = "1568-4946",
doi = "https://doi.org/10.1016/j.asoc.2018.04.017",
url = "http://www.sciencedirect.com/science/article/pii/S1568494618302084",
author = "Ahmed Helmi and Mohamed W. Fakhr and Amir F. Atiya",
keywords = "Time series forecasting, Sparse coding, Local learning, Machine learning",
abstract = "Sparse coding is based on the concept of having a large dictionary of candidate basis vectors. Any given vector is expressed as a sparse linear combination of the dictionary vectors. It has been developed in the signal processing field, and has many applications in data compression and image processing. In this paper we propose applying sparse coding to the time series forecasting field. Specifically, the paper investigates different dictionary based local learning techniques for building predictive models for the time series forecasting problem. The proposed methodology is based on a local learning framework whereby the query point is embedded and coded in terms of a sparse combination of the training dictionary atoms (vectors). Then this embedding is used for estimating the target value of the query point, by applying the same embedding to the target vectors of the dictionary training atoms. We present an experimental study of several sparse coding algorithms. Experiments are performed on the large monthly time series benchmark from the M3 competition, and these experiments showed that the sparse methods Lasso and Elastic-Net presented the best results among the sparse coding algorithms. Moreover, they outperformed the K-nearest neighbor (KNN) regression and most of the compared machine learning and statistical forecasting techniques, especially for higher horizons."
}
@article{LIU20181329,
title = "Generative Predictive Codes by Multiplexed Hippocampal Neuronal Tuplets",
journal = "Neuron",
volume = "99",
number = "6",
pages = "1329 - 1341.e6",
year = "2018",
issn = "0896-6273",
doi = "https://doi.org/10.1016/j.neuron.2018.07.047",
url = "http://www.sciencedirect.com/science/article/pii/S0896627318306457",
author = "Kefei Liu and Jeremie Sibille and George Dragoi",
keywords = "predictive codes, Markov model, plasticity, tuplet, neuronal ensemble, preplay, hippocampus, replay, sequence editing, intrinsic-unlikely signal",
abstract = "Summary
Rapid internal representations are continuously formed based on single experiential episodes in space and time, but the neuronal ensemble mechanisms enabling rapid encoding without constraining the capacity for multiple distinct representations are unknown. We developed a probabilistic statistical model of hippocampal spontaneous sequential activity and revealed existence of an internal model of generative predictive codes for the regularities of multiple future novel spatial sequences. During navigation, the inferred difference between external stimuli and the internal model was encoded by emergence of intrinsic-unlikely, novel functional connections, which updated the model by preferentially potentiating post-experience. This internal model and these predictive codes depended on neuronal organization into inferred modules of short, high-repeat sequential neuronal “tuplets” operating as “neuro-codons.” We propose that flexible multiplexing of neuronal tuplets into repertoires of extended sequences vastly expands the capacity of hippocampal predictive codes, which could initiate top-down hierarchical cortical loops for spatial and mental navigation and rapid learning."
}
@article{LOU201720,
title = "Use of ontology structure and Bayesian models to aid the crowdsourcing of ICD-11 sanctioning rules",
journal = "Journal of Biomedical Informatics",
volume = "68",
pages = "20 - 34",
year = "2017",
issn = "1532-0464",
doi = "https://doi.org/10.1016/j.jbi.2017.02.004",
url = "http://www.sciencedirect.com/science/article/pii/S1532046417300254",
author = "Yun Lou and Samson W. Tu and Csongor Nyulas and Tania Tudorache and Robert J.G. Chalmers and Mark A. Musen",
keywords = "Sanctioning rules, Ontology, ICD, Post-coordination, Crowdsourcing, Bayesian network",
abstract = "The International Classification of Diseases (ICD) is the de facto standard international classification for mortality reporting and for many epidemiological, clinical, and financial use cases. The next version of ICD, ICD-11, will be submitted for approval by the World Health Assembly in 2018. Unlike previous versions of ICD, where coders mostly select single codes from pre-enumerated disease and disorder codes, ICD-11 coding will allow extensive use of multiple codes to give more detailed disease descriptions. For example, “severe malignant neoplasms of left breast” may be coded using the combination of a “stem code” (e.g., code for malignant neoplasms of breast) with a variety of “extension codes” (e.g., codes for laterality and severity). The use of multiple codes (a process called post-coordination), while avoiding the pitfall of having to pre-enumerate vast number of possible disease and qualifier combinations, risks the creation of meaningless expressions that combine stem codes with inappropriate qualifiers. To prevent that from happening, “sanctioning rules” that define legal combinations are necessary. In this work, we developed a crowdsourcing method for obtaining sanctioning rules for the post-coordination of concepts in ICD-11. Our method utilized the hierarchical structures in the domain to improve the accuracy of the sanctioning rules and to lower the crowdsourcing cost. We used Bayesian networks to model crowd workers’ skills, the accuracy of their responses, and our confidence in the acquired sanctioning rules. We applied reinforcement learning to develop an agent that constantly adjusted the confidence cutoffs during the crowdsourcing process to maximize the overall quality of sanctioning rules under a fixed budget. Finally, we performed formative evaluations using a skin-disease branch of the draft ICD-11 and demonstrated that the crowd-sourced sanctioning rules replicated those defined by an expert dermatologist with high precision and recall. This work demonstrated that a crowdsourcing approach could offer a reasonably efficient method for generating a first draft of sanctioning rules that subject matter experts could verify and edit, thus relieving them of the tedium and cost of formulating the initial set of rules."
}
@article{LEE2016185,
title = "Co-changing code volume prediction through association rule mining and linear regression model",
journal = "Expert Systems with Applications",
volume = "45",
pages = "185 - 194",
year = "2016",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2015.09.023",
url = "http://www.sciencedirect.com/science/article/pii/S0957417415006454",
author = "Shin-Jie Lee and Li Hsiang Lo and Yu-Cheng Chen and Shi-Min Shen",
keywords = "Co-changing code volume prediction, Co-changing methods identification",
abstract = "Code smells are symptoms in the source code that indicate possible deeper problems and may serve as drivers for code refactoring. Although effort has been made on identifying divergent changes and shotgun surgeries, little emphasis has been put on predicting the volume of co-changing code that appears in the code smells. More specifically, when a software developer intends to perform a particular modification task on a method, a predicted volume of code that will potentially be co-changed with the method could be considered as significant information for estimating the modification effort. In this paper, we propose an approach to predicting volume of co-changing code affected by a method to be modified. The approach has the following key features: co-changing methods can be identified for detecting divergent changes and shotgun surgeries based on association rules mined from change histories; and volume of co-changing code affected by a method to be modified can be predicted through a derived fitted regression line with t-test based on the co-changing methods identification results. The experimental results show that the success rate of co-changing methods identification is 82% with a suggested threshold, and the numbers of correct identifications would not be influenced by the increasing number of commits as a project continuously evolves. Additionally, the mean absolute error of co-changing code volume predictions is 133 lines of code which is 95.3% less than the one of a naive approach."
}
@article{HUTTEN2019336,
title = "CLUMPY v3: γ-ray and ν signals from dark matter at all scales",
journal = "Computer Physics Communications",
volume = "235",
pages = "336 - 345",
year = "2019",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2018.10.001",
url = "http://www.sciencedirect.com/science/article/pii/S0010465518303497",
author = "Moritz Hütten and Céline Combet and David Maurin",
keywords = "Cosmology, Dark matter, Indirect detection, Gamma-rays, Neutrinos",
abstract = "We present the third release of the CLUMPY code for calculating γ-ray and ν signals from annihilations or decays in dark matter structures. This version includes the mean extragalactic signal with several pre-defined options and keywords related to cosmological parameters, mass functions for the dark matter structures, and γ-ray absorption up to high redshift. For more flexibility and consistency, dark matter halo masses and concentrations are now defined with respect to a user-defined overdensity Δ. We have also made changes for the user’s benefit: distribution and versioning of the code via git, less dependencies and a simplified installation, better handling of options in run command lines, consistent naming of parameters, and a new Sphinx documentation at http://lpsc.in2p3.fr/clumpy/.
Program summary
Program Title:CLUMPY Program Files doi:http://dx.doi.org/10.17632/4n33mbh9bc.1 Licensing provisions: GPLv2 Programming language: C/C++ External routines/libraries:GSL (http://www.gnu.org/software/gsl), cfitsio ( http://heasarc.gsfc.nasa.gov/fitsio/fitsio.html), CERN ROOT (http://root.cern.ch; optional, for interactive figures and stochastic simulation of halo substructures), GreAT (http://lpsc.in2p3.fr/great; optional, for MCMC Jeans analyses) Nature of problem: Calculation of the γ-ray and ν signals from dark matter annihilation/decay at any redshift z. Solution method: New in this release: Numerical integration of moments (in redshift and mass) of the mass function, absorption, and intensity multiplier (related to the DM density along the line of sight). Restrictions: Secondary radiation from dark matter leptons, which depends on astrophysical ingredients (radiation fields in the Universe) is the last missing piece to provide a full description of the expected signal."
}
@article{ZIEB201877,
title = "Review of heavy charged particle transport in MCNP6.2",
journal = "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
volume = "886",
pages = "77 - 87",
year = "2018",
issn = "0168-9002",
doi = "https://doi.org/10.1016/j.nima.2018.01.002",
url = "http://www.sciencedirect.com/science/article/pii/S0168900218300020",
author = "K. Zieb and H.G. Hughes and M.R. James and X.G. Xu",
keywords = "MCNP6, Hadrons, Charged particles, Ions, Monte Carlo, Radiation transport",
abstract = "The release of version 6.2 of the MCNP6 radiation transport code is imminent. To complement the newest release, a summary of the heavy charged particle physics models used in the 1 MeV to 1 GeV energy regime is presented. Several changes have been introduced into the charged particle physics models since the merger of the MCNP5 and MCNPX codes into MCNP6. This paper discusses the default models used in MCNP6 for continuous energy loss, energy straggling, and angular scattering of heavy charged particles. Explanations of the physics models’ theories are included as well."
}
@article{SUMNER2018422,
title = "Neural plasticity is modified over the human menstrual cycle: Combined insight from sensory evoked potential LTP and repetition suppression",
journal = "Neurobiology of Learning and Memory",
volume = "155",
pages = "422 - 434",
year = "2018",
issn = "1074-7427",
doi = "https://doi.org/10.1016/j.nlm.2018.08.016",
url = "http://www.sciencedirect.com/science/article/pii/S1074742718302156",
author = "R.L. Sumner and M.J. Spriggs and R.L. McMillan and F. Sundram and I.J. Kirk and S.D. Muthukumaraswamy",
keywords = "Long-term potentiation, Mismatch negativity, EEG, Predictive coding, Hebbian learning, Female",
abstract = "In healthy women, fluctuations in hormones including progesterone and oestradiol lead to functional changes in the brain over the course of each menstrual cycle. Though considerable attention has been directed towards understanding changes in human cognition over the menstrual cycle, changes in underlying processes such as neural plasticity have largely only been studied in animals. In this study we explored predictive coding and repetition suppression via the roving mismatch negativity paradigm as a model of short-term plasticity (Garrido, Kilner, Kiebel, et al., 2009), and Hebbian learning via visual sensory long-term potentiation (LTP) as a model of long-term plasticity (Teyler et al., 2005). Electroencephalography (EEG) was recorded in 20 females during their early follicular and mid-luteal phases. Event-related potential (ERP) analyses were complemented with dynamic causal modelling (DCM) to characterise changes in the underlying neural architecture. More sustained variability in the ERP response to a change in tone during the luteal phase are interpreted as a delayed habituation of the P3a component in the luteal relative to the follicular phase. The additional increased forward connection strength over tone repetitions compared to the follicular phase suggests that, in this phase, females may be less efficient when processing deviations from predicted sensory input (error). In contrast, there appears to be no reliable change in sensory LTP. This suggests that predictive coding, but not Hebbian plasticity is modified in the mid-luteal compared to the follicular phase, at least at the days of the menstrual cycle tested. This finding implicates the human menstrual cycle in complex changes in neural plasticity and provides further evidence for the importance of considering the menstrual cycle when including females in electrophysiological research."
}
@article{ECKERMANN201816,
title = "Hyperactive piggyBac transposase improves transformation efficiency in diverse insect species",
journal = "Insect Biochemistry and Molecular Biology",
volume = "98",
pages = "16 - 24",
year = "2018",
issn = "0965-1748",
doi = "https://doi.org/10.1016/j.ibmb.2018.04.001",
url = "http://www.sciencedirect.com/science/article/pii/S0965174818301413",
author = "Kolja N. Eckermann and Hassan M.M. Ahmed and Mohammad KaramiNejadRanjbar and Stefan Dippel and Christian E. Ogaugwu and Peter Kitzmann and Musa D. Isah and Ernst A. Wimmer",
keywords = "Coleoptera, Molecular entomology, Tephritid fruit flies, Transgenics, Transposon",
abstract = "Even in times of advanced site-specific genome editing tools, the improvement of DNA transposases is still on high demand in the field of transgenesis: especially in emerging model systems where evaluated integrase landing sites have not yet been created and more importantly in non-model organisms such as agricultural pests and disease vectors, in which reliable sequence information and genome annotations are still pending. In fact, random insertional mutagenesis is essential to identify new genomic locations that are not influenced by position effects and thus can serve as future stable transgene integration sites. In this respect, a hyperactive version of the most widely used piggyBac transposase (PBase) has been engineered. The hyperactive version (hyPBase) is currently available with the original insect codon-based coding sequence (ihyPBase) as well as in a mammalian codon-optimized (mhyPBase) version. Both facilitate significantly higher rates of transposition when expressed in mammalian in vitro and in vivo systems compared to the classical PBase at similar protein levels. Here we demonstrate that the usage of helper plasmids encoding the hyPBase - irrespective of the codon-usage - also strikingly increases the rate of successful germline transformation in the Mediterranean fruit fly (Medfly) Ceratitis capitata, the red flour beetle Tribolium castaneum, and the vinegar fly Drosophila melanogaster. hyPBase-encoding helpers are therefore highly suitable for the generation of transgenic strains of diverse insect orders. Depending on the species, we achieved up to 15-fold higher germline transformation rates compared to PBase and generated hard to obtain transgenic T. castaneum strains that express constructs affecting fitness and viability. Moreover, previously reported high sterility rates supposedly caused by hyPBase (iPB7), encoded by ihyPBase, could not be confirmed by our study. Therefore, we value hyPBase as an effective genetic engineering tool that we highly recommend for insect transgenesis."
}
@article{RUTTER2018174,
title = "C2x: A tool for visualisation and input preparation for Castep and other electronic structure codes",
journal = "Computer Physics Communications",
volume = "225",
pages = "174 - 179",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2017.12.008",
url = "http://www.sciencedirect.com/science/article/pii/S0010465517304137",
author = "M.J. Rutter",
keywords = "Spglib, Electronic structure visualisation, Crystallographic symmetry, Supercell generation",
abstract = "The c2x code fills two distinct roles. Its first role is in acting as a converter between the binary format .check files from the widely-used Castep [1] electronic structure code and various visualisation programs. Its second role is to manipulate and analyse the input and output files from a variety of electronic structure codes, including Castep, Onetep and Vasp, as well as the widely-used ‘Gaussian cube’ file format. Analysis includes symmetry analysis, and manipulation arbitrary cell transformations. It continues to be under development, with growing functionality, and is written in a form which would make it easy to extend it to working directly with files from other electronic structure codes. Data which c2x is capable of extracting from Castep’s binary checkpoint files include charge densities, spin densities, wavefunctions, relaxed atomic positions, forces, the Fermi level, the total energy, and symmetry operations. It can recreate .cell input files from checkpoint files. Volumetric data can be output in formats useable by many common visualisation programs, and c2x will itself calculate integrals, expand data into supercells, and interpolate data via combinations of Fourier and trilinear interpolation. It can extract data along arbitrary lines (such as lines between atoms) as 1D output. C2x is able to convert between several common formats for describing molecules and crystals, including the .cell format of Castep. It can construct supercells, reduce cells to their primitive form, and add specified k-point meshes. It uses the spglib library [2] to report symmetry information, which it can add to .cell files. C2x is a command-line utility, so is readily included in scripts. It is available under the GPL and can be obtained from http://www.c2x.org.uk. It is believed to be the only open-source code which can read Castep’s .check files, so it will have utility in other projects.
Program summary
Program Title: c2x Program Files doi:http://dx.doi.org/10.17632/wj5hcj7x39.1 Licensing provisions: GPLv3 Programming language: C Nature of problem: C2x is able to extract a large variety of data from Castep’s [1] large, binary-format, output files to facilitate further processing or visualisation. These binary files are optimised for rapid input and output by Castep, but are impossible to read without detailed knowledge of their internal structure, which varies between different versions of Castep. In them wavefunctions are stored as plane wave coefficients, so need to be Fourier transformed before they can be visualised in real space. C2x can manipulate the input files of Castep, and other common crystallographic formats, performing functions useful for setting up calculations, such as supercell construction, format conversions, and symmetry analysis. Different electronic structure codes use different file formats for outputting densities. C2x is able to convert between some of the major formats, allowing visualisation and post-processing tools targeted at one electronic structure code to be used with others. Solution method: C2x is a command-line utility written in standard C. It is able to process large (multi-GB) binary files efficiently, extracting much smaller datasets. C2x has considerable knowledge of the structure of the checkpoint files as written by various versions of Castep. Wavefunctions are optionally converted to densities, and then transformed to real space using its own FFT routine. Weighted sums of densities from multiple bands can be accumulated. C2x interpolates volumetric data by using user-specified combinations of trilinear and Fourier interpolation. For symmetry analysis it relies on the existing spglib [2] library, which is also used by Castep. It supports cell transformations with the axes of the new cell expressed in absolute terms, or in terms of the original axes. It has its own internal representation of the unit cell and its contents, and has several routines for converting this to and from common crystallographic file formats. C2x is able to read densities from Castep’s binary formats, and both read and write Castep’s formatted density files, Vasp’s formatted density files, and the .cube files used by Gaussian and Onetep. Additional comments: Primary site for distribution and documentation: www.c2x.org.uk [1] S. J. Clark, M. D. Segall, C. J. Pickard, P. J. Hasnip, M. J. Probert, K. Refson, M. Payne, First principles methods using CASTEP, Z. Kristall. 220 (2005) 567–570. [2] A. Togo, Spglib, a library for finding and handling crystal symmetries [cited 2017-02-23]. https://atztogo.github.io/spglib/"
}
@article{HAITZER2014135,
title = "Semi-automated architectural abstraction specifications for supporting software evolution",
journal = "Science of Computer Programming",
volume = "90",
pages = "135 - 160",
year = "2014",
note = "Special Issue on Component-Based Software Engineering and Software Architecture",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2013.10.004",
url = "http://www.sciencedirect.com/science/article/pii/S0167642313002542",
author = "Thomas Haitzer and Uwe Zdun",
keywords = "Architectural abstraction, Architectural component and connector views, Software evolution, UML, Model transformation",
abstract = "In this paper we present an approach for supporting the semi-automated architectural abstraction of architectural models throughout the software life-cycle. It addresses the problem that the design and implementation of a software system often drift apart as software systems evolve, leading to architectural knowledge evaporation. Our approach provides concepts and tool support for the semi-automatic abstraction of architecture component and connector views from implemented systems and keeping the abstracted architecture models up-to-date during software evolution. In particular, we propose architecture abstraction concepts that are supported through a domain-specific language (DSL). Our main focus is on providing architectural abstraction specifications in the DSL that only need to be changed, if the architecture changes, but can tolerate non-architectural changes in the underlying source code. Once the software architect has defined an architectural abstraction in the DSL, we can automatically generate architectural component views from the source code using model-driven development (MDD) techniques and check whether architectural design constraints are fulfilled by these models. Our approach supports the automatic generation of traceability links between source code elements and architectural abstractions using MDD techniques to enable software architects to easily link between components and the source code elements that realize them. It enables software architects to compare different versions of the generated architectural component view with each other. We evaluate our research results by studying the evolution of architectural abstractions in different consecutive versions of five open source systems and by analyzing the performance of our approach in these cases."
}
@article{GE2016112,
title = "Dynamic background estimation and complementary learning for pixel-wise foreground/background segmentation",
journal = "Pattern Recognition",
volume = "59",
pages = "112 - 125",
year = "2016",
note = "Compositional Models and Structured Learning for Visual Recognition",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2016.01.031",
url = "http://www.sciencedirect.com/science/article/pii/S0031320316000522",
author = "Weifeng Ge and Zhenhua Guo and Yuhan Dong and Youbin Chen",
keywords = "Background subtraction (BS), GMM, ViBe, CB, Dynamic background estimation, Complementary learning",
abstract = "Change and motion detection plays a basic and guiding role in surveillance video analysis. Since most outdoor surveillance videos are taken in native and complex environments, these “static” backgrounds change in some unknown patterns, which make perfect foreground extraction very difficult. This paper presents two universal modifications for pixel-wise foreground/background segmentation: dynamic background estimation and complementary learning. These two modifications are embedded in three classic background subtraction algorithms: probability based background subtraction (Gaussian mixture model, GMM), sample based background subtraction (visual background extractor, ViBe) and code words based background subtraction (code book, CB). Experiments on several popular public datasets prove the effectiveness and real-time performance of the proposed method. Both GMM and CB with the proposed modifications have better performance than the original versions. Especially, ViBe with the modifications outperforms some state-of-art algorithms presented on the CHANGEDETECTION website."
}
@article{YANG2017206,
title = "TLEL: A two-layer ensemble learning approach for just-in-time defect prediction",
journal = "Information and Software Technology",
volume = "87",
pages = "206 - 220",
year = "2017",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2017.03.007",
url = "http://www.sciencedirect.com/science/article/pii/S0950584917302501",
author = "Xinli Yang and David Lo and Xin Xia and Jianling Sun",
keywords = "Ensemble learning, Just-in-time defect prediction, Cost effectiveness",
abstract = "Context
Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time [1].
Objective
Ensemble learning becomes a hot topic in recent years. There have been several studies about applying ensemble learning to defect prediction [2–5]. Traditional ensemble learning approaches only have one layer, i.e., they use ensemble learning once. There are few studies that leverages ensemble learning twice or more. To bridge this research gap, we try to hybridize various ensemble learning methods to see if it will improve the performance of just-in-time defect prediction. In particular, we focus on one way to do this by hybridizing bagging and stacking together and leave other possibly hybridization strategies for future work.
Method
In this paper, we propose a two-layer ensemble learning approach TLEL which leverages decision tree and ensemble learning to improve the performance of just-in-time defect prediction. In the inner layer, we combine decision tree and bagging to build a Random Forest model. In the outer layer, we use random under-sampling to train many different Random Forest models and use stacking to ensemble them once more.
Results
To evaluate the performance of TLEL, we use two metrics, i.e., cost effectiveness and F1-score. We perform experiments on the datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. Also, we compare our approach with three baselines, i.e., Deeper, the approach proposed by us [6], DNC, the approach proposed by Wang et al. [2], and MKEL, the approach proposed by Wang et al. [3]. The experimental results show that on average across the six datasets, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code, as compared with about 50% for the baselines. In addition, the F1-scores TLEL can achieve are substantially and statistically significantly higher than those of three baselines across the six datasets.
Conclusion
TLEL can achieve a substantial and statistically significant improvement over the state-of-the-art methods, i.e., Deeper, DNC and MKEL. Moreover, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code."
}
@article{CARLI20196,
title = "Sensitivity analysis of plasma edge code parameters through algorithmic differentiation",
journal = "Nuclear Materials and Energy",
volume = "18",
pages = "6 - 11",
year = "2019",
issn = "2352-1791",
doi = "https://doi.org/10.1016/j.nme.2018.11.027",
url = "http://www.sciencedirect.com/science/article/pii/S2352179118302321",
author = "Stefano Carli and Maarten Blommaert and Wouter Dekeyser and Martine Baelmans",
keywords = "Plasma edge modelling, SOLPS-ITER, Algorithmic differentiation, Sensitivity analysis",
abstract = "Anomalous radial transport coefficients, boundary conditions and reaction rates are among the main sources of uncertainty within plasma edge modeling. In principle, an analysis to determine the sensitivity of code results to changes in uncertain model parameters can be easily implemented through finite differences. However, this incurs in error accumulations and allows scanning only one parameter at a time, requiring a huge computational effort. Algorithmic Differentiation (AD) is a possible alternative to finite differences already applied to several transport codes in different research domains but not yet in plasma edge modeling. AD tools preprocess the source code, identifying elementary operations for which the differential form is well known, and producing a new version of the code that contains the additional derivative information. In this paper, the feasibility of applying AD to plasma edge codes is demonstrated using the TAPENADE tool on the SOLPS-ITER code. As a first preliminary step, the AD tool is applied in the so-called “forward” mode on the B2.5 plasma solver, adopting a fluid neutral approximation to obtain the sensitivities of the calculated quantities of interest on selected code parameters. The proof of principle is carried out by comparing the AD results with those evaluated with finite differences on an ITER H-only case. The sensitivities of the target peak heat load and maximum electron temperature with respect to anomalous radial transport coefficients and core input power are assessed. The comparison with finite differences results in a relative error lower than 10−6. This proves that, in a next step, AD can be exploited for an efficient and accurate sensitivity analysis in the framework of plasma edge simulations."
}
@article{ZHANG2018162,
title = "Estimating residential energy consumption in metropolitan areas: A microsimulation approach",
journal = "Energy",
volume = "155",
pages = "162 - 173",
year = "2018",
issn = "0360-5442",
doi = "https://doi.org/10.1016/j.energy.2018.04.161",
url = "http://www.sciencedirect.com/science/article/pii/S0360544218307849",
author = "Wenwen Zhang and Caleb Robinson and Subhrajit Guhathakurta and Venu M. Garikapati and Bistra Dilkina and Marilyn A. Brown and Ram M. Pendyala",
keywords = "Residential energy consumption, Data synthesis, Statistical matching, Machine learning",
abstract = "Prior research has shown that land use patterns and the spatial configurations of cities have a significant impact on residential energy demand. Given the pressing issues surrounding energy security and climate change, there is renewed interest in developing and retrofitting cities to make them more energy efficient. Yet deriving micro-scale residential energy footprints of metropolitan areas is challenging because high resolution data from energy providers is generally unavailable. In this study, a bottom-up model is proposed to estimate residential energy demand using datasets that are commonly available in the United States. The model applies novel machine learning methods to match records in the Residential Energy Consumption Survey with Public Use Microdata samples. This matching and machine learning produce a synthetic household energy distribution at a neighborhood scale. The model was tested and validated with data from the Atlanta metropolitan region to demonstrate its application and promise."
}
@article{CHOCHLOV2017110,
title = "A historical, textual analysis approach to feature location",
journal = "Information and Software Technology",
volume = "88",
pages = "110 - 126",
year = "2017",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2017.04.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584917303397",
author = "Muslim Chochlov and Michael English and Jim Buckley",
keywords = "Feature location, Version histories, Dataset expansion, Software systems’ characterization, Search effort",
abstract = "Context
Feature location is the task of finding the source code that implements specific functionality in software systems. A common approach is to leverage textual information in source code against a query, using Information Retrieval (IR) techniques. To address the paucity of meaningful terms in source code, alternative, relevant source-code descriptions, like change-sets could be leveraged for these IR techniques. However, the extent to which these descriptions are useful has not been thoroughly studied.
Objective
This work rigorously characterizes the efficacy of source-code lexical annotation by change-sets (ACIR), in terms of its best-performing configuration.
Method
A tool, implementing ACIR, was used to study different configurations of the approach and to compare them to a baseline approach (thus allowing comparison against other techniques going forward). This large-scale evaluation employs eight subject systems and 600 features.
Results
It was found that, for ACIR: (1) method level granularity demands less search effort; (2) using more recent change-sets improves effectiveness; (3) aggregation of recent change-sets by change request, decreases effectiveness; (4) naive, text-classification-based filtering of “management” change-sets also decreases the effectiveness. In addition, a strongly pronounced dichotomy of subject systems emerged, where one set recorded better feature location using ACIR and the other recorded better feature location using the baseline approach. Finally, merging ACIR and the baseline approach significantly improved performance over both standalone approaches for all systems.
Conclusion
The most fundamental finding is the importance of rigorously characterizing proposed feature location techniques, to identify their optimal configurations. The results also suggest it is important to characterize the software systems under study when selecting the appropriate feature location technique. In the past, configuration of the techniques and characterization of subject systems have not been considered first-class entities in research papers, whereas the results presented here suggests these factors can have a big impact."
}
@article{SOYKAN2017871,
title = "Examining studies on learning management systems in SSCI database: A content analysis study",
journal = "Procedia Computer Science",
volume = "120",
pages = "871 - 876",
year = "2017",
note = "9th International Conference on Theory and Application of Soft Computing, Computing with Words and Perception, ICSCCW 2017, 22-23 August 2017, Budapest, Hungary",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2017.11.320",
url = "http://www.sciencedirect.com/science/article/pii/S1877050917325322",
author = "Fatih Soykan and Burak Şimşek",
keywords = "Learning management systems, distance education, open source, commercial learning management systems, developed learning management systems",
abstract = "Distance education systems have become more prevalent over time and their use continue to increase. Learning Management Systems (LMS) is the main component for distance education to be carried out effectively and distantly. In this stuy, it is aimed to examine current studies on LMS published between 2010 and 2014 and provide an instructive source for researchers. The obtained data were analyzed and interpreted based on certain criteria. According to the results, LMS studies are mostly published in Computer & Science Journal, however there is a decline in the number of studies in 2014. Results also showed that researchers mostly conducted studies with university students and they examined the systems either developed by themselves or by an institution. It was revealed that Moodle is mostly used as open source code systems and WebCT (Blackboard) is mostly used as commercial. It can be indicated that seeking for new LMS by researchers would change based on the requirements and technological advances in the further research."
}
@article{THOMAS2014457,
title = "Studying software evolution using topic models",
journal = "Science of Computer Programming",
volume = "80",
pages = "457 - 479",
year = "2014",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2012.08.003",
url = "http://www.sciencedirect.com/science/article/pii/S0167642312001621",
author = "Stephen W. Thomas and Bram Adams and Ahmed E. Hassan and Dorothea Blostein",
keywords = "Software evolution, Topic model, Latent Dirichlet allocation, Mining software repositories",
abstract = "Topic models are generative probabilistic models which have been applied to information retrieval to automatically organize and provide structure to a text corpus. Topic models discover topics in the corpus, which represent real world concepts by frequently co-occurring words. Recently, researchers found topics to be effective tools for structuring various software artifacts, such as source code, requirements documents, and bug reports. This research also hypothesized that using topics to describe the evolution of software repositories could be useful for maintenance and understanding tasks. However, research has yet to determine whether these automatically discovered topic evolutions describe the evolution of source code in a way that is relevant or meaningful to project stakeholders, and thus it is not clear whether topic models are a suitable tool for this task. In this paper, we take a first step towards evaluating topic models in the analysis of software evolution by performing a detailed manual analysis on the source code histories of two well-known and well-documented systems, JHotDraw and jEdit. We define and compute various metrics on the discovered topic evolutions and manually investigate how and why the metrics evolve over time. We find that the large majority (87%–89%) of topic evolutions correspond well with actual code change activities by developers. We are thus encouraged to use topic models as tools for studying the evolution of a software system."
}
@article{CHEN2018377,
title = "Structural damage detection via adaptive dictionary learning and sparse representation of measured acceleration responses",
journal = "Measurement",
volume = "128",
pages = "377 - 387",
year = "2018",
issn = "0263-2241",
doi = "https://doi.org/10.1016/j.measurement.2018.06.046",
url = "http://www.sciencedirect.com/science/article/pii/S0263224118305724",
author = "Zepeng Chen and Chudong Pan and Ling Yu",
keywords = "Structural damage detection (SDD), Sparse representation, Empirical mode decomposition (EMD), Sparse coding (SC), Damage indicator",
abstract = "Extracting damage-sensitive features from measured acceleration responses is still a big challenge in structural damage detection (SDD). In order to obtain the sparse representation of acceleration responses for damage identification, two dictionary learning methods and a damage indicator based on the change in mean sparsity (CMS) have been proposed in this paper. The first dictionary is empirical mode decomposition (EMD)-based dictionary generated by collecting a series of intrinsic mode functions (IMFs) while the second one is sparse coding (SC)-based dictionary learned by adaptively iterative optimization. The CMS is based on variation of a sparse features set. Numerical simulations on a simply-supported beam and on a 31-bar planar truss under different damage severities illustrate the effectiveness of the proposed methods for sparse representation and the capabilities of the defined CMS for damage indication. Comparative studies show that the SC-based dictionary outperforms the EMD-based dictionary in sparse representation. Moreover, a series of SDD experimental verifications on a simply-supported beam with a rectangular section in laboratory provide a further support to the proposed methods potentially using in-site acceleration measurements."
}
@article{HORA2015192,
title = "Automatic detection of system-specific conventions unknown to developers",
journal = "Journal of Systems and Software",
volume = "109",
pages = "192 - 204",
year = "2015",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2015.08.007",
url = "http://www.sciencedirect.com/science/article/pii/S0164121215001727",
author = "André Hora and Nicolas Anquetil and Anne Etien and Stéphane Ducasse and Marco Túlio Valente",
keywords = "Automatic coding convention detection, Mining software repositories, Software evolution",
abstract = "In Apache Ant, a convention to improve maintenance was introduced in 2004 stating a new way to close files instead of the Java generic InputStream.close(). Yet, six years after its introduction, this convention was still not generally known to the developers. Two existing solutions could help in these cases. First, one can deprecate entities, but, in our example, one can hardly deprecate Java’s method. Second, one can create a system-specific rule to be automatically enforced. In a preceding publication, we showed that system-specific rules are more likely to be noticed by developers than generic ones. However, in practice, developers rarely create specific rules. We therefore propose to free the developers from the need to create rules by automatically detecting such conventions from source code repositories. This is done by mining the change history of the system to discover similar changes being applied over several revisions. The proposed approach is applied to a real-world system, and the extracted rules are validated with the help of experts. The results show that many rules are in fact relevant for the experts."
}
@article{ABDALLAH2018364,
title = "PRISM revisited: Declarative implementation of a probabilistic programming language using multi-prompt delimited control",
journal = "International Journal of Approximate Reasoning",
volume = "103",
pages = "364 - 382",
year = "2018",
issn = "0888-613X",
doi = "https://doi.org/10.1016/j.ijar.2018.10.012",
url = "http://www.sciencedirect.com/science/article/pii/S0888613X1830272X",
author = "Samer Abdallah",
keywords = "Probabilistic programming, Logic programming, Delimited continuations, Algebraic effect handlers, Machine learning",
abstract = "PRISM is a probabilistic programming language based on Prolog, augmented with primitives to represent probabilistic choice. It is implemented using a combination of low level support from a modified version of B-Prolog, source level program transformation, and libraries for inference and learning implemented in C. More recently, developers working with functional programming languages have taken the approach of embedding probabilistic primitives into an existing language, with little or no modification to the host language, often by using delimited continuations. Captured continuations represent pieces of the probabilistic program which can be manipulated to achieve a great variety of computational effects useful for inference. In this paper, I will describe an approach based on delimited control operators recently introduced into SWI Prolog. These are used to create a system of nested effect handlers which together implement a core functionality of PRISM—the building of explanation graphs—entirely in Prolog and using an order of magnitude less code. Other declarative programming tools, such as constraint logic programming, are used to implement tools for inference, such as the inside-outside and EM algorithms, lazy best-first explanation search, and MCMC samplers. By embedding the functionality of PRISM into SWI Prolog, users gain access to its rich libraries and development environment. By expressing the functionality of PRISM in a small amount of pure, high-level Prolog, this implementation facilitates further experimentation with the mechanisms of probabilistic logic programming, including new probabilistic modelling features and inference algorithms, such as variational inference in models with real-valued variables."
}
@article{VISWASV2018812,
title = "Preventing Pollution Attacks in Cloud Storages",
journal = "Procedia Computer Science",
volume = "143",
pages = "812 - 819",
year = "2018",
note = "8th International Conference on Advances in Computing & Communications (ICACC-2018)",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2018.10.385",
url = "http://www.sciencedirect.com/science/article/pii/S1877050918320829",
author = "Aswin Viswas V and Philip Samuel",
keywords = "Cloud Computing, Cloud Storage, Coding, Data Integrity, Pollution Attack",
abstract = "Cloud storage is a cloud-computing model in which data is stored on remote servers accessed from the internet. It has significantly changed the way users and administrators manage and access their data. Using remote storages to store data has many advantages in terms of availability and operational costs, but the security of such data is still one of the major concerns for the users. Pollution attack, where an adversary modifies some of the stored data is one of the many potent risks that affect the cloud data. In this paper, we show how disastrous pollution attack can be in coding based block level cloud storages, and how our algorithm using LRC, a version of Raptor codes, can identify an attack even before decoding all of the received packets."
}
@article{ZENG2015336,
title = "Numerical simulation of single bubble condensation in subcooled flow using OpenFOAM",
journal = "Progress in Nuclear Energy",
volume = "83",
pages = "336 - 346",
year = "2015",
issn = "0149-1970",
doi = "https://doi.org/10.1016/j.pnucene.2015.04.011",
url = "http://www.sciencedirect.com/science/article/pii/S0149197015000992",
author = "Qingyun Zeng and Jiejin Cai and Huaqiang Yin and Xingtuan Yang and Tadashi Watanabe",
keywords = "Bubble behavior, Condensation, OpenFOAM, CLSVOF method",
abstract = "The single condensing bubble behavior in subcooled flow has been numerical investigated using the open source code OpenFOAM. A coupled Level Set (LS) and Volume of Fluid (VOF) method (CLSVOF) model with a phase change model for condensation was developed and implemented in the code. The simulated results were firstly compared with the experimental results, they were in great agreements, and thus the simulation model was validated. The validated numerical model was then used to analyze the condensing bubble deformation, bubble lifetime, bubble size history, condensate Nusselt number and other interesting parameters with different variables in subcooled flow. The numerical results indicated that the initial bubble size, subcooling of liquid and system pressure play an important role to influence the condensing bubble behaviors significantly and bubble will be pierced when the subcooling and initial diameter reach a certain value at the later condensing stage. The bubble diameter history and condensate Nusselt number were found in good agreement with the empirical correlation. The drag force coefficient was predicted well by introducing a reduced drag coefficient."
}
@article{YOUM2017177,
title = "Improved bug localization based on code change histories and bug reports",
journal = "Information and Software Technology",
volume = "82",
pages = "177 - 192",
year = "2017",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2016.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S0950584916303056",
author = "Klaus Changsun Youm and June Ahn and Eunseok Lee",
keywords = "Bug localization, Information retrieval, Bug reports, Stack traces, Code change history, Method analysis",
abstract = "Context
Several issues or defects in released software during the maintenance phase are reported to the development team. It is costly and time-consuming for developers to precisely localize bugs. Bug reports and the code change history are frequently used and provide information for identifying fault locations during the software maintenance phase.
Objective
It is difficult to standardize the style of bug reports written in natural languages to improve the accuracy of bug localization. The objective of this paper is to propose an effective information retrieval-based bug localization method to find suspicious files and methods for resolving bugs.
Method
In this paper, we propose a novel information retrieval-based bug localization approach, termed Bug Localization using Integrated Analysis (BLIA). Our proposed BLIA integrates analyzed data by utilizing texts, stack traces and comments in bug reports, structured information of source files, and the source code change history. We improved the granularity of bug localization from the file level to the method level by extending previous bug repository data.
Results
We evaluated the effectiveness of our approach based on experiments using three open-source projects, namely AspectJ, SWT, and ZXing. In terms of the mean average precision, on average our approach improves the metric of BugLocator, BLUiR, BRTracer, AmaLgam and the preliminary version of BLIA by 54%, 42%, 30%, 25% and 15%, respectively, at the file level of bug localization.
Conclusion
Compared with prior tools, the results showed that BLIA outperforms these other methods. We analyzed the influence of each score of BLIA from various combinations based on the analyzed information. Our proposed enhancement significantly improved the accuracy. To improve the granularity level of bug localization, a new approach at the method level is proposed and its potential is evaluated."
}
@article{JIMBOREAN20132575,
title = "Dynamic and Speculative Polyhedral Parallelization of Loop Nests Using Binary Code Patterns",
journal = "Procedia Computer Science",
volume = "18",
pages = "2575 - 2578",
year = "2013",
note = "2013 International Conference on Computational Science",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2013.05.443",
url = "http://www.sciencedirect.com/science/article/pii/S1877050913005863",
author = "Alexandra Jimborean and Philippe Clauss and Jean-François Dollinger and Vincent Loechner and Juan Manuel Martinez Caamaño",
keywords = "Dynamic optimizations, speculative parallelization, Binary code patterns, Templates, Polyhedral model, Runtime",
abstract = "Speculative parallelization is a classic strategy for automatically parallelizing codes that cannot be handled at compile-time due to the use of dynamic data and control structures. Another motivation of being speculative is to adapt the code to the current execution context, by selecting at run-time an efficient parallel schedule. However, since this parallelization scheme requires on-the-fly semantics verification, it is in general difficult to perform advanced transformations for optimization and parallelism extraction. We propose a framework dedicated to speculative parallelization of scientific nested loop kernels, able to transform the code at runtime by re-scheduling the iterations to exhibit parallelism and data locality. The run-time process includes a transformation selection guided by profiling phases on short samples, using an instrumented version of the code. During this phase, the accessed memory addresses are interpolated to build a predictor of the forthcoming accesses. The collected addresses are also used to compute on-the-fly dependence distance vectors by tracking accesses to common addresses. Interpolating functions and distance vectors are then employed in dynamic dependence analysis and in selecting a parallelizing transformation that, if the prediction is correct, does not induce any rollback during execution. In order to ensure that the rollback time overhead stays low, the code is executed in successive slices of the outermost original loop of the nest. Each slice can be either a parallelized version, a sequential original version, or an instrumented version. Moreover, such slicing of the execution provides the opportunity of transforming differently the code to adapt to the observed execution phases. Parallel code generation is achieved almost at no cost by using binary code patterns that are generated at compile-time and that are simply patched at run-time to result in the transformed code. The framework has been implemented with extensions of the LLVM compiler and an x86-64 runtime system. Significant speed-ups are shown on a set of benchmarks that could not have been handled efficiently by a compiler."
}
@article{OEDA2017614,
title = "Log-Data Clustering Analysis for Dropout Prediction in Beginner Programming Classes",
journal = "Procedia Computer Science",
volume = "112",
pages = "614 - 621",
year = "2017",
note = "Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2017.08.088",
url = "http://www.sciencedirect.com/science/article/pii/S1877050917314321",
author = "Shinichi Oeda and Genki Hashimoto",
keywords = "Educational data mining, Dropout prediction, Dynamic time warping, k-means++, UNIX command history",
abstract = "Educational data mining (EDM) involves the application of data mining, machine learning, and statistics to information generated from an educational setting. In most school education, one teacher teaches many students. A periodic examination is used as a method to confirm that students have acquired skills. However, it is difficult to grasp the status of the student from each lesson, since examinations cannot be carried out easily. On the other hand, in programming classes, the students’ history of UNIX commands and source-code editing can be easily and automatically stored as log-data. Therefore, attempts have been made to estimate the student’s performance from this log-data, although their estimation accuracy is not high. In this research, we aim to extract those students who cannot keep up with programming lessons, rather than estimating the student’s performance from the log-data. Specifically, we propose a method for predicting dropouts using outlier detection to cluster data with unsupervised learning."
}
@article{GAITANI201533,
title = "Automated refactoring to the Null Object design pattern",
journal = "Information and Software Technology",
volume = "59",
pages = "33 - 52",
year = "2015",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2014.10.010",
url = "http://www.sciencedirect.com/science/article/pii/S0950584914002389",
author = "Maria Anna G. Gaitani and Vassilis E. Zafeiris and N.A. Diamantidis and E.A. Giakoumakis",
keywords = "Refactoring, Design patterns, Null Object, Optional fields, Null checks",
abstract = "Context
Null-checking conditionals are a straightforward solution against null dereferences. However, their frequent repetition is considered a sign of poor program design, since they introduce source code duplication and complexity that impacts code comprehension and maintenance. The Null Object design pattern enables the replacement of null-checking conditionals with polymorphic method invocations that are bound, at runtime, to either a real object or a Null Object.
Objective
This work proposes a novel method for automated refactoring to Null Object that eliminates null-checking conditionals associated with optional class fields, i.e., fields that are not initialized in all class instantiations and, thus, their usage needs to be guarded in order to avoid null dereferences.
Method
We introduce an algorithm for automated discovery of refactoring opportunities to Null Object. Moreover, we specify the source code transformation procedure and an extensive set of refactoring preconditions for safely refactoring an optional field and its associated null-checking conditionals to the Null Object design pattern. The method is implemented as an Eclipse plug-in and is evaluated on a set of open source Java projects.
Results
Several refactoring candidates are discovered in the projects used in the evaluation and their refactoring lead to improvement of the cyclomatic complexity of the affected classes. The successful execution of the projects’ test suites, on their refactored versions, provides empirical evidence on the soundness of the proposed source code transformation. Runtime performance results highlight the potential for applying our method to a wide range of project sizes.
Conclusion
Our method automates the elimination of null-checking conditionals through refactoring to the Null Object design pattern. It contributes to improvement of the cyclomatic complexity of classes with optional fields. The runtime processing overhead of applying our method is limited and allows its integration to the programmer’s routine code analysis activities."
}
@article{RUBBO20131157,
title = "Inference rules for generic code migration of aspect-oriented programs",
journal = "Science of Computer Programming",
volume = "78",
number = "8",
pages = "1157 - 1175",
year = "2013",
note = "Special section on software evolution, adaptability, and maintenance & Special section on the Brazilian Symposium on Programming Languages",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2012.09.004",
url = "http://www.sciencedirect.com/science/article/pii/S0167642312001694",
author = "Fernando Barden Rubbo and Eduardo Kessler Piveta and Daltro José Nunes",
keywords = "Parametric polymorphism, Code migration, Aspect-oriented programming",
abstract = "Several changes occurred in the AspectJ language to provide support for parametric polymorphism. Such changes aim to improve the source code type safety and to prepare the language to support generic code migration. Current approaches for this kind of migration focus only on object-oriented code. Therefore, they do not consider the use of aspects to encapsulate crosscutting concerns. We propose a collection of type constraint rules for the polymorphic version of AspectJ. These rules are used together with an existing constraint based algorithm to enable the conversion of non-generic code to add actual type parameters in both Java and AspectJ languages."
}
@article{CHEN2016116,
title = "Uncooperative gait recognition: Re-ranking based on sparse coding and multi-view hypergraph learning",
journal = "Pattern Recognition",
volume = "53",
pages = "116 - 129",
year = "2016",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2015.11.016",
url = "http://www.sciencedirect.com/science/article/pii/S0031320315004367",
author = "Xin Chen and Jiaming Xu",
keywords = "Uncooperative gait recognition, Sparse Coding, Hypergraph learning, Re-ranking",
abstract = "Gait is an important biometric which can operate from a distance without subject cooperation. However, it is easily affected by changes in covariate conditions (carrying, clothing, view angle, walking speed, random noise etc.). It is hard for training set to cover all conditions. Bipartite ranking model has achieved success in gait recognition without assumption of subject cooperation. We propose a multi-view hypergraph learning re-ranking (MHLRR) method by integrating multi-view hypergraph learning (MHL) with hypergraph-based re-ranking framework. Sparse coding re-ranking (SCRR) and MHLRR are integrated under the graph-based framework to get a model. We define it as the sparse coding multi-view hypergraph learning re-ranking (SCMHLRR) method, which makes our approach achieve higher recognition accuracy under a genuine uncooperative setting. Extensive experiments demonstrate that our approach drastically outperforms existing ranking based methods, achieving good increase in recognition rate under the most difficult uncooperative settings."
}
@article{MAUSOLFF2018236,
title = "Enhanced geometric capabilities for the transient analysis code T-ReX and its application to simulating TREAT experiments",
journal = "Progress in Nuclear Energy",
volume = "105",
pages = "236 - 246",
year = "2018",
issn = "0149-1970",
doi = "https://doi.org/10.1016/j.pnucene.2018.01.013",
url = "http://www.sciencedirect.com/science/article/pii/S0149197018300222",
author = "Zander Mausolff and Mark DeHart and Sedat Goluoglu",
keywords = "Transient, IQS method, Neutron transport, Time dependent, TREAT, T-ReX",
abstract = "Advances in computational architecture have prompted a resurgence in the simulation of reactor transients from first principles. Most codes are unable to simulate transient events with complex models, and require numerous approximations. The code T-ReX (Transient-Reactor eXperiment simulator), an extensive update to TDKENO, has been developed as a transient analysis tool with few geometric limitations, and minimal theoretical approximations. T-ReX achieves this by employing the Improved Quasi-Static (IQS) method to solve the time-dependent Boltzmann transport equation with explicit representation of delayed neutrons. The primary change in T-ReX relative to TDKENO is the incorporation of a modified version of the Monte Carlo code KENO-VI to calculate the flux shape and model the geometry of a problem. Using KENO-VI to model systems allows exact representation of the geometry. The changes to T-ReX are verified by comparison of solutions to computational benchmark problems found with a previous version of TDKENO that made use of KENO V.a, and several other codes with time-dependent capabilities. In addition, a three-dimensional KENO-VI model of the Transient Reactor Test Facility (TREAT) core is used in simulations of several temperature-limited transient experiments from the M8 Calibration series. T-ReX produces results that agree with benchmark problems and are in better agreement with TREAT experimental data than TDKENO."
}
@article{CHADHA20173,
title = "Facilitating the development of cross-platform software via automated code synthesis from web-based programming resources",
journal = "Computer Languages, Systems & Structures",
volume = "48",
pages = "3 - 19",
year = "2017",
note = "Special Issue on the 14th International Conference on Generative Programming: Concepts & Experiences (GPCE'15)",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2016.08.005",
url = "http://www.sciencedirect.com/science/article/pii/S1477842415300634",
author = "Sanchit Chadha and Antuan Byalik and Eli Tilevich and Alla Rozovskaya",
keywords = "Recommendation systems, Code synthesis, Mobile computing, Android, IOS, Java, Swift",
abstract = "When a mobile application is supported on multiple major platforms, its market penetration is maximized. Such cross-platform native applications essentially deliver the same core functionality, albeit within the conventions of each supported platform. Maintaining and evolving a cross-platform native application is tedious and error-prone, as each modification requires replicating the changes for each of the application׳s platform-specific variants. Syntax-directed source-to-source translation proves inadequate to alleviate the problem, as native API access is always domain-specific. In this paper, we present a novel approach—Native-2-Native—that uses program transformations performed on one platform to automatically synthesize equivalent code blocks to be used on another platform. When a programmer modifies the source version of an application, the changes are captured. Based on the changes, Native-2-Native identifies the semantic content of the source code block and formulates an appropriate query to search for the equivalent target code block using popular web-based programming resources. The discovered target code block is then presented to the programmer as an automatically synthesized target language source file for further fine-tuning and subsequent integration into the mobile application׳s target version. We evaluate the proposed method using common native resources, such as sensors, network access, and canonical data structures. We show that our approach can correctly synthesize more than 74% of iOS code from the provided Android source code and 91% of Android code from the provided iOS source code. The presented approach effectively automates the process of extracting the source code block׳s semantics and discovering existing target examples with the equivalent functionality, thus alleviating some of the most laborious and intellectually tiresome programming tasks in modern mobile development."
}
@article{UQUILLASGOMEZ201244,
title = "Ring: A unifying meta-model and infrastructure for Smalltalk source code analysis tools",
journal = "Computer Languages, Systems & Structures",
volume = "38",
number = "1",
pages = "44 - 60",
year = "2012",
note = "SMALLTALKS 2010",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2011.11.001",
url = "http://www.sciencedirect.com/science/article/pii/S1477842411000443",
author = "Verónica Uquillas Gómez and Stéphane Ducasse and Theo D'Hondt",
keywords = "Source code meta-model, Versioning, Refactoring, Monticello, Smalltalk",
abstract = "Source code management systems record different versions of code. Tool support can then compute deltas between versions. To ease version history analysis we need adequate models to represent source code entities. Now naturally the questions of their definition, the abstractions they use, and the APIs of such models are raised, especially in the context of a reflective system which already offers a model of its own structure. We believe that this problem is due to the lack of a powerful code meta-model as well as an infrastructure. In Smalltalk, often several source code meta-models coexist: the Smalltalk reflective API coexists with the one of the Refactoring engine or distributed versioning system such as Monticello or Store. While having specific meta-models is an adequate engineered solution, it multiplies meta-models and it requires more maintenance efforts (e.g., duplication of tests, transformation between models), and more importantly hinders navigation tool reuse when meta-models do not offer polymorphic APIs. As a first step to provide an infrastructure to support history analysis, this article presents Ring, a unifying source code meta-model that can be used to support several activities and proposes a unified and layered approach to be the foundation for building an infrastructure for version and stream of change analyses. We re-implemented three tools based on Ring to show that it can be used as the underlying meta-model for remote and off-image browsing, scoping refactoring, and visualizing and analyzing changes. As a future work and based on Ring we will build a new generation of history analysis tools."
}
@article{HADI2013544,
title = "A Support System for Generating SCORM Compliant Open Source Software Usage Manuals",
journal = "Procedia Computer Science",
volume = "22",
pages = "544 - 550",
year = "2013",
note = "17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2013.09.134",
url = "http://www.sciencedirect.com/science/article/pii/S1877050913009277",
author = "Akhmad Syaikhul Hadi and Takashi Yukawa and Yukikazu Murakami",
keywords = "open source software, installation manual, SCORM, learning management system",
abstract = "Open Source Software (OSS) is software whose source code that is open to the public through the Internet. Currently, OSS is widely used in many aspects of IT society. Because OSS development is community based, unlike commercial software, the lack of good documentation or the maintenance of manuals is one of the main problems of using OSS. Due to its rapid development, OSS manuals become easily obsolete. Moreover, the installation or the usage varies depending on the operating system. To solve the documentation problems, Murakami et al. proposed a method of automatically generating a web manual for installing an OSS by editing the log information recorded during the installation process. Unfortunately, the web manual generated by this system was not suitable for wide use in learning management systems. Therefore, this paper extends the sys- tem by Murakami et al. to one with the ability to deliver an automatically generated Web manual on an e-learning management system, modify the content of the manual, and skip unnecessary information in the learning process."
}
@article{DIGIULIO20141,
title = "RNA editing and modifications of RNAs might have favoured the evolution of the triplet genetic code from an ennuplet code",
journal = "Journal of Theoretical Biology",
volume = "359",
pages = "1 - 5",
year = "2014",
issn = "0022-5193",
doi = "https://doi.org/10.1016/j.jtbi.2014.05.037",
url = "http://www.sciencedirect.com/science/article/pii/S0022519314003208",
author = "Massimo Di Giulio and Marco Moracci and Beatrice Cobucci-Ponzano",
keywords = "Molecular fossils, tmRNA, Ribosome frameshifting, Genetic code origin",
abstract = "Here we suggest that the origin of the genetic code, that is to say, the birth of first mRNAs has been triggered by means of a widespread modification of all RNAs (proto-mRNAs and proto-tRNAs), as today observed in the RNA editing and in post-transcriptional modifications of RNAs, which are considered as fossils of this evolutionary stage of the genetic code origin. We consider also that other mechanisms, such as the trans-translation and ribosome frameshifting, could have favoured the transition from an ennuplet code to a triplet code. Therefore, according to our hypothesis all these mechanisms would be reflexive of this period of the evolutionary history of the genetic code."
}
@article{UQUILLASGOMEZ201484,
title = "Supporting streams of changes during branch integration",
journal = "Science of Computer Programming",
volume = "96",
pages = "84 - 106",
year = "2014",
note = "Special issue on Advances in Smalltalk based Systems",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2014.07.012",
url = "http://www.sciencedirect.com/science/article/pii/S0167642314003384",
author = "Verónica Uquillas Gómez and Stéphane Ducasse and Andy Kellens",
keywords = "Branch, Source code changes, Stream of changes, Change dependencies, Merge",
abstract = "When developing large applications, integrators face the problem of integrating changes between branches or forks. While version control systems provide support for merging changes, this support is mostly text-based, and does not take the program entities into account. Furthermore, there exists no support for assessing which other changes a particular change depends on have to be integrated. Consequently, integrators are left to perform a manual and tedious comparison of the changes within the sequence of their branch and to successfully integrate them. In this paper, we present an approach that analyzes changes within a sequence of changes (stream of changes): such analysis identifies and characterizes dependencies between the changes. The approach identifies changes as autonomous, only used by others, only using other changes, or both. Such a characterization aims at easing the integrator's work. In addition, the approach supports important queries that an integrator otherwise has to perform manually. We applied the approach to a stream of changes representing 5 years of development work on an open-source project and report our experiences."
}
@article{GODBOLEY201861,
title = "Scaling modified condition/decision coverage using distributed concolic testing for Java programs",
journal = "Computer Standards & Interfaces",
volume = "59",
pages = "61 - 86",
year = "2018",
issn = "0920-5489",
doi = "https://doi.org/10.1016/j.csi.2018.02.005",
url = "http://www.sciencedirect.com/science/article/pii/S0920548917301988",
author = "Sangharatna Godboley and Arpita Dutta and Durga Prasad Mohapatra and Rajib Mall",
keywords = "Java concolic testing, Modified Condition/Decision Coverage, Code transformation",
abstract = "Object-Oriented languages such as Java language introduce advantageous features which overcome the demerits of procedural languages to some extent. Therefore, Java language is now going to be used by the industries to develop their critical safety system software products. In this paper, we propose some code transformation methodologies, which are implemented in Java language to test Java written code. We apply Java Distributed Concolic testing technique to improve the code coverage, which is more powerful than non-distributed concolic testing in terms of speed of test case generation. We develop a Java coverage analyzer according to the test cases produced by Java distributed concolic testers. This version of the MC/DC analyzer is more powerful than that of procedural languages. Our core idea is to integrate the existing and developed modules to produce a single tool for measuring MC/DC score. This novel idea automates the flow of testing 100%. Our experimental results present different scenarios, and suggest the stronger one. On an average, for forty-five Java programs using three nodes in the client–server architecture, we achieved higher MC/DC score."
}
@article{SCHUETRUMPF2018211,
title = "The TDHF code Sky3D version 1.1",
journal = "Computer Physics Communications",
volume = "229",
pages = "211 - 213",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2018.03.012",
url = "http://www.sciencedirect.com/science/article/pii/S0010465518300845",
author = "B. Schuetrumpf and P.-G. Reinhard and P.D. Stevenson and A.S. Umar and J.A. Maruhn",
keywords = "Hartree–Fock, BCS, Density-functional theory, Skyrme energy functional, Giant resonances, Heavy-ion collisions",
abstract = "The nuclear mean-field model based on Skyrme forces or related density functionals has found widespread application to the description of nuclear ground states, collective vibrational excitations, and heavy-ion collisions. The code Sky3D solves the static or dynamic equations on a three-dimensional Cartesian mesh with isolated or periodic boundary conditions and no further symmetry assumptions. Pairing can be included in the BCS approximation for the static case. The code is implemented with a view to allow easy modifications for including additional physics or special analysis of the results.
New version program summary
Program title: Sky3D Program Files doi:http://dx.doi.org/10.17632/vzbrzvyrn4.1 Licensing provisions: GPLv3 Programming language: Fortran 90. The OpenMP version requires a relatively recent compiler; it was found to work using gfortran 4.6.2 or later and the Intel compiler version 12 or later. Journal reference of previous version: J. A. Maruhn, P.-G. Reinhard, P. D. Stevenson, and A. S. Umar, “The TDHF Code Sky3D”, Comp. Phys. Comm. 185, 2195 (2014). Does the new version supersede the previous version?: Yes. Nature of problem: The time-dependent Hartree–Fock equations can be used to simulate nuclear vibrations and collisions between nuclei for low energies. This code implements the equations based on a Skyrme energy functional and also allows the determination of the ground-state structure of nuclei through the static version of the equations. For the case of vibrations the principal aim is to calculate the excitation spectra by Fourier-analyzing the time dependence of suitable observables. In collisions, the formation of a neck between nuclei, the dissipation of energy from collective motion, processes like charge transfer and the approach to fusion are of principal interest. Solution method: The nucleonic wave function spinors are represented on a three-dimensional Cartesian mesh with no further symmetry restrictions. The boundary conditions are always periodic for the wave functions, while the Coulomb potential can also be calculated for an isolated charge distribution. All spatial derivatives are evaluated using the finite Fourier transform method. The code solves the static Hartree–Fock equations with a damped gradient iteration method and the time-dependent Hartree–Fock equations with an expansion of the time-development operator. Any number of initial nuclei can be placed into the mesh with arbitrary positions and initial velocities. Reasons for the new version: A few bugs were fixed and a number of enhancements added concerning faster convergence, better stability, and more sophisticated analysis of some results. Summary of revisions: The following is a brief summary. A more complete documentation can be found as update.pdf in the Documentation subdirectory. New documentation: It was decided to switch the documentation to using the Doxygen system (available from www.doxygen.org), which can generate the documentation in a variety of formats. To generate the documentation, go into the Doc-doxygen subdirectory and execute make html, make latex, or make all to produce the corresponding version or both of them. The documentation inserted into the source files accounts for most of the formal changes in them. The general documentation is also updated and present as “Documentation.pdf”. Bug fixes: 1.In the force database forces.data two digits were interchanged in the definition of SLy4d, leading to wrong results for that force.2.If a restart is done for a two-body collision, the code changed the number of fragments to nof =1. The restart is then initialized like a single-nucleus case with nof =1. But two-body analysis was activated only for nof =1 such that it was absent after restart.3.In the time-dependent mode, the wave functions were only save at intervals of mprint and mrest, respectively. If a calculation stops because of reaching the final distance or fulfilling the convergence criterion, this may lead to a loss of information, so that now both are done also in this event before the job finishes.4.The external field parameters were calculated directly from the input in getin_external. Since this is called before the fragment initialization is done, coefficients depending on proton or neutron number will not be calculated correctly. For this reason, the calculation of these coefficients is separated into a new routine init_external, which is called directly before the dynamic calculation starts. Array allocation: It turned out that having the working arrays as automatic variables could cause problems, as they are allocated on the stack and the proper stack size must be calculated. Therefore in all cases where a larger array is concerned, it is now changed to ALLOCATABLE and allocated and deallocated as necessary. Elimination of “guru” mode of FFTW3 While the guru mode as defined in the FFTW3 package (see fftw.org) offers an elegant formulation of complicated multidimensional transforms, it is not implemented in some support libraries like the Intel® MKL. There is not much loss in speed when this is replaced by standard transforms with some explicit loops added where necessary. This affects the wave function transforms in the y and z direction. Enhancement of the makefile In the previous version there were several versions of the makefile, which had to be edited by hand to use different compilers. This was reformulated using a more flexible file with various targets predefined. Thus to generate the executable code, it is sufficient to execute “maketarget” in the Code subdirectory, where target is one of the following: •seq : simple sequential version with the gfortran compiler.•ifort, ifort_seq : sequential version using the Intel compiler.•omp and ifort_omp produce the OpenMP version for the gfortran and Intel compiler, respectively.•mpi : MPI version, which uses the compiler mpif90.•mpi-omp : MPI version also using OpenMP on each node.•debug, seq_debug, omp_debug, mpi_debug : enable debugging mode for these cases. The first three use the gfortran compiler.•clean : removes the generated object and module files.•clean-exec : same as clean but removes the executable files as well.The generated executable files are called sky3d.seq, sky3d.ifort.seq, sky3d.mpi, sky3d.omp, sky3d.ifort.omp, and sky3d.mpi-omp, which should be self-explanatory. Thus several versions may be kept in the code directory, but a make clean should be done before producing a new version to make sure the object and module files are correct. Skyrme-force compatibility for static restart: the code normally checks that the Skyrme forces for all the input wave functions agree. It may be useful, however, to initialize a static calculation from results for a different Skyrme force. Therefore the consistency check was eliminated for the static case. Acceleration of the static calculations: The basic parameters for the static iterations are (see Eq. 12 of the original paper) x0 (variable x0dmp), which determines the size of the gradient step, and E0 (variable e0dmp) for the energy damping. These were read in and never changed throughout the calculation, except possibly through a restart. This can cause slow convergence, so that a method was developed to change x0dmp during the iterations. The value from the input is now regarded as the minimum allowed one and saved in x0dmpmin. At the start of the iterations, however, x0dmp is multiplied by 3 to attempt a faster convergence. The change in x0dmp is then implemented by comparing the HF energy ehf and the fluctuations efluct1 and efluct2 to the previous values saved in the variables ehfprev, efluct1prev, and efluct2prev. If the energy decreases or one of the fluctuations decreases by a factor of less than 1−10−5, x0dmp is increased by a factor 1.005 to further speed up convergence. If none of these conditions holds, it is assumed that the step was too large and x0dmp is reduced by a factor 0.8, but is never allowed to fall below x0dmpmin. This whole process is turned on only if the input variable tvaryx_0 in the namelist “static” is .TRUE. The default value is .FALSE. A speedup up to a factor of 3 has been observed. External field expectation value This value, which is printed in the file external.res, was calculated from the spatial field including the (time-independent) amplitude amplq0. The temporal Fourier transform then becomes quadratic in the amplitude, as the fluctuations in the density also grow linearly in amplq0 (provided the perturbation is not strong enough to take it into the nonlinear regime). This may be confusing and we therefore divided the expectation value by this factor. Note that if the external field is composed of a mixture of different multipoles (not coded presently), an overall scale factor should instead be used. Enhanced two-body analysis: the analysis of the final two-body quantities after breakup included directly in the code was very simplified and actually it was superfluous to do this so frequently. This is replaced by a much more thorough analysis, including determination of the internal angular momenta of the fragments and of a quite accurate Coulomb energy. It is done only when the final separation is reached, while a simple determination of whether the fragments have separated and, if so, what their distance is, is performed every time step. Diagonalization In the original program the diagonalization of the Hamiltonian in the subroutine diagstep was carried out employing an eigenvalue decomposition using the LAPACK routine ZHBEVD which is optimized for banded matrices. This routine is replaced in the update by the routine ZHEEVD which is optimized for general hermitian matrices. This change should result in a moderate speed up for very large calculations. Furthermore the array unitary, previously a nstmax×nstmax array has been reduced to a nlin×nlin array, where nlin is the number of wave functions of either neutrons or protons. This array is now used as input and output for ZHEEVD. New formulation of the spin–orbit term: The action of the spin–orbit term has been corrected to comply with a strictly variational form. Starting from the spin–orbit energy (1)Els=tls∫d3rJ→⋅∇ρ,we obtain by variation with respect to the s.p. wavefunction ψ∗ the spin–orbit term in the mean field in the symmetrized form (2)hˆlsψ=i2W→⋅(σ→×∇)ψ+σ→⋅(∇×(W→ψ))where W→=tls∇ρ. In the previous version of the code, this term was simplified by applying the product rule for the ∇ operator yielding (3)i2W→⋅(σ→×∇)ψ+σ→⋅(∇×(W→ψ))=iW→⋅(σ→×∇)ψ.Closer inspection reveals that the product rule is not perfectly fulfilled if the ∇ operator is evaluated with finite Fourier transformation as inevitably done in the grid representation of the code. It turned out that this slight mismatch can accumulate to instabilities in TDHF runs over long times. Thus the variationally correct form (2) has been implemented now, although it leads to slightly longer running times. Supplementary material: Extensive documentation and a number of utility programs to analyze the results and prepare them for graphics output using the Silo library (http://wci.llnl.gov/simulation/computer-codes/silo) for use in VisIT [1] or Paraview (https://www.paraview.org). The code can serve as a template for interfacing to other database or graphics systems. External routines/libraries: LAPACK, FFTW3. Restrictions: The reliability of the mean-field approximation is limited by the absence of hard nucleon–nucleon collisions. This limits the scope of applications to collision energies about a few MeV per nucleon above the Coulomb barrier and to relatively short interaction times. Similarly, some of the missing time-odd terms in the implementation of the Skyrme interaction may restrict the applications to even–even nuclei. Unusual features: The possibility of periodic boundary conditions and the highly flexible initialization make the code also suitable for astrophysical nuclear-matter applications. Acknowledgments This work was supported by DOE under contract numbers DE-SC0013847, DE-NA0002847, DE-SC0013365, and DE-SC0008511, by BMBF under contract number 05P15RDFN1, and by UK STFC under grant number ST/P005314/1. References[1]H. Childs, E. Brugger, B. Whitlock, J. Meredith, S. Ahern, D. Pugmire, K. Biagas, M. Miller, C. Harrison, G. H. Weber, H. Krishnan, T. Fogal, A. Sanderson, C. Garth, E. W. Bethel, D. Camp, O. Rübel, M. Durant, J. M. Favre, P. Navrátil, VisIt: An End-User Tool For Visualizing and Analyzing Very Large Data, in: High Performance Visualization–Enabling Extreme-Scale Scientific Insight, 2012, pp. 357–372."
}
@article{LAMBERT2012711,
title = "TIM, a ray-tracing program for METATOY research and its dissemination",
journal = "Computer Physics Communications",
volume = "183",
number = "3",
pages = "711 - 732",
year = "2012",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2011.11.011",
url = "http://www.sciencedirect.com/science/article/pii/S0010465511003730",
author = "Dean Lambert and Alasdair C. Hamilton and George Constable and Harsh Snehanshu and Sharvil Talati and Johannes Courtial",
keywords = "Ray tracing, Geometrical optics, METATOYs",
abstract = "TIM (The Interactive METATOY) is a ray-tracing program specifically tailored towards our research in METATOYs, which are optical components that appear to be able to create wave-optically forbidden light-ray fields. For this reason, TIM possesses features not found in other ray-tracing programs. TIM can either be used interactively or by modifying the openly available source code; in both cases, it can easily be run as an applet embedded in a web page. Here we describe the basic structure of TIMʼs source code and how to extend it, and we give examples of how we have used TIM in our own research.
Program summary
Program title: TIM Catalogue identifier: AEKY_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEKY_v1_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: GNU General Public License No. of lines in distributed program, including test data, etc.: 124 478 No. of bytes in distributed program, including test data, etc.: 4 120 052 Distribution format: tar.gz Programming language: Java Computer: Any computer capable of running the Java Virtual Machine (JVM) 1.6 Operating system: Any; developed under Mac OS X Version 10.6 RAM: Typically 145 MB (interactive version running under Mac OS X Version 10.6) Classification: 14, 18 External routines: JAMA [1] (source code included) Nature of problem: Visualisation of scenes that include scene objects that create wave-optically forbidden light-ray fields. Solution method: Ray tracing. Unusual features: Specifically designed to visualise wave-optically forbidden light-ray fields; can visualise ray trajectories; can visualise geometric optic transformations; can create anaglyphs (for viewing with coloured “3D glasses”) and random-dot autostereograms of the scene; integrable into web pages. Running time: Problem-dependent; typically seconds for a simple scene. References:[1]JAMA: A Java matrix package, http://math.nist.gov/javanumerics/jama/."
}
@article{REBELO20131137,
title = "Optimizing generated aspect-oriented assertion checking code for JML using program transformations: An empirical study",
journal = "Science of Computer Programming",
volume = "78",
number = "8",
pages = "1137 - 1156",
year = "2013",
note = "Special section on software evolution, adaptability, and maintenance & Special section on the Brazilian Symposium on Programming Languages",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2012.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S0167642312001682",
author = "Henrique Rebêlo and Ricardo Lima and Gary T. Leavens and Márcio Cornélio and Alexandre Mota and César Oliveira",
keywords = "Aspect-oriented programming, Program transformation, JML",
abstract = "The AspectJ JML compiler (ajmlc) explores aspect-oriented programming (AOP) mechanisms to implement JML specifications, such as pre- and postconditions, and enforce them during runtime. This compiler was created to improve source-code modularity. Some experiments were conducted to evaluate the performance of the code generated through ajmlc. Results demonstrated that the strategy of adopting AOP to implement JML specifications is very promising. However, there is still a need for optimization of the generated code’s bytecode size and running time. This paper presents a catalog of transformations which represent the optimizations implemented in the new optimized version of the ajmlc compiler. We employ such transformations to reduce the bytecode size and running time of the code generated through the ajmlc compiler. Aiming at demonstrating the impact of such transformation on the code quality, we conduct an empirical study using four applications in optimized and non-optimized versions generated by ajmlc. We show that our AOP transformations provide a significant improvement, regarding bytecode size and running time."
}
@article{CHEN2018149,
title = "Collaborative multiview hashing",
journal = "Pattern Recognition",
volume = "75",
pages = "149 - 160",
year = "2018",
note = "Distance Metric Learning for Pattern Recognition",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2017.02.026",
url = "http://www.sciencedirect.com/science/article/pii/S0031320317300894",
author = "Zhixiang Chen and Jie Zhou",
keywords = "Multiview hashing, View collaboration, Nonlinear hashing, Binary code",
abstract = "In this paper, we propose a collaborative multiview hashing (CMH) approach to incorporate multiview representations into the binary code learning for scalable visual retrieval. Unlike most existing multiview hashing methods which learn linear projections to preserve the fused similarity relationship across different views in unsupervised manner, we employ the nonlinear hashing functions as the projection in each view and exploit the diverse information of multiview representations by utilizing the collaboration between view representations and the correlation between the view representations and the semantic labels. Specifically, the binary codes in each view are constrained to be predictive to each other to exploit the collaboration between the descriptors in different views that describe the same sample. Furthermore, the binary codes in all views are enforced to preserve the semantic relationship between data samples. The hashing functions are implemented in the form of multi-layer neural network with nonlinear transformations at each layer and trained with both the view collaboration and semantic preserving constraints on the outputs. Experimental results on two datasets validate the superiority of the proposed approach in comparison with several state-of-the-art hashing methods."
}
@article{LIANNEHUIBERS2018,
title = "Conversion of STOPP/START version 2 into coded algorithms for software implementation: a multidisciplinary consensus procedure",
journal = "International Journal of Medical Informatics",
year = "2018",
issn = "1386-5056",
doi = "https://doi.org/10.1016/j.ijmedinf.2018.12.010",
url = "http://www.sciencedirect.com/science/article/pii/S1386505618308190",
author = "Corlina J.A. (Lianne) Huibers and Bastiaan T.G.M. Sallevelt and Dominique A. de Groot and Maarten J. Boer and Jos P.C.M. van Campen and Cathelijn J. Davids and Jacqueline G. Hugtenburg and Annemieke M.A. Vermeulen Windsant-Van den Tweel and Hein P.J. van Hout and Rob J. van Marum and Michiel C. Meulendijk",
keywords = "START STOPP criteria, computer decision support system, algorithm, inappropriate prescribing, explicit screening tool, international classification of medication and disease",
abstract = "Background
The rapid digitalization of medical practice has attracted growing interest in developing software applications for clinical guidelines and explicit screening tools to detect potentially inappropriate prescribing, such as STOPP/START criteria. The aim of the current study was to develop and provide logically unambiguous algorithms of STOPP/START criteria version 2, encoded with international disease and medication classification codes, to facilitate the development of software applications for multiple purposes.
Methods
A four round multidisciplinary consensus and validation procedure was conducted to develop implementable coded algorithms for software applications of STOPP/START criteria version 2, based on ICD, ICPC, LOINC and ATC classification databases.
Results
Consensus was reached for all 34 START criteria and 76 out of 80 STOPP criteria. The resulting 110 algorithms, modeled as inference rules in decision tables, are provided as supplementary data.
Conclusion
This is the first study providing implementable algorithms for software applications based on STOPP/START version 2, validated in a computer decision support system. These algorithms could serve as a template for applying STOPP/START criteria version 2 to any software application, allowing for adaptations of the included ICD, ICPC and ATC codes and changing the cut-off levels for laboratory measurements to match local guidelines or clinical expertise."
}
@article{ZHAO201737,
title = "Towards an understanding of change types in bug fixing code",
journal = "Information and Software Technology",
volume = "86",
pages = "37 - 53",
year = "2017",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2017.02.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584917301313",
author = "Yangyang Zhao and Hareton Leung and Yibiao Yang and Yuming Zhou and Baowen Xu",
keywords = "Change, Bug fixing code, Empirical study, Understanding, Software quality",
abstract = "Context
As developing high quality software becomes increasingly challenging because of the explosive growth of scale and complexity, bugs become inevitable in software systems. The knowledge of bugs will naturally guide software development and hence improve software quality. As changes in bug fixing code provide essential insights into the original bugs, analyzing change types is an intuitive and effective way to understand the characteristics of bugs.
Objective
In this work, we conduct a thorough empirical study to investigate the characteristics of change types in bug fixing code.
Method
We first propose a new change classification scheme with 5 change types and 9 change subtypes. We then develop an automatic classification tool CTforC to categorize changes. To gain deeper insights into change types, we perform our empirical study based on three questions from three perspectives, i.e. across project, across domain and across version.
Results
Based on 17 versions of 11 systems with thousands of faulty functions, we find that: (1) across project: the frequencies of change subtypes are significantly similar across most studied projects; interface related code changes are the most frequent bug-fixing changes (74.6% on average); most of faulty functions (65.2% on average) in studied projects are finally fixed by only one or two change subtypes; function call statements are likely to be changed together with assignment statements or branch statements; (2) across domain: the frequencies of change subtypes share similar trends across studied domains; changes on function call, assignment, and branch statements are often the three most frequent changes in studied domains; and (3) across version: change subtypes occur with similar frequencies across studied versions, and the most common subtype pairs tend to be same.
Conclusion
Our experimental results improve the understanding of changes in bug fixing code and hence the understanding of the characteristics of bugs."
}
@article{HSIAO2018177,
title = "The History and Impact of Molecular Coding Changes on Coverage and Reimbursement of Molecular Diagnostic Tests: Transition from Stacking Codes to the Current Molecular Code Set Including Genomic Sequencing Procedures",
journal = "The Journal of Molecular Diagnostics",
volume = "20",
number = "2",
pages = "177 - 183",
year = "2018",
issn = "1525-1578",
doi = "https://doi.org/10.1016/j.jmoldx.2017.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S1525157817304233",
author = "Susan J. Hsiao and Mahesh M. Mansukhani and Melissa C. Carter and Anthony N. Sireci",
abstract = "Changes in coding and coverage generate an uncertain reimbursement environment for molecular pathology laboratories. We analyzed our experience with two representative molecular oncology tests: a T-cell receptor (TCR) β rearrangement test and a large (467-gene) cancer next-generation sequencing panel, the Columbia Combined Cancer Panel (CCCP). Before 2013, the TCR β test was coded using stacked current procedural terminology codes and subsequently transitioned to a tier 1 code. CCCP was coded using a combination of tier 1 and 2 codes until 2015, when a new Genomic Sequencing Procedure code was adopted. A decrease in reimbursement of 61% was observed for the TCR β test on moving from stacking to tier 1 codes. No initial increase in total rejection rate was observed, but a subsequent increase in rejection rates in 2015 and 2016 was noted. The CCCP test showed a similar decrease (48%) in reimbursement after adoption of the new Genomic Sequencing Procedure code and was accompanied by a sharp increase in rejection rates both on implementation of the new code and over time. Changes in coding can result in substantial decreases in reimbursement. This may be a barrier to patient access because of the high cost of molecular diagnostics. Revisions to the molecular code set will continue. These findings help laboratories and manufacturers prepare for the financial impact and advocate appropriately."
}
@article{CHANDRA2013169,
title = "Source Code Editing Evaluator for Learning Programming",
journal = "Procedia Technology",
volume = "11",
pages = "169 - 175",
year = "2013",
note = "4th International Conference on Electrical Engineering and Informatics, ICEEI 2013",
issn = "2212-0173",
doi = "https://doi.org/10.1016/j.protcy.2013.12.177",
url = "http://www.sciencedirect.com/science/article/pii/S2212017313003319",
author = "Timotius Nugroho Chandra and Inggriani Liem",
keywords = "source code editor, programming course, programming best practice, integrated system, autograder, web application",
abstract = "Each semester, School of Electrical Engineering and Informatics (SEEI) ITB manages ±400 students taking programming courses with many examinations and exercises but can only provides limited feedback and facilities. The number of PCs are much less than the students. Although most students have personal computing devices, those devices have various specifications that create difficulties in evaluation. To tackle these issues, we provide an integrated system for learning programming that covers source code editing, compiling, execution, submission and evaluation of the process and the result. In this paper, we emphasize on source code editing environment named Doppel and Ganger (D&G). D&G is a web based application aimed for monitoring coding process by recording typing activities. It also supports online compilation and execution using an autograder. We have experimented programming practice using this environment and it has shown that D&G can significantly support the learning process of programming."
}
@article{CHOUDHARY201815,
title = "Empirical analysis of change metrics for software fault prediction",
journal = "Computers & Electrical Engineering",
volume = "67",
pages = "15 - 24",
year = "2018",
issn = "0045-7906",
doi = "https://doi.org/10.1016/j.compeleceng.2018.02.043",
url = "http://www.sciencedirect.com/science/article/pii/S0045790617336121",
author = "Garvit Rajesh Choudhary and Sandeep Kumar and Kuldeep Kumar and Alok Mishra and Cagatay Catal",
keywords = "Software fault prediction, Eclipse, Change log, Metrics, Software quality, Defect prediction",
abstract = "A quality assurance activity, known as software fault prediction, can reduce development costs and improve software quality. The objective of this study is to investigate change metrics in conjunction with code metrics to improve the performance of fault prediction models. Experimental studies are performed on different versions of Eclipse projects and change metrics are extracted from the GIT repositories. In addition to the existing change metrics, several new change metrics are defined and collected from the Eclipse project repository. Machine learning algorithms are applied in conjunction with the change and source code metrics to build fault prediction models. The classification model with new change metrics performs better than the models using existing change metrics. In this work, the experimental results demonstrate that change metrics have a positive impact on the performance of fault prediction models, and high-performance models can be built with several change metrics."
}
@article{CHEN2018174,
title = "Improving regression test efficiency with an awareness of refactoring changes",
journal = "Information and Software Technology",
volume = "103",
pages = "174 - 187",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.07.003",
url = "http://www.sciencedirect.com/science/article/pii/S095058491830137X",
author = "Zhiyuan Chen and Hai-Feng Guo and Myoungkyu Song",
keywords = "Software evolution, Fault localization, Change impact analysis, Test selection, Refactorings",
abstract = "Context. Developers often improve software quality through refactorings—the practice of behavior-preserving changes to existing code. Recent studies showed that, despite their awareness of tool support for automated refactorings, developers prefer manual refactorings. This practice can be often error-prone and increase testing cost. Objective. To address the problem, we present the Refactorings Investigation and Testing technique, called Rit. Rit improves the testing efficiency for validating refactoring changes and providing confidence that changed parts behave as intended. As testing is expensive for developers of high-assurance software, Rit reduces a considerable amount of its costs by only identifying dependent statements on a failure in each test and by detecting specific refactoring edits responsible for testing failures. Method. Our approach identifies refactorings by analyzing original and edited versions of a program. It then uses the semantic impact of a set of identified refactoring changes to detect tests whose behavior may have been affected and modified by refactoring edits. Given each failed asserts after running regression test suites, Rit helps developers focus their attention on logically related program statements by applying program slicing for minimizing each test. For debugging purposes, Rit determines specific failure-inducing refactoring edits, separating from other changes that only affect other asserts or tests. Results. We evaluated Rit on three open source projects, and found that Rit detected tests affected by refactorings with 80.9% accuracy on average. Furthermore, it identified and formed partitions relating program statements only dependent on failed asserts with 97.2% accuracy on average. Conclusion. Rit, which combines a refactoring reconstruction technique with change impact analysis to localize failure-inducing program edits, helps developers localize fault causes by focusing on refactoring changes as opposed to all the code fragments in the new version."
}
@article{LI2017236,
title = "R2PCAH: Hashing with two-fold randomness on principal projections",
journal = "Neurocomputing",
volume = "235",
pages = "236 - 244",
year = "2017",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2017.01.019",
url = "http://www.sciencedirect.com/science/article/pii/S0925231217300504",
author = "Peng Li and Peng Ren",
keywords = "Hashing, PCA, Random, Fast similarity search",
abstract = "Hashing based strategies have recently been widely used in fast similarity search on large scale datasets. Data-independent methods such as Locality Sensitive Hashing (LSH) usually adopt random projections as hash functions, with theoretical guarantees that the performance improves with the increasing code length. Thus they require relatively long codes, making them less effective than data-dependent methods. On the other hand, in many data-dependent hashing methods, Principal Component Analysis (PCA) is widely used to generate compact hash codes. However, PCA based methods tend not to be effective for generating long codes because projections with small variances may induce certain redundancy and noise. In order to address these deficiencies, we present a R2PCAH framework that conducts two-fold random transformations based on principal projections for hash code learning. Specifically, only the top PCA projections of the training data are extracted and two-fold random transformations, i.e. random rotations and random shifts are performed on the projected data to generate several pieces of component short codes. The multiple component short codes are then concatenated into one piece of long code. We observe that our method shares the advantages of both LSH and PCA based hashing methods. Extensive experiments demonstrate the effectiveness of the proposed method."
}
@article{KLONEK2015284,
title = "Coding interactions in Motivational Interviewing with computer-software: What are the advantages for process researchers?",
journal = "Computers in Human Behavior",
volume = "44",
pages = "284 - 292",
year = "2015",
issn = "0747-5632",
doi = "https://doi.org/10.1016/j.chb.2014.10.034",
url = "http://www.sciencedirect.com/science/article/pii/S0747563214005573",
author = "Florian E. Klonek and Vicenç Quera and Simone Kauffeld",
keywords = "Motivational Interviewing, Motivational Interviewing Treatment Integrity code, Software coding, Observational methods, Timed-event data, Behavioral slicing",
abstract = "Motivational Interviewing (MI) is an evidence-based behavior change intervention. The interactional change processes that make MI effective have been increasingly studied using observational coding schemes. We introduce an implementation of a software-supported MI coding scheme—the Motivational Interviewing Treatment Integrity code (MITI)—and discuss advantages for process researchers. Furthermore, we compared reliability of the software version with prior results of the paper version. A sample of 14 double-coded dyadic interactions showed good to excellent interrater reliabilities. We selected a second sample of 22 sessions to obtain convergent validity results of the software version: substantial correlations were obtained between the software instrument and the Rating Scales for the Assessment of Empathic Communication. Finally, we demonstrate how the software version can be used to test whether single code frequencies obtained by using intervals shorter than 20min (i.e., 5 or 10min) are accurate estimates of the respective code frequencies for the entire session (i.e., behavior slicing). Our results revealed that coding only a 10-min interval provides accurate estimates of the entire session. Our study demonstrates that the software implementation of the MITI is a reliable and valid instrument. We discuss advantages of the software version for process research in MI."
}
@article{DIEDEREN20161127,
title = "Adaptive Prediction Error Coding in the Human Midbrain and Striatum Facilitates Behavioral Adaptation and Learning Efficiency",
journal = "Neuron",
volume = "90",
number = "5",
pages = "1127 - 1138",
year = "2016",
issn = "0896-6273",
doi = "https://doi.org/10.1016/j.neuron.2016.04.019",
url = "http://www.sciencedirect.com/science/article/pii/S0896627316300782",
author = "Kelly M.J. Diederen and Tom Spencer and Martin D. Vestergaard and Paul C. Fletcher and Wolfram Schultz",
keywords = "reward, standard deviation, normalization, performance, fMRI",
abstract = "Summary
Effective error-driven learning benefits from scaling of prediction errors to reward variability. Such behavioral adaptation may be facilitated by neurons coding prediction errors relative to the standard deviation (SD) of reward distributions. To investigate this hypothesis, we required participants to predict the magnitude of upcoming reward drawn from distributions with different SDs. After each prediction, participants received a reward, yielding trial-by-trial prediction errors. In line with the notion of adaptive coding, BOLD response slopes in the Substantia Nigra/Ventral Tegmental Area (SN/VTA) and ventral striatum were steeper for prediction errors occurring in distributions with smaller SDs. SN/VTA adaptation was not instantaneous but developed across trials. Adaptive prediction error coding was paralleled by behavioral adaptation, as reflected by SD-dependent changes in learning rate. Crucially, increased SN/VTA and ventral striatal adaptation was related to improved task performance. These results suggest that adaptive coding facilitates behavioral adaptation and supports efficient learning."
}
@article{VANDERBAUWHEDE20181,
title = "Domain-specific acceleration and auto-parallelization of legacy scientific code in FORTRAN 77 using source-to-source compilation",
journal = "Computers & Fluids",
volume = "173",
pages = "1 - 5",
year = "2018",
issn = "0045-7930",
doi = "https://doi.org/10.1016/j.compfluid.2018.06.005",
url = "http://www.sciencedirect.com/science/article/pii/S0045793018302950",
author = "Wim Vanderbauwhede and Gavin Davidson",
keywords = "Fortran, GPGPU, OpenCL, Source-to-source compilation, Auto-parallelization, Acceleration",
abstract = "Massively parallel accelerators such as GPGPUs, manycores and FPGAs represent a powerful and affordable tool for scientists who look to speed up simulations of complex systems. However, porting code to such devices requires a detailed understanding of heterogeneous programming tools and effective strategies for parallelization. In this paper we present a source to source compilation approach with whole-program analysis to automatically transform single-threaded FORTRAN 77 legacy code into OpenCL-accelerated programs with parallelized kernels. The main contributions of our work are: (1) whole-source refactoring to allow any subroutine in the code to be offloaded to an accelerator. (2) Minimization of the data transfer between the host and the accelerator by eliminating redundant transfers. (3) Pragmatic auto-parallelization of the code to be offloaded to the accelerator by identification of parallelizable maps and reductions. We have validated the code transformation performance of the compiler on the NIST FORTRAN 78 test suite and several real-world codes: the Large Eddy Simulator for Urban Flows, a high-resolution turbulent flow model; the shallow water component of the ocean model Gmodel; the Linear Baroclinic Model, an atmospheric climate model and Flexpart-WRF, a particle dispersion simulator. The automatic parallelization component has been tested on as 2-D Shallow Water model (2DSW) and on the Large Eddy Simulator for Urban Flows (UFLES) and produces a complete OpenCL-enabled code base. The fully OpenCL-accelerated versions of the 2DSW and the UFLES are resp. 9x and 20x faster on GPU than the original code on CPU, in both cases this is the same performance as manually ported code."
}
@article{GUAN201619,
title = "Open source FreeRTOS as a case study in real-time operating system evolution",
journal = "Journal of Systems and Software",
volume = "118",
pages = "19 - 35",
year = "2016",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2016.04.063",
url = "http://www.sciencedirect.com/science/article/pii/S0164121216300383",
author = "Fei Guan and Long Peng and Luc Perneel and Martin Timmerman",
keywords = "Embedded system, FreeRTOS, Real-time OS, Software evolution",
abstract = "This paper studies the evolution of a real-time operating system, the open source FreeRTOS. We focus on the changes in real-time performance and behaviour over the last ten years. Six major release versions are benchmarked, presenting quantitative and qualitative development trends. We also use the available source code to discover the reasons for the changes. By analysing the results, we draw some conclusions related to this RTOS’s evolution which can be useful for the FreeRTOS group, other RTOS developments, and also RTOS users."
}
@article{LEE20171,
title = "How deep learning extracts and learns leaf features for plant classification",
journal = "Pattern Recognition",
volume = "71",
pages = "1 - 13",
year = "2017",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2017.05.015",
url = "http://www.sciencedirect.com/science/article/pii/S003132031730198X",
author = "Sue Han Lee and Chee Seng Chan and Simon Joseph Mayo and Paolo Remagnino",
keywords = "Plant recognition, Deep learning, Feature visualisation",
abstract = "Plant identification systems developed by computer vision researchers have helped botanists to recognize and identify unknown plant species more rapidly. Hitherto, numerous studies have focused on procedures or algorithms that maximize the use of leaf databases for plant predictive modeling, but this results in leaf features which are liable to change with different leaf data and feature extraction techniques. In this paper, we learn useful leaf features directly from the raw representations of input data using Convolutional Neural Networks (CNN), and gain intuition of the chosen features based on a Deconvolutional Network (DN) approach. We report somewhat unexpected results: (1) different orders of venation are the best representative features compared to those of outline shape, and (2) we observe multi-level representation in leaf data, demonstrating the hierarchical transformation of features from lower-level to higher-level abstraction, corresponding to species classes. We show that these findings fit with the hierarchical botanical definitions of leaf characters. Through these findings, we gained insights into the design of new hybrid feature extraction models which are able to further improve the discriminative power of plant classification systems. The source code and models are available at: https://github.com/cs-chan/Deep-Plant."
}
@article{GRZESIAK201831,
title = "Spiking signal processing: Principle and applications in control system",
journal = "Neurocomputing",
volume = "308",
pages = "31 - 48",
year = "2018",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2018.03.054",
url = "http://www.sciencedirect.com/science/article/pii/S0925231218304430",
author = "L.M. Grzesiak and V. Meganck",
keywords = "Spiking neuron, Firing rate, Geometric series, Spiking signal processing, Digital signal processing, Linear control system, Golden ratio",
abstract = "This paper introduces an innovative interpretation of spiking signal processing (SSP) and proposes applications in control system. Taking the firing rate as the coding principle employed by biological neurons, we have transformed continuous signals in geometric spiking series evolving over time. New fundamental SSP equations are presented and compared to classical digital signal processing (DSP). Application to the linear system analysis and control drive using spiking transformation is finally presented. This paper launches the theoretical SSP basis to investigate more deeply the geometric neural networks with learning ability."
}
@article{YAMASHITA2016459,
title = "Affine-transformation and 2D-projection invariant k-NN classification of handwritten characters via a new matching measure",
journal = "Pattern Recognition",
volume = "52",
pages = "459 - 470",
year = "2016",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2015.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S0031320315003696",
author = "Yukihiko Yamashita and Toru Wakahara",
keywords = "Matching, Affine transformation, Two-dimensional projection transformation, -nearest neighbors, Nearest-neighbor distance of equi-gradient direction",
abstract = "Pattern recognition based on matching remains important because it is a fundamental technique, it does not require a learning process, and the result of matching provides intuitive and geometrical information. Wakahara et al. proposed global affine transformation (GAT) correlation matching, which can compensate for affine transformations imposed on a pattern. GAT correlation matching with an acceleration method and a new matching measure, called the nearest-neighbor distance of equi-gradient direction (NNDEGD), achieved high performance in experiments using the MNIST database. The GAT matching measure was extended to a global projection transformation (GPT) matching measure to allow deformation by 2D projection transformations. The purpose of this paper is threefold. First, we develop an acceleration method for GPT correlation matching. Second, in order to improve recognition performance, we apply the curvature of edges in strokes to the matching measure. Curvature is often used as a feature of characters. However, in this paper, we use it as a weight in the NNDEGD. Third, to investigate the performance of the proposed methods, we apply them to image matching and recognition from the MNIST and the IPTP databases for k-nearest neighbors (k-NN). In the experiment with the MNIST database, the GPT correlation matching with the curvature-weighted NNDEGD matching measure achieves the lowest error rate of 0.30% among k-NN based methods."
}
@article{SHENEAMER2018405,
title = "A detection framework for semantic code clones and obfuscated code",
journal = "Expert Systems with Applications",
volume = "97",
pages = "405 - 420",
year = "2018",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2017.12.040",
url = "http://www.sciencedirect.com/science/article/pii/S0957417417308631",
author = "Abdullah Sheneamer and Swarup Roy and Jugal Kalita",
keywords = "Code obfuscation, Semantic code clones, Machine learning, Bytecode dependency graph, Program dependency graph",
abstract = "Code obfuscation is a staple tool in malware creation where code fragments are altered substantially to make them appear different from the original, while keeping the semantics unaffected. A majority of the obfuscated code detection methods use program structure as a signature for detection of unknown codes. They usually ignore the most important feature, which is the semantics of the code, to match two code fragments or programs for obfuscation. Obfuscated code detection is a special case of the semantic code clone detection task. We propose a detection framework for detecting both code obfuscation and clone using machine learning. We use features extracted from Java bytecode dependency graphs (BDG), program dependency graphs (PDG) and abstract syntax trees (AST). BDGs and PDGs are two representations of the semantics or meaning of a Java program. ASTs capture the structural aspects of a program. We use several publicly available code clone and obfuscated code datasets to validate the effectiveness of our framework. We use different assessment parameters to evaluate the detection quality of our proposed model. Experimental results are excellent when compared with contemporary obfuscated code and code clone detectors. Interestingly, we achieve 100% success in detecting obfuscated code based on recall, precision, and F1-Score. When we compare our method with other methods for all of obfuscations types, viz, contraction, expansion, loop transformation and renaming, our model appears to be the winner. In case of clone detection our model achieve very high detection accuracy in comparison to other similar detectors."
}
@article{SUN2017355,
title = "Enhancing developer recommendation with supplementary information via mining historical commits",
journal = "Journal of Systems and Software",
volume = "134",
pages = "355 - 368",
year = "2017",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.09.021",
url = "http://www.sciencedirect.com/science/article/pii/S0164121217302091",
author = "Xiaobing Sun and Hui Yang and Xin Xia and Bin Li",
keywords = "Collaborative topic modeling, Developer recommendation, Bug assignment, Personalized recommendation, Supplementary information recommendation, Commit repository",
abstract = "Given a software issue request, one important activity is to recommend suitable developers to resolve it. A number of approaches have been proposed on developer recommendation. These developer recommendation techniques tend to recommend experienced developers, i.e., the more experienced a developer is, the more possible he/she is recommended. However, if the experienced developers are hectic, the junior developers may be employed to finish the incoming issue. But they may have difficulty in these tasks for lack of development experience. In this article, we propose an approach, EDR_SI, to enhance developer recommendation by considering their expertise and developing habits. Furthermore, EDR_SI also provides the personalized supplementary information for developers to use, such as personalized source code files, developer network and source-code change history. An empirical study on five open source subjects is conducted to evaluate the effectiveness of EDR_SI. In our study, EDR_SI is also compared with the state-of-art developer recommendation techniques, iMacPro, Location and ABA-Time-tf-idf, to evaluate the effectiveness of developer recommendation, and the results show that EDR_SI can not only improve the accuracy of developer recommendation, but also effectively provide useful supplementary information for them to use when they implement the incoming issue requests."
}
@article{HUSIEN2017460,
title = "Towards a Severity and Activity based Assessment of Code Smells",
journal = "Procedia Computer Science",
volume = "116",
pages = "460 - 467",
year = "2017",
note = "Discovery and innovation of computer science technology in artificial intelligence era: The 2nd International Conference on Computer Science and Computational Intelligence (ICCSCI 2017)",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2017.10.040",
url = "http://www.sciencedirect.com/science/article/pii/S1877050917320884",
author = "Harris Kristanto Husien and Muhammad Firdaus Harun and Horst Lichter",
keywords = "Code smell activity, Code smell impact, Change-likelihood, Evolution, Harmful code smell",
abstract = "Code smells are the structural weaknesses which reside in a software system. They evolve negatively over time reducing the system quality i.e., maintainability, understandability etc. Therefore, they should be detected and prioritized based on criticality in order to be refactored. Most of the existing approaches are based on severity score, but little works have been done to include the information from changes history. Thus, we introduce a Harmfulness Model that integrates both information: severity and changes history (i.e., code smells activity). This study characterizes a god class activity based on its severity and change frequency of the JHotDraw open source system. The result indicates that there are two main activities of god class that can be assessed as active and passive smells. In fact, an active god class can be differentiated as strong, stable, and ameliorate smells while a passive god class has one type called dormant. Besides that, from severity and activity information, the model can compute the harmfulness score and also indicate the degree of harmfulness level. The harmfulness level may be useful to improve change likelihood estimation and refactoring candidates prioritization."
}
@article{BARNAFOLDI2017373,
title = "First Results with HIJING++ in High-Energy Heavy-Ion Collisions",
journal = "Nuclear and Particle Physics Proceedings",
volume = "289-290",
pages = "373 - 376",
year = "2017",
note = "8th International Conference on Hard and Electromagnetic Probes of High Energy Nuclear Collisions",
issn = "2405-6014",
doi = "https://doi.org/10.1016/j.nuclphysbps.2017.05.086",
url = "http://www.sciencedirect.com/science/article/pii/S2405601417302857",
author = "Gergely Gábor Barnaföldi and Gábor Bíró and Miklos Gyulassy and Szilveszter Miklós Haranozó and Péter Lévai and Guoyang Ma and Gábor Papp and Xin-Nian Wang and Ben-Wei Zhang",
keywords = "HIJING++, HIJING, Monte Carlo particle event generator, high-energy heavy ion collisions",
abstract = "First calculated results with the new HIJING++ are presented for identified hadron production in high-energy heavy ion collisions. The recently developed HIJING++ version is based on the latest version of PYTHIA8 and contains all the nuclear effects has been included in the HIJING2.552, which will be improved by a new version of the shadowing parametrization and jet quenching module. Here, we summarize the major changes of the new program code beside the comparison between experimental data for some specific high-energy nucleus-nucleus collisions."
}
@article{YU20131446,
title = "DNAD, a simple tool for automatic differentiation of Fortran codes using dual numbers",
journal = "Computer Physics Communications",
volume = "184",
number = "5",
pages = "1446 - 1452",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2012.12.025",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513000027",
author = "Wenbin Yu and Maxwell Blair",
keywords = "Automatic differentiation, Sensitivity, Fortran 90/95/2003, Dual numbers, DNAD",
abstract = "DNAD (dual number automatic differentiation) is a simple, general-purpose tool to automatically differentiate Fortran codes written in modern Fortran (F90/ 95/2003) or legacy codes written in previous version of the Fortran language. It implements the forward mode of automatic differentiation using the arithmetic of dual numbers and the operator overloading feature of F90/ 95/2003. Very minimum changes of the source codes are needed to compute the first derivatives of Fortran programs. The advantages of DNAD in comparison to other existing similar computer codes are its programming simplicity, extensibility, and computational efficiency. Specifically, DNAD is more accurate and efficient than the popular complex-step approximation. Several examples are used to demonstrate its applications and advantages.
Program summary
Program title: DNAD Catalogue identifier: AEOS_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEOS_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 3922 No. of bytes in distributed program, including test data, etc.: 18275 Distribution format: tar.gz Programming language: Fortran 90/95/2003. Computer: All computers with a modern FORTRAN compiler. Operating system: All platforms with a modern FORTRAN compiler. Classification: 4.12, 6.2. Nature of problem: Derivatives of outputs with respect to inputs of a Fortran code are often needed in physics, chemistry, and engineering. The author of the analysis code may no longer be available and the user may not have a deep knowledge of the code. Thus a simple tool is necessary to automatically differentiate the code with very minimum change to the source codes. This can be achieved using dual number arithmetic and operator overloading. Solution method: A new data type is defined with the first scalar component holding the function value and the second array component holding the first derivatives. All the basic operations and functions are overloaded with the new definitions according to dual number arithmetic. To differentiate an existing code, all real numbers should be replaced with this new data type and the input/output of the code should also be modified accordingly. Running time: For each additional independent variable, DNAD takes less time than the running time of the original analysis code. However, the actual running time depends on the compiler, the computer, and the operations involved in the code to be differentiated."
}
@article{OWENS201748,
title = "Psychometric properties of the Motivational Interviewing Treatment Integrity coding system 4.2 with jail inmates",
journal = "Addictive Behaviors",
volume = "73",
pages = "48 - 52",
year = "2017",
issn = "0306-4603",
doi = "https://doi.org/10.1016/j.addbeh.2017.04.015",
url = "http://www.sciencedirect.com/science/article/pii/S0306460317301624",
author = "Mandy D. Owens and Lauren N. Rowell and Theresa Moyers",
keywords = "Motivational interviewing, Motivational interviewing treatment integrity coding system, Therapy process, Jail inmates, Substance use",
abstract = "Motivational Interviewing (MI) is an evidence-based approach shown to be helpful for a variety of behaviors across many populations. Treatment fidelity is an important tool for understanding how and with whom MI may be most helpful. The Motivational Interviewing Treatment Integrity coding system was recently updated to incorporate new developments in the research and theory of MI, including the relational and technical hypotheses of MI (MITI 4.2). To date, no studies have examined the MITI 4.2 with forensic populations. In this project, twenty-two brief MI interventions with jail inmates were evaluated to test the reliability of the MITI 4.2. Validity of the instrument was explored using regression models to examine the associations between global scores (Empathy, Partnership, Cultivating Change Talk and Softening Sustain Talk) and outcomes. Reliability of this coding system with these data was strong. We found that therapists had lower ratings of Empathy with participants who had more extensive criminal histories. Both Relational and Technical global scores were associated with criminal histories as well as post-intervention ratings of motivation to decrease drug use. Findings indicate that the MITI 4.2 was reliable for coding sessions with jail inmates. Additionally, results provided information related to the relational and technical hypotheses of MI. Future studies can use the MITI 4.2 to better understand the mechanisms behind how MI works with this high-risk group."
}
@article{HUPPMANN2019143,
title = "The MESSAGEix Integrated Assessment Model and the ix modeling platform (ixmp): An open framework for integrated and cross-cutting analysis of energy, climate, the environment, and sustainable development",
journal = "Environmental Modelling & Software",
volume = "112",
pages = "143 - 156",
year = "2019",
issn = "1364-8152",
doi = "https://doi.org/10.1016/j.envsoft.2018.11.012",
url = "http://www.sciencedirect.com/science/article/pii/S1364815218302330",
author = "Daniel Huppmann and Matthew Gidden and Oliver Fricko and Peter Kolp and Clara Orthofer and Michael Pimmer and Nikolay Kushin and Adriano Vinca and Alessio Mastrucci and Keywan Riahi and Volker Krey",
keywords = "Open-source, Energy system optimization, Integrated assessment, Strategic planning tool, Scenario management, Data warehouse",
abstract = "The MESSAGE Integrated Assessment Model (IAM) developed by IIASA has been a central tool of energy-environment-economy systems analysis in the global scientific and policy arena. It played a major role in the Assessment Reports of the Intergovernmental Panel on Climate Change (IPCC); it provided marker scenarios of the Representative Concentration Pathways (RCPs) and the Shared Socio-Economic Pathways (SSPs); and it underpinned the analysis of the Global Energy Assessment (GEA). Alas, to provide relevant analysis for current and future challenges, numerical models of human and earth systems need to support higher spatial and temporal resolution, facilitate integration of data sources and methodologies across disciplines, and become open and transparent regarding the underlying data, methods, and the scientific workflow. In this manuscript, we present the building blocks of a new framework for an integrated assessment modeling platform; the “ecosystem” comprises: i) an open-source GAMS implementation of the MESSAGE energy++ system model integrated with the MACRO economic model; ii) a Java/database back-end for version-controlled data management, iii) interfaces for the scientific programming languages Python & R for efficient input data and results processing workflows; and iv) a web-browser-based user interface for model/scenario management and intuitive “drag-and-drop” visualization of results. The framework aims to facilitate the highest level of openness for scientific analysis, bridging the need for transparency with efficient data processing and powerful numerical solvers. The platform is geared towards easy integration of data sources and models across disciplines, spatial scales and temporal disaggregation levels. All tools apply best-practice in collaborative software development, and comprehensive documentation of all building blocks and scripts is generated directly from the GAMS equations and the Java/Python/R source code."
}
@article{FAN2018290,
title = "Version 4.0 of code Java for 3D simulation of the CCA model",
journal = "Computer Physics Communications",
volume = "228",
pages = "290 - 292",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2018.03.014",
url = "http://www.sciencedirect.com/science/article/pii/S0010465518300869",
author = "Linyu Fan and Jianwei Liao and Junsen Zuo and Kebo Zhang and Chao Li and Hailing Xiong",
keywords = "CCA, Aggregation model, Optimization",
abstract = "This paper presents a new version Java code for the three-dimensional simulation of Cluster–Cluster Aggregation (CCA) model to replace the previous version. Many redundant traverses of clusters-list in the program were totally avoided, so that the consumed simulation time is significantly reduced. In order to show the aggregation process in a more intuitive way, we have labeled different clusters with varied colors. Besides, a new function is added for outputting the particle’s coordinates of aggregates in file to benefit coupling our model with other models.
New version program summary
Program Title: CCA v04 Program Files doi:http://dx.doi.org10.17632/zwz37tvjny.1 Licensing provisions: Apache-2.0 Programming language: Java Journal reference of previous version: Computer Physics Communications 207 (2016) 547–548 Does the new version supersede the previous version?: Yes Nature of problem: The previous program for CCA model can be optimized for yielding better operation efficiency and visualization effects. Besides, it can be extended for coupling with other models by adding some new functions. Solution method: Some redundant traverses of the clusters-list for updating an important variable were removed technically. Furthermore, displaying different clusters with distinct colors can benefit to the effectiveness of visualization. We have also introduced a new function for outputting the aggregated clusters, so that this model can be easily coupled with other models. Reasons for the new version: 1. In the previous versions [1–3], the global variable of Dmax, which means the maximum diffusion coefficient of clusters, is supposed to be calculated when aggregation occurs. In fact, this variable is critical to determine whether the currently selected cluster carries out diffusion or not. In the previous version, we had to traverse the clusters-list to update Dmax, even though it may keep unchanged after a round of aggregation. But, we have explored that it is not necessary to traverse all clusters, if we compare the previous variable of Dmax with the diffusion coefficient of the new cluster, which is formed by aggregating two clusters. In the case of the new one is larger, Dmax will be updated to it; otherwise, Dmax keep unchanged. Obviously, removing redundant traverses can enhance algorithm efficiency, and the time complexity of the algorithm is closely related to the loop in it. As a result, the new version of code can greatly cut down the simulation time, in contrast to the previous codes. 2. The old versions paint all particles with same color, which does not help to identify different clusters. In order to show the visualization of the 3D aggregate process in different states intuitively, we introduce labeling different clusters with different colors. Consequently, the particles belong to one cluster are painted with the same color. 3. A new function is added to record intermediate or final state of clusters so that it can be used for further research when using other relevant models. Summary of revisions: 1. Reduce unnecessary traverses of the clusters-list for speeding up simulation. 2. Paint clusters with different colors to pursuit better visualization effects. 3. Add a new function of exporting the coordinates of clusters to a text file for benefiting other models. Additional comments: To test the optimization effects offered by the new version of code, we have carried out a comparison experimentto measure the simulation time with different initial particles, by using the new and the previous version 3.0 of code. In the experiments, we set concentration C to 0.01, diffusion exponent γ to 0, sticking probability exponent σ to 1, the sticking probability P1 to 0.1, absolute temperature T to 298 K, and the side length L of cube was respectively set to 30, 40, 50, 60, 70 and 80 to yield different initial particles, as the number of particles N can be calculated by the formula N=C∗L3 [1]. To avoid the influences of hardware and software, we use Monte Carlo steps (MCS) as our simulation time [4–6]. As shown in Fig. 1, the slope coefficient caused by this new version of code is about 1.3813, it is close to line complexity, as for the previous version, it is 1.5703, larger than the new one. It is obvious the new one has a faster running speed than the previous one. More importantly, the reduction of simulation time becomes more obvious when the number of simulated particles is getting larger. The changed visualization of our program is shownin Fig. 2. We can clearly see how much clusters in simulation system, and how they distribute. And the button “save” marked red in Fig. 2 can help realize our new function by exporting the particle coordinates to a text file, which also store the related initial conditions. Acknowledgments This work was partially supported by “National Natural Science Foundation of China (Nos. 41271292, 61303038)”. References[1]C. Li, H. Xiong, 3D simulation of the Cluster-Cluster aggregation model, Computer Physics Communications 185 (12) (2014) 3424–3429.[2]K. Zhang, H. Xiong, C. Li, A new version of code Java for 3D simulation of the CCA model, Computer Physics Communications 204 (2016) 214–215.[3]K. Zhang, J. Zuo, Y. Dou, C. Li, H. Xiong, Version 3.0 of code Java for 3D simulation of the CCA model, Computer Physics Communications 207 (2016) 547–548.[4]T. Vicsek, Dynamic scaling for aggregation of clusters, Physical Review Letters 52 (19) (1984) 1669–1672.[5]P. A. Netz, D. Samios, The study of network formation as a cluster-cluster diffusion-limited aggregation process: Modelling of the curing of an epoxy-resin using the Monte Carlo method, Macromolecular Theory Simulations 3 (3) (1994) 607–621.[6]H. Xiong, H. Li, W. Chen, L. Wu, Data structure for on-lattice cluster-cluster aggregation model performance optimization, Computer Physics Communications 185 (3) (2014) 836–840."
}
@article{PATEL2017184,
title = "Deconstructing the symbol digit modalities test in multiple sclerosis: The role of memory",
journal = "Multiple Sclerosis and Related Disorders",
volume = "17",
pages = "184 - 189",
year = "2017",
issn = "2211-0348",
doi = "https://doi.org/10.1016/j.msard.2017.08.006",
url = "http://www.sciencedirect.com/science/article/pii/S221103481730189X",
author = "Viral P. Patel and Lisa A.S. Walker and Anthony Feinstein",
keywords = "Multiple sclerosis, Neuropsychology, Processing speed, SDMT, Memory",
abstract = "Background
The Symbol Digit Modalities Test (SDMT) is a sensitive measure of impaired cognition in people with MS. While the SDMT is primarily considered a test of information processing speed, other components such as visual scanning and oral-motor ability have also been linked to performance. The objective of this study was to determine the role of memory in the performance of the SDMT.
Methods
Two version of a modified computerized SDMT (c-SDMT) were employed, a fixed and a variable. For each group 50 MS and 33 healthy control (HC) participants were recruited. In the fixed c-SDMT, the symbol-digit code is kept constant for the entire test whereas in the variable version, it changes eight times. Unlike the traditional SDMT which records the correct number of responses, the c-SDMT presented here measures the mean response time (in seconds) for the eight trials.
Results
MS participants were slower than HC on the fixed (p < 0.001) and variable (p = 0.005) c-SDMT. Trend analysis showed performance improvement on the fixed, but not on the variable c-SDMT in both MS and HC groups. Furthermore, immediate visual memory recall was associated with the fixed (β = −0.299, p = 0.017), but not variable (B = −0.057, p = 0.260) c-SDMT. Immediate verbal memory was not associated with either versions of the c-SDMT.
Conclusions
Given that the fixed and variable c-SDMTs are identical in every way apart from the fixity of the code, the ability of participants to speed up responses over the course of the fixed version only points to the contribution of incidental visual memory in test performance."
}
@article{ALONSORODRIGUEZ2017154,
title = "Infographic restitution of the historic centre of the Spanish town of Oviedo",
journal = "Computers, Environment and Urban Systems",
volume = "64",
pages = "154 - 168",
year = "2017",
issn = "0198-9715",
doi = "https://doi.org/10.1016/j.compenvurbsys.2017.01.009",
url = "http://www.sciencedirect.com/science/article/pii/S019897151730056X",
author = "Marta Alonso-Rodriguez and Antonio Alvaro-Tordesillas and Eduardo Carazo-Lefort",
keywords = "Historical maps, Urban development, Urban graphic history, Computer design, Planimetric accuracy, Graphical analysis, Colour code",
abstract = "Our historic town centres possess an evocative quality that has recently led to considering these town centres a subject of scientific research, overtaking, if not contradicting, the romantic vision by which they obtained importance in the Western world from the late 18th century. Thus, the present work is framed in the existing and ultimately consolidated method of European universities, which analyse, with an increasing scientific rigour from many different perspectives, the knowledge of the abundant and rich historic town centres that geographically define our living environment. This study focuses on the reconstruction of the urban shape of the town of Oviedo in a series of stages that are marked by the cartographic data collected during the time of the study and that involve substantial changes to the urban fabric of the town. The study attempts to be a historical analysis of urban transformation. This research established a systematic study of earlier times when the urban form offered space patterns different from existing patterns. Using the current structure of the historic centre of Oviedo and the analysis of its pathologies throughout history as a starting point, this study considers the possibility of a methodical study of the past. This project presents the coexistence of parallel textual and visual accounts, which can be read together, as a new methodology to understand the spatial formation and transformation of urban landscapes."
}
@article{LEEBAE2016164,
title = "A coding tool for examining the substance of teacher professional learning and change with example cases from middle school science lesson study",
journal = "Teaching and Teacher Education",
volume = "60",
pages = "164 - 178",
year = "2016",
issn = "0742-051X",
doi = "https://doi.org/10.1016/j.tate.2016.08.016",
url = "http://www.sciencedirect.com/science/article/pii/S0742051X1630261X",
author = "Christine Lee Bae and Kathryn N. Hayes and Jeffery Seitz and Dawn O'Connor and Rachelle DiStefano",
keywords = "Lesson study, Professional learning, Teacher change, Middle school, Science, Coding",
abstract = "Although lesson study is increasingly adopted in the United States (U.S.), the impact of lesson study on teacher learning is uncertain. This study presents a theoretically grounded set of codes to systematically document the various aspects of teacher learning and change (knowledge and beliefs, professional learning community, resources) in lesson study across contexts. To present examples of the codes in use, a subset of codes related to change in teacher knowledge and beliefs were applied to analyze teachers' professional discourse in three middle school science lesson study teams."
}
@article{YALLOP201882,
title = "A modular foreign function interface",
journal = "Science of Computer Programming",
volume = "164",
pages = "82 - 97",
year = "2018",
note = "Special issue of selected papers from FLOPS 2016",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2017.04.002",
url = "http://www.sciencedirect.com/science/article/pii/S0167642317300709",
author = "Jeremy Yallop and David Sheets and Anil Madhavapeddy",
keywords = "Foreign functions, Functional programming, Modularity",
abstract = "Foreign function interfaces are typically organised monolithically, tying together the specification of each foreign function with the mechanism used to make the function available in the host language. This leads to inflexible systems, where switching from one binding mechanism to another (say from dynamic binding to static code generation) often requires changing tools and rewriting large portions of code. We show that ML-style module systems support exactly the kind of abstraction needed to separate these two aspects of a foreign function binding, leading to declarative foreign function bindings that support switching between a wide variety of binding mechanisms — static and dynamic, synchronous and asynchronous, etc. — with no changes to the function specifications. Note. This is a revised and expanded version of an earlier paper, Declarative Foreign Function Binding Through Generic Programming[19]. This paper brings a greater focus on modularity, and adds new sections on error handling, and on the practicality of the approach we describe."
}
@article{JASWAL201853,
title = "Multiple feature fusion for unconstrained palm print authentication",
journal = "Computers & Electrical Engineering",
volume = "72",
pages = "53 - 78",
year = "2018",
issn = "0045-7906",
doi = "https://doi.org/10.1016/j.compeleceng.2018.09.006",
url = "http://www.sciencedirect.com/science/article/pii/S0045790617321419",
author = "Gaurav Jaswal and Amit Kaul and Ravinder Nath",
keywords = "Biometrics, Palm print, Fractional mask, Sift, Vector quantization, Principal subspace learning, SVM",
abstract = "Over the last decade, palm print recognition has emerged as the strongest technology for human authentication in many aspects. To carry out an effective recognition, this paper presents a feature level fusion of block-wise scale invariant feature transform and texture code co-occurrence matrix based features. Initially, an attempt to access the quality of extracted region of interest image is made. This is followed by application of fractional differential mask resulting in improvement of textural detail. In order to select the most discriminate palm features, a feature transformation algorithm inspired by subspace learning is employed. It led to reduction in computation time and feature dimensions, along with higher level of performance. A trained support vector machine utilizes the selected features to determine whether image belongs to genuine or imposter class. Comparative experimental analysis described in this paper indicates customarily outperforming results than competing methods and validate efficacy of proposed approach."
}
@article{SHEMYAKIN2019378,
title = "TFmix: A high-precision implementation of the finite-temperature Thomas–Fermi model for a mixture of atoms",
journal = "Computer Physics Communications",
volume = "235",
pages = "378 - 387",
year = "2019",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2018.09.008",
url = "http://www.sciencedirect.com/science/article/pii/S0010465518303229",
author = "O.P. Shemyakin and P.R. Levashov and P.A. Krasnova",
keywords = "Finite-temperature Thomas–Fermi model, Equation of state, Mixture of elements, Tables of thermodynamic functions",
abstract = "In this work we present a TFmix code intended for numerical calculation of the thermal part of electronic thermodynamic properties of a mixture of elements by the finite-temperature Thomas–Fermi model. The code is based on analytical models for both first and second derivatives of Helmholtz thermodynamic potential. All numerical calculations are made within a controlled high accuracy: tests for thermodynamic consistency give at least 11 coinciding decimal digits. The code calculates thermodynamic functions on a regular grid of isotherms and isochores; at each grid point some extensive parameters and the number of free electrons are output both for the whole mixture and for each component. Other extensive or intensive thermodynamic properties, including pressure, entropy, isochoric and isobaric heat capacities, isothermal and adiabatic sound velocities can be easily calculated from the information available at each grid point. Several unit systems are provided for convenience. A cross-platform graphical user interface is developed to simplify the use of the code.
Program summary
Program Title: TFmix, version 1.0 Program Files doi:http://dx.doi.org/10.17632/mc3vj77jfn.1 Licensing provisions: GPLv3 Programming language: C, Python Nature of problem: Any substance consists of elements so its equation of state contains a contribution of electronic gas. Thermodynamics of the electronic gas in a mixture of ions and electrons has been studied in many approaches. Thermodynamic properties of a uniform ideal electron gas can be calculated using the well-known analytical model of Fermi-gas. On the other hand, models of electron gas which take into account interaction effects are quite complicated and require sophisticated computational techniques. Even a simplified semiclassical Thomas–Fermi model is based upon the numerical solution of a non-linear boundary problem. Two main issues of the Thomas–Fermi model restrict its usage: uncontrolled accuracy of calculated thermodynamic functions (especially second derivatives of a thermodynamic potential), and unphysical behavior of the model at relatively low temperatures. Solution method: Each atom in the mixture is surrounded by a spherical cell. The radii of the cells are fitted to equalize the chemical potentials of all atoms. A guaranteed accuracy of first derivatives of the thermodynamic potential is provided by a transformation of integrals over the Thomas–Fermi potential to a system of differential equations. One of equations in the system is the Thomas–Fermi equation. Second derivatives of the thermodynamic potential are calculated similarly with the only difference that a corresponding derivative of the Thomas–Fermi equation is used in the system of differential equation. To avoid the unphysical behavior of the Thomas–Fermi model at low temperatures we extract a thermal contribution to thermodynamic properties which vanishes at zero temperature. To eliminate the error which appears from the subtraction of the cold part at low temperatures we use asymptotic expressions for thermodynamic functions and the Thomas–Fermi equation. The code calculates regular tables of thermodynamic functions on a grid of isotherms and isochores including second derivatives of a thermodynamic potential. This information is necessary for astrophysical applications, for continuum mechanics simulation of processes in plasma and for the creation of wide-range equations of state. A graphical user interface is provided with the code and allows to specify input parameters, to perform calculations and to plot the results. Additional comments including restrictions and unusual features: GSL library version 1.16 or 2.x is required for compilation; matplotlib Python library is required to run the graphical user interface."
}
@article{OLIVEIRA20181,
title = "Do android developers neglect error handling? a maintenance-Centric study on the relationship between android abstractions and uncaught exceptions",
journal = "Journal of Systems and Software",
volume = "136",
pages = "1 - 18",
year = "2018",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.10.032",
url = "http://www.sciencedirect.com/science/article/pii/S0164121217302558",
author = "Juliana Oliveira and Deise Borges and Thaisa Silva and Nelio Cacho and Fernando Castor",
keywords = "Exception handling, Android, Robustness, Maintainability",
abstract = "All the mainstream programming languages in widespread use for mobile app development provide error handling mechanisms to support the implementation of robust apps. Android apps, in particular, are usually written in the Java programming language. Java includes an exception handling mechanism that allows programs to signal the occurrence of errors by throwing exceptions and to handle these exceptions by catching them. All the Android-specific abstractions, such as activities and asynctasks, can throw exceptions when errors occur. When an app catches the exceptions that it or the libraries upon which it depends throw, it can resume its activity or, at least, fail in a graceful way. On the other hand, uncaught exceptions can lead an app to crash, particularly if they occur within the main thread. Previous work has shown that, in real Android apps available at the Play Store, uncaught exceptions thrown by Android-specific abstractions often cause these apps to fail. This paper presents an empirical study on the relationship between the usage of Android abstractions and uncaught exceptions. Our approach is quantitative and maintenance-centric. We analyzed changes to both normal and exception handling code in 112 versions extracted from 16 software projects covering a number of domains, amounting to more than 3 million LOC. Change impact analysis and exception flow analysis were performed on those versions of the projects. The main finding of this study is that, during the evolution of the analyzed apps, an increase in the use of Android abstractions exhibits a positive and statistically significant correlation with the number of uncaught exception flows. Since uncaught exceptions cause apps to crash, this result suggests that these apps are becoming potentially less robust as a consequence of exception handling misuse. Analysis of multiple versions of these apps revealed that Android developers usually employ abstractions that may throw exceptions without adding the appropriate handlers for these exceptions. This study highlights the need for better testing and verification tools with a focus on exception handling code and for a change of culture in Android development or, at least, in the design of its APIs."
}
@article{YEH2014891,
title = "Self-learning-based post-processing for image/video deblocking via sparse representation",
journal = "Journal of Visual Communication and Image Representation",
volume = "25",
number = "5",
pages = "891 - 903",
year = "2014",
issn = "1047-3203",
doi = "https://doi.org/10.1016/j.jvcir.2014.02.012",
url = "http://www.sciencedirect.com/science/article/pii/S1047320314000509",
author = "Chia-Hung Yeh and Li-Wei Kang and Yi-Wen Chiou and Chia-Wen Lin and Shu-Jhen Fan Jiang",
keywords = "Blocking artifact, Deblocking, Sparse representation, Dictionary learning, Morphological component analysis, Image/video restoration, Image/video enhancement, Post-processing",
abstract = "Blocking artifact, characterized by visually noticeable changes in pixel values along block boundaries, is a common problem in block-based image/video compression, especially at low bitrate coding. Various post-processing techniques have been proposed to reduce blocking artifacts, but they usually introduce excessive blurring or ringing effects. This paper proposes a self-learning-based post-processing framework for image/video deblocking by properly formulating deblocking as an MCA (morphological component analysis)-based image decomposition problem via sparse representation. Without the need of any prior knowledge (e.g., the positions where blocking artifacts occur, the algorithm used for compression, or the characteristics of image to be processed) about the blocking artifacts to be removed, the proposed framework can automatically learn two dictionaries for decomposing an input decoded image into its “blocking component” and “non-blocking component.” More specifically, the proposed method first decomposes a frame into the low-frequency and high-frequency parts by applying BM3D (block-matching and 3D filtering) algorithm. The high-frequency part is then decomposed into a blocking component and a non-blocking component by performing dictionary learning and sparse coding based on MCA. As a result, the blocking component can be removed from the image/video frame successfully while preserving most original visual details. Experimental results demonstrate the efficacy of the proposed algorithm."
}
@article{CANTY2012107,
title = "Linear and kernel methods for multivariate change detection",
journal = "Computers & Geosciences",
volume = "38",
number = "1",
pages = "107 - 114",
year = "2012",
issn = "0098-3004",
doi = "https://doi.org/10.1016/j.cageo.2011.05.012",
url = "http://www.sciencedirect.com/science/article/pii/S0098300411001889",
author = "Morton J. Canty and Allan A. Nielsen",
keywords = "CUDA, ENVI, IDL, IR-MAD, iMAD, Kernel methods, Matlab, Radiometric normalization, Remote sensing, Multiresolution",
abstract = "The iteratively reweighted multivariate alteration detection (IR-MAD) algorithm may be used both for unsupervised change detection in multi- and hyperspectral remote sensing imagery and for automatic radiometric normalization of multitemporal image sequences. Principal components analysis (PCA), as well as maximum autocorrelation factor (MAF) and minimum noise fraction (MNF) analyses of IR-MAD images, both linear and kernel-based (nonlinear), may further enhance change signals relative to no-change background. IDL (Interactive Data Language) implementations of IR-MAD, automatic radiometric normalization, and kernel PCA/MAF/MNF transformations are presented that function as transparent and fully integrated extensions of the ENVI remote sensing image analysis environment. The train/test approach to kernel PCA is evaluated against a Hebbian learning procedure. Matlab code is also available that allows fast data exploration and experimentation with smaller datasets. New, multiresolution versions of IR-MAD that accelerate convergence and that further reduce no-change background noise are introduced. Computationally expensive matrix diagonalization and kernel image projections are programmed to run on massively parallel CUDA-enabled graphics processors, when available, giving an order of magnitude enhancement in computational speed. The software is available from the authors' Web sites."
}
@article{CACCIAGLIA2019200,
title = "Auditory predictions shape the neural responses to stimulus repetition and sensory change",
journal = "NeuroImage",
volume = "186",
pages = "200 - 210",
year = "2019",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2018.11.007",
url = "http://www.sciencedirect.com/science/article/pii/S1053811918320779",
author = "Raffaele Cacciaglia and Jordi Costa-Faidella and Katarzyna Zarnowiec and Sabine Grimm and Carles Escera",
keywords = "Auditory deviance detection, Repetition suppression, Repetition enhancement, Predictive coding, Event-related fMRI",
abstract = "Perception is a highly active process relying on the continuous formulation of predictive inferences using short-term sensory memory templates, which are recursively adjusted based on new input. According to this idea, earlier studies have shown that novel stimuli preceded by a higher number of repetitions yield greater novelty responses, indexed by larger mismatch negativity (MMN). However, it is not clear whether this MMN memory trace effect is driven by more adapted responses to prior stimulation or rather by a heightened processing of the unexpected deviant, and only few studies have so far attempted to characterize the functional neuroanatomy of these effects. Here we implemented a modified version of the auditory frequency oddball paradigm that enables modeling the responses to both repeated standard and deviant stimuli. Fifteen subjects underwent functional magnetic resonance imaging (fMRI) while their attention was diverted from auditory stimulation. We found that deviants with longer stimulus history of standard repetitions yielded a more robust and widespread activation in the bilateral auditory cortex. Standard tones repetition yielded a pattern of response entangling both suppression and enhancement effects depending on the predictability of upcoming stimuli. We also observed that regularity encoding and deviance detection mapped onto spatially segregated cortical subfields. Our data provide a better understanding of the neural representations underlying auditory repetition and deviance detection effects, and further support that perception operates through the principles of Bayesian predictive coding."
}
@article{SCHNEIDER2018192,
title = "A new Fortran 90 program to compute regular and irregular associated Legendre functions (new version announcement)",
journal = "Computer Physics Communications",
volume = "225",
pages = "192 - 193",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2017.12.013",
url = "http://www.sciencedirect.com/science/article/pii/S0010465517304186",
author = "Barry I. Schneider and Javier Segura and Amparo Gil and Xiaoxu Guan and Klaus Bartschat",
keywords = "Associated Legendre functions, Recursion relations, Miller’s algorithm, Continued fractions",
abstract = "This is a revised and updated version of a modern Fortran 90 code to compute the regular Plm(x) and irregular Qlm(x) associated Legendre functions for all x∈(−1,+1) (on the cut) and |x|>1 and integer degree (l) and order (m). The necessity to revise the code comes as a consequence of some comments of Prof. James Bremer of the UC//Davis Mathematics Department, who discovered that there were errors in the code for large integer degree and order for the normalized regular Legendre functions on the cut.
New version program summary
Program Title: Associated Legendre Functions Program Files doi:http://dx.doi.org/10.17632/29j2mvwgwt.1 Licensing provisions: GPLv2 Programming language: Fortran 90 Does the new version supersede the previous version?: Yes Journal reference of previous version: Comput. Phys. Commun. 181 (2010) 2091 Reason for new version: The necessity to revise the code comes as a consequence of some comments of Prof. James Bremer of the UC//Davis Mathematics Department, who discovered that there were errors in the code for large integer degree and order for the normalized regular Legendre functions on the cut. This occurred because renormalization of the un-normalized functions involved the manipulation of factorials of large integer value. The problem was solved by a direct computation of the normalized functions from a revised recursion formula. This completely avoids the manipulation of the factorials that were present in the first version. In re-examining the code we also noticed that there were cases for large degree, order and argument that led to overflows causing the results to display infinities or NaN when printed out, without crashing. There does not appear to be any good approach, other than resorting to extended precision arithmetic, to cure this pathology. Since it was not our original intention to provide such a code, all we can do is to point out that this can happen and that the user should take care not to blindly accept the results of the computation for large degree and order. We have not found a general approach to predict in advance when this will occur in this three dimensional parameter space. Summary of revisions: None of the basic algorithms has been changed. We have eliminated the computation of factorials for large integer values and now calculate the normalized regular Legendre functions on the cut (-1,+1) directly from the recursion relation for those functions rather than computing the unnormalized functions and then normalizing after the computation. This required some modifications of the original Fortran subroutines as well as the addition of a new module. Nature of problem: Compute the regular and irregular associated Legendre functions for integer values of the degree and order and for all real arguments. The computation of the interaction of two electrons, 1∕|r1−r2|, in prolate spheroidal coordinates is used as one example where these functions are required for all values of the argument and we are able to easily compare the series expansion in associated Legendre functions and the exact value. Solution method: The code evaluates the regular and irregular associated Legendre functions using forward recursion when |x|<1 starting the recursion with the analytically known values of the first two members of the sequence. For values of the argument |x|<1, the upward recursion over the degree for the regular functions is numerically stable. For the irregular functions, backward recursion must be applied and a suitable method of starting the recursion is required. The program has two options; a modified version of Miller’s algorithm and the use of the wronskian relation between the regular and irregular functions, which was the method considered in [1]. Both approaches require the computation of a continued fraction to begin the recursion. The wronskian method (which can also be described as a modified Miller’s method) is a convenient method of computations when both the regular and irregular functions are needed. References[1]A. Gil, J. Segura, A code to evaluate prolate and oblate spheroidal harmonics, Comput. Phys. Commun. 108 (1998) 267–278."
}
@article{HAM201980,
title = "Extensions to hybrid code networks for FAIR dialog dataset",
journal = "Computer Speech & Language",
volume = "53",
pages = "80 - 91",
year = "2019",
issn = "0885-2308",
doi = "https://doi.org/10.1016/j.csl.2018.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S0885230818300871",
author = "Jiyeon Ham and Soohyun Lim and Kyeng-Hun Lee and Kee-Eung Kim",
keywords = "Dialog systems technology, Goal-oriented dialog system, Extended hybrid code network, DSTC6",
abstract = "Goal-oriented dialog systems require a different approach from chit-chat conversational systems in that they should perform various subtasks as well as continue the conversation itself. Since these systems typically interact with an external knowledge base that changes over time, it is desirable to incorporate domain knowledge to deal with such changes, yet with minimum human effort. This paper presents an extended version of the Hybrid Code Network (HCN) developed for the Facebook AI research (FAIR) dialog dataset used in the Sixth Dialog System Technology Challenge (DSTC6). Compared to the original HCN, the system was more adaptable to changes in the knowledge base due to the modules that are extended to be learned from data. Using the proposed learning scheme with fairly elementary domain-specific rules, the proposed model achieved 100% accuracy in all test datasets."
}
@article{CZIBULA20191,
title = "An aggregated coupling measure for the analysis of object-oriented software systems",
journal = "Journal of Systems and Software",
volume = "148",
pages = "1 - 20",
year = "2019",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2018.10.052",
url = "http://www.sciencedirect.com/science/article/pii/S0164121218302371",
author = "Istvan Gergely Czibula and Gabriela Czibula and Diana-Lucia Miholca and Zsuzsanna Onet-Marian",
keywords = "Coupling measure, Structural coupling, Conceptual coupling, Unsupervised learning",
abstract = "Coupling is a fundamental property of software systems which is strongly connected with the quality of software design and has high impact on program understanding. The coupling between software components influences software maintenance and evolution as well. In order to ease the maintenance and evolution processes it is essential to estimate the impact of changes made in the software system, coupling indicating such a possible impact. This paper introduces a new aggregated coupling measurement which captures both the structural and the conceptual characteristics of coupling between the software components. The proposed measure combines the textual information contained in the source code with the structural relationships between software components. We conduct several experiments which underline that the proposed aggregated coupling measure reveals new characteristics of coupling and is also effective for change impact analysis."
}
@article{NAZARI2018644,
title = "Securing templates in a face recognition system using Error-Correcting Output Code and chaos theory",
journal = "Computers & Electrical Engineering",
volume = "72",
pages = "644 - 659",
year = "2018",
issn = "0045-7906",
doi = "https://doi.org/10.1016/j.compeleceng.2018.01.029",
url = "http://www.sciencedirect.com/science/article/pii/S0045790616303846",
author = "Sara Nazari and Mohammad-Shahram Moin and Hamidreza Rashidy Kanan",
keywords = "Face template protection, Cryptosystem, Chaos feature permutation, Error-Correcting Output Code, Discriminant binarization transformation",
abstract = "In biometric cryptosystems, biometric data is combined with cryptography algorithms to generate secure templates. In these systems, creating protected templates with both high discriminability and high security is a challenging issue. To address this issue, this paper proposes a new face cryptosystem based on binarization transformation, chaos feature permutation and fuzzy commitment scheme. To enhance discriminability, real-valued templates are converted into their binary versions using a new discriminant binarization transformation based on Error-Correcting Output Code. Then, the chaos feature permutation is used to increase the security and privacy of binary templates, and also to protect the fuzzy commitment scheme against cross-matching attacks. The proposed scheme is evaluated on three well-known face databases, i.e. CMU PIE, FEI, and Extended Yale B. Experimental results show that the proposed method improves discriminability, as well as privacy and security of the system, compared to the existing face template protection algorithms."
}
@article{GUDA2018,
title = "Quantitative structural determination of active sites from in situ and operando XANES spectra: From standard ab initio simulations to chemometric and machine learning approaches",
journal = "Catalysis Today",
year = "2018",
issn = "0920-5861",
doi = "https://doi.org/10.1016/j.cattod.2018.10.071",
url = "http://www.sciencedirect.com/science/article/pii/S0920586118307703",
author = "Alexander A. Guda and Sergey A. Guda and Kirill A. Lomachenko and Mikhail A. Soldatov and Ilia A. Pankin and Alexander V. Soldatov and Luca Braglia and Aram L. Bugaev and Andrea Martini and Matteo Signorile and Elena Groppo and Alessandro Piovano and Elisa Borfecchia and Carlo Lamberti",
keywords = " XANES, Structure determination, Time dependent DFT, Finite difference method, Multivariate curve resolution, Machine learning",
abstract = "In the last decade the appearance of progressively more sophisticated codes, together with the increased computational capabilities, has made XANES a spectroscopic technique able to quantitatively confirm (or discard) a structural model, thus becoming a new fundamental diagnostic tool in catalysis, where the active species are often diluted metal centers supported on a matrix. After providing a brief historical introduction and the basic insights on the technique, in this review article, we provide a selection of four examples where operando XANES technique has been able to provide capital information on the structure of the active site in catalysts of industrial relevance: (i) Phillips catalyst for ethylene polymerization reaction; (ii) TS-1 catalyst for selective hydrogenation reactions; (iii) carbon supported Pd nanoparticles for hydrogenation reactions; (iv) Cu-CHA zeolite for NH3-assisted selective reduction of NOx and for partial oxidation of methane to methanol. The last example testifies how the multivariate curve resolution supported by the alternating least-squares algorithm applied to a high number of XANES spectra collected under operando conditions allows to quantitatively determine different species in mutual transformation. This approach is particularly powerful in the analysis of experiments where a large number of spectra has been collected, typical of time- or space-resolved experiments. Finally, machine learning approaches (both indirect and direct) have been applied to determine, from the XANES spectra, the structure of CO, CO2 and NO adsorbed on Ni2+ sites of activated CPO-27-Ni metal-organic framework."
}
@article{TRIBELLO2014604,
title = "PLUMED 2: New feathers for an old bird",
journal = "Computer Physics Communications",
volume = "185",
number = "2",
pages = "604 - 613",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.09.018",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513003196",
author = "Gareth A. Tribello and Massimiliano Bonomi and Davide Branduardi and Carlo Camilloni and Giovanni Bussi",
keywords = "Free energy, Molecular dynamics, Enhanced sampling, Dimensional reduction",
abstract = "Enhancing sampling and analyzing simulations are central issues in molecular simulation. Recently, we introduced PLUMED, an open-source plug-in that provides some of the most popular molecular dynamics (MD) codes with implementations of a variety of different enhanced sampling algorithms and collective variables (CVs). The rapid changes in this field, in particular new directions in enhanced sampling and dimensionality reduction together with new hardware, require a code that is more flexible and more efficient. We therefore present PLUMED 2 here—a complete rewrite of the code in an object-oriented programming language (C++). This new version introduces greater flexibility and greater modularity, which both extends its core capabilities and makes it far easier to add new methods and CVs. It also has a simpler interface with the MD engines and provides a single software library containing both tools and core facilities. Ultimately, the new code better serves the ever-growing community of users and contributors in coping with the new challenges arising in the field.
Program summary
Program title: PLUMED 2 Catalogue identifier: AEEE_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEEE_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Yes No. of lines in distributed program, including test data, etc.: 700646 No. of bytes in distributed program, including test data, etc.: 6618136 Distribution format: tar.gz Programming language: ANSI-C++. Computer: Any computer capable of running an executable produced by a C++ compiler. Operating system: Linux operating system, Unix OSs. Has the code been vectorized or parallelized?: Yes, parallelized using MPI. RAM: Depends on the number of atoms, the method chosen and the collective variables used. Classification: 3, 7.7, 23. Catalogue identifier of previous version: AEEE_v1_0. Journal reference of previous version: Comput. Phys. Comm. 180 (2009) 1961. External routines: GNU libmatheval, Lapack, Blas, MPI. Does the new version supersede the previous version?: This version supersedes the previous version for the most part. There are a small number of very specific situations where the previous version is better, due to performance or to non-ported features. We are actively working on porting these last few features into the new code. Nature of problem: Calculation of free-energy surfaces for molecular systems of interest in biology, chemistry and materials science, on the fly and a posteriori analysis of molecular dynamics trajectories using advanced collective variables. Solution method: Implementations of various collective variables and enhanced sampling techniques. Reasons for new version: The old version was difficult to maintain and its design was not as flexible as this new version. This lack of flexibility made it difficult to implement a number of novel methods that have emerged since the release of the original code. Summary of revisions: The new version of the code has a completely redesigned architecture, which allows for several important enhancements. This allows for a much simpler and robust input syntax and for improved performance. In addition, it provides several, more-complex collective variables which could not have been written using the previous implementation. Furthermore, the entire code is fully documented so it is easier to extend. Finally, the code is designed so that users can implement new variables directly in the input files and thus develop bespoke applications of these powerful algorithms. Unusual features: PLUMED 2 can be used either as a standalone program, e.g. for a posteriori analysis of trajectories, or as a library embedded in a molecular dynamics code (such as GROMACS, NAMD, Quantum ESPRESSO, and LAMMPS). Interfaces with these particular codes are provided in patches, which a simple script will insert into the underlying molecular dynamics codes source code files. For other molecular dynamics codes there is extensive documentation on how to add PLUMED in our manual. Additional comments: The distribution file contains a test suite, user and developer documentation and a collection of patches and utilities. Running time: Depends on the number of atoms, the method chosen and the collective variables used. The regression test suite provided takes approximately 1 min to run."
}
@article{ABDOLLAHI20181011,
title = "A revised dosimetric characterization of 60Co BEBIG source: From single-source data to clinical dose distribution",
journal = "Brachytherapy",
volume = "17",
number = "6",
pages = "1011 - 1022",
year = "2018",
issn = "1538-4721",
doi = "https://doi.org/10.1016/j.brachy.2018.08.011",
url = "http://www.sciencedirect.com/science/article/pii/S1538472118305002",
author = "Sara Abdollahi and Mahdieh Dayyani and Elie Hoseinian-Azghadi and Hashem Miri-Hakimabad and Laleh Rafat-Motavalli",
keywords = "HDR brachytherapy, Co0.A86 source, TG-43 parameters, Uncertainty analysis",
abstract = "Purpose
Although the dosimetric characterization of 60Co BEBIG source can be found in several literature studies, the data sets show major discrepancies and the lack of uncertainty analyses. This study tried to determine an accurate dosimetric data set for this source using Monte Carlo (MC) simulations along with detailed uncertainty analysis. To explore how different dosimetric data sets can make changes in practical situations, clinical dose distributions based on our results were compared with the dose distributions derived from Granero et al. and consensus data sets.
Methods and Materials
The MC simulations were performed with Monte Carlo N-Particle eXtended code (MCNPX) version 2.6.0 and the TG-43 parameters were estimated adhering to the American Association of Physicists in Medicine (AAPM) and European SocieTy for Radiotherapy and Oncology (ESTRO) 229 report. The dose rate distributions for single-source and two typical clinical cases, including one intracavitary and one interstitial, were calculated using an in-house code on the basis of the TG-43 formalism.
Results
The total uncertainties for water dose rate on source transverse axis at 1 cm and 5 cm, air kerma strength, and dose rate constant were evaluated to be 0.10%, 0.09%, 0.04%, and 0.11%, respectively. Meaningful differences were found for the interstitial case in which 22% of clinical target volume (CTV) showed differences from ±1% to ±10% or even larger.
Conclusions
The MC uncertainty was derived about 16 times smaller than the typical MC component stated in TG-138, partly because of large number of histories and partly because the spectra of 60Co and also its photons' attenuation coefficients are adequately accurate. The results showed that in the clinical situations, the applicator geometry and the superposition of single-source dose distributions can reduce the differences observed between several data sets."
}
@article{CONSIGLIO2018237,
title = "PArthENoPE reloaded",
journal = "Computer Physics Communications",
volume = "233",
pages = "237 - 242",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2018.06.022",
url = "http://www.sciencedirect.com/science/article/pii/S0010465518302303",
author = "R. Consiglio and P.F. de Salas and G. Mangano and G. Miele and S. Pastor and O. Pisanti",
keywords = "Primordial nucleosynthesis, Cosmology, Neutrino physics",
abstract = "We describe the main features of a new and updated version of the program PArthENoPE, which computes the abundances of light elements produced during Big Bang Nucleosynthesis. As the previous first release in 2008, the new one, PArthENoPE2.0, is publicly available and distributed from the code site, http://parthenope.na.infn.it. Apart from minor changes, which will be also detailed, the main improvements are as follows. The powerful, but not freely accessible, NAG routines have been substituted by ODEPACK libraries, without any significant loss in precision. Moreover, we have developed a Graphical User Interface (GUI) which allows a friendly use of the code and a simpler implementation of running for grids of input parameters.
New Version program summary
Program Title:PArthENoPE2.0 Program Files doi:http://dx.doi.org/10.17632/wvgr7d8yt9.1 Licensing provisions: GPLv3 Programming language: Fortran 77 and Python Supplementary material: User Manual available on the web page http://parthenope.na.infn.it Journal reference of previous version: Comput. Phys. Commun. 178 (2008) 956-971 Does the new version supersede the previous version?: Yes Reasons for the new version: Make the code more versatile and user friendly Summary of revisions: (1) Publicly available libraries (2) GUI for configuration Nature of problem: Computation of yields of light elements synthesized in the primordial universe Solution method: Livermore Solver for Ordinary Differential Equations (LSODE) for stiff and nonstiff systems"
}
@article{HASIUK201725,
title = "TouchTerrain: A simple web-tool for creating 3D-printable topographic models",
journal = "Computers & Geosciences",
volume = "109",
pages = "25 - 31",
year = "2017",
issn = "0098-3004",
doi = "https://doi.org/10.1016/j.cageo.2017.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S0098300416304824",
author = "Franciszek J. Hasiuk and Chris Harding and Alex Raymond Renner and Eliot Winer",
keywords = "3D printing, Topography, Terrain, Google Earth Engine, Python, Direct Digital Manufacturing",
abstract = "An open-source web-application, TouchTerrain, was developed to simplify the production of 3D-printable terrain models. Direct Digital Manufacturing (DDM) using 3D Printers can change how geoscientists, students, and stakeholders interact with 3D data, with the potential to improve geoscience communication and environmental literacy. No other manufacturing technology can convert digital data into tangible objects quickly at relatively low cost; however, the expertise necessary to produce a 3D-printed terrain model can be a substantial burden: knowledge of geographical information systems, computer aided design (CAD) software, and 3D printers may all be required. Furthermore, printing models larger than the build volume of a 3D printer can pose further technical hurdles. The TouchTerrain web-application simplifies DDM for elevation data by generating digital 3D models customized for a specific 3D printer's capabilities. The only required user input is the selection of a region-of-interest using the provided web-application with a Google Maps-style interface. Publically available digital elevation data is processed via the Google Earth Engine API. To allow the manufacture of 3D terrain models larger than a 3D printer's build volume the selected area can be split into multiple tiles without third-party software. This application significantly reduces the time and effort required for a non-expert like an educator to obtain 3D terrain models for use in class. The web application is deployed at http://touchterrain.geol.iastate.edu, while source code and installation instructions for a server and a stand-alone version are available at Github: https://github.com/ChHarding/TouchTerrain_for_CAGEO."
}
@article{LIU20183,
title = "Deep Learning-based Multimodal Control Interface for Human-Robot Collaboration",
journal = "Procedia CIRP",
volume = "72",
pages = "3 - 8",
year = "2018",
note = "51st CIRP Conference on Manufacturing Systems",
issn = "2212-8271",
doi = "https://doi.org/10.1016/j.procir.2018.03.224",
url = "http://www.sciencedirect.com/science/article/pii/S2212827118303846",
author = "Hongyi Liu and Tongtong Fang and Tianyu Zhou and Yuquan Wang and Lihui Wang",
keywords = "Human-robot collaboration, Deep learning, Robot control",
abstract = "In human-robot collaborative manufacturing, industrial robot is required to dynamically change its pre-programmed tasks and collaborate with human operators at the same workstation. However, traditional industrial robot is controlled by pre-programmed control codes, which cannot support the emerging needs of human-robot collaboration. In response to the request, this research explored a deep learning-based multimodal robot control interface for human-robot collaboration. Three methods were integrated into the multimodal interface, including voice recognition, hand motion recognition, and body posture recognition. Deep learning was adopted as the algorithm for classification and recognition. Human-robot collaboration specific datasets were collected to support the deep learning algorithm. The result presented at the end of the paper shows the potential to adopt deep learning in human-robot collaboration systems."
}
@article{HATTORI2013755,
title = "Answering software evolution questions: An empirical evaluation",
journal = "Information and Software Technology",
volume = "55",
number = "4",
pages = "755 - 775",
year = "2013",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2012.09.001",
url = "http://www.sciencedirect.com/science/article/pii/S095058491200184X",
author = "Lile Hattori and Marco D’Ambros and Michele Lanza and Mircea Lungu",
keywords = "Software evolution, Empirical evaluation, Controlled experiment, Software change history, Mining software repositories",
abstract = "Context
Developers often need to find answers to questions regarding the evolution of a system when working on its code base. While their information needs require data analysis pertaining to different repository types, the source code repository has a pivotal role for program comprehension tasks. However, the coarse-grained nature of the data stored by commit-based software configuration management systems often makes it challenging for a developer to search for an answer.
Objective
We present Replay, an Eclipse plug-in that allows developers to explore the change history of a system by capturing the changes at a finer granularity level than commits, and by replaying the past changes chronologically inside the integrated development environment, with the source code at hand.
Method
We conducted a controlled experiment to empirically assess whether Replay outperforms a baseline (SVN client in Eclipse) on helping developers to answer common questions related to software evolution.
Results
The experiment shows that Replay leads to a decrease in completion time with respect to a set of software evolution comprehension tasks.
Conclusion
We conclude that there are benefits in using Replay over the state of the practice tools for answering questions that require fine-grained change information and those related to recent changes."
}
@article{MOUSTAFA20182763,
title = "Software bug prediction using weighted majority voting techniques",
journal = "Alexandria Engineering Journal",
volume = "57",
number = "4",
pages = "2763 - 2774",
year = "2018",
issn = "1110-0168",
doi = "https://doi.org/10.1016/j.aej.2018.01.003",
url = "http://www.sciencedirect.com/science/article/pii/S1110016818300747",
author = "Sammar Moustafa and Mustafa Y. ElNainay and Nagwa El Makky and Mohamed S. Abougabal",
keywords = "Modeling and prediction, Product metrics, Process metrics, Classifier design and evaluation",
abstract = "Mining software repositories is a growing research field where rich data available in the different development software repositories, are analyzed and cross-linked to uncover useful information. Bug prediction is one of the potential benefits that can be gained through mining software repositories. Predicting potential defects early as they are introduced to the version control system would definitely help in saving time and effort during testing or maintenance phases. In this paper, defect prediction models that uses ensemble classification techniques have been proposed. The proposed models have been applied using different sets of software metrics as attributes of the classification techniques and tested on datasets of different sizes. The results show that change metrics outperform static code metrics and the combined model of change and static code metrics. Ensembles tend to be more accurate than their base classifiers. Defect prediction models using change metrics and ensemble classifiers have revealed the best performance, especially when the datasets used have imbalanced class distribution."
}
@article{GUISE2017183,
title = "Medial Prefrontal Cortex Reduces Memory Interference by Modifying Hippocampal Encoding",
journal = "Neuron",
volume = "94",
number = "1",
pages = "183 - 192.e8",
year = "2017",
issn = "0896-6273",
doi = "https://doi.org/10.1016/j.neuron.2017.03.011",
url = "http://www.sciencedirect.com/science/article/pii/S0896627317301940",
author = "Kevin G. Guise and Matthew L. Shapiro",
keywords = "hippocampus, CA1, learning, memory, neuronal representation, prefrontal cortex, proactive interference, recall, retrieval",
abstract = "Summary
The prefrontal cortex (PFC) is crucial for accurate memory performance when prior knowledge interferes with new learning, but the mechanisms that minimize proactive interference are unknown. To investigate these, we assessed the influence of medial PFC (mPFC) activity on spatial learning and hippocampal coding in a plus maze task that requires both structures. mPFC inactivation did not impair spatial learning or retrieval per se, but impaired the ability to follow changing spatial rules. mPFC and CA1 ensembles recorded simultaneously predicted goal choices and tracked changing rules; inactivating mPFC attenuated CA1 prospective coding. mPFC activity modified CA1 codes during learning, which in turn predicted how quickly rats adapted to subsequent rule changes. The results suggest that task rules signaled by the mPFC become incorporated into hippocampal representations and support prospective coding. By this mechanism, mPFC activity prevents interference by “teaching” the hippocampus to retrieve distinct representations of similar circumstances."
}
@article{ELHEWEITY2018,
title = "Numerical simulation of buffeting longitudinal wind forces on buildings",
journal = "Alexandria Engineering Journal",
year = "2018",
issn = "1110-0168",
doi = "https://doi.org/10.1016/j.aej.2018.08.001",
url = "http://www.sciencedirect.com/science/article/pii/S1110016818301558",
author = "Mohamed M. El-Heweity and Mohamed H. Abdelnaby and Elsayed M. Eshra",
keywords = "Wind, Simulation, Spectral representation, Correlation, Buffeting",
abstract = "A developed numerical simulation technique for buffeting longitudinal wind forces on rectangular buildings is presented in this paper. This technique mainly aims to generate time histories with prescribed probabilistic characteristics for the longitudinal wind load turbulent component which is considered as a zero mean stationary multivariate one-dimensional stochastic process, then the instantaneous wind load time histories are obtained by adding each mean wind load component to the corresponding generated time history of the turbulent component. A simplified procedure, which takes the advantage of aerodynamic admittance function, is proposed to estimate the longitudinal wind load on buildings in the frequency domain. A very efficient simulation algorithm based on spectral representation method which depends on superposition of trigonometric functions with random phase angles is used to perform the transformation from the frequency domain to the time domain. A MATLAB function is coded to implement the proposed simulation technique. By testing the proposed technique statistically, it has been noted that there are good agreements between the temporal and target auto/cross-correlation functions of the simulated wind forces, and by testing it structurally, the generated instantaneous wind load time histories show a good ability in giving a reasonable dynamic response for the studied building."
}
@article{CAPEL201742,
title = "Teaching concurrent and parallel programming by patterns: An interactive ICT approach",
journal = "Journal of Parallel and Distributed Computing",
volume = "105",
pages = "42 - 52",
year = "2017",
note = "Keeping up with Technology: Teaching Parallel, Distributed and High-Performance Computing",
issn = "0743-7315",
doi = "https://doi.org/10.1016/j.jpdc.2017.01.010",
url = "http://www.sciencedirect.com/science/article/pii/S0743731517300163",
author = "Manuel I. Capel and Antonio J. Tomeu and Alberto G. Salguero",
keywords = "Teaching innovation, Teaching improvement, Virtual campus, ICT integration, Lecturing model, Concurrent programming, Parallel programming, Code, Performance improvement, Interactive theoretical teaching, Students, Patterns",
abstract = "The use of programming patterns is considered to be a conceptual aid for programmers for developing understandable and testable concurrent and parallel code which is not only well built but also safe. By using programming patterns and their implementations as computer programs, difficult new concepts can be smoothly taught in lectures to students who before trying this teaching approach would have been reluctant to enroll on Parallel and Concurrent Programming courses. The approach presented in this paper consists in changing the traditional programming teaching and learning model to one where students are first introduced to syntactical constructs through selected introductory program code-patterns. In the theory lessons that follow, through the use of laptops with multi-core processors and access to the Virtual Campus services of our university, the students are easily able to implement and master the new concepts as they are taught. This teaching experiment was implemented to teach a concurrent and real-time programming course which is part of the computer engineering (CE) degree and taught during the third semester of the CE curriculum. Evaluation of the students’ academic performance when they had been taught with this approach revealed a 20.6% improvement in the students’ end-of-course grades."
}
@article{ZHANG2016547,
title = "Version 3.0 of code Java for 3D simulation of the CCA model",
journal = "Computer Physics Communications",
volume = "207",
pages = "547 - 548",
year = "2016",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2016.05.031",
url = "http://www.sciencedirect.com/science/article/pii/S001046551630176X",
author = "Kebo Zhang and Junsen Zuo and Yifeng Dou and Chao Li and Hailing Xiong",
keywords = "Running efficiency, Simplification, Optimization",
abstract = "In this paper we provide a new version of program for replacing the previous version. The frequency of traversing the clusters-list was reduced, and some code blocks were optimized properly; in addition, we appended and revised the comments of the source code for some methods or attributes. The compared experimental results show that new version has better time efficiency than the previous version.
New version program summary
Program title: CCA v03 Program Files doi:10.17632/gzncs28f95.1 Licensing provisions: Apache-2.0 License Programming language: Java Journal Reference of previous version: Computer Physics Communications 204 (2016) 214–215. Does the new version supersede the previous version?: Yes Nature of problem: There is poor running efficiency in the previous version. In addition, some methods in Java entity classes are short of necessary optimization. Solution method: A number of redundant steps, and running frequency of loop statements, are optimized into a more reasonable range. Object oriented solution, easy to reuse, extend and customize, in any development environment which supports Java JDK. Reasons for the new version: 1. In the previous version [2], for a random selected cluster moving a unit length, it must firstly obtain the maximum diffusion coefficient Dmax [1] which is searched from the clusters-list. In fact, it is not necessary that the program traverse the clusters-list for every diffusion step. If there is no aggregation between two clusters in the process of diffusion, the program does not have to traverse the clusters-list to get the Dmax in this step, because of no change for every cluster’s property around the system. Dmax just be saved as a global variable which is used to store the latest maximum value. When aggregation takes place, we just update the current maximum value of Dmax, so greatly reduce loop steps. Although contrary to the principles of object-oriented programming to some extent, it is worthy to make such sacrifice. 2. Some methods in Java entity classes are short of necessary optimization, containing redundant codes which consume extra hardware resources. 3. The source code of old program lacks necessary explanations for some methods or attributes. Moreover, some code comments, which are incorrect or inexplicable, need revise. Summary of revisions: 1. Cut down the frequency of traversing the clusters-list. 2. Optimize some code blocks. 3. Append and revise the comments of the source code for some methods or attributes, so as to easily for users to understand, debug and maintain. Additional comments:Fig. 1Log–log plot of simulation time (measurement by millisecond) vs. number of particles (N). For constant concentration (C=0.01), N varies in 270, 640, 1250, 2160 and 3430. We provide a referential log–log plot of simulation time versus number of particles as shown in Fig. 1 by a general PC. In this simulation, constant concentration C is set to 0.01, diffusion exponent is set to 0.5, sticking probability exponent is set to 0, and absolute temperature is set to 298K, but the side length L of cube varies in 30, 40, 50, 60 and 70 (these parameters come from literature [1]). The number of particles N varies with the side length L(N=C∗L3), so we just adjust the side length to get different particle numbers. As we can see from Fig. 1, the coefficient of x is 2.2413 of the new version, which is near to quadratic complexity, the coefficient of x is 2.9367 of the previous version, which is very close to cubic complexity. Obviously, the new version has a better running efficiency than the previous version. The new program brings a high running speed, but cause another problem that one user cannot promptly click the “Suspend” button to see a snapshot what he or she wanted to see. For this fault, we append a “Slow down” checkbox which can reduce the running speed for users’ choice, so it improves user-experience. Running time: Determined by the initial parameters. References:[1]C. Li, H. Xiong, Computer Physics Communications 185 (2014) 3424–3429.[2]K. Zhang, H. Xiong, C. Li, Computer Physics Communications 204 (2016) 214–215."
}
@article{CLEARY2018,
title = "Socioemotional wealth in family firms: A longitudinal content analysis of corporate disclosures",
journal = "Journal of Family Business Strategy",
year = "2018",
issn = "1877-8585",
doi = "https://doi.org/10.1016/j.jfbs.2018.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S1877858517300529",
author = "Peter Cleary and Martin Quinn and Alonso Moreno",
keywords = "Socioemotional wealth, FIBER, Content analysis, Chairman’s Statement",
abstract = "Family business literature has noted the nature and presence of socioemotional wealth (SEW) in family firms. One method of observing SEW is by a five-dimension approach, collectively termed FIBER. While the dimensions are well defined, they have been critiqued, as have the theoretical foundations of SEW. Regardless, given the concept of SEW is about a decade old and the FIBER dimensions less so, it is reasonable to argue more research is needed. One potentially useful research approach is an historical one, which we will here term SEW history – the use of historical research to support (or question) the development of SEW as a concept. We undertake a content analysis of corporate disclosures through the Chairman’s Statement of two Irish family breweries over a period of about two decades. To conduct the analysis, we develop a coding scheme based on the FIBER dimensions and offer some research propositions around these dimensions of SEW being stable (or not) over time. Our findings reveal that the Chairman’s Statement does include FIBER dimensions in both breweries and they do change over time. Subsequent statistical analysis reveals significant differences in the FIBER dimensions between the two breweries and context is revealed as a key issue in the assessment of SEW, something prior research has noted. The study also raises some questions on the nature of some FIBER dimensions, in particular the “I” dimension."
}
@article{DAM2016137,
title = "Consistent merging of model versions",
journal = "Journal of Systems and Software",
volume = "112",
pages = "137 - 155",
year = "2016",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2015.06.044",
url = "http://www.sciencedirect.com/science/article/pii/S016412121500134X",
author = "Hoa Khanh Dam and Alexander Egyed and Michael Winikoff and Alexander Reder and Roberto E. Lopez-Herrejon",
keywords = "Model merging, Inconsistency management, Model versioning",
abstract = "While many engineering tasks can, and should be, manageable independently, it does place a great burden on explicit collaboration needs—including the need for frequent and incremental merging of artifacts that software engineers manipulate using these tools. State-of-the-art merging techniques are often limited to textual artifacts (e.g., source code) and they are unable to discover and resolve complex merging issues beyond simple conflicts. This work focuses on the merging of models where we consider not only conflicts but also arbitrary syntactic and semantic consistency issues. Consistent artifacts are merged fully automatically and only inconsistent/conflicting artifacts are brought to the users’ attention, together with a systematic proposal of how to resolve them. Our approach is neutral with regard to who made the changes and hence reduces the bias caused by any individual engineer’s limited point of view. Our approach also applies to arbitrary design or models, provided that they follow a well-defined metamodel with explicit constraints—the norm nowadays. The extensive empirical evaluation suggests that our approach scales to practical settings."
}
@article{JAMU2016107,
title = "Just in time? Using QR codes for multi-professional learning in clinical practice",
journal = "Nurse Education in Practice",
volume = "19",
pages = "107 - 112",
year = "2016",
issn = "1471-5953",
doi = "https://doi.org/10.1016/j.nepr.2016.03.007",
url = "http://www.sciencedirect.com/science/article/pii/S1471595316300099",
author = "Joseph Tawanda Jamu and Hannah Lowi-Jones and Colin Mitchell",
keywords = "Smartphones, Just-in-Time learning, QR codes, Clinical practice",
abstract = "Clinical guidelines and policies are widely available on the hospital intranet or from the internet, but can be difficult to access at the required time and place. Clinical staff with smartphones could use Quick Response (QR) codes for contemporaneous access to relevant information to support the Just in Time Learning (JIT-L) paradigm. There are several studies that advocate the use of smartphones to enhance learning amongst medical students and junior doctors in UK. However, these participants are already technologically orientated. There are limited studies that explore the use of smartphones in nursing practice. QR Codes were generated for each topic and positioned at relevant locations on a medical ward. Support and training were provided for staff. Website analytics and semi-structured interviews were performed to evaluate the efficacy, acceptability and feasibility of using QR codes to facilitate Just in Time learning. Use was intermittently high but not sustained. Thematic analysis of interviews revealed a positive assessment of the Just in Time learning paradigm and context-sensitive clinical information. However, there were notable barriers to acceptance, including usability of QR codes and appropriateness of smartphone use in a clinical environment. The use of Just in Time learning for education and reference may be beneficial to healthcare professionals. However, alternative methods of access for less technologically literate users and a change in culture of mobile device use in clinical areas may be needed."
}
@article{SWAMI2018120,
title = "Present status of theoretical understanding of charge changing processes at low beam energies",
journal = "Radiation Physics and Chemistry",
volume = "153",
pages = "120 - 130",
year = "2018",
issn = "0969-806X",
doi = "https://doi.org/10.1016/j.radphyschem.2018.08.034",
url = "http://www.sciencedirect.com/science/article/pii/S0969806X17312938",
author = "D.K. Swami and T. Nandi",
keywords = "Charge state distributions, Model calculations, Non-perturbative processes, Nonradiative electron capture, Wake and dynamic screening, Exit surface effects",
abstract = "A model for the evaluation of charge-state distributions of fast heavy ions in solid targets is being developed since late eighties in terms of ETACHA code. Time to time it is being updated to deal with more number of electrons and non-perturbative processes. The calculation approach of the recent one, which is formulated for handling the non-perturbative processes better, is different from the earlier ones. However, the experimental results for the projectiles up to 28 electrons can be compared with the predictions from any versions of ETACHA code. Though earlier versions are not meant for the non-perturbative cases, but the detail comparison suggests that predictions from an earlier version is somewhat superior to that of the recent version. However, certain difference up to 4 units of charge found between the earlier version and experimental results on the mean charge states and it is attributed to nonradiative electron capture taking place at the exit surface in the influence of wake and dynamic screening effects. This mechanism can play its role in multiply charge formation in the electrospray ionization of big molecules."
}
@article{KAUR201931,
title = "Cognitive complexity as a quantifier of version to version Java-based source code change: An empirical probe",
journal = "Information and Software Technology",
volume = "106",
pages = "31 - 48",
year = "2019",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.09.002",
url = "http://www.sciencedirect.com/science/article/pii/S0950584918301903",
author = "Loveleen Kaur and Ashutosh Mishra",
keywords = "Cognitive complexity, Software change, Software metrics, Logistic regression analysis, Machine learning",
abstract = "Context
It has been often argued that it is challenging to modify code fragments from existing software that contains files that are difficult to comprehend. Since systematic software maintenance includes an extensive human activity, cognitive complexity is one of the intrinsic factors that could potentially contribute to or impede an efficient software maintenance practice, the empirical validation of which remains vastly unaddressed.
Objective
This study conducts an experimental analysis in which the software developer's level of difficulty in comprehending the software: the cognitive complexity, is theoretically computed and empirically evaluated for estimating its relevance to actual software change.
Method
For multiple successive releases of two Java-based software projects, where the source code of a previous release has been substantively used in a novel release, we calculate the change results and the values of the cognitive complexity for each of the version's source code Java files. We construct eight datasets and build predictive models using statistical analysis and machine learning techniques.
Results
The pragmatic comparative examination of the estimated cognitive complexity against prevailing metrics of software change and software complexity clearly validates the cognitive complexity metric as a noteworthy measure of version to version source code change."
}
@article{WANG2019134,
title = "Bayesian denoising hashing for robust image retrieval",
journal = "Pattern Recognition",
volume = "86",
pages = "134 - 142",
year = "2019",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2018.09.006",
url = "http://www.sciencedirect.com/science/article/pii/S003132031830325X",
author = "Dong Wang and Ge Song and Xiaoyang Tan",
keywords = "Image retrieval, Denoising hashing, Probabilistic model, Variational Bayes",
abstract = "Learning to hash is one of the most popular techniques in image retrieval, but few work investigates its robustness to noise corrupted images in which the unknown pattern of noise would heavily deteriorate the performance. To deal with this issue, we present in this paper a Bayesian denoising hashing algorithm whose output can be regarded a denoised version of the input hash code. We show that our method essentially seeks to reconstruct a new but more robust hash code by preserving the original input information while imposing extra constraints so as to correct the corrupted bits. We optimized this model in variational Bayes framework which has a closed-form update in each iteration that is more efficient than numerical optimization. Furthermore, our method can be added at the top of any original hashing layer, serving as a post-processing denoising layer with no change to previous training procedure. Experiments on three popular datasets demonstrate that the proposed method yields robust and meaningful hash code, which significantly improves the performance of state-of-the-art hash learning methods on challenging tasks such as large-scale natural image retrieval and retrieval with corrupted images."
}
@article{OGUSLU201892,
title = "Detection of seagrass scars using sparse coding and morphological filter",
journal = "Remote Sensing of Environment",
volume = "213",
pages = "92 - 103",
year = "2018",
issn = "0034-4257",
doi = "https://doi.org/10.1016/j.rse.2018.05.009",
url = "http://www.sciencedirect.com/science/article/pii/S003442571830230X",
author = "E. Oguslu and Kazi Islam and Daniel Perez and V.J. Hill and W.P. Bissett and R.C. Zimmerman and J. Li",
keywords = "Remote sensing, Sparse coding, Seagrass scar detection",
abstract = "The proximity of seagrass meadows to centers of human activity makes them vulnerable to a variety of habitat degrading insults. Physical scarring has long been recognized as an important but difficult-to-quantify source of habitat fragmentation and seagrass loss. We present a pixel-based algorithm to detect seafloor propeller seagrass scars in shallow water that promises to automate the detection and measurement of scars across the submarine landscape.11A preliminary version of the paper was presented at the SPIE Remote Sensing Conference, Amsterdam, Netherlands, 2014 (Oguslu et al., 2014). We applied the algorithm to multispectral and panchromatic images captured at the Deckle Beach, Florida using the WorldView-2 commercial satellite. The algorithm involves four steps using spectral and spatial information from radiometrically calibrated multispectral and panchromatic images. First, we fused multispectral and panchromatic images using a principal component analysis (PCA)-based pan-sharpening method to obtain multispectral pan-sharpened bands. In the second step, we enhanced the image contrast of the pan-sharpened bands for better scar detection. In the third step, we classified the contrast enhanced image pixels into scar and non-scar categories based on a sparse coding algorithm that produced an initial scar map in which false positive scar pixels were also present. In the fourth step, we applied post-processing techniques including a morphological filter and local orientation to reduce false positives. Our results show that the proposed method may be implemented on a regular basis to monitor changes in habitat characteristics of coastal waters."
}
@article{CHU2016174,
title = "Balancing the Robustness and Efficiency of Odor Representations during Learning",
journal = "Neuron",
volume = "92",
number = "1",
pages = "174 - 186",
year = "2016",
issn = "0896-6273",
doi = "https://doi.org/10.1016/j.neuron.2016.09.004",
url = "http://www.sciencedirect.com/science/article/pii/S0896627316305633",
author = "Monica W. Chu and Wankun L. Li and Takaki Komiyama",
abstract = "Summary
For reliable stimulus identification, sensory codes have to be robust by including redundancy to combat noise, but redundancy sacrifices coding efficiency. To address how experience affects the balance between the robustness and efficiency of sensory codes, we probed odor representations in the mouse olfactory bulb during learning over a week, using longitudinal two-photon calcium imaging. When mice learned to discriminate between two dissimilar odorants, responses of mitral cell ensembles to the two odorants gradually became less discrete, increasing the efficiency. In contrast, when mice learned to discriminate between two very similar odorants, the initially overlapping representations of the two odorants became progressively decorrelated, enhancing the robustness. Qualitatively similar changes were observed when the same odorants were experienced passively, a condition that would induce implicit perceptual learning. These results suggest that experience adjusts odor representations to balance the robustness and efficiency depending on the similarity of the experienced odorants."
}