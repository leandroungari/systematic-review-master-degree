@INPROCEEDINGS{6405336, 
author={S. Hayashi and T. Omori and T. Zenmyo and K. Maruyama and M. Saeki}, 
booktitle={2012 28th IEEE International Conference on Software Maintenance (ICSM)}, 
title={Refactoring edit history of source code}, 
year={2012}, 
volume={}, 
number={}, 
pages={617-620}, 
abstract={This paper proposes a concept for refactoring an edit history of source code and a technique for its automation. The aim of our history refactoring is to improve the clarity and usefulness of the history without changing its overall effect. We have defined primitive history refactorings including their preconditions and procedures, and large refactorings composed of these primitives. Moreover, we have implemented a supporting tool that automates the application of history refactorings in the middle of a source code editing process. Our tool enables developers to pursue some useful applications using history refactorings such as task level commit from an entangled edit history and selective undo of past edit operations.}, 
keywords={configuration management;software maintenance;source code edit primitive history refactoring;history usefulness improvement;history clarity improvement;task level commit;edit operation selective undo;History;Conferences;Software maintenance;Browsers;Usability;Electronic mail;refactoring;edit history;software configuration management}, 
doi={10.1109/ICSM.2012.6405336}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{7476693, 
author={Y. Nishimura and K. Maruyama}, 
booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Supporting Merge Conflict Resolution by Using Fine-Grained Code Change History}, 
year={2016}, 
volume={1}, 
number={}, 
pages={661-664}, 
abstract={Modern version control systems facilitate concurrent work in software development by providing a mechanism to merge revisions that are independently modified by multiple programmers. However, merge conflicts might emerge due to concurrent modifications, and their resolution might require the programmers to scrutinize every modification in the revisions. This paper presents a tool that can alleviate this cumbersome task of merging conflicting revisions. Our tool exploits the fine-grained edit operation history of Java source code and extracts only the edit operations that affect the revision of a particular class member. By just replaying the extracted edit operations, it helps programmers detect merge conflicts between class members within the two revisions and understand the modifications of the conflicting class members. Moreover, it can artificially merge two snapshots that appear during the evolution of the two revisions and show the programmers a unique artificial snapshot that is consistent with both the merged snapshots. By replaying the fine-grained edits that cause merge conflicts and showing the apparent snapshots with no conflicts as hints of the merged revision, the tool reduces the burden of inspecting the code changes behind the conflicts and reconciling the conflicting revisions.}, 
keywords={configuration management;Java;software engineering;source code (software);fine-grained code change history;modern version control systems;software development;Java source code;concurrent modifications;History;Merging;Inspection;Software;Java;Control systems;Real-time systems;version control;merge conflicts;distributed software development;code changes}, 
doi={10.1109/SANER.2016.46}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8323089, 
author={M. Sasaki and S. Matsumoto and S. Kusumoto}, 
booktitle={2018 IEEE Workshop on Mining and Analyzing Interaction Histories (MAINT)}, 
title={Integrating source code search into git client for effective retrieving of change history}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In order to achieve effective development management, it is important to manipulate and understand the change histories of source code in a repository. Although general version control systems provide change history manipulation, these systems are restricted to line-based and textual operations such as grep and diff. As such, these systems cannot follow the syntax/semantics of the source code. While various studies have examined querying and searching source codes, these methods cannot follow historical changes. The key concept of this paper is the integration of a source code search technique into Git commands that manipulate historical data in a repository. This paper presents MJgit, a prototype tool for achieving the above goal. In order to evaluate the proposed tool, we conducted a performance experiment using actual software repositories.}, 
keywords={data handling;query processing;source code (software);git client;general version control systems;change history manipulation;source code search technique;development management;search code querying;software repositories;History;Tools;Computer bugs;Software;Java;Standards;Prototypes;Code change history;source code search;MJgit;Git;abstract syntax tree}, 
doi={10.1109/MAINT.2018.8323089}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6805419, 
author={E. Kitsu and T. Omori and K. Maruyama}, 
booktitle={2013 20th Asia-Pacific Software Engineering Conference (APSEC)}, 
title={Detecting Program Changes from Edit History of Source Code}, 
year={2013}, 
volume={1}, 
number={}, 
pages={299-306}, 
abstract={Detecting program changes helps maintainers to figure out the evolution of the changed program. For this, several line-based difference tools have been proposed, which extract differences between two versions of the program. Unfortunately, these tools do not provide enough support to program comprehension since a single commitment stored in a version control system contains multiple changes that are intermingled with each other. Therefore, the maintainers have to untangle them by hand. This work is troublesome and time-consuming. This paper proposes a novel mechanism that automatically detects individual program changes. For this, it restores snapshots of the program from the history of edit operations for the target source code and compares class members that result from syntax analysis for respective snapshots. In addition, the mechanism provides several options of aggregating fine-grained changes detected based on the edit history. The maintainers can select their suitable levels of summarization of program changes. The paper also shows experimental results with a running implementation of the change detection tool. Through the experiment, the detection mechanism presents various kinds of summarized information on program changes, which might facilitate maintainers' activities for program comprehension.}, 
keywords={configuration management;software maintenance;source code (software);program change detection;source code edit history;changed program evolution;line-based difference tools;program comprehension;version control system;class members;syntax analysis;program change summarization;History;Java;Educational institutions;Syntactics;Control systems;Software;Software change detection;program analysis;software evolution;change-aware development environment}, 
doi={10.1109/APSEC.2013.48}, 
ISSN={1530-1362}, 
month={Dec},}
@INPROCEEDINGS{6693086, 
author={F. Palomba and G. Bavota and M. Di Penta and R. Oliveto and A. De Lucia and D. Poshyvanyk}, 
booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Detecting bad smells in source code using change history information}, 
year={2013}, 
volume={}, 
number={}, 
pages={268-278}, 
abstract={Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately. We propose an approach to detect five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy, by exploiting change history information mined from versioning systems. We applied approach, coined as HIST (Historical Information for Smell deTection), to eight software projects written in Java, and wherever possible compared with existing state-of-the-art smell detectors based on source code analysis. The results indicate that HIST's precision ranges between 61% and 80%, and its recall ranges between 61% and 100%. More importantly, the results confirm that HIST is able to identify code smells that cannot be identified through approaches solely based on code analysis.}, 
keywords={fault tolerant computing;software maintenance;software management;source code (software);bad smells detection;change history information;code smells;fault proneness;code elements;structural information;divergent change;shotgun surgery;parallel inheritance;blob;feature envy;versioning systems;HIST;Historical Information for Smell deTection;software projects;Java;smell detectors;source code analysis;History;Feature extraction;Surgery;Association rules;Detectors;Measurement;Code Smells;Change History Information}, 
doi={10.1109/ASE.2013.6693086}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7081858, 
author={S. Hayashi and D. Hoshino and J. Matsuda and M. Saeki and T. Omori and K. Maruyama}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Historef: A tool for edit history refactoring}, 
year={2015}, 
volume={}, 
number={}, 
pages={469-473}, 
abstract={This paper presents Historef, a tool for automating edit history refactoring on Eclipse IDE for Java programs. The aim of our history refactorings is to improve the understandability and/or usability of the history without changing its whole effect. Historef enables us to apply history refactorings to the recorded edit history in the middle of the source code editing process by a developer. By using our integrated tool, developers can commit the refactored edits into underlying SCM repository after applying edit history refactorings so that they are easy to manage their changes based on the performed edits.},
keywords={configuration management;Java;programming environments;software maintenance;software management;source code (software);Historef;edit history refactoring;Eclipse IDE;Java program;understandability;usability;source code editing process;integrated tool;SCM repository;History;Usability;Java;Electronic mail;Color;software evolution;refactoring;tangled changes}, 
doi={10.1109/SANER.2015.7081858}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{6967127, 
author={Z. Yuan and C. Liu and L. Yu and L. Zhang}, 
booktitle={Proceedings of 2013 3rd International Conference on Computer Science and Network Technology}, 
title={ChangeChecker: A tool for defect prediction in source code changes based on incremental learning method}, 
year={2013}, 
volume={}, 
number={}, 
pages={349-354}, 
abstract={In software development process, software developers may introduce defects as they make changes to software projects. Being aware of introduced defects immediately upon the completion of the change would allow software developers or testers to allocate more resources of testing and inspecting on the current risky change timely, which can shorten the process of defect finding and fixing effectively. In this paper, we propose a software tool called ChangeChecker to help software developers predict whether current source code change has any defects or not during the software development process. This tool infers the existence of defect by dynamically mining patterns of the source code changes in the revision history of the software project. It mainly consists of three components: (1) incremental feature collection and transformation, (2) real-time defect prediction for source code changes, and (3) dynamic update of the learning model. The tool has been evaluated in a large famous open source project Eclipse and applied to a real software development scenario.}, 
keywords={data mining;learning (artificial intelligence);program testing;project management;public domain software;software management;source code (software);defect prediction tool;source code changes;incremental learning method;software development process;software projects;resource allocation;software testing;software inspection;ChangeChecker software tool;dynamic pattern mining;incremental feature collection;incremental feature transformation;real-time defect prediction;dynamic learning model update;Eclipse open source project;Software;Measurement;Feature extraction;History;Complexity theory;Predictive models;Control systems;software engineering;source code change;defect prediction;incremental learning}, 
doi={10.1109/ICCSNT.2013.6967127}, 
ISSN={}, 
month={Oct},}
@ARTICLE{8574031, 
author={}, 
journal={NESC Handbook, Premier Edition – Spanish version - A Discussion of the National Electrical Safety Code(R) + COMENTARIOS EN ESPAÑOL}, 
title={NESC Handbook, Premier Edition – Spanish version - A Discussion of the National Electrical Safety Code(R) plus COMENTARIOS EN ESPAÑOL}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-826}, 
abstract={}, 
keywords={National Electric Code;Electrical safety;Electricity supply industry;Occupational safety;Voltage control;Underground power cables;Spanish;communications industry safety; construction of communication lines; construction of electric supply lines; electrical safety; electric supply stations; electric utility stations; high-voltage safety; operation of communications sys­tems; operation of electric supply systems; power station equipment; power station safety; public utility safety; safety work rules; underground communication line safety; underground electric line safety}, 
doi={10.1109/IEEESTD.2018.8574031}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8530046, 
author={X. Liu and L. Huang and C. Li and V. Ng}, 
booktitle={2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Linking Source Code to Untangled Change Intents}, 
year={2018}, 
volume={}, 
number={}, 
pages={393-403}, 
abstract={Previous work [13] suggests that tangled changes (i.e., different change intents aggregated in one single commit message) could complicate tracing to different change tasks when developers manage software changes. Identifying links from changed source code to untangled change intents could help developers solve this problem. Manually identifying such links requires lots of experience and review efforts, however. Unfortunately, there is no automatic method that provides this capability. In this paper, we propose AutoCILink, which automatically identifies code to untangled change intent links with a pattern-based link identification system (AutoCILink-P) and a supervised learning-based link classification system (AutoCILink-ML). Evaluation results demonstrate the effectiveness of both systems: the pattern-based AutoCILink-P and the supervised learning-based AutoCILink-ML achieve average accuracy of 74.6% and 81.2%, respectively.}, 
keywords={learning (artificial intelligence);pattern classification;software maintenance;source code (software);linking source code;single commit message;software changes;changed source code;untangled change intent links;pattern-based link identification system;supervised learning-based link classification system;pattern-based AutoCILink-P;supervised learning-based AutoCILink-ML achieve average accuracy;Software;Task analysis;Computer bugs;Machine learning;Cognition;Gold;Additives;Commit, Code change, Machine learning}, 
doi={10.1109/ICSME.2018.00047}, 
ISSN={2576-3148}, 
month={Sep.},}
@INPROCEEDINGS{7882009, 
author={L. Kumar and S. K. Rath and A. Sureka}, 
booktitle={2017 IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE)}, 
title={Using source code metrics to predict change-prone web services: A case-study on ebay services}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Predicting change-prone object-oriented software using source code metrics is an area that has attracted several researchers attention. However, predicting change-prone web services in terms of changes in the WSDL (Web Service Description Language) Interface using source code metrics implementing the services is a relatively unexplored area. We conduct a case-study on change proneness prediction on an experimental dataset consisting of several versions of eBay web services wherein we compute the churn between different versions of the WSDL interfaces using the WSDLDiff Tool. We compute 21 source code metrics using Chidamber and Kemerer Java Metrics (CKJM) extended tool serving as predictors and apply Least Squares Support Vector Machines (LSSVM) based technique to develop a change proneness estimator. Our experimental results demonstrates that a predictive model developed using all 21 metrics and linear kernel yields the best results.}, 
keywords={least squares approximations;object-oriented programming;software metrics;source code (software);support vector machines;Web Services Business Process Execution Language;source code metrics;change-prone Web services;ebay services;object-oriented software;WSDL interface;Web service description language interface;WSDLDiff tool;Chidamber and Kemerer Java metrics;CKJM;least squares support vector machines;LSSVM;change proneness estimator;Measurement;Web services;Correlation;Java;Support vector machines;Predictive models;Kernel;Change Proneness;Kernel Based Machine Learning;Least Squares Support Vector Machines (LSSVM);Source Code Metrics;Web Services}, 
doi={10.1109/MALTESQUE.2017.7882009}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7781814, 
author={L. Moonen and S. D. Alesio and T. Rolfsnes and D. W. Binkley}, 
booktitle={2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
title={Exploring the Effects of History Length and Age on Mining Software Change Impact}, 
year={2016}, 
volume={}, 
number={}, 
pages={207-216}, 
abstract={The goal of Software Change Impact Analysis is to identify artifacts (typically source-code files) potentially affected by a change. Recently, there is an increased interest in mining software change impact based on evolutionary coupling. A particularly promising approach uses association rule mining to uncover potentially affected artifacts from patterns in the system's change history. Two main considerations when using this approach are the history length, the number of transactions from the change history used to identify the impact of a change, and history age, the number of transactions that have occurred since patterns were last mined from the history. Although history length and age can significantly affect the quality of mining results, few guidelines exist on how to best select appropriate values for these two parameters. In this paper, we empirically investigate the effects of history length and age on the quality of change impact analysis using mined evolutionary couplings. Specifically, we report on a series of systematic experiments involving the change histories of two large industrial systems and 17 large open source systems. In these experiments, we vary the length and age of the history used to mine software change impact, and assess how this affects precision and applicability. Results from the study are used to derive practical guidelines for choosing history length and age when applying association rule mining to conduct software change impact analysis.}, 
keywords={data mining;public domain software;software maintenance;source code (software);history length effect;age effect;software change impact mining;software change impact analysis;artifact identification;source-code files;evolutionary coupling;association rule mining;industrial systems;open source systems;History;Data mining;Couplings;Software systems;Guidelines;Computer bugs;change impact analysis;evolutionary coupling;association rule mining;parameter tuning}, 
doi={10.1109/SCAM.2016.9}, 
ISSN={2470-6892}, 
month={Oct},}
@INPROCEEDINGS{7582801, 
author={G. Dotzler and M. Philippsen}, 
booktitle={2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Move-optimized source code tree differencing}, 
year={2016}, 
volume={}, 
number={}, 
pages={660-671}, 
abstract={When it is necessary to express changes between two source code files as a list of edit actions (an edit script), modern tree differencing algorithms are superior to most text-based approaches because they take code movements into account and express source code changes more accurately. We present 5 general optimizations that can be added to state-of-the-art tree differencing algorithms to shorten the resulting edit scripts. Applied to Gumtree, RTED, JSync, and ChangeDistiller, they lead to shorter scripts for 1898% of the changes in the histories of 9 open-source software repositories. These optimizations also are parts of our novel Move-optimized Tree DIFFerencing algorithm (MTD-IFF) that has a higher accuracy in detecting moved code parts. MTDIFF (which is based on the ideas of ChangeDistiller) further shortens the edit script for another 20% of the changes in the repositories. MTDIFF and all the benchmarks are available under an open-source license.}, 
keywords={optimisation;public domain software;software maintenance;source code (software);move-optimized source code tree differencing algorithm;source code files;edit actions;edit script;text-based approaches;code movements;source code changes;optimizations;Gumtree;RTED;JSync;ChangeDistiller;open-source software repositories;MTD-IFF;moved code parts;open-source license;software maintenance;Optimization;Open source software;Runtime;Complexity theory;Programming;History;Tree Differencing;Optimizations;Source Code}, 
doi={}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8039090, 
author={Y. Huang and Z. Liu and X. Chen and X. Luo}, 
booktitle={2016 6th International Conference on Digital Home (ICDH)}, 
title={Automatic Matching Release Notes and Source Code by Generating Summary for Software Change}, 
year={2016}, 
volume={}, 
number={}, 
pages={104-109}, 
abstract={To quickly locate the source code that maps to a specific change described in change history, establishing traceability links between release notes and source code is a necessary task. Current works on the traceability link recovery can be used to find out source code changes which are of higher textual similarities with the release note. However, these approaches rely on consistency of the text used in artifacts at various abstraction levels, and the completeness of text descriptions. In this paper, we propose to leverage source code change information for improving the accuracy of release note to source code traceability recovery tasks. In order to reduce the complexity of link recovery, our approach first performs change impact analysis to cluster the source code changes for the same purpose as a virtual class. After that, our approach employs a natural language generation algorithm to generate readable summary sentence for each virtual class. The traceability links are built between release notes and clusters of program entities by computing the linguistic similarity of sentences. We conduct case studies on 26 releases of 3 popular softwares to evaluate the approach, and the results indicate that our proposed method can improve the accuracy of traceability link recovery compared to other IR-based techniques.}, 
keywords={natural language processing;program diagnostics;software maintenance;source code (software);text analysis;source code change information;IR-based techniques;sentences linguistic similarity;readable summary sentence;natural language generation algorithm;virtual class;change history;specific change;software change;automatic matching release notes;traceability link recovery;source code traceability recovery tasks;Software;Pragmatics;Computer bugs;Natural languages;Syntactics;Semantics;History;Summary Generation;Traceability Link Recovery;Syntactic and Semantic Similarity}, 
doi={10.1109/ICDH.2016.031}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7476695, 
author={T. Molderez and C. De Roover}, 
booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Automated Generalization and Refinement of Code Templates with Ekeko/X}, 
year={2016}, 
volume={1}, 
number={}, 
pages={669-672}, 
abstract={Code templates are an intuitive means to specify source code snippets of interest, such as all instances of a bug, groups of snippets that need to be refactored or transformed, or instances of design patterns. While intuitive, it is not always straightforward to write a template that produces only the desired matches. A template could produce either more snippets than desired, or too few. To assist the users of EKEKO/X, our template-based search and transformation tool for Java, we have extended it with two components: The first is a suite of mutation operators that simplifies the process of modifying templates. The second is a system that can automatically suggest a sequence of mutations to a given template, such that it matches only with a set of desired snippets. In this tool paper, we highlight the key design decisions in implementing these two components of EKEKO/X, and demonstrate their use by walking through an example sequence of mutations suggested by the system.}, 
keywords={application program interfaces;Java;source code (software);automated generalization;code template refinement;source code snippets;EKEKO/X;template-based search tool;transformation tool;Java;mutation operator;Java;Writing;History;Software;Concrete;Syntactics;User interfaces;program search and transformation tools;source code templates;search-based software engineering;genetic search algorithm}, 
doi={10.1109/SANER.2016.19}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6498479, 
author={Z. Tóth and G. Novák and R. Ferenc and I. Siket}, 
booktitle={2013 17th European Conference on Software Maintenance and Reengineering}, 
title={Using Version Control History to Follow the Changes of Source Code Elements}, 
year={2013}, 
volume={}, 
number={}, 
pages={319-322}, 
abstract={Version control systems store the whole history of the source code. Since the source code of a system is organized into files and folders, the history tells us the concerned files and their changed lines only but software engineers are also interested in which source code elements (e.g. classes or methods) are affected by a change. Unfortunately, in most programming languages source code elements do not follow the file system hierarchy, which means that a file can contain more classes and methods and a class can be stored in more files, which makes it difficult to determine the changes of classes by using the changes of files. To solve this problem we developed an algorithm, which is able to follow the changes of the source code elements by using the changes of files and we successfully applied it on the Web Kit open source system.}, 
keywords={configuration management;file organisation;Internet;public domain software;version control history;source code element;version control system;software engineer;programming language;file system hierarchy;file change;Web Kit open source system;Software;History;Control systems;Data mining;Measurement;Algorithm design and analysis;Software algorithms;Version Control System;Repository Mining;Static Analysis}, 
doi={10.1109/CSMR.2013.40}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{6650547, 
author={F. Servant and J. A. Jones}, 
booktitle={2013 First IEEE Working Conference on Software Visualization (VISSOFT)}, 
title={Chronos: Visualizing slices of source-code history}, 
year={2013}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={In this paper, we present CHRONOS-a tool that enables the querying, exploration, and discovery of historical change events to source code. Unlike traditional Revision-Control-System tools, CHRONOS allows queries across any subset of the code, down to the line-level, which can potentially be contiguous or disparate, even among multiple files. In addition, CHRONOS provides change history across all historical versions (i.e., it is not limited to a pairwise “diff”). The tool implements a zoom-able user interface as a visualization of the history of the queried code to provide both a high-level view of the changes, which supports pattern recognition and discovery, and a low-level view that supports semantic comprehension for tasks such as reverse engineering and identifying design rationale. In this paper, we describe use cases in which CHRONOS may be helpful, provide a motivating example to demonstrate the benefits brought by CHRONOS, and describe its visualization in detail.}, 
keywords={data visualisation;pattern recognition;reverse engineering;source coding;user interfaces;chronos;visualizing slices;source code history;historical change events;revision control system tools;change history;zoom able user interface;visualization;queried code;high level view;pattern recognition;low level view;semantic comprehension;reverse engineering;identifying design rationale;History;Visualization;Conferences;Navigation;Software systems;Data mining}, 
doi={10.1109/VISSOFT.2013.6650547}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8530115, 
author={Y. Arimatsu and Y. Ishida and K. Noda and T. Kobayashi}, 
booktitle={2018 IEEE Third International Workshop on Dynamic Software Documentation (DySDoc3)}, 
title={Enriching API Documentation by Relevant API Methods Recommendation Based on Version History}, 
year={2018}, 
volume={}, 
number={}, 
pages={15-16}, 
abstract={Application programming interfaces (APIs) enable developers to increase their productivity; however, developers have to expend much of their time to search API usages because API documentation often lacks some types of information such as co-use relationships between API methods or code examples. In this paper, we propose a technique to automatically generate enriched API documentation. Our technique identifies groups of relevant API methods by analyzing a software change history and inserts those into standard API documentation. Using our document, developers can easily find relevant API methods interactively according to their various development purposes.}, 
keywords={application program interfaces;configuration management;software maintenance;system documentation;version history;application programming interfaces;API usages;code examples;standard API documentation;software change history;Documentation;History;Standards;Java;Software;Feature extraction;Computer bugs;document-generation;mining-software-version-history;software-maintenance},
doi={10.1109/DySDoc3.2018.00014}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7816466, 
author={W. Lin and Z. Chen and W. Ma and L. Chen and L. Xu and B. Xu}, 
booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={An Empirical Study on the Characteristics of Python Fine-Grained Source Code Change Types}, 
year={2016}, 
volume={}, 
number={}, 
pages={188-199}, 
abstract={Software has been changing during its whole life cycle. Therefore, identification of source code changes becomes a key issue in software evolution analysis. However, few current change analysis research focus on dynamic language software. In this paper, we pay attention to the fine-grained source code changes of Python software. We implement an automatic tool named PyCT to extract 77 kinds of fine-grained source code change types from commit history information. We conduct an empirical study on ten popular Python projects from five domains, with 132294 commits, to investigate the characteristics of dynamic software source code changes. Analyzing the source code changes in four aspects, we distill 11 findings, which are summarized into two insights on software evolution: change prediction and fault code fix. In addition, we provide direct evidence on how developers use and change dynamic features. Our results provide useful guidance and insights for improving the understanding of source code evolution of dynamic language software.}, 
keywords={high level languages;software fault tolerance;software maintenance;source code (software);fine-grained source code change types;software life cycle;software evolution analysis;dynamic language software;Python software;automatic tool;PyCT;dynamic software source code changes;change prediction;fault code fix;source code evolution;Software;Maintenance engineering;Feature extraction;Data mining;Computer bugs;Taxonomy;Heuristic algorithms;fine-grained change types;Python;software evolution}, 
doi={10.1109/ICSME.2016.25}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7054763, 
author={E. Kodhai and B. Dhivya}, 
booktitle={2014 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2014]}, 
title={Detecting and investigating the source code changes using logical rules}, 
year={2014}, 
volume={}, 
number={}, 
pages={1603-1608}, 
abstract={Software developers often need to examine program differences between two versions and reason about the changes. Analyzing the changes is the task. To facilitate the programmers to represent the high level source code changes, this proposed system introduces the rule-based program differencing approach to represent the changes as logical rules. This approach is instantiated with three levels: first level describes the changes in method header names and signature; second level captures change in the code level and structural dependences; and third level identifies the same set of function with different name. This approach concisely represents the systematic changes and helps the software engineers to recognize the program differences. This approach can be applied in open source project to examine the difference among program version.}, 
keywords={logic programming;public domain software;software maintenance;source code (software);logical rules;rule-based program differencing approach;open source project;software engineers;high level source code changes;Software;Systematics;Computers;Syntactics;Merging;Crawlers;Abstracts;source code changes;logical rules;open source}, 
doi={10.1109/ICCPCT.2014.7054763}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6975661, 
author={L. F. Cortés-Coy and M. Linares-Vásquez and J. Aponte and D. Poshyvanyk}, 
booktitle={2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation}, 
title={On Automatically Generating Commit Messages via Summarization of Source Code Changes}, 
year={2014}, 
volume={}, 
number={}, 
pages={275-284}, 
abstract={Although version control systems allow developers to describe and explain the rationale behind code changes in commit messages, the state of practice indicates that most of the time such commit messages are either very short or even empty. In fact, in a recent study of 23K+ Java projects it has been found that only 10% of the messages are descriptive and over 66% of those messages contained fewer words as compared to a typical English sentence (i.e., 15-20 words). However, accurate and complete commit messages summarizing software changes are important to support a number of development and maintenance tasks. In this paper we present an approach, coined as Change Scribe, which is designed to generate commit messages automatically from change sets. Change Scribe generates natural language commit messages by taking into account commit stereotype, the type of changes (e.g., files rename, changes done only to property files), as well as the impact set of the underlying changes. We evaluated Change Scribe in a survey involving 23 developers in which the participants analyzed automatically generated commit messages from real changes and compared them with commit messages written by the original developers of six open source systems. The results demonstrate that automatically generated messages by Change Scribe are preferred in about 62% of the cases for large commits, and about 54% for small commits.}, 
keywords={configuration management;public domain software;software maintenance;source code (software);source code change summarization;version control systems;Java projects;descriptive messages;commit message summarizing software changes;software development task;software maintenance task;ChangeScribe;change sets;automatic natural language commit message generation;commit stereotype;change type;file rename;property file changes;open source systems;Java;Software;Abstracts;Context;Natural languages;Visualization;Production facilities;commit message;summarization;code changes}, 
doi={10.1109/SCAM.2014.14}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6976145, 
author={R. Stevens and C. D. Roover}, 
booktitle={2014 IEEE International Conference on Software Maintenance and Evolution}, 
title={Querying the History of Software Projects Using QWALKEKO}, 
year={2014}, 
volume={}, 
number={}, 
pages={585-588}, 
abstract={We present the QwalKeko meta-programming library for Clojure that enables querying the history of versioned software projects in a declarative manner. Unique to this library is its support for regular path expressions within history queries. Regular path expressions are akin to regular expressions, except that they match a sequence of successive snapshots of a software project along which user-specified logic conditions must hold. Such logic conditions can concern the source code within a snapshot, versioning information associated with the snapshot, as well as patterns of source code changes with respect to other snapshots. We have successfully used the resulting multi-faceted queries to detect refactorings in project histories. In this paper, we discuss how applicative logic meta-programming enabled combining the heterogenous components of QwalKeko into a uniform whole. We focus on the applicative logic interface to a new implementation of a well-known change distilling algorithm. We use the problem of detecting and categorizing changes made to Selenium-based test scripts for illustration purposes.}, 
keywords={logic programming;query processing;software maintenance;source code (software);history querying;software projects;QwalKeko metaprogramming library;Clojure;regular path expressions;user-specified logic conditions;source code;snapshot;versioning information;multifaceted queries;refactoring detection;project histories;heterogenous components;applicative logic interface;change distilling algorithm;SELENIUM-based test scripts;illustration purposes;applicative logic metaprogramming;History;Libraries;Database languages;Java;Software maintenance;Software engineering;declarative programming;program querying;history querying;mining software repositories}, 
doi={10.1109/ICSME.2014.101}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{7335397, 
author={M. Brandtner and P. Leitner and H. C. Gall}, 
booktitle={2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
title={Intent, tests, and release dependencies: Pragmatic recipes for source code integration}, 
year={2015}, 
volume={}, 
number={}, 
pages={11-20}, 
abstract={Continuous integration of source code changes, for example, via pull-request driven contribution channels, has become standard in many software projects. However, the decision to integrate source code changes into a release is complex and has to be taken by a software manager. In this work, we identify a set of three pragmatic recipes plus variations to support the decision making of integrating code contributions into a release. These recipes cover the isolation of source code changes, contribution of test code, and the linking of commits to issues. We analyze the development history of 21 open-source software projects, to evaluate whether, and to what extent, those recipes are followed in open-source projects. The results of our analysis showed that open-source projects largely follow recipes on a compliance level of &gt; 75%. Hence, we conclude that the identified recipes plus variations can be seen as wide-spread relevant best-practices for source code integration.}, 
keywords={decision making;program testing;project management;public domain software;source code (software);intent;release dependency;pragmatic recipe;source code integration;decision making;source code change isolation;test code contribution;open-source software projects;Interviews;Open source software;Companies;Guidelines;Pragmatics;Lead}, 
doi={10.1109/SCAM.2015.7335397}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7965270, 
author={K. Kevic}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)}, 
title={Using Eye Gaze Data to Recognize Task-Relevant Source Code Better and More Fine-Grained}, 
year={2017}, 
volume={}, 
number={}, 
pages={103-105}, 
abstract={Models to assess a source code element's relevancy for a given change task are the basis of many software engineering tools, such as recommender systems, for code comprehension. To improve such relevancy models and to aid developers in finding relevant parts in the source code faster, we studied developer's fine-grained navigation patterns with eye tracking technology. By combining the captured eye gaze data with interaction data of 12 developers working on a change task, we were able to identify relevant methods with high accuracy and improve precision and recall compared to the widely used click frequency technique by 77% and 24% respectively. Furthermore, we were able to show that the captured gaze data enables to retrace which source code lines developers found relevant. Our results thus provide evidence that eye gaze data can be used to improve existing models in terms of accuracy and granularity.}, 
keywords={software engineering;source code (software);eye gaze data;recognize task relevant source code;source code element relevancy;software engineering tools;fine grained navigation patterns;eye tracking technology;captured eye gaze data;click frequency technique;Software;Gaze tracking;Tools;Software engineering;Data models;Navigation;History;eye-gaze;relevancy model;recommender system}, 
doi={10.1109/ICSE-C.2017.152}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6363290, 
author={P. Bangcharoensap and A. Ihara and Y. Kamei and K. Matsumoto}, 
booktitle={2012 Fourth International Workshop on Empirical Software Engineering in Practice}, 
title={Locating Source Code to Be Fixed Based on Initial Bug Reports - A Case Study on the Eclipse Project}, 
year={2012}, 
volume={}, 
number={}, 
pages={10-15}, 
abstract={In most software development, a Bug Tracking System is used to improve software quality. Based on bug reports managed by the bug tracking system, triagers who assign a bug to fixers and fixers need to pinpoint buggy files that should be fixed. However if triagers do not know the details of the buggy file, it is difficult to select an appropriate fixer. If fixers can identify the buggy files, they can fix the bug in a short time. In this paper, we propose a method to quickly locate the buggy file in a source code repository using 3 approaches, text mining, code mining, and change history mining to rank files that may be causing bugs. (1) The text mining approach ranks files based on the textual similarity between a bug report and source code. (2) The code mining approach ranks files based on prediction of the fault-prone module using source code product metrics. (3) The change history mining approach ranks files based on prediction of the fault-prone module using change process metrics. Using Eclipse platform project data, our proposed model gains around 20% in TOP1 prediction. This result means that the buggy files are ranked first in 20% of bug reports. Furthermore, bug reports that consist of a short description and many specific words easily identify and locate the buggy file.}, 
keywords={data mining;program debugging;software metrics;text analysis;bug reports;eclipse project;software development;bug tracking system;software quality;triagers;buggy files;source code repository;text mining;code mining;rank files;textual similarity;fault prone module;source code product metrics;change history mining;change process metrics;Eclipse platform project data;Measurement;Accuracy;Text mining;History;Computer bugs;Mathematical model;Bug localization;Text mining;Code mining;Change history mining}, 
doi={10.1109/IWESEP.2012.14}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6405306, 
author={M. Linares-Vásquez and K. Hossen and H. Dang and H. Kagdi and M. Gethers and D. Poshyvanyk}, 
booktitle={2012 28th IEEE International Conference on Software Maintenance (ICSM)}, 
title={Triaging incoming change requests: Bug or commit history, or code authorship?}, 
year={2012}, 
volume={}, 
number={}, 
pages={451-460}, 
abstract={There is a tremendous wealth of code authorship information available in source code. Motivated with the presence of this information, in a number of open source projects, an approach to recommend expert developers to assist with a software change request (e.g., a bug fixes or feature) is presented. It employs a combination of an information retrieval technique and processing of the source code authorship information. The relevant source code files to the textual description of a change request are first located. The authors listed in the header comments in these files are then analyzed to arrive at a ranked list of the most suitable developers. The approach fundamentally differs from its previously reported counterparts, as it does not require software repository mining. Neither does it require training from past bugs/issues, which is often done with sophisticated techniques such as machine learning, nor mining of source code repositories, i.e., commits. An empirical study to evaluate the effectiveness of the approach on three open source systems, ArgoUML, JEdit, and MuCommander, is reported. Our approach is compared with two representative approaches: (1) using machine learning on past bug reports, and (2) based on commit logs. The presented approach is found to provide recommendation accuracies that are equivalent or better than the two compared approaches. These findings are encouraging, as it opens up a promising and orthogonal possibility of recommending developers without the need of any historical change information.}, 
keywords={information retrieval;learning (artificial intelligence);program debugging;public domain software;recommender systems;open source projects;bug fixing;information retrieval technique;source code authorship information;source code files;software change request textual description;header comments;ArgoUML;JEdit;MuCommander;machine learning;commit log history;recommendation accuracies;Data mining;Accuracy;Software maintenance;Unified modeling language;Large scale integration;Support vector machines;code authorship;information retrieval;change request;triaging;expert developer recommendations}, 
doi={10.1109/ICSM.2012.6405306}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{6494927, 
author={K. Maruyama and E. Kitsu and T. Omori and S. Hayashi}, 
booktitle={2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering}, 
title={Slicing and replaying code change history}, 
year={2012}, 
volume={}, 
number={}, 
pages={246-249}, 
abstract={Change-aware development environments have recently become feasible and reasonable. These environments can automatically record fine-grained code changes on a program and allow programmers to replay the recorded changes in chronological order. However, they do not always need to replay all the code changes to investigate how a particular entity of the program has been changed. Therefore, they often skip several code changes of no interest. This skipping action is an obstacle that makes many programmers hesitate in using existing replaying tools. This paper proposes a slicing mechanism that can extract only code changes necessary to construct a particular class member of a Java program from the whole history of past code changes. In this mechanism, fine-grained code changes are represented by edit operations recorded on source code of a program. The paper also presents a running tool that implements the proposed slicing and replays its resulting slices. With this tool, programmers can avoid replaying edit operations nonessential to the construction of class members they want to understand.}, 
keywords={Java;software maintenance;code change history;change-aware development environment;fine-grained code change;skipping action;slicing mechanism;Java program;edit operation;program source code;running tool;software maintenance;Code change;Integrated development environments;Program comprehension;Program slicing;Software maintenance and evolution}, 
doi={10.1145/2351676.2351713}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7884626, 
author={S. Proksch and S. Nadi and S. Amann and M. Mezini}, 
booktitle={2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={Enriching in-IDE process information with fine-grained source code history}, 
year={2017}, 
volume={}, 
number={}, 
pages={250-260}, 
abstract={Current studies on software development either focus on the change history of source code from version-control systems or on an analysis of simplistic in-IDE events without context information. Each of these approaches contains valuable information that is unavailable in the other case. Our work proposes enriched event streams, a solution that combines the best of both worlds and provides a holistic view on the software development process. Enriched event streams not only capture developer activities in the IDE, but also specialized context information, such as source-code snapshots for change events. To enable the storage of such code snapshots in an analyzable format, we introduce a new intermediate representation called Simplified Syntax Trees (SSTs) and build CA□RET, a platform that offers reusable components to conveniently work with enriched event streams. We implement FEEDBAG++, an instrumentation for Visual Studio that collects enriched event streams with code snapshots in the form of SSTs. We share a dataset of enriched event streams captured from 58 users and representing 915 days of work. Additionally, to demonstrate usefulness, we present three research applications that have already made use of CA□RET and FEEDBAG++.}, 
keywords={programming environments;software tools;source code (software);in-IDE process information;fine-grained source code history;software development;version-control systems;context information;code snapshots;simplified syntax trees;SSTs;CA□RET;FEEDBAG++;visual studio instrumentation;Context;History;Visualization;Software;Syntactics;Cognition;Instruments}, 
doi={10.1109/SANER.2017.7884626}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8530726, 
author={S. Pugh and D. Binkley and L. Moonen}, 
booktitle={2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
title={[Research Paper] The Case for Adaptive Change Recommendation}, 
year={2018}, 
volume={}, 
number={}, 
pages={129-138}, 
abstract={As the complexity of a software system grows, it becomes increasingly difficult for developers to be aware of all the dependencies that exist between artifacts (e.g., files or methods) of the system. Change impact analysis helps to overcome this problem, as it recommends to a developer relevant source-code artifacts related to her current changes. Association rule mining has shown promise in determining change impact by uncovering relevant patterns in the system's change history. State-of-the-art change impact mining algorithms typically make use of a change history of tens of thousands of transactions. For efficiency, targeted association rule mining focuses on only those transactions potentially relevant to answering a particular query. However, even targeted algorithms must consider the complete set of relevant transactions in the history. This paper presents ATARI, a new adaptive approach to association rule mining that considers a dynamic selection of the relevant transactions. It can be viewed as a further constrained version of targeted association rule mining, in which as few as a single transaction might be considered when determining change impact. Our investigation of adaptive change impact mining empirically studies seven algorithm variants. We show that adaptive algorithms are viable, can be just as applicable as the start-of-the-art complete-history algorithms, and even outperform them for certain queries. However, more important than the direct comparison, our investigation lays necessary groundwork for the future study of adaptive techniques and their application to challenges such as the on-the-fly style of impact analysis that is needed at the GitHub-scale.}, 
keywords={data mining;software maintenance;adaptive change recommendation;software system;change impact analysis;change history;adaptive approach;adaptive change impact;adaptive algorithms;adaptive techniques;source-code artifacts;association rule mining;complete-history algorithms;change impact mining algorithms;ATARI;History;Data mining;Adaptive algorithms;Heuristic algorithms;Software systems;Couplings;change impact analysis;evolutionary coupling;association rule mining}, 
doi={10.1109/SCAM.2018.00022}, 
ISSN={2470-6892}, 
month={Sep.},}
@INPROCEEDINGS{6688888, 
author={C. Tantithamthavorn and R. Teekavanich and A. Ihara and K. Matsumoto}, 
booktitle={2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)}, 
title={Mining A change history to quickly identify bug locations : A case study of the Eclipse project}, 
year={2013}, 
volume={}, 
number={}, 
pages={108-113}, 
abstract={In this study, we proposed an approach to mine a change history to improve the bug localization performance. The key idea is that a recently fixed file may be fixed in the near future. We used a combination of textual feature and mining the change history to recommend source code files that are likely to be fixed for a given bug report. First, we adopted the Vector Space Model (VSM) to find relevant source code files that are textually similar to the bug report. Second, we analyzed the change history to identify previously fixed files. We then estimated the fault proneness of these files. Finally, we combined the two scores, from textual similarity and fault proneness, for every source code file. We then recommend developers examine source code files with higher scores. We evaluated our approach based on 1,212 bug reports from the Eclipse Platform and Eclipse JDT. The experimental results show that our proposed approach can improve the bug localization performance and effectively identify buggy files.}, 
keywords={data mining;information retrieval;program debugging;project management;source code (software);bug location identification;Eclipse project;change history mining;bug localization performance improvement;fixed-files;textual similarity feature;source code file recommendation;bug report;vector space model;VSM;file fault proneness estimation;Eclipse JDT;History;Vectors;Mathematical model;Accuracy;Equations;Computer bugs;Indexes;Software Debugging;Bug Localization;Mining Change History;Information Retrieval}, 
doi={10.1109/ISSREW.2013.6688888}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6975636, 
author={C. De Roover and K. Inoue}, 
booktitle={2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation}, 
title={The Ekeko/X Program Transformation Tool}, 
year={2014}, 
volume={}, 
number={}, 
pages={53-58}, 
abstract={Developers often need to perform repetitive changes to source code. For instance, to repair several instances of a bug or to update all clients of a library to a newer version. Manually performing such changes is laborious and error-prone. Program transformation tools enable automating changes, but specifying changes as a program transformation requires significant expertise. Code templates are often touted as a remedy, yet have never been endorsed wholeheartedly. Their use is mostly limited to expressing the syntactic characteristics of the intended change subjects. Less familiar means have to be resorted to for expressing their structural, control flow, and data flow characteristics. In this tool paper, we introduce a decidedly template-driven program transformation tool called Ekeko/X. Its specifications feature templates for specifying all of the aforementioned characteristics of its subjects. To this end, developers can associate different directives with individual components of a template. Each matching directive imposes particular constraints on the matches for the component it is associated with. Rewriting directives, on the other hand, determine how each match should be changed. We develop Ekeko/X from the ground up, starting from its applicative logic meta-programming foundation. We highlight the key choices in this implementation and demonstrate its use through two example program transformations.}, 
keywords={program compilers;program debugging;source code (software);Ekeko/X program transformation tool;source code;code templates;syntactic characteristics;data flow characteristics;control flow characteristics;structural characteristics;rewriting directives;metaprogramming foundation;Syntactics;Receivers;Libraries;Reactive power;Concrete;Java;Abstracts;program transformation;code templates;refactoring}, 
doi={10.1109/SCAM.2014.32}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7575342, 
author={M. Piancó and B. Fonseca and N. Antunes}, 
booktitle={2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop (DSN-W)}, 
title={Code Change History and Software Vulnerabilities}, 
year={2016}, 
volume={}, 
number={}, 
pages={6-9}, 
abstract={Usually, the most critical modules of the system receive extra attention. But even these modules might be too large to be thoroughly inspected so it is useful to know where to apply the majority of the efforts. Thus, knowing which code changes are more prone to contain vulnerabilities may allow security experts to concentrate on a smaller subset of submitted code changes. In this paper we discuss the change history of functions and its impact on the existence of vulnerabilities. For this, we analyzed the commit history of two software projects widely exposed to attacks (Mozilla and Linux Kernel). Starting from security bugs, we analyzed more than 95k functions (with and without vulnerabilities), and systematized the changes in each function according to a subset of the patterns described in the Orthogonal Defects Classification. The results show that the frequency of changes can allow to distinguish functions more prone to have vulnerabilities.}, 
keywords={Linux;program debugging;project management;safety-critical software;source code (software);code change history;software vulnerabilities;software projects;Mozilla attack;Linux kernel attack;security bugs;orthogonal defect classification;History;Measurement;Security;Kernel;Predictive models;Sociology;Change History;ODC;Software Vulnerabilities}, 
doi={10.1109/DSN-W.2016.50}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8260630, 
author={M. Shalaby and T. Mehrez and A. El Mougy and K. Abdulnasser and A. Al-Safty}, 
booktitle={2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
title={Automatic Algorithm Recognition of Source-Code Using Machine Learning}, 
year={2017}, 
volume={}, 
number={}, 
pages={170-177}, 
abstract={As codebases for software projects get larger, reaching ranges of millions of lines of code, the need for computer-aided program comprehension grows. We define one of the tasks of program comprehension to be algorithm recognition: given a piece of source-code from a file, identify the algorithm this code is implementing, such as brute-force or dynamic programming. Most research in this area is making use of pattern matching, which involves much human effort and is of questionable accuracy when the structure and semantics of programs change. Thus, this paper proposes to let go of defined patterns, and make use of simpler features, such as counts of variables and counts of different constructs to recognize algorithms. We then feed these features to a classification algorithm to predict the class or type of algorithm used in this source code. We show through experimental results that our proposed method achieves a good improvement over baseline.}, 
keywords={feature extraction;learning (artificial intelligence);pattern classification;pattern matching;software engineering;source code (software);source code;automatic algorithm recognition;machine learning;software projects;program comprehension;classification algorithm;Feature extraction;Heuristic algorithms;Classification algorithms;Software algorithms;Prediction algorithms;Software;Algorithm recognition;machine learning;feature recognition}, 
doi={10.1109/ICMLA.2017.00033}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7838573, 
author={A. Khan and S. N. Ahsan}, 
booktitle={2016 International Conference on Open Source Systems Technologies (ICOSST)}, 
title={Predicting bug inducing source code change patterns}, 
year={2016}, 
volume={}, 
number={}, 
pages={29-35}, 
abstract={A change in source code without the prior analysis of its impact may generate one or more defects. Fixing of such defects consumes maintenance time which ultimately increases the cost of software maintenance. Therefore, in the recent years, several research works have been done to develop techniques for the automatic impact analysis of changes in source code. In this paper, we propose to use Frequent Pattern Mining (FPM) technique of machine learning for the automatic impact analysis of those changes in source code which may induce bugs. Therefore, to find patterns associated with some specific types of software changes, we applied FPM's algorithms' Apriori and Predictive Apriori on the stored data of software changes of the following three Open-Source Software (OSS) projects: Mozilla, GNOME, and Eclipse. We grouped the data of software changes into two major categories: changes to meet bug fixing requirements and changes to meet requirements other than bug fixing. In the case of bug fixing requirements, we predict source files which are frequently changed together to fix any one of the following four types of bugs related to: memory (MEMORY), variables locking (LOCK), system (SYSTEM) and graphical user interface (UI). Our experimental results predict several interesting software change patterns which may induce bugs. The obtained bug inducing patterns have high confidence and accuracy value i.e., more than 90%.}, 
keywords={data mining;object-oriented methods;program debugging;public domain software;software maintenance;source code (software);bug inducing source code change patterns;frequent pattern mining technique;FPM technique;machine learning;automatic impact analysis;predictive apriori algorithm;open-source software projects;OSS projects;Mozilla;GNOME;Eclipse;bug fixing requirements;source files;memory;variables locking;graphical user interface;software change patterns;Computer bugs;Data mining;Prediction algorithms;Databases;Software maintenance;Machine learning algorithms;Software Changes;Frequent Pattern Mining;Impact Analysis;Specific Types of Bugs;Transaction Data}, 
doi={10.1109/ICOSST.2016.7838573}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7816467, 
author={H. A. Nguyen and A. T. Nguyen and T. N. Nguyen}, 
booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Using Topic Model to Suggest Fine-Grained Source Code Changes}, 
year={2016}, 
volume={}, 
number={}, 
pages={200-210}, 
abstract={Prior research has shown that source code and its changes are repetitive. Several approaches have leveraged that phenomenon to detect and recommend change and fix patterns. In this paper, we propose TasC, a model that leverages the context of change tasks in development history to suggest fine-grained code change and fix at the program statement level. We use Latent Dirichlet Allocation (LDA) to capture the change task context via co-occurring program elements in the changes in a context. We also propose a novel technique for measuring the similarity of code fragments and code changes using the task context. We conducted an empirical evaluation on a large dataset of 88 open-source Java projects containing more than 200 thousand source files and 3.5 million source lines of code in their last revisions with 423 thousand changed methods. Our result shows that TasC relatively improves recommendation accuracy up to 130%-250% in comparison with the base models that do not use task context. Compared with other types of contexts, TasC outperforms the models using structural and co-change contexts.}, 
keywords={Java;public domain software;software engineering;source code (software);statistical analysis;topic model;source code change;TasC model;software development;latent Dirichlet allocation;LDA;code fragment similarity;task context;open-source Java project;Context;Context modeling;History;Electronic mail;Resource management;Open source software;Java;Code Recommendation;Fine-grained code changes;Topic Modeling;Statistical Models}, 
doi={10.1109/ICSME.2016.40}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8425000, 
author={E. Ufuktepe and T. Tuglular}, 
booktitle={2018 IEEE International Conference on Software Quality, Reliability and Security (QRS)}, 
title={A Program Slicing-Based Bayesian Network Model for Change Impact Analysis}, 
year={2018}, 
volume={}, 
number={}, 
pages={490-499}, 
abstract={Change impact analysis plays an important role in identifying potential affected areas that are caused by changes that are made in a software. Most of the existing change impact analysis techniques are based on architectural design and change history. However, source code-based change impact analysis studies are very few and they have shown higher precision in their results. In this study, a static method-granularity level change impact analysis, that uses program slicing and Bayesian Network technique has been proposed. The technique proposes a directed graph model that also represents the call dependencies between methods. In this study, an open source Java project with 8999 to 9445 lines of code and from 505 to 528 methods have been analyzed through 32 commits it went. Recall and f-measure metrics have been used for evaluation of the precision of the proposed method, where each software commit has been analyzed separately.}, 
keywords={belief networks;Java;program slicing;public domain software;software architecture;program slicing-based bayesian network model;architectural design;change history;static method-granularity level change impact analysis;Bayesian Network technique;change impact analysis techniques;source code-based change impact analysis;open source Java project;Bayes methods;Software;History;Probabilistic logic;Computational modeling;Measurement;Lattices;change impact analysis;program analysis;bayesian network}, 
doi={10.1109/QRS.2018.00062}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8307864, 
author={R. Malhotra and B. Bansal and C. Jain and E. Punia}, 
booktitle={2017 2nd International Conference on Man and Machine Interfacing (MAMI)}, 
title={An automated tool for collection of code attributes for cross project defect prediction}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={This paper presents a tool that automates the process of data collection for defect or change analysis. Prediction of defects in early phases has become crucial to reduce the efforts and costs incurred due to defects. This tool extracts the information from Git Version Control System of open source projects. Two consecutive versions of one single project have been used by the tool to obtain results. The tool generates a matrix containing code churn (added lines, deleted lines, modified lines, total LoC), complexity, pre-release bugs and post-release bugs of each file of source code. The obtained software metrics can be used to measure the development process of a software and therefore in analysis and prediction purposes.}, 
keywords={configuration management;program debugging;software fault tolerance;software maintenance;software metrics;software quality;source code (software);automated tool;code attributes;cross project defect prediction;data collection;Git Version Control System;open source projects;consecutive versions;matrix containing code churn;source code;development process;code churn;Tools;Computer bugs;Software;Complexity theory;Software metrics;defect prediction;code churn;open source;prerelease bugs;post release bugs}, 
doi={10.1109/MAMI.2017.8307864}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6405327, 
author={T. Omori and H. Kuwabara and K. Maruyama}, 
booktitle={2012 28th IEEE International Conference on Software Maintenance (ICSM)}, 
title={A study on repetitiveness of code completion operations}, 
year={2012}, 
volume={}, 
number={}, 
pages={584-587}, 
abstract={In current software development, code completion is necessary to enhance productivity of our programming tasks. However, how developers use code completion tools on integrated development environments is still not elucidated completely. Aiming to improve such tools, we performed an investigation in terms of code completion use. We investigated developers' operation histories on an integrated development environment and found that code completion operations inserting the same text tend to be repetitively performed in a short time period. We also propose new code completion strategies to reduce such repetitive code completion.}, 
keywords={software engineering;source coding;code completion operation repetitiveness;software development;programming task productivity;code completion tools;integrated development environments;code completion use;developer operation histories;repetitive code completion;History;Proposals;Java;Conferences;Software maintenance;Context;code completion;edit history;change-aware development environments}, 
doi={10.1109/ICSM.2012.6405327}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{7732136, 
author={A. Kaur and K. Kaur and S. Jain}, 
booktitle={2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, 
title={Predicting software change-proneness with code smells and class imbalance learning}, 
year={2016}, 
volume={}, 
number={}, 
pages={746-754}, 
abstract={The objective of this paper is to study the relationship between different types of object-oriented software metrics, code smells and actual changes in software code that occur during maintenance period. It is hypothesized that code smells are indicators of maintenance problems. To understand the relationship between code smells and maintenance problems, we extract code smells in a Java based mobile application called MOBAC. Four versions of MOBAC are studied. Machine learning techniques are applied to predict software change-proneness with code smells as predictor variables. The results of this paper indicate that codes smells are more accurate predictors of change-proneness than static code metrics for all machine learning methods. However, class imbalance techniques did not outperform class balance machine learning techniques in change-proneness prediction. The results of this paper are based on accuracy measures such as F-measure and area under ROC curve.}, 
keywords={Java;learning (artificial intelligence);object-oriented methods;software maintenance;software metrics;software change-proneness;code smells;class imbalance learning;object-oriented software metrics;maintenance problems;Java based mobile application;MOBAC;machine learning techniques;Software;Maintenance engineering;Couplings;Software metrics;Informatics;Data collection;Code smells;Exception handling smells;Software change-proneness;Machine learning techniques;Class imbalance learning}, 
doi={10.1109/ICACCI.2016.7732136}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8478911, 
author={Y. A. Alshehri and K. Goseva-Popstojanova and D. G. Dzielski and T. Devine}, 
booktitle={SoutheastCon 2018}, 
title={Applying Machine Learning to Predict Software Fault Proneness Using Change Metrics, Static Code Metrics, and a Combination of Them}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Predicting software fault proneness is very important as the process of fixing these faults after the release is very costly and time-consuming. In order to predict software fault proneness, many machine learning algorithms (e.g., Logistic regression, Naive Bayes, and J48) were used on several datasets, using different metrics as features. The question is what algorithm is the best under which circumstance and what metrics should be applied. Related works suggested that using change metrics leads to the highest accuracy in prediction. In addition, some algorithms perform better than others in certain circumstances. In this work, we use three machine learning algorithms (i.e., logistic regression, Naive Bayes, and J48) on three Eclipse releases (i.e., 2.0, 2.1, 3.0). The results showed that accuracy is slightly better and false positive rates are lower, when we use the reduced set of metrics compared to all change metrics set. However, the recall and the G score are better when we use the complete set of change metrics. Furthermore, J48 outperformed the other classifiers with respect to the G score for the reduced set of change metrics, as well as in almost all cases when the complete set of change metrics, static code metrics, and the combination of both were used.}, 
keywords={Bayes methods;learning (artificial intelligence);pattern classification;regression analysis;software fault tolerance;software metrics;software quality;logistic regression;change metrics;machine learning algorithms;static code metrics;naive Bayes algorithm;software fault proneness;J48 algorithm;G score;Measurement;Software;Machine learning algorithms;Prediction algorithms;Logistics;Computer bugs;Machine learning;Software Engineering;Software faults proneness;Reliability;Machine learning;Logistic regression;Naive Bayes;Decision tree;Prediction;Static code metrics;Change metrics}, 
doi={10.1109/SECON.2018.8478911}, 
ISSN={1558-058X}, 
month={April},}
@INPROCEEDINGS{7372057, 
author={K. Muslu and L. Swart and Y. Brun and M. D. Ernst}, 
booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Development History Granularity Transformations (N)}, 
year={2015}, 
volume={}, 
number={}, 
pages={697-702}, 
abstract={Development histories can simplify some software engineering tasks, butdifferent tasks require different history granularities. For example, a history that includes every edit that resulted in compiling code is needed when searching for the cause of a regression, whereas a history that contains only changes relevant to a feature is needed for understanding the evolution of the feature. Unfortunately, today, both manual and automated history generation result in a single-granularity history. This paper introduces the concept of multi-grained development history views and the architecture of Codebase Manipulation, a tool that automatically records a fine-grained history and manages its granularity by applying granularity transformations.}, 
keywords={regression analysis;software engineering;development history granularity transformations;software engineering tasks;compiling code;regression;automated history generation;single-granularity history;multigrained development history views;Codebase Manipulation;History;Compounds;Manuals;Maintenance engineering;Software;Computer architecture;Transforms;automated version control;fine-grained development history;history rewriting;history transformation;Codebase Manipulation}, 
doi={10.1109/ASE.2015.53}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6178855, 
author={M. Hashimoto and A. Mori}, 
booktitle={2012 16th European Conference on Software Maintenance and Reengineering}, 
title={Enhancing History-Based Concern Mining with Fine-Grained Change Analysis}, 
year={2012}, 
volume={}, 
number={}, 
pages={75-84}, 
abstract={Maintenance of large software projects is often hindered by cross-cutting concerns scattered over multiple modules. History-based mining techniques have been proposed to mitigate the difficultly by examining changes related to methods/functions in development history to suggest potential concerns. However, the techniques do not cope well with renamed entities and may lead to irrelevant information about concerns. The intricate procedures of the methods also make the results difficult for others to reproduce, utilize or improve. In this paper, we reinforce history-based concern mining techniques with fine-grained change analysis based on tree differencing on abstract syntax trees. Source code changes are recorded as facts over source code regions according to the RDF (Resource Description Framework) data model so that the analysis can be performed in terms of fact base queries. To show the capability of the method, we report on an experiment that emulates the state-of-the-art concern mining technique called COMMIT using our own change analysis tool called Diff/TS. A comparative case study on several open source projects written in C and Java shows that our technique improves results and overcomes the language barrier in the analysis.}, 
keywords={data mining;data models;Java;public domain software;software maintenance;tree data structures;history-based concern mining techniques;fine-grained change analysis;software project maintenance;development history;tree differencing;abstract syntax trees;source code changes;source code regions;RDF data model;resource description framework;COMMIT;Diff/TS;open source projects;Java;Data mining;Java;Resource description framework;Syntactics;History;Data models;XML;concern mining;change analysis;tree differencing;resource description framework}, 
doi={10.1109/CSMR.2012.18}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{8595176, 
author={M. Paixao and J. Krinke and D. Han and M. Harman}, 
booktitle={2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)}, 
title={CROP: Linking Code Reviews to Source Code Changes}, 
year={2018}, 
volume={}, 
number={}, 
pages={46-49}, 
abstract={Code review has been widely adopted by both industrial and open source software development communities. Research in code review is highly dependant on real-world data, and although existing researchers have attempted to provide code review datasets, there is still no dataset that links code reviews with complete versions of the system's code base mainly because reviewed versions are not kept in the system's version control repository. Thus, we present CROP, the Code Review Open Platform, the first curated code review repository that links review data with isolated complete versions (snapshots) of the source code at the time of review. CROP currently provides data for 8 software systems, 48,975 reviews and 112,617 patches, including versions of the systems that are inaccessible in the systems' original repositories. Moreover, CROP is extensible, and it will be continuously curated and extended.}, 
keywords={Agriculture;Data mining;Metadata;Servers;History;Open source software;Code Review;Repository;Platform;Software Change Analysis}, 
doi={}, 
ISSN={2574-3864}, 
month={May},}
@INPROCEEDINGS{6645254, 
author={Y. Yoon and B. A. Myers and S. Koo}, 
booktitle={2013 IEEE Symposium on Visual Languages and Human Centric Computing}, 
title={Visualization of fine-grained code change history}, 
year={2013}, 
volume={}, 
number={}, 
pages={119-126}, 
abstract={Conventional version control systems save code changes at each check-in. Recently, some development environments retain more fine-grain changes. However, providing tools for developers to use those histories is not a trivial task, due to the difficulties in visualizing the history. We present two visualizations of fine-grained code change history, which actively interact with the code editor: a timeline visualization, and a code history diff view. Our timeline and filtering options allow developers to navigate through the history and easily focus on the information they need. The code history diff view shows the history of any particular code fragment, allowing developers to move through the history simply by dragging the marker back and forth through the timeline to instantly see the code that was in the snippet at any point in the past. We augment the usefulness of these visualizations with richer editor commands including selective undo and search, which are all implemented in an Eclipse plug-in called “Azurite”. Azurite helps developers with answering common questions developers ask about the code change history that have been identified by prior research. In addition, many of users' backtracking tasks can be achieved using Azurite, which would be tedious or error-prone otherwise.}, 
keywords={data visualisation;software engineering;fine-grained code change history visualization;code editor interaction;timeline visualization;code history diff view;Eclipse plug-in;Azurite;user backtracking tasks;History;Visualization;Software;Filtering;Layout;Navigation;Real-time systems;program comprehension;software visualization;integrated development environments;selective undo}, 
doi={10.1109/VLHCC.2013.6645254}, 
ISSN={1943-6092}, 
month={Sep.},}
@ARTICLE{7328331, 
author={M. B. Zanjani and H. Kagdi and C. Bird}, 
journal={IEEE Transactions on Software Engineering}, 
title={Automatically Recommending Peer Reviewers in Modern Code Review}, 
year={2016}, 
volume={42}, 
number={6}, 
pages={530-543}, 
abstract={Code review is an important part of the software development process. Recently, many open source projects have begun practicing code review through “modern” tools such as GitHub pull-requests and Gerrit. Many commercial software companies use similar tools for code review internally. These tools enable the owner of a source code change to request individuals to participate in the review, i.e., reviewers. However, this task comes with a challenge. Prior work has shown that the benefits of code review are dependent upon the expertise of the reviewers involved. Thus, a common problem faced by authors of source code changes is that of identifying the best reviewers for their source code change. To address this problem, we present an approach, namely cHRev, to automatically recommend reviewers who are best suited to participate in a given review, based on their historical contributions as demonstrated in their prior reviews. We evaluate the effectiveness of cHRev on three open source systems as well as a commercial codebase at Microsoft and compare it to the state of the art in reviewer recommendation. We show that by leveraging the specific information in previously completed reviews (i.e.,quantification of review comments and their recency), we are able to improve dramatically on the performance of prior approaches, which (limitedly) operate on generic review information (i.e., reviewers of similar source code file and path names) or source coderepository data. We also present the insights into why our approach cHRev outperforms the existing approaches.}, 
keywords={software engineering;software reviews;peer reviewer recommendation;code review;software development process;open source projects;GitHub pull-requests tool;Gerrit tool;source code change;cHRev approach;commercial codebase;History;Electronic mail;Birds;Inspection;Androids;Humanoid robots;Software;Modern code review;reviewer recommendation;code change;Gerrit;Modern code review;reviewer recommendation;code change;Gerrit}, 
doi={10.1109/TSE.2015.2500238}, 
ISSN={0098-5589}, 
month={June},}
@INPROCEEDINGS{6676938, 
author={M. Asaduzzaman and C. K. Roy and K. A. Schneider and M. D. Penta}, 
booktitle={2013 IEEE International Conference on Software Maintenance}, 
title={LHDiff: Tracking Source Code Lines to Support Software Maintenance Activities}, 
year={2013}, 
volume={}, 
number={}, 
pages={484-487}, 
abstract={Tracking lines across versions of a file is a necessary step for solving a number of problems during software development and maintenance. Examples include, but are not limited to, locating bug-inducing changes, tracking code fragments or vulnerable instructions across versions, co-change analysis, merging file versions, reviewing source code changes, and software evolution analysis. In this tool demonstration, we present a language-independent line-level location tracker, named LHDiff, that can be used to track lines and analyze changes in various kinds of software artifacts, ranging from source code to arbitrary text files. The tool can effectively detect changed or moved lines across versions of a file, has the ability to detect line splits, and can easily be integrated with existing version control systems. It overcomes the limitations of existing language-independent techniques and is even comparable to tools that are language dependent. In addition to describing the tool, we also describe its effectiveness in analyzing source code artifacts.}, 
keywords={file organisation;program diagnostics;software maintenance;source code artifact analysis;line split detection;arbitrary text files;software artifacts;language-independent line-level location tracker;software evolution analysis;co-change analysis;code fragment tracking;bug-inducing change location;LHDiff;software development;software maintenance activity;source code line tracking;Context;Tracking;Position measurement;Syntactics;Benchmark testing;Software maintenance;differencing tools;line tracking;language-independent differencing tool}, 
doi={10.1109/ICSM.2013.78}, 
ISSN={1063-6773}, 
month={Sep.},}
@ARTICLE{6226427, 
author={S. Shivaji and E. James Whitehead and R. Akella and S. Kim}, 
journal={IEEE Transactions on Software Engineering}, 
title={Reducing Features to Improve Code Change-Based Bug Prediction}, 
year={2013}, 
volume={39}, 
number={4}, 
pages={552-569}, 
abstract={Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine (SVM) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The SVM's improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features.}, 
keywords={belief networks;learning (artificial intelligence);pattern classification;program debugging;support vector machines;code change-based bug prediction;machine learning classifier;source code file;software history;classifier-based bug prediction;machine learned feature reduction;feature selection technique;classification performance;naive Bayes classifier;support vector machine;SVM classifier;software project;buggy F-measure;Software;Support vector machines;History;Machine learning;Feature extraction;Measurement;Computer bugs;Reliability;bug prediction;machine learning;feature selection}, 
doi={10.1109/TSE.2012.43}, 
ISSN={0098-5589}, 
month={April},}
@INPROCEEDINGS{6747209, 
author={A. Hora and A. Etien and N. Anquetil and S. Ducasse and M. T. Valente}, 
booktitle={2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)}, 
title={APIEvolutionMiner: Keeping API evolution under control}, 
year={2014}, 
volume={}, 
number={}, 
pages={420-424}, 
abstract={During software evolution, source code is constantly refactored. In real-world migrations, many methods in the newer version are not present in the old version (e.g.,60% of the methods in Eclipse 2.0 were not in version 1.0). This requires changes to be consistently applied to reflect the new API and avoid further maintenance problems. In this paper, we propose a tool to extract rules by monitoring API changes applied in source code during system evolution. In this process, changes are mined at revision level in code history. Our tool focuses on mining invocation changes to keep track of how they are evolving. We also provide three case studies in order to evaluate the tool.}, 
keywords={application program interfaces;data mining;software engineering;source code (software);APIEvolutionMiner;API evolution;software evolution;source code;real-world migrations;API changes;code history;invocation change mining;Itemsets;History;Association rules;Receivers;Software;Browsers}, 
doi={10.1109/CSMR-WCRE.2014.6747209}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7918891, 
author={M. S. Khan and I. Koo}, 
booktitle={2017 International Conference on Communication, Computing and Digital Systems (C-CODE)}, 
title={Primary user detection in cognitive radio networks through quickest detection}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Spectrum sensing is one of the key functionalities in cognitive radio (CR) which enables opportunistic spectrum access. How to promptly sense the presence of the primary user (PU) is a key issue to CR network. The quick detection of the PU is critical such that violation of the detection on time will cause an interference to the PU. Particularly the spectrum sensing can be evaluated by how quickly it detect the changes. This involves detecting reliably and quickly possibly weak primary user signal. In this work, we propose a cluster based quickest change detection algorithm for the spectrum sensing. The Proposed scheme utilizes one-sided CUSUM (cumulative sum) algorithm at the user level, and proposed an algorithm at the cluster level which maintain 1-slot history of the user at the cluster head (CH), which also helps in detecting the change at the cluster level. CH only collect data from the users which observe change in their slot. By exploiting the advantage of one-sided CUSUM algorithm with cluster-based approach, the proposed scheme achieve the minimum detection delay subject to false alarm constraint. With simulation results, we demonstrate that the proposed scheme has faster detection as compare to conventional detection scheme.}, 
keywords={cognitive radio;radio spectrum management;signal detection;primary user detection;cognitive radio networks;spectrum sensing;opportunistic spectrum access;CR network;PU detection;weak primary user signal;cluster-based quickest change detection algorithm;one-sided CUSUM algorithm;cumulative sum algorithm;cluster head;cluster-based approach;minimum detection delay;false alarm constraint;Delays;Clustering algorithms;Copper;History;Cognitive radio;Interference;Sensors;change detection point;detection delay;log-likelihood ratio;one-sided CUSUM (cumulative sum) algorithm;time-based window}, 
doi={10.1109/C-CODE.2017.7918891}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8530035, 
author={V. Frick and T. Grassauer and F. Beck and M. Pinzger}, 
booktitle={2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Generating Accurate and Compact Edit Scripts Using Tree Differencing}, 
year={2018}, 
volume={}, 
number={}, 
pages={264-274}, 
abstract={For analyzing changes in source code, edit scriptsare used to describe the differences between two versions of afile. These scripts consist of a list of actions that, applied to thesource file, result in the new version of the file. In contrast toline-based source code differencing, tree-based approaches suchas GumTree, MTDIFF, or ChangeDistiller extract changes bycomparing the abstract syntax trees (AST) of two versions of asource file. One benefit of tree-based approaches is their abilityto capture moved (sub) trees in the AST. Our approach, theIterative Java Matcher (IJM), builds upon GumTree and aims atgenerating more accurate and compact edit scripts that capturethe developer's intent. This is achieved by improving the qualityof the generated move and update actions, which are the mainsource of inaccurate actions generated by previous approaches. To evaluate our approach, we conducted a study with 11 external experts and manually analyzed the accuracy of 2400 randomly selected editactions. Comparing IJM to GumTree and MTDIFF, the resultsshow that IJM provides better accuracy for move and updateactions and is more beneficial to understanding the changes.}, 
keywords={computational linguistics;configuration management;Java;management of change;software development management;software maintenance;source code (software);trees (mathematics);compact edit scripts;abstract syntax trees;AST;IJM;GumTree;source file;line-based source code;Syntactics;Java;Merging;Optimization;Runtime;Integrated circuits;1/f noise;change extraction;tree differencing;abstract syntax trees;software evolution}, 
doi={10.1109/ICSME.2018.00036}, 
ISSN={2576-3148}, 
month={Sep.},}
@INPROCEEDINGS{7328016, 
author={M. C. D. Oliveira and R. B. D. Almeida and G. N. Ramos and M. Ribeiro}, 
booktitle={2015 29th Brazilian Symposium on Software Engineering}, 
title={On the Conceptual Cohesion of Co-Change Clusters}, 
year={2015}, 
volume={}, 
number={}, 
pages={120-129}, 
abstract={The analysis of co-change clusters as an alternative software decomposition can provide insights on different perspectives of modularity. But the usual approach using coarse-grained entities does not provide relevant information, like the conceptual cohesion of the modular abstractions that emerge from co-change clusters. This work presents a novel approach to analyze the conceptual cohesion of the source-code associated with co-change clusters of fine-grained entities. We obtain from the change history information found in version control systems. We describe the use of our approach to analyze six well established and currently active open-source projects from different domains and one of the most relevant systems of the Brazilian Government for the financial domain. The results show that co-change clusters offer a new perspective on the code based on groups with high conceptual cohesion between its entities (up to 69% more than the original package decomposition), and, thus, are suited to detect concepts pervaded on codebases, opening new possibilities of comprehension of source-code by means of the concepts embodied in the co-change clusters.}, 
keywords={government data processing;pattern clustering;software packages;source code (software);co-change cluster analysis;software decomposition;modularity analysis;modular abstractions;conceptual cohesion analysis;fine-grained entities;change history information;version control systems;open-source projects;Brazilian government;financial domain;package decomposition;source-code comprehension;Software;Indexes;Measurement;History;Buildings;Electronic mail;Java}, 
doi={10.1109/SBES.2015.16}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{6963448, 
author={F. Palomba and G. Bavota and M. D. Penta and R. Oliveto and D. Poshyvanyk and A. De Lucia}, 
journal={IEEE Transactions on Software Engineering}, 
title={Mining Version Histories for Detecting Code Smells}, 
year={2015}, 
volume={41}, 
number={5}, 
pages={462-489}, 
abstract={Code smells are symptoms of poor design and implementation choices that may hinder code comprehension, and possibly increase changeand fault-proneness. While most of the detection techniques just rely on structural information, many code smells are intrinsically characterized by how code elements change overtime. In this paper, we propose Historical Information for Smell deTection (HIST), an approach exploiting change history information to detect instances of five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy. We evaluate HIST in two empirical studies. The first, conducted on 20 open source projects, aimed at assessing the accuracy of HIST in detecting instances of the code smells mentioned above. The results indicate that the precision of HIST ranges between 72 and 86 percent, and its recall ranges between 58 and 100 percent. Also, results of the first study indicate that HIST is able to identify code smells that cannot be identified by competitive approaches solely based on code analysis of a single system's snapshot. Then, we conducted a second study aimed at investigating to what extent the code smells detected by HIST (and by competitive code analysis techniques) reflect developers' perception of poor design and implementation choices. We involved 12 developers of four open source projects that recognized more than 75 percent of the code smell instances identified by HIST as actual design/implementation problems.}, 
keywords={data mining;program compilers;public domain software;code smell detection;historical information for smell detection;divergent change;shotgun surgery;parallel inheritance;blob;feature envy;HIST;code analysis;single system snapshot;open source project;mining version history;History;Feature extraction;Surgery;Accuracy;Association rules;Detectors;Code smells;mining software repositories;empirical studies;Code smells;mining software repositories;empirical studies}, 
doi={10.1109/TSE.2014.2372760}, 
ISSN={0098-5589}, 
month={May},}
@INPROCEEDINGS{6233416, 
author={O. Denninger}, 
booktitle={2012 Third International Workshop on Recommendation Systems for Software Engineering (RSSE)}, 
title={Recommending relevant code artifacts for change requests using multiple predictors}, 
year={2012}, 
volume={}, 
number={}, 
pages={78-79}, 
abstract={Finding code artifacts affected by a given change request is a time-consuming process in large software systems. Various approaches have been proposed to automate this activity, e.g., based on information retrieval. The performance of a particular prediction approach often highly depends on attributes like coding style or writing style of change request. Thus, we propose to use multiple prediction approaches in combination with machine learning. First experiments show that machine learning is well suitable to weight different prediction approaches for individual software projects and hence improve prediction performance.}, 
keywords={learning (artificial intelligence);recommender systems;software maintenance;relevant code artifacts recommendation;change requests;code artifacts finding;time-consuming process;large software systems;automated activity;information retrieval;coding style;writing style;multiple prediction approach;machine learning;individual software projects;prediction performance;Machine learning;Software engineering;Indexing;Information retrieval;Software maintenance;Neural networks;recommendation systems;software maintenance}, 
doi={10.1109/RSSE.2012.6233416}, 
ISSN={2327-0934}, 
month={June},}
@INPROCEEDINGS{6693078, 
author={H. A. Nguyen and A. T. Nguyen and T. T. Nguyen and T. N. Nguyen and H. Rajan}, 
booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={A study of repetitiveness of code changes in software evolution}, 
year={2013}, 
volume={}, 
number={}, 
pages={180-190}, 
abstract={In this paper, we present a large-scale study of repetitiveness of code changes in software evolution. We collected a large data set of 2,841 Java projects, with 1.7 billion source lines of code (SLOC) at the latest revisions, 1.8 million code change revisions (0.4 million fixes), 6.2 million changed files, and 2.5 billion changed SLOCs. A change is considered repeated within or cross-project if it matches another change having occurred in the history of the project or another project, respectively. We report the following important findings. First, repetitiveness of changes could be as high as 70-100% at small sizes and decreases exponentially as size increases. Second, repetitiveness is higher and more stable in the cross-project setting than in the within-project one. Third, fixing changes repeat similarly to general changes. Importantly, learning code changes and recommending them in software evolution is beneficial with accuracy for top-1 recommendation of over 30% and top-3 of nearly 35%. Repeated fixing changes could also be useful for automatic program repair.}, 
keywords={automatic programming;Java;software maintenance;source code (software);software evolution;Java projects;source lines of code;SLOC;code change revisions;automatic program repair;code change learning;code change repetitiveness;Software;Vegetation;Databases;History;Maintenance engineering;Libraries;Programming;Repetitive Code Changes;Software Evolution}, 
doi={10.1109/ASE.2013.6693078}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7467300, 
author={K. C. Youm and J. Ahn and J. Kim and E. Lee}, 
booktitle={2015 Asia-Pacific Software Engineering Conference (APSEC)}, 
title={Bug Localization Based on Code Change Histories and Bug Reports}, 
year={2015}, 
volume={}, 
number={}, 
pages={190-197}, 
abstract={A bug report is mainly used to find a fault location in software maintenance. It contains several fields such as summary, description, status and version. The description field includes detail scenario and stack traces if exceptional messages are presented. Recently researchers have proposed several approaches for automatic bug localization by using information retrieval and data mining. We propose BLIA, a statically integrated analysis approach of IR-based bug localization by utilizing texts and stack traces in bug reports, structured information of source files, and source code change histories. We performed experiments on three open source projects, namely AspectJ, SWT and ZXing. Compared with prior tools, our experiment results showed that BLIA outperforms the existing tools in terms of mean average precision. Our approach on average improved the metric of BugLocator, BLUiR, BRTracer and AmaLgam by 34%, 23%, 17% and 8%, respectively.}, 
keywords={data mining;fault location;information retrieval;program debugging;program diagnostics;public domain software;software fault tolerance;software maintenance;text analysis;AmaLgam;BRTracer;BLUiR;BugLocator;ZXing;SWT;AspectJ;open source projects;source files;text utilization;stack traces;IR-based bug localization;BLIA;data mining;information retrieval;version field;status field;description field;summary field;software maintenance;fault location;bug reports;source code change histories;automatic bug localization;History;Computer bugs;Information retrieval;Software maintenance;Indexes;Data mining;bug localization;fault localization;information retrieval;bug report;stack traces;code change history}, 
doi={10.1109/APSEC.2015.23}, 
ISSN={1530-1362}, 
month={Dec},}
@INPROCEEDINGS{7962375, 
author={T. Molderez and R. Stevens and C. De Roover}, 
booktitle={2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)}, 
title={Mining Change Histories for Unknown Systematic Edits}, 
year={2017}, 
volume={}, 
number={}, 
pages={248-256}, 
abstract={Software developers often need to repeat similar modifications in multiple different locations of a system's source code. These repeated similar modifications, or systematic edits, can be both tedious and error-prone to perform manually. While there are tools that can be used to assist in automating systematic edits, it is not straightforward to find out where the occurrences of a systematic edit are located in an existing system. This knowledge is valuable to help decide whether refactoring is needed, or whether future occurrences of an existing systematic edit should be automated. In this paper, we tackle the problem of finding unknown systematic edits using a closed frequent itemset mining algorithm, operating on sets of distilled source code changes. This approach has been implemented for Java programs in a tool called SysEdMiner. To evaluate the tool's precision and scalability, we have applied it to an industrial use case.}, 
keywords={data mining;history;Java;software maintenance;source code (software);unknown systematic edits;automating systematic edits;closed frequent itemset mining algorithm;distilled source code changes;Java programs;SysEdMiner;industrial use case;change history mining;Systematics;Tools;Itemsets;Data mining;Java;Software;Indexes;Systematic edits;change distilling;frequent itemset mining}, 
doi={10.1109/MSR.2017.12}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7884680, 
author={T. Omori}, 
booktitle={2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={Lost comments support program comprehension}, 
year={2017}, 
volume={}, 
number={}, 
pages={567-568}, 
abstract={Source code comments are valuable to keep developers' explanations of code fragments. Proper comments help code readers understand the source code quickly and precisely. However, developers sometimes delete valuable comments since they do not know about the readers' knowledge and think the written comments are redundant. This paper describes a study of lost comments based on edit operation histories of source code. The experimental result shows that developers sometimes delete comments although their associated code fragments are not changed. Lost comments contain valuable descriptions that can be utilized as new data sources to support program comprehension.}, 
keywords={software engineering;source code (software);lost comments support program comprehension;source code comments;code fragments explanation;software developers;edit operation histories;History;Software;Encoding;Syntactics;Computer science;Writing;Data mining}, 
doi={10.1109/SANER.2017.7884680}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8109012, 
author={F. L. Teixeira and M. L. Pilla and A. R. D. Bois and D. Mosse}, 
booktitle={2017 International Symposium on Computer Architecture and High Performance Computing Workshops (SBAC-PADW)}, 
title={Impact of Version Management for Transactional Memories on Phase-Change Memories}, 
year={2017}, 
volume={}, 
number={}, 
pages={91-96}, 
abstract={Two of the major issues in current computer systems are energy consumption and how to explore concurrent systems in a correct and efficient way. Solutions for these hazards may be sought both in hardware and in software. Phase-Change Memory (PCM) is a memory technology intended to replace DRAMs (Dynamic Random Access Memories) as the main memory, providing reduced static power consumption. Their main problem is related to write operations that are slow and wear their material. Transactional Memories are synchronization methods developed to reduce the limitations of lock-based synchronization. Their main advantages are related to being high-level and allowing composition and reuse of code, besides the absence of deadlocks. The objective of this study is to analyze the impact of different versioning managers (VMs) for transactional memories in PCMs. The lazy versioning/lazy acquisition scheme for version management presented the lowest wear on the PCM in 3 of 7 benchmarks analyzed, and results similar to the alternative versioning for the other 4~benchmarks. These results are related to the number of aborts of VMs, where this VM presents a much smaller number of aborts than the others, up to 39 times less aborts in the experiment with the benchmark Kmeans with 64 threads.}, 
keywords={concurrency control;configuration management;DRAM chips;phase change memories;power consumption;synchronisation;transactional memories;phase-change memories;energy consumption;concurrent systems;memory technology;Dynamic Random Access Memories;static power consumption;computer systems;version management impact;PCM;lazy versioning-lazy acquisition scheme;DRAMs;code reuse;lock-based synchronization method;alternative versioning;Phase change materials;Benchmark testing;Memory management;Energy consumption;Phase change memory;Random access memory;Software;Phase Change Memory;Software Transactional Memory;Memory Hierarchies}, 
doi={10.1109/SBAC-PADW.2017.24}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7884618, 
author={G. Santos and K. V. R. Paixão and N. Anquetil and A. Etien and M. de Almeida Maia and S. Ducasse}, 
booktitle={2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={Recommending source code locations for system specific transformations}, 
year={2017}, 
volume={}, 
number={}, 
pages={160-170}, 
abstract={From time to time, developers perform sequences of code transformations in a systematic and repetitive way. This may happen, for example, when introducing a design pattern in a legacy system: similar classes have to be introduced, containing similar methods that are called in a similar way. Automation of these sequences of transformations has been proposed in the literature to avoid errors due to their repetitive nature. However, developers still need support to identify all the relevant code locations that are candidate for transformation. Past research showed that these kinds of transformation can lag for years with forgotten instances popping out from time to time as other evolutions bring them into light. In this paper, we evaluate three distinct code search approaches (“structural”, based on Information Retrieval, and AST based algorithm) to find code locations that would require similar transformations. We validate the resulting candidate locations from these approaches on real cases identified previously in literature. The results show that looking for code with similar roles, e.g., classes in the same hierarchy, provides interesting results with an average recall of 87% and in some cases the precision up to 70%.}, 
keywords={information retrieval;recommender systems;software maintenance;source code (software);source code locations recommendation;system specific transformations;code transformations;legacy system;design pattern;code locations;distinct code search approaches;Computer bugs;Systematics;Syntactics;Java;History;Vocabulary;Color}, 
doi={10.1109/SANER.2017.7884618}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8330255, 
author={K. Maruyama and S. Hayashi and T. Omori}, 
booktitle={2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={ChangeMacroRecorder: Recording fine-grained textual changes of source code}, 
year={2018}, 
volume={}, 
number={}, 
pages={537-541}, 
abstract={Recording code changes comes to be well recognized as an effective means for understanding the evolution of existing programs and making their future changes efficient. Although fine-grained textual changes of source code are worth leveraging in various situations, there is no satisfactory tool that records such changes. This paper proposes a yet another tool, called ChangeMacroRecorder, which automatically records all textual changes of source code while a programmer writes and modifies it on the Eclipse's Java editor. Its capability has been improved with respect to both the accuracy of its recording and the convenience for its use. Tool developers can easily and cheaply create their new applications that utilize recorded changes by embedding our proposed recording tool into them.}, 
keywords={Java;programming environments;software maintenance;software tools;source code (software);Eclipses Java editor;source code changes recording;fine-grained textual changes recording;ChangeMacroRecorder;recording tool;Tools;Java;History;Task analysis;Aggregates;Indexes;Software;Fine-grained changes;change recording;integrated development environments}, 
doi={10.1109/SANER.2018.8330255}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8181481, 
author={M. Alohaly and H. Takabi}, 
booktitle={2017 IEEE 3rd International Conference on Collaboration and Internet Computing (CIC)}, 
title={When Do Changes Induce Software Vulnerabilities?}, 
year={2017}, 
volume={}, 
number={}, 
pages={59-66}, 
abstract={Version control systems (VCSs) have almost become the de facto standard for the management of open-source projects and the development of their source code. In VCSs, source code which can potentially be vulnerable is introduced to a system through what are so called commits. Vulnerable commits force the system into an insecure state. The farreaching impact of vulnerabilities attests to the importance of identifying and understanding the characteristics of prior vulnerable changes (or commits), in order to detect future similar ones. The concept of change classification was previously studied in the literature of bug detection to identify commits with defects. In this paper, we borrow the notion of change classification from the literature of defect detection to further investigate its applicability to vulnerability detection problem using semi-supervised learning. In addition, we also experiment with new vulnerability predictors, and compare the predictive power of our proposed features with vulnerability prediction techniques based on text mining. The experimental results show that our semi-supervised approach holds promise in improving change classification effectiveness by leveraging unlabeled data.}, 
keywords={data mining;learning (artificial intelligence);pattern classification;program debugging;text analysis;change classification effectiveness;VCSs;open-source projects;source code;bug detection;defect detection;vulnerability detection problem;semisupervised learning;vulnerability prediction techniques;software vulnerabilities;text mining;version control systems;Software;Feature extraction;Computer bugs;Security;Predictive models;Semisupervised learning;History;Software Security;Software Vulnerabilities;Vulnerability Prediction;Source Code;Semi supervised Learning}, 
doi={10.1109/CIC.2017.00020}, 
ISSN={}, 
month={Oct},}
@ARTICLE{7950877, 
author={L. MacLeod and M. Greiler and M. Storey and C. Bird and J. Czerwonka}, 
journal={IEEE Software}, 
title={Code Reviewing in the Trenches: Challenges and Best Practices}, 
year={2018}, 
volume={35}, 
number={4}, 
pages={34-42}, 
abstract={Code review has been widely adopted by and adapted to open source and industrial projects. Code review practices have undergone extensive research, with most studies relying on trace data from tool reviews, sometimes augmented by surveys and interviews. Several recent industrial research studies, along with blog posts and white papers, have revealed additional insights on code reviewing “from the trenches.” Unfortunately, the lessons learned about code reviewing are widely dispersed and poorly summarized by the existing literature. In particular, practitioners wishing to adopt or reflect on an existing or new code review process might have difficulty determining what challenges to expect and which best practices to adopt for their development context. Building on the existing literature, this article adds insights from a recent large-scale study of Microsoft developers to summarize the challenges that code-change authors and reviewers face, suggest best code-reviewing practices, and discuss tradeoffs that practitioners should consider. This article is part of a theme issue on Process Improvement.}, 
keywords={software quality;software reviews;source code (software);tool reviews;code-change authors;code-reviewing practices;Microsoft developers;code-change reviewers;Encoding;Best practices;Interviews;Context awareness;Object recognition;Stakeholders;peer review;social technologies;learning technologies;code inspection;code walkthroughs;testing;debugging;software engineering;software development}, 
doi={10.1109/MS.2017.265100500}, 
ISSN={0740-7459}, 
month={July},}
@INPROCEEDINGS{7476643, 
author={T. Rolfsnes and S. D. Alesio and R. Behjati and L. Moonen and D. W. Binkley}, 
booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Generalizing the Analysis of Evolutionary Coupling for Software Change Impact Analysis}, 
year={2016}, 
volume={1}, 
number={}, 
pages={201-212}, 
abstract={Software change impact analysis aims to find artifacts potentially affected by a change. Typical approaches apply language-specific static or dynamic dependence analysis, and are thus restricted to homogeneous systems. This restriction is a major drawback given today's increasingly heterogeneous software. Evolutionary coupling has been proposed as a language-agnostic alternative that mines relations between source-code entities from the system's change history. Unfortunately, existing evolutionary coupling based techniques fall short. For example, using Singular Value Decomposition (SVD) quickly becomes computationally expensive. An efficient alternative applies targeted association rule mining, but the most widely known approach (ROSE) has restricted applicability: experiments on two large industrial systems, and four large open source systems, show that ROSE can only identify dependencies about 25% of the time. To overcome this limitation, we introduce TARMAQ, a new algorithm for mining evolutionary coupling. Empirically evaluated on the same six systems, TARMAQ performs consistently better than ROSE and SVD, is applicable 100% of the time, and runs orders of magnitude faster than SVD. We conclude that the proposed algorithm is a significant step forward towards achieving robust change impact analysis for heterogeneous systems.}, 
keywords={data mining;evolutionary computation;singular value decomposition;software maintenance;source code (software);evolutionary coupling;software change impact analysis;language-specific static analysis;dynamic dependence analysis;source-code entities;singular value decomposition;SVD;association rule mining;TARMAQ;ROSE;History;Couplings;Software algorithms;Data mining;Software systems;Algorithm design and analysis;evolutionary coupling;software change impact analysis;targeted association rule mining;change recommendations;recommender systems}, 
doi={10.1109/SANER.2016.101}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6295253, 
author={Y. Liang and L. Yansheng}, 
booktitle={2012 7th International Conference on Computer Science Education (ICCSE)}, 
title={The atomic change set of Java programming language}, 
year={2012}, 
volume={}, 
number={}, 
pages={1090-1092}, 
abstract={Software configuration management computes the diff between the old version and new version of the program and represents the diff as added, deleted and updated text lines. The source code change represents the change type of code revisions in fine-grained levels such as class or field but rather to added, deleted and updated text lines. Atomic change is the minimal source code change. An atomic change can not be decomposed into other atomic changes. This paper defines the atomic change and the atomic change set of Java programming language. It analyzes properties of the atomic change set of Java programming language. It also emphasizes the differences of the binary compatibility and source compatibility of atomic changes of Java programming language. Finally, it gives a method for computing all source compatible atomic changes of Java programming language.}, 
keywords={configuration management;Java;programming languages;software engineering;atomic change set;Java programming language;software configuration management;text lines;code revisions;minimal source code change;binary compatibility;source compatibility;Java;XML;Object oriented programming;Software;Educational institutions;atomic change;atomic change set;source compatibility;binary compatibility}, 
doi={10.1109/ICCSE.2012.6295253}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{6224284, 
author={E. Giger and M. Pinzger and H. C. Gall}, 
booktitle={2012 9th IEEE Working Conference on Mining Software Repositories (MSR)}, 
title={Can we predict types of code changes? An empirical analysis}, 
year={2012}, 
volume={}, 
number={}, 
pages={217-226}, 
abstract={There exist many approaches that help in pointing developers to the change-prone parts of a software system. Although beneficial, they mostly fall short in providing details of these changes. Fine-grained source code changes (SCC) capture such detailed code changes and their semantics on the statement level. These SCC can be condition changes, interface modifications, inserts or deletions of methods and attributes, or other kinds of statement changes. In this paper, we explore prediction models for whether a source file will be affected by a certain type of SCC. These predictions are computed on the static source code dependency graph and use social network centrality measures and object-oriented metrics. For that, we use change data of the Eclipse platform and the Azureus 3 project. The results show that Neural Network models can predict categories of SCC types. Furthermore, our models can output a list of the potentially change-prone files ranked according to their change-proneness, overall and per change type category.}, 
keywords={learning (artificial intelligence);neural nets;object-oriented methods;software maintenance;software metrics;software quality;software system;fine-grained source code change;SCC;semantics;statement level;condition change;interface modification;statement change;prediction model;source file;static source code dependency graph;social network centrality measures;object-oriented metrics;Eclipse platform;Azureus 3 project;neural network model;change-prone file;change-proneness;software maintenance;machine learning;software quality;Correlation;Object oriented modeling;Measurement;Predictive models;Computational modeling;Artificial neural networks;Semantics;Software maintenance;Machine Learning;Software quality}, 
doi={10.1109/MSR.2012.6224284}, 
ISSN={2160-1860}, 
month={June},}
@INPROCEEDINGS{8512651, 
author={D. Achanccaray and J. Astucuri and M. Hayashibe and J. Pirca and V. Espinoza}, 
booktitle={2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
title={Implication of N400 and P600 waves in the Linguistic Code Change in Monolinguals and Bilinguals}, 
year={2018}, 
volume={}, 
number={}, 
pages={2032-2035}, 
abstract={There is evidence of the importance of N400 and P600 waves in linguistic processes, theses brain waves are related to syntax. This work proposes to evaluate learning process through the analysis of responses generated when formulation of word is requested, an artificial grammar test (AGT) is developed and N400 and P600 peaks are taken as indicators of performance; and two different groups of subjects took the AGT, 5 monolinguals and 5 bilinguals. The AGT is composed by 30 hybrids, each hybrid defines rules to formulate words; then if this word accomplished the rules, it is considered as grammatical. The N400 and P600 waves are computed by each word letter, and the mean for all 30 hybrids is compared between both two groups by electrode. Greater amplitudes for N400 and P600 peaks was found for monolinguals in comparison with bilinguals.}, 
keywords={computational linguistics;grammars;learning (artificial intelligence);natural language processing;monolinguals;bilinguals;N400;linguistic processes;learning process;artificial grammar test;AGT;word letter;linguistic code change;brain waves;P600 wave;Syntactics;Grammar;Linguistics;Task analysis;Learning (artificial intelligence);Electrodes;Electroencephalography}, 
doi={10.1109/EMBC.2018.8512651}, 
ISSN={1558-4615}, 
month={July},}
@INPROCEEDINGS{7884619, 
author={R. Stevens and C. De Roover}, 
booktitle={2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={Extracting executable transformations from distilled code changes}, 
year={2017}, 
volume={}, 
number={}, 
pages={171-181}, 
abstract={Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. Using examples, we demonstrate the expressiveness of specifying source code evolutions through intermediate ASTs. We also show that our approach is able to recall different implementation variants of the same source code evolution in open-source histories.}, 
keywords={software maintenance;software tools;source code (software);executable transformation extraction;distilled code changes;change distilling;mining software repositories;MSR;source code evolution;open-source histories;software evolution;Software;Transforms;Navigation;Data mining;History;Concrete;Database languages}, 
doi={10.1109/SANER.2017.7884619}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8530721, 
author={M. J. Decker and C. D. Newman and N. Dragan and M. L. Collard and J. I. Maletic and N. A. Kraft}, 
booktitle={2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
title={[Research Paper] Which Method-Stereotype Changes are Indicators of Code Smells?}, 
year={2018}, 
volume={}, 
number={}, 
pages={82-91}, 
abstract={A study of how method roles evolve during the lifetime of a software system is presented. Evolution is examined by analyzing when the stereotype of a method changes. Stereotypes provide a high-level categorization of a method's behavior and role, and also provide insight into how a method interacts with its environment and carries out tasks. The study covers 50 open-source systems and 6 closed-source systems. Results show that method behavior with respect to stereotype is highly stable and constant over time. Overall, out of all the history examined, only about 10% of changes to methods result in a change in their stereotype. Examples of methods that change stereotype are further examined. A select number of these types of changes are indicators of code smells.}, 
keywords={public domain software;software maintenance;software system;high-level categorization;closed-source systems;open-source systems;code smells;software system lifetime;software evolution;software change;Open source software;Taxonomy;History;Degradation;Manuals;Data models;method stereotypes;software change;software evolution;code smells;empirical}, 
doi={10.1109/SCAM.2018.00017}, 
ISSN={2470-6892}, 
month={Sep.},}
@INPROCEEDINGS{7081841, 
author={X. Xia and D. Lo and S. McIntosh and E. Shihab and A. E. Hassan}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Cross-project build co-change prediction}, 
year={2015}, 
volume={}, 
number={}, 
pages={311-320}, 
abstract={Build systems orchestrate how human-readable source code is translated into executable programs. In a software project, source code changes can induce changes in the build system (aka. build co-changes). It is difficult for developers to identify when build co-changes are necessary due to the complexity of build systems. Prediction of build co-changes works well if there is a sufficient amount of training data to build a model. However, in practice, for new projects, there exists a limited number of changes. Using training data from other projects to predict the build co-changes in a new project can help improve the performance of the build co-change prediction. We refer to this problem as cross-project build co-change prediction. In this paper, we propose CroBuild, a novel cross-project build co-change prediction approach that iteratively learns new classifiers. CroBuild constructs an ensemble of classifiers by iteratively building classifiers and assigning them weights according to its prediction error rate. Given that only a small proportion of code changes are build co-changing, we also propose an imbalance-aware approach that learns a threshold boundary between those code changes that are build co-changing and those that are not in order to construct classifiers in each iteration. To examine the benefits of CroBuild, we perform experiments on 4 large datasets including Mozilla, Eclipse-core, Lucene, and Jazz, comprising a total of 50,884 changes. On average, across the 4 datasets, CroBuild achieves a F1-score of up to 0.408. We also compare CroBuild with other approaches such as a basic model, AdaBoost proposed by Freund et al., and TrAdaBoost proposed by Dai et al.. On average, across the 4 datasets, the CroBuild approach yields an improvement in F1-scores of 41.54%, 36.63%, and 36.97% over the basic model, AdaBoost, and TrAdaBoost, respectively.}, 
keywords={program compilers;project management;software maintenance;source code (software);cross-project build co-change prediction;human-readable source code;executable programs;software project;build systems complexity;CroBuild;classifiers;prediction error rate;code changes;imbalance-aware approach;threshold boundary;Mozilla;Eclipse-core;Lucene;Jazz;TrAdaBoost;Predictive models;Training;Data models;Measurement;Error analysis;Buildings;Training data;Cross-project;Build Co-change Prediction;Transfer Learning;Imbalance Data}, 
doi={10.1109/SANER.2015.7081841}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{7551999, 
author={Q. He and B. Shen and Y. Chen}, 
booktitle={2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)}, 
title={Software Defect Prediction Using Semi-Supervised Learning with Change Burst Information}, 
year={2016}, 
volume={1}, 
number={}, 
pages={113-122}, 
abstract={Software defect prediction is an important software quality assurance technique. It utilizes historical project data and previously discovered defects to predict potential defects. However, most of existing methods assume that large amounts of labeled historical data are available for prediction, while in the early stage of the life cycle, projects may lack the data needed for building such predictors. In addition, most of existing techniques use static code metrics as predictors, while they omit change information that may introduce risks into software development. In this paper, we take these two issues into consideration, and propose a semi-supervised based defect prediction approach - extRF. extRF extends the classical supervised Random Forest algorithm by self-training paradigm. It also employs change burst information for improving accuracy of software defect prediction. We also conduct an experiment to evaluate extRF against three other supervised machine learners (i.e. Logistic Regression, Naive Bayes, Random Forest) and compare the effectiveness of code metrics, change burst metrics, and a combination of them. Experimental results show that extRF trained with a small size of labeled dataset achieves comparable performance to some supervised learning approaches trained with a larger size of labeled dataset. When only 2% of Eclipse 2.0 data are used for training, extRF can achieve F-measure about 0.562, approximate to that of LR (a supervised learning approach) at labeled sampling rate of 50%. Besides, change burst metrics outperform code metrics in that F-measure rises to a peak value of 0.75 for Eclipse 3.0 and JDT.Core.}, 
keywords={learning (artificial intelligence);software metrics;software quality;software defect prediction;semisupervised learning;change burst metrics;software quality assurance technique;historical project data;static code metrics;software development risks;semisupervised based defect prediction approach;extRF;supervised random forest algorithm;accuracy improvement;supervised machine learners;supervised learning approaches;Measurement;Software;Predictive models;Vegetation;Data models;Supervised learning;Training;Defect Prediction;Software Quality Assurance;Semi-supervised Learning;Change Metrics}, 
doi={10.1109/COMPSAC.2016.193}, 
ISSN={0730-3157}, 
month={June},}
@INPROCEEDINGS{8411742, 
author={A. Tornhill}, 
booktitle={2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={Assessing Technical Debt in Automated Tests with CodeScene}, 
year={2018}, 
volume={}, 
number={}, 
pages={122-125}, 
abstract={Test automation promises several advantages such as shorter lead times, higher code quality, and an executable documentation of the system's behavior. However, test automation won't deliver on those promises unless the quality of the automated test code itself is maintained, and to manually inspect the evolution of thousands of tests that change on a daily basis is impractical at best. This paper investigates how CodeScene - a tool for predictive analyses and visualizations - could be used to identify technical debt in automated test code. CodeScene combines repository mining, static code analysis, and machine learning to prioritize potential code improvements based on the most likely return on investment.}, 
keywords={data mining;inspection;learning (artificial intelligence);program diagnostics;program testing;software quality;source code (software);CodeScene;automated test code;static code analysis;test automation;code improvements;technical debt assessment;repository mining;machine learning;predictive analyses;Conferences;Software testing;technical debt;test automation;code quality;repository mining;vendor tools}, 
doi={10.1109/ICSTW.2018.00039}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7832884, 
author={Q. Luo and D. Poshyvanyk and M. Grechanik}, 
booktitle={2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)}, 
title={Mining Performance Regression Inducing Code Changes in Evolving Software}, 
year={2016}, 
volume={}, 
number={}, 
pages={25-36}, 
abstract={During software evolution, the source code of a system frequently changes due to bug fixes or new feature requests. Some of these changes may accidentally degrade performance of a newly released software version. A notable problem of regression testing is how to find problematic changes (out of a large number of committed changes) that may be responsible for performance regressions under certain test inputs. We propose a novel recommendation system, coined as PERFIMPACT, for automatically identifying code changes that may potentially be responsible for performance regressions using a combination of search-based input profiling and change impact analysis techniques. PERFIMPACT independently sends the same input values to two releases of the application under test, and uses a genetic algorithm to mine execution traces and explore a large space of input value combinations to find specific inputs that take longer time to execute in a new release. Since these input values are likely to expose performance regressions, PERFIMPACT automatically mines the corresponding execution traces to evaluate the impact of each code change on the performance and ranks the changes based on their estimated contribution to performance regressions. We implemented PERFIMPACT and evaluated it on different releases of two open-source web applications. The results demonstrate that PERFIMPACT effectively detects input value combinations to expose performance regressions and mines the code changes are likely to be responsible for these performance regressions.}, 
keywords={configuration management;genetic algorithms;program debugging;program diagnostics;program testing;regression analysis;software maintenance;software performance evaluation;source code (software);performance regression mining;evolving software;software evolution;source code;bug fixes;software version;regression testing;recommendation system;PERFIMPACT;code changes identification;search-based input profiling;change impact analysis;genetic algorithm;execution traces mining;Software;Testing;Genetic algorithms;Data mining;Degradation;Stakeholders;Fuel processing industries;Performance regression testing;mining execution traces;change impact analysis;genetic algorithms}, 
doi={10.1109/MSR.2016.013}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7357203, 
author={Y. Yoon and B. A. Myers}, 
booktitle={2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
title={Semantic zooming of code change history}, 
year={2015}, 
volume={}, 
number={}, 
pages={95-99}, 
abstract={Previously, we presented our technique for visualizing fine-grained code changes in a timeline view, designed to facilitate reviewing and interacting with the code change history. During user evaluations, it became evident that users often wanted to see the code changes at a higher level of abstraction. Therefore, we developed a novel approach to automatically summarize fine-grained code changes into more conceptual, higher-level changes in real time. Our system provides four collapse levels, which are integrated with the timeline via semantic zooming: raw level (no collapsing), statement level, method level, and type level. Compared to the raw level, the number of code changes shown in the timeline at each level is reduced by 55%, 77%, and 83%, respectively. This implies that the semantic zooming would help users better understand and interact with the history by minimizing the potential information overload.}, 
keywords={programming environments;user interfaces;code change history;raw level;statement level;method level;type level;semantic zooming;semantic zooming;edit collapsing;program comprehension;software visualization;timeline visualization;Azurite}, 
doi={10.1109/VLHCC.2015.7357203}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8530068, 
author={A. Sabetta and M. Bezzi}, 
booktitle={2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={A Practical Approach to the Automatic Classification of Security-Relevant Commits}, 
year={2018}, 
volume={}, 
number={}, 
pages={579-582}, 
abstract={The lack of reliable sources of detailed information on the vulnerabilities of open-source software (OSS) components is a major obstacle to maintaining a secure software supply chain and an effective vulnerability management process. Standard sources of advisories and vulnerability data, such as the National Vulnerability Database (NVD), are known to suffer from poor coverage and inconsistent quality. To reduce our dependency on these sources, we propose an approach that uses machine-learning to analyze source code repositories and to automatically identify commits that are security-relevant (i.e., that are likely to fix a vulnerability). We treat the source code changes introduced by commits as documents written in natural language, classifying them using standard document classification methods. Combining independent classifiers that use information from different facets of commits, our method can yield high precision (80%) while ensuring acceptable recall (43%). In particular, the use of information extracted from the source code changes yields a substantial improvement over the best known approach in state of the art, while requiring a significantly smaller amount of training data and employing a simpler architecture.}, 
keywords={database management systems;learning (artificial intelligence);pattern classification;public domain software;security of data;software engineering;security-relevant commits;reliable sources;detailed information;open-source software components;secure software supply chain;vulnerability data;National Vulnerability Database;source code repositories;independent classifiers;automatic classification;vulnerability management process;machine learning;document classification methods;OSS components;Security;Standards;Open source software;Databases;Predictive models;Machine learning;machine learning;open source software;vulnerabilities;NVD;CVE;commit classification;source code repositories;code change classification}, 
doi={10.1109/ICSME.2018.00058}, 
ISSN={2576-3148}, 
month={Sep.},}
@INPROCEEDINGS{7832964, 
author={Z. Chen}, 
booktitle={2016 IEEE/ACM International Conference on Mobile Software Engineering and Systems (MOBILESoft)}, 
title={Helping Mobile Software Code Reviewers: A Study of Bug Repair and Refactoring Patterns}, 
year={2016}, 
volume={}, 
number={}, 
pages={34-35}, 
abstract={Mobile Developers commonly spend a significant amount of time and effort on conducting code reviews on newly introduced and domain-specific practices, such as platform-specific feature addition, quality of service anti-pattern refactorings, and battery-related bug fixes. To address these problems, we conducted a large empirical study over the software change history of 318 open source projects and investigated platform-dependent code changes from open source projects. Our analysis focuses on what types of changes mobile application developers typically make and how they perceive, recall, and communicate changed and affected code. Our study required the development of an automated strategy to examine open source repositories and categorize platform-related refactoring edits, bug repairs, and API updates, mining 1,961,990 commit changes. Our findings call for the need to develop a new recommendation system aimed at efficiently identifying required changes such as bug fixes and refactorings during mobile application code reviews.}, 
keywords={mobile computing;public domain software;software maintenance;software reviews;source code (software);mobile software code reviewer;bug repair;refactoring pattern;mobile application developer;open source repository;recommendation system;Computer bugs;Mobile communication;Maintenance engineering;Mobile applications;Software;History;Cloning;mobile application development;software repair;refactoring}, 
doi={10.1109/MobileSoft.2016.026}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6498483, 
author={R. Stevens and C. De Roover and C. Noguera and V. Jonckers}, 
booktitle={2013 17th European Conference on Software Maintenance and Reengineering}, 
title={A History Querying Tool and Its Application to Detect Multi-version Refactorings}, 
year={2013}, 
volume={}, 
number={}, 
pages={335-338}, 
abstract={Version Control Systems (VCS) have become indispensable in developing software. In order to provide support for change management, they track the history of software projects. Tool builders can exploit this latent historical information to provide insights in the evolution of the project. For example, the information needed to identify when and where a particular refactoring was applied is implicitly present in the VCS. However, tool support for eliciting this information is lacking. So far, no general-purpose history querying tool capable of answering a wide variety of questions about the evolution of software exists. Therefore, we generalize the idea of a program querying tool to a history querying tool. A program querying tool reifies the program's code into a knowledge base, from which it retrieves elements that exhibit characteristics specified through a user-provided program query. Our history querying tool, QwalKeko, enables specifying the evolution of source code characteristics across multiple versions of Java projects versioned in Git. We apply QwalKeko to the problem of detecting refactorings, specified as the code changes induced by each refactoring. These specifications stem from the literature, but are limited to changes between two successive versions. We demonstrate the expressiveness of our tool by generalizing the specifications such that refactorings can span multiple versions.}, 
keywords={configuration management;Java;query processing;software maintenance;history querying tool;multiversion refactoring detection;VCS;version control system;software development;change management;QwalKeko history querying tool;user-provided program query;Java project;History;Java;Database languages;Europe;Software maintenance;Control systems;program comprehension tools;software repositories;refactoring}, 
doi={10.1109/CSMR.2013.44}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{7081824, 
author={P. Thongtanunam and C. Tantithamthavorn and R. G. Kula and N. Yoshida and H. Iida and K. Matsumoto}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Who should review my code? A file location-based code-reviewer recommendation approach for Modern Code Review}, 
year={2015}, 
volume={}, 
number={}, 
pages={141-150}, 
abstract={Software code review is an inspection of a code change by an independent third-party developer in order to identify and fix defects before an integration. Effectively performing code review can improve the overall software quality. In recent years, Modern Code Review (MCR), a lightweight and tool-based code inspection, has been widely adopted in both proprietary and open-source software systems. Finding appropriate code-reviewers in MCR is a necessary step of reviewing a code change. However, little research is known the difficulty of finding code-reviewers in a distributed software development and its impact on reviewing time. In this paper, we investigate the impact of reviews with code-reviewer assignment problem has on reviewing time. We find that reviews with code-reviewer assignment problem take 12 days longer to approve a code change. To help developers find appropriate code-reviewers, we propose RevFinder, a file location-based code-reviewer recommendation approach. We leverage a similarity of previously reviewed file path to recommend an appropriate code-reviewer. The intuition is that files that are located in similar file paths would be managed and reviewed by similar experienced code-reviewers. Through an empirical evaluation on a case study of 42,045 reviews of Android Open Source Project (AOSP), OpenStack, Qt and LibreOffice projects, we find that RevFinder accurately recommended 79% of reviews with a top 10 recommendation. RevFinder also correctly recommended the code-reviewers with a median rank of 4. The overall ranking of RevFinder is 3 times better than that of a baseline approach. We believe that RevFinder could be applied to MCR in order to help developers find appropriate code-reviewers and speed up the overall code review process.}, 
keywords={public domain software;software reviews;file location;code-reviewer recommendation approach;modern code review;software code review;MCR;proprietary software system;open-source software system;Android open source project;AOSP;OpenStack;Qt project;LibreOffice project;RevFinder;Androids;Humanoid robots;Open source software;Software systems;Manuals;History;Distributed Software Development;Modern Code Review;Code-Reviewer Recommendation}, 
doi={10.1109/SANER.2015.7081824}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{6975652, 
author={A. Lozano and C. Noguera and V. Jonckers}, 
booktitle={2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation}, 
title={Explaining Why Methods Change Together}, 
year={2014}, 
volume={}, 
number={}, 
pages={185-194}, 
abstract={By analyzing historical information from Source Code Management systems, previous research has observed that certain methods tend to change together consistently. Co-change has been identified as a good predictor of the entities that are likely to be affected by a change, which ones might be missing modifications, and which ones might change in the future. However, existing co-change analysis provides no insight on why methods consistently co-change. Being able to identify the rationale that explains co-changes could allow to document and enforce design knowledge. This paper proposes an automatic approach to derive the reason behind a co-change. We define the reason of a (set) of co-changes as a set of properties common to the elements that co-change. We consider two kinds of properties: structural properties which indicate explicit dependencies, and semantic properties which reveal implicit dependencies. Then we attempt to identify the reasons behind single commits, as well as the reasons behind co-changes that repeatedly affect the same set of methods. These sets of methods are identified by clustering methods that tend to be modified in the same commit-transactions. We perform our analysis over the history of two open-source systems, analyzing nearly 19.000 methods and over 3700 commits. We show that it is possible to automatically extract explanations for co-changes, that the quality of such explanations improves when structural and semantic properties are taken into account, and when the methods analyzed co-change recurrently.}, 
keywords={pattern clustering;program diagnostics;public domain software;co-change analysis;structural properties;explicit dependencies;semantic properties;implicit dependencies;clustering methods;commit-transactions;open-source systems;Semantics;History;Documentation;Measurement;Java;Libraries;Clustering methods;Co-change;Empirical software engineering;Program comprehension}, 
doi={10.1109/SCAM.2014.27}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8368456, 
author={L. Vidács and M. Pinzger}, 
booktitle={2018 IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE)}, 
title={Co-evolution analysis of production and test code by learning association rules of changes}, 
year={2018}, 
volume={}, 
number={}, 
pages={31-36}, 
abstract={Many modern software systems come with automated tests. While these tests help to maintain code quality by providing early feedback after modifications, they also need to be maintained. In this paper, we replicate a recent pattern mining experiment to find patterns on how production and test code co-evolve over time. Understanding co-evolution patterns may directly affect the quality of tests and thus the quality of the whole system. The analysis takes into account fine grained changes in both types of code. Since the full list of fine grained changes cannot be perceived, association rules are learned from the history to extract co-change patterns. We analyzed the occurrence of 6 patterns throughout almost 2500 versions of a Java system and found that patterns are present, but supported by weaker links than in previously reported. Hence we experimented with weighting methods and investigated the composition of commits.}, 
keywords={data mining;Java;program testing;software maintenance;software quality;test code;association rules;modern software systems;automated tests;code quality;co-change patterns;Java system;Co-evolution analysis;co-evolution patterns;pattern mining;Production;History;Data mining;Software;Java;Tools;Software engineering;software evolution;change analysis;machine learning;co-evolution patterns;testing}, 
doi={10.1109/MALTESQUE.2018.8368456}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6224271, 
author={V. S. Sinha and S. Mani and M. Gupta}, 
booktitle={2012 9th IEEE Working Conference on Mining Software Repositories (MSR)}, 
title={MINCE: Mining change history of Android project}, 
year={2012}, 
volume={}, 
number={}, 
pages={132-135}, 
abstract={An analysis of commit history of Android reveals that Android has a code base of 550K files, where on an average each file has been modified 8.7 times. 41% of files have been modified at-least once. In terms of contributors, it has an overall contributor community of 1563, with 58.5% of them having made &gt;; 5 commits. Moreover, the contributor community shows high churn levels, with only 13 of contributors continuing from 2005 to 2011. In terms of industry participation, Google &amp; Android account for 22% of developers. Intel and RedHat account for 2% of contributors each and IBM, Oracle, TI, SGI account for another 1% each. Android code can be classified into 5 sub-projects: kernel, platform, device, tools and toolchain. In this paper, we profile each of these sub-projects in terms of change volumes, contributor and industry participation. We further picked specific framework topics such as UI, security, whose understanding is required from perspective of developing apps over Android, and present some insights on community participation around the same.}, 
keywords={codes;data mining;operating systems (computers);MINCE;Android project;contributor community;churn levels;Google & Android account;Intel and RedHat account;IBM;Oracle;TI;SGI;Android code;kernel subprojects;platform subproject;device subprojects;tools subprojects;toolchain subprojects;change volumes;industry participation;Androids;Humanoid robots;Kernel;Companies;Smart phones;History;Communities}, 
doi={10.1109/MSR.2012.6224271}, 
ISSN={2160-1860}, 
month={June},}
@ARTICLE{8493303, 
author={M. Wen and R. Wu and S. C. Cheung}, 
journal={IEEE Transactions on Software Engineering}, 
title={How Well Do Change Sequences Predict Defects? Sequence Learning from Software Changes}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Software defect prediction, which aims to identify defective modules, can assist developers in finding bugs and prioritizing limited quality assurance resources. Various features to build defect prediction models have been proposed and evaluated. Among them, process metrics are one important category. Yet, existing process metrics are mainly encoded manually from change histories and ignore the sequential information arising from the changes during software evolution. Unlike traditional process metrics used for existing defect prediction models, change sequences are mostly vectors of variable length. This makes it difficult to apply such sequences directly in prediction models that are driven by conventional classifiers. To resolve this challenge, we utilize Recurrent Neural Network (RNN), which is a deep learning technique, to encode features from sequence data automatically. In this paper, we propose a novel approach called Fences, which extracts six types of change sequences covering different aspects of software changes via fine-grained change analysis. It approaches defects prediction by mapping it to a sequence labeling problem solvable by RNN. Our evaluations on 10 open source projects show that Fences can predict defects with high performance. Fences also outperforms the state-of-the-art technique which learns semantic features automatically from static code via deep learning.}, 
keywords={Measurement;Software;Predictive models;Semantics;History;Machine learning;Feature extraction;Defect Prediction;Process Metrics;Sequence Learning}, 
doi={10.1109/TSE.2018.2876256}, 
ISSN={0098-5589}, 
month={},}
@INPROCEEDINGS{8087896, 
author={Q. Xiao and Q. Zhang and X. Wu and X. Han and R. Li}, 
booktitle={2017 3rd IEEE International Conference on Control Science and Systems Engineering (ICCSSE)}, 
title={Learning binary code features for UAV target tracking}, 
year={2017}, 
volume={}, 
number={}, 
pages={65-68}, 
abstract={During target tracking, in order to obtain a higher tracking accuracy, the region we would like to track should have a good feature expression. Furthermore, we need to extract multilevel and complex features to deal with problems which are usually encountered during UAV tracking, such as the target deformation, scale change and occlusion. However, such features make tracker more complex which would seriously affect the real-time tracking. Considering the above problems, we take the advantage of random forest for features selection, and then transform the features to binary code, which can not only reduce redundancy but speed up the tracker. In order to further improve the accuracy of UAV tracking, we utilize structured SVM for online learning to distinguish object from background. In addition, we apply the scale pyramid to achieve the scale invariance of tracker, which help to obtain a more precise position of the object. We have verified the effectiveness and robustness of our method on the classical UAV object tracking dataset UAV123.}, 
keywords={autonomous aerial vehicles;binary codes;feature extraction;learning (artificial intelligence);object tracking;support vector machines;binary code features;UAV target tracking;target deformation;occlusion;real-time tracking;features selection;object tracking;structured SVM;online learning;Target tracking;Binary codes;Support vector machines;Feature extraction;Redundancy;Decision trees;Robustness;UAV tracking;binary code;random forest;scale invariance;structured SVM}, 
doi={10.1109/CCSSE.2017.8087896}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7321194, 
author={S. Fu and B. Shen}, 
booktitle={2015 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)}, 
title={Code Bad Smell Detection through Evolutionary Data Mining}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={The existence of code bad smell has a severe impact on the software quality. Numerous researches show that ignoring code bad smells can lead to failure of a software system. Thus, the detection of bad smells has drawn the attention of many researchers and practitioners. Quite a few approaches have been proposed to detect code bad smells. Most approaches are solely based on structural information extracted from source code. However, we have observed that some code bad smells have the evolutionary property, and thus propose a novel approach to detect three code bad smells by mining software evolutionary data: duplicated code, shotgun surgery, and divergent change. It exploits association rules mined from change history of software systems, upon which we define heuristic algorithms to detect the three bad smells. The experimental results on five open source projects demonstrate that the proposed approach achieves higher precision, recall and F-measure.}, 
keywords={data mining;evolutionary computation;software quality;source code (software);code bad smell detection;software quality;source code;software evolutionary data mining;duplicated code;shotgun surgery;divergent change;Software systems;History;Association rules;Surgery;Couplings}, 
doi={10.1109/ESEM.2015.7321194}, 
ISSN={1949-3770}, 
month={Oct},}
@INPROCEEDINGS{6606642, 
author={V. Balachandran}, 
booktitle={2013 35th International Conference on Software Engineering (ICSE)}, 
title={Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation}, 
year={2013}, 
volume={}, 
number={}, 
pages={931-940}, 
abstract={Peer code review is a cost-effective software defect detection technique. Tool assisted code review is a form of peer code review, which can improve both quality and quantity of reviews. However, there is a significant amount of human effort involved even in tool based code reviews. Using static analysis tools, it is possible to reduce the human effort by automating the checks for coding standard violations and common defect patterns. Towards this goal, we propose a tool called Review Bot for the integration of automatic static analysis with the code review process. Review Bot uses output of multiple static analysis tools to publish reviews automatically. Through a user study, we show that integrating static analysis tools with code review process can improve the quality of code review. The developer feedback for a subset of comments from automatic reviews shows that the developers agree to fix 93% of all the automatically generated comments. There is only 14.71% of all the accepted comments which need improvements in terms of priority, comment message, etc. Another problem with tool assisted code review is the assignment of appropriate reviewers. Review Bot solves this problem by generating reviewer recommendations based on change history of source code lines. Our experimental results show that the recommendation accuracy is in the range of 60%-92%, which is significantly better than a comparable method based on file change history.}, 
keywords={program diagnostics;program testing;software engineering;quality improvement;peer code review;automatic static analysis;reviewer recommendation;cost-effective software defect detection technique;tool assisted code review;static analysis tools;coding standard violation checking automation;defect pattern;Review Bot tool;developer feedback;comment message;source code line change history;file change history;Encoding;Standards;Java;History;Software;Algorithm design and analysis;Inspection}, 
doi={10.1109/ICSE.2013.6606642}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{8453110, 
author={F. Palomba and G. Bavota and M. Di Penta and F. Fasano and R. Oliveto and A. De Lucia}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)}, 
title={[Journal First] On the Diffuseness and the Impact on Maintainability of Code Smells: A Large Scale Empirical Investigation}, 
year={2018}, 
volume={}, 
number={}, 
pages={482-482}, 
abstract={Code smells are symptoms of poor design and implementation choices that may hinder code comprehensibility and maintainability. Despite the effort devoted by the research community in studying code smells, the extent to which code smells in software systems affect software maintainability remains still unclear. In this paper we present a large scale empirical investigation on the diffuseness of code smells and their impact on code change- and fault-proneness. The study was conducted across a total of 395 releases of 30 open source projects and considering 17,350 manually validated instances of 13 different code smell types. The results show that smells characterized by long and/or complex code (e.g., Complex Class) are highly diffused, and that smelly classes have a higher change- and fault-proneness than smell-free classes.}, 
keywords={public domain software;software maintenance;software maintainability;code fault-proneness;code change-proneness;open source projects;code comprehensibility;scale empirical investigation;code smells;smell-free classes;complex code;Software engineering;History;Detectors;Maintenance engineering;Software systems;Correlation;code smells;empirical studies;mining software repositories}, 
doi={}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{8170128, 
author={Y. Huang and Q. Zheng and X. Chen and Y. Xiong and Z. Liu and X. Luo}, 
booktitle={2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)}, 
title={Mining Version Control System for Automatically Generating Commit Comment}, 
year={2017}, 
volume={}, 
number={}, 
pages={414-423}, 
abstract={Commit comments increasingly receive attention as an important complementary component in code change comprehension. To address the comment scarcity issue, a variety of automatic approaches for commit comment generation have been intensively proposed. However, most of these approaches mechanically outline a superficial level summary of the changed software entities, the change intent behind the code changes is lost (e.g., the existing approaches cannot generate such comment: "fixing null pointer exception"). Considering the comments written by developers often describe the intent behind the code change, we propose a method to automatically generate commit comment by reusing the existing comments in version control system. Specifically, for an input commit, we apply syntax, semantic, pre-syntax, and pre-semantic similarities to discover the similar commits from half a million commits, and recommend the reusable comments to the input commit from the ones of the similar commits. We evaluate our approach on 7 projects. The results show that 9.1% of the generated comments are good, 27.7% of the generated comments need minor fix, and 63.2% are bad, and we also analyze the reasons that make a comment available or unavailable.}, 
keywords={configuration management;data mining;programming language semantics;software maintenance;text analysis;mining version control system;code change comprehension;comment scarcity issue;changed software entities;change intent;input commit;reusable comments;automatic commit comment generation;syntax similarities;semantic similarities;presyntax similarities;presemantic similarities;Syntactics;Semantics;Control systems;Software;Java;Algorithm design and analysis;Data mining;Commit Comment Generation;Code Syntax Similarity;Code Semantic Similarity;Code Change Comprehension}, 
doi={10.1109/ESEM.2017.56}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7816469, 
author={F. Thung and X. D. Le and D. Lo and J. Lawall}, 
booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Recommending Code Changes for Automatic Backporting of Linux Device Drivers}, 
year={2016}, 
volume={}, 
number={}, 
pages={222-232}, 
abstract={Device drivers are essential components of any operating system (OS). They specify the communication protocol that allows the OS to interact with a device. However, drivers for new devices are usually created for a specific OS version. These drivers often need to be backported to the older versions to allow use of the new device. Backporting is often done manually, and is tedious and error prone. To alleviate this burden on developers, we propose an automatic recommendation system to guide the selection of backporting changes. Our approach analyzes the version history for cues to recommend candidate changes. We have performed an experiment on 100 Linux driver files and have shown that we can give a recommendation containing the correct backport for 68 of the drivers. For these 68 cases, 73.5%, 85.3%, and 88.2% of the correct recommendations are located in the Top-1, Top-2, and Top-5 positions of the recommendation lists respectively. The successful cases cover various kinds of changes including change of record access, deletion of function argument, change of a function name, change of constant, and change of if condition. Manual investigation of failed cases highlights limitations of our approach, including inability to infer complex changes, and unavailability of relevant cues in version history.}, 
keywords={device drivers;Linux;operating systems (computers);protocols;recommender systems;software maintenance;source code (software);recommendation code;Linux device driver backporting;operating system;OS;communication protocol;recommendation system;record access change;function argument deletion;function name change;constant change;IF condition change;Linux;Kernel;History;Control systems;Libraries;Conferences;Backporting;Recommendation System;Linux;Device Drivers}, 
doi={10.1109/ICSME.2016.71}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7180066, 
author={Y. Jiang and B. Adams}, 
booktitle={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories}, 
title={Co-evolution of Infrastructure and Source Code - An Empirical Study}, 
year={2015}, 
volume={}, 
number={}, 
pages={45-55}, 
abstract={Infrastructure-as-code automates the process of configuring and setting up the environment (e.g., servers, VMs and databases) in which a software system will be tested and/or deployed, through textual specification files in a language like Puppet or Chef. Since the environment is instantiated automatically by the infrastructure languages' tools, no manual intervention is necessary apart from maintaining the infrastructure specification files. The amount of work involved with such maintenance, as well as the size and complexity of infrastructure specification files, have not yet been studied empirically. Through an empirical study of the version control system of 265 Open Stack projects, we find that infrastructure files are large and churn frequently, which could indicate a potential of introducing bugs. Furthermore, we found that the infrastructure code files are coupled tightly with the other files in a project, especially test files, which implies that testers often need to change infrastructure specifications when making changes to the test framework and tests.}, 
keywords={formal specification;source code (software);infrastructure coevolution;source code;infrastructure code files;infrastructure specifications;Couplings;Measurement;Association rules;Servers;Cloud computing;Maintenance engineering}, 
doi={10.1109/MSR.2015.12}, 
ISSN={2160-1852}, 
month={May},}
@INPROCEEDINGS{6975650, 
author={H. Cai and S. Jiang and R. Santelices and Y. Zhang and Y. Zhang}, 
booktitle={2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation}, 
title={SENSA: Sensitivity Analysis for Quantitative Change-Impact Prediction}, 
year={2014}, 
volume={}, 
number={}, 
pages={165-174}, 
abstract={Sensitivity analysis determines how a system responds to stimuli variations, which can benefit important software-engineering tasks such as change-impact analysis. We present SENSA, a novel dynamic-analysis technique and tool that combines sensitivity analysis and execution differencing to estimate the dependencies among statements that occur in practice. In addition to identifying dependencies, SENSA quantifies them to estimate how much or how likely a statement depends on another. Quantifying dependencies helps developers prioritize and focus their inspection of code relationships. To assess the benefits of quantifying dependencies with SENSA, we applied it to various statements across Java subjects to find and prioritize the potential impacts of changing those statements. We found that SENSA predicts the actual impacts of changes to those statements more accurately than static and dynamic forward slicing. Our SENSA prototype tool is freely available for download.}, 
keywords={program slicing;software engineering;sensitivity analysis;SENSA;quantitative change impact prediction;stimuli variations;software engineering tasks;dynamic analysis technique;code relationships;dynamic forward slicing;static forward slicing;History;Semantics;Runtime;Instruments;Sensitivity analysis;Syntactics;Change-impact prediction;dependence analysis;sensitivity analysis;execution differencing}, 
doi={10.1109/SCAM.2014.25}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7180063, 
author={M. Hashimoto and M. Terai and T. Maeda and K. Minami}, 
booktitle={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories}, 
title={Extracting Facts from Performance Tuning History of Scientific Applications for Predicting Effective Optimization Patterns}, 
year={2015}, 
volume={}, 
number={}, 
pages={13-23}, 
abstract={To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.}, 
keywords={data mining;optimisation;program compilers;performance tuning history;scientific applications;predicting effective optimization patterns;large-scale scientific applications;compiler options;program parameters;syntactic structure;combinatorial explosion;evidence based performance tuning;EBT;version control systems;program transformation;source code snapshots;pattern identification tool;data mining;Tuning;Arrays;Kernel;Ontologies;Phylogeny;History;Data mining;large-scale scientific computing;application performance tuning;abstract syntax tree differencing;semantic web;machine learning}, 
doi={10.1109/MSR.2015.9}, 
ISSN={2160-1852}, 
month={May},}
@INPROCEEDINGS{8169980, 
author={J. Liu and Y. Zhou and Y. Yang and H. Lu and B. Xu}, 
booktitle={2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)}, 
title={Code Churn: A Neglected Metric in Effort-Aware Just-in-Time Defect Prediction}, 
year={2017}, 
volume={}, 
number={}, 
pages={11-19}, 
abstract={Background: An increasing research effort has devoted to just-in-time (JIT) defect prediction. A recent study by Yang et al. at FSE'16 leveraged individual change metrics to build unsupervised JIT defect prediction model. They found that many unsupervised models performed similarly to or better than the state-of-the-art supervised models in effort-aware JIT defect prediction. Goal: In Yang et al.'s study, code churn (i.e. the change size of a code change) was neglected when building unsupervised defect prediction models. In this study, we aim to investigate the effectiveness of code churn based unsupervised defect prediction model in effort-aware JIT defect prediction. Methods: Consistent with Yang et al.'s work, we first use code churn to build a code churn based unsupervised model (CCUM). Then, we evaluate the prediction performance of CCUM against the state-of-the-art supervised and unsupervised models under the following three prediction settings: cross-validation, time-wise cross-validation, and cross-project prediction. Results: In our experiment, we compare CCUM against the state-of-the-art supervised and unsupervised JIT defect prediction models. Based on six open-source projects, our experimental results show that CCUM performs better than all the prior supervised and unsupervised models. Conclusions: The result suggests that future JIT defect prediction studies should use CCUM as a baseline model for comparison when a novel model is proposed.}, 
keywords={learning (artificial intelligence);public domain software;software metrics;source code (software);code churn;effort-aware just-in-time defect;just-in-time defect prediction;unsupervised JIT defect prediction model;effort-aware JIT defect prediction;code change;CCUM;cross-project prediction;supervised JIT defect prediction models;Predictive models;Measurement;Software;Buildings;Data models;Linear regression;Computational modeling;Defect;prediction;changes;just-in-time;code churn;unsupervised models;supervised models}, 
doi={10.1109/ESEM.2017.8}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7332454, 
author={G. Bavota and B. Russo}, 
booktitle={2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Four eyes are better than two: On the impact of code reviews on software quality}, 
year={2015}, 
volume={}, 
number={}, 
pages={81-90}, 
abstract={Code review is advocated as one of the best practices to improve software quality and reduce the likelihood of introducing defects during code change activities. Recent research has shown how code components having a high review coverage (i.e., a high proportion of reviewed changes) tend to be less involved in post-release fixing activities. Yet the relationship between code review and bug introduction or the overall software quality is still largely unexplored.}, 
keywords={software quality;source code (software);code review;software quality;code change activities;code components;review coverage;postrelease fixing activities;bug introduction;Data mining;Computer bugs;Androids;Humanoid robots;History;Software quality;Couplings;Code Review;Mining Software Repositories;Empirical Studies}, 
doi={10.1109/ICSM.2015.7332454}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8079826, 
author={L. Xia and A. Yadav and N. Duobiao}, 
booktitle={2016 13th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)}, 
title={Gated factored 3-way RBM for image transformation}, 
year={2016}, 
volume={}, 
number={}, 
pages={150-153}, 
abstract={The Factored 3-way Restricted Boltzmann Machine has encoded the image transformation successfully. But when utilize the code to unknown image, the result was much affected by the feature of training samples. Based on the model, we separated the transformation feature out of the hidden representation and designed a new probabilistic model with gate for learning distributed representations of image transformations. Inference in the model consists extracting the transformation, find the mapping code, training filters to fit for the affine or more general transformations. We also provide experimental results to validate the performance of our model to a various tasks.}, 
keywords={affine transforms;Boltzmann machines;image representation;learning (artificial intelligence);probability;unknown image;transformation feature;probabilistic model;image transformation;affine transformations;gated factored 3-way RBM;Restricted Boltzmann Machine;training samples;hidden representation;distributed representations;mapping code;Training;Transforms;Logic gates;Feature extraction;Encoding;Tensile stress;Writing;Transformation Coding;Factored;Restricted Boltzmann Machine}, 
doi={10.1109/ICCWAMTIP.2016.8079826}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7180070, 
author={M. B. Zanjani and H. Kagdi and C. Bird}, 
booktitle={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories}, 
title={Using Developer-Interaction Trails to Triage Change Requests}, 
year={2015}, 
volume={}, 
number={}, 
pages={88-98}, 
abstract={The paper presents an approach, namely iHDev, to recommend developers who are most likely to implement incoming change requests. The basic premise of iHDev is that the developers who interacted with the source code relevant to a given change request are most likely to best assist with its resolution. A machine-learning technique is first used to locate source code entities relevant to the textual description of a given change request. Ihdev then mines interaction trails (i.e., Mylyn sessions) associated with these source code entities to recommend a ranked list of developers. Ihdev integrates the interaction trails in a unique way to perform its task, which was not investigated previously. An empirical study on open source systems Mylyn and Eclipse Project was conducted to assess the effectiveness of iHDev. A number of change requests were used in the evaluated bench-mark. Recall for top one to five recommended developers and Mean Reciprocal Rank (MRR) values are reported. Furthermore, a comparative study with two previous approaches that use commit histories and/or the source code authorship information for developer recommendation was performed. Results show that iHDev could provide a recall gain of up to 127.27% with equivalent or improved MRR values by up to 112.5%.}, 
keywords={learning (artificial intelligence);program compilers;recommender systems;software maintenance;developer interaction trails;iHDev;source code;machine-learning technique;textual description;Eclipse Project;evaluated benchmark;mean reciprocal rank;source code authorship information;History;Computer bugs;Mathematical model;Software;Data mining;XML;Context}, 
doi={10.1109/MSR.2015.16}, 
ISSN={2160-1852}, 
month={May},}
@INPROCEEDINGS{7081846, 
author={Qingtao Jiang and X. Peng and Hai Wang and Z. Xing and W. Zhao}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Summarizing Evolutionary Trajectory by Grouping and Aggregating relevant code changes}, 
year={2015}, 
volume={}, 
number={}, 
pages={361-370}, 
abstract={The lifecycle of a large-scale software system can undergo many releases. Each release often involves hundreds or thousands of revisions committed by many developers over time. Many code changes are made in a systematic and collaborative way. However, such systematic and collaborative code changes are often undocumented and hidden in the evolution history of a software system. It is desirable to recover commonalities and associations among dispersed code changes in the evolutionary trajectory of a software system. In this paper, we present SETGA (Summarizing Evolutionary Trajectory by Grouping and Aggregation), an approach to summarizing historical commit records as trajectory patterns by grouping and aggregating relevant code changes committed over time. SETGA extracts change operations from a series of commit records from version control systems. It then groups extracted change operations by their common properties from different dimensions such as change operation types, developers and change locations. After that, SETGA aggregates relevant change operation groups by mining various associations among them. The proposed approach has been implemented and applied to three open-source systems. The results show that SETGA can identify various types of trajectory patterns that are useful for software evolution management and quality assurance.}, 
keywords={public domain software;software maintenance;software quality;large-scale software system;evolution history;SETGA;summarizing evolutionary trajectory by grouping and aggregation;historical commit records;trajectory patterns;relevant code changes;open-source systems;software evolution management;software quality assurance;Trajectory;Software systems;Data mining;History;Systematics;Evolution;Version Control System;Code Change;Pattern;Mining}, 
doi={10.1109/SANER.2015.7081846}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{7883396, 
author={T. D. Nguyen and A. T. Nguyen and T. N. Nguyen}, 
booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)}, 
title={Mapping API Elements for Code Migration with Vector Representations}, 
year={2016}, 
volume={}, 
number={}, 
pages={756-758}, 
abstract={Mapping API elements has a significant role in software development, especially in code migration. A manual process of defining the migration is tedious and error-prone while recent approaches to automatically mine API mappings are limited to discover the mappings with textually similar APIs' names. This leads to the low accuracy in existing migration tools.We propose an approach to automatically mine API mappings which overcomes the lexical mismatch problem. We represent an API by its usages instead of its name.To characterize an API with its context consisting of surrounding APIs in its usages, we take advantage of Word2Vec model to project the APIs of Java JDK and C# .NET into corresponding continuous vector spaces. The semantic relations among APIs will be observed in those continuous space as the geometric arrangements between their representation vectors in two vector spaces.We use a learning approach to derive the linear (e.g., rotating and scaling) transformation function between two vector spaces. Transformation function is trained from human-defined pairs of API mappings from Java to C#. To find the C# API mapping with a given Java API, we use the learned function to compute its transformed vector in the C# vector space. Then, the C# API which has the most similar vector with the transformed vector is considered as the result. Our experiment shows that for just one suggestion, we are able to correctly derive the API in C# in almost 43% of the cases. With 5 suggestions, we can correctly suggest the correct C# API in almost 3 out of 4 cases (73.2%).}, 
keywords={application program interfaces;C# language;data mining;Java;learning (artificial intelligence);software engineering;linear transformation function;learning approach;semantic relations;continuous vector spaces;C# .NET;Java JDK;Word2Vec model;migration tools;code migration;software development;vector representations;API elements mapping;C# languages;Java;Software engineering;Data mining;Training;Aerospace electronics;Libraries;Code and Language Migration;API Mappings;Word2Vec}, 
doi={}, 
ISSN={}, 
month={May},}
@ARTICLE{7817894, 
author={M. Tufano and F. Palomba and G. Bavota and R. Oliveto and M. D. Penta and A. De Lucia and D. Poshyvanyk}, 
journal={IEEE Transactions on Software Engineering}, 
title={When and Why Your Code Starts to Smell Bad (and Whether the Smells Go Away)}, 
year={2017}, 
volume={43}, 
number={11}, 
pages={1063-1088}, 
abstract={Technical debt is a metaphor introduced by Cunningham to indicate “not quite right code which we postpone making it right”. One noticeable symptom of technical debt is represented by code smells, defined as symptoms of poor design and implementation choices. Previous studies showed the negative impact of code smells on the comprehensibility and maintainability of code. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced, what is their survivability, and how they are removed by developers. To empirically corroborate such anecdotal evidence, we conducted a large empirical study over the change history of 200 open source projects. This study required the development of a strategy to identify smell-introducing commits, the mining of over half a million of commits, and the manual analysis and classification of over 10K of them. Our findings mostly contradict common wisdom, showing that most of the smell instances are introduced when an artifact is created and not as a result of its evolution. At the same time, 80 percent of smells survive in the system. Also, among the 20 percent of removed instances, only 9 percent are removed as a direct consequence of refactoring operations.}, 
keywords={data mining;public domain software;software maintenance;software quality;source code (software);open source projects;code smells;code comprehensibility;code maintainability;commits mining;smell instances;smell-introducing commits;code quality;technical debt;Ecosystems;History;Androids;Humanoid robots;Software systems;Maintenance engineering;Code smells;empirical study;mining software repositories}, 
doi={10.1109/TSE.2017.2653105}, 
ISSN={0098-5589}, 
month={Nov},}
@INPROCEEDINGS{6671308, 
author={A. Hora and N. Anquetil and S. Ducasse and M. T. Valente}, 
booktitle={2013 20th Working Conference on Reverse Engineering (WCRE)}, 
title={Mining system specific rules from change patterns}, 
year={2013}, 
volume={}, 
number={}, 
pages={331-340}, 
abstract={A significant percentage of warnings reported by tools to detect coding standard violations are false positives. Thus, there are some works dedicated to provide better rules by mining them from source code history, analyzing bug-fixes or changes between system releases. However, software evolves over time, and during development not only bugs are fixed, but also features are added, and code is refactored. In such cases, changes must be consistently applied in source code to avoid maintenance problems. In this paper, we propose to extract system specific rules by mining systematic changes over source code history, i.e., not just from bug-fixes or system releases, to ensure that changes are consistently applied over source code. We focus on structural changes done to support API modification or evolution with the goal of providing better rules to developers. Also, rules are mined from predefined rule patterns that ensure their quality. In order to assess the precision of such specific rules to detect real violations, we compare them with generic rules provided by tools to detect coding standard violations on four real world systems covering two programming languages. The results show that specific rules are more precise in identifying real violations in source code than generic ones, and thus can complement them.}, 
keywords={application program interfaces;data mining;program debugging;software maintenance;system specific rule mining;change patterns;source code history mining;bug-fixes analysis;code refactoring;system specific rule extraction;systematic change mining;structural changes;API modification;API evolution;coding standard violation detection;History;Data mining;Standards;Receivers;Encoding;Java;Software}, 
doi={10.1109/WCRE.2013.6671308}, 
ISSN={1095-1350}, 
month={Oct},}
@ARTICLE{7360924, 
author={Y. Xu and X. Fang and J. Wu and X. Li and D. Zhang}, 
journal={IEEE Transactions on Image Processing}, 
title={Discriminative Transfer Subspace Learning via Low-Rank and Sparse Representation}, 
year={2016}, 
volume={25}, 
number={2}, 
pages={850-863}, 
abstract={In this paper, we address the problem of unsupervised domain transfer learning in which no labels are available in the target domain. We use a transformation matrix to transfer both the source and target data to a common subspace, where each target sample can be represented by a combination of source samples such that the samples from different domains can be well interlaced. In this way, the discrepancy of the source and target domains is reduced. By imposing joint low-rank and sparse constraints on the reconstruction coefficient matrix, the global and local structures of data can be preserved. To enlarge the margins between different classes as much as possible and provide more freedom to diminish the discrepancy, a flexible linear classifier (projection) is obtained by learning a non-negative label relaxation matrix that allows the strict binary label matrix to relax into a slack variable matrix. Our method can avoid a potentially negative transfer by using a sparse matrix to model the noise and, thus, is more robust to different types of noise. We formulate our problem as a constrained low-rankness and sparsity minimization problem and solve it by the inexact augmented Lagrange multiplier method. Extensive experiments on various visual domain adaptation tasks show the superiority of the proposed method over the state-of-the art methods. The MATLAB code of our method will be publicly available at http://www.yongxu.org/lunwen.html.}, 
keywords={data structures;image classification;image reconstruction;image representation;matrix algebra;minimisation;unsupervised learning;MATLAB code;visual domain adaptation tasks;inexact augmented Lagrange multiplier method;sparsity minimization problem;constrained low-rankness minimization problem;slack variable matrix;strict binary label matrix;non negative label relaxation matrix;flexible linear classifier;local data structure;global data structure;reconstruction coefficient matrix;transformation matrix;unsupervised domain transfer learning;sparse representation;low-rank representation;discriminative transfer subspace learning;Sparse matrices;Adaptation models;Learning systems;Image reconstruction;Visualization;Electronic mail;Mathematical model;Source domain;target domain;low-rank and sparse constraints;knowledge transfer,;subspace learning;Source domain;target domain;low-rank and sparse constraints;knowledge transfer;subspace learning}, 
doi={10.1109/TIP.2015.2510498}, 
ISSN={1057-7149}, 
month={Feb},}
@INPROCEEDINGS{6619529, 
author={M. Staron and W. Meding and C. Höglund and P. Eriksson and J. Nilsson and J. Hansson}, 
booktitle={2013 39th Euromicro Conference on Software Engineering and Advanced Applications}, 
title={Identifying Implicit Architectural Dependencies Using Measures of Source Code Change Waves}, 
year={2013}, 
volume={}, 
number={}, 
pages={325-332}, 
abstract={The principles of Agile software development are increasingly used in large software development projects, e.g. using Scrum of Scrums or combining Agile and Lean development methods. When large software products are developed by self-organized, usually feature-oriented teams, there is a risk that architectural dependencies between software components become uncontrolled. In particular there is a risk that the prescriptive architecture models in form of diagrams are outdated and implicit architectural dependencies may become more frequent than the explicit ones. In this paper we present a method for automated discovery of potential dependencies between software components based on analyzing revision history of software repositories. The result of this method is a map of implicit dependencies which is used by architects in decisions on the evolution of the architecture. The software architects can assess the validity of the dependencies and can prevent unwanted component couplings and design erosion hence minimizing the risk of post-release quality problems. Our method was evaluated in a case study at one large product at Saab Electronic Defense Systems (Saab EDS) and one large software product at Ericsson AB.}, 
keywords={risk management;software architecture;software prototyping;software quality;implicit architectural dependency identification;source code change wave measures;agile software development projects;software products;self-organized team;feature-oriented teams;software components;automated potential dependency discovery;revision history analysis;software repositories;implicit dependencies;software architecture;software architects;unwanted component coupling prevention;design erosion prevention;risk minimization;post-release quality problems;Saab Electronic Defense Systems;Saab EDS;Ericsson AB;Software;Computer architecture;Companies;Software measurement;Monitoring;measure;metric;mining software repositories;quality;dependency;architecture;case study;industry}, 
doi={10.1109/SEAA.2013.9}, 
ISSN={1089-6503}, 
month={Sep.},}
@INPROCEEDINGS{6227182, 
author={S. Ifrah and D. H. Lorenz}, 
booktitle={2012 34th International Conference on Software Engineering (ICSE)}, 
title={Crosscutting revision control system}, 
year={2012}, 
volume={}, 
number={}, 
pages={321-330}, 
abstract={Large and medium scale software projects often require a source code revision control (RC) system. Unfortunately, RC systems do not perform well with obliviousness and quantification found in aspect-oriented code. When classes are oblivious to aspects, so is the RC system, and the crosscutting effect of aspects is not tracked. In this work, we study this problem in the context of using AspectJ (a standard AOP language) with Subversion (a standard RC system). We describe scenarios where the crosscutting effect of aspects combined with the concurrent changes that RC supports can lead to inconsistent states of the code. The work contributes a mechanism that checks-in with the source code versions of crosscutting metadata for tracking the effect of aspects. Another contribution of this work is the implementation of a supporting Eclipse plug-in (named XRC) that extends the JDT, AJDT, and SVN plug-ins for Eclipse to provide crosscutting revision control (XRC) for aspect-oriented programming.},
keywords={aspect-oriented programming;configuration management;Java;software maintenance;crosscutting revision control system;large and medium scale software project;source code revision control;RC system;aspect-oriented code;crosscutting effect;AspectJ;standard AOP language;Subversion;concurrent change;source code version;crosscutting metadata;Eclipse plug-in;XRC;AJDT plug-in;SVN plug-in;aspect-oriented programming;version control;Visualization;Control systems;Programming;Engines;History;Weaving;Software;revision control;version control;aspects}, 
doi={10.1109/ICSE.2012.6227182}, 
ISSN={1558-1225}, 
month={June},}
@INPROCEEDINGS{6175252, 
author={S. N. Beevi and D. C. Prasad and S. S. V. Chandra}, 
booktitle={2012 International Conference on Power, Signals, Controls and Computation}, 
title={Enhancing flexibility and portability of Execution Preserving Language Transformation using Meta programming}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={This paper describes flexibility and effectiveness of Execution Preserving Language Transformation (EPLT) using a meta framework. Program transformation is visualized as transforming the program written in legacy code to a more contemporary environment. Pure program transformation systems translate source code to target code preserving the functionality of legacy systems. Augmented versions of existing languages can be developed by combining good properties of two languages. In this work a meta framework is developed from C++ and Java language. The growing popularity of Java language forces the programmer to implement data structures and algorithms of other languages in Java. This meta framework enhances the conversion of unsafe source code written in C++ and Java to safe byte code. It provides a transformational scheme which unifies the syntax and semantics of existing languages and reduce the learning curves.}, 
keywords={C++ language;Java;program interpreters;execution preserving language transformation;meta programming;EPLT;legacy code;pure program transformation systems;source code;target code;C++;Java language;Java;Syntactics;Grammar;Data structures;Programming;Generators;Metaprogramming;lex;yacc;C++;Java}, 
doi={10.1109/EPSCICON.2012.6175252}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{6227172, 
author={M. Gethers and B. Dit and H. Kagdi and D. Poshyvanyk}, 
booktitle={2012 34th International Conference on Software Engineering (ICSE)}, 
title={Integrated impact analysis for managing software changes}, 
year={2012}, 
volume={}, 
number={}, 
pages={430-440}, 
abstract={The paper presents an adaptive approach to perform impact analysis from a given change request to source code. Given a textual change request (e.g., a bug report), a single snapshot (release) of source code, indexed using Latent Semantic Indexing, is used to estimate the impact set. Should additional contextual information be available, the approach configures the best-fit combination to produce an improved impact set. Contextual information includes the execution trace and an initial source code entity verified for change. Combinations of information retrieval, dynamic analysis, and data mining of past source code commits are considered. The research hypothesis is that these combinations help counter the precision or recall deficit of individual techniques and improve the overall accuracy. The tandem operation of the three techniques sets it apart from other related solutions. Automation along with the effective utilization of two key sources of developer knowledge, which are often overlooked in impact analysis at the change request level, is achieved. To validate our approach, we conducted an empirical evaluation on four open source software systems. A benchmark consisting of a number of maintenance issues, such as feature requests and bug fixes, and their associated source code changes was established by manual examination of these systems and their change history. Our results indicate that there are combinations formed from the augmented developer contextual information that show statistically significant improvement over standalone approaches.}, 
keywords={data mining;indexing;information retrieval;management of change;public domain software;software maintenance;software management;system monitoring;integrated impact analysis;software change management;adaptive approach;source code single snapshot;textual change request;latent semantic indexing;impact set estimation;best-fit combination;contextual information;execution trace;initial source code entity;information retrieval;dynamic analysis;data mining;developer knowledge source;open source software systems;maintenance issues;Software;Couplings;Data mining;Maintenance engineering;History;Information retrieval;Automation}, 
doi={10.1109/ICSE.2012.6227172}, 
ISSN={1558-1225}, 
month={June},}
@INPROCEEDINGS{7809767, 
author={V. Musco and A. Carette and M. Monperrus and P. Preux}, 
booktitle={2016 IEEE/ACM 5th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE)}, 
title={A Learning Algorithm for Change Impact Prediction}, 
year={2016}, 
volume={}, 
number={}, 
pages={8-14}, 
abstract={Change impact analysis (CIA) consists in predicting the impact of a code change in a software application. In this paper, the artifacts that are considered for CIA are methods of object-oriented software; the change under study is a change in the code of the method, the impact is the test methods that fail because of the change that has been performed. We propose LCIP, a learning algorithm that learns from past impacts to predict future impacts. To evaluate LCIP, we consider Java software applications that are strongly tested. We simulate 6000 changes and their actual impact through code mutations, as done in mutation testing. We find that LCIP can predict the impact with a precision of 74%, a recall of 85%, corresponding to a F-score of 64%. This shows that taking a learning perspective on change impact analysis let us achieve good precision and recall in change impact analysis.}, 
keywords={Java;learning (artificial intelligence);object-oriented methods;program testing;change impact analysis;code change prediction;software application;CIA;LCIP;learning algorithm;Java software applications;object-oriented software;code mutations;mutation testing;F-score;Software;Prediction algorithms;Algorithm design and analysis;Software algorithms;Artificial intelligence;Java;Testing}, 
doi={10.1109/RAISE.2016.010}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7816458, 
author={M. Rahimi and W. Goss and J. Cleland-Huang}, 
booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Evolving Requirements-to-Code Trace Links across Versions of a Software System}, 
year={2016}, 
volume={}, 
number={}, 
pages={99-109}, 
abstract={Trace links provide critical support for numerous software engineering activities including safety analysis, compliance verification, test-case selection, and impact prediction. However, as the system evolves over time, there is a tendency for the quality of trace links to degrade into a tangle of inaccurate and untrusted links. This is especially true with the links between source-code and upstream artifacts such as requirements - because developers frequently refactor and change code without updating the links. We present TLE (Trace Link Evolver), a solution for automating the evolution of trace links as changes are introduced to source code. We use a set of heuristics, open source tools, and information retrieval methods to detect common change scenarios across different versions of software. Each change scenario is then associated with a set of link evolution heuristics which are used to evolve trace links. We evaluate our approach through a controlled experiment and also through applying it across 27 releases of the Cassandra Database System. Results show that the trace links evolved using our approach are significantly more accurate than those generated using information retrieval alone.}, 
keywords={formal specification;formal verification;information retrieval;program diagnostics;public domain software;software tools;source code (software);systems analysis;requirements-to-code trace link;software system version;software engineering;source code;upstream artifact;trace link evolver;TLE;open source tool;information retrieval;traceability;Feature extraction;Crawlers;Conferences;Software systems;Computer science;Traceability;Evolution;Maintenance}, 
doi={10.1109/ICSME.2016.57}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8543406, 
author={S. Mertoguno and R. Craven and D. Koller and M. Mickelson}, 
booktitle={2018 IEEE Cybersecurity Development (SecDev)}, 
title={Reducing Attack Surface via Executable Transformation}, 
year={2018}, 
volume={}, 
number={}, 
pages={138-138}, 
abstract={Modern software development and deployment practices encourage complexity and bloat while unintentionally sacrificing efficiency and security. A major driver in this is the overwhelming emphasis on programmers' productivity. The constant demands to speed up development while reducing costs have forced a series of individual decisions and approaches throughout software engineering history that have led to this point. The current state-of-the-practice in the field is a patchwork of architectures and frameworks, packed full of features in order to appeal to: the greatest number of people, obscure use cases, maximal code reuse, and minimal developer effort. The Office of Naval Research (ONR) Total Platform Cyber Protection (TPCP) program seeks to de-bloat software binaries late in the life-cycle with little or no access to the source code or the development process.}, 
keywords={security of data;software maintenance;software reusability;telecommunication security;attack surface;executable transformation;modern software development;deployment practices;security;overwhelming emphasis;constant demands;individual decisions;software engineering history;obscure use cases;maximal code reuse;minimal developer effort;de-bloat software binaries;individual approaches;programmers productivity;office of naval research;total platform cyber protection;ONR;TPCP;Software;Productivity;Libraries;Complexity theory;Security;Conferences;Software engineering;Binary;Binary Transformation;Attack Surface;Debloat;ONR;TPCP;Navy;Late Stage Customization}, 
doi={10.1109/SecDev.2018.00034}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6883057, 
author={M. Erwig and K. Smeltzer and K. Xu}, 
booktitle={2014 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
title={A notation for non-linear program edits}, 
year={2014}, 
volume={}, 
number={}, 
pages={205-206}, 
abstract={We present a visual notation to support the understanding and reasoning about program edits. The graph-based representation directly supports a number of editing operations beyond those offered by a typical, linear program-edit model and makes obvious otherwise hidden states the code can reach through selectively undoing or redoing changes.}, 
keywords={reasoning about programs;source code (software);text editing;visual programming;nonlinear program edit model;visual notation;reasoning about program edits;graph-based representation;editing operations;code;History;Educational institutions;Cognition;Programming;Analytical models;Software;Software engineering}, 
doi={10.1109/VLHCC.2014.6883057}, 
ISSN={1943-6092}, 
month={July},}
@INPROCEEDINGS{6958061, 
author={uhang He and hi Chen and ifeng Pan and ai Ni}, 
booktitle={17th International IEEE Conference on Intelligent Transportation Systems (ITSC)}, 
title={Using edit distance and junction feature to detect and recognize arrow road marking}, 
year={2014}, 
volume={}, 
number={}, 
pages={2317-2323}, 
abstract={Arrow road markings usually appear on freeway surface and they convey important navigation information to autonomous driving. But detecting and recognizing them is a tough task because they suffer from numerous deviations like objects' interference and themselves' abrasion, etc. We therefore propose a novel local junction feature (L-junction) to describe each road marking as a junction string, different deviation is dispersed into different junction. We encode those junctions within a range as the same code. To measure the similarity between detected junction string and ground truth junction string, we design a weighted edit distance strategy and assign different deviation with different weight so that our framework is robust enough to deviations in arrow road marking but sensitive to non- arrow road markings' deviations. To test our framework, we collect three freeway datasets with our self-driving car: clean/dirty arrow road marking images (300 images respectively), a video dataset (arrow road marking and non-road marking images (670 images)). Another deep learning framework (Boosting+Convolutional Deep Neural Network (CDNN)) is also implemented for comparison. Extensive experimental results well demonstrate the superior performance of our framework.}, 
keywords={driver information systems;image recognition;learning (artificial intelligence);neural nets;object detection;video coding;freeway surface;navigation information;autonomous driving;local junction feature;L-junction;junction encoding;ground truth junction string;weighted edit distance strategy;arrow road marking;nonarrow road marking deviations;freeway datasets;self-driving car;dirty arrow road marking image;video dataset;deep learning framework;boosting-convolutional deep neural network;CDNN;arrow road marking detection;arrow road marking recognition;clean arrow road marking images;Junctions;Roads;Encoding;Topology;Image recognition;Feature extraction;Boosting}, 
doi={10.1109/ITSC.2014.6958061}, 
ISSN={2153-0009}, 
month={Oct},}
@INPROCEEDINGS{7356532, 
author={T. W. Davis and G. S. Kane}, 
booktitle={2015 IEEE AUTOTESTCON}, 
title={Deploying an enterprise-class Software Lifecycle Management solution for Test Program Sets}, 
year={2015}, 
volume={}, 
number={}, 
pages={450-455}, 
abstract={Information Technology tools for Software Lifecycle Management have advanced in recent years to enable organizations to implement a more centralized, enterprise-level lifecycle management solution. Despite these tools having become commonly used in the software development world, the Test Program Set (TPS) community remained behind the times. This paper presents the history and implementation of TPS Lifecycle Management as deployed by the TPS community for the Consolidated Automated Support System (CASS) Family of Testers (FoT). Prior to any efforts to enact centralized TPS lifecycle (i.e. configuration) management, TPS owners controlled their software in a variety of ways. The results of their efforts could be characterized as barely effective and often problematic. The initial attempt at solving the TPS lifecycle management problem was to mandate the use of a commercially available product which was not well supported by the developer of the application. That, coupled with both political issues and technical problems, relegated this system to becoming simply a repository for data. After addressing many of the problems with the initial solution and moving to a more widely-used product, the next iteration of TPS lifecycle management has been much more successful. It has become a legitimate enterprise solution providing both version control and implementing change management principles to include traceability between software change requests and released code. Many of the specific design details are discussed in the paper and include: a flexible workflow, a defined code branching strategy, an ownership and protection scheme and a process for release. The paper concludes with a detailed example of how TPS lifecycle management has benefitted the CASS team and, in particular, those who provide support of TPSs. It also shows how those benefits can be attained by other TPS support teams looking to effectively manage their test programs on other types of Automatic Test Equipment.}, 
keywords={configuration management;management of change;program testing;software development management;enterprise-class software lifecycle management solution;test program sets;information technology tools;centralized enterprise-level lifecycle management solution;TPS lifecycle management;consolidated automated support system;CASS;family of testers;FoT;version control;change management principles;software change requests;software released code;flexible workflow;code branching strategy;ownership and protection scheme;automatic test equipment;Servers;Project management;Best practices;File systems;Lead;Organizations;tps;management;configuration;lifecycle;cass;software;support}, 
doi={10.1109/AUTEST.2015.7356532}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7758034, 
author={Viet Anh Phan and Ngoc Phuong Chau and Minh Le Nguyen}, 
booktitle={2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)}, 
title={Exploiting tree structures for classifying programs by functionalities}, 
year={2016}, 
volume={}, 
number={}, 
pages={85-90}, 
abstract={Analyzing source code to solve software engineering problems such as fault prediction, cost, and effort estimation always receives attention of researchers as well as companies. The traditional approaches are based on machine learning, and software metrics obtained by computing standard measures of software projects. However, these methods have faced many challenges due to limitations of using software metrics which were not enough to capture the complexity of programs. The aim of this paper is to apply several natural language processing techniques, which deal with software engineering problems by exploring information of programs' abstract syntax trees (ASTs) instead of software metrics. To speed up computational time, we propose a pruning tree technique to eliminate redundant branches of ASTs. In addition, the k-Nearest Neighbor (kNN) algorithm was adopted to compare with other methods whereby the distance between programs is measured by using the tree edit distance (TED) and the Levenshtein distance. These algorithms are evaluated based on the performance of solving 104-label program classification problem. The experiments show that due to the use of appropriate data structures although kNN is a simple machine learning algorithm, the classifiers achieve the promising results.}, 
keywords={computational linguistics;learning (artificial intelligence);natural language processing;pattern classification;software engineering;source code (software);tree data structures;source code;software engineering;natural language processing;program abstract syntax trees;pruning tree;AST;k-nearest neighbor algorithm;tree edit distance;TED;Levenshtein distance;program classification;data structures;machine learning;Software;Syntactics;Software engineering;Software metrics;Feature extraction;Convolutional codes;Machine learning algorithms}, 
doi={10.1109/KSE.2016.7758034}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7549384, 
author={J. Cong}, 
booktitle={2016 IEEE International Conference on Networking, Architecture and Storage (NAS)}, 
title={Keynote 1}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-2}, 
abstract={Customizable computing has been of interest to the research community for over three decades. The interest has intensified in the recent years as the power and energy become a significant limiting factor to the computing industry. For example, the energy consumed by the datacenters of some large internet service providers is well over 109 Kilowatt-hours. FPGA-based acceleration has shown 10-100X performance/energy efficiency over the general-purpose processors in many applications. With Intel's $17B acquisition of Altera completed in December 2015, customizable computing is going from advanced research projects into mainstream computing technologies. In this talk, I shall first present several successful examples of customizable computing from my lab on CPU+FPGA platforms in multiple application domains, including medical imaging, machine learning, and computational genomics. Programming effort remains to be a serious challenge. So, the second part of my talk discusses our ongoing effort in automating compilation with source-code level transformation and optimization coupled with high-level synthesis, as well as developing efficient runtime support for scheduling and transparent resource management for integration of FPGAs for cloud-scale acceleration.}, 
keywords={computer centres;electronic engineering computing;field programmable gate arrays;high level synthesis;Internet;microprocessor chips;optimisation;research and development;scheduling;source code (software);cloud-scale acceleration;FPGAs integration;transparent resource management;runtime scheduling support development;high-level synthesis;optimization;source-code level transformation;compilation automation;CPU+FPGA platforms;mainstream computing technologies;advanced research projects;Altera;Intel;general-purpose processors;FPGA-based acceleration;internet service providers;computing industry;datacenter scale;customizable computing}, 
doi={10.1109/NAS.2016.7549384}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8330247, 
author={R. Dantas and A. Carvalho and D. Marcílio and L. Fantin and U. Silva and W. Lucas and R. Bonifácio}, 
booktitle={2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={Reconciling the past and the present: An empirical study on the application of source code transformations to automatically rejuvenate Java programs}, 
year={2018}, 
volume={}, 
number={}, 
pages={497-501}, 
abstract={Software systems change frequently over time, either due to new business requirements or technology pressures. Programming languages evolve in a similar constant fashion, though when a language release introduces new programming constructs, older constructs and idioms might become obsolete. The coexistence between newer and older constructs leads to several problems, such as increased maintenance efforts and steeper learning curve for developers. In this paper we present a RASCAL Java transformation library that evolves legacy systems to use more recent programming language constructs (such as multi-catch and lambda expressions). In order to understand how relevant automatic software rejuvenation is, we submitted 2462 transformations to 40 open source projects via the GitHub pull request mechanism. Initial results show that simple transformations, for instance the introduction of the diamond operator, are more likely to be accepted than transformations that change the code substantially, such as refactoring enhanced for loops to the newer functional style.}, 
keywords={Java;learning (artificial intelligence);object-oriented programming;public domain software;software maintenance;software metrics;software quality;source code transformations;Java programs;business requirements;programming languages;idioms;steeper learning curve;RASCAL Java transformation library;legacy systems;lambda expressions;GitHub pull request mechanism;software systems;maintenance efforts;open source projects;programming language constructs;automatic software rejuvenation;Java;Aging;Tools;Libraries;Diamond;Syntactics}, 
doi={10.1109/SANER.2018.8330247}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8368455, 
author={G. Catolino and F. Ferrucci}, 
booktitle={2018 IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE)}, 
title={Ensemble techniques for software change prediction: A preliminary investigation}, 
year={2018}, 
volume={}, 
number={}, 
pages={25-30}, 
abstract={Predicting the classes more likely to change in the future helps developers to focus on the more critical parts of a software system, with the aim of preventively improving its maintainability. The research community has devoted a lot of effort in the definition of change prediction models, i.e., models exploiting a machine learning classifier to relate a set of independent variables to the change-proneness of classes. Besides the good performances of such models, key results of previous studies highlight how classifiers tend to perform similarly even though they are able to correctly predict the change-proneness of different code elements, possibly indicating the presence of some complementarity among them. In this paper, we aim at analyzing the extent to which ensemble methodologies, i.e., machine learning techniques able to combine multiple classifiers, can improve the performances of change-prediction models. Specifically, we empirically compared the performances of three ensemble techniques (i.e., Boosting, Random Forest, and Bagging) with those of standard machine learning classifiers (i.e., Logistic Regression and Naive Bayes). The study was conducted on eight open source systems and the results showed how ensemble techniques, in some cases, perform better than standard machine learning approaches, even if the differences among them is small. This requires the need of further research aimed at devising effective methodologies to ensemble different classifiers.}, 
keywords={learning (artificial intelligence);pattern classification;software maintenance;change prediction models;ensemble methodologies;multiple classifiers;standard machine learning classifiers;software change prediction;software system;Predictive models;Measurement;Standards;Training;Boosting;Analytical models;Change Prediction;Ensemble Techniques;Empirical Study}, 
doi={10.1109/MALTESQUE.2018.8368455}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6340250, 
author={Y. Malheiros and A. Moraes and C. Trindade and S. Meira}, 
booktitle={2012 IEEE 36th Annual Computer Software and Applications Conference}, 
title={A Source Code Recommender System to Support Newcomers}, 
year={2012}, 
volume={}, 
number={}, 
pages={19-24}, 
abstract={Newcomers in a software development project often need assistance to complete their first tasks. Then a mentor, an experienced member of the team, usually teaches the newcomers what they need to complete their tasks. But, to allocate an experienced member of a team to teach a newcomer during a long time is neither always possible nor desirable, because the mentor could be more helpful doing more important tasks. During the development the team interacts with a version control system, bug tracking and mailing lists, and all these tools record data creating the project memory. Recommender systems can use the project memory to help newcomers in some tasks answering their questions, thus in some cases the developers do not need a mentor. In this paper we present Mentor, a recommender system to help newcomers to solve change requests. Mentor uses the Prediction by Partial Matching (PPM) algorithm and some heuristics to analyze the change requests, and the version control data, and recommend potentially relevant source code that will help the developer in the change request solution. We did three experiments to compare the PPM algorithm with the Latent Semantic Indexing (LSI). Using PPM we achieved results for recall rate between 37% and 66.8%, and using LSI the results were between 20.3% and 51.6%.}, 
keywords={configuration management;indexing;pattern matching;program debugging;recommender systems;software maintenance;source code recommender system;software development project;newcomers;experienced team member;version control system;bug tracking;mailing lists;project memory;Mentor;recommender system;prediction by partial matching algorithm;version control data;PPM algorithm;latent semantic indexing;LSI;Large scale integration;Context;Entropy;Measurement;Recommender systems;Databases;Software;recommender systems;software engineering;software maintenance;information theory}, 
doi={10.1109/COMPSAC.2012.11}, 
ISSN={0730-3157}, 
month={July},}
@INPROCEEDINGS{7273434, 
author={K. Mori and O. Mizuno}, 
booktitle={2015 IEEE 39th Annual Computer Software and Applications Conference}, 
title={An Implementation of Just-in-Time Fault-Prone Prediction Technique Using Text Classifier}, 
year={2015}, 
volume={3}, 
number={}, 
pages={609-612}, 
abstract={Since the fault prediction is an important technique to help allocating software maintenance effort, much research on fault prediction has been proposed so far. The goal of these studies is applying their prediction technique to actual software development. In this paper, we implemented a prototype fault-prone module prediction tool using a text-filtering based technique named "Fault-Prone Filtering". Our tool aims to show the result of fault prediction for each change (i.e., Commits) as a probability that a source code file to be faulty. The result is shown on a Web page and easy to track the histories of prediction. A case study performed on three open source projects shows that our tool could detect 90 percent of the actual fault modules (i.e., The recall of 0.9) with the accuracy of 0.67 and the precision of 0.63 on average.}, 
keywords={information filters;pattern classification;public domain software;software fault tolerance;software maintenance;source code (software);text analysis;Web sites;just-in-time fault-prone prediction technique;text classifier;fault prediction;software maintenance;software development;prototype fault-prone module prediction tool;text-filtering based technique;fault-prone filtering;probability;source code;Web page;open source project;Predictive models;Filtering;Data mining;Accuracy;Software maintenance;Databases;software development support tool;fault prediction;software maintenance;mining software repository;machine learning;spam filter}, 
doi={10.1109/COMPSAC.2015.143}, 
ISSN={0730-3157}, 
month={July},}
@INPROCEEDINGS{7985655, 
author={C. Vendome and M. Linares-Vásquez and G. Bavota and M. Di Penta and D. German and D. Poshyvanyk}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)}, 
title={Machine Learning-Based Detection of Open Source License Exceptions}, 
year={2017}, 
volume={}, 
number={}, 
pages={118-129}, 
abstract={From a legal perspective, software licenses govern the redistribution, reuse, and modification of software as both source and binary code. Free and Open Source Software (FOSS) licenses vary in the degree to which they are permissive or restrictive in allowing redistribution or modification under licenses different from the original one(s). In certain cases, developers may modify the license by appending to it an exception to specifically allow reuse or modification under a particular condition. These exceptions are an important factor to consider for license compliance analysis since they modify the standard (and widely understood) terms of the original license. In this work, we first perform a large-scale empirical study on the change history of over 51K FOSS systems aimed at quantitatively investigating the prevalence of known license exceptions and identifying new ones. Subsequently, we performed a study on the detection of license exceptions by relying on machine learning. We evaluated the license exception classification with four different supervised learners and sensitivity analysis. Finally, we present a categorization of license exceptions and explain their implications.}, 
keywords={learning (artificial intelligence);pattern classification;public domain software;software reusability;source code (software);sensitivity analysis;supervised learners;license exception classification;license compliance analysis;FOSS licenses;free and open source software;source code;binary code;software reuse;software redistribution;software modification;software licenses;open source license exceptions;machine learning-based detection;Software engineering;Software Licenses;Empirical Studies;Classifiers}, 
doi={10.1109/ICSE.2017.19}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{7503710, 
author={V. Singh and L. L. Pollock and W. Snipes and N. A. Kraft}, 
booktitle={2016 IEEE 24th International Conference on Program Comprehension (ICPC)}, 
title={A case study of program comprehension effort and technical debt estimations}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper describes a case study of using developer activity logs as indicators of a program comprehension effort by analyzing temporal sequences of developer actions (e.g., navigation and edit actions). We analyze developer activity data spanning 109,065 events and 69 hours of work on a medium-sized industrial application. We examine potential correlations between different measures of developer activity, code change metrics and code smells to gain insight into questions that could direct future technical debt interest estimation. To gain more insights into the data, we follow our analysis with commit message analysis and a developer interview. Our results indicate that developer activity as an estimate of program comprehension effort is correlated with both change proneness and static metrics for code smells.}, 
keywords={program diagnostics;software metrics;source code (software);developer activity logs;program comprehension effort;temporal sequence analysis;developer activity data analysis;medium-sized industrial application;code change metrics;code smells;technical debt interest estimation;commit message analysis;change proneness;static metrics;Measurement;Couplings;Navigation;Software;Maintenance engineering;Correlation;History;program comprehension effort;technical debt interest;developer activity logging;code smells}, 
doi={10.1109/ICPC.2016.7503710}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7178187, 
author={R. Rama Varior and G. Wang}, 
booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={A data-driven color feature learning scheme for image retrieval}, 
year={2015}, 
volume={}, 
number={}, 
pages={1334-1338}, 
abstract={This paper addresses content based image retrieval based on color features. Several previous works have addressed color based image retrieval based on hand-crafted features. In this paper, a data-driven learning framework is proposed for generating color based signatures. To obtain the features, a linear transformation is learned from the pixel values based on its reconstruction error. Using this linear transformation, the original pixel values are transformed into a higher dimensional space. In the higher dimensional space, a dictionary is learned to obtain the sparse codes of the pixels. A max pooling strategy is used to obtain the dominant color features of a region and the final feature vector for an image is obtained by concatenating the pooled features. We evaluate our approach following the standard evaluation criteria for the INRIA Holidays and University of Kentucky Benchmark datasets. The approach is compared with several baselines such as histograms in RGB, HSV, YUV and Lab color spaces and several other color based features proposed for addressing this problem. Our approach shows competitive results on these datasets and outperforms all the baselines.}, 
keywords={feature extraction;image colour analysis;image reconstruction;image retrieval;learning (artificial intelligence);data-driven color feature learning scheme;image retrieval;hand crafted feature;image pixel;image reconstruction error;linear transformation;dictionary learning;sparse code;max pooling strategy;Image color analysis;Dictionaries;Image retrieval;Histograms;Standards;Encoding;Computer vision;Image retrieval;color features;feature learning;data-driven framework}, 
doi={10.1109/ICASSP.2015.7178187}, 
ISSN={1520-6149}, 
month={April},}
@INPROCEEDINGS{6975631, 
author={M. Mondal and C. K. Roy and K. A. Schneider}, 
booktitle={2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation}, 
title={Automatic Identification of Important Clones for Refactoring and Tracking}, 
year={2014}, 
volume={}, 
number={}, 
pages={11-20}, 
abstract={Code cloning is a controversial software engineering practice due to contradictory claims regarding its impacts on software evolution and maintenance. While a number of studies identify some positive aspects of code clones, there is strong empirical evidence of some negative impacts of clones too. Focusing on the issues related to clones researchers suggest to manage code clones through detection, refactoring, and tracking. However, all clones in a software system are not suitable for refactoring or tracking. Thus, it is important to identify which clones we should consider for refactoring and which clones should be considered for tracking. In this research work we apply the concept of evolutionary coupling to identify clones that are important for refactoring or tracking. By mining software evolution history, we determine and analyze constrained association rules of clone fragments that evolved following a particular change pattern called Similarity Preserving Change Pattern and are important from the perspective of refactoring and tracking. According to our investigation with rigorous manual analysis on thousands of revisions of six diverse subject systems covering two programming languages, overall 13.20% of all clones in a software system are important candidates for refactoring, and overall 10.27% of all clones are important candidates for tracking. Our implemented system can automatically identify these important candidates and thus, can help us in better maintenance of code clones in terms of refactoring and tracking.}, 
keywords={software maintenance;automatic identification;important clones;refactoring;tracking;code cloning;software engineering;software evolution;software maintenance;evolutionary coupling;software evolution history mining;constrained association rules;similarity preserving change pattern;programming languages;Cloning;Couplings;Software systems;Association rules;Maintenance engineering;Java;Code Clones;Clone Tracking;Clone Refactoring;Evolutionary Coupling;Association Rule}, 
doi={10.1109/SCAM.2014.11}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6178857, 
author={J. J´sz and L. Schrettner and Á. Beszédes and C. Osztrogon´c and T. Gyimóthy}, 
booktitle={2012 16th European Conference on Software Maintenance and Reengineering}, 
title={Impact Analysis Using Static Execute After in WebKit}, 
year={2012}, 
volume={}, 
number={}, 
pages={95-104}, 
abstract={Insufficient propagation of changes causes the majority of regression errors in heavily evolving software systems. Impact analysis of a particular change can help identify those parts of the system that also need to be investigated and potentially propagate the change. A static code analysis technique called Static Execute After can be used to automatically infer such impact sets. The method is safe and comparable in precision to more detailed analyses. At the same time it is significantly more efficient, hence we could apply it to different large industrial systems, including the open source Web Kit project. We overview the benefits of the method, its existing implementations, and present our experiences in adapting the method to such a complex project. Finally, using this particular analysis on the Web Kit project, we verify whether applying the method we can actually predict the required change propagation and hence reduce regression errors. We report on the properties of the resulting impact sets computed for the change history, and their relationship to the actual fixes required. We looked at actual defects provided by the regression test suite along with their fixes taken from the version control repository, and compared these fixes to the predicted impact sets computed at the changes that caused the failing tests. The results show that the method is applicable for the analysis of the system, and that the impact sets can predict the required changes in a fair amount of cases, but that there are still open issues for the improvement of the method.}, 
keywords={Internet;program compilers;program diagnostics;program testing;public domain software;regression analysis;statistical testing;impact analysis;static execute after;WebKit;insufficient propagation;regression errors;heavily evolving software systems;static code analysis technique;industrial systems;open source Web Kit project;change propagation;change history;regression test suite;version control repository;failing tests;system analysis;Algorithm design and analysis;Software algorithms;Software systems;Prediction algorithms;Testing;Approximation methods;Change impact analysis;source code analysis;Static Execute After;regression testing}, 
doi={10.1109/CSMR.2012.20}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{7476642, 
author={P. Tourani and B. Adams}, 
booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={The Impact of Human Discussions on Just-in-Time Quality Assurance: An Empirical Study on OpenStack and Eclipse}, 
year={2016}, 
volume={1}, 
number={}, 
pages={189-200}, 
abstract={In order to spot defect-introducing code changes during review before they are integrated into a project's version control system, a variety of defect prediction models have been designed. Most of these models focus exclusively on source code properties, like the number of added or deleted lines, or developer-related measures like experience. However, a code change is only the outcome of a much longer process, involving discussions on an issue report and review discussions on (different versions of) a patch. % Ignoring the characteristics of these activities during prediction is unfortunate, since Similar to how body language implicitly can reveal a person's real feelings, the length, intensity or positivity of these discussions can provide important additional clues about how risky a particular patch is or how confident developers and reviewers are about the patch. In this paper, we build logistic regression models to study the impact of the characteristics of issue and review discussions on the defect-proneness of a patch. Comparison of these models to conventional source code-based models shows that issue and review metrics combined improve precision and recall of the explanatory models up to 10%. Review time and issue discussion lag are amongst the most important metrics, having a positive (i.e., increasing) relation with defect-proneness.}, 
keywords={configuration management;just-in-time;project management;quality assurance;regression analysis;software fault tolerance;software quality;software reviews;source code (software);human discussions;just-in-time quality assurance;OpenStack;Eclipse;defect-introducing code changes;project version control system;defect prediction models;source code properties;developer-related measures;person real feelings;logistic regression models;source code-based models;review metrics;precision;recall;review time;issue discussion lag;defect-proneness;Measurement;Predictive models;Software;Control systems;Biological system modeling;Data models;Databases;Just-In-Time Quality Assurance;Human Discussion Metrics}, 
doi={10.1109/SANER.2016.113}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6676914, 
author={M. Martinez and L. Duchien and M. Monperrus}, 
booktitle={2013 IEEE International Conference on Software Maintenance}, 
title={Automatically Extracting Instances of Code Change Patterns with AST Analysis}, 
year={2013}, 
volume={}, 
number={}, 
pages={388-391}, 
abstract={A code change pattern represents a kind of recurrent modification in software. For instance, a known code change pattern consists of the change of the conditional expression of an if statement. Previous work has identified different change patterns. Complementary to the identification and definition of change patterns, the automatic extraction of pattern instances is essential to measure their empirical importance. For example, it enables one to count and compare the number of conditional expression changes in the history of different projects. In this paper we present a novel approach for search patterns instances from software history. Our technique is based on the analysis of Abstract Syntax Trees (AST) files within a given commit. We validate our approach by counting instances of 18 change patterns in 6 open-source Java projects.}, 
keywords={computational linguistics;program diagnostics;software maintenance;code change pattern;recurrent software modification;pattern instances automatic extraction;search patterns instances;software history;abstract syntax trees;AST files;open-source Java projects;History;Java;Syntactics;Open source software;Context;Catalogs}, 
doi={10.1109/ICSM.2013.54}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{6624019, 
author={B. Dit and A. Holtzhauer and D. Poshyvanyk and H. Kagdi}, 
booktitle={2013 10th Working Conference on Mining Software Repositories (MSR)}, 
title={A dataset from change history to support evaluation of software maintenance tasks}, 
year={2013}, 
volume={}, 
number={}, 
pages={131-134}, 
abstract={Approaches that support software maintenance need to be evaluated and compared against existing ones, in order to demonstrate their usefulness in practice. However, oftentimes the lack of well-established sets of benchmarks leads to situations where these approaches are evaluated using different datasets, which results in biased comparisons. In this data paper we describe and make publicly available a set of benchmarks from six Java applications, which can be used in the evaluation of various software engineering (SE) tasks, such as feature location and impact analysis. These datasets consist of textual description of change requests, the locations in the source code where they were implemented, and execution traces. Four of the benchmarks were already used in several SE research papers, and two of them are new. In addition, we describe in detail the methodology used for generating these benchmarks and provide a suite of tools in order to encourage other researchers to validate our datasets and generate new benchmarks for other subject software systems. Our online appendix: http://www.cs.wm.edu/semeru/data/msr13/.}, 
keywords={Java;software maintenance;source code;change request textual description;impact analysis;feature location;SE tasks;software engineering tasks;Java applications;software maintenance tasks;change history;Gold;Java;Software systems;Benchmark testing;Software maintenance;Large scale integration;Generate Benchmarks;datasets;feature location;impact analysis}, 
doi={10.1109/MSR.2013.6624019}, 
ISSN={2160-1860}, 
month={May},}
@ARTICLE{6749076, 
author={G. L. Ramos and C. G. Rego and A. R. Fonseca}, 
journal={IEEE Transactions on Magnetics}, 
title={Parallel Implementation of a Combined Moment Expansion and Spherical-Multipole Time-Domain Near-Field to Far-Field Transformation}, 
year={2014}, 
volume={50}, 
number={2}, 
pages={609-612}, 
abstract={A time-domain spherical-multipole near-to-far-field algorithm running in a parallelized code version is introduced in this paper. Such an approach is employed to obtain UWB antenna radiated fields and radiation patterns directly in time domain, which is more convenient to perform a unified characterization in time and frequency domains. We propose the use of the OpenMP, an application program interface that permits the code parallelization with almost no intervention. The results show that the proposed technique allows a code running 28 times faster when using a computer with 24 processors, each one with two threads, when compared with the sequential one. It also suggested a moment expansion technique to improve the accuracy and computational efficiency when using Gaussian pulse excitation when compared with the Fourier transform.}, 
keywords={antenna radiation patterns;application program interfaces;Fourier transforms;frequency-domain analysis;Gaussian distribution;time-domain analysis;ultra wideband antennas;combined moment expansion;spherical-multipole transformation;time-domain transformation;near-field transformation;far-field transformation;time-domain spherical-multipole algorithm;near-to-far-field algorithm;parallelized code version;UWB antenna radiated fields;radiation patterns;frequency domains;OpenMP;application program interface;code parallelization;processors;computational efficiency;Gaussian pulse excitation;Fourier transform;Antenna radiation patterns;Time-domain analysis;Ultra wideband antennas;Instruction sets;Frequency-domain analysis;Parallel processing;Finite difference methods;OpenMP;parallel processing;time-domain spherical-multipole expansion;UWB antenna characterization}, 
doi={10.1109/TMAG.2013.2280796}, 
ISSN={0018-9464}, 
month={Feb},}
@INPROCEEDINGS{7816476, 
author={J. Shimagaki and Y. Kamei and S. McIntosh and D. Pursehouse and N. Ubayashi}, 
booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Why are Commits Being Reverted?: A Comparative Study of Industrial and Open Source Projects}, 
year={2016}, 
volume={}, 
number={}, 
pages={301-311}, 
abstract={Software development is a cyclic process of integrating new features while introducing and fixing defects. During development, commits that modify source code files are uploaded to version control systems. Occasionally, these commits need to be reverted, i.e., the code changes need to be completely backed out of the software project. While one can often speculate about the purpose of reverted commits (e.g., the commit may have caused integration or build problems), little empirical evidence exists to substantiate such claims. The goal of this paper is to better understand why commits are reverted in large software systems. To that end, we quantitatively and qualitatively study two proprietary and four open source projects to measure: (1) the proportion of commits that are reverted, (2) the amount of time that commits that are eventually reverted linger within a codebase, and (3) the most frequent reasons why commits are reverted. Our results show that 1%-5% of the commits in the studied systems are reverted. Those commits that are eventually reverted linger within the studied codebases for 1-35 days (median). Furthermore, we identify 13 common reasons for reverting commits, and observe that the frequency of reverted commits of each reason varies broadly from project to project. A complementary qualitative analysis suggests that many reverted commits could have been avoided with better team communication and change awareness. Our findings made Sony Mobile's stakeholders aware that internally reverted commits can be reduced by paying more attention to their own changes. On the other hand, externally reverted commits could be minimized only if external stakeholders are involved to improve inter-company communication or requirements elicitation.}, 
keywords={project management;public domain software;software engineering;software management;source code (software);industrial project;open source project;software development;source code;version control systems;software project;reverted commits;large software systems;qualitative analysis;Mobile communication;Androids;Humanoid robots;Stakeholders;Software systems;Mobile handsets;mining software repository;software evolution;code review;code inspections;revert}, 
doi={10.1109/ICSME.2016.83}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6976099, 
author={W. Wang and M. W. Godfrey}, 
booktitle={2014 IEEE International Conference on Software Maintenance and Evolution}, 
title={Recommending Clones for Refactoring Using Design, Context, and History}, 
year={2014}, 
volume={}, 
number={}, 
pages={331-340}, 
abstract={Developers know that copy-pasting code (aka code cloning) is often a convenient shortcut to achieving a design goal, albeit one that carries risks to the code quality over time. However, deciding which, if any, clones should be eliminated within an existing system is a daunting task. Fixing a clone usually means performing an invasive refactoring, and not all clones may be worth the effort, cost, and risk that such a change entails. Furthermore, sometimes cloning fulfils a useful design role, and should not be refactored at al. And clone detection tools often return very large result sets, making it hard to choose which clones should be investigated and possibly removed. In this paper, we propose an automated approach to recommend clones for refactoring by training a decision tree-based classifier. We analyze more than 600 clone instances in three medium-to large-sized open source projects, and we collect features that are associated with the source code, the context, and the history of clone instances. Our approach achieves a precision of around 80% in recommending clone refactoring instances for each target system, and similarly good precision is achieved in cross-project evaluation. By recommending which clones are appropriate for refactoring, our approach allows for better resource allocation for refactoring itself after obtaining clone detection results, and can thus lead to improved clone management in practice.}, 
keywords={decision trees;pattern classification;software maintenance;decision tree-based classifier;open source projects;source code;clone instance history;clone instance context;clone refactoring instances;resource allocation;clone detection;clone management;code cloning;Cloning;Feature extraction;History;Context;Software systems;Detectors;Software Clones;Code Refactoring;Clone Management;Software Recommendation System}, 
doi={10.1109/ICSME.2014.55}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{8605988, 
author={L. Kaur and A. Mishra}, 
booktitle={2018 International Conference on Innovations in Information Technology (IIT)}, 
title={A comparative analysis of evolutionary algorithms for the prediction of software change}, 
year={2018}, 
volume={}, 
number={}, 
pages={187-192}, 
abstract={Change-proneness prediction of software components has become a significant research area wherein the quest for the best classifier still persists. Although numerous statistical and Machine Learning (ML) techniques have been presented and employed in the past literature for an efficient generation of change-proneness prediction models, evolutionary algorithms, on the other hand, remain vastly unexamined and unaddressed for this purpose. Bearing this in mind, this research work targets to probe the potency of six evolutionary algorithms for developing such change prediction models, specifically for source code files. We employ apposite object oriented metrics to construct four software datasets from four consecutive releases of a software project. Furthermore, the prediction capability of the selected evolutionary algorithms is evaluated, ranked and compared against two statistical classifiers using the Wilcoxon signed rank test and Friedman statistical test. On the basis of the results obtained from the experiments conducted in this article, it can be ascertained that the evolutionary algorithms possess a capability for predicting change-prone files with high accuracies, sometimes even higher than the selected statistical classifiers.}, 
keywords={Software;Predictive models;Measurement;Java;Evolutionary computation;Prediction algorithms;Object oriented modeling;Software change;source code change;Evolutionary algorithms;change prediction}, 
doi={10.1109/INNOVATIONS.2018.8605988}, 
ISSN={2325-5498}, 
month={Nov},}
@INPROCEEDINGS{6613838, 
author={M. Mondal and C. K. Roy and K. A. Schneider}, 
booktitle={2013 21st International Conference on Program Comprehension (ICPC)}, 
title={Insight into a method co-change pattern to identify highly coupled methods: An empirical study}, 
year={2013}, 
volume={}, 
number={}, 
pages={103-112}, 
abstract={In this paper, we describe an empirical study of a unique method co-change pattern that has the potential to pinpoint design deficiency in a software system. We automatically identify this pattern by inspecting the method co-change history using reasonable constraints on method association rules. We also investigate the effect of code clones on the method co-changes identified according to the pattern, because there is a common intuition that clone fragments from the same clone class often require corresponding changes to ensure they remain consistent with each other. According to our in-depth investigation on hundreds of revisions of seven open-source software systems considering three types of clones (Type 1, Type 2, Type 3), our identified pattern helps us detect methods that are logically coupled with multiple other methods and that exhibit a significantly higher modification frequency than other methods. We call the methods detected by the pattern MMCGs (Methods appearing in Multiple Commit Groups) considering the pattern semantic. MMCGs can be considered as the candidates for restructuring in order to minimize coupling as well as to reduce the change-proneness of a software system. According to our observation, code clones have a significant effect on method co-changes as well as on MMCGs. We believe that clone refactoring can help us minimize evolutionary coupling among methods.}, 
keywords={data mining;open systems;public domain software;software maintenance;method co-change pattern;highly coupled methods;pinpoint design deficiency;software system;pattern identification;method association rules;method co-change history;code clone effect;clone fragments;open-source software systems;detect methods;MMCGs pattern;methods appearing in multiple commit groups;pattern semantic;change-proneness reduction;evolutionary coupling minimization;clone refactoring;software maintenance;Cloning;Software systems;Couplings;Association rules;History;Java;Manuals;Association Rules;Evolutionary Coupling;Life Span;Modification Occurrence Rate;Method Co-change Pattern;Method Genealogy}, 
doi={10.1109/ICPC.2013.6613838}, 
ISSN={1092-8138}, 
month={May},}
@INPROCEEDINGS{8530720, 
author={S. Ozcan Kini and A. Tosun}, 
booktitle={2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
title={[Research Paper] Periodic Developer Metrics in Software Defect Prediction}, 
year={2018}, 
volume={}, 
number={}, 
pages={72-81}, 
abstract={Defect prediction studies have proposed several data-driven approaches, and recently, this field has put more emphasis on whether the people factor is associated software defects. Developer metrics can capture experience, code ownership, coding skills and techniques, and commit activities. These metrics have so far been measured at a specified snapshot of the codebase although developer's knowledge on a source module could change over time. In this paper, we propose to measure periodic developer experience with regard to contextual knowledge on files and directories. We extract periodic experience metrics capturing the previous activities of developers on source files and investigate the explanatory effect of these metrics on defects. We also use activity-based (churn) metrics to observe the performance of both metric types on defect prediction. We used two large-scale open source projects, Lucene and Jackrabbit, for model evaluation. We calculate periodic developer experience metrics and churn metrics at two granularity levels: file level and commit level. We build the models using five popular machine learning algorithms in defect prediction literature. The models with the two best performing algorithms are assessed in terms of Precision, Recall, False Positive Rate, and F-measure. The set of metrics that explains software defects the best is also identified using correlation-based feature selection method. Results show that periodic developer experience metrics extracted at file level are good merits for defect prediction, accompanied with churn. When there is not enough data to extract the contextual knowledge of developers on source files, churn metrics play an important role on defect prediction.}, 
keywords={feature selection;learning (artificial intelligence);public domain software;software metrics;software quality;coding skills;contextual knowledge;source files;activity-based metrics;large-scale open source projects;periodic developer experience metrics;churn metrics;file level;defect prediction literature;periodic developer metrics;software defect prediction;coding techniques;commit level;machine learning algorithms;correlation-based feature selection method;developers knowledge;model evaluation;Measurement;Computer bugs;Predictive models;History;Prediction algorithms;Software quality;software defect prediction, periodic developer experience, code ownership, churn metrics}, 
doi={10.1109/SCAM.2018.00016}, 
ISSN={2470-6892}, 
month={Sep.},}
@INPROCEEDINGS{6475397, 
author={M. Steff and B. Russo}, 
booktitle={Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement}, 
title={Characterizing the roles of classes and their fault-proneness through change metrics}, 
year={2012}, 
volume={}, 
number={}, 
pages={59-68}, 
abstract={Many approaches to determine the fault-proneness of code artifacts rely on historical data of and about these artifacts. These data include the code and how it was changed over time, and information about the changes from version control systems. Each of these can be considered at different levels of granularity. The level of granularity can substantially influence the estimated fault-proneness of a code artifact. Typically, the level of detail oscillates between releases and commits on the one hand, and single lines of code and whole files on the other hand. Not every information may be readily available or feasible to collect at every level, though, nor does more detail necessarily improve the results. Our approach is based on time series of changes in method-level dependencies and churn on a commit-to-commit basis for two systems, Spring and Eclipse. We identify sets of classes with distinct properties of the time series of their change histories. We differentiate between classes based on temporal patterns of change. Based on this differentiation, we show that our measure of structural change in concert with its complement, churn, effectively indicates fault-proneness in classes. We also use windows on time series to select sets of commits and show that changes over short amounts of time do effectively indicate the fault-proneness of classes.}, 
keywords={configuration management;software fault tolerance;software metrics;software class;software fault-proneness;software change metrics;code artifact;version control system;granularity level;method-level dependency;commit-to-commit churn;Spring system;Eclipse system;change history;time series;temporal change pattern;Vectors;Time series analysis;Measurement;History;Software;Computer bugs;Springs;product metrics;fault-proneness;software architectures}, 
doi={10.1145/2372251.2372261}, 
ISSN={1949-3770}, 
month={Sep.},}
@INPROCEEDINGS{7377808, 
author={T. Brunner and N. Pataki and Z. Porkolab}, 
booktitle={2015 IEEE 13th International Scientific Conference on Informatics}, 
title={Tool for detecting standardwise differences in C++ legacy code}, 
year={2015}, 
volume={}, 
number={}, 
pages={57-62}, 
abstract={Programming languages are continuously evolving as the experiences are accumulated, developers face new problems and other requirements such as increasing support for multi-threading is emerging. These changes are reflected in new language standards and compiler versions. Although these changes are carefully planned to keep reverse compatibility with previous versions to keep the syntax and the semantics of earlier written code, sometimes languages break this rule. In case of silent semantic changes, when the earlier written code is recompiled with the new version, this is especially harmful. The new C++11 standard introduced major changes in the core language. This changes are widely believed to be reverse compatible, i.e. a simple recompilation of earlier written code will keep the old semantics. Recently we found examples that the backward compatibility between language versions is broken. The previously written code compiled with a new C++ compiler may change the program behaviour without any compiler diagnostic message. In a large code base such issues are very hard to catch by manual inspection, therefore some automatic tool support is required for this purpose. In this paper we propose a tool support to detect such backward incompatibilities in C++. The basic idea is to parse the source code using different standards, and then compare the abstract syntax trees. We implemented a proof of concept prototype tool to demonstrate our idea based on the LLVM/Clang compiler infrastructure.}, 
keywords={C++ language;program compilers;software maintenance;source code (software);standardwise differences detection;C++ legacy code;backward incompatibilities;source code parsing;abstract syntax trees;LLVM/Clang compiler infrastructure;Standards;Semantics;Computer languages;Program processors;Syntactics;Libraries;Prototypes}, 
doi={10.1109/Informatics.2015.7377808}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8377642, 
author={S. Sothornprapakorn and S. Hayashi and M. Saeki}, 
booktitle={2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)}, 
title={Visualizing a Tangled Change for Supporting Its Decomposition and Commit Construction}, 
year={2018}, 
volume={01}, 
number={}, 
pages={74-79}, 
abstract={Developers often save multiple kinds of source code edits into a commit in a version control system, producing a tangled change, which is difficult to understand and revert. However, its separation using an existing sequence-based change representation is tough. We propose a new visualization technique to show the details of a tangled change and align its component edits in a tree structure for expressing multiple groups of changes. Our technique is combined with utilizing refactoring detection and change relevance calculation techniques for constructing the structural tree. Our combination allows us to divide the change into several associations. We have implemented a tool and conducted a controlled experiment with industrial developers to confirm its usefulness and efficiency. Results show that by using our tool with tree visualization, the subjects could understand and decompose tangled changes easier, faster, and higher accuracy than the baseline file list visualization.}, 
keywords={configuration management;data visualisation;software maintenance;trees (mathematics);tangled change;tree visualization;sequence-based change representation;source code;version control system;refactoring detection;Tools;Task analysis;Visualization;Computer bugs;Feature extraction;Electronic mail;Usability;tangled change, refactoring, visualization}, 
doi={10.1109/COMPSAC.2018.00018}, 
ISSN={0730-3157}, 
month={July},}
@INPROCEEDINGS{8115655, 
author={M. M. Rahman and C. K. Roy}, 
booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Improved query reformulation for concept location using CodeRank and document structures}, 
year={2017}, 
volume={}, 
number={}, 
pages={428-439}, 
abstract={During software maintenance, developers usually deal with a significant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique-ACER-that takes an initial query, identifies appropriate search terms from the source code using a novel term weight-CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with five closely related existing techniques in query reformulation not only validates our empirical findings but also demonstrates the superiority of our technique.}, 
keywords={learning (artificial intelligence);query formulation;query processing;software maintenance;source code (software);text analysis;initial query;request texts;relevant source code locations;software system;concept location;source document structures;query quality analysis;machine learning;improved query reformulation;software maintenance;software change requests;baseline queries;weight-CodeRank;search terms;Natural languages;Periodic structures;Measurement;Java;Software maintenance;Query reformulation;CodeRank;term weighting;query quality analysis;concept location;data resampling}, 
doi={10.1109/ASE.2017.8115655}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6475415, 
author={E. Giger and M. D'Ambros and M. Pinzger and H. C. Gall}, 
booktitle={Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement}, 
title={Method-level bug prediction}, 
year={2012}, 
volume={}, 
number={}, 
pages={171-180}, 
abstract={Researchers proposed a wide range of approaches to build effective bug prediction models that take into account multiple aspects of the software development process. Such models achieved good prediction performance, guiding developers towards those parts of their system where a large share of bugs can be expected. However, most of those approaches predict bugs on file-level. This often leaves developers with a considerable amount of effort to examine all methods of a file until a bug is located. This particular problem is reinforced by the fact that large files are typically predicted as the most bug-prone. In this paper, we present bug prediction models at the level of individual methods rather than at file-level. This increases the granularity of the prediction and thus reduces manual inspection efforts for developers. The models are based on change metrics and source code metrics that are typically used in bug prediction. Our experiments-performed on 21 Java open-source (sub-)systems-show that our prediction models reach a precision and recall of 84% and 88%, respectively. Furthermore, the results indicate that change metrics significantly outperform source code metrics.}, 
keywords={Java;program debugging;public domain software;software metrics;method-level bug prediction;software development process;file-level;individual methods;change metrics;source code metrics;Java open-source system;Measurement;Predictive models;Computer bugs;History;Java;Support vector machines;Complexity theory;method-level bug prediction;fine-grained source code changes;code metrics}, 
doi={10.1145/2372251.2372285}, 
ISSN={1949-3770}, 
month={Sep.},}
@INPROCEEDINGS{7050491, 
author={Jayashree H. V and V. K. Agrawal and Shishir Bharadwaj N}, 
booktitle={2015 International Conference on VLSI Systems, Architecture, Technology and Applications (VLSI-SATA)}, 
title={Quantum cost realization of new reversible gates with transformation based synthesis technique}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Reversible computing appears to be promising due its applications in emerging technologies. To compute any reversible function it is necessary to build the system with reversible gates. Simplified version of transformation technique [3,5] to synthesize new reversible gates with Fredkin and Toffoli gates network is presented in this paper. Basic and bidirectional transformation algorithms with an example are illustrated, which uncovers every step of the algorithm. The same example is used for both the algorithms to give clarity on the difference in the efficiency of the algorithms. Simple pseudo code is also presented to illustrate the steps of the algorithm. The best quantum cost obtained is listed in this paper.}, 
keywords={logic circuits;logic design;logic gates;quantum cost realization;reversible gates;transformation based synthesis technique;reversible computing;Fredkin gates network;Toffoli gates network;bidirectional transformation algorithms;pseudo code;Logic gates;Transformation;synthesis;New Reversible gates;Fredkin;Toffoli}, 
doi={10.1109/VLSI-SATA.2015.7050491}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{6606672, 
author={R. Sukkerd and I. Beschastnikh and J. Wuttke and S. Zhang and Y. Brun}, 
booktitle={2013 35th International Conference on Software Engineering (ICSE)}, 
title={Understanding regression failures through test-passing and test-failing code changes}, 
year={2013}, 
volume={}, 
number={}, 
pages={1177-1180}, 
abstract={Debugging and isolating changes responsible for regression test failures are some of the most challenging aspects of modern software development. Automatic bug localization techniques reduce the manual effort developers spend examining code, for example, by focusing attention on the minimal subset of recent changes that results in the test failure, or on changes to components with most dependencies or highest churn. We observe that another subset of changes is worth the developers' attention: the complement of the maximal set of changes that does not produce the failure. While for simple, independent source-code changes, existing techniques localize the failure cause to a small subset of those changes, we find that when changes interact, the failure cause is often in our proposed subset and not in the subset existing techniques identify. In studying 45 regression failures in a large, open-source project, we find that for 87% of those failures, the complement of the maximal passing set of changes is different from the minimal failing set of changes, and that for 78% of the failures, our technique identifies relevant changes ignored by existing work. These preliminary results suggest that combining our ideas with existing techniques, as opposed to using either in isolation, can improve the effectiveness of bug localization tools.}, 
keywords={program debugging;program testing;public domain software;set theory;system recovery;test-passing code changes;test-failing code changes;software debugging change;software isolation change;regression test failures;software development;automatic bug localization techniques;independent source-code changes;failure cause localization;open-source project;maximal passing change set;minimal failing change set;Debugging;History;Open source software;Java;Educational institutions;Interference}, 
doi={10.1109/ICSE.2013.6606672}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{8452881, 
author={S. Young and T. Abdou and A. Bener}, 
booktitle={2018 IEEE/ACM 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE)}, 
title={A Replication Study: Just-in-Time Defect Prediction with Ensemble Learning}, 
year={2018}, 
volume={}, 
number={}, 
pages={42-47}, 
abstract={Just-in-time defect prediction, which is also known as change-level defect prediction, can be used to efficiently allocate resources and manage project schedules in the software testing and debugging process. Just-in-time defect prediction can reduce the amount of code to review and simplify the assignment of developers to bug fixes. This paper reports a replicated experiment and an extension comparing the prediction of defect-prone changes using traditional machine learning techniques and ensemble learning. Using datasets from six open source projects, namely Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL we replicate the original approach to verify the results of the original experiment and use them as a basis for comparison for alternatives in the approach. Our results from the replicated experiment are consistent with the original. The original approach uses a combination of data preprocessing and a two-layer ensemble of decision trees. The first layer uses bagging to form multiple random forests. The second layer stacks the forests together with equal weights. Generalizing the approach to allow the use of any arbitrary set of classifiers in the ensemble, optimizing the weights of the classifiers, and allowing additional layers, we apply a new deep ensemble approach, called deep super learner, to test the depth of the original study. The deep super learner achieves statistically significantly better results than the original approach on five of the six projects in predicting defects as measured by F1 score.}, 
keywords={decision trees;learning (artificial intelligence);pattern classification;program debugging;program testing;public domain software;software fault tolerance;replication study;just-in-time defect prediction;ensemble learning;change-level defect prediction;software testing;debugging process;defect-prone changes;machine learning;deep ensemble;Bugzilla;Columba;JDT;Platform;Mozilla;PostgreSQL;open source projects;data preprocessing;multiple random forests;classifiers;deep super learner;DSL;Forestry;Machine learning;Prediction algorithms;Classification algorithms;Entropy;Decision trees;Supervised learning by classification;Machine learning approaches;Ensemble methods;Software defect analysis;Deep Learning;Defect Prediction},
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7321218, 
author={X. Tang and S. Wang and K. Mao}, 
booktitle={2015 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)}, 
title={Will This Bug-Fixing Change Break Regression Testing?}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={Context: Software source code is frequently changed for fixing revealed bugs. These bug-fixing changes might introduce unintended system behaviors, which are inconsistent with scenarios of existing regression test cases, and consequently break regression testing. For validating the quality of changes, regression testing is a required process before submitting changes during the development of software projects. Our pilot study shows that 48.7% bug-fixing changes might break regression testing at first run, which means developers have to run regression testing at least a couple of times for 48.7% changes. Such process can be tedious and time consuming. Thus, before running regression test suite, finding these changes and corresponding regression test cases could be helpful for developers to quickly fix these changes and improve the efficiency of regression testing. Goal: This paper proposes bug- fixing change impact prediction (BFCP), for predicting whether a bug-fixing change will break regression testing or not before running regression test cases, by mining software change histories. Method: Our approach employs the machine learning algorithms and static call graph analysis technique. Given a bug-fixing change, BFCP first predicts whether it will break existing regression test cases; second, if the change is predicted to break regression test cases, BFCP can further identify the might-be-broken test cases. Results: Results of experiments on 552 real bug-fixing changes from four large open source projects show that BFCP could achieve prediction precision up to 83.3%, recall up to 92.3%, and F-score up to 81.4%. For identifying the might-be-broken test cases, BFCP could achieve 100% recall.}, 
keywords={graph theory;learning (artificial intelligence);program debugging;program diagnostics;program testing;regression testing;software source code;software project development;bug- fixing change impact prediction;BFCP;machine learning algorithms;static call graph analysis technique;might-be-broken test case identification;Testing;Measurement;Computer bugs;Software;Semantics;Predictive models;History}, 
doi={10.1109/ESEM.2015.7321218}, 
ISSN={1949-3770}, 
month={Oct},}
@INPROCEEDINGS{7965350, 
author={T. Gu and X. Ma and C. Xu and Y. Jiang and C. Cao and J. Lü}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)}, 
title={Synthesizing Object Transformation for Dynamic Software Updating}, 
year={2017}, 
volume={}, 
number={}, 
pages={336-338}, 
abstract={Dynamic software updating (DSU) can upgrade a running program on-the-fly by directly replacing the in-memory code and reusing existing runtime state (e.g., heap objects) for the updated execution. Additionally, it is usually necessary to transform the runtime state into a proper new state to avoid inconsistencies that arise during runtime states reuse among different versions of a program. However, such transformations mostly require human efforts, which is time-consuming and error-prone. This paper presents AOTES, an approach to automating object transformations for dynamic updating of Java programs. AOTES tries to generate the new state by re-executing a method invocation history and leverages symbolic execution to synthesize the history from the current object state without any recording. We evaluated AOTES on software updates taken from Apache Tomcat, Apache FTP Server and Apache SSHD Server. Experimental results show that AOTES successfully handled 47 of 57 object transformations of 18 updated classes, while two state-of-the-art approaches only handled 11 and 6 of 57, respectively.}, 
keywords={Java;program processors;software reusability;object transformation synthesis;dynamic software updating;DSU;runtime state;AOTES;object transformation automation;Java programs;method invocation history;symbolic execution;software updates;Apache Tomcat;Apache FTP Server;Apache SSHD Server;History;Software;Inverse problems;Runtime;Java;Servers;Containers;dynamic software updating;inverse method generation;execution synthesis;symbolic execution}, 
doi={10.1109/ICSE-C.2017.96}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6671311, 
author={N. Ali and F. Jaafar and A. E. Hassan}, 
booktitle={2013 20th Working Conference on Reverse Engineering (WCRE)}, 
title={Leveraging historical co-change information for requirements traceability}, 
year={2013}, 
volume={}, 
number={}, 
pages={361-370}, 
abstract={Requirements traceability (RT) links requirements to the corresponding source code entities, which implement them. Information Retrieval (IR) based RT links recovery approaches are often used to automatically recover RT links. However, such approaches exhibit low accuracy, in terms of precision, recall, and ranking. This paper presents an approach (CoChaIR), complementary to existing IR-based RT links recovery approaches. CoChaIR leverages historical co-change information of files to improve the accuracy of IR-based RT links recovery approaches. We evaluated the effectiveness of CoChaIR on three datasets, i.e., iTrust, Pooka, and SIP Communicator. We compared CoChaIR with two different IR-based RT links recovery approaches, i.e., vector space model and Jensen-Shannon divergence model. Our study results show that CoChaIR significantly improves precision and recall by up to 12.38% and 5.67% respectively; while decreasing the rank of true positive links by up to 48% and reducing false positive links by up to 44%.}, 
keywords={information retrieval;software engineering;historical co-change information;requirements traceability;IR based RT links recovery approaches;information retrieval;CoChaIR approach;iTrust dataset;Pooka dataset;SIP Communicator dataset;vector space model;Jensen-Shannon divergence model;precision;recall;true positive links;false positive links;Accuracy;Vectors;History;Engines;Calculators;Probability distribution;Joining processes;Traceability;requirements;source code;co-change;information retrieval}, 
doi={10.1109/WCRE.2013.6671311}, 
ISSN={1095-1350}, 
month={Oct},}
@INPROCEEDINGS{6975655, 
author={S. Raemaekers and A. van Deursen and J. Visser}, 
booktitle={2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation}, 
title={Semantic Versioning versus Breaking Changes: A Study of the Maven Repository}, 
year={2014}, 
volume={}, 
number={}, 
pages={215-224}, 
abstract={For users of software libraries or public programming interfaces (APIs), backward compatibility is a desirable trait. Without compatibility, library users will face increased risk and cost when upgrading their dependencies. In this study, we investigate semantic versioning, a versioning scheme which provides strict rules on major versus minor and patch releases. We analyze seven years of library release history in Maven Central, and contrast version identifiers with actual incompatibilities. We find that around one third of all releases introduce at least one breaking change, and that this figure is the same for minor and major releases, indicating that version numbers do not provide developers with information in stability of interfaces. Additionally, we find that the adherence to semantic versioning principles has only marginally increased over time. We also investigate the use of deprecation tags and find out that methods get deleted without applying deprecated tags, and methods with deprecated tags are never deleted. We conclude the paper by arguing that the adherence to semantic versioning principles should increase because it provides users of an interface with a way to determine the amount of rework that is expected when upgrading to a new version.}, 
keywords={configuration management;software libraries;Maven Repository;library release history;contrast version identifiers;version numbers;semantic versioning principles;deprecation tags;major patch releases;minor patch releases;software libraries;Semantics;Java;Software;Software libraries;Distributed databases;Electronic mail;Semantic versioning;Software libraries}, 
doi={10.1109/SCAM.2014.30}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{8474479, 
author={G. J. Holzmann}, 
journal={IEEE Software}, 
title={Code Vault}, 
year={2018}, 
volume={35}, 
number={5}, 
pages={85-87}, 
abstract={So, what has changed since that first NATO software engineering conference in 1968? Depending on your point of view, nothing much has changed, or everything has changed. The part that didn't change much is that we still struggle with writing code that's robust enough to trust. The part that has changed dramatically is the performance of the hardware that runs our code. This article is part of a theme issue on software engineering's 50th anniversary.}, 
keywords={software engineering;code vault;NATO software engineering conference;hardware performance;Error analysis;Software development;Software reliability;Random access memory;Computer bugs;software errors;software technology;history of software engineering;software engineering;software development;Reliable Code}, 
doi={10.1109/MS.2018.3571225}, 
ISSN={0740-7459}, 
month={Sep.},}
@INPROCEEDINGS{8320252, 
author={A. Kaur and S. Jain and S. Goel}, 
booktitle={2017 International Conference on Machine Learning and Data Science (MLDS)}, 
title={A Support Vector Machine Based Approach for Code Smell Detection}, 
year={2017}, 
volume={}, 
number={}, 
pages={9-14}, 
abstract={Code smells may be introduced in software due to market rivalry, work pressure deadline, improper functioning, skills or inexperience of software developers. Code smells indicate problems in design or code which makes software hard to change and maintain. Detecting code smells could reduce the effort of developers, resources and cost of the software. Many researchers have proposed different techniques like DETEX for detecting code smells which have limited precision and recall. To overcome these limitations, a new technique named as SVMCSD has been proposed for the detection of code smells, based on support vector machine learning technique. Four code smells are specified namely God Class, Feature Envy, Data Class and Long Method and the proposed technique is validated on two open source systems namely ArgoUML and Xerces. The accuracy of SVMCSD is found to be better than DETEX in terms of two metrics, precision and recall, when applied on a subset of a system. While considering the entire system, SVMCSD detect more occurrences of code smells than DETEX.}, 
keywords={learning (artificial intelligence);public domain software;software maintenance;software metrics;software quality;support vector machines;support vector machine;software developers;code smell detection;code smells;DETEX;SVMCSD;Support vector machines;Measurement;Software systems;Training;Software quality;Computer science;Code smells;Anti-patterns;Support Vector Machine;Software maintenance}, 
doi={10.1109/MLDS.2017.8}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7081861, 
author={M. Mondal and C. K. Roy and K. A. Schneider}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={SPCP-Miner: A tool for mining code clones that are important for refactoring or tracking}, 
year={2015}, 
volume={}, 
number={}, 
pages={484-488}, 
abstract={Code cloning has both positive and negative impacts on software maintenance and evolution. Focusing on the issues related to code cloning, researchers suggest to manage code clones through refactoring and tracking. However, it is impractical to refactor or track all clones in a software system. Thus, it is essential to identify which clones are important for refactoring and also, which clones are important for tracking. In this paper, we present a tool called SPCP-Miner which is the pioneer one to automatically identify and rank the important refactoring as well as important tracking candidates from the whole set of clones in a software system. SPCP-Miner implements the existing techniques that we used to conduct a large scale empirical study on SPCP clones (i.e., the clones that evolved following a Similarity Preserving Change Pattern called SPCP). We believe that SPCP-Miner can help us in better management of code clones by suggesting important clones for refactoring or tracking.}, 
keywords={software maintenance;source code (software);SPCP-Miner;similarity preserving change pattern;code cloning;software maintenance;software refactoring;software tracking;Cloning;Software systems;History;Detectors;Couplings;Focusing}, 
doi={10.1109/SANER.2015.7081861}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{8170085, 
author={I. Ahmed and C. Brindescu and U. A. Mannan and C. Jensen and A. Sarma}, 
booktitle={2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)}, 
title={An Empirical Examination of the Relationship between Code Smells and Merge Conflicts}, 
year={2017}, 
volume={}, 
number={}, 
pages={58-67}, 
abstract={Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both "smelly" and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.}, 
keywords={data mining;program debugging;public domain software;software maintenance;software metrics;conflicting change;method-level code;semantic conflicts;code changes;code elements;resulting code quality;merge conflicts Background;code smells;conflict resolution;Bad code design;Software;Merging;Computer bugs;Tools;Software measurement;Semantics;Code Smell;Merge Conflict;Empirical Analysis;Machine Learning}, 
doi={10.1109/ESEM.2017.12}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8543430, 
author={L. Pascarella}, 
booktitle={2018 IEEE/ACM 5th International Conference on Mobile Software Engineering and Systems (MOBILESoft)}, 
title={Classifying Code Comments in Java Mobile Applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={39-40}, 
abstract={Developers adopt code comments for different reasons such as document source codes or change program flows. Due to a variety of use scenarios, code comments may impact on readability and maintainability. In this study, we investigate how developers of 5 open-source mobile applications use code comments to document their projects. Additionally, we evaluate the performance of two machine learning models to automatically classify code comments. Initial results show marginal differences between desktop and mobile applications.}, 
keywords={Java;mobile computing;public domain software;java mobile applications;document source codes;open-source mobile applications;code comments classifications;Java;Taxonomy;Open source software;Machine learning;Classification algorithms;Mobile applications;Android;Mining Software Repositories;Code Comments}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6606596, 
author={N. Meng and M. Kim and K. S. McKinley}, 
booktitle={2013 35th International Conference on Software Engineering (ICSE)}, 
title={Lase: Locating and applying systematic edits by learning from examples}, 
year={2013}, 
volume={}, 
number={}, 
pages={502-511}, 
abstract={Adding features and fixing bugs often require systematic edits that make similar, but not identical, changes to many code locations. Finding all the relevant locations and making the correct edits is a tedious and error-prone process for developers. This paper addresses both problems using edit scripts learned from multiple examples. We design and implement a tool called LASE that (1) creates a context-aware edit script from two or more examples, and uses the script to (2) automatically identify edit locations and to (3) transform the code. We evaluate LASE on an oracle test suite of systematic edits from Eclipse JDT and SWT. LASE finds edit locations with 99% precision and 89% recall, and transforms them with 91% accuracy. We also evaluate LASE on 37 example systematic edits from other open source programs and find LASE is accurate and effective. Furthermore, we confirmed with developers that LASE found edit locations which they missed. Our novel algorithm that learns from multiple examples is critical to achieving high precision and recall; edit scripts created from only one example produce too many false positives, false negatives, or both. Our results indicate that LASE should help developers in automating systematic editing. Whereas most prior work either suggests edit locations or performs simple edits, LASE is the first to do both for nontrivial program edits.}, 
keywords={program debugging;program testing;public domain software;text editing;bug fixing;nontrivial program edit;false negatives;false positives;open source programs;SWT;Eclipse JDT;oracle test suite;code transformation;automatic edit location identification;context-aware edit script;locating and applying systematic edit;LASE;Context;Abstracts;Concrete;Systematics;Cloning;Computer bugs;Transforms}, 
doi={10.1109/ICSE.2013.6606596}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{8109101, 
author={P. Loyola and Y. Matsuo}, 
booktitle={2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE)}, 
title={Learning Feature Representations from Change Dependency Graphs for Defect Prediction}, 
year={2017}, 
volume={}, 
number={}, 
pages={361-372}, 
abstract={Given the heterogeneity of the data that can be extracted from the software development process, defect prediction techniques have focused on associating different sources of data with the introduction of faulty code, usually relying on handcrafted features. While these efforts have generated considerable progress over the years, little attention has been given to the fact that the performance of any predictive model depends heavily on the representation of the data used, and that different representations can lead to different results. We consider this a relevant problem, as it could be affecting directly the efforts towards generating safer software systems. Therefore, we propose to study the impact of the representation of the data in defect prediction models. To this end, we focus on the use of developer activity data, from which we structure dependency graphs. Then, instead of manually generating features, such as network metrics, we propose two models inspired by recent advances in representation learning which are able to automatically generate feature representations from graph data. These new representations are compared against manually crafted features for defect prediction in real world software projects. Our results show that automatically learned features are competitive, reaching increments in prediction performance up to 13%.}, 
keywords={graph theory;learning (artificial intelligence);software maintenance;software metrics;software quality;feature representations;change dependency graphs;software development process;defect prediction techniques;faulty code;developer activity data;structure dependency graphs;automatically learned features;prediction performance;data heterogeneity;Software;Predictive models;Data models;Feature extraction;Measurement;Complexity theory;representation learning;defect prediction;machine learning}, 
doi={10.1109/ISSRE.2017.30}, 
ISSN={2332-6549}, 
month={Oct},}
@INPROCEEDINGS{7180123, 
author={F. Palomba and D. Di Nucci and M. Tufano and G. Bavota and R. Oliveto and D. Poshyvanyk and A. De Lucia}, 
booktitle={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories}, 
title={Landfill: An Open Dataset of Code Smells with Public Evaluation}, 
year={2015}, 
volume={}, 
number={}, 
pages={482-485}, 
abstract={Code smells are symptoms of poor design and implementation choices that may hinder code comprehension and possibly increase change- and fault-proneness of source code. Several techniques have been proposed in the literature for detecting code smells. These techniques are generally evaluated by comparing their accuracy on a set of detected candidate code smells against a manually-produced oracle. Unfortunately, such comprehensive sets of annotated code smells are not available in the literature with only few exceptions. In this paper we contribute (i) a dataset of 243 instances of five types of code smells identified from 20 open source software projects, (ii) a systematic procedure for validating code smell datasets, (iii) LANDFILL, a Web-based platform for sharing code smell datasets, and (iv) a set of APIs for programmatically accessing LANDFILL's contents. Anyone can contribute to Landfill by (i) improving existing datasets (e.g., Adding missing instances of code smells, flagging possibly incorrectly classified instances), and (ii) sharing and posting new datasets. Landfill is available at www.sesa.unisa.it/landfill/, while the video demonstrating its features in action is available at http://www.sesa.unisa.it/tools/landfill.jsp.}, 
keywords={Internet;software engineering;source code (software);open dataset;candidate code smells;open source software projects;systematic procedure;LANDFILL;Web-based platform;History;Manuals;Software systems;Androids;Humanoid robots;Buildings}, 
doi={10.1109/MSR.2015.69}, 
ISSN={2160-1852}, 
month={May},}
@ARTICLE{7164225, 
author={I. S. Wiese and R. T. Kuroda and R. Ré and R. S. Bulhóes and G. A. Oliva and M. A. Gerosa}, 
journal={IEEE Latin America Transactions}, 
title={Do historical metrics and developers communication aid to predict change couplings?}, 
year={2015}, 
volume={13}, 
number={6}, 
pages={1979-1988}, 
abstract={Developers have contributed to open-source projects by forking the code and submitting pull requests. Once a pull request is submitted, interested parties can review the set of changes, discuss potential modifications, and even push additional commits if necessary. Mining artifacts that were committed together during history of pull-requests makes it possible to infer change couplings among these artifacts. Supported by the Conway's Law, whom states that “organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations”, we hypothesize that social network analysis (SNA) is able to identify strong and weak change dependencies. In this paper, we used statistical models relying on centrality, ego, and structural holes metrics computed from communication networks to predict co-changes among files included in pull requests submitted to the Ruby on Rails project. To the best of our knowledge, this is the first study to employ SNA metrics to predict change dependencies from Github projects.}, 
keywords={public domain software;software metrics;historical metrics;developer communication;change coupling prediction;open-source projects;pull requests;Conway law;social network analysis;SNA metrics;change dependencies;statistical models;structural hole metrics;communication networks;Ruby on Rails project;Github projects;Measurement;Receivers;change coupling;communication network;Conway's law;social network analysis;structural holes metrics}, 
doi={10.1109/TLA.2015.7164225}, 
ISSN={1548-0992}, 
month={June},}
@INPROCEEDINGS{6603803, 
author={O. Baysal and O. Kononenko and R. Holmes and M. W. Godfrey}, 
booktitle={2013 1st International Workshop on Data Analysis Patterns in Software Engineering (DAPSE)}, 
title={Extracting artifact lifecycle models from metadata history}, 
year={2013}, 
volume={}, 
number={}, 
pages={17-19}, 
abstract={Software developers and managers make decisions based on the understanding they have of their software systems. This understanding is both built up experientially and through investigating various software development artifacts. While artifacts can be investigated individually, being able to summarize characteristics about a set of development artifacts can be useful. In this paper we propose lifecycle models as an effective way to gain an understanding of certain development artifacts. Lifecycle models capture the dynamic nature of how various development artifacts change over time in a graphical form that can be easily understood and communicated. Lifecycle models enables reasoning of the underlying processes and dynamics of the artifacts being analyzed. In this paper we describe how lifecycle models can be generated and demonstrate how they can be applied to the code review process of a development project.}, 
keywords={meta data;software engineering;artifact lifecycle model extraction;metadata history;software systems;software development artifacts;code review process;software development project;Data models;History;Software;Time measurement;Educational institutions;Control systems;Data mining}, 
doi={10.1109/DAPSE.2013.6603803}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6405263, 
author={X. Qu and M. Acharya and B. Robinson}, 
booktitle={2012 28th IEEE International Conference on Software Maintenance (ICSM)}, 
title={Configuration selection using code change impact analysis for regression testing}, 
year={2012}, 
volume={}, 
number={}, 
pages={129-138}, 
abstract={Configurable systems that let users customize system behaviors are becoming increasingly prevalent. Testing a configurable system with all possible configurations is very expensive and often impractical. For a single version of a configurable system, sampling approaches exist that select a subset of configurations from the full configuration space for testing. However, when a configurable system changes and evolves, existing approaches for regression testing select all configurations that are used to test the old versions for testing the new version. As demonstrated in our experiments, this retest-all approach for regression testing configurable systems turns out to be highly redundant. To address this redundancy, we propose a configuration selection approach for regression testing. Formally, given two versions of a configurable system, S (old) and S' (new), and given a set of configurations C<sub>S</sub> for testing S, our approach selects a subset C<sub>S'</sub> of C<sub>S</sub> for regression testing S'. Our study results on two open source systems and a large industrial system show that, compared to the retest-all approach, our approach discards 15% to 60% of configurations as redundant. Our approach also saves 20% to 55% of the regression testing time, while retaining the same fault detection capability and code coverage of the retest-all approach.}, 
keywords={program slicing;program testing;regression analysis;configuration selection;code change impact analysis;regression testing time;configurable system testing;system behavior customization;configuration space;fault detection capability;code coverage;static program slicing;Fault detection;Conferences;Software maintenance;System testing;Argon;Redundancy;Configurable System Testing;Configuration Selection;Regression Testing;Static Program Slicing;Change Impact Analysis}, 
doi={10.1109/ICSM.2012.6405263}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{6639006, 
author={Y. Tachioka and S. Watanabe and J. R. Hershey}, 
booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
title={Effectiveness of discriminative training and feature transformation for reverberated and noisy speech}, 
year={2013}, 
volume={}, 
number={}, 
pages={6935-6939}, 
abstract={Automatic speech recognition in the presence of non-stationary interference and reverberation remains a challenging problem. The 2<sup>nd</sup> `CHiME' Speech Separation and Recognition Challenge introduces a new and difficult task with time-varying reverberation and non-stationary interference including natural background speech, home noises, or music. This paper establishes baselines using state-of-the-art ASR techniques such as discriminative training and various feature transformation on the middle-vocabulary sub-task of this challenge. In addition, we propose an augmented discriminative feature transformation that introduces arbitrary features to a discriminative feature transformation. We present experimental results showing that discriminative training of model parameters and feature transforms is highly effective for this task, and that the augmented feature transformation provides some preliminary benefits. The training code will be released as an advanced ASR baseline.}, 
keywords={learning (artificial intelligence);speech recognition;training;transforms;discriminative training;reverberated speech;noisy speech;automatic speech recognition;nonstationary interference;CHiME;speech separation;recognition challenge;time-varying reverberation;natural background speech;home noises;music;ASR techniques;middle-vocabulary sub-task;augmented discriminative feature transformation;model parameters;feature transforms;training code;Training;Noise measurement;Speech;Speech recognition;Hidden Markov models;Noise;Mel frequency cepstral coefficient;Discriminative training;Feature transformation;Augmented discriminative feature transformation;CHiME challenge;Kaldi}, 
doi={10.1109/ICASSP.2013.6639006}, 
ISSN={1520-6149}, 
month={May},}
@INPROCEEDINGS{7961515, 
author={F. Palomba and A. Zaidman and R. Oliveto and A. De Lucia}, 
booktitle={2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)}, 
title={An Exploratory Study on the Relationship between Changes and Refactoring}, 
year={2017}, 
volume={}, 
number={}, 
pages={176-185}, 
abstract={Refactoring aims at improving the internal structure of a software system without changing its external behavior. Previous studies empirically assessed, on the one hand, the benefits of refactoring in terms of code quality and developers' productivity, and on the other hand, the underlying reasons that push programmers to apply refactoring. Results achieved in the latter investigations indicate that besides personal motivation such as the responsibility concerned with code authorship, refactoring is mainly performed as a consequence of changes in the requirements rather than driven by software quality. However, these findings have been derived by surveying developers, and therefore no software repository study has been carried out to corroborate the achieved findings. To bridge this gap, we provide a quantitative investigation on the relationship between different types of code changes (i.e., Fault Repairing Modification, Feature Introduction Modification, and General Maintenance Modification) and 28 different refactoring types coming from 3 open source projects. Results showed that developers tend to apply a higher number of refactoring operations aimed at improving maintainability and comprehensibility of the source code when fixing bugs. Instead, when new features are implemented, more complex refactoring operations are performed to improve code cohesion. Most of the times, the underlying reasons behind the application of such refactoring operations are represented by the presence of duplicate code or previously introduced self-admitted technical debts.}, 
keywords={software maintenance;software change;software refactoring;code quality;developer productivity;software behavior;code authorship;refactoring operation;source code maintainability;source code comprehensibility;self-admitted technical debt;Feature extraction;Maintenance engineering;Logistics;Software systems;History;Computer bugs;Refactoring;Code Changes;Empirical Studies}, 
doi={10.1109/ICPC.2017.38}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6598517, 
author={C. Tantithamthavorn and A. Ihara and K. Matsumoto}, 
booktitle={2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing}, 
title={Using Co-change Histories to Improve Bug Localization Performance}, 
year={2013}, 
volume={}, 
number={}, 
pages={543-548}, 
abstract={A large open source software (OSS) project receives many bug reports on a daily basis. Bug localization techniques automatically pinpoint source code fragments that are relevant to a bug report, thus enabling faster correction. Even though many bug localization methods have been introduced, their performance is still not efficient. In this research, we improved on existing bug localization methods by taking into account co-change histories. We conducted experiments on two OSS datasets, the Eclipse SWT 3.1 project and the Android ZXing project. We validated our approach by evaluating effectiveness compared to the state-of-the-art approach Bug Locator. In the Eclipse SWT 3.1 project, our approach reliably identified source code that should be fixed for a bug in 72.46% of the total bugs, while Bug Locator identified only 51.02%. In the Android ZXing project, our approach identified 85.71%, while Bug Locator identified 60%.}, 
keywords={program debugging;project management;public domain software;software maintenance;software performance evaluation;co-change histories;bug localization performance improvement;open source software project;source code fragments;Eclipse SWT 3.1 project;Android ZXing project;OSS datasets;software project;software maintenance;History;Mathematical model;Equations;Androids;Humanoid robots;Computer bugs;Vectors;Software Maintenance;Co-Change Histories;Bug Localization;Information Retrieval}, 
doi={10.1109/SNPD.2013.92}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7476625, 
author={H. Liu and Y. Wu and W. Liu and Q. Liu and C. Li}, 
booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Domino Effect: Move More Methods Once a Method is Moved}, 
year={2016}, 
volume={1}, 
number={}, 
pages={1-12}, 
abstract={Software refactoring is a popular technology to improve the design of existing source code, and thus it is widely used to facilitate software evolution. Moving methods is one of the most popular refactorings. It helps to reduce coupling between classes and to improve cohesion of involved classes. However, it is difficult to manually identify such methods that should be moved. Consequently, a number of approaches and tools have been proposed to identify such methods based on source code metrics, change history, and textual information. In this paper we propose a new way to identify methods that should be moved. Whenever a method is moved, the approach checks other methods within the same class, and suggests to move the one with the greatest similarity and strongest relationship with the moved method. The rational is that similar and closely related methods should be moved together. The approach has been evaluated on open-source applications by comparing the recommended move method refactorings against refactoring histories of the involved applications. Our evaluation results show that the approach is accurate in recommending methods to be moved (average precision 76%) and in recommending destinations for such methods (average precision 83%). Our evaluation results also show that for a substantial percentage (27%) of move method refactorings, the proposed approach succeeds in identifying additional refactoring opportunities.}, 
keywords={public domain software;software maintenance;software quality;source code (software);domino effect;software refactoring;software evolution;move more methods;source code metrics;change history;textual information;open-source applications;Software Refactoring;Move Method;Recommendation;Refactoring Opportunities}, 
doi={10.1109/SANER.2016.14}, 
ISSN={}, 
month={March},}
@ARTICLE{7587653, 
author={C. A. Silveira Lelis and J. Fernandes Tavares and M. A. Pereira Araujo and J. M. Nazar David}, 
journal={IEEE Latin America Transactions}, 
title={GiveMe Trace: A Software Evolution Traceability Support Tool}, 
year={2016}, 
volume={14}, 
number={7}, 
pages={3444-3454}, 
abstract={Traceability is a key factor in the analysis of the changes that software undergoes throughout its evolution. The main purpose of analysis is to minimize the side effects of these changes and, when it is made to the source at a lower level of abstraction (methods) and in an integrated manner, it can provide more accurate data in order to support decision making. This article presents the GiveMe Trace tool, integrated with a multiple view interactive environment that, among other features, can generate information about the traceability between source code and artifacts its different versions. This information is based on software versions analysis from software repository. As a result, occurrences of changes in classes or methods are shown. A proof of concept was carried out through which repositories versions of two distinct real projects were analyzed. At the end, it was possible to obtain evidences on the feasibility of the use of GiveMe Trace to support traceability between the source code and versions.}, 
keywords={data visualisation;decision making;interactive systems;software tools;source code (software);GiveMe Trace;software evolution traceability support tool;decision making;interactive environment;source code;software artifact;software version analysis;software repository;software visualization;Software;Visualization;Manuals;Sociology;Statistics;Context;Configuration management;Change Management;Software Evolution;Software Repository;Software Traceability;Software Visualization}, 
doi={10.1109/TLA.2016.7587653}, 
ISSN={1548-0992}, 
month={July},}
@INPROCEEDINGS{8094460, 
author={F. Ebert and F. Castor and N. Novielli and A. Serebrenik}, 
booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Confusion Detection in Code Reviews}, 
year={2017}, 
volume={}, 
number={}, 
pages={549-553}, 
abstract={Code reviews are an important mechanism for assuring quality of source code changes. Reviewers can either add general comments pertaining to the entire change or pinpoint concerns or shortcomings about a specific part of the change using inline comments. Recent studies show that reviewers often do not understand the change being reviewed and its context.Our ultimate goal is to identify the factors that confuse code reviewers and understand how confusion impacts the efficiency and effectiveness of code review(er)s. As the first step towards this goal we focus on the identification of confusion in developers' comments. Based on an existing theoretical framework categorizing expressions of confusion, we manually classify 800 comments from code reviews of the Android project. We observe that confusion can be reasonably well-identified by humans: raters achieve moderate agreement (Fleiss' kappa 0.59 for the general comments and 0.49 for the inline ones). Then, for each kind of comment we build a series of automatic classifiers that, depending on the goals of the further analysis, can be trained to achieve high precision (0.875 for the general comments and 0.615 for the inline ones), high recall (0.944 for the general comments and 0.988 for the inline ones), or substantial precision and recall (0.696 and 0.542 for the general comments and 0.434 and 0.583 for the inline ones, respectively). These results motivate further research on the impact of confusion on the code review process. Moreover, other researchers can employ the proposed classifiers to analyze confusion in other contexts where software development-related discussions occur, such as mailing lists.}, 
keywords={Android (operating system);human factors;pattern classification;program testing;software quality;confusion detection;source code changes;inline comments;code review process;Android project;automatic classifiers;precision value;recall value;Androids;Humanoid robots;Uncertainty;Software;Labeling;Training;Manuals;code review;confusion;machine learning}, 
doi={10.1109/ICSME.2017.40}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{8403258, 
author={K. Yamanishi and S. Fukushima}, 
journal={IEEE Transactions on Information Theory}, 
title={Model Change Detection With the MDL Principle}, 
year={2018}, 
volume={64}, 
number={9}, 
pages={6115-6126}, 
abstract={We are concerned with the issue of detecting model changes in probability distributions. We specifically consider the strategies based on the minimum description length (MDL) principle. We theoretically analyze their basic performance from the two aspects: data compression and hypothesis testing. From the view of data compression, we derive a new bound on the minimax regret for model changes. Here, the mini-max regret is defined as the minimum of the worst-case code-length relative to the least normalized maximum likelihood code-length over all model changes. From the view of hypothesis testing, we reduce the model change detection into a simple hypothesis testing problem. We thereby derive upper bounds on error probabilities for the MDL-based model change test. The error probabilities are valid for finite sample size and are related to the information-theoretic complexity as well as the discrepancy measure of the hypotheses to be tested.}, 
keywords={data compression;maximum likelihood estimation;minimax techniques;probability;model change detection;probability distributions;minimum description length principle;data compression;minimax regret;worst-case code-length;normalized maximum likelihood code-length;simple hypothesis testing problem;error probabilities;MDL-based model change test;Error probability;Testing;Analytical models;Hidden Markov models;Data models;Prediction algorithms;Data compression;MDL principle;change detection;hypothesis testing;machine learning}, 
doi={10.1109/TIT.2018.2852747}, 
ISSN={0018-9448}, 
month={Sep.},}
@INPROCEEDINGS{7539890, 
author={K. Yang and C. Tsai and L. Lee and C. Chen}, 
booktitle={2016 International Conference on Applied System Innovation (ICASI)}, 
title={The employment of online self-learning coding course to enhance logical reasoning ability for fifth and sixth grader students}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={This paper was aimed to investigate the influence of online coding courses on logical reasoning ability towards fifth and sixth graders. Students used "Code.org online programming courses" about three months. Through "Raven's Standard Progressive Matrices" before and after testing to understand the change of students' logical reasoning ability in the experiment. And based on "Technology Acceptance Model"' researcher edit "Code.org Learning Attitude Questionnaire" to explore the attitude of students towards the courses and the factors of influencing students learning online courses.}, 
keywords={computer aided instruction;computer science education;educational courses;interactive programming;Web sites;online self-learning coding course;logical reasoning ability enhancement;fifth grader students;sixth grader students;Code.org online programming courses;Raven standard progressive matrices;technology acceptance model;Code.org learning attitude questionnaire;Decision support systems;Programming;Collaboration;online self-learning;computer programming;logical reasoning ability;technology acceptance model (TAM)}, 
doi={10.1109/ICASI.2016.7539890}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7094955, 
author={B. Brenner}, 
booktitle={2015 IEEE IAS Electrical Safety Workshop}, 
title={Changing the Code® culture}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-3}, 
abstract={Much attention is given to educating and instilling a fundamental safety culture in the electrical industry's skilled workforce; as it should. However, given the prevalence of electrically-related fires, injuries, and deaths, one could argue that there is a lack of electrical safety culture among the general public; or at least a lack in sufficient knowledge and behaviors to effectively reduce electrical incidents. This paper will explore the shortcomings of the public's electrical safety culture and their impact. Particular attention will be devoted to the National Electrical Code® (NEC) and its role in saving lives and preventing injury. With the introduction of each new version, the National Electrical Code faces resistance as adoption is delayed, enforcement is lax, and general apathy prevails to varying degrees across the United States. Additionally this paper will outline opportunities to educate and engage the public to help the NEC gain widespread support while grooming safety advocates from all industries and backgrounds.}, 
keywords={electrical safety;fires;injuries;personnel;professional aspects;code culture change;electrical industry skilled workforce;electrical fires;electrical injuries;electrical deaths;public electrical safety culture;electrical incident reduction;national electrical code;NEC;United States;Electrical safety;Fires;Injuries;Industries;Electric shock;Electrical safety;National Electrical Code®;NEC}, 
doi={10.1109/ESW.2015.7094955}, 
ISSN={2326-3288}, 
month={Jan},}
@INPROCEEDINGS{8312531, 
author={E. M. Arvanitou and A. Ampatzoglou and K. Tzouvalidis and A. Chatzigeorgiou and P. Avgeriou and I. Deligiannis}, 
booktitle={2017 24th Asia-Pacific Software Engineering Conference Workshops (APSECW)}, 
title={Assessing Change Proneness at the Architecture Level: An Empirical Validation}, 
year={2017}, 
volume={}, 
number={}, 
pages={98-105}, 
abstract={Change proneness is a characteristic of software artifacts that represents their probability to change in future. Change proneness can be assessed at different levels of granularity, ranging from classes to modules. Although change proneness can be successfully assessed at the source code level (i.e., methods and classes), it remains rather unexplored for architectures. Additionally, the methods that have been introduced at the source code level are not directly transferable to the architecture level. In this paper, we propose and empirically validate a method for assessing the change proneness of architectural modules. Assessing change proneness at the level of architectural modules requires information from two sources: (a) the history of changes in the module, as a proxy of how frequently the module itself undergoes changes; and (b) the dependencies with other modules that affect the probability of a change being propagated from one module to the other. To validate the proposed approach, we performed a case study on five open-source projects. Specifically, we compared the accuracy of the proposed approach to the use of software package metrics as assessors of modules change proneness, based on the 1061-1998 IEEE Standard. The results suggest that compared to examined metrics, the proposed method is a better assessor of change proneness. Therefore, we believe that the method and accompanying tool can effectively aid architects during software maintenance and evolution.}, 
keywords={IEEE standards;software architecture;software maintenance;software metrics;software packages;software quality;architecture level;source code level;change proneness;software artifacts;empirical validation;open-source projects;software package metrics;1061-1998 IEEE standard;1061-1998 IEEE standard;software maintenance;software maintenance;Computer architecture;Probability;Couplings;IEEE Standards;Software measurement;Software reliability;Architectural metrics;Change proneness;Empirical}, 
doi={10.1109/APSECW.2017.21}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8049004, 
author={H. Takizawa and T. Reimann and K. Komatsu and T. Soga and R. Egawa and A. Musa and H. Kobayashi}, 
booktitle={2017 IEEE International Conference on Cluster Computing (CLUSTER)}, 
title={Vectorization-Aware Loop Optimization with User-Defined Code Transformations}, 
year={2017}, 
volume={}, 
number={}, 
pages={685-692}, 
abstract={The cost of maintaining an application code would significantly increase if the application code is branched into multiple versions, each of which is optimized for a different architecture. In this work, default and vector versions of a realworld application code are refactored to be a single version, and the differences between the versions are expressed as user-defined code transformations. As a result, application developers can maintain only the single version, and transform it to its vector version just before the compilation. Although code optimizations for a vector processor are sometimes different from those for other processors, application developers can enjoy the performance of the vector processor without increasing the code complexity. Evaluation results demonstrate that vectorization-aware loop optimization for a vector processor can be expressed as user-defined code transformation rules, and thereby significantly improve the performance of a vector processor without major code modifications.}, 
keywords={optimising compilers;program control structures;software architecture;software maintenance;software metrics;software performance evaluation;user-defined code transformations;application developers;code optimizations;vector processor;code complexity;vectorization-aware loop optimization;code transformation rules;application code;performance improvement;code architecture;Vector processors;Program processors;Optimization;Kernel;Mathematical model;Cache memory;Computer architecture;Vectorization-aware loop optimization;user-defined code transformaiton;Xevolver}, 
doi={10.1109/CLUSTER.2017.102}, 
ISSN={2168-9253}, 
month={Sep.},}
@INPROCEEDINGS{7081844, 
author={M. Dias and A. Bacchelli and G. Gousios and D. Cassou and S. Ducasse}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Untangling fine-grained code changes}, 
year={2015}, 
volume={}, 
number={}, 
pages={341-350}, 
abstract={After working for some time, developers commit their code changes to a version control system. When doing so, they often bundle unrelated changes (e.g., bug fix and refactoring) in a single commit, thus creating a so-called tangled commit. Sharing tangled commits is problematic because it makes review, reversion, and integration of these commits harder and historical analyses of the project less reliable. Researchers have worked at untangling existing commits, i.e., finding which part of a commit relates to which task. In this paper, we contribute to this line of work in two ways: (1) A publicly available dataset of untangled code changes, created with the help of two developers who accurately split their code changes into self contained tasks over a period of four months; (2) a novel approach, EpiceaUntangler, to help developers share untangled commits (aka. atomic commits) by using fine-grained code change information. EpiceaUntangler is based and tested on the publicly available dataset, and further evaluated by deploying it to 7 developers, who used it for 2 weeks. We recorded a median success rate of 91% and average one of 75%, in automatically creating clusters of untangled fine-grained code changes.}, 
keywords={software maintenance;fine-grained code change untangling;version control system;fine-grained code change information;EpiceaUntangler;Software;Clustering algorithms;Testing;Reliability;Machine learning algorithms;Training;Java}, 
doi={10.1109/SANER.2015.7081844}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{8095113, 
author={J. López-Fandiño and D. B. Heras and F. Argüello and R. J. Duro}, 
booktitle={2017 9th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)}, 
title={CUDA multiclass change detection for remote sensing hyperspectral images using extended morphological profiles}, 
year={2017}, 
volume={1}, 
number={}, 
pages={404-409}, 
abstract={The need for information of the Earth's surface is growing as it is the base for applications such as monitoring the land uses or performing environmental studies, for example. In this context the effective change detection (CD) among multitemporal datasets is a key process that must produce accurate results obtained by computationally efficient algorithms. Most of the CD methods are focused on binary detection (presence or absence of changes) or in the clustering of the different detected types of changes. In this paper, a CUDA scheme to perform pixel-based multiclass CD for hyperspectral datasets is introduced. The scheme combines multiclass CD with binary CD to obtain an accurate multiclass change map. The combination with the binary map contributes to reducing the execution time of the CUDA code. The binary CD is based on performing the difference among images based on Euclidean and Spectral Angle Mapper (SAM) distances and a later thresholding by Otsu's algorithm to detect the changed pixels. The multiclass CD begins with the fusion of the multitemporal data following with feature extraction by Principal Component Analysis (PCA) and incorporating spatial features by means of an Extended Morphological Profile (EMP). The resulting dataset is filtered using the binary CD map and classified pixel by pixel by the supervised algorithms Extreme Learning Machine (ELM) and Support Vector Machine (SVM). The scheme was validated in a non-synthetic multitemporal hyperspectral dataset.}, 
keywords={feature extraction;geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;image segmentation;learning (artificial intelligence);principal component analysis;remote sensing;support vector machines;binary map;CUDA code;changed pixels;multiclass CD;Extended Morphological Profile;resulting dataset;binary CD map;nonsynthetic multitemporal hyperspectral dataset;CUDA multiclass change detection;remote sensing hyperspectral images;extended morphological profiles;land uses;performing environmental studies;context the effective change detection;multitemporal datasets;computationally efficient algorithms;CD methods;binary detection;CUDA scheme;hyperspectral datasets;multiclass change map;Graphics processing units;Hyperspectral imaging;Feature extraction;Principal component analysis;Support vector machines;Instruction sets;Hyperspectral Change Detection;Direct Multidate Classification;Extended Morphological Profiles;Segmentation;Spectral Angle Mapper;GPU;CUDA}, 
doi={10.1109/IDAACS.2017.8095113}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6883030, 
author={Y. S. Yoon and B. A. Myers}, 
booktitle={2014 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
title={A longitudinal study of programmers' backtracking}, 
year={2014}, 
volume={}, 
number={}, 
pages={101-108}, 
abstract={Programming often involves reverting source code to an earlier state, which we call backtracking. We performed a longitudinal study of programmers' backtracking, analyzing 1,460 hours of fine-grained code editing logs collected from 21 people. Our analysis method keeps track of the change history of each abstract syntax tree node and looks for backtracking instances within each node. Using this method, we detected a total of 15,095 backtracking instances, which gives an average backtracking rate of 10.3/hour. The size of backtracking varied con-siderably, ranging from a single character to thousands of char-acters. 34% of the backtracking was performed by manually deleting or typing the desired code, and 9.5% of all backtracking was selective, meaning that it could not have been performed using the conventional undo command present in the IDE. The study results show that programmers need better backtracking tools, and also provide design implications for such tools.}, 
keywords={backtracking;interactive programming;software tools;source code (software);programmer backtracking;programming;source code;longitudinal study;fine-grained code editing logs;abstract syntax tree node;backtracking instances;backtracking rate;undo command;IDE;backtracking tools;design implications;interactive development environments;History;Programming;Encoding;Educational institutions;Face;Java;Detectors;empirical study;backtracking;undo;interactive development environments (IDE)}, 
doi={10.1109/VLHCC.2014.6883030}, 
ISSN={1943-6092}, 
month={July},}
@INPROCEEDINGS{8555738, 
author={A. I. Elkhawas and N. Abdelbaki}, 
booktitle={2018 26th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)}, 
title={Malware Detection using Opcode Trigram Sequence with SVM}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Malicious software also known as “Malware” is software that uses legitimate instructions or code to perform malicious actions. Malware poses a major threat for computer security and information security in general. Over the years, malware has evolved to the point that a single malware specimen can have hundreds or maybe thousands of variants using polymorphic and metamorphic transformation to change the signature of the malware variant in propagation. The common signature-based malware detection methods are no longer robust to detect these variants due to the alteration of code. Static analysis is required to obtain these signatures and anti-virus companies are required to propagate these signature updates to their software. A faster detection method is needed to compensate the exponentially increasing number of malware variants. Machine learning is a trending approach for malware detection. This removes the need to use signature-based detection and is also faster. Software companies do not need to propagate signatures as often. Machine learning algorithms using opcode sequences can recognise patterns in the malicious code that are not present in common signatures and classify them more efficiently. Therefore, a machine learning approach for malware detection should be adopted for faster and more efficient detection. Most research in malware detection using machine learning used static attributes such as network connections, processes spawned, hashes, etc., that were not that robust to changes. In this paper we introduced our novel approach in using trigrams and PE file attributes as features for malware detection. We took a text mining approach to make our detection method more robust to polymorphism and metamorphism. The instruction sequence for critical code in malware on the assembly level is basically the same across malware families. We used opcode trigram sequences as the main feature for our machine learning algorithm. We used Support Vector Machine(SVM) as our classifying algorithm which is a discriminative classifier model that gives a definite decision whether the predicted outcome belongs to the learned class or not. The above shows our novel approach that enabled us to get higher detection rates with less features.}, 
keywords={data mining;digital signatures;DP industry;invasive software;learning (artificial intelligence);support vector machines;malware detection;common signature;support vector machine;SVM;software companies;text mining;trigrams;PE file attributes;polymorphic transformation;metamorphic transformation;signature-based detection;opcode trigram sequence;machine learning;Malware;Feature extraction;Support vector machines;Machine learning;Machine learning algorithms;Classification algorithms;Malware Detection;Machine Learning;N-grams;Opcode Sequence;Support Vector Machine;SVM}, 
doi={10.23919/SOFTCOM.2018.8555738}, 
ISSN={1847-358X}, 
month={Sep.},}
@INPROCEEDINGS{6978164, 
author={F. Palma and L. An and F. Khomh and N. Moha and Y. Guéhéneuc}, 
booktitle={2014 IEEE 7th International Conference on Service-Oriented Computing and Applications}, 
title={Investigating the Change-Proneness of Service Patterns and Antipatterns}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Like any other software systems, service-based systems (SBSs) evolve frequently to accommodate new user requirements. This evolution may degrade their design and implementation and may cause the introduction of common bad practice solutions -- antipatterns -- in opposition to patterns which are good solutions to common recurring design problems. We believe that the degradation of the design of SBSs does not only affect the clients of the SBSs but also the maintenance and evolution of the SBSs themselves. This paper presents the results of an empirical study that aimed to quantify the impact of service patterns and antipatterns on the maintenance and evolution of SBSs. We measure the maintenance effort of a service implementation in terms of the number of changes and the size of changes (i.e., Code churns) performed by developers to maintain and evolve the service, two effort metrics that have been widely used in software engineering studies. Using data collected from the evolutionary history of the SBS FraSCAti, we investigate if (1) services involved in patterns require less maintenance effort, (2) services detected as antipatterns require more maintenance effort than other services, and (3) if some particular service antipatterns are more change-prone than others. Results show that (1) services involved in patterns require less maintenance effort, but not at statistically significant level, (2) services detected as antipatterns require significantly more maintenance effort than non-antipattern services, and (3) services detected as God Component, Multi Service, and Service Chain antipatterns are more change-prone (i.e., Require more maintenance effort) than the services involved in other antipatterns. We also analysed the relation between object-oriented code smells and service patterns/antipatterns and found a significant difference in the proportion of code smells contained in the implementations of service patterns and antipatterns.}, 
keywords={object-oriented programming;service-oriented architecture;software maintenance;software metrics;service pattern change-proneness;antipattern change-proneness;SBS maintenance;SBS evolution;service-based systems;effort metrics;software engineering;SBS FraSCAti;God component antipattern;multiservice antipattern;service chain antipattern;object-oriented code smells;service oriented architecture;SOA;Maintenance engineering;Java;History;Service-oriented architecture;Data mining;Quality of service;Size measurement;SOA;services;patterns;antipatterns;maintenance;change-proneness;empirical software engineering}, 
doi={10.1109/SOCA.2014.43}, 
ISSN={2163-2871}, 
month={Nov},}
@INPROCEEDINGS{6632584, 
author={N. Shafiei and F. van Breugel}, 
booktitle={2013 3rd International Workshop on Games and Software Engineering: Engineering Computer Games to Enable Positive, Progressive Change (GAS)}, 
title={Towards model checking of computer games with Java PathFinder}, 
year={2013}, 
volume={}, 
number={}, 
pages={15-21}, 
abstract={We show that Java source code of computer games can be checked for bugs such as uncaught exceptions by the model checker Java PathFinder (JPF). To model check Java games, we need to tackle the state space explosion problem and handle native calls. To address those two challenges we use our extensions of JPF, jpf-probabilistic and jpf-nhandler. The former deals with the randomization in the source code of the game, which is a cause of the state space explosion problem. The latter handles native calls automatically. We show how JPF enhanced with our extensions can check games such as the text based game Hamurabi and a graphics based version of rock-paper-scissors.}, 
keywords={computer games;Java;program verification;model checking;computer games;Java PathFinder;Java source code;jpf-probabilistic;jpf-nhandler;source code randomization;state space explosion problem;Hamurabi text based game;rock-paper-scissors graphics version;Games;Java;Computer bugs;Explosions;Markov processes;Model checking;Probabilistic logic}, 
doi={10.1109/GAS.2013.6632584}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7966072, 
author={T. Moraitis and A. Sebastian and I. Boybat and M. Le Gallo and T. Tuma and E. Eleftheriou}, 
booktitle={2017 International Joint Conference on Neural Networks (IJCNN)}, 
title={Fatiguing STDP: Learning from spike-timing codes in the presence of rate codes}, 
year={2017}, 
volume={}, 
number={}, 
pages={1823-1830}, 
abstract={Spiking neural networks (SNNs) could play a key role in unsupervised machine learning applications, by virtue of strengths related to learning from the fine temporal structure of event-based signals. However, some spike-timing-related strengths of SNNs are hindered by the sensitivity of spike-timing-dependent plasticity (STDP) rules to input spike rates, as fine temporal correlations may be obstructed by coarser correlations between firing rates. In this article, we propose a spike-timing-dependent learning rule that allows a neuron to learn from the temporally-coded information despite the presence of rate codes. Our long-term plasticity rule makes use of short-term synaptic fatigue dynamics. We show analytically that, in contrast to conventional STDP rules, our fatiguing STDP (FSTDP) helps learn the temporal code, and we derive the necessary conditions to optimize the learning process. We showcase the effectiveness of FSTDP in learning spike-timing correlations among processes of different rates in synthetic data. Finally, we use FSTDP to detect correlations in real-world weather data from the United States in an experimental realization of the algorithm that uses a neuro-morphic hardware platform comprising phase-change memristive devices. Taken together, our analyses and demonstrations suggest that FSTDP paves the way for the exploitation of the spike-based strengths of SNNs in real-world applications.}, 
keywords={data handling;geophysics computing;meteorology;neural nets;unsupervised learning;spike-timing codes;rate codes;spiking neural networks;SNN;unsupervised machine learning applications;fine temporal structure;event-based signals;spike-timing-dependent plasticity sensitivity;fine temporal correlations;input spike rates;firing rates;spike-timing-dependent learning rule;temporally-coded information;long-term plasticity rule;short-term synaptic fatigue dynamics;fatiguing STDP;FSTDP;temporal code;necessary conditions;spike-timing correlation learning;real-world weather data;United States;neuromorphic hardware platform;phase-change memristive devices;Neurons;Correlation;Fatigue;Timing;Biological neural networks;Biological information theory;Unsupervised Learning}, 
doi={10.1109/IJCNN.2017.7966072}, 
ISSN={2161-4407}, 
month={May},}
@INPROCEEDINGS{8473199, 
author={M. Malhotra and J. Kumar Chhabra}, 
booktitle={2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT)}, 
title={Class Level Code Summarization Based on Dependencies and Micro Patterns}, 
year={2018}, 
volume={}, 
number={}, 
pages={1011-1016}, 
abstract={Modifications in any software need to be carries out on various entities. Change in one entity may force some changes in many other dependent entities. Complete understanding of entities and there dependencies are highly desirable to carry out modifications in an efficient manner. Automated summarization of classes in object oriented software can be a good step in this direction. This paper proposes a natural language summary based code summarization of those java classes which are more change prone. Code summary is generated by using concept of micro patterns and change proneness is identified by computing different kinds of dependencies among classes. A threshold is decided to identify the classes which are more sensitive to change. The empirical evaluation of some open source classes has been carried out which clearly indicates the usefulness of the proposed work.}, 
keywords={Java;natural language processing;object-oriented programming;class level code summarization;micropatterns;dependent entities;complete understanding;automated summarization;object oriented software;natural language summary based code summarization;java classes;code summary;change proneness;open source classes;Sensitivity;Java;Software systems;Natural languages;Maintenance engineering;Machine learning;Program comprehension;Class level summarization;change sensitivity}, 
doi={10.1109/ICICCT.2018.8473199}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7381819, 
author={L. Amorim and E. Costa and N. Antunes and B. Fonseca and M. Ribeiro}, 
booktitle={2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)}, 
title={Experience report: Evaluating the effectiveness of decision trees for detecting code smells}, 
year={2015}, 
volume={}, 
number={}, 
pages={261-269}, 
abstract={Developers continuously maintain software systems to adapt to new requirements and to fix bugs. Due to the complexity of maintenance tasks and the time-to-market, developers make poor implementation choices, also known as code smells. Studies indicate that code smells hinder comprehensibility, and possibly increase change- and fault-proneness. Therefore, they must be identified to enable the application of corrections. The challenge is that the inaccurate definitions of code smells make developers disagree whether a piece of code is a smell or not, consequently, making difficult creation of a universal detection solution able to recognize smells in different software projects. Several works have been proposed to identify code smells but they still report inaccurate results and use techniques that do not present to developers a comprehensive explanation how these results have been obtained. In this experimental report we study the effectiveness of the Decision Tree algorithm to recognize code smells. For this, it was applied in a dataset containing 4 open source projects and the results were compared with the manual oracle, with existing detection approaches and with other machine learning algorithms. The results showed that the approach was able to effectively learn rules for the detection of the code smells studied. The results were even better when genetic algorithms are used to pre-select the metrics to use.}, 
keywords={decision trees;genetic algorithms;learning (artificial intelligence);program debugging;software maintenance;software quality;decision trees;code smells;software systems;bugs;maintenance tasks;universal detection solution;software projects;decision tree algorithm;manual oracle;machine learning algorithms;genetic algorithms;Decision trees;Measurement;Machine learning algorithms;Software algorithms;Algorithm design and analysis;Software;Genetic algorithms;Software Quality;Code Smells;Decision Tree;Genetic Algorithm}, 
doi={10.1109/ISSRE.2015.7381819}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7181431, 
author={T. B. Le and M. Linares-Vasquez and D. Lo and D. Poshyvanyk}, 
booktitle={2015 IEEE 23rd International Conference on Program Comprehension}, 
title={RCLinker: Automated Linking of Issue Reports and Commits Leveraging Rich Contextual Information}, 
year={2015}, 
volume={}, 
number={}, 
pages={36-47}, 
abstract={Links between issue reports and their corresponding commits in version control systems are often missing. However, these links are important for measuring the quality of various parts of a software system, predicting defects, and many other tasks. A number of existing approaches have been designed to solve this problem by automatically linking bug reports to source code commits via comparison of textual information in commit messages with textual contents in the bug reports. Yet, the effectiveness of these techniques is oftentimes sub optimal when commit messages are empty or only contain minimum information, this particular problem makes the process of recovering trace ability links between commits and bug reports particularly challenging. In this work, we aim at improving the effectiveness of existing bug linking techniques by utilizing rich contextual information. We rely on a recently proposed tool, namely Change Scribe, which generates commit messages containing rich contextual information by using a number of code summarization techniques. Our approach then extracts features from these automatically generated commit messages and bug reports and inputs them into a classification technique that creates a discriminative model used to predict if a link exists between a commit message and a bug report. We compared our approach, coined as RCLinker (Rich Context Linker), to MLink, which is an existing state-of-the-art bug linking approach. Our experiment results on bug reports from 6 software projects show that RCLinker can outperform MLink in terms of F-measure by 138.66%.}, 
keywords={configuration management;program debugging;program diagnostics;software quality;source code (software);automated bug issue report linking;automated source code commit linking;rich-contextual information leveraging;version control systems;software system quality;defect prediction;textual information;commit messages;traceability links;ChangeScribe;code summarization techniques;classification technique;discriminative model;RCLinker;Rich-Context Linker;MLink;software projects;F-measure;Feature extraction;Predictive models;Training;Metadata;Joining processes;XML;Control systems;Recovering Missing Links;Classification;Feature Extraction;ChangeScribe}, 
doi={10.1109/ICPC.2015.13}, 
ISSN={1092-8138}, 
month={May},}
@INPROCEEDINGS{8323088, 
author={T. Omori}, 
booktitle={2018 IEEE Workshop on Mining and Analyzing Interaction Histories (MAINT)}, 
title={Privacy preservation in interaction history on integrated development environments}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={The interaction history in a software development environment allows us to analyze how developers change source code and how they use tools on the integrated development environment. Sharing the interaction history with tool providers increases the chances that developers obtain better tools. However, the interaction history sometimes contains privacy-sensitive information, which is an obstacle in collecting and using the interaction history. As an attempt to tackle this issue, this paper proposes a technique to replace sensitive text in a recorded interaction history. This paper describes the proposed technique, its current implementation, the results of a preliminary survey on how potential privacy-sensitive information exists in recorded interaction histories, and how privacy issues in sharing interaction histories can be ameliorated.}, 
keywords={data privacy;integrated development environment;software development environment;interaction history;privacy preservation;source code;privacy-sensitive information;History;Tools;Privacy;Software;Manuals;Data privacy;Publishing}, 
doi={10.1109/MAINT.2018.8323088}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7589806, 
author={L. Meurice and C. Nagy and A. Cleve}, 
booktitle={2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)}, 
title={Detecting and Preventing Program Inconsistencies under Database Schema Evolution}, 
year={2016}, 
volume={}, 
number={}, 
pages={262-273}, 
abstract={Nowadays, data-intensive applications tend to access their underlying database in an increasingly dynamic way. The queries that they send to the database server are usually built at runtime, through String concatenation, or Object-Relational-Mapping (ORM) frameworks. This level of dynamicity significantly complicates the task of adapting application programs to database schema changes. Failing to correctly adapt programs to an evolving database schema results in program inconsistencies, which in turn may cause program failures. In this paper, we present a tool-supported approach, that allows developers to (1) analyze how the source code and database schema co-evolved in the past and (2) simulate a database schema change and automatically determine the set of source code locations that would be impacted by this change. Developers are then provided with recommendations about what they should modify at those source code locations in order to avoid inconsistencies. The approach has been designed to deal with Java systems that use dynamic data access frameworks such as JDBC, Hibernate and JPA. We motivate and evaluate the proposed approach, based on three real-life systems of different size and nature.}, 
keywords={database management systems;Java;object-oriented programming;query processing;software fault tolerance;software tools;source code (software);program inconsistencies detection;program inconsistencies prevention;database schema evolution;data-intensive applications;queries;database server;string concatenation;object-relational-mapping;ORM frameworks;application programs;program failures;tool-supported approach;source code locations;Java systems;dynamic data access frameworks;Java;History;Relational databases;Open source software;Libraries;Industries;what-if approach;database schema evolution;ORM;JDBC;program-database co-evolution}, 
doi={10.1109/QRS.2016.38}, 
ISSN={}, 
month={Aug},}
@ARTICLE{7898457, 
author={S. McIntosh and Y. Kamei}, 
journal={IEEE Transactions on Software Engineering}, 
title={Are Fix-Inducing Changes a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Prediction}, 
year={2018}, 
volume={44}, 
number={5}, 
pages={412-428}, 
abstract={Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.}, 
keywords={data mining;just-in-time;learning (artificial intelligence);public domain software;software management;software quality;source code (software);Just-In-Time models;fix-inducing code changes;code change properties;target moving;just-in-time defect prediction;fix-inducing changes;JIT models;OpenStack systems;Qt systems;mining software repositories;Predictive models;Data models;Software;Complexity theory;Market research;Context modeling;Calibration;Just-In-Time prediction;defect prediction;mining software repositories}, 
doi={10.1109/TSE.2017.2693980}, 
ISSN={0098-5589}, 
month={May},}
@ARTICLE{7270338, 
author={M. Mirakhorli and J. Cleland-Huang}, 
journal={IEEE Transactions on Software Engineering}, 
title={Detecting, Tracing, and Monitoring Architectural Tactics in Code}, 
year={2016}, 
volume={42}, 
number={3}, 
pages={205-220}, 
abstract={Software architectures are often constructed through a series of design decisions. In particular, architectural tactics are selected to satisfy specific quality concerns such as reliability, performance, and security. However, the knowledge of these tactical decisions is often lost, resulting in a gradual degradation of architectural quality as developers modify the code without fully understanding the underlying architectural decisions. In this paper we present a machine learning approach for discovering and visualizing architectural tactics in code, mapping these code segments to tactic traceability patterns, and monitoring sensitive areas of the code for modification events in order to provide users with up-to-date information about underlying architectural concerns. Our approach utilizes a customized classifier which is trained using code extracted from fifty performance-centric and safety-critical open source software systems. Its performance is compared against seven off-the-shelf classifiers. In a controlled experiment all classifiers performed well; however our tactic detector outperformed the other classifiers when used within the larger context of the Hadoop Distributed File System. We further demonstrate the viability of our approach for using the automatically detected tactics to generate viable and informative messages in a simulation of maintenance events mined from Hadoop's change management system.}, 
keywords={learning (artificial intelligence);pattern classification;program diagnostics;public domain software;software architecture;system monitoring;code architectural tactics monitoring;code architectural tactics detection;code architectural tactics tracing;machine learning;code architectural tactics visualization;tactic traceability patterns;performance-centric software systems;safety-critical open source software systems;off-the-shelf classifiers;Hadoop Distributed File System;Hadoops change management system;Heart beat;Monitoring;Detectors;Reliability;Biomedical monitoring;Authentication;Architecture;traceability;tactics;traceability information models;Architecture;traceability;tactics;traceability information models}, 
doi={10.1109/TSE.2015.2479217}, 
ISSN={0098-5589}, 
month={March},}
@INPROCEEDINGS{7816531, 
author={B. Li}, 
booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Automatically Documenting Software Artifacts}, 
year={2016}, 
volume={}, 
number={}, 
pages={631-635}, 
abstract={Software artifacts constantly change during evolution and maintenance of software systems. One critical artifact that developers need to be able to maintain during evolution and maintenance of software systems is up-to-date and complete documentation. However, recent studies on the co-evolution of comments and code showed that the comments are rarely maintained or updated when the respective source code is changed. In order to understand developer practices regarding documenting two kinds of software artifacts, unit test cases and database-related operations, we designed two empirical studies both composed of (i) an online survey with contributors of open source projects and (ii) a mining-based analysis of method comments in these projects. Later, motivated by the findings of the studies, we proposed two novel approaches. UnitTestScribe is an approach for automatically documenting test cases, while DBScribe is an approach for automatically documenting test cases. We evaluated our tools by means of an online survey with industrial developers and graduate students. In general, participants indicated that descriptions generated by our tools are complete, concise, and easy to read.}, 
keywords={data mining;database management systems;public domain software;software maintenance;software tools;source code (software);system documentation;software artifacts;software system evolution;software system maintenance;comment coevolution;code coevolution;source code;unit test cases;database related operations;open source projects;mining-based analysis;UnitTestScribe;automatic documentation test cases;DBScribe;Databases;Documentation;Maintenance engineering;Software systems;Java;History;Documenting;Unit Tests;Database}, 
doi={10.1109/ICSME.2016.56}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6698911, 
author={K. Herzig and S. Just and A. Rau and A. Zeller}, 
booktitle={2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)}, 
title={Predicting defects using change genealogies}, 
year={2013}, 
volume={}, 
number={}, 
pages={118-127}, 
abstract={When analyzing version histories, researchers traditionally focused on single events: e.g. the change that causes a bug, the fix that resolves an issue. Sometimes however, there are indirect effects that count: Changing a module may lead to plenty of follow-up modifications in other places, making the initial change having an impact on those later changes. To this end, we group changes into change genealogies, graphs of changes reflecting their mutual dependencies and influences and develop new metrics to capture the spatial and temporal influence of changes. In this paper, we show that change genealogies offer good classification models when identifying defective source files: With a median precision of 73% and a median recall of 76%, change genealogy defect prediction models not only show better classification accuracies as models based on code complexity, but can also outperform classification models based on code dependency network metrics.}, 
keywords={data mining;graph theory;pattern classification;program debugging;software metrics;software quality;defect prediction;change genealogies;change graphs;mutual dependencies;classification models;median precision;median recall;defective source file identification;code complexity;code dependency network metrics;data mining;software engineering;software quality estimation;software quality prediction;Measurement;Predictive models;Complexity theory;Object oriented modeling;History;Computer bugs;Computational modeling;Data mining;Predictive models;Software qual-ity;Software engineering}, 
doi={10.1109/ISSRE.2013.6698911}, 
ISSN={1071-9458}, 
month={Nov},}
@INPROCEEDINGS{8453123, 
author={S. McIntosh and Y. Kamei}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)}, 
title={[Journal First] Are Fix-Inducing Changes a Moving Target?: A Longitudinal Case Study of Just-in-Time Defect Prediction}, 
year={2018}, 
volume={}, 
number={}, 
pages={560-560}, 
abstract={Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.}, 
keywords={data mining;just-in-time;learning (artificial intelligence);public domain software;software fault tolerance;software management;fix-inducing changes;JIT models;just-in-time models;just-in-time defect prediction;Qt systems;OpenStack systems;fix-inducing code changes;code change properties;Software engineering;Predictive models;History;Fluctuations;Training;Data models;Software;Just In Time prediction;Defect prediction;Mining software repositories}, 
doi={}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{7961530, 
author={S. Jiang and C. McMillan}, 
booktitle={2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)}, 
title={Towards Automatic Generation of Short Summaries of Commits}, 
year={2017}, 
volume={}, 
number={}, 
pages={320-323}, 
abstract={Committing to a version control system means submitting a software change to the system. Each commit can have a message to describe the submission. Several approaches have been proposed to automatically generate the content of such messages. However, the quality of the automatically generated messages falls far short of what humans write. In studying the differences between auto-generated and human-written messages, we found that 82% of the human-written messages have only one sentence, while the automatically generated messages often have multiple lines. Furthermore, we found that the commit messages often begin with a verb followed by an direct object. This finding inspired us to use a "verb+object" format in this paper to generate short commit summaries. We split the approach into two parts: verb generation and object generation. As our first try, we trained a classifier to classify a diff to a verb. We are seeking feedback from the community before we continue to work on generating direct objects for the commits.}, 
keywords={configuration management;pattern classification;automatic generation;short commit summaries;version control system;software change;message content;auto-generated message;human-written message;classifier training;Data analysis;Software;Grammar;Syntactics;Histograms;Control systems;Training;program comprehension;commit log;commit message;version control system;code differencing;natural language processing}, 
doi={10.1109/ICPC.2017.12}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6755360, 
author={K. K. Chaturvedi and P. Bedi and S. Misra and V. B. Singh}, 
booktitle={2013 IEEE 16th International Conference on Computational Science and Engineering}, 
title={An Empirical Validation of the Complexity of Code Changes and Bugs in Predicting the Release Time of Open Source Software}, 
year={2013}, 
volume={}, 
number={}, 
pages={1201-1206}, 
abstract={With the increasing popularity of open source software, the changes in source code are inevitable. These changes in code are due to feature enhancement, new feature introduction and bug repair or fixed. It is important to note that these changes can be quantified by using entropy based measures. The pattern of bug fixing scenario with complexity of code change is responsible for the next release as these changes will cover the number of requirements and fixes. In this paper, we are proposing a method to predict the next release problem based on the complexity of code change and bugs fixed. We applied multiple linear regression to predict the time of the next release of the product and measured the performance using different residual statistics, goodness of fit curve and R2. We observed from the results of multiple linear regression that the predicted value of release time is fitting well with the observed value of number of months for the next release.}, 
keywords={program debugging;public domain software;regression analysis;open source software;code changes complexity;bugs complexity;feature enhancement;feature introduction;bug repair;bug fixed;multiple linear regression;Computer bugs;Complexity theory;Linear regression;Open source software;History;Maintenance engineering;open source software;complexity of code change;next release time;bug repositories;software configuration management repositories}, 
doi={10.1109/CSE.2013.201}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6240501, 
author={H. Sajnani}, 
booktitle={2012 20th IEEE International Conference on Program Comprehension (ICPC)}, 
title={Automatic software architecture recovery: A machine learning approach}, 
year={2012}, 
volume={}, 
number={}, 
pages={265-268}, 
abstract={Automatically recovering functional architecture of the software can facilitate the developer's understanding of how the system works. In legacy systems, original source code is often the only available source of information about the system and it is very time consuming to understand source code. Current architecture recovery techniques either require heavy human intervention or fail to recover quality components. To alleviate these shortcomings, we propose use of machine learning techniques which use structural, runtime behavioral, domain, textual and contextual (e.g. code authorship, line co-change) features. These techniques will allow us to experiment with a large number of features of the software artifacts without having to establish a priori our own insights about what is important and what is not important. We believe this is a promising approach that may finally start to produce usable solutions to this elusive problem.}, 
keywords={learning (artificial intelligence);object-oriented programming;software architecture;software maintenance;software quality;automatic software architecture recovery;automatic functional architecture recovery;legacy systems;source code;quality component recovery;machine learning;runtime behavioral;structural features;domain features;textual features;contextual textual;contextual features;software artifacts;Software;Computer architecture;Documentation;Software algorithms;Machine learning;Clustering algorithms;Feature extraction}, 
doi={10.1109/ICPC.2012.6240501}, 
ISSN={1092-8138}, 
month={June},}
@INPROCEEDINGS{7821007, 
author={Z. Xia and X. Feng and J. Peng and A. Hadid}, 
booktitle={2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)}, 
title={Unsupervised deep hashing for large-scale visual search}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Learning based hashing plays a pivotal role in large-scale visual search. However, most existing hashing algorithms tend to learn shallow models that do not seek representative binary codes. In this paper, we propose a novel hashing approach based on unsupervised deep learning to hierarchically transform features into hash codes. Within the heterogeneous deep hashing framework, the autoencoder layers with specific constraints are considered to model the nonlinear mapping between features and binary codes. Then, a Restricted Boltzmann Machine (RBM) layer with constraints is utilized to reduce the dimension in the hamming space. The experiments on the problem of visual search demonstrate the competitiveness of our proposed approach compared to the state of the art.}, 
keywords={Boltzmann machines;feature extraction;file organisation;image retrieval;unsupervised learning;unsupervised deep learning;visual image search;learning based hashing;feature transformation;hash code;autoencoder layer;nonlinear mapping modelling;restricted Boltzmann machine;RBM;Binary codes;Machine learning;Visualization;Training;Optimization;Linear programming;Image reconstruction;Learning based hashing;Unsupervised learning;Deep learning;Autoencoder;RBM}, 
doi={10.1109/IPTA.2016.7821007}, 
ISSN={2154-512X}, 
month={Dec},}
@INPROCEEDINGS{7739672, 
author={A. Z. Henley and S. D. Fleming}, 
booktitle={2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
title={Yestercode: Improving code-change support in visual dataflow programming environments}, 
year={2016}, 
volume={}, 
number={}, 
pages={106-114}, 
abstract={In this paper, we present the Yestercode tool for supporting code changes in visual dataflow programming environments. In a formative investigation of LabVIEW programmers, we found that making code changes posed a significant challenge. To address this issue, we designed Yestercode to enable the efficient recording, retrieval, and juxtaposition of visual dataflow code while making code changes. To evaluate Yestercode, we implemented our design as a prototype extension to the LabVIEW programming environment, and ran a user study involving 14 professional LabVIEW programmers that compared Yestercode-extended LabVIEW to the standard LabVIEW IDE. Our results showed that Yestercode users introduced fewer bugs during tasks, completed tasks in about the same time, and experienced lower cognitive loads on tasks. Moreover, participants generally reported that Yestercode was easy to use and that it helped in making change tasks easier.}, 
keywords={data flow computing;programming environments;virtual instrumentation;visual programming;code-change support;Yestercode tool;visual dataflow programming environments;LabVIEW programming environment;LabVIEW IDE;Visualization;Wires;Programming environments;Interviews;History;Programming profession}, 
doi={10.1109/VLHCC.2016.7739672}, 
ISSN={1943-6106}, 
month={Sep.},}
@INPROCEEDINGS{8296973, 
author={V. E. Liong and J. Lu and Y. Tan}, 
booktitle={2017 IEEE International Conference on Image Processing (ICIP)}, 
title={Learning a cross-modal hashing network for multimedia search}, 
year={2017}, 
volume={}, 
number={}, 
pages={3700-3704}, 
abstract={In this paper, we propose a cross-modal hashing network (CMHN) method to learn compact binary codes for cross-modality multimedia search. Unlike most existing cross-modal hashing methods which learn a single pair of projections to map each example into a binary vector, we design a deep neural network to learn multiple pairs of hierarchical non-linear transformations, under which the nonlinear characteristics of samples can be well exploited and the modality gap is well reduced. Our model is trained under an iterative optimization procedure which learns a (1) unified binary code discretely and discriminatively through a classification-based hinge-loss criterion, and (2) cross-modal hashing network, one deep network for each modality, through minimizing the quantization loss between real-valued neural code and binary code, and maximizing the variance of the learned neural codes. Experimental results on two benchmark datasets show the efficacy of the proposed approach.}, 
keywords={binary codes;information retrieval;learning (artificial intelligence);neural nets;cross-modal hashing network method;compact binary codes;cross-modality multimedia search;deep neural network;modality gap;deep network;learned neural codes;cross-modal hashing methods;unified binary code;quantization loss;real-valued neural code;CMHN method;hierarchical nonlinear transformation;Binary codes;Optimization;Semantics;Training;Multimedia communication;Quantization (signal);Benchmark testing;hashing;cross-modal retrieval;binary code learning}, 
doi={10.1109/ICIP.2017.8296973}, 
ISSN={2381-8549}, 
month={Sep.},}
@INPROCEEDINGS{8530032, 
author={X. Zhang and Y. Chen and Y. Gu and W. Zou and X. Xie and X. Jia and J. Xuan}, 
booktitle={2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={How do Multiple Pull Requests Change the Same Code: A Study of Competing Pull Requests in GitHub}, 
year={2018}, 
volume={}, 
number={}, 
pages={228-239}, 
abstract={GitHub is a widely used collaborative platform for global software development. A pull request plays an important role in bridging code changes with version controlling. Developers can freely and parallelly submit pull requests to base branches and wait for the merge of their contributions. However, several developers may submit pull requests to edit the same lines of code; such pull requests result in a latent collaborative conflict. We refer such pull requests that tend to change the same lines and remain open during an overlapping time period to as competing pull requests. In this paper, we conduct a study on 9,476 competing pull requests from 60 Java repositories in GitHub. The data are collected by mining pull requests that are submitted in 2017 from top Java projects with the most forks. We explore how multiple pull requests change the same code via answering four research questions, including the distribution of competing pull requests, the involved developers, the changed lines of code, and the impact on pull request integration. Our study shows that there indeed exist competing pull requests in GitHub: in 45 out of 60 repositories, over 31% of pull requests belong to competing pull requests; 20 repositories have more than 100 groups of competing pull requests, each of which is submitted by over five developers; 42 repositories have over 10% of competing pull requests with over 10 same lines of code. Meanwhile, we observe that attributes of competing pull requests do not have strong impacts on pull request integration, comparing with other types of pull requests. Our study provides a preliminary analysis for further research that aims to detect and eliminate conflicts among competing pull requests.}, 
keywords={configuration management;data mining;groupware;software engineering;GitHub;collaborative platform;global software development;code changes;version controlling;pull requests mining;Collaboration;Java;Merging;Software;Data mining;Control systems;Correlation;Pull requests;collaborative development;merge conflict;GitHub}, 
doi={10.1109/ICSME.2018.00032}, 
ISSN={2576-3148}, 
month={Sep.},}
@INPROCEEDINGS{6396471, 
author={S. Arabyarmohamady and H. Moradi and M. Asadpour}, 
booktitle={Proceedings of 2012 International Conference on Interactive Mobile and Computer Aided Learning (IMCL)}, 
title={A coding style-based plagiarism detection}, 
year={2012}, 
volume={}, 
number={}, 
pages={180-186}, 
abstract={In this paper a plagiarism detection framework is proposed based on coding style. Furthermore, the typical style-based approach is improved to better detect plagiarism in programming codes. The plagiarism detection is performed in two phases: in the first phase the main features representing a coding style are extracted. In the second phase the extracted features are used in three different modules to detect the plagiarized codes and to determine the giver and takers of the codes. The extracted features for each code developer are kept in a history log, i.e. a user profile as his/her style of coding, and would be used to determine the change in coding style. The user profile allows the system to detect if a code is truly developed by the claimed developer or it is written by another person, having another style. Furthermore, the user profile allows determining the code giver and code taker when two codes are similar by comparing the codes' styles with the style of the programmers. Also if a code is copied from the internet or developed by a third party, then the style of who claims the ownership of the code is normally less proficient in coding than the third party and can be detected. The difference between the style levels is done through the style level checker module in the proposed framework. The proposed framework has been implemented and tested and the results are compared to Moss which shows comparable performance in detecting plagiarized codes.}, 
keywords={computer science education;document handling;Internet;program verification;security of data;coding style-based plagiarism detection;programming code;coding style extraction;plagiarized code detection;feature extraction;code development;Internet;style level checker module;Plagiarism;Feature extraction;Encoding;Vectors;Watermarking;Internet;Conferences;Plagiarism detection;author identification;software forensics;source code}, 
doi={10.1109/IMCL.2012.6396471}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6645278, 
author={Y. Yoon}, 
booktitle={2013 IEEE Symposium on Visual Languages and Human Centric Computing}, 
title={Better backtracking support for programmers}, 
year={2013}, 
volume={}, 
number={}, 
pages={187-188}, 
abstract={Programmers often need to backtrack while coding, yet there is only limited support for backtracking in modern programming tools. Our study results confirmed the prevalence of backtracking and identified several problems programmers face while backtracking. To mitigate these problems, we are building an IDE plug-in aimed at providing better support for backtracking by combining a selective undo mechanism, novel visualizations, and code change history search features. We envision that this approach will help programmers perform backtracking tasks more easily.}, 
keywords={backtracking;program visualisation;software development management;backtracking;modern programming tool;IDE plug-in;visualization;history search feature change;program coding;integrated development environment;History;Visualization;User interfaces;Software;Encoding;Face;Buildings;backtracking;selective undo;history search;integrated development environments;software visualization}, 
doi={10.1109/VLHCC.2013.6645278}, 
ISSN={1943-6092}, 
month={Sep.},}
@INPROCEEDINGS{6404736, 
author={H. Kanazawa and K. Yamanishi}, 
booktitle={2012 IEEE Information Theory Workshop}, 
title={An MDL-based change-detection algorithm with its applications to learning piecewise stationary memoryless sources}, 
year={2012}, 
volume={}, 
number={}, 
pages={557-561}, 
abstract={Kleinberg has proposed an algorithm for detecting bursts from a data sequence, which has turned out to be effective in the scenario of data mining, such as topic detection, change-detection. In this paper we extend Kleinberg's algorithm in an information-theoretic fashion to obtain a new class of algorithms and apply it into learning of piecewise stationary memoryless sources (PSMSs). The keys of the proposed algorithm are; 1) the parameter space is discretized so that discretization scale depends on the Fisher information, and 2) the optimal path over the discretized parameter space is efficiently computed using the dynamic programming method so that the sum of the data and parameter description lengths is minimized on the basis of the MDL principle. We prove that an upper bound on the total code-length for the proposed algorithm asymptotically matches Merhav's lower bound.}, 
keywords={data mining;dynamic programming;learning (artificial intelligence);MDL-based change detection algorithm;minimum description length;piecewise stationary memoryless source learning;data sequence;data mining;topic detection;Kleinberg algorithm;PSMS learning;parameter space;discretization scale;dynamic programming method;parameter description length;Erbium}, 
doi={10.1109/ITW.2012.6404736}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7873015, 
author={Q. Zhang and L. Zhou and W. Li and K. Ricanek and X. Li}, 
booktitle={2016 International Conference on Machine Learning and Cybernetics (ICMLC)}, 
title={Face detection method based on histogram of sparse code in tree deformable model}, 
year={2016}, 
volume={2}, 
number={}, 
pages={996-1002}, 
abstract={Face detection is a challenging research area and crucial step of face detection system. Because of the factors of rotation, pose change, and complicated background, false faces also can be found in detection results. This paper puts forward a new approach based on the landmark localization to detect face image which includes various pose variation. Furthermore, the proposed histogram of sparse code-based method is very effective and it can capture global elastic and multi-view deformation which can be optimized easily. The proposed method achieved higher effectiveness and efficiency in comparison with the existing face detection methods on different data sets.}, 
keywords={face recognition;image coding;object detection;pose estimation;face detection method;sparse code histogram;tree deformable model;pose change;complicated background;landmark localization;pose variation;sparse code-based method;global elastic deformation;multi-view deformation;Face;Face detection;Histograms;Dictionaries;Feature extraction;Deformable models;Training;Face detection;Deformable part model;Sparse code}, 
doi={10.1109/ICMLC.2016.7873015}, 
ISSN={2160-1348}, 
month={July},}
@INPROCEEDINGS{6614727, 
author={K. Kevic and S. C. Müller and T. Fritz and H. C. Gall}, 
booktitle={2013 6th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE)}, 
title={Collaborative bug triaging using textual similarities and change set analysis}, 
year={2013}, 
volume={}, 
number={}, 
pages={17-24}, 
abstract={Bug triaging assigns a bug report, which is also known as a work item, an issue, a task or simply a bug, to the most appropriate software developer for fixing or implementing it. However, this task is tedious, time-consuming and error-prone if not supported by effective means. Current techniques either use information retrieval and machine learning to find the most similar bugs already fixed and recommend expert developers, or they analyze change information stemming from source code to propose expert bug solvers. Neither technique combines textual similarity with change set analysis and thereby exploits the potential of the interlinking between bug reports and change sets. In this paper, we present our approach to identify potential experts by identifying similar bug reports and analyzing the associated change sets. Studies have shown that effective bug triaging is done collaboratively in a meeting, as it requires the coordination of multiple individuals, the understanding of the project context and the understanding of the specific work practices. Therefore, we implemented our approach on a multi-touch table to allow multiple stakeholders to interact simultaneously in the bug triaging and to foster their collaboration. In the current stage of our experiments we have experienced that the expert recommendations are more specific and useful when the rationale behind the expert selection is also presented to the users.}, 
keywords={program debugging;text analysis;collaborative bug triaging;textual similarity;change set analysis;work item;software developer;information retrieval;machine learning;change information analysis;source code;bug report identification;multitouch table approach;Collaboration;Vectors;Software;User interfaces;Data models;Computer bugs;Prototypes;bug triaging;collaboration;expert recommendation;multi-touch}, 
doi={10.1109/CHASE.2013.6614727}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7532883, 
author={Z. Yang and Z. Cheng and C. C. Loy and W. C. Lau and C. M. Li and G. Li}, 
booktitle={2016 IEEE International Conference on Image Processing (ICIP)}, 
title={Towards robust color recovery for high-capacity color QR codes}, 
year={2016}, 
volume={}, 
number={}, 
pages={2866-2870}, 
abstract={Color brings extra data capacity for QR codes, but it also brings tremendous challenges to the decoding because of color interference and illumination variation, especially for high-density QR codes. In this paper, we put forth a framework for high-capacity QR codes, HiQ, which optimizes the decoding algorithm for high-density QR codes to achieve robust and fast decoding on mobile devices, and adopts a learning-based approach for color recovery. Moreover, we propose a robust geometric transformation algorithm to correct the geometric distortion. We also provide a challenging color QR code dataset, CUHK-CQRC, which consists of 5390 high-density color QR code samples captured by different smartphones under different lighting conditions. Experimental results show that HiQ outperforms the baseline [1] by 286% in decoding success rate and 60% in bit error rate.}, 
keywords={decoding;geometry;image colour analysis;learning (artificial intelligence);mobile computing;optimisation;QR codes;CUHK-CQRC;geometric distortion;robust geometric transformation algorithm;learning-based approach;mobile devices;decoding algorithm optimization;high-density QR codes;illumination variation;color interference;HiQ;high-capacity color QR codes;robust color recovery;Image color analysis;Decoding;Robustness;Lighting;Distortion;Encoding;Interference;QR code;color recovery;color interference;illumination variation;high capacity}, 
doi={10.1109/ICIP.2016.7532883}, 
ISSN={2381-8549}, 
month={Sep.},}
@INPROCEEDINGS{7528218, 
author={R. Sarkar and A. Vaccari and S. T. Acton}, 
booktitle={2016 IEEE 12th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)}, 
title={SSPARED: Saliency and sparse code analysis for rare event detection in video}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={The problem of detecting rare and unusual events in video is critical to the analysis of large video datasets. Such events are identified as those occurrences within a sequence that cause a significant change in the scene. We propose to determine the significance of a frame, while preserving its compact representation, by introducing a saliency-driven dictionary learning technique. The derived sparse codes are then leveraged, together with the Kullback-Leibler divergence, in the design of a histogram-based metric that we use to evaluate the scene changes between consecutive frames. Our method, SSPARED, is compared with two state of the art methods for anomaly detection and shows significant improvement in detecting abnormal incidents and reduced false alarm generation.}, 
keywords={image sequences;video coding;saliency and sparse code analysis;SSPARED;rare event detection;unusual event detection;video dataset;saliency-driven dictionary learning technique;sparse code;Kullback-Leibler divergence;histogram-based metric;abnormal incident detection;false alarm generation reduction;Dictionaries;Histograms;Video sequences;Event detection;Cameras;Matching pursuit algorithms;Sparse matrices;saliency;dictionary learning;sparse representation;rare event detection}, 
doi={10.1109/IVMSPW.2016.7528218}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{6624018, 
author={K. Herzig and A. Zeller}, 
booktitle={2013 10th Working Conference on Mining Software Repositories (MSR)}, 
title={The impact of tangled code changes}, 
year={2013}, 
volume={}, 
number={}, 
pages={121-130}, 
abstract={When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found up to 15% of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6% of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.}, 
keywords={configuration management;Java;program debugging;public domain software;open-source JAVA projects;bug fixes;multiple tangled changes;multipredictor approach;source files;bug reports;change organization;version control systems;Partitioning algorithms;Noise;History;Open source software;Accuracy;Manuals;Mining software repositories;tangled code changes;data quality;noise;bias}, 
doi={10.1109/MSR.2013.6624018}, 
ISSN={2160-1860}, 
month={May},}
@BOOK{8106902, 
author={Dimosthenis E. Bolanakis}, 
booktitle={Microcontroller Education: Do it Yourself, Reinvent the Wheel, Code to Learn}, 
title={Microcontroller Education: Do it Yourself, Reinvent the Wheel, Code to Learn}, 
year={2017}, 
volume={}, 
number={}, 
pages={}, 
abstract={Microcontroller education has experienced tremendous change in recent years. This book attempts to keep pace with the most recent technology while holding an opposing attitude to the No Need to Reinvent the Wheel philosophy. The choice strategies are in agreement with the employment of today's flexible and low-cost Do-It-Yourself (DYI) microcontroller hardware, along with an embedded C programming approach able to be adapted by different hardware and software development platforms. Modern embedded C compilers employ built-in features for keeping programs short and manageable and, hence, speeding up the development process. However, those features eliminate the reusability of the source code among diverse systems. The recommended programming approach relies on the motto Code More to Learn Even More, and directs the reader toward a low-level accessibility of the microcontroller device. The examples addressed herein are designed to meet the demands of Electrical & Electronic Engineering discipline, where the microcontroller learning processes definitely bear the major responsibility. The programming strategies are in line with the two virtues of C programming language, that is, the adaptability of the source code and the low-level accessibility of the hardware system.}, 
keywords={microcontroller education;embedded C;ANSI C;microcontrollers;PIC;AVR}, 
doi={}, 
ISSN={}, 
publisher={Morgan & Claypool}, 
isbn={9781681731933}, 
url={https://ieeexplore.ieee.org/document/8106902},}
@INPROCEEDINGS{7997433, 
author={X. Zhang and Y. Liang and J. Fang}, 
booktitle={2017 IEEE International Conference on Communications (ICC)}, 
title={Bayesian learning based multiuser detection for M2M communications with time-varying user activities}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Machine-to-Machine (M2M) communication plays a significant role in supporting Internet of Thing (IoT). This paper is concerned about multiuser detection (MUD) for massive M2M supported by Low-Activity Code Division Multiple Access (LA-CDMA). In previous work, maximum likelihood (ML) and maximum a posterior probability (MAP) detectors have been developed for such system. The ML detector has exponential complexity, while the MAP detector requires perfect knowledge of user activity factor. In practice, the user activity factor may not be known and could change from time to time. To design MUD detectors addressing these problems, in this paper, we formulate multiple measurement vector (MMV) model for uplink LA-CDMA system with time-varying user activities. Since the transmitted signals have block sparse structure, we introduce the pattern coupled spare Bayesian learning (PCSBL) by using the neighbour coherence of each transmitted signal, which effectively solves the user activity factor unknown problem. Furthermore, we embed the generalized approximate message passing (GAMP) to PCSBL and develop a novel algorithm, called generalized approximate message passing pattern coupled sparse Bayesian learning (GAMP-PCSBL). The GAMP-PCSBL does not require activity factor either, and greatly reduces the computational complexity. Simulation results have shown that the proposed algorithms have superior recovery performance than the conventional algorithms.}, 
keywords={Bayes methods;code division multiple access;computational complexity;Internet of Things;learning (artificial intelligence);machine-to-machine communication;maximum likelihood estimation;message passing;multiuser detection;Bayesian learning based multiuser detection;M2M communications;computational complexity;GAMP-PCSBL;generalized approximate message passing pattern coupled sparse Bayesian learning;neighbour coherence;pattern coupled spare Bayesian learning;PCSBL;block sparse structure;uplink LA-CDMA system;MMV model;multiple measurement vector model;user activity factor;exponential complexity;MAP detectors;maximum a posterior probability detectors;maximum likelihood detectors;LA-CDMA;low-activity code division multiple access;MUD;IoT;Internet of Thing;machine-to-machine communication;time-varying user activities;Multiuser detection;Signal processing algorithms;Detectors;Machine-to-machine communications;Uplink;Approximation algorithms;Bayes methods}, 
doi={10.1109/ICC.2017.7997433}, 
ISSN={1938-1883}, 
month={May},}
@INPROCEEDINGS{7509482, 
author={A. Oprisnik and D. Hein and P. Teufl}, 
booktitle={2014 11th International Conference on Security and Cryptography (SECRYPT)}, 
title={Identifying cryptographic functionality in Android applications}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={Mobile devices in corporate IT infrastructures are frequently used to process security-critical data. Over the past few years powerful security features have been added to mobile platforms. However, for legal and organisational reasons it is difficult to pervasively enforce using these features in consumer applications or Bring-Your-Own-Device (BYOD) scenarios. Thus application developers need to integrate custom implementations of security features such as encryption in security-critical applications. Our manual analysis of container applications and password managers has shown that custom implementations of cryptographic functionality often suffer from critical mistakes. During manual analysis, finding the custom cryptographic code was especially time consuming. Therefore, we present the Semdroid framework for simplifying application analysis of Android applications. Here, we use Semdroid to apply machine-learning techniques for detecting non-standard symmetric and asymmetric cryptography implementations. The identified code fragments can be used as starting points for subsequent manual analysis. Thus manual analysis time is greatly reduced. The capabilities of Semdroid have been evaluated on 98 password-safe applications downloaded from Google Play. Our evaluation shows the applicability of Semdroid and its potential to significantly improve future application analysis processes.}, 
keywords={Android (operating system);Bring Your Own Device;cryptography;learning (artificial intelligence);mobile computing;cryptographic functionality identification;Android applications;mobile devices;corporate IT infrastructures;security-critical data processing;consumer applications;bring-your-own-device scenarios;BYOD scenarios;encryption;container applications;password managers;custom implementations;custom cryptographic code;Semdroid framework;machine-learning techniques;nonstandard symmetric cryptography implementation detection;nonstandard asymmetric cryptography implementation detection;code fragment identification;manual analysis time reduction;password-safe applications;Google Play;Androids;Humanoid robots;Encryption;Mobile handsets;Semantics;Mobile Application Security;Machine Learning;Detection of Cryptographic Code;Container Applications;Password Managers;Data Encryption on Mobile Devices;Semantic Pattern Transformation;Correct Deployment of Symmetric and Asymmetric Cryptography}, 
doi={}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6385118, 
author={F. Zhang and F. Khomh and Y. Zou and A. E. Hassan}, 
booktitle={2012 19th Working Conference on Reverse Engineering}, 
title={An Empirical Study on Factors Impacting Bug Fixing Time}, 
year={2012}, 
volume={}, 
number={}, 
pages={225-234}, 
abstract={Fixing bugs is an important activity of the software development process. A typical process of bug fixing consists of the following steps: 1) a user files a bug report, 2) the bug is assigned to a developer, 3) the developer fixes the bug, 4) changed code is reviewed and verified, and 5) the bug is resolved. Many studies have investigated the process of bug fixing. However, to the best of our knowledge, none has explicitly analyzed the interval between bug assignment and the time when bug fixing starts. After a bug assignment, some developers will immediately start fixing the bug while others will start bug fixing after a long period. We are blind on developer's delays when fixing bugs. This paper explores such delays of developers through an empirical study on three open source software systems. We examine factors affecting bug fixing time along three dimensions: bug reports, source code involved in the fix, and code changes that are required to fix the bug. We further compare different factors by descriptive logistic regression models. Our results can help development teams better understand factors behind delays, and then improve bug fixing process.}, 
keywords={program debugging;public domain software;regression analysis;software engineering;bug fixing time;software development process;bug report;code review;code verification;bug resolution;bug assignment;bug fixing;open source software system;source code;code change;descriptive logistic regression model;Delay;Computer bugs;Operating systems;Distributed Bragg reflectors;Logistics;History;bug fixing process;change request;fixing time;empirical software engineering;bug report;mylyn}, 
doi={10.1109/WCRE.2012.32}, 
ISSN={2375-5369}, 
month={Oct},}
@ARTICLE{7637026, 
author={Z. Chen and J. Lu and J. Feng and J. Zhou}, 
journal={IEEE Transactions on Multimedia}, 
title={Nonlinear Discrete Hashing}, 
year={2017}, 
volume={19}, 
number={1}, 
pages={123-135}, 
abstract={In this paper, we propose a nonlinear discrete hashing approach to learn compact binary codes for scalable image search. Instead of seeking a single linear projection in most existing hashing methods, we pursue a multilayer network with nonlinear transformations to capture the local structure of data samples. Unlike most existing hashing methods that adopt an error-prone relaxation to learn the transformations, we directly solve the discrete optimization problem to eliminate the quantization error accumulation. Specifically, to leverage the similarity relationships between data samples and exploit the semantic affinities of manual labels, the binary codes are learned with the objective to: 1) minimize the quantization error between the original data samples and the learned binary codes; 2) preserve the similarity relationships in the learned binary codes; 3) maximize the information content with independent bits; and 4) maximize the accuracy of the predicted labels based on the binary codes. With an alternating optimization, the nonlinear transformation and the discrete quantization are jointly optimized in the hashing learning framework. Experimental results on four datasets including CIFAR10, MNIST, SUN397, and ILSVRC2012 demonstrate that the proposed approach is superior to several state-of-the-art hashing methods.}, 
keywords={file organisation;image retrieval;learning (artificial intelligence);optimisation;nonlinear discrete hashing approach;compact binary code learning;image search;multilayer network;nonlinear transformations;data sample local structure;error-prone relaxation;transformation learning;discrete optimization problem;quantization error accumulation elimination;similarity relationships;manual label semantic affinity;quantization error minimization;information content maximization;alternating optimization;discrete quantization;hashing learning framework;CIFAR10;MNIST;SUN397;ILSVRC2012;Binary codes;Quantization (signal);Optimization;Semantics;Manuals;Multi-layer neural network;Data structures;Binary code;discrete optimization;hashing;multilayer neural network;nonlinear transformation}, 
doi={10.1109/TMM.2016.2620604}, 
ISSN={1520-9210}, 
month={Jan},}
@INPROCEEDINGS{8428782, 
author={S. DiRose and M. Mansouri}, 
booktitle={2018 13th Annual Conference on System of Systems Engineering (SoSE)}, 
title={Comparison and Analysis of Governance Mechanisms Employed by Blockchain-Based Distributed Autonomous Organizations}, 
year={2018}, 
volume={}, 
number={}, 
pages={195-202}, 
abstract={One of the distinguishing features of blockchain-based Distributed Autonomous Organizations(DAO) is lack of a central authority. Changes to blockchain data is achieved through consensus amongst blockchain network participants, rather than through a central node's authoritative decision. Similarly, governance, i.e., changes to features and underlying source code, is achieved through a decentralized mechanism. As adoption of blockchain has increased, the need to evolve and adopt new features has grown. These changes highlight the mechanism by which the network, rather than a central node, makes decisions. One change in particular, proposed increases to the block size to address scalability limitations, has been particularly demonstrative of the governance mechanisms employed by disparate blockchains. For example, two digital currency projects, Bitcoin and Dash, employ significantly different governance mechanisms: the Dash Decentralized Governance By Blockchain (DGBB) process, and the Bitcoin Improvement Proposal (BIP) process, to decide what changes to make to their blockchains. Dash governance was able to decide to alter the block size in a matter of hours, while Bitcoin governance took several years to make the same decision. This paper evaluates the governance mechanisms of blockchain projects using the change in block size as an exemplar. Two prominent governance mechanisms are described, compared, and assessed based upon how effective they came to consensus and made the decision to change to support the disparate needs of stakeholders.}, 
keywords={distributed processing;electronic money;source code (software);Bitcoin Improvement Proposal process;Dash governance;Bitcoin governance;blockchain projects;decentralized mechanism;governance mechanisms;source code;Distributed Autonomous Organizations;blockchain network;Dash Decentralized Governance By Blockchain;DGBB;Bitcoin;Organizations;Scalability;History;Face;blockchain;bitcoin;dash;dao;distributed autonomous organization;governance}, 
doi={10.1109/SYSOSE.2018.8428782}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7962377, 
author={D. Silva and M. T. Valente}, 
booktitle={2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)}, 
title={RefDiff: Detecting Refactorings in Version Histories}, 
year={2017}, 
volume={}, 
number={}, 
pages={269-279}, 
abstract={Refactoring is a well-known technique that is widely adopted by software engineers to improve the design and enable the evolution of a system. Knowing which refactoring operations were applied in a code change is a valuable information to understand software evolution, adapt software components, merge code changes, and other applications. In this paper, we present RefDiff, an automated approach that identifies refactorings performed between two code revisions in a git repository. RefDiff employs a combination of heuristics based on static analysis and code similarity to detect 13 well-known refactoring types. In an evaluation using an oracle of 448 known refactoring operations, distributed across seven Java projects, our approach achieved precision of 100% and recall of 88%. Moreover, our evaluation suggests that RefDiff has superior precision and recall than existing state-of-the-art approaches.}, 
keywords={program diagnostics;software maintenance;RefDiff;refactoring detection;version histories;software engineers;software evolution;code revisions;git repository;static analysis;code similarity;Java projects;Software;Crawlers;History;Tools;Java;Measurement;Syntactics;refactoring;software evolution;software repositories;git}, 
doi={10.1109/MSR.2017.14}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7081840, 
author={M. Brandtner and S. C. Müller and P. Leitner and H. C. Gall}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={SQA-Profiles: Rule-based activity profiles for Continuous Integration environments}, 
year={2015}, 
volume={}, 
number={}, 
pages={301-310}, 
abstract={Continuous Integration (CI) environments cope with the repeated integration of source code changes and provide rapid feedback about the status of a software project. However, as the integration cycles become shorter, the amount of data increases, and the effort to find information in CI environments becomes substantial. In modern CI environments, the selection of measurements (e.g., build status, quality metrics) listed in a dashboard does only change with the intervention of a stakeholder (e.g., a project manager). In this paper, we want to address the shortcoming of static views with so-called Software Quality Assessment (SQA) profiles. SQA-Profiles are defined as rule-sets and enable a dynamic composition of CI dashboards based on stakeholder activities in tools of a CI environment (e.g., version control system). We present a set of SQA-Profiles for project management committee (PMC) members: Bandleader, Integrator, Gatekeeper, and Onlooker. For this, we mined the commit and issue management activities of PMC members from 20 Apache projects. We implemented a framework to evaluate the performance of our rule-based SQA-Profiles in comparison to a machine learning approach. The results showed that project-independent SQA-Profiles can be used to automatically extract the profiles of PMC members with a precision of 0.92 and a recall of 0.78.}, 
keywords={knowledge based systems;software quality;source code (software);SQA profile;software quality assessment;rule-based activity profile;continuous integration environment;CI;source code change;software project;Data mining;Software;Logic gates;Electronic mail;Control systems;Project management;Communities}, 
doi={10.1109/SANER.2015.7081840}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{8118554, 
author={I. Orosz and T. Orosz}, 
booktitle={2017 IEEE 21st International Conference on Intelligent Engineering Systems (INES)}, 
title={Code reusability in cloud based ERP solutions}, 
year={2017}, 
volume={}, 
number={}, 
pages={000193-000198}, 
abstract={Cloud-based technology has created a new software abstraction layer above the implementation layers, and has therefore changed the way in which enterprise resource planning (ERP) systems are developed and implemented over the hardware abstraction layers. The traditional release-by-release update methodology governed by main version change (from pre-alpha to gold release) was changed to a continuous release management. Within the cloud based Software as a Service (SaaS) model, the core business logic is implied above the physical implementation layer. This scenario can predict that the software product can have a longer lifetime, because it is segregated from the always changing physical implementation layer. As the sudden change of technology is present in nowadays IT architecture, the presence of this new abstraction layer seems logical, because the basic business processes are not changing this rapidly. The SaaS type life cycle management means that the heavily technology independent part are not describing the business processes anymore. Previous lifecycle implementations from the assessment phase to the post go-live and support phase dealt the business logic as one entity with its implementation. That means, that the question of code reusability has a different role as in the standard on premise model. This paper introduces a new method of encapsulating and identifying the software parts, which can be later reused in a cloud SaaS environment.}, 
keywords={cloud computing;enterprise resource planning;heavily technology independent part;code reusability;software parts;cloud SaaS environment;ERP solutions;cloud based technology;software abstraction layer;hardware abstraction layers;traditional release-by-release update methodology;continuous release management;core business logic;physical implementation layer;software product;basic business processes;SaaS type life cycle management;enterprise resource planning systems;Unified modeling language;Business;Software as a service;Cloud computing;Computer architecture;Computational modeling;ERP;Code reusability;SaaS;BPR}, 
doi={10.1109/INES.2017.8118554}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7332472, 
author={X. Xia and D. Lo and X. Wang and X. Yang}, 
booktitle={2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Who should review this change?: Putting text and file location analyses together for more accurate recommendations}, 
year={2015}, 
volume={}, 
number={}, 
pages={261-270}, 
abstract={Software code review is a process of developers inspecting new code changes made by others, to evaluate their quality and identify and fix defects, before integrating them to the main branch of a version control system. Modern Code Review (MCR), a lightweight and tool-based variant of conventional code review, is widely adopted in both open source and proprietary software projects. One challenge that impacts MCR is the assignment of appropriate developers to review a code change. Considering that there could be hundreds of potential code reviewers in a software project, picking suitable reviewers is not a straightforward task. A prior study by Thongtanunam et al. showed that the difficulty in selecting suitable reviewers may delay the review process by an average of 12 days. In this paper, to address the challenge of assigning suitable reviewers to changes, we propose a hybrid and incremental approach Tie which utilizes the advantages of both Text mIning and a filE location-based approach. To do this, Tie integrates an incremental text mining model which analyzes the textual contents in a review request, and a similarity model which measures the similarity of changed file paths and reviewed file paths. We perform a large-scale experiment on four open source projects, namely Android, OpenStack, QT, and LibreOffice, containing a total of 42,045 reviews. The experimental results show that on average Tie can achieve top-1, top-5, and top-10 accuracies, and Mean Reciprocal Rank (MRR) of 0.52, 0.79, 0.85, and 0.64 for the four projects, which improves the state-of-the-art approach RevFinder, proposed by Thongtanunam et al., by 61%, 23%, 8%, and 37%, respectively.}, 
keywords={configuration management;data mining;public domain software;text analysis;software code review;modern code review;MCR;open source;software project;filE location-based approach;TIE;text mining model;textual content analysis;Android;OpenStack;QT;LibreOffice;mean reciprocal rank;MRR;version control system;Text mining;Software;Computational modeling;Feature extraction;Analytical models;Accuracy;Control systems;Modern Code Review;Recommendation System;Text Mining;Path Similarity}, 
doi={10.1109/ICSM.2015.7332472}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{8502853, 
author={S. Wang and T. Liu and J. Nam and L. Tan}, 
journal={IEEE Transactions on Software Engineering}, 
title={Deep Semantic Feature Learning for Software Defect Prediction}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Software defect prediction, which predicts defective code regions, can assist developers in finding bugs and prioritizing their testing efforts. Traditional defect prediction features often fail to capture the semantic differences between different programs. This degrades the performance of the prediction models built on these traditional features. Thus, the capability to capture the semantics in programs is required to build accurate prediction models. To bridge the gap between semantics and defect prediction features, we propose leveraging a powerful representation-learning algorithm, deep learning, to learn the semantic representations of programs automatically from source code files and code changes. Specifically, we leverage a deep belief network (DBN) to automatically learn semantic features using token vectors extracted from the programs' abstract syntax trees (AST) (for file-level defect prediction models) and source code changes (for change-level defect prediction models). We examine the effectiveness of our approach on two file-level defect prediction tasks (i.e., file-level within-project defect prediction and file-level cross-project defect prediction) and two change-level defect prediction tasks (i.e., change-level within-project defect prediction and change-level cross-project defect prediction). Our experimental results indicate that the DBN-based semantic features can significantly improve the examined defect prediction tasks. Specifically, the improvements of semantic features against existing traditional features (in F1) range from 2.1 to 41.9 percentage points for file-level within-project defect prediction, from 1.5 to 13.4 percentage points for file-level cross-project defect prediction, from 1.0 to 8.6 percentage points for change-level within-project defect prediction, and from 0.6 to 9.9 percentage points for change-level cross-project defect prediction.}, 
keywords={Semantics;Predictive models;Feature extraction;Task analysis;Computer bugs;Data models;Defect prediction;quality assurance;deep learning;semantic features}, 
doi={10.1109/TSE.2018.2877612}, 
ISSN={0098-5589}, 
month={},}
@INPROCEEDINGS{7885872, 
author={H. Aman and R. Ibrahim}, 
booktitle={2016 International Conference on Information Science and Security (ICISS)}, 
title={XML_DocTracker: Generating Software Requirements Specification (SRS) from XML Schema}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Agile software development methodology is an iterative and incremental method in making interactions more important than process and tools. The method also emphasizes more on developing software rather than making a comprehensive documentation. Therefore, web developers like to adapt agile software development methodology in their web development. The reason is because the methodology delivers web application faster than the traditional software development methodology. As advantages of this method, web application is developed in a short time. Although these make huge benefits, the most important thing in software development life cycle has been ignored. That is, documentation process in capturing requirements and design. Therefore, this paper presents a tool named XML_DocTracker for generating the software requirements specification (SRS) from XML schema as well as addressing the versioning problems during generating the SRS. XML_DocTracker is implemented based on the framework for transformation rules from XML Schema. The framework also addresses the versioning factor using traceability for detecting the document changes. Based on the framework, XML_DocTracker is developed and the tool is able to generate the SRS from the XML schema as well as able to detect document changes in SRS due to traceability factor that is embedded inside the tool. The tool can be used for software community who want to generate the SRS from the source codes if the SRS did not exist for that particular software. This paper contribution is detecting new type of element evolution in SRS when new XML schema version is introduced.}, 
keywords={formal specification;Internet;iterative methods;program diagnostics;reverse engineering;software development management;software prototyping;source code (software);system documentation;Unified Modeling Language;XML;XML_DocTracker;software requirements specification;SRS;XML schema;agile software development;iterative method;incremental method;Web application;software development life cycle;traceability;document change detection;source codes;UML specification;XML;Unified modeling language;Software;Standards;Documentation;Reverse engineering;Agile software development}, 
doi={10.1109/ICISSEC.2016.7885872}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7880507, 
author={M. Mondal and C. K. Roy and K. A. Schneider}, 
booktitle={2017 IEEE 11th International Workshop on Software Clones (IWSC)}, 
title={Does cloned code increase maintenance effort?}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={In-spite of a number of in-depth investigations regarding the impact of clones in the maintenance phase there is no concrete answer to the long lived research question, “Does the presence of code clones increase maintenance effort?”. Existing studies have measured different change related metrics for cloned and non-cloned regions, however, no study calculates the maintenance effort spent for these code regions. In this paper, we perform an in-depth empirical study in order to compare the maintenance efforts required for cloned and non-cloned code. For the purpose of our study we implement a prototype tool which is capable of estimating the effort spent by a developer for changing a particular method. It can also predict effort that might need to be spent for making some changes to a particular method. Our estimation and prediction involve automatic extraction and analysis of the entire evolution history of a candidate software system. We applied our tool on hundreds of revisions of six open source subject systems written in three different programming languages for calculating the efforts spent for cloned and non-cloned code. According to our experimental results: (i) cloned code requires more effort in the maintenance phase than non-cloned code, and (ii) Type 2 and Type 3 clones require more effort compared to the efforts required by Type 1 clones. According to our findings, we should prioritize Type 2 and Type 3 clones when making clone management decisions.}, 
keywords={public domain software;software maintenance;software metrics;cloned code;maintenance phase;change related metrics;code regions;evolution history;candidate software system;open source subject systems;programming languages;noncloned code;clone management decisions;Cloning;Maintenance engineering;Estimation;Software systems;Computer bugs;Software maintenance;Mathematical model}, 
doi={10.1109/IWSC.2017.7880507}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7000096, 
author={T. Wetzlmaier and C. Klammer and R. Ramler}, 
booktitle={2014 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement}, 
title={Extracting Dependencies from Software Changes: An Industry Experience Report}, 
year={2014}, 
volume={}, 
number={}, 
pages={163-168}, 
abstract={Retrieving and analyzing information from software repositories and detecting dependencies are important tasks supporting software evolution. Dependency information is used for change impact analysis, defect prediction as well as cohesion and coupling measurement. In this paper we report our experience from extracting dependency information from the change history of a commercial software system. We analyzed the software system's evolution of about six years, from the start of development to the transition to product releases and maintenance. Analyzing the co-evolution of software artifacts allows detecting logical dependencies between system parts implemented with heterogeneous technologies as well as between different types of development artifacts such as source code, data models or documentation. However, the quality of the extracted dependencies relies on established development practices and conformance to a defined change process. In this paper we indicate resulting limitations and recommend further processing and filtering steps to prepare the dependency data for subsequent analysis and measurement activities.}, 
keywords={information analysis;information retrieval;software maintenance;dependency extraction;software change;information retrieval;information analysis;software repositories;dependency detection;change impact analysis;defect prediction;cohesion measurement;coupling measurement;commercial software system;software artifact coevolution;processing step;filtering step;Software systems;History;Servers;Couplings;Java;Data mining;dependency analysis;logical coupling;change history;mining software repositories}, 
doi={10.1109/IWSM.Mensura.2014.12}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6404680, 
author={E. Sakurai and K. Yamanishi}, 
booktitle={2012 IEEE Information Theory Workshop}, 
title={Comparison of dynamic model selection with infinite HMM for statistical model change detection}, 
year={2012}, 
volume={}, 
number={}, 
pages={302-306}, 
abstract={In this study, we address the issue of tracking changes in statistical models under the assumption that the statistical models used for generating data may change over time. This issue is of great importance for learning from non-stationary data. One of the promising approaches for resolving this issue is the use of the dynamic model selection (DMS) method, in which a model sequence is estimated on the basis of the minimum description length (MDL) principle. Another approach is the use of the infinite hidden Markov model (HMM), which is a non-parametric learning method for the case with an infinite number of states. In this study, we propose a few new variants of DMS and propose efficient algorithms to minimize the total code-length by using the sequential normalized maximum likelihood. We compare these algorithms with infinite HMM to investigate their statistical model change detection performance, and we empirically demonstrate that one of our variants of DMS significantly outperforms infinite HMM in terms of change-point detection accuracy.}, 
keywords={hidden Markov models;maximum likelihood estimation;dynamic model selection;infinite HMM;hidden Markov model;statistical model change detection;DMS method;minimum description length principle;MDL principle;nonparametric learning method;code-length;sequential normalized maximum likelihood;model change detection performance;change-point detection accuracy;Hidden Markov models;Maximum likelihood estimation;Data models;Encoding;Maximum likelihood decoding;Maximum likelihood detection;Accuracy}, 
doi={10.1109/ITW.2012.6404680}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6493641, 
author={H. Esmaeilzadeh and A. Sampson and L. Ceze and D. Burger}, 
booktitle={2012 45th Annual IEEE/ACM International Symposium on Microarchitecture}, 
title={Neural Acceleration for General-Purpose Approximate Programs}, 
year={2012}, 
volume={}, 
number={}, 
pages={449-460}, 
abstract={This paper describes a learning-based approach to the acceleration of approximate programs. We describe the \emph{Parrot transformation}, a program transformation that selects and trains a neural network to mimic a region of imperative code. After the learning phase, the compiler replaces the original code with an invocation of a low-power accelerator called a \emph{neural processing unit} (NPU). The NPU is tightly coupled to the processor pipeline to accelerate small code regions. Since neural networks produce inherently approximate results, we define a programming model that allows programmers to identify approximable code regions -- code that can produce imprecise but acceptable results. Offloading approximable code regions to NPUs is faster and more energy efficient than executing the original code. For a set of diverse applications, NPU acceleration provides whole-application speedup of 2.3× and energy savings of 3.0× on average with quality loss of at most 9.6%.}, 
keywords={learning (artificial intelligence);low-power electronics;neural nets;pipeline processing;power aware computing;program compilers;program interpreters;neural acceleration;general-purpose approximate programs;learning-based approach;approximate program acceleration;Parrot transformation;program transformation;neural network;compiler;low-power accelerator;neural processing unit;processor pipeline;programming model;approximable code regions;offloading approximable code;NPU acceleration;energy savings;quality loss;Approximate Computing;Neural Networks;Accelerator;Neural Processing Unit;NPU}, 
doi={10.1109/MICRO.2012.48}, 
ISSN={1072-4451}, 
month={Dec},}
@INPROCEEDINGS{8530724, 
author={M. A. Islam and M. M. Islam and M. Mondal and B. Roy and C. K. Roy and K. A. Schneider}, 
booktitle={2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
title={[Research Paper] Detecting Evolutionary Coupling Using Transitive Association Rules}, 
year={2018}, 
volume={}, 
number={}, 
pages={113-122}, 
abstract={If two or more program entities (such as files, classes, methods) co-change (i.e., change together) frequently during software evolution, then it is likely that these two entities are coupled (i.e., the entities are related). Such a coupling is termed as evolutionary coupling in the literature. The concept of traditional evolutionary coupling restricts us to assume coupling among only those entities that changed together in the past. The entities that did not co-change in the past might also have coupling. However, such couplings can not be retrieved using the current concept of detecting evolutionary coupling in the literature. In this paper, we investigate whether we can detect such couplings by applying transitive rules on the evolutionary couplings detected using the traditional mechanism. We call these couplings that we detect using our proposed mechanism as transitive evolutionary couplings. According to our research on thousands of revisions of four subject systems, transitive evolutionary couplings combined with the traditional ones provide us with 13.96% higher recall and 5.56% higher precision in detecting future co-change candidates when compared with a state-of-the-art technique.}, 
keywords={couplings;data mining;evolutionary computation;program entities;transitive evolutionary couplings;transitive association rules;software evolution;Couplings;Computer science;History;Software systems;Frequency measurement;Software engineering;Evolutionary Coupling;Transitive Association Rule;Support;Confidence}, 
doi={10.1109/SCAM.2018.00020}, 
ISSN={2470-6892}, 
month={Sep.},}
@INPROCEEDINGS{6983819, 
author={L. Mariani and D. Micucci and F. Pastore}, 
booktitle={2014 IEEE International Symposium on Software Reliability Engineering Workshops}, 
title={Early Conflict Detection with Mined Models}, 
year={2014}, 
volume={}, 
number={}, 
pages={126-127}, 
abstract={The key idea introduced in this paper consists of running a multi-branch server-side dynamic analysis at every commit operation. The analysis will execute the test cases available in the source code management (SCM) system to trace the behavior of the application and automatically derive models that capture how the program behaves according to multiple dimensions. For instance, the functional behavior of the program can be represented with method pre- and post-conditions, API usage protocols, and precedence rules among method invocations. The temporal behavior of a program can be represented with models that capture aspects, such as deadlines, periodicity, and constraints on the timing of the tasks. These models are used on the server side to run automated conflict detection. Models are derived for every version in every branch, and automatically compared every time a change is introduced. Comparing models allows identifying behavioral conflicts regardless the presence of textual conflicts, which do not need to be resolved to run the analysis. We call this analysis Behavioral Driven Continuous Integration (BDCI).}, 
keywords={application program interfaces;data mining;program diagnostics;model mining;multibranch server-side dynamic analysis;source code management system;SCM system;application behavior tracing;functional behavior;API usage protocols;precedence rules;method invocations;program temporal behavior;automated conflict detection;behavioral conflict identification;textual conflicts;behavioral driven continuous integration;BDCI;Sockets;Analytical models;Merging;Software;Servers;Timing;conflict detection;configuration management;version control}, 
doi={10.1109/ISSREW.2014.33}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6895415, 
author={H. Cai and R. Santelices and T. Xu}, 
booktitle={2014 Eighth International Conference on Software Security and Reliability (SERE)}, 
title={Estimating the Accuracy of Dynamic Change-Impact Analysis Using Sensitivity Analysis}, 
year={2014}, 
volume={}, 
number={}, 
pages={48-57}, 
abstract={The reliability and security of software are affected by its constant changes. For that reason, developers use change-impact analysis early to identify the potential consequences of changing a program location. Dynamic impact analysis, in particular, identifies potential impacts on concrete, typical executions. However, the accuracy (precision and recall) of dynamic impact analyses for predicting the actual impacts of changes has not been studied. In this paper, we present a novel approach based on sensitivity analysis and execution differencing to estimate, for the first time, the accuracy of dynamic impact analyses. Unlike approaches that only use software repositories, which might not be available or might contain insufficient changes, our approach makes changes to every part of the software to identify actually impacted code and compare it with the predictions of dynamic impact analysis. Using this approach in addition to changes made by other researchers on multiple Java subjects, we estimated the accuracy of the best method-level dynamic impact analysis in the literature. Our results suggest that dynamic impact analysis can be surprisingly inaccurate with an average precision of 47-52% and recall of 56-87%. This study offers insights to developers into the effectiveness of existing dynamic impact analyses and motivates the future development of more accurate analyses.}, 
keywords={Java;program diagnostics;security of data;sensitivity analysis;software reliability;dynamic change-impact analysis;sensitivity analysis;software reliability;software security;program location;concrete typical executions;execution differencing;software repositories;impacted code;Java subjects;method-level dynamic impact analysis;Accuracy;Runtime;Software;Java;Performance analysis;History;Semantics;software evolution;change-impact analysis;dynamic analysis;empirical studies;sensitivity analysis;execution differencing}, 
doi={10.1109/SERE.2014.18}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8462891, 
author={P. Sermanet and C. Lynch and Y. Chebotar and J. Hsu and E. Jang and S. Schaal and S. Levine and G. Brain}, 
booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Time-Contrastive Networks: Self-Supervised Learning from Video}, 
year={2018}, 
volume={}, 
number={}, 
pages={1134-1141}, 
abstract={We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a triplet loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at sermanet.github.io/imitate.}, 
keywords={image representation;learning (artificial intelligence);pose estimation;robot programming;robot vision;video signal processing;time-contrastive networks;robotic behaviors;robotic imitation settings;human poses;viewpoint-invariant representation;end-effectors;reinforcement learning algorithm;self-supervised learning;robotic systems;Robots;Task analysis;Visualization;Learning (artificial intelligence);Training;Liquids;Lighting}, 
doi={10.1109/ICRA.2018.8462891}, 
ISSN={2577-087X}, 
month={May},}
@INPROCEEDINGS{6747191, 
author={H. Osman and M. Lungu and O. Nierstrasz}, 
booktitle={2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)}, 
title={Mining frequent bug-fix code changes}, 
year={2014}, 
volume={}, 
number={}, 
pages={343-347}, 
abstract={Detecting bugs as early as possible plays an important role in ensuring software quality before shipping. We argue that mining previous bug fixes can produce good knowledge about why bugs happen and how they are fixed. In this paper, we mine the change history of 717 open source projects to extract bug-fix patterns. We also manually inspect many of the bugs we found to get insights into the contexts and reasons behind those bugs. For instance, we found out that missing null checks and missing initializations are very recurrent and we believe that they can be automatically detected and fixed.}, 
keywords={data mining;program debugging;public domain software;software quality;mining frequent bug-fix code changes;bugs detection;software quality;open source projects;bug-fix pattern;null checks;Computer bugs;Software;History;Java;Data mining;Cloning;Software engineering}, 
doi={10.1109/CSMR-WCRE.2014.6747191}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{8609602, 
author={M. Leithner and D. E. Simos}, 
booktitle={2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI)}, 
title={DOMdiff: Identification and Classification of Inter-DOM Modifications}, 
year={2018}, 
volume={}, 
number={}, 
pages={262-269}, 
abstract={Current web crawlers, document databases and change monitoring systems for web sites are commonly limited to static content and analysis of code as retrieved from the server, an approach that is not suitable for modern dynamic web applications. The canonical representation of the contents of a single web page at any given time is an instance of the Document Object Model (DOM), a tree structure that forms the basis for rendering and processing of the page within the browser and is updated when content is modified. This work presents DOMdiff, an algorithm to identify changes between two different DOM instances, as well as a method to classify these changes in terms of a ranking that represents the distance between the two trees. We compare a manually derived classifier with the results of PRank, a ranked version of the Perceptron algorithm, a simple machine learning approach that generates a multiclass classifier based on formulae in a constrained predicate logic, and the established statistical classifier C5.0. Our results indicate that DOMdiff is suitable to large-scale change identification and that entropy-based statistical classifiers are more accurate than our simple predicate-based classifier for the problem at hand, but require a larger decision tree. We additionally identify a shortcoming of PRank when handling features with low information gain/high entropy.}, 
keywords={XML;Machine learning algorithms;Decision trees;Urban areas;Monitoring;Servers;Web pages;web crawling;change monitoring;document object model;machine learning}, 
doi={10.1109/WI.2018.00-81}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7780226, 
author={Z. Chen and W. Ma and W. Lin and L. Chen and B. Xu}, 
booktitle={2016 Third International Conference on Trustworthy Systems and their Applications (TSA)}, 
title={Tracking Down Dynamic Feature Code Changes against Python Software Evolution}, 
year={2016}, 
volume={}, 
number={}, 
pages={54-63}, 
abstract={Python, a typical dynamic programming language, is increasingly used in many application domains. Dynamic features in Python allow developers to change the code at runtime. Some dynamic features such as dynamic type checking play an active part in maintenance activities, thus dynamic feature code is often changed to cater to software evolution. The aim of this paper is exploring and validating the characteristics of feature changes in Python. We collected change occurrences in 85 open-source projects and discovered the relationship between feature changes and bug-fix activities. Furthermore, we went into 358 change occurrences to explore the causes and behaviors of feature changes. The results show that: (1) dynamic features are increasingly used and the code is changeable; (2) most dynamic features may behave that feature code is more likely to be changed in bug-fix activities than non-bugfix activities; (3) dynamic feature code plays both positive and negative roles in maintenance activities. Our results provide useful guidance and insights for improving automatic program repair and refactoring tools.}, 
keywords={dynamic programming;feature extraction;program debugging;programming languages;public domain software;software maintenance;software reliability;Python software evolution;dynamic feature code changes;dynamic programming language;dynamic features;software maintenance activities;software evolution;open-source projects;bug-fix activities;non-bugfix activities;automatic program repair tools;refactoring tools;Feature extraction;Computer bugs;Maintenance engineering;History;Software;Dynamic programming;Runtime;Python;code changes;dynamic features;software evolution}, 
doi={10.1109/TSA.2016.19}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{7518675, 
author={X. Xia and D. Lo and X. Wang and X. Yang}, 
journal={IEEE Transactions on Reliability}, 
title={Collective Personalized Change Classification With Multiobjective Search}, 
year={2016}, 
volume={65}, 
number={4}, 
pages={1810-1829}, 
abstract={Many change classification techniques have been proposed to identify defect-prone changes. These techniques consider all developers' historical change data to build a global prediction model. In practice, since developers have their own coding preferences and behavioral patterns, which causes different defect patterns, a separate change classification model for each developer can help to improve performance. Jiang, Tan, and Kim refer to this problem as personalized change classification, and they propose PCC+ to solve this problem. A software project has a number of developers; for a developer, building a prediction model not only based on his/her change data, but also on other relevant developers' change data can further improve the performance of change classification. In this paper, we propose a more accurate technique named collective personalized change classification (CPCC), which leverages a multiobjective genetic algorithm. For a project, CPCC first builds a personalized prediction model for each developer based on his/her historical data. Next, for each developer, CPCC combines these models by assigning different weights to these models with the purpose of maximizing two objective functions (i.e., F1-scores and cost effectiveness). To further improve the prediction accuracy, we propose CPCC+ by combining CPCC with PCC proposed by Jiang, Tan, and Kim To evaluate the benefits of CPCC+ and CPCC, we perform experiments on six large software projects from different communities: Eclipse JDT, Jackrabbit, Linux kernel, Lucene, PostgreSQL, and Xorg. The experiment results show that CPCC+ can discover up to 245 more bugs than PCC+ (468 versus 223 for PostgreSQL) if developers inspect the top 20% lines of code that are predicted buggy. In addition, CPCC+ can achieve F1-scores of 0.60-0.75, which are statistically significantly higher than those of PCC+ on all of the six projects.}, 
keywords={genetic algorithms;program debugging;collective personalized change classification technique;multiobjective search;PCC+;software project;CPCC;multiobjective genetic algorithm;personalized prediction model;Eclipse JDT;Jackrabbit;Linux kernel;Lucene;PostgreSQL;Xorg;Data models;Predictive models;Software;Computer bugs;Genetic algorithms;Feature extraction;Buildings;Cost effectiveness;developer;machine learning;multiobjective genetic algorithm;personalized change classification (PCC)}, 
doi={10.1109/TR.2016.2588139}, 
ISSN={0018-9529}, 
month={Dec},}
@INPROCEEDINGS{7811381, 
author={Y. Zhang and F. Xu and E. Frise and S. Wu and B. Yu and W. Xu}, 
booktitle={2016 IEEE/ACM 2nd International Workshop on Big Data Software Engineering (BIGDSE)}, 
title={DataLab: A Version Data Management and Analytics System}, 
year={2016}, 
volume={}, 
number={}, 
pages={12-18}, 
abstract={One challenge in big data analytics is the lack of tools to manage the complex interactions among code, data and parameters, especially in the common situation where all these factors can change a lot. We present our preliminary experience with DataLab, a system we build to manage the big data workflow. DataLab improves big data analytical workflow in several novel ways. 1) DataLab manages the revision of both code and data in a coherent system, and includes a distributed code execution engine to run users' code; 2) DataLab keeps track of all the data analytics results in a data work flow graph, and is able to compare the code / results between any two versions, making it easier for users to intuitively see the results of their code change; 3) DataLab provides an efficient data management system to separate data from their metadata, allowing efficient preprocessing filters; and 4) DataLab provides a common API so people can build different applications on top of it. We also present our experience of applying a DataLab prototype in a real bioinformatics application.}, 
keywords={application program interfaces;Big Data;distributed processing;graph theory;workflow management software;DataLab;data management;analytics system;big data analytics;big data workflow;distributed code execution engine;data workflow graph;API;Metadata;Distributed databases;Engines;Data science;Big data;Bioinformatics;software engineering;data management;data analytics;version control}, 
doi={10.1109/BIGDSE.2016.011}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6262548, 
author={M. Cretu and C. Apostol}, 
booktitle={2012 9th International Conference on Communications (COMM)}, 
title={A modified version of Rijndael algorithm implemented to analyze the cyphertexts correlation for switched S-Boxes}, 
year={2012}, 
volume={}, 
number={}, 
pages={331-334}, 
abstract={There are more than eleven years since Rijndael algorithm was declared the winner of the NIST contest for the new AES election. All this time the original algorithm was analyzed and attacked by cryptanalysts and hackers in order to find its vulnerabilities. The modified version of Rijndael we analyze in this paper randomly changes the accessing order of S-Boxes implemented in the source code of the original algorithm, due to affine transformation and inverse matrix properties. The goal is to obtain two different cyphertexts, keeping the plaintext and the secret key. For this to be possible, a PRNG designed by Gorge Marsaglia was implemented in the software solution.}, 
keywords={affine transforms;cryptography;matrix algebra;Rijndael algorithm;cyphertexts correlation;switched S-boxes;NIST contest;AES election;cryptanalysts;hackers;source code;affine transformation;inverse matrix properties;plaintext;secret key;software solution;Algorithm design and analysis;Correlation;Polynomials;Encryption;Software algorithms;Magnetic resonance imaging;Rijndael;AES;PRNG;correlation;histogram}, 
doi={10.1109/ICComm.2012.6262548}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7335411, 
author={D. Steidl and F. Deissenboeck}, 
booktitle={2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
title={How do Java methods grow?}, 
year={2015}, 
volume={}, 
number={}, 
pages={151-160}, 
abstract={Overly long methods hamper the maintainability of software - they are hard to understand and to change, but also difficult to test, reuse, and profile. While technically there are many opportunities to refactor long methods, little is known about their origin and their evolution. It is unclear how much effort should be spent to refactor them and when this effort is spent best. To obtain a maintenance strategy, we need a better understanding of how software systems and their methods evolve. This paper presents an empirical case study on method growth in Java with nine open source and one industry system. We show that most methods do not increase their length significantly; in fact, about half of them remain unchanged after the initial commit. Instead, software systems grow by adding new methods rather than by modifying existing methods.}, 
keywords={Java;software maintenance;Java method;software maintainability;maintenance strategy;History;Tracking;Software systems;Java;Control systems;Cloning}, 
doi={10.1109/SCAM.2015.7335411}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7346154, 
author={R. Braud and A. Pitti and P. Gaussier}, 
booktitle={2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)}, 
title={Comparison of absolute and relative strategies to encode sensorimotor transformations in tool-use}, 
year={2015}, 
volume={}, 
number={}, 
pages={267-268}, 
abstract={We explore different strategies to overcome the problem of sensorimotor transformation that babies face during development, especially in the case of tool-use. From a developmental perspective, we investigate a model based on absolute coordinate frames of reference, and another one based on relative coordinate frames of reference. In a situation of sensorimotor learning and of adaptation to tool-use, we perform a computer simulation of a 4 degrees of freedom robot. We show that the relative coordinate strategy is the most rapid and robust to re-adapt the neural code.}, 
keywords={learning (artificial intelligence);robots;sensors;absolute strategies;encode sensorimotor transformations;absolute coordinate frames;relative coordinate frames;sensorimotor learning;computer simulation;4 degrees of freedom robot;relative coordinate strategy;Robot sensing systems;Adaptation models;Least squares approximations;Robot kinematics;Brain modeling;Surface acoustic waves}, 
doi={10.1109/DEVLRN.2015.7346154}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7126259, 
author={T. Wang}, 
booktitle={2015 International Conference on Learning and Teaching in Computing and Engineering}, 
title={Case-Based Instruction of "How Computer Works" Courses for High School Students}, 
year={2015}, 
volume={}, 
number={}, 
pages={207-208}, 
abstract={The concept of binary, which is the basis of how computer works, is often taught in Taiwan's high school computer courses as the introductory unit of the class "Introduction to Computers". In the past, teachers used to lecture through binary concepts such as the decimal binary conversion, ASCII code transformation, and several traditional courses. However, most students did not know why they needed to learn binary concepts, and seemed less interested in learning. To enhance the effectiveness of learning, we incorporated some examples corresponding to each binary concept to link students' life experiences to the concepts in order to make learning meaningful. In this paper, we will share our teaching materials for the binary concept from the course "Introduction to Computers". The lesson starts from how computer stores values, text or image data to introduce basic concepts of binary data interpretation and hex conversion. Afterwards, a combination of simple logic gates adder instance is introduced to guide students to learn how to perform a binary compute operation. Finally, software CPU simulator is covered to allow students to learn and visualize how computer uses binary data and basic operations to perform some specific compute functions.}, 
keywords={computer aided instruction;computer science education;educational courses;educational institutions;teaching;case-based instruction;how-computer works course;Taiwan high-school computer courses;binary concepts;decimal binary conversion;ASCII code transformation;student life experiences;teaching materials;introduction-to-computers course;binary data interpretation;hex conversion;logic gate adder;binary compute operation;software CPU simulator;compute functions;Computers;Education;Games;Central Processing Unit;Psychology;Logic gates;Concrete;How computer works;binary concept teaching;case-based instruction}, 
doi={10.1109/LaTiCE.2015.56}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{7985680, 
author={R. Rolim and G. Soares and L. D'Antoni and O. Polozov and S. Gulwani and R. Gheyi and R. Suzuki and B. Hartmann}, 
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)}, 
title={Learning Syntactic Program Transformations from Examples}, 
year={2017}, 
volume={}, 
number={}, 
pages={404-415}, 
abstract={Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.}, 
keywords={automatic programming;program processors;syntactic program transformations learning;REFAZER;programming-by-example methodology;domain-specific language;domain-specific deductive algorithms;DSL;code edits;C# open-source projects;DSL;Programming profession;Tools;C# languages;Pattern matching;Open source software;Program transformation;program synthesis;tutoring systems;refactoring}, 
doi={10.1109/ICSE.2017.44}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{7774538, 
author={S. Just and K. Herzig and J. Czerwonka and B. Murphy}, 
booktitle={2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)}, 
title={Switching to Git: The Good, the Bad, and the Ugly}, 
year={2016}, 
volume={}, 
number={}, 
pages={400-411}, 
abstract={Since its introduction 10 years ago, GIT has taken the world of version control systems (VCS) by storm. Its success is partly due to creating opportunities for new usage patterns that empower developers to work more efficiently. However, the resulting change in both user behavior and the way GIT stores changes impacts data mining and data analytics procedures [6], [13]. While some of these unique characteristics can be managed by adjusting mining and analytical techniques, others can lead to severe data loss and the inability to audit code changes, e.g. knowing the full history of changes of code related to security and privacy functionality. Thus, switching to GIT comes with challenges to established development process analytics. This paper is based on our experience in attempting to provide continuous process analysis for Microsoft product teams who switching to GIT as their primary VCS. We illustrate how GIT's concepts and usage patterns create a need for changing well-established data analytic processes. The goal of this paper is to raise awareness how certain GIT operations may damage or even destroy information about historical code changes necessary for continuous data development process analytics. To that end, we provide a list of common GIT usage patterns with a description of how these operations impact data mining applications. Finally, we provide examples of how one may counteract the effects of such destructive operations in the future. We further provide a new algorithm to detect integration paths that is specific to distributed version control systems like GIT, which allows us to reconstruct the information that is crucial to most development process analytics.}, 
keywords={data mining;GIT;version control systems;VCS;user behavior;data mining;data analytics procedures;audit code;data loss;privacy functionality;security functionality;data development process analytics;Data mining;Switches;History;Software;Data analysis;Process control;mining software repositories;process analysis;version control systems}, 
doi={10.1109/ISSRE.2016.38}, 
ISSN={2332-6549}, 
month={Oct},}
@INPROCEEDINGS{7081818, 
author={K. Kobori and M. Matsushita and K. Inoue}, 
booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Evolution analysis for Accessibility Excessiveness in Java}, 
year={2015}, 
volume={}, 
number={}, 
pages={83-90}, 
abstract={In Java programs, access modifiers are used to control the accessibility of fields and methods from other objects. Choosing appropriate access modifiers is one of the key factors to improve program quality and to reduce potential vulnerability. In our previous work, we presented a static analysis method named Accessibility Excessiveness (AE) detection for each field and method in Java program. We have also developed an AE analysis tool named ModiChecker that analyzes each field and method of the input Java programs, and reports their excessiveness. In this paper, we have applied ModiChecker to several OSS repositories to investigate the evolution of AE over versions, and identified transition of AE status and the difference in the amount of AE change between major version releases and minor ones. Also we propose when to evaluate source code with AE analysis.}, 
keywords={Java;program diagnostics;program verification;source code (software);accessibility excessiveness detection;Java programs;access modifiers;accessibility control;program quality;program vulnerability;static analysis method;AE detection;AE analysis tool;ModiChecker;OSS repositories;source code;Java;Measurement;Abstracts;Programming;Maintenance engineering;Libraries;Databases}, 
doi={10.1109/SANER.2015.7081818}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{7194592, 
author={M. Tufano and F. Palomba and G. Bavota and R. Oliveto and M. Di Penta and A. De Lucia and D. Poshyvanyk}, 
booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering}, 
title={When and Why Your Code Starts to Smell Bad}, 
year={2015}, 
volume={1}, 
number={}, 
pages={403-414}, 
abstract={In past and recent years, the issues related to managing technical debt received significant attention by researchers from both industry and academia. There are several factors that contribute to technical debt. One of these is represented by code bad smells, i.e., Symptoms of poor design and implementation choices. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced. To fill this gap, we conducted a large empirical study over the change history of 200 open source projects from different software ecosystems and investigated when bad smells are introduced by developers, and the circumstances and reasons behind their introduction. Our study required the development of a strategy to identify smell-introducing commits, the mining of over 0.5M commits, and the manual analysis of 9,164 of them (i.e., Those identified as smell-introducing). Our findings mostly contradict common wisdom stating that smells are being introduced during evolutionary tasks. In the light of our results, we also call for the need to develop a new generation of recommendation systems aimed at properly planning smell refactoring activities.}, 
keywords={recommender systems;software maintenance;software quality;technical debt management;code quality;software ecosystems;smell-introducing commits identification;recommendation systems;smell refactoring activities;Measurement;Software;Ecosystems;History;Androids;Humanoid robots;Maintenance engineering;bad code smells;mining software repositories;empirical study}, 
doi={10.1109/ICSE.2015.59}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{8530022, 
author={R. Yue and Z. Gao and N. Meng and Y. Xiong and X. Wang and J. D. Morgenthaler}, 
booktitle={2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Automatic Clone Recommendation for Refactoring Based on the Present and the Past}, 
year={2018}, 
volume={}, 
number={}, 
pages={115-126}, 
abstract={When many clones are detected in software programs, not all clones are equally important to developers. To help developers refactor code and improve software quality, various tools were built to recommend clone-removal refactorings based on the past and the present information, such as the cohesion degree of individual clones or the co-evolution relations of clone peers. The existence of these tools inspired us to build an approach that considers as many factors as possible to more accurately recommend clones. This paper introduces CREC, a learning-based approach that recommends clones by extracting features from the current status and past history of software projects. Given a set of software repositories, CREC first automatically extracts the clone groups historically refactored (R-clones) and those not refactored (NR-clones) to construct the training set. CREC extracts 34 features to characterize the content and evolution behaviors of individual clones, as well as the spatial, syntactical, and co-change relations of clone peers. With these features, CREC trains a classifier that recommends clones for refactoring. We designed the largest feature set thus far for clone recommendation, and performed an evaluation on six large projects. The results show that our approach suggested refactorings with 83% and 76% F-scores in the within-project and cross-project settings. CREC significantly outperforms a state-of-the-art similar approach on our data set, with the latter one achieving 70% and 50% F-scores. We also compared the effectiveness of different factors and different learning algorithms.}, 
keywords={feature extraction;learning (artificial intelligence);pattern classification;software quality;NR-clones;CREC;automatic clone recommendation;R-clones;not refactored;software programs;software projects;feature extraction;software repositories;learning algorithms;classifier;Cloning;Feature extraction;History;Software;Tools;Training;Machine learning;clone refactoring;clone recommendation;machine learning}, 
doi={10.1109/ICSME.2018.00021}, 
ISSN={2576-3148}, 
month={Sep.},}
@INPROCEEDINGS{6405296, 
author={S. Raemaekers and A. van Deursen and J. Visser}, 
booktitle={2012 28th IEEE International Conference on Software Maintenance (ICSM)}, 
title={Measuring software library stability through historical version analysis}, 
year={2012}, 
volume={}, 
number={}, 
pages={378-387}, 
abstract={Backward compatibility is a major concern for any library developer. In this paper, we evaluate how stable a set of frequently used third-party libraries is in terms of method removals, implementation change, the ratio of change in old methods to change in new ones and the percentage of new methods in each snapshot. We provide a motivating example of a commercial company which demonstrates several issues associated with the usage of third-party libraries. To obtain dependencies from software systems we developed a framework which extracts dependencies from Maven build files and which analyzes system and library code. We propose four metrics which provide different insights in the implementation and interface stability of a library. The usage frequency of library methods is utilized as a weight in the final metric and is obtained from a dataset of more than 2300 snapshots of 140 industrial Java systems. We finally describe three scenarios and an example of the application of our metrics.}, 
keywords={application program interfaces;Java;software libraries;software metrics;software reusability;software library stability measurement;historical version analysis;backward compatibility;third-party libraries;commercial company;Maven build files;library code;software metrics;system code;interface stability;library method usage frequency;industrial Java systems;API usage;software reusability;Libraries;Measurement;Java;Stability analysis;Security;Software;Conferences;Third-party Libraries;API Usage;API Stability;Software Reuse}, 
doi={10.1109/ICSM.2012.6405296}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{6580248, 
author={D. U. Campos-Delgado and J. M. Luna-Rivera}, 
booktitle={2013 American Control Conference}, 
title={Power control in the uplink of a wireless multi-carrier CDMA system}, 
year={2013}, 
volume={}, 
number={}, 
pages={2733-2738}, 
abstract={This paper addresses the power allocation problem in the uplink of wireless multi-carrier code-division multiple-access (MC-CDMA) in order to guarantee QoS requirements. QoS is evaluated with respect to the signal to interference-noise ratio (SINR), estimated after the detection process at the base station (BS). First, departing of the received signal model in a MC-CDMA system, the SINR is computed by considering the application of linear multiuser detectors and a common transmission power through all subcarriers. By assuming that the measured SINR must follow an objective SINR value, an open-loop solution is introduced to the power allocation problem. Next, an iterative version is derived that relies on an integral correction of the required transmission power. By applying a loop transformation and the small-gain theorem, general distributed controllers can be proposed, where specific stability and performance conditions are introduced. A simulation evaluation is presented at different load levels in the MC-CDMA, and time-varying channel gains. In all the studied cases, the resulting power allocation system was able to allocate transmission power to achieve the objective SINR.}, 
keywords={code division multiple access;integral equations;iterative methods;multiuser detection;power control;quality of service;radiofrequency interference;telecommunication control;time-varying channel gains;general distributed controllers;small-gain theorem;loop transformation;integral correction;iterative version;open-loop solution;SINR value;common transmission power;linear multiuser detectors;received signal model;base station;signal-to-interference-noise ratio;QoS requirements;wireless multicarrier code-division multiple-access;transmission power allocation problem;wireless multicarrier CDMA system;power control;Interference;Signal to noise ratio;Uplink;Multicarrier code division multiple access;Resource management;Quality of service;Detectors;Power allocation;quality of service;closed-loop control;MC-CDMA}, 
doi={10.1109/ACC.2013.6580248}, 
ISSN={0743-1619}, 
month={June},}
@INPROCEEDINGS{6850772, 
author={Amit Seal Ami and M. S. Islam}, 
booktitle={2014 International Conference on Informatics, Electronics Vision (ICIEV)}, 
title={An efficient approach for providing rationale of method change for object oriented programming}, 
year={2014}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Software engineering requires modification of code during development and maintenance phase. During modification, a difficult task is to understand rationale of code changed by others. Present Integrated Development Environments (IDEs) attempt to help this by providing features integrated with different types of repositories. However, these features still consume developers' time as he has to switch from editor to another window for this purpose. Moreover, these features focus on elements available in present version of code, thus increasing the difficulty of finding rationale of an element removed or modified earlier. Leveraging different sources for providing information through code completion menus has been shown to be valuable, even when compared to standalone counterparts offering similar functionalities in literature. Literature also shows that it is one of the most used features for consuming information within IDE. Based on that, we prepare an Eclipse plug-in and a framework that allows providing reason of code change, at method granularity, across versions through a new code completion menu in IDE. These allow a software engineer to gain insight about rationale of removed or modified methods which are otherwise not available in present version of code. Professional software engineers participated in our empirical evaluation process and we observed that more than 80% participants considered this to be a useful approach for saving time and effort to understand rationale of method change.}, 
keywords={object-oriented programming;software engineering;object oriented programming;software engineering;integrated development environments;Eclipse plug-in;software development;software maintenance;code completion menus;Software;Databases;Crawlers;Informatics;Software engineering;History;Prototypes}, 
doi={10.1109/ICIEV.2014.6850772}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6676896, 
author={X. Meng and B. P. Miller and W. R. Williams and A. R. Bernat}, 
booktitle={2013 IEEE International Conference on Software Maintenance}, 
title={Mining Software Repositories for Accurate Authorship}, 
year={2013}, 
volume={}, 
number={}, 
pages={250-259}, 
abstract={Code authorship information is important for analyzing software quality, performing software forensics, and improving software maintenance. However, current tools assume that the last developer to change a line of code is its author regardless of all earlier changes. This approximation loses important information. We present two new line-level authorship models to overcome this limitation. We first define the repository graph as a graph abstraction for a code repository, in which nodes are the commits and edges represent the development dependencies. Then for each line of code, structural authorship is defined as a sub graph of the repository graph recording all commits that changed the line and the development dependencies between the commits, weighted authorship is defined as a vector of author contribution weights derived from the structural authorship of the line and based on a code change measure between commits, for example, best edit distance. We have implemented our two authorship models as a new git built-in tool git-author. We evaluated git-author in an empirical study and a comparison study. In the empirical study, we ran git-author on five open source projects and found that git-author can recover more information than a current tool (git-blame) for about 10% of lines. In the comparison study, we used git-author to build a line-level model for bug prediction. We compared our line-level model with an existing file-level model. The results show that our line-level model performs consistently better than the file-level model when evaluated on our data sets produced from the Apache HTTP server project.}, 
keywords={data mining;digital forensics;graph theory;program debugging;software maintenance;software quality;software repositories mining;code authorship information;software quality;software forensics;software maintenance;repository graph;graph abstraction;code repository;bug prediction;Apache HTTP server project;History;Predictive models;Software quality;Vectors;Radio access networks;Data models;Software quality;Version control system;Author contribution;Line-level bug prediction}, 
doi={10.1109/ICSM.2013.36}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{7474550, 
author={M. Malchow and J. Renz and M. Bauer and C. Meinel}, 
booktitle={2016 IEEE Global Engineering Education Conference (EDUCON)}, 
title={Enhance embedded system E-leaming experience with sensors}, 
year={2016}, 
volume={}, 
number={}, 
pages={175-183}, 
abstract={Earlier research shows that using an embedded LED system motivates students to learn programming languages in massive open online courses (MOOCs) efficiently. Since this earlier approach was very successful the system should be improved to increase the learning experience for students during programming exercises. The problem of the current system is that only a static image was shown on the LED matrix controlled by students' array programming over the embedded system. The idea of this paper is to change this static behavior into a dynamic display of information on the LED matrix by the use of sensors which are connected with the embedded system. For this approach a light sensor and a temperature sensor are connected to an analog-to-digital converter (ADC) port of the embedded system. These sensors' values can be read by the students to compute the correct output for the LED matrix. The result is captured and sent back to the students for direct feedback. Furthermore, unit tests can be used to automatically evaluate the programming results. The system was evaluated during a MOOC course about Web Technologies using JavaScript. Evaluation results are taken from the student's feedback and an evaluation of the students' code executions on the system. The positive feedback and the evaluation of the students' executions, which shows a higher amount of code executions compared to standard programming tasks and the fact that students solving these tasks have overall better course results, highlight the advantage of the approach. Due to the evaluation results, this approach should be used in e-learning e.g. MOOCs teaching programming languages to increase the learning experience and motivate students to learn programming.}, 
keywords={computer aided instruction;computer science education;educational courses;embedded systems;optical sensors;programming languages;temperature sensors;embedded system e-learning experience;electronic learning;embedded LED system;light emitting diode;programming language learning;massive open online courses;MOOC;learning experience;programming exercises;light sensor;temperature sensor;analog-to-digital converter;ADC;Web technologies;JavaScript;student feedback;student code execution evaluation;student motivation;Light emitting diodes;Embedded systems;Temperature sensors;Sensor systems;Clocks;Pins;Teleteaching;Tele-Lecturing;Distance Learning;E-Learning;Virtual Lab}, 
doi={10.1109/EDUCON.2016.7474550}, 
ISSN={2165-9567}, 
month={April},}
@INPROCEEDINGS{7169326, 
author={C. Zhang and J. Yang and X. You and S. Xu}, 
booktitle={2015 IEEE International Symposium on Circuits and Systems (ISCAS)}, 
title={Pipelined implementations of polar encoder and feed-back part for SC polar decoder}, 
year={2015}, 
volume={}, 
number={}, 
pages={3032-3035}, 
abstract={In this paper, we first reveal the similarity of polar encoder and fast Fourier transform (FFT) processor. Based on this, both feed-forward and feed-back pipelined implementations of polar encoder are proposed. It is pointed out that the feedback part of SC polar decoder is nothing but a simplified version of polar encoder and therefore can be pipelined implemented also. Moreover, a general approach which uniformly constructs most pipelined polar encoders via folding transformation is proposed. Implementation results have shown that both proposed pipelined polar encoder architectures achieve more than 98.3% complexity reduction and more than 9.86% speed-up compared to the conventional implementation.}, 
keywords={decoding;fast Fourier transforms;feed-back part;SC polar decoder;fast Fourier transform;FFT processor;folding transformation;pipelined polar encoder architectures;Decoding;Switches;Computer architecture;Discrete Fourier transforms;Hardware;Delays;Logic gates;Polar code;encoder;pipelined processing;folding transformation}, 
doi={10.1109/ISCAS.2015.7169326}, 
ISSN={0271-4302}, 
month={May},}
@INPROCEEDINGS{7924936, 
author={Xiaohan Zhang and Zhengping Jin}, 
booktitle={2016 2nd IEEE International Conference on Computer and Communications (ICCC)}, 
title={A new semantics-based android malware detection}, 
year={2016}, 
volume={}, 
number={}, 
pages={1412-1416}, 
abstract={With its high market share, the Android platform has become a growing target for mobile malware, which posed great threat to customers' safety. Meanwhile, malwares employed various techniques, take code obfuscation for example, to evade detection. The commercial mobile anti-malware products, however, are vulnerable to common code transformation techniques. This paper proposes an enhanced malware detection approach which combines advantage of static analysis and performance of ensemble learning to improve Android malware detection accuracy. The model extracts semantics-based features which can resist common obfuscation techniques, and also uses feature collection from code and app characteristics through static analysis. Real-world malware samples are used to evaluate the model and the results of experiments have proved that this approach improved the efficiency with AUC of 2.06% higher than previous approach.}, 
keywords={Android (operating system);feature extraction;invasive software;mobile computing;program diagnostics;semantics-based Android malware detection;mobile malware;customer safety;code obfuscation;mobile antimalware products;code transformation;static analysis;semantics-based feature extraction;feature collection;app characteristics;code characteristics;static analysis;Feature extraction;Malware;Semantics;Smart phones;Detectors;Androids;Humanoid robots;Android;malware detection;semantics-based;ensemble learning}, 
doi={10.1109/CompComm.2016.7924936}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7476681, 
author={H. Borck and M. Boddy and I. J. D. Silva and S. Harp and K. Hoyme and S. Johnston and A. Schwerdfeger and M. Southern}, 
booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
title={Frankencode: Creating Diverse Programs Using Code Clones}, 
year={2016}, 
volume={1}, 
number={}, 
pages={604-608}, 
abstract={In this paper, we present an approach to detecting novel cyber attacks though a form of program diversification, similar to the use of n-version programming for fault tolerant systems. Building on extensive previous and ongoing work by others on the use of code clones in a wide variety of areas, our Functionally Equivalent Variants using Information Synchronization (FEVIS) system automatically generates program variants to berun in parallel, seeking to detect attacks through divergence in behavior. Unlike approaches to diversification that only change program memory layout and behavior, FEVIS can detect attacks exploiting vulnerabilities in execution timing, string processing, and other logic errors. We are in the early stages of research and development for this approach, but have made sufficient progress to provide a proof of concept and some lessons learned. In this paper we describe FEVIS and its application to diversifying an open-source webserver, with results on several different example classes of attack which FEVIS will detect.}, 
keywords={Internet;parallel programming;public domain software;security of data;synchronisation;cyber attacks;program diversification;n-version programming;functionally equivalent variants using information synchronization system;FEVIS system;code clones;Frankencode;program memory layout;program behavior;execution timing;string processing;logic errors;open-source Web server;Cloning;Redundancy;Semantics;Synchronization;Open source software;Manuals;Code Clones;Cyber Security;Software Diversity}, 
doi={10.1109/SANER.2016.95}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7575871, 
author={S. Bosse}, 
booktitle={2016 IEEE 4th International Conference on Future Internet of Things and Cloud (FiCloud)}, 
title={Mobile Multi-agent Systems for the Internet-of-Things and Clouds Using the JavaScript Agent Machine Platform and Machine Learning as a Service}, 
year={2016}, 
volume={}, 
number={}, 
pages={244-253}, 
abstract={The Internet-of-Things (IoT) gets real in today's life and is becoming part of pervasive and ubiquitous computing networks offering distributed and transparent services. A unified and common data processing and communication methodology is required to merge the IoT, sensor networks, and Cloud-based environments seamless, which can be fulfilled by the mobile agent-based computing paradigm, discussed in this work. Currently, portability, resource constraints, security, and scalability of Agent Processing Platforms (APP) are essential issues for the deployment of Multi-agent Systems (MAS) in strong heterogeneous networks including the Internet, addressed in this work. To simplify the development and deployment of MAS it would be desirable to implement agents directly in JavaScript, which is a well known and public widespread used programming language, and JS VMs are available on all host platforms including WEB browsers. The novel proposed JS Agent Machine (JAM) is capable to execute AgentJS agents in a sandbox environment with full run-time protection and Machine learning as a service. Agents can migrate between different JAM nodes seamless preserving their data and control state by using a on-the-fly code-to-text transformation in an extended JSON+ format. A Distributed Organization System (DOS) layer provides JAM node connectivity and security in the Internet, completed by a Directory-Name Service offering an organizational graph structure. Agent authorization and platform security is ensured with capability-based access and different agent privilege levels.}, 
keywords={authorisation;cloud computing;data protection;graph theory;Internet of Things;Java;learning (artificial intelligence);mobile agents;mobile computing;multi-agent systems;online front-ends;virtual machines;sandbox environment;run-time protection;data preservation;control state;on-the-fly code-to-text transformation;extended JSON+ format;distributed organization system layer;DOS layer;JAM node connectivity;JAM node security;directory-name service;organizational graph structure;agent authorization;platform security;capability-based access;agent privilege levels;AgentJS agents;JS agent machine;Web browsers;host platforms;JS VM;heterogeneous networks;MAS;APP;agent processing platforms;scalability constraint;security constraint;resource constraint;portability constraint;cloud-based environment;sensor networks;IoT;communication methodology;data processing;ubiquitous computing network;pervasive computing network;machine learning-as-a-service;JavaScript agent machine platform;Internet-of-Things;mobile multiagent systems;Cloud computing;Internet of things;Mobile communication;Computer architecture;Sensors;Security;Agents;IoT;Cloud Computing;Agent Platforms}, 
doi={10.1109/FiCloud.2016.43}, 
ISSN={}, 
month={Aug},}
@ARTICLE{8128815, 
author={F. Zhang and S. Khoo and X. Su}, 
journal={Chinese Journal of Electronics}, 
title={Machine-Learning Aided Analysis of Clone Evolution}, 
year={2017}, 
volume={26}, 
number={6}, 
pages={1132-1138}, 
abstract={Code clones are similar code fragments appearing in software. As software evolves, code clones may be subjected to changes as well; we term this clone evolution. There have not been many investigations into clone evolution characteristics. Therefore, we tackle this by exploring useful information associated with changes of clones during evolution. We focus on three perspectives of clone evolution, ranging from individual clone changes to characterization of clone genealogies. With the help X-means clustering, we establish associations between clone changes and life of clones. Our experimental results on two softwares show that clones are mostly stable throughout software evolution. For the relatively smaller group of “unstable” clones, changes usually happen after several versions, and consistent changes appear more frequently than inconsistent ones. We suggest that developers should pay more attention to relatively longer genealogies, and should consider applying changes consistently to clone group when a constituent clone fragment has undergone change.}, 
keywords={learning (artificial intelligence);pattern clustering;software maintenance;machine-learning aided analysis;clone evolution;unstable clones;software evolution;X-means clustering;clone genealogies;clone changes;code fragments;code clones}, 
doi={10.1049/cje.2017.08.012}, 
ISSN={1022-4653}, 
month={},}
@INPROCEEDINGS{7800312, 
author={K. K. Zaw and N. Funabiki and M. Kuribayashi}, 
booktitle={2016 IEEE 5th Global Conference on Consumer Electronics}, 
title={A proposal of three extensions in blank element selection algorithm for Java programming learning assistant system}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-3}, 
abstract={To assist Java programming educations, we have developed a Web-based Java Programming Learning Assistant System (JPLAS). JPLAS provides fill-in-blank problems to let students study Java grammar and basic programming skills by filling the blanked elements in a given Java code. To generate the feasible problems, we have proposed a blank element selection algorithm using the constraint graph to select as many blanks as possible such that they have grammatically correct and unique answers. In this paper, to further increase the number of blanks and control the difficulty of the generated problem, we extend this algorithm by 1) adding operators in conditional expressions for blank candidates, 2) improving the edge generation method in the constraint graph to increase the number of blanks, and 3) introducing two parameters to change the frequency of selecting blanks. To verify the effectiveness, we apply the extended algorithm to 55 Java codes for fundamental data structure or algorithms, and confirm that these extensions can increase the number of blanks and change the problem difficulty.}, 
keywords={computer aided instruction;computer science education;graph theory;Internet;Java;programming;Java programming educations;Web-based Java programming learning assistant system;JPLAS;fill-in-blank problems;Java grammar;basic programming skills;Java code;blank element selection algorithm;constraint graph;conditional expressions;blank candidates;edge generation;fundamental data structure;Java;Programming profession;Data structures;Conferences;Consumer electronics;Proposals}, 
doi={10.1109/GCCE.2016.7800312}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{7967120, 
author={D. Beckingsale and O. Pearce and I. Laguna and T. Gamblin}, 
booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
title={Apollo: Reusable Models for Fast, Dynamic Tuning of Input-Dependent Code}, 
year={2017}, 
volume={}, 
number={}, 
pages={307-316}, 
abstract={Increasing architectural diversity makes performance portability extremely important for parallel simulation codes. Emerging on-node parallelization frameworks such as Kokkos and RAJA decouple the work done in kernels from the parallelization mechanism, allowing for a single source kernel to be tuned for different architectures at compile time. However, computational demands in production applications change at runtime, and performance depends both on the architecture and the input problem, and tuning a kernel for one set of inputs may not improve its performance on another. The statically optimized versions need to be chosen dynamically to obtain the best performance. Existing auto-tuning approaches can handle slowly evolving applications effectively, but are too slow to tune highly input-dependent kernels. We developed Apollo, an auto-tuning extension for RAJA that uses pre-trained, reusable models to tune input-dependent code at runtime. Apollo is designed for highly dynamic applications; it generates sufficiently low-overhead code to tune parameters each time a kernel runs, making fast decisions. We apply Apollo to two hydrodynamics benchmarks and to a production multi-physics code, and show that it can achieve speedups from 1.2x to 4.8x.}, 
keywords={hydrodynamics;parallel processing;physics computing;program compilers;Apollo;reusable models;dynamic input-dependent code tuning;performance portability;parallel simulation codes;on-node parallelization frameworks;Kokkos;RAJA;single-source kernel;compile time;autotuning approach;code generation;hydrodynamics benchmarks;multiphysics code;Kernel;Tuning;Runtime;Hydrodynamics;Computer architecture;C++ languages;Training;machine learning;autotuning;performance;programming models}, 
doi={10.1109/IPDPS.2017.38}, 
ISSN={1530-2075}, 
month={May},}
@INPROCEEDINGS{6413855, 
author={Z. Xu and K. Kersting and C. Bauckhage}, 
booktitle={2012 IEEE 12th International Conference on Data Mining}, 
title={Efficient Learning for Hashing Proportional Data}, 
year={2012}, 
volume={}, 
number={}, 
pages={735-744}, 
abstract={Spectral hashing (SH) seeks compact binary codes of data points so that Hamming distances between codes correlate with data similarity. Quickly learning such codes typically boils down to principle component analysis (PCA). However, this is only justified for normally distributed data. For proportional data (normalized histograms), this is not the case. Due to the sum-to-unity constraint, features that are as independent as possible will not all be uncorrelated. In this paper, we show that a linear-time transformation efficiently copes with sum-to-unity constraints: first, we select a small number K of diverse data points by maximizing the volume of the simplex spanned by these prototypes; second, we represent each data point by means of its cosine similarities to the K selected prototypes. This maximum volume hashing is sensible since each dimension in the transformed space is likely to follow a von Mises (vM) distribution, and, in very high dimensions, the vM distribution closely resembles a Gaussian distribution. This justifies to employ PCA on the transformed data. Our extensive experiments validate this: maximum volume hashing outperforms spectral hashing and other state of the art techniques.}, 
keywords={cryptography;Gaussian distribution;learning (artificial intelligence);principal component analysis;proportional data hashing;spectral hashing;binary code;Hamming distance;data similarity;data learning;principle component analysis;PCA;normalized histogram;sum-to-unity constraint;linear-time transformation;simplex volume;maximum volume hashing;von Mises distribution;Gaussian distribution;Eigenvalues and eigenfunctions;Principal component analysis;Gaussian distribution;Binary codes;Vectors;Optimization;Semantics;Spectral Hashing;Proportional Data;von Mises Distribution;Dimensionality Reduction}, 
doi={10.1109/ICDM.2012.142}, 
ISSN={1550-4786}, 
month={Dec},}
@ARTICLE{7270328, 
author={X. Ye and R. Bunescu and C. Liu}, 
journal={IEEE Transactions on Software Engineering}, 
title={Mapping Bug Reports to Relevant Files: A Ranking Model, a Fine-Grained Benchmark, and Feature Evaluation}, 
year={2016}, 
volume={42}, 
number={4}, 
pages={379-402}, 
abstract={When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and improve productivity. This paper introduces an adaptive ranking approach that leverages project knowledge through functional decomposition of source code, API descriptions of library components, the bug-fixing history, the code change history, and the file dependency graph. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluate the ranking system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the learning-to-rank approach outperforms three recent state-of-the-art methods. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70 percent of the bug reports in the Eclipse Platform and Tomcat projects.}, 
keywords={Software;History;Computational modeling;Computer bugs;Collaboration;Benchmark testing;Standards;Bug reports;software maintenance;learning to rank;Bug reports;software maintenance;learning to rank}, 
doi={10.1109/TSE.2015.2479232}, 
ISSN={0098-5589}, 
month={April},}
@INPROCEEDINGS{8060313, 
author={Xuechao Wei and Cody Hao Yu and Peng Zhang and Youxiang Chen and Yuxin Wang and Han Hu and Yun Liang and J. Cong}, 
booktitle={2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC)}, 
title={Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Convolutional neural networks (CNNs) have been widely applied in many deep learning applications. In recent years, the FPGA implementation for CNNs has attracted much attention because of its high performance and energy efficiency. However, existing implementations have difficulty to fully leverage the computation power of the latest FPGAs. In this paper we implement CNN on an FPGA using a systolic array architecture, which can achieve high clock frequency under high resource utilization. We provide an analytical model for performance and resource utilization and develop an automatic design space exploration framework, as well as source-to-source code transformation from a C program to a CNN implementation using systolic array. The experimental results show that our framework is able to generate the accelerator for real-life CNN models, achieving up to 461 GFlops for floating point data type and 1.2 Tops for 8-16 bit fixed point.}, 
keywords={convolution;field programmable gate arrays;learning (artificial intelligence);logic design;neural nets;systolic arrays;source-to-source code transformation;CNN implementation;automated systolic array architecture synthesis;convolutional neural networks;deep learning applications;automatic design space exploration;FPGA;high throughput CNN inference;Field programmable gate arrays;Arrays;Convolutional codes;Analytical models;Optimization;Parallel processing}, 
doi={10.1145/3061639.3062207}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{6385141, 
author={F. Zhang and F. Khomh and Y. Zou and A. E. Hassan}, 
booktitle={2012 19th Working Conference on Reverse Engineering}, 
title={An Empirical Study of the Effect of File Editing Patterns on Software Quality}, 
year={2012}, 
volume={}, 
number={}, 
pages={456-465}, 
abstract={While some developers like to work on multiple code change requests, others might prefer to handle one change request at a time. This juggling of change requests and the large number of developers working in parallel often lead to files being edited as part of different change requests by one or several developers. Existing research has warned the community about the potential negative impacts of some file editing patterns on software quality. For example, when several developers concurrently edit a file as part of different change requests, they are likely to introduce bugs due to limited awareness of other changes. However, very few studies have provided quantitative evidence to support these claims. In this paper, we identify four file editing patterns. We perform an empirical study on three open source software systems to investigate the individual and the combined impact of the four patterns on software quality. We find that: (1) files that are edited concurrently by many developers have on average 2.46 times more future bugs than files that are not concurrently edited, (2) files edited in parallel with other files by the same developer have on average 1.67 times more future bugs than files individually edited, (3) files edited over an extended period (i.e., above the third quartile) of time have 2.28 times more future bugs than other files, and (4) files edited with long interruptions (i.e., above the third quartile) have 2.1 times more future bugs than other files. When more than one editing patterns are followed by one or many developers during the editing of a file, we observe that the number of future bugs in the file can be as high as 1.6 times the average number of future bugs in files edited following a single editing pattern. These results can be used by software development teams to warn developers about risky file editing patterns.}, 
keywords={file organisation;program debugging;software quality;file editing pattern;software quality;code change request;software development;software bug;open source software system;single editing pattern;Computer bugs;History;Equations;Software quality;Measurement;Programming;ile editing pattern;change request;bug;software quality;empirical software engineering;mylyn}, 
doi={10.1109/WCRE.2012.55}, 
ISSN={2375-5369}, 
month={Oct},}
@ARTICLE{6698391, 
author={P. Collingbourne and C. Cadar and P. H. J. Kelly}, 
journal={IEEE Transactions on Software Engineering}, 
title={Symbolic Crosschecking of Data-Parallel Floating-Point Code}, 
year={2014}, 
volume={40}, 
number={7}, 
pages={710-737}, 
abstract={We present a symbolic execution-based technique for cross-checking programs accelerated using SIMD or OpenCL against an unaccelerated version, as well as a technique for detecting data races in OpenCL programs. Our techniques are implemented in KLEE-CL, a tool based on the symbolic execution engine KLEE that supports symbolic reasoning on the equivalence between expressions involving both integer and floating-point operations. While the current generation of constraint solvers provide effective support for integer arithmetic, the situation is different for floating-point arithmetic, due to the complexity inherent in such computations. The key insight behind our approach is that floating-point values are only reliably equal if they are essentially built by the same operations. This allows us to use an algorithm based on symbolic expression matching augmented with canonicalisation rules to determine path equivalence. Under symbolic execution, we have to verify equivalence along every feasible control-flow path. We reduce the branching factor of this process by aggressively merging conditionals, if-converting branches into select operations via an aggressive phi-node folding transformation. To support the Intel Streaming SIMD Extension (SSE) instruction set, we lower SSE instructions to equivalent generic vector operations, which in turn are interpreted in terms of primitive integer and floating-point operations. To support OpenCL programs, we symbolically model the OpenCL environment using an OpenCL runtime library targeted to symbolic execution. We detect data races by keeping track of all memory accesses using a memory log, and reporting a race whenever we detect that two accesses conflict. By representing the memory log symbolically, we are also able to detect races associated with symbolically-indexed accesses of memory objects. We used KLEE-CL to prove the bounded equivalence between scalar and data-parallel versions of floating-point programs and find a number of issues in a variety of open source projects that use SSE and OpenCL, including mismatches between implementations, memory errors, race conditions and a compiler bug.}, 
keywords={data handling;floating point arithmetic;parallel processing;program debugging;symbolic crosschecking;data parallel floating point code;symbolic execution based technique;crosschecking programs;SIMD;OpenCL programs;KLEE-CL;symbolic execution engine;symbolic reasoning;floating-point operations;integer operations;integer arithmetic;floating point arithmetic;symbolic expression matching;phi node folding transformation;intel streaming SIMD extension;SSE instruction set;equivalent generic vector operations;floating point operations;primitive integer operations;OpenCL environment;memory accesses;memory log;open source projects;floating-point programs;compiler bug;memory errors;race conditions;Vectors;Kernel;Computational modeling;Computer architecture;Semantics;Programming;Parallel processing;Data-parallel code;floating point;symbolic execution;SIMD;OpenCL;KLEE-CL}, 
doi={10.1109/TSE.2013.2297120}, 
ISSN={0098-5589}, 
month={July},}
@INPROCEEDINGS{7194576, 
author={Y. Yoon and B. A. Myers}, 
booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering}, 
title={Supporting Selective Undo in a Code Editor}, 
year={2015}, 
volume={1}, 
number={}, 
pages={223-233}, 
abstract={Programmers often need to revert some code to an earlier state, or restore a block of code that was deleted a while ago. However, support for this backtracking in modern programming environments is limited. Many of the backtracking tasks can be accomplished by having a selective undo feature in code editors, but this has major challenges: there can be conflicts among edit operations, and it is difficult to provide usable interfaces for selective undo. In this paper, we present AZURITE, an Eclipse plug-in that allows programmers to selectively undo fine-grained code changes made in the code editor. With AZURITE, programmers can easily perform backtracking tasks, even when the desired code is not in the undo stack or a version control system. AZURITE also provides novel user interfaces specifically designed for selective undo, which were iteratively improved through user feedback gathered from actual users in a preliminary field trial. A formal lab study showed that programmers can successfully use AZURITE, and were twice as fast as when limited to conventional features.}, 
keywords={programming environments;text editing;user interfaces;selective undo;code editor;programming environments;backtracking tasks;AZURITE;Eclipse plug-in;user interfaces;user feedback;History;Layout;Encoding;Control systems;Graphical user interfaces;selective undo;backtracking}, 
doi={10.1109/ICSE.2015.43}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{6494984, 
author={M. Zhou and B. Wu and Y. Ding and X. Shen}, 
booktitle={Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
title={Profmig: A framework for flexible migration of program profiles across software versions}, 
year={2013}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={Offline program profiling is costly, especially when software update is frequent. In this paper, we initiate a systematic exploration in cross-version program profile migration, which tries to effectively reuse the valid part of the behavior profiles of an old version of a software for a new version. We explore the effects imposed on profile reusability by the various factors in program behaviors, profile formats, and impact analysis, and introduce ProfMig, a framework for flexible migrations of various profiles. We demonstrate the effectiveness of the techniques on migrating loop trip-count profiles and dynamic call graphs. The migration saves significant (48-67% on average) profiling time with less than 10% accuracy compromised for most programs.}, 
keywords={program diagnostics;software reusability;ProfMig;flexible migration;offline program profiling;software update;software versions;systematic exploration;cross-version program profile migration;profile reusability;program behaviors;profile formats;impact analysis;loop trip-count profiles;dynamic call graphs;Software;Complexity theory;Hardware;Switches;Context;Merging;Accuracy;Profile;Driven Optimizations;Profile Migration;Software Update;Change Impact Analysis}, 
doi={10.1109/CGO.2013.6494984}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{6423031, 
author={W. H. Rankothge and S. V. Sendanayake and R. G. P. Sudarshana and B. G. G. H. Balasooriya and D. R. Alahapperuma and Y. Mallawarachchi}, 
booktitle={International Conference on Advances in ICT for Emerging Regions (ICTer2012)}, 
title={Technology assisted tool for learning skills development in early childhood}, 
year={2012}, 
volume={}, 
number={}, 
pages={165-168}, 
abstract={Importance of proper early childhood education is a fact which is often overlooked and neglected. But it is a very important phase in a child's life and it cannot be under-estimated. Experiences during this phase extensively influence physical and neurological developments, which drive biological, psychological and social responses throughout the entire human lifespan. This research introduces a technology assisted tool for the learning skills development in early childhood. The final outcome is a Tablet PC based application to help the toddlers in their learning experience at early ages. This tool is able to improve the writing and speaking skills of the toddler in an entertainment based way. This features an easy way to teach the toddlers in a productive and efficient manner. The objective is to ensure the children are trained and practiced in early days of their lives using standards and cutting edge methodologies, so that they will be ready to face the challenges in their future.}, 
keywords={computer aided instruction;entertainment;human computer interaction;notebook computers;technology assisted tool;learning skills development;early childhood education;physical developments;neurological developments;biological responses;psychological responses;social responses;tablet PC based application;writing skills improvement;speaking skills improvement;entertainment based way;Training;Writing;Feature extraction;Psychology;Mirrors;Speech;Fuzzy logic;Guided handwriting training;Unguided handwriting training;Letter speech training;Word speech training;Fourier transformation;Control point evaluation;Chain code analysis}, 
doi={10.1109/ICTer.2012.6423031}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6947290, 
author={G. Matasci and F. de Morsier and M. Kanevski and D. Tuia}, 
booktitle={2014 IEEE Geoscience and Remote Sensing Symposium}, 
title={Domain adaptation in remote sensing through cross-image synthesis with dictionaries}, 
year={2014}, 
volume={}, 
number={}, 
pages={3714-3717}, 
abstract={This contribution studies an approach based on dictionary learning which enables the alignment of the sparse representations of two images. Set in a domain adaptation context, the purpose of this work is to re-synthesize the pixels of a remote sensing image so that, for a given land-cover class, the new values of the samples are comparable across acquisitions. Consequently, the data space of a given source image can be converted to that of a related target image, or vice-versa. After the mentioned transformation, the performance of a classifier trained on the source image and used to predict the thematic classes on the target image is expected to be more robust. A linear transformation is derived thanks to an algorithm simultaneously learning the image-specific dictionaries and the mapping function bridging them via their respective sparse codes. Experiments on knowledge transfer among two co-registered VHR images acquired with different off-nadir angles show promising results. An appropriate cross-image synthesis yields an increased land-cover model portability from one acquisition to another.}, 
keywords={dictionaries;geophysical image processing;image classification;image coding;image registration;image representation;land cover;learning (artificial intelligence);remote sensing;transforms;cross-image synthesis;dictionary learning;sparse image representation alignment;domain adaptation context;remote sensing image pixel resynthesis;land-cover class;linear transformation;sparse code;knowledge transfer;coregistered VHR image acquisition;off-nadir angle;Dictionaries;Remote sensing;Training;Sparse matrices;Support vector machines;Earth;Radiometry;image classification;dataset shift;dictionary learning;sparse representation}, 
doi={10.1109/IGARSS.2014.6947290}, 
ISSN={2153-6996}, 
month={July},}
@INPROCEEDINGS{8530077, 
author={B. Jansen and F. Hermans and E. Tazelaar}, 
booktitle={2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Detecting and Predicting Evolution in Spreadsheets - A Case Study in an Energy Network Company}, 
year={2018}, 
volume={}, 
number={}, 
pages={645-654}, 
abstract={The use of spreadsheets in industry is widespread and the information that they provide is often used for decisions. Research has shown that spreadsheets are error-prone, leading to the risk that decisions are made on incorrect information. Software Evolution is a well-researched topic and the results have proven to support developers in creating better software. Could this also be applied to spreadsheets? Unfortunately, the research on spreadsheet evolution is still limited. Therefore, the aim of this paper is to obtain a better understanding of how spreadsheets evolve over time and if the results of such a study provide similar benefits for spreadsheets as it does for source code. In this study, we cooperated with Alliander, a large energy network company in the Netherlands. We conducted two case studies on two different set of spreadsheets that both were already maintained for a period of three years. To have a better understanding of the spreadsheets itself and the context in which they evolved, we also interviewed the creators of the spreadsheets. We focus on the changes that are made over time in the formulas. Changes in these formulas change the behavior of the spreadsheet and could possibly introduce errors. To effectively analyze these changes we developed an algorithm that is able to detect and visualize these changes. Results indicate that studying the evolution of a spreadsheet helps to identify areas in the spreadsheet that are error-prone, likely to change or that could benefit from refactoring. Furthermore, by analyzing the frequency in which formulas are changed from version to version, it is possible to predict which formulas need to be changed when a new version of the spreadsheet is created.}, 
keywords={software maintenance;spreadsheet programs;spreadsheet evolution;Energy Network Company;incorrect information;software evolution;source code;Alliander;energy network company;Software;Tools;Companies;Industries;Visualization;Control systems;Computer languages;Spreadsheet Evolution;Software Evolution;End User Programming;Version}, 
doi={10.1109/ICSME.2018.00074}, 
ISSN={2576-3148}, 
month={Sep.},}
@ARTICLE{8013683, 
author={F. Isikdogan and A. C. Bovik and P. Passalacqua}, 
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
title={Surface Water Mapping by Deep Learning}, 
year={2017}, 
volume={10}, 
number={11}, 
pages={4909-4918}, 
abstract={Mapping of surface water is useful in a variety of remote sensing applications, such as estimating the availability of water, measuring its change in time, and predicting droughts and floods. Using the imagery acquired by currently active Landsat missions, a surface water map can be generated from any selected region as often as every 8 days. Traditional Landsat water indices require carefully selected threshold values that vary depending on the region being imaged and on the atmospheric conditions. They also suffer from many false positives, arising mainly from snow and ice, and from terrain and cloud shadows being mistaken for water. Systems that produce high-quality water maps usually rely on ancillary data and complex rule-based expert systems to overcome these problems. Here, we instead adopt a data-driven, deep-learning-based approach to surface water mapping. We propose a fully convolutional neural network that is trained to segment water on Landsat imagery. Our proposed model, named Deep-WaterMap, learns the characteristics of water bodies from data drawn from across the globe. The trained model separates water from land, snow, ice, clouds, and shadows using only Landsat bands as input. Our code and trained models are publicly available at http://live.ece.utexas.edu/research/deepwatermap/.}, 
keywords={expert systems;floods;geophysical image processing;hydrological techniques;image classification;image processing;image segmentation;learning (artificial intelligence);neural nets;remote sensing;statistical analysis;water;surface water mapping;currently active Landsat missions;surface water map;Traditional Landsat water indices;high-quality water maps;deep-learning-based approach;segment water;water bodies;trained model separates water;time 8.0 d;Remote sensing;Earth;Satellites;Image segmentation;Water;Biological neural networks;Computer vision;convolutional neural networks;landsat;machine learning;remote sensing}, 
doi={10.1109/JSTARS.2017.2735443}, 
ISSN={1939-1404}, 
month={Nov},}
@INPROCEEDINGS{8031199, 
author={M. Harrell and M. Levy and D. Fabbri}, 
booktitle={2017 IEEE International Conference on Healthcare Informatics (ICHI)}, 
title={Supervised Machine Learning to Predict Follow-Up Among Adjuvant Endocrine Therapy Patients}, 
year={2017}, 
volume={}, 
number={}, 
pages={490-495}, 
abstract={Long-term adjuvant endocrine therapy patients often fail to follow-up with their care providers for the recommended duration of time. We used electronic health record data, tumor registry records, and appointment logs to predict follow-up for an adjuvant endocrine therapy patient cohort. Learning predictors for follow-up may facilitate interventions that improve follow-up rates, and ultimately improve patient care in the adjuvant endocrine therapy patient population.We selected 1455 adjuvant endocrine therapy patients at Vanderbilt University Medical Center, and modeled them as a matrix of medical-related, appointment-related, and demographic related features derived from EHR data. We built and optimized a random forest classifier and neural network to differentiate between patients that follow-up, or fail to follow-up, with their care provider for at least five years. We measured follow-up three different ways: thought appointments with any care providers, appointments with an oncologist, and adjuvant endocrine therapy medication records. Classifiers make predictions at the start of adjuvant endocrine therapy, and additionally use temporal subsets of data to learn the change in accuracy as patient data accrues.Our best model is a random forest classifier combining medical-related, appointment-related, and demographic-related features to achieve an AUC of 0.74. The most predictive features for follow-up in our random forest model are total medication counts, patient age, and median income for zip code. We suggest that reliable prediction for follow-up may be correlated with amount of care received at VUMC (i.e., VUMC primary care).This study achieved moderately accurate prediction for followup in adjuvant endocrine therapy patients from electronic health record data. Predicting follow-up can facilitate interventions for improving follow-up rates and improve patient care for adjuvant endocrine therapy cohorts. This study demonstrates the ability to find opportunities for patient care improvement from EHR data.}, 
keywords={cancer;electronic health records;health care;learning (artificial intelligence);medical computing;neural nets;patient care;patient treatment;tumours;adjuvant endocrine therapy medication records;random forest classifier;electronic health record data;adjuvant endocrine therapy patient cohort;adjuvant endocrine therapy patient population;demographic related features;patient care;supervised machine learning;tumor registry records;appointment logs;Vanderbilt University Medical Center;EHR data;VUMC;appointment-related features;medical-related features;neural network;Medical diagnostic imaging;Tumors;Electronic medical records;Neural networks;Drugs;Breast cancer;electronic health records;machine learning;breast cancer}, 
doi={10.1109/ICHI.2017.46}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{7968556, 
author={D. Suciu and R. Sion}, 
booktitle={2017 21st International Conference on Control Systems and Computer Science (CSCS)}, 
title={Droidsentry: Efficient Code Integrity and Control Flow Verification on TrustZone Devices}, 
year={2017}, 
volume={}, 
number={}, 
pages={156-158}, 
abstract={The fast evolution of mobile devices has made them the center of attention for not only the research industry, but also malicious actors, as smartphones are used to store, transmit and process sensitive information. The diversity and number of typically installed applications create windows of opportunity for attackers. Attackers can use vulnerable applications to gain control over the device or change the behavior of applications relied on to manage user's finances or store their secret data. Thus, current mobile systems need application execution verification mechanisms. In consequence, we present a framework for current ARM mobile devices that can detect application control flow manipulation attempts by looking at the history of executed control flow altering instructions on the processor. This history examination provides enough information to implement the state-of-the-art fine-grained control policies, without additional binary instrumentation. Moreover, this framework is designed to work with existing hardware and have a minimal impact on performance.}, 
keywords={mobile computing;security of data;smart phones;Droidsentry;code integrity;control flow verification;TrustZone device;mobile devices;smart phones;vulnerable applications;application execution verification mechanisms;ARM mobile devices;application control flow manipulation attempts;fine-grained control policies;Hardware;Kernel;Security;Mobile handsets;Registers;Aerospace electronics;Performance evaluation;return oriented programming;TrustZone;control flow integrity;program tracing macrocell}, 
doi={10.1109/CSCS.2017.28}, 
ISSN={2379-0482}, 
month={May},}
@INPROCEEDINGS{6980188, 
author={C. S. Corley and K. L. Kashuda and D. S. May and N. A. Kraft}, 
booktitle={2014 IEEE 4th Workshop on Mining Unstructured Data}, 
title={Modeling Changeset Topics}, 
year={2014}, 
volume={}, 
number={}, 
pages={6-10}, 
abstract={Topic modeling has been applied to several areas of software engineering, such as bug localization, feature location, triaging change requests, and traceability link recovery. Many of these approaches combine mining unstructured data, such as bug reports, with topic modeling a snapshot (or release) of source code. However, source code evolves, which causes models to become obsolete. In this paper, we explore the approach of topic modeling changesets over the traditional release approach. We conduct an exploratory study of four open source systems. We investigate the differences in corpora in each project, and evaluate the topic distinctness of the models.}, 
keywords={data mining;program debugging;public domain software;software engineering;changeset topics modeling;software engineering;bug localization;feature location;triaging change requests;traceability link recovery;unstructured data mining;bug reports;source code;open source systems;topic distinctness evaluation;Data models;Resource management;Data mining;History;Software maintenance;Java;Mining software repositories;changesets;topic modeling;latent Dirichlet allocation}, 
doi={10.1109/MUD.2014.9}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8409131, 
author={O. Drozd and M. Kuznietsov and O. Martynyuk and M. Drozd}, 
booktitle={2018 IEEE 9th International Conference on Dependable Systems, Services and Technologies (DESSERT)}, 
title={A method of the hidden faults elimination in FPGA projets for the critical applications}, 
year={2018}, 
volume={}, 
number={}, 
pages={218-221}, 
abstract={This paper addresses to a problem of the hidden faults which is inherent for the instrumentation and control safety-related systems. These systems are designed for operation in two modes: normal and emergency. The hidden faults can be accumulated throughout the long normal mode and reduce a fault tolerance of the digital components and the functional safety of systems and control objects in the most responsible emergency mode. A method of the hidden faults elimination by an increase in a checkability of the circuits constructed in the LUT-oriented architecture of FPGA projects is suggested. The method is based on the use of several versions of a program code of the FPGA project when saving its hardware implementation. Serial change of versions in a normal mode allows to control the bits of LUT memory used only in an emergency mode.}, 
keywords={fault tolerance;field programmable gate arrays;safety systems;semiconductor device reliability;FPGA project;hidden faults elimination;FPGA projets;instrumentation;control safety-related systems;fault tolerance;LUT-oriented architecture;Table lookup;Circuit faults;Manganese;Field programmable gate arrays;Testing;Color;Fault tolerance;safety-related system;digital component;FPGA;LUT;hidden fault;version of a program code}, 
doi={10.1109/DESSERT.2018.8409131}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7883400, 
author={D. Dig and R. Johnson and D. Marinov and B. Bailey and D. Batory}, 
booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)}, 
title={COPE: Vision for a Change-Oriented Programming Environment}, 
year={2016}, 
volume={}, 
number={}, 
pages={773-776}, 
abstract={Software engineering involves a lot of change as code artifacts are not only created once but maintained over time. In the last 25 years, major paradigms of program development have arisen - agile development with refactorings, software product lines, moving sequential code to multicore or cloud, etc. Each is centered on particular kinds of change; their conceptual foundations rely on transformations that (semi-) automate these changes. We are exploring how transformations can be placed at the center of software development in future IDEs, and when such a view can provide benefits over the traditional view. COPE, a Change-Oriented Programming Environment, looks at 5 activities: (1) analyze what changes programmers typically make and how they perceive, recall, and communicate changes, (2) automate transformations to make it easier to apply and script changes, (3) develop tools that compose and manipulate transformations to make it easier to reuse them, (4) integrate transformations with version control to pro- vide better ways for archiving and understanding changes, and (5) develop tools that infer higher-level transformations from lower-level changes. Characterizing software development in terms of transformations is an essential step to take software engineering from manual development to (semi-) automated development of software.}, 
keywords={program processors;software reusability;program transformations;IDEs;software development;sequential code;software product lines;software refactorings;agile development;program development;software engineering;change-oriented programming environment;COPE;Software;Security;Software engineering;Optimization;Computer bugs;Algorithm design and analysis;Visualization;software evolution;program transformations;software changes}, 
doi={}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7178316, 
author={T. Liu and R. R. Varior and G. Wang}, 
booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Visual tracking using learned color features}, 
year={2015}, 
volume={}, 
number={}, 
pages={1976-1980}, 
abstract={Robust object tracking is a challenging task in computer vision. Color features have been popularly used in visual tracking. However, most conventional color-based trackers either rely on luminance information or use simple color representations for image description. During the tracking sequences, the perceived color of the target may change because of the varying lighting conditions. In this paper, we learn the color patterns offline from pixels sampled from images across different camera views. In the new color feature space, the proposed tracking method performs robustly in various environment. The new color feature space is learned by learning a linear transformation and a dictionary to encode pixel values. To speedup the feature extraction, we use the marginal regression to calculate the sparse feature codes. Experimental results demonstrate that significant improvement can be achieved by using our learned color features, especially on the video sequences with complicated lighting conditions.}, 
keywords={brightness;computer vision;feature extraction;image colour analysis;image resolution;image sensors;image sequences;learning (artificial intelligence);object tracking;regression analysis;video coding;visual tracking;color feature space learning;robust object tracking;computer vision;color-based trackers;luminance information;color representations;image description;tracking sequences;varying lighting conditions;camera views;linear transformation;pixel value encoding;feature extraction;marginal regression;sparse feature code calculation;video sequences;Image color analysis;Target tracking;Visualization;Encoding;Lighting;Robustness;Computer vision;Visual tracking;color features;marginal regression;feature learning}, 
doi={10.1109/ICASSP.2015.7178316}, 
ISSN={1520-6149}, 
month={April},}
@INPROCEEDINGS{8049073, 
author={S. Giraddi and S. Yaligar and H. S. Kavitha}, 
booktitle={2016 IEEE 4th International Conference on MOOCs, Innovation and Technology in Education (MITE)}, 
title={Problem and Project-Based Learning in Scripting Lab}, 
year={2016}, 
volume={}, 
number={}, 
pages={152-156}, 
abstract={Scripting language employ high-level constructs to interpret and execute one command at a time. In general scripting languages are easier to learn and faster to code than structured and compiled languages such as C and C++. Scripting languages have many important advantages over traditional programming languages. In future the usage of these languages is likely to increase. In this paper we discuss and report our experience in teaching scripting languages lab at the undergraduate level, 4th semester. Scripting language is an umbrella term used for languages like unix shell, TCL, perl, java, python and LISP. Out of these, we have chosen UNIX shell programming and python for our curriculum. The authors report various pedagogical activities like multiple assignments, peer assessment within a group, self learning through e-resources and course project that were employed during the course. The course projects were specially designed so as to make students explore the vast number of python packages. The authors found that these activities definitely enhance the learning experience and there was a remarkable change in the learning level of the students as compared to previous years as evident in the grades obtained by the students.}, 
keywords={authoring languages;computer science education;educational courses;further education;programming environments;software packages;teaching;Unix;problem-based learning;project-based learning;PBL;scripting language lab teaching;high-level constructs;undergraduate level;UNIX shell programming;Python packages;pedagogical activities;course projects design;Computer languages;Technological innovation;Engineering education;Programming profession;Scripting language;Python;pedagogy;assessment strategy;peer assessment}, 
doi={10.1109/MITE.2016.039}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{8170087, 
author={D. Falessi and B. Russo and K. Mullen}, 
booktitle={2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)}, 
title={What if I Had No Smells?}, 
year={2017}, 
volume={}, 
number={}, 
pages={78-84}, 
abstract={What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.}, 
keywords={software maintenance;software quality;code smell;total defective files;Tools;Maintenance engineering;Software;Logic gates;Estimation;History;Business;code smells;machine learning;software estimation;technical debt}, 
doi={10.1109/ESEM.2017.14}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6240476, 
author={B. Silva and C. Sant'Anna and C. Chavez and A. Garcia}, 
booktitle={2012 20th IEEE International Conference on Program Comprehension (ICPC)}, 
title={Concern-based cohesion: Unveiling a hidden dimension of cohesion measurement}, 
year={2012}, 
volume={}, 
number={}, 
pages={103-112}, 
abstract={Cohesion has been avidly recognized as a key property of software modularity. Ideally, a software module is considered to be cohesive if it represents an abstraction of a single concern of the software. Modules with several concerns may be harder to understand because developers must mentally separate the source code related to each concern. Also, modules implementing several concerns are more likely to undergo changes as much as distinct development tasks may target its different concerns. The most well-known cohesion metrics are defined in terms of the syntactical structure of a module, and as a consequence fail to capture the amount of concerns realized by the module. In this context, we investigated the potential of a new metric, called Lack of Concern-based Cohesion. This metric explicitly counts the number of concerns realized by each module. We compared this metric with other five structural cohesion metrics by applying them over six open source software systems. We studied how those metrics are associated with module changes by mining over 16,000 repository revisions. Our results pointed out that the concern-based metric captured a cohesion dimension that is not reflected by structural metrics, and, as a consequence, adds to the association of cohesion and change-proneness.}, 
keywords={public domain software;software metrics;software quality;concern based cohesion;hidden dimension;cohesion measurement;software modularity;software module;source code;distinct development;cohesion metrics;syntactical structure;open source software systems;cohesion dimension;structural metrics;software quality;Measurement;Correlation;Java;Software;Principal component analysis;History;Libraries;cohesion;software metrics;concern-driven metrics;maintainability;change-proneness}, 
doi={10.1109/ICPC.2012.6240476}, 
ISSN={1092-8138}, 
month={June},}
@INPROCEEDINGS{7968059, 
author={J. Campinhos and J. C. Seco and J. Cunha}, 
booktitle={2017 IEEE/ACM 2nd International Workshop on Variability and Complexity in Software Design (VACE)}, 
title={Type-Safe Evolution of Web Services}, 
year={2017}, 
volume={}, 
number={}, 
pages={20-26}, 
abstract={Applications based on micro or web services have had significant growth due to the exponential increase in the use of mobile devices. However, using such kind of loosely coupled interfaces provides almost no guarantees to the developer in terms of evolution. Changes to service interfaces can be introduced at any moment, which may cause the system to fail due to mismatches between communicating parts. In this paper, we present a programming model that allows the development of web service applications, server end-points and their clients, in such a way that the evolution of services' implementation does not cause the disruption of the client. Our approach is based on a type based code slicing technique that ensures that each version only refers to type compatible code, of the same version or of a compatible version, and that each client request is redirected to the most recent type compatible version implemented by the server. We abstract the notion of version and parametrize type compatibility on the relation between versions. The relation between versions is tagged with compatibility levels, so to capture the common conventions used in software development. Our implementation allows multiple versions of a service to be deployed simultaneously, while reusing code between versions in a type safe way. We describe a prototype framework, based on code transformation, for server-side JavaScript code, and using Flow as verification tool.}, 
keywords={Java;mobile computing;program slicing;program verification;software reusability;Web services;type-safe evolution;Web service applications;mobile devices;loosely coupled interfaces;communicating parts;server end-points;type based code slicing technique;compatibility levels;software development;code reuse;prototype framework;code transformation;server-side JavaScript code;verification tool;Servers;Web services;Context;Dispatching;Prototypes;API evolution;web services;type safe;JavaScript}, 
doi={10.1109/VACE.2017.6}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7962382, 
author={J. Cito and G. Schermann and J. E. Wittern and P. Leitner and S. Zumberi and H. C. Gall}, 
booktitle={2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)}, 
title={An Empirical Analysis of the Docker Container Ecosystem on GitHub}, 
year={2017}, 
volume={}, 
number={}, 
pages={323-333}, 
abstract={Docker allows packaging an application with its dependencies into a standardized, self-contained unit (a so-called container), which can be used for software development and to run the application on any system. Dockerfiles are declarative definitions of an environment that aim to enable reproducible builds of the container. They can often be found in source code repositories and enable the hosted software to come to life in its execution environment. We conduct an exploratory empirical study with the goal of characterizing the Docker ecosystem, prevalent quality issues, and the evolution of Dockerfiles. We base our study on a data set of over 70000 Dockerfiles, and contrast this general population with samplings that contain the Top-100 and Top-1000 most popular Docker-using projects. We find that most quality issues (28.6%) arise from missing version pinning (i.e., specifying a concrete version for dependencies). Further, we were not able to build 34% of Dockerfiles from a representative sample of 560 projects. Integrating quality checks, e.g., to issue version pinning warnings, into the container build process could result into more reproducible builds. The most popular projects change more often than the rest of the Docker population, with 5.81 revisions per year and 5 lines of code changed on average. Most changes deal with dependencies, that are currently stored in a rather unstructured manner. We propose to introduce an abstraction that, for instance, could deal with the intricacies of different package managers and could improve migration to more light-weight images.}, 
keywords={software engineering;source code (software);docker container ecosystem;GitHub;software development;dockerfile evolution;docker-using projects;Containers;Software;Ecosystems;Tools;Computer languages;Sociology;Statistics;empirical software engineering;GitHub;Docker}, 
doi={10.1109/MSR.2017.67}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7023341, 
author={H. Jing and S. Lin}, 
booktitle={2014 IEEE International Conference on Data Mining}, 
title={Neural Conditional Energy Models for Multi-label Classification}, 
year={2014}, 
volume={}, 
number={}, 
pages={240-249}, 
abstract={Multi-label classification (MLC) is a type of structured output prediction problems where a given instance can be associated to more than one labels at a time. From the probabilistic point of view, a model predicts a set of labels y given an input vector v by learning a conditional distribution p(y|v). This paper presents a powerful model called a Neural Conditional Energy Model (NCEM) to solve MLC. The model can be viewed as a hybrid deterministic-stochastic network of which we use a deterministic neural network to transform the input data, before contributing to the energy landscape of v, y, and a single stochastic hidden layer h. Non-linear transformation given by the neural network makes our model more expressive and more capable of capturing complex relations between input and output, and using deterministic neurons facilitates exact inference. We present an efficient learning algorithm that is simple to implement. We conduct extensive experiments on 15 real-world datasets from wide variety of domains with various evaluation metrics to confirm that NCEM is significantly superior to current state-of-the-art models most of the time based on pair-wise t-test at 5% significance level. The MATLAB source code to replicate our experiments are available at https://github.com/Kublai-Jing/NCEM.}, 
keywords={learning (artificial intelligence);mathematics computing;neural nets;pattern classification;probability;stochastic processes;vectors;neural conditional energy models;multilabel classification;MLC;probabilistic point of view;vector;conditional distribution;NCEM;deterministic neural network;stochastic hidden layer;nonlinear transformation;Matlab source code;Training;Stochastic processes;Computational modeling;Vectors;Mathematical model;Data models;Correlation;Multi-Label Classification;Probabilistic Modeling}, 
doi={10.1109/ICDM.2014.39}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{6418284, 
author={M. Blaško and P. Raschman}, 
booktitle={2012 IEEE 10th International Conference on Emerging eLearning Technologies and Applications (ICETA)}, 
title={Management of the Quality of Teaching at Universities - a course for teachers at the Technical University of Kosice developed within the operational programme “Education”}, 
year={2012}, 
volume={}, 
number={}, 
pages={45-48}, 
abstract={The aim of this contribution is to provide the basic information concerning the course “Management of the Quality of Teaching at Universities”. This course was prepared within the framework of the operational programme Education “Package of Innovative Elements for the Transformation of Education at TUKE” (code ITMS 26110230018). In recent weeks we finished the implementation of the pilot course that was focused to the development of professional skills of teachers and to the enhancement of quality education at TUKE (Technical University of Kosice).}, 
keywords={computer aided instruction;educational courses;educational institutions;quality management;teaching;teaching quality management;universities;teacher course;Technical University of Kosice;operational programme;education programme;innovative elements;TUKE;education transformation;teacher professional skill development;Educational institutions;Electronic learning;Quality management;Engineering profession;Seminars;Materials;modern educational process;program of quality teaching;teacher - the manager of the quality teaching;system of management of quality teaching;e-learning}, 
doi={10.1109/ICETA.2012.6418284}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6613048, 
author={T. Lavoie and E. Merlo}, 
booktitle={2013 7th International Workshop on Software Clones (IWSC)}, 
title={How much really changes? A case study of firefox version evolution using a clone detector}, 
year={2013}, 
volume={}, 
number={}, 
pages={83-89}, 
abstract={This paper focuses on the applicability of clone detectors for system evolution understanding. Specifically, it is a case study of Firefox for which the development release cycle changed from a slow release cycle to a fast release cycle two years ago. Since the transition of the release cycle, three times more versions of the software were deployed. To understand whether or not the changes between the newer versions are as significant as the changes in the older versions, we measured the similarity between consecutive versions.We analyzed 82MLOC of C/C++ code to compute the overall change distribution between all existing major versions of Firefox. The results indicate a significant decrease in the overall difference between many versions in the fast release cycle. We discuss the results and highlight how differently the versions have evolved in their respective release cycle. We also relate our results with other results assessing potential changes in the quality of Firefox. We conclude the paper by raising questions on the impact of a fast release cycle.}, 
keywords={online front-ends;Firefox version evolution;clone detector;82MLOC;C/C++ code;software evolution;Software;Cloning;Detectors;Vectors;Approximation methods;Indexes;Servers;clone detection;software evolution;firefox}, 
doi={10.1109/IWSC.2013.6613048}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7980492, 
author={R. M. N. Hemal and K. G. A. S. Waidyasekara and E. M. A. C. Ekanayake}, 
booktitle={2017 Moratuwa Engineering Research Conference (MERCon)}, 
title={Integrating design freeze in to large-scale construction projects in Sri Lanka}, 
year={2017}, 
volume={}, 
number={}, 
pages={259-264}, 
abstract={Change is inevitable in most construction projects due to uniqueness of projects and limited resources. Previous studies revealed that design changes are accountable for 78% of the total cost deviation in construction projects. Design Freeze (DF) is one method used during design development stage to mitigate the risks associated with change. This organizes and complies the design process, control changes, and force the completion of design stages on time. However, this is a novel concept to the construction industry. Hence, this study focused on investigating the DF concept in terms of its adoptability in large-scale construction projects to enhance project performance by minimizing changes. The present study adopted a mixed approach where the questionnaire survey and expert interviews helped to derive findings. Questionnaire survey was administered to hundred (100) industry professionals with a response rate of 87%. Further, an expert opinion survey was performed with nine subject matter experts. The quantitative data were analysed using SPSS software, whereas code based content analysis using Nvivo version 11 supported to analyse qualitative data. The findings revealed that the client and the consultant are the major parties causing design changes. Selecting an experienced and qualified design team, permitting adequate time frame for design development, and etc. are the significant factors to be considered when implementing a DF. Furthermore, the study identified the impracticability of adopting a 100% design freeze in large-scale construction projects, and the interviewees accepted the concept of partial design freeze by identifying strategies targeted to maximize the design percentage.}, 
keywords={design engineering;management of change;project engineering;strategic planning;structural engineering;design freeze;construction projects;Sri Lanka;design development stage;risk mitigation;SPSS software;change management;Buildings;Construction industry;Interviews;Tools;Schedules;Economics;Design Change;Design Freeze;Construction Industry}, 
doi={10.1109/MERCon.2017.7980492}, 
ISSN={}, 
month={May},}
@ARTICLE{6471255, 
author={D. M. Coleman and D. R. Feldman}, 
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
title={Porting Existing Radiation Code for GPU Acceleration}, 
year={2013}, 
volume={6}, 
number={6}, 
pages={2486-2491}, 
abstract={Graphics processing units (GPUs) have proven very robust architectures for performing intensive scientific calculations, resulting in speedups as high as several hundred times. In this paper, the GPU acceleration of a radiation code for use in creating simulated satellite observations of predicted climate change scenarios is explored, particularly the prospect of porting an already existing and widely used radiation transport code to a GPU version that fully exploits the parallel nature of GPUs. The porting process is attempted with a simple radiation code, revealing that this process centers on creating many copies of variables and inlining function/subroutine calls. A resulting speedup of about 25x is reached. This is less than the speedup achieved from a radiation code built for CUDA from scratch, but it was achieved with an already existing radiation code using the PGI Accelerator to automatically generate CUDA kernels, and this demonstrates a possible strategy to speed up other existing models like MODTRAN and LBLRTM.}, 
keywords={atmospheric techniques;geophysics computing;graphics processing units;LBLRTM model;MODTRAN model;CUDA kernels;PGI accelerator;subroutine call;function call;radiation transport code;climate change scenarios;simulated satellite observations;intensive scientific calculations;graphics processing units;GPU acceleration;Graphics processing units;Computational modeling;Meteorology;Atmospheric modeling;Acceleration;Mathematical model;Runtime;Atmospheric modeling;computer graphics;hyperspectral sensors;low earth orbit satellites}, 
doi={10.1109/JSTARS.2013.2247379}, 
ISSN={1939-1404}, 
month={Dec},}
@INPROCEEDINGS{6787253, 
author={A. Acosta and F. Almeida}, 
booktitle={2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing}, 
title={Performance Analysis of Paralldroid Generated Programs}, 
year={2014}, 
volume={}, 
number={}, 
pages={60-67}, 
abstract={The advent of emergent System-on-Chip (SoCs) and multiprocessor System-on-Chip (MPSocs) opens a new era on the small mobile devices (Smartphones, Tablets, ...) in terms of computing capabilities and applications to be addressed. The efficient use of such devices, including the parallel power, is still a challenge for general purpose programmers due to the very high learning curve demanding very specific knowledge of the devices. While some efforts are currently being made, mainly in the scientific scope, the scenario is still quite far from being the desirable for non-scientific applications where very few of them take advantage of the parallel capabilities of the devices. We develop a performance analysis in several SoCs using Paralldroid. Paralldroid (Framework for Parallelism in Android), is a parallel development framework oriented to general purpose programmers for standard mobile devices. Paralldroid presents a programming model that unifies the different programming models of Android. The user just implements a Java application and introduces a set of Paralldroid annotations in the sections of code to be optimized. The Paralldroid system automatically generates the native C, OpenCL or Renderscript code for the annotated section. The Paralldroid transformation model involves source-to-source transformations and skeletal programming.}, 
keywords={Android (operating system);Java;multiprocessing systems;parallel programming;program compilers;software performance evaluation;system-on-chip;skeletal programming;source-to-source transformations;Paralldroid transformation model;OpenCL code generation;native C code generation;Renderscript code generation;Java application;programming model;parallel development framework;parallelism in Android framework;nonscientific applications;general purpose programmers;mobile devices;MPSoC;multiprocessor system-on-chip;SoC;Paralldroid generated programs;performance analysis;Java;Androids;Humanoid robots;Programming;Graphics processing units;Gray-scale;Smart phones;Renderscript;source-to-source transformation;Android}, 
doi={10.1109/PDP.2014.14}, 
ISSN={1066-6192}, 
month={Feb},}
@INPROCEEDINGS{6525110, 
author={Y. Tawk and J. Costantine and E. Al-Zuraiqi and C. G. Christodoulou}, 
booktitle={2013 US National Committee of URSI National Radio Science Meeting (USNC-URSI NRSM)}, 
title={Cognitive radio antennas that learn and adapt using Neural Networks}, 
year={2013}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Cognition added to RF/antenna systems has extended software defined radio (SDR) communication systems into cognitive radio systems. Software defined radio has been established as a key enabling technology to realize cognitive radio. Thus a cognitive radio is an SDR that is aware of its environment, and autonomously adjusts its operations to achieve the designated objectives. A cognitive radio system is able to sense, reason, learn and be aware of its environment. A dynamic communication application such as cognitive radio requires antenna researchers to design software controlled reconfigurable antennas. The tuning ability of such antennas and the switching time are important to satisfy the requirements of continuously changing communication channels. Neural Networks (NNs) arose as a perfect candidate to control these antennas through Field Programmable Gate Arrays (FPGAs). NNs represent a perfect solution to add learning and reasoning to the cognitive radio antenna systems. In this work, a NN is applied on a reconfigurable antenna where switches are used to connect and disconnect the different parts of its structure. Reconfigurable antennas are potential candidate for cognitive radio since they are able to change their operating characteristics based on the channel activity. Applying NNs to such antennas result in the association of different antenna configurations with the various frequency responses. This association allows training the NN to be able to configure the antenna and regenerate switch combinations/frequency responses on demand. The NN is built and trained in Matlab Simulink and a Xilinx system generator creates the NN VHDL code to be transferred to the FPGA. The FPGA now controls the switches that are incorporated within the reconfigurable antenna structure. The application of NN on cognitive radio antenna systems allows such systems to react swiftly to any change in their environment. The cognitive radio antennas will regenerate the appropriate switch combinations using NN previous training. This will allow communicating over the unoccupied parts of the spectrum which are called white spaces. The dynamic changes that occur in the spectrum require a robust and fast antenna software control. Thus NN prove to be a valid and necessary technique to employ on CR antennas.}, 
keywords={cognitive radio;field programmable gate arrays;frequency response;hardware description languages;learning (artificial intelligence);neural nets;software radio;wireless channels;FPGA;VHDL code;Xilinx system generator;frequency response;channel activity;reasoning;learning;field programmable gate array;software controlled reconfigurable antenna;dynamic communication;SDR;software defined radio;RF-antenna system;neural network;cognitive radio antenna system;Cognitive radio;Artificial neural networks;Field programmable gate arrays;Adaptive arrays;Educational institutions}, 
doi={10.1109/USNC-URSI-NRSM.2013.6525110}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{6365977, 
author={V. Jevremovic and S. Petrovski}, 
booktitle={2012 18th International Conference on Virtual Systems and Multimedia}, 
title={MUZZEUM — Augmented Reality and QR codes enabled mobile platform with digital library, used to Guerrilla open the National Museum of Serbia}, 
year={2012}, 
volume={}, 
number={}, 
pages={561-564}, 
abstract={Serbian National Museum has been closed for past 10 years for renovation. Furthermore, certain inadequate actions of the Government and national cultural institutions have been made. A group of activists decided to make a statement and perform a virtual exhibition in front of the National museum by using QR codes and Augmented Reality. This paper focuses on technical and social implications of consumption and implementation of QR codes and Augmented Reality in terms of museology, cultural heritage, objects and relationship they create with people who are their potential consumers. In the case study, we are tackling here, we investigate the transformation in definitions of space, artifacts, visitor and the power of representation in terms of museology. Since the artifacts which represent the Serbian national heritage of the highest rank are being held in museum depots far from the eyes of the public for such a long time, the project team has produced QR codes which will bring some of the masterpieces back to its audience. The objects from different museums will be represented by special ARQR codes outside of the museum building, reachable by smartphones. This socially engaged project sends direct message to the wider public that usage of IT, mobile technologies, Augmented Reality and QR codes can potentially transform the way we communicate cultural heritage.}, 
keywords={augmented reality;codes;digital libraries;exhibitions;government;history;Internet;mobile computing;museums;smart phones;MUZZEUM project;augmented reality;QR codes;digital library;guerrilla open;museum renovation;government;national cultural institutions;virtual exhibition;museology;cultural heritage;Serbian national heritage;museum depots;masterpieces;ARQR codes;museum building;smartphones;IT;mobile technology;Internet;National Museum of Serbia;Cultural differences;Augmented reality;Mobile communication;Libraries;Internet;Global communication;Art;MUZZEUM;Augmented;Reality;QR;Code;Cultural;Heritage;National;Museum;Serbia;Mobile;Internet;3D;objects}, 
doi={10.1109/VSMM.2012.6365977}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7058047, 
author={Wenrong Zeng and X. Chen and Hong Cheng}, 
booktitle={2014 International Conference on Data Science and Advanced Analytics (DSAA)}, 
title={Pseudo labels for imbalanced multi-label learning}, 
year={2014}, 
volume={}, 
number={}, 
pages={25-31}, 
abstract={The classification with instances which can be tagged with any of the 2<sup>L</sup> possible subsets from the predefined L labels is called multi-label classification. Multi-label classification is commonly applied in domains, such as multimedia, text, web and biological data analysis. The main challenge lying in multi-label classification is the dilemma of optimising label correlations over exponentially large label powerset and the ignorance of label correlations using binary relevance strategy (1-vs-all heuristic). The classification with label powerset usually encounters with highly skewed data distribution, called imbalanced problem. While binary relevance strategy reduces the problem from exponential to linear, it totally neglects the label correlations. In this artical, we propose a novel strategy of introducing Balanced Pseudo-Labels (BPL) which build more robust classifiers for imbalanced multi-label classification, which embeds imbalanced data in the problems innately. By incorporating the new balanced labels we aim to increase the average distances among the distinct label vectors. In this way, we also code the label correlation implicitly in the algorithm. Another advantage of the proposed method is that it can combined with any classifier and it is proportional to linear label transformation. In the experiment, we choose five multi-label benchmark data sets and compare our algorithm with the most state-of-art algorithms. Our algorithm outperforms them in standard multi-label evaluation in most scenarios.}, 
keywords={learning (artificial intelligence);pattern classification;pseudo label;multilabel learning;multilabel classification;label correlation;label powerset;binary relevance strategy;data distribution;imbalanced problem;balanced pseudo-label;BPL;robust classifier;imbalanced data;label vector;linear label transformation;multilabel benchmark data set;Vectors;Correlation;Prediction algorithms;Kernel;Power line communications;Classification algorithms;Linear programming}, 
doi={10.1109/DSAA.2014.7058047}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6962349, 
author={A. H. Ashouri and G. Mariani and G. Palermo and C. Silvano}, 
booktitle={2014 IEEE 12th Symposium on Embedded Systems for Real-time Multimedia (ESTIMedia)}, 
title={A Bayesian network approach for compiler auto-tuning for embedded processors}, 
year={2014}, 
volume={}, 
number={}, 
pages={90-97}, 
abstract={The complexity and diversity of today's architectures require an additional effort from the programmers in porting and tuning the application code across different platforms. The problem is even more complex when considering that also the compiler requires some tuning, since standard optimization options have been customized for specific architectures or designed for the average case. This paper proposes a machine-learning approach for reducing the cost of the compiler auto-tuning phase and to speedup the application performance in embedded architectures. The proposed framework is based on an application characterization done dynamically with microarchitecture independent features and based on the usage of Bayesian Networks. The main characteristic of the Bayesian Network approach consists of not describing the solution as a strict set of compiler transformations to be applied, but as a complex probability distribution function to be sampled. Experimental results, carried out on an ARM platform and GCC transformation space, proved the effectiveness of the proposed methodology for the selected benchmarks. The selected set of solutions (less than 10% of the search space) demonstrated to be very close to the optimal sequence of transformations, showing also an applications performance speedup up to 2.8 (1.5 on average) with respect to -O2 and -O3 for the cBench suite. Additionally, the proposed method demonstrated a 3× speedup in terms of search time with respect to an iterative compilation approach, given the same quality of the solutions1.}, 
keywords={belief networks;computer architecture;electronic engineering computing;embedded systems;learning (artificial intelligence);microprocessor chips;optimisation;program compilers;statistical distributions;Bayesian network;compiler auto-tuning;embedded processors;application code porting;application code tuning;standard optimization options;machine-learning approach;cost reduction;embedded architectures;microarchitecture;compiler transformations;complex probability distribution function;ARM platform;GCC transformation space;Optimization;Bayes methods;Probability distribution;Training;Vectors;Program processors;Network topology}, 
doi={10.1109/ESTIMedia.2014.6962349}, 
ISSN={2325-1271}, 
month={Oct},}
@INPROCEEDINGS{8099683, 
author={Y. Jeon and J. Kim}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Active Convolution: Learning the Shape of Convolution for Image Classification}, 
year={2017}, 
volume={}, 
number={}, 
pages={1846-1854}, 
abstract={In recent years, deep learning has achieved great success in many computer vision applications. Convolutional neural networks (CNNs) have lately emerged as a major approach to image classification. Most research on CNNs thus far has focused on developing architectures such as the Inception and residual networks. The convolution layer is the core of the CNN, but few studies have addressed the convolution unit itself. In this paper, we introduce a convolution unit called the active convolution unit (ACU). A new convolution has no fixed shape, because of which we can define any form of convolution. Its shape can be learned through backpropagation during training. Our proposed unit has a few advantages. First, the ACU is a generalization of convolution, it can define not only all conventional convolutions, but also convolutions with fractional pixel coordinates. We can freely change the shape of the convolution, which provides greater freedom to form CNN structures. Second, the shape of the convolution is learned while training and there is no need to tune it by hand. Third, the ACU can learn better than a conventional unit, where we obtained the improvement simply by changing the conventional convolution to an ACU. We tested our proposed method on plain and residual networks, and the results showed significant improvement using our method on various datasets and architectures in comparison with the baseline. Code is available at https://github.com/jyh2986/Active-Convolution.}, 
keywords={backpropagation;computer vision;convolution;image classification;image processing;learning (artificial intelligence);neural nets;deep learning;shape learning;CNN;ACU;backpropagation;active convolution unit;convolutional neural networks;image classification;Convolution;Shape;Neurons;Interpolation;Lattices;Convolutional codes}, 
doi={10.1109/CVPR.2017.200}, 
ISSN={1063-6919}, 
month={July},}
@INPROCEEDINGS{7498272, 
author={M. R. Anderson and M. Cafarella}, 
booktitle={2016 IEEE 32nd International Conference on Data Engineering (ICDE)}, 
title={Input selection for fast feature engineering}, 
year={2016}, 
volume={}, 
number={}, 
pages={577-588}, 
abstract={The application of machine learning to large datasets has become a vital component of many important and sophisticated software systems built today. Such trained systems are often based on supervised learning tasks that require features, signals extracted from the data that distill complicated raw data objects into a small number of salient values. A trained system's success depends substantially on the quality of its features. Unfortunately, feature engineering-the process of writing code that takes raw data objects as input and outputs feature vectors suitable for a machine learning algorithm-is a tedious, time-consuming experience. Because “big data” inputs are so diverse, feature engineering is often a trial-and-error process requiring many small, iterative code changes. Because the inputs are so large, each code change can involve a time-consuming data processing task (over each page in a Web crawl, for example). We introduce Zombie, a data-centric system that accelerates feature engineering through intelligent input selection, optimizing the “inner loop” of the feature engineering process. Our system yields feature evaluation speedups of up to 8× in some cases and reduces engineer wait times from 8 to 5 hours in others.}, 
keywords={data handling;feature selection;learning (artificial intelligence);input selection;fast feature engineering;machine learning;supervised learning tasks;Big Data;data processing task;Zombie data-centric system;Feature extraction;Indexes;Data processing;Training;Learning systems;Data mining;Monitoring}, 
doi={10.1109/ICDE.2016.7498272}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7489879, 
author={S. Wei and Gaoxiang Wu and Ziyang Zhou and L. Yang}, 
booktitle={2015 IEEE International Conference on Progress in Informatics and Computing (PIC)}, 
title={Mining network traffic for application category recognition on Android platform}, 
year={2015}, 
volume={}, 
number={}, 
pages={409-413}, 
abstract={Signature-based static mobile malware detection is fragile when facing code obfuscation and transformation attacks. Behavior based malware detection mechanisms have been widely studied and experimented. So far only the application's running behaviors, such as API calls and resource consumption are used, which can also be easily concealed and obfuscated with various coding tricks. Most mobile malware need either cellular or network connection to conduct their malicious activities. We propose to monitor an application's network behavior and interaction to characterize application behaviors. An integrated testbed system has been designed and prototyped for such network behavior collection. Statistical features are derived from application network traffic, which are further fed to a machine-learning based classifier to build one general model for each typical category of mobile applications. Experiments show that applications in each category with identical functionality exhibit similar network behaviors, which makes it possible to use the derived category model of network behaviors to evaluate future unknown application for its trustworthiness.}, 
keywords={Android (operating system);application program interfaces;data mining;digital signatures;invasive software;learning (artificial intelligence);mobile computing;pattern classification;trusted computing;network traffic mining;application category recognition;Android platform;signature-based static mobile malware detection;code obfuscation attacks;code transformation attacks;behavior based malware detection;API calls;resource consumption;application network behavior;integrated testbed system;statistical features;machine-learning based classifier;trustworthiness;Portals;Monitoring;Internet;Firewalls (computing);Malware;Privacy;Android;malware;network behavior}, 
doi={10.1109/PIC.2015.7489879}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6645276, 
author={S. K. Kuttal}, 
booktitle={2013 IEEE Symposium on Visual Languages and Human Centric Computing}, 
title={Variation support for end users}, 
year={2013}, 
volume={}, 
number={}, 
pages={183-184}, 
abstract={End-user programming environments provide central repositories where users can execute and store their programs. However, these environments do not provide facilities by which users can keep track of the variations that they create for their programs. In the professional world, software developers use variation management for code reuse, program understanding, change traceability, debugging and maintenance.}, 
keywords={program debugging;software maintenance;software reusability;user interfaces;variation support;end-user programming environments;central repositories;software development;code reuse;program understanding;change traceability;debugging;maintenance;Debugging;Mashups;Computer bugs;Programming;History;Visualization}, 
doi={10.1109/VLHCC.2013.6645276}, 
ISSN={1943-6092}, 
month={Sep.},}
@INPROCEEDINGS{8506535, 
author={M. Ichinco}, 
booktitle={2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
title={A Vision for Interactive Suggested Examples for Novice Programmers}, 
year={2018}, 
volume={}, 
number={}, 
pages={303-304}, 
abstract={Many systems aim to support programmers within a programming context, whether they recommend API methods, example code, or hints to help novices solve a task. The recommendations may change based on the user's code context, history, or the source of the recommendation content. They are designed to primarily support users in improving their code or working toward a task solution. The recommendations themselves rarely provide support for a user to interact with them directly, especially in ways that benefit the knowledge or understanding of the user. This poster presents a vision and preliminary designs for three ways a user might learn from interactions with suggested examples: describing examples, providing detailed relevance feedback, and selective visualization and tinkering.}, 
keywords={application program interfaces;computer science education;data visualisation;relevance feedback;software engineering;preliminary designs;interactive suggested examples;novice programmers;support programmers;programming context;API methods;example code;code context;recommendation content;support users;task solution;selective visualization;Visualization;Fading channels;Task analysis;Programming profession;Problem-solving;Human factors;Novice programmers;interactive suggestions;example code}, 
doi={10.1109/VLHCC.2018.8506535}, 
ISSN={1943-6106}, 
month={Oct},}
@ARTICLE{7526285, 
author={N. G. Bingel III and C. C. Bleakley and A. L. Clapp and J. H. Dagenhart and M. A. Konz and L. E. Gaunt and M. B. Gunter and M. F. Jurgemeyer}, 
journal={National Electrical Safety Code (NESC) Handbook, Eighth Edition}, 
title={2017 NESC(R) Handbook, Eighth Edition}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-428}, 
abstract={The 2017 NESC Handbook, Eight Edition, a Discussion of the National Electrical Safety Code(R), is an essential companion to the Code. This handbook includes text directly from Code which provides users an easy reference back to the code, ruling-by-ruling. It gives users insight into what lies behind the NESC(R)s rules and how to apply them. The Handbook was developed for use at many levels in the electric and communication industries, including those involved in system design, construction, maintenance, inspection, standards development and worker training. The Handbook also discusses how the NESC Committee has developed the rules in the Code and responded to change proposals during the past 100 years. This allows users to understand how questions they may have were dealt with in the past. These are key points from the 2017 Handbook Edition:- Revising the purpose rule to include only the safeguarding of persons and utility facilities and clarifying the application- Deleting unused definitions and adding definitions for communication and supply space.- Revising the substation impenetrable fence requirements.- Adding an exception to exempt underground cable grounding requirements from the 4 grounds in each mile rule under certain conditions.- Revising and reorganizing the guy insulator placement rules along with eliminating the voltage transfer requirements associated with them.- Requiring a 40 vertical clearance from communication cables in the communication space if a luminaire is not effectively grounded.- Deleting the conductance requirement for underground insulating jacketed grounded neutral supply cables and revising the grounding and bonding rules for supply and communication cables in random separation installations.- Revising and reorganizing the Grades of Construction Table 242-1 that will now include service drops.- Revising the strength rules to require that all conductors be considered for damage due to Aeolian vibration.- Revising the rules in Part 4 to align with changes made to 29 CFR by the Occupational Safety and Health Administration (OSHA).}, 
keywords={ANSI Standards;Electricity supply industry;Electrical safety;Voltage control;Power transmission lines;National electric code;communications industry safety;construction of communication lines;construction of electric supply lines;electrical safety;electric supply stations;electric utility stations;high-voltage safety;operation of communications systems;operation of electric supply systems;power station equipment;power station safety;public utility safety;safety work rules;underground communication line safety;underground electric line safety.}, 
doi={10.1109/IEEESTD.2016.7526285}, 
ISSN={}, 
month={Aug},}
@ARTICLE{7526288, 
author={}, 
journal={2017 NESC(R) Handbook, Premier Edition}, 
title={2017 NESC(R) Handbook, Premier Edition}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-711}, 
abstract={The 2017 NESC(R) Handbook, Premier Edition, is an essential companion to the Code. This handbook includes text directly from Code which provides users an easy reference back to the code, ruling-by-ruling. It gives users insight into what lies behind the NESC's rules and how to apply them. The Handbook was developed for use at many levels in the electric and communication industries, including those involved in system design, construction, maintenance, inspection, standards development and worker training. The Handbook also discusses how the NESC Committee has developed the rules in the Code and responded to change proposals during the past 100 years. This allows users to understand how questions they may have were dealt with in the past. The Premier Edition includes: Revising the purpose rule to include only the safeguarding of persons and utility facilities and clarifying the application rules. Deleting unused definitions and adding definitions for communication and supply space. Revising the substation impenetrable fence requirements. Adding an exception to exempt underground cable grounding requirements from the 4 grounds in each mile rule under certain conditions. Revising and reorganizing the guy insulator placement rules along with eliminating the voltage transfer requirements associated with them. Requiring a 40 vertical clearance from communication cables in the communication space if a luminaire is not effectively grounded. Deleting the conductance requirement for underground insulating jacketed grounded neutral supply cables and revising the grounding and bonding rules for supply and communication cables in random separation installations. Revising and reorganizing the Grades of Construction Table 242-1 that will now include service drops. Revising the strength rules to require that all conductors be considered for damage due to Aeolian vibration. Revising the rules in Part 4 to align with changes made to 29 CFR by the Occupational Safety and Health Administration (OSHA).}, 
keywords={earthing;electricity supply industry;IEEE standards;underground cables;OSHA;Occupational Safety and Health Administration;Aeolian vibration;strength rules;service drops;Grades of Construction Table 242-1;random separation installations;bonding rules;grounding rules;underground insulating jacketed grounded neutral supply cables;conductance requirement;communication cables;vertical clearance;voltage transfer requirements;guy insulator placement rules;underground cable grounding requirements;substation impenetrable fence requirements;application rules;utility facilities safeguarding;purpose rule;NESC Committee;communication industries;electric industries;ruling-by-ruling;2017 NESC(R) Handbook Premier Edition;ANSI Standards;Electricity supply industry;Electrical safety;Voltage control;Power transmission lines;National electric code;communications industry safety;construction of communication lines;construction of electric supply lines;electrical safety;electric supply stations;electric utility stations;high-voltage safety;operation of communications systems;operation of electric supply systems;power station equipment;power station safety;public utility safety;safety work rules;underground communication line safety;underground electric line safety.}, 
doi={10.1109/IEEESTD.2016.7526288}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8226300, 
author={Jyoti and J. K. Chhabra}, 
booktitle={2017 2nd International Conference for Convergence in Technology (I2CT)}, 
title={Filtering of false positives from IR-based traceability links among software artifacts}, 
year={2017}, 
volume={}, 
number={}, 
pages={1111-1115}, 
abstract={Correlation among software artifacts (also known as traceability links) of object oriented software plays a vital role in its maintenance. These traceability links are being commonly identified through Information Retrieval (IR) based techniques. But, it has been found that the resulting links from IR contain many false positives and some complementary approaches have been suggested for the purpose. Still, it usually requires manual verification of links which is neither desirable nor reliable. This paper suggests a new technique which can automatically filter out the false positives links (between requirement and source code) from IR and thus can help in reducing dependence as well as incorrectness of manual verification process. The proposed approach works on the basis of finding correlations among classes using either structural or co-changed dependency or both. A threshold is selected as a cut off on computed dependency values, to accept the presence of structural and co-changed dependency each. Now the traceability links are verified using these dependencies. If atleast one of the structural or co-change information validates the link obtained from IR approach, then that link is selected as candidate link, otherwise removed. Different thresholds have been experimented and comparison of results obtained from IR and the proposed approach is done. The results show that precision increases for all values of thresholds. Further analysis of results indicates that threshold in the range of 0.3 to 0.5 give better results. Hence, the proposed approach can be used as complementary to other Improved IR approaches to filter out false positives.}, 
keywords={information retrieval;object-oriented programming;program diagnostics;program verification;software maintenance;information retrieval-based techniques;improved IR;IR approach;structural co-changed dependency;object oriented software;software artifacts;traceability links;Filtering;Correlation;Software;Tools;Information retrieval;Manuals;History;Requirement Traceability;Filtering Technique for false positives;Information Retrieval;Change History;Structural Information}, 
doi={10.1109/I2CT.2017.8226300}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{6578797, 
author={K. W. Hamlen}, 
booktitle={2013 IEEE International Conference on Intelligence and Security Informatics}, 
title={Stealthy software: Next-generation cyber-attacks and defenses}, 
year={2013}, 
volume={}, 
number={}, 
pages={109-112}, 
abstract={Weaponized software is the latest development in a decades-old battle of virus-antivirus co-evolution. Reactively adaptive malware and automated binary transformation are two recently emerging offensive and defensive (respectively) technologies that may shape future cyberwarfare weapons. The former intelligently learns and adapts to antiviral defenses fully automatically in the wild, while the latter applies code mutation technology to defense, transforming potentially dangerous programs into safe programs. These technologies and their roles within the landscape of malware attack and defense are examined and discussed.}, 
keywords={computer viruses;learning (artificial intelligence);military computing;program interpreters;weapons;stealthy software;next-generation Cyber-attacks;weaponized software;virus-antivirus coevolution;reactively adaptive malware;automated binary transformation;defensive technologies;offensive technologies;cyberwarfare weapons;intelligent learning;antiviral defenses;code mutation technology;potentially dangerous program transformation;safe programs;malware attack;malware defense;Malware;Software;Weapons;Monitoring;Safety;Payloads}, 
doi={10.1109/ISI.2013.6578797}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7841778, 
author={T. Shibahara and T. Yagi and M. Akiyama and D. Chiba and T. Yada}, 
booktitle={2016 IEEE Global Communications Conference (GLOBECOM)}, 
title={Efficient Dynamic Malware Analysis Based on Network Behavior Using Deep Learning}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Malware authors or attackers always try to evade detection methods to accomplish their mission. Such detection methods are broadly divided into three types: static feature, host-behavior, and network-behavior based. Static feature-based methods are evaded using packing techniques. Host- behavior-based methods also can be evaded using some code injection methods, such as API hook and dynamic link library hook. This arms race regarding static feature-based and host-behavior- based methods increases the importance of network-behavior-based methods. The necessity of communication between infected hosts and attackers makes it difficult to evade network-behavior- based methods. The effectiveness of such methods depends on how we collect a variety of communications by using malware samples. However, analyzing all new malware samples for a long period is infeasible. Therefore, we propose a method for determining whether dynamic analysis should be suspended based on network behavior to collect malware communications efficiently and exhaustively. The key idea behind our proposed method is focused on two characteristics of malware communication: the change in the communication purpose and the common latent function. These characteristics of malware communications resemble those of natural language from the viewpoint of data structure, and sophisticated analysis methods have been proposed in the field of natural language processing. For this reason, we applied the recursive neural network, which has recently exhibited high classification performance, to our proposed method. In the evaluation with 29,562 malware samples, our proposed method reduced 67.1% of analysis time while keeping the coverage of collected URLs to 97.9% of the method that continues full analyses.}, 
keywords={data structures;feature extraction;invasive software;learning (artificial intelligence);pattern classification;recurrent neural nets;classification performance;recursive neural network;natural language processing;data structure;malware communication;code injection methods;packing techniques;network-behavior detection method;host-behavior detection method;static feature detection method;malware detection methods;malware authors;deep learning;dynamic malware analysis;Malware;Feature extraction;Protocols;Neural networks;Servers;Tensile stress;IP networks}, 
doi={10.1109/GLOCOM.2016.7841778}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6407032, 
author={M. I. Afridi}, 
booktitle={2012 Fifth International Symposium on Computational Intelligence and Design}, 
title={Selection and Ranking of Optimal Routes through Genetic Algorithm in a Cognitive Routing System for Mobile Ad Hoc Network}, 
year={2012}, 
volume={1}, 
number={}, 
pages={507-510}, 
abstract={Genetic algorithm can be used for proper selection and ranking of all possible variable route addresses in mobile ad-hoc network (MANET). Ranking is based upon priority code of the links. a priority code is calculated by respective routing protocol, which depends on different parameters and metrics. a node can change its position and new nodes may join the MANET, so genetic algorithm can better estimate such kind of variations through its crossover and mutation genetic operators. Genetic algorithm is especially useful in cases of novel cognitive routing for MANET. Cognition in MANET is either based upon learning automata method as in some wireless sensor networks or specialized cognitive neural networks. Ranking of optimal links in MANET after a regular interval through genetic algorithm enhance the performance of cognitive routing. It help in proper selection of desired routing protocol for a given set of network conditions.}, 
keywords={automata theory;genetic algorithms;mobile ad hoc networks;routing protocols;optimal route selection;genetic algorithm;cognitive routing system;mobile ad-hoc network;variable route ranking;MANET;priority code;routing protocol;mutation genetic operators;crossover genetic operators;cognitive routing;learning automata method;wireless sensor networks;cognitive neural networks;Routing;Mobile ad hoc networks;Genetic algorithms;Routing protocols;Sociology;Statistics;Mobile communication;Reactive and proactive protocols;Hybrid routing system;Cognitive routing system;Mobile ad-hoc network (MANET);Neural network;Genetic algorithm;learning automata}, 
doi={10.1109/ISCID.2012.132}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8073777, 
author={R. K. Sungkur and V. Neermul and V. Tauckoor}, 
booktitle={2016 International Conference on Advances in Computing and Communication Engineering (ICACCE)}, 
title={Exploring the educational potential of QR codes}, 
year={2016}, 
volume={}, 
number={}, 
pages={368-373}, 
abstract={Education through the past years has changed the way teaching and learning is inculcated to the new generation. The rapid growth of technology has accelerated the change in teaching methods and has given birth to new methods like distance learning and online learning, where lectures notes and material are shared via educational platforms. This leaves traditional face to face lecture session to a discussion type session where tutor and student discuss about topics and do more class activity. Technology offers new ways in transferring data and information in large amounts very rapidly and also offers the accessibility not only to large information pools such as the internet but also offers access to online storage remotely accessible. One way education can benefit from this is to make use of Quick Response (QR) Code. With its potential, QR Codes can be exploited to change the ways both tutors and students interact in class. This can be achieved by combining Mobile technology, Quick Response Code technology and Cloud technology.}, 
keywords={computer aided instruction;computer science education;distance learning;Internet;mobile computing;QR codes;teaching;Mobile technology;Quick Response Code technology;Cloud technology;educational potential;online learning;lectures notes;educational platforms;lecture session;discussion type session;class activity;accessibility;information pools;online storage;teaching;distance learning;QR Codes;Education;Mobile communication;Business;Cloud computing;Bar codes;Mobile applications;Uniform resource locators;QR codes;education;educational platforms;mobile technology;cloud technology}, 
doi={10.1109/ICACCE.2016.8073777}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8330225, 
author={E. C. Neto and D. A. da Costa and U. Kulesza}, 
booktitle={2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={The impact of refactoring changes on the SZZ algorithm: An empirical study}, 
year={2018}, 
volume={}, 
number={}, 
pages={380-390}, 
abstract={SZZ is a widely used algorithm in the software engineering community to identify changes that are likely to introduce bugs (i.e., bug-introducing changes). Despite its wide adoption, SZZ still has room for improvements. For example, current SZZ implementations may still flag refactoring changes as bug-introducing. Refactorings should be disregarded as bug-introducing because they do not change the system behaviour. In this paper, we empirically investigate how refactorings impact both the input (bug-fix changes) and the output (bug-introducing changes) of the SZZ algorithm. We analyse 31,518 issues of ten Apache projects with 20,298 bug-introducing changes. We use an existing tool that automatically detects refactorings in code changes. We observe that 6.5% of lines that are flagged as bug-introducing changes by SZZ are in fact refactoring changes. Regarding bug-fix changes, we observe that 19.9% of lines that are removed during a fix are related to refactorings and, therefore, their respective inducing changes are false positives. We then incorporate the refactoring-detection tool in our Refactoring Aware SZZ Implementation (RA-SZZ). Our results reveal that RA-SZZ reduces 20.8% of the lines that are flagged as bug-introducing changes compared to the state-of-the-art SZZ implementations. Finally, we perform a manual analysis to identify change patterns that are not captured by the refactoring identification tool used in our study. Our results reveal that 47.95% of the analyzed bug-introducing changes contain additional change patterns that RA-SZZ should not flag as bug-introducing.}, 
keywords={program debugging;public domain software;software maintenance;software quality;bug-fix changes;Refactoring Aware SZZ Implementation;RA-SZZ;analyzed bug-introducing changes;SZZ algorithm;flag refactoring changes;Apache projects;refactoring-detection tool;Computer bugs;Tools;Java;History;Software algorithms;Prediction algorithms;Software systems;SZZ algorithm;refactoring;bug-introducing change;bug-fix change}, 
doi={10.1109/SANER.2018.8330225}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6400841, 
author={K. de Barbaro and C. M. Johnson and D. Forster and G. Littlewort and G. Deak}, 
booktitle={2012 IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL)}, 
title={Sensory-motor dynamics of mother-infant-object interactions: Longitudinal changes in micro-behavioral patterns across the first year}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-2}, 
abstract={Summary form only given. Infants rapidly develop the skills to coordinate attention to objects and people. In particular, the period 9-12 months is thought to bring a new ability to coordinate with and to share the objects of another persons' attention-like following the gaze of the mother or playing a ball tossing game together. This has been termed “triadic” or “you-me-it” attention and is thought to be the basis for future learning, including early language. The development of triadic attention was investigated in a longitudinal sample of 26 mother-infant dyads interacting with objects in a naturalistic setting. Video recordings of sessions when the infants were four, six, nine, and twelve months of age were analyzed frame-by-frame, coding multiple dimensions of mother and infant sensory-motor contact as they attended to one another and to a set of shared objects. Specifically, we coded targets of gaze and manual contact of both participants, noting all changes occurring at 10Hz. Additionally, we coded salient macro level features of the interaction, such as moments of imitation and games. Databases detailing the micro behaviors of the interaction are only beginning to be coded in developmental psychology. Our dataset is the first of its kind to be created in a naturalistic home environment with infants younger than a year. Previous accounts code at the level of the triad (e.g. jointly attending to a toy vs. infant solo toy play). This has led to the conclusion that the development of triadic attention is the result of a sudden, qualitative leap in infant cognitive abilities. By contrast, our analyses of changes in sensory motor dynamics of attending indicate that gradual shifts in sensory motor coordination lead to continuous changes in mother-infant-object interactions over the first year. Our analyses indicate that at four months, infants converged all modalities (gaze and two hands) to a single target. Across individual infants, such sensory-motor coupling decreased gradually over the first year of life. This change fundamentally alters the dyadic social interaction. Specifically, we found that the degree of decoupling was related to the infant's responses to mother's actions on toys. Young infants transitioned all of their modalities to toys manipulated by mom. Over the course of the first year, infants increasingly distributed their attention between toys manipulated by mother and toys in their own possession. This allows the infant to observe manipulations made by mom on one set of objects while simultaneously maintaining a high degree of sensory motor contact on with another object, upon which they can practice performing those same manipulations. We confirmed these relations quantitatively by first creating an index of “total modality time” spent attending to "already-available" versus "maternal bid" objects. This was a temporal summation of all infant modalities directed towards both kinds of objects, after the mother presented a new object. We modeled session by session changes in the distribution of attending with a beta model, indicating a gradual shift in relative proportions of attending across the first year. Split halves analyses indicated that within a single session, infants who showed higher proportions of sensory-motor coupling showed less mature patterns of responding to the maternal bids, whereas those who decoupled more resembled the profiles of older infants. Overall, our results indicate that triadic attention emerges not as a novel form of social-cognition, but as a continuous product of sensory-motor maturation . In particular, infants changing abilities to distribute attention between multiple targets allows for increasing complexity of mother-infant-object manipulations.}, 
keywords={cognition;mechanoception;paediatrics;psychology;sensory-motor dynamics;mother-infant-object interactions;microbehavioral patterns;playing;ball tossing game;learning;language;triadic attention development;mother-infant dyads;naturalistic setting;video recordings;frame-by-frame analysis;multiple dimension coding;infant sensory-motor contact;mother sensory-motor contact;coded salient macrolevel features;developmental psychology;naturalistic home environment;infant cognitive ability;sensory motor coordination;dyadic social interaction;total modality time;maternal bid objects;already-available objects;temporal summation;split halves analysis;social-cognition form;sensory-motor maturation;mother-infant-object manipulations;frequency 10 Hz;Moment methods;Games;Couplings;Cognition;Cognitive science;Educational institutions;Video recording}, 
doi={10.1109/DevLrn.2012.6400841}, 
ISSN={2161-9476}, 
month={Nov},}
@INPROCEEDINGS{7396406, 
author={I. U. Nisa and S. N. Ahsan}, 
booktitle={2015 International Conference on Open Source Systems Technologies (ICOSST)}, 
title={Fault prediction model for software using soft computing techniques}, 
year={2015}, 
volume={}, 
number={}, 
pages={78-83}, 
abstract={Faulty modules of any software can be problematic in terms of accuracy, hence may encounter more costly redevelopment efforts in later phases. These problems could be addressed by incorporating the ability of accurate prediction of fault prone modules in the development process. Such ability of the software enables developers to reduce the faults in the whole life cycle of software development, at the same time it benefits automation process, and reduces the overall cost and efforts of the software maintenance. In this paper, we propose to design fault prediction model by using a set of code and design metrics; applying various machine learning (ML) classifiers; also used transformation techniques for feature reduction and dealing class imbalance data to improve fault prediction model. The data sets were obtained from publicly available PROMISE repositories. The results of the study revealed that there was no significant impact on the ability to accurately predict the fault-proneness of modules by applying PCA in reducing the dimensions; the results were improved after balancing data by SMOTE, Resample techniques, and by applying PCA with Resample in combination. It has also been seen that Random Forest, Random Tree, Logistic Regression, and Kstar machine learning classifiers have relatively better consistency in prediction accuracy as compared to other techniques.}, 
keywords={data mining;learning (artificial intelligence);principal component analysis;program testing;random processes;regression analysis;sampling methods;software fault tolerance;software maintenance;software metrics;trees (mathematics);fault prediction model;soft computing technique;software faulty module;fault prone module;software development;software maintenance;transformation technique;feature reduction;class imbalance data;PCA;SMOTE;resample technique;random forest;random tree;logistic regression;Kstar machine learning classifier;Software;Measurement;Predictive models;Principal component analysis;Data models;Prediction algorithms;Data mining;Data mining;Software metrics;Machine learning;Fault prediction model;Class imbalance;Feature reduction}, 
doi={10.1109/ICOSST.2015.7396406}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6933517, 
author={Y. Xia and G. Yan and H. Zhang}, 
booktitle={2014 IEEE 5th International Conference on Software Engineering and Service Science}, 
title={Analyzing the significance of process metrics for TT amp;amp;C software defect prediction}, 
year={2014}, 
volume={}, 
number={}, 
pages={77-81}, 
abstract={In the existing studies on software prediction, the most proposed methods are usually assessed over the public datasets like NASA metrics data repository, which include a combination of code metrics merely. Obviously, the process metric is also one of the key factors that affect the defect-proneness of software modules. In this paper, life-cycle based management process metrics set and history change process metrics set have been proposed based on the characteristics of development process. In order to analyze the importance of these different metrics for predicting defects in aerospace tracking telemetry and control (TT&amp;C) software, an improved PSO optimized support vector machine algorithm (PSO-SVM) has been presented and took into application. The experiment results over the actual TT&amp;C projects suggest that the prediction performance can be significance improved if the 2 kinds of process metrics are included in the model.}, 
keywords={aerospace computing;particle swarm optimisation;software metrics;support vector machines;telemetry;aerospace TT&C software defect prediction;NASA metrics data repository;code metrics;software module defect-proneness;life-cycle based management process metrics set;history change process metrics set;aerospace tracking telemetry and control software;PSO optimized support vector machine algorithm;PSO-SVM;Software;Software metrics;Predictive models;Process control;History;Error analysis;defect prediction;process metrics;TT&C software;software metrics}, 
doi={10.1109/ICSESS.2014.6933517}, 
ISSN={2327-0594}, 
month={June},}
@INPROCEEDINGS{7358903, 
author={A. Jalal and S. Kamal and D. Kim}, 
booktitle={2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)}, 
title={Individual detection-tracking-recognition using depth activity images}, 
year={2015}, 
volume={}, 
number={}, 
pages={450-455}, 
abstract={In this paper, a depth camera-based novel approach for human activity recognition is presented using robust depth silhouettes context features and advanced Hidden Markov Models (HMMs). During HAR framework, at first, depth maps are processed to identify human silhouettes from noisy background by considering frame differentiation constraints of human body motion and compute depth silhouette area for each activity to track human movements in a scene. From the depth silhouettes context features, temporal frames information are computed for intensity differentiation measurements, depth history features are used to store gradient orientation change in overall activity sequence and motion difference features are extracted for regional motion identification. Then, these features are processed by Principal component analysis for dimension reduction and k-mean clustering for code generation to make better activity representation. Finally, we proposed a new way to model, train and recognize different activities using advanced HMM. Experimental results show superior recognition rate, resulting up to the mean recognition of 57.69% over the state of the art methods using IM-DailyDepthActivity dataset. In addition, MSRAction3D dataset also showed some promising results.}, 
keywords={feature extraction;hidden Markov models;image recognition;object detection;object tracking;pattern clustering;principal component analysis;individual detection-tracking-recognition;depth activity images;human activity recognition;robust depth silhouettes context features;advanced hidden Markov models;HMM;HAR framework;depth maps;human silhouettes;noisy background;frame differentiation constraints;human body motion;depth silhouette area;human movement tracking;temporal frames information;intensity differentiation measurements;depth history features;gradient orientation change;overall activity sequence;motion difference feature extraction;regional motion identification;principal component analysis;dimension reduction;k-mean clustering;code generation;activity representation;superior recognition rate;mean recognition;IM-DailyDepthActivity dataset;MSRAction3D dataset;Feature extraction;Hidden Markov models;Context;Training;History;Cameras;Videos;Human activity recognition;depth camera;spatiotemporal features;advanced HMM}, 
doi={10.1109/URAI.2015.7358903}, 
ISSN={}, 
month={Oct},}
@ARTICLE{8436444, 
author={M. M. Strout and M. Hall and C. Olschanowsky}, 
journal={Proceedings of the IEEE}, 
title={The Sparse Polyhedral Framework: Composing Compiler-Generated Inspector-Executor Code}, 
year={2018}, 
volume={106}, 
number={11}, 
pages={1921-1934}, 
abstract={Irregular applications such as big graph analysis, material simulations, molecular dynamics simulations, and finite element analysis have performance problems due to their use of sparse data structures. Inspector-executor strategies improve sparse computation performance through parallelization and data locality optimizations. An inspector reschedules and reorders data at runtime, and an executor is a transformed version of the original computation that uses the newly reorganized schedules and data structures. Inspector-executor transformations are commonly written in a domain-specific or even application-specific fashion. Significant progress has been made in incorporating such inspector-executor transformations into existing compiler transformation frameworks, thus enabling their use with compile-time transformations. However, composing inspector-executor transformations in a general way has only been done in the context of the Sparse Polyhedral Framework (SPF). Though SPF enables the general composition of such transformations, the resulting inspector and executor performance suffers due to missed specialization opportunities. This paper reviews the history and current state of the art for inspector-executor strategies and reviews how the SPF enables the composition of inspector-executor transformations. Further, it describes a research vision to combine this generality in SPF with specialization to achieve composable and high performance inspectors and executors, producing a powerful compiler framework for sparse matrix computations.}, 
keywords={data structures;program compilers;sparse matrices;sparse polyhedral framework;compiler-generated inspector-executor code;sparse data structures;inspector-executor strategies;inspector-executor transformations;compiler transformation frameworks;compile-time transformations;high performance inspectors;Sparse matrices;High performance computing;Runtime;Dynamic compiler;Optimization;Intermediate representations;irregular computations;program optimization and parallelization;sparse matrices}, 
doi={10.1109/JPROC.2018.2857721}, 
ISSN={0018-9219}, 
month={Nov},}
@ARTICLE{8126804, 
author={C. Bai and M. Li and T. Zhao and W. Wang}, 
journal={IEEE Access}, 
title={Learning Binary Descriptors for Fingerprint Indexing}, 
year={2018}, 
volume={6}, 
number={}, 
pages={1583-1594}, 
abstract={Fingerprint indexing is studied widely with the real-valued features, but few works focus on the binary feature descriptors, which are more appropriate to retrieve fingerprints efficiently in the largescale fingerprint database. In this paper, the binary fingerprint descriptor (BFD), which is an effective and discriminative binary feature representation for fingerprint indexing, is proposed based on minutia cylinder code (MCC). Specifically, we first analyze MCC to find that it has characteristics of the high dimensionality, redundancy, and quantization loss. Accordingly, we propose an optimization model to learn a feature-transformation matrix, resulting in dimensionality reduction and diminishing quantization loss. Meanwhile, we also incorporate the balance, independence, and similarity-preservation properties in this learning process. Eventually, a multi-index hashing-based fingerprint indexing scheme further accelerate the exact search in Hamming space. The experiments on numerous public databases show that the BFD is discriminative and compact and that the proposed approach is outstanding for fingerprint indexing.}, 
keywords={binary codes;feature extraction;fingerprint identification;image representation;indexing;largescale fingerprint database;binary fingerprint descriptor;feature-transformation matrix;dimensionality reduction;diminishing quantization loss;real-valued features;binary feature descriptors;Hamming space;multiindex hashing -based fingerprint indexing scheme;optimization model;MCC;BFD;Indexing;Quantization (signal);Fingerprint recognition;Optimization;Binary codes;Robustness;Binary fingerprint descriptor;fingerprint indexing;minutia cylinder code}, 
doi={10.1109/ACCESS.2017.2779562}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{6623716, 
author={P. Mersini and E. Sakkopoulos and A. Tsakalidis}, 
booktitle={IISA 2013}, 
title={APPification of hospital healthcare and data management using QRcodes}, 
year={2013}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In this work, we describe an integrated system, developed for use by the healthcare personnel within healthcare facilities, adapted to smartphones, tablets and handheld devices. Our key goal is to facilitate doctors, nurses and the involved personnel throughout the facility, regardless of the existence of network connection in the area using a typical smartphone. The proposed application and its backend system support access to patient's history, i.e. previous diagnoses, medication, and specification of allergies. More features include updates on the progress of the patient, sending referrals directly to hematology, microbiology or biochemistry laboratory and instant notification for the retrieval of laboratory results within a hospital or any healthcare institution. Additionally, we integrate Quick Response (QR code) for coding and accessing medical related data of the patient using a smartphone or a tablet, to be used by the facility itself or anyone else certified. The QR code significantly improves interoperability cases for legacy and non interrelated systems based on HL7 and XML transformation of the HL7 patient referrals.}, 
keywords={health care;hospitals;medical information systems;mobile computing;open systems;smart phones;XML;hospital healthcare appification;QRcodes;data management appification;integrated system;healthcare personnel;healthcare facilities;smartphones;tablets;handheld devices;network connection;patient history;patient diagnoses;patient medication;patient allergy specification;microbiology laboratory;biochemistry laboratory;hematology laboratory;healthcare institution;quick response code;medical related data coding;medical related data accessing;interoperability;noninterrelated systems;HL7 transformation;XML transformation;HL7 patient referrals;Hospitals;Medical diagnostic imaging;Personnel;Smart phones;Databases;Servers;m-health systems;Healthcare Information Systems;Wireless and Mobile Technology;Quick Response Barcodes}, 
doi={10.1109/IISA.2013.6623716}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{6619339, 
author={B. B. P. Cafeo and F. Dantas and E. J. R. Cirilo and A. Garcia}, 
booktitle={2013 4th International Workshop on Emerging Trends in Software Metrics (WETSoM)}, 
title={Towards indicators of instabilities in software product lines: An empirical evaluation of metrics}, 
year={2013}, 
volume={}, 
number={}, 
pages={69-75}, 
abstract={A Software Product Line (SPL) is a set of software systems (products) that share common functionalities, so-called features. The success of a SPL design is largely dependent on its stability; otherwise, a single implementation change will cause ripple effects in several products. Therefore, there is a growing concern in identifying means to either indicate or predict design instabilities in the SPL source code. However, existing studies up to now rely on conventional metrics as indicators of SPL instability. These conventional metrics, typically used in standalone systems, are not able to capture the properties of SPL features in the source code, which in turn might neglect frequent causes of SPL instabilities. On the other hand, there is a small set of emerging software metrics that take into account specific properties of SPL features. The problem is that there is a lack of empirical validation of the effectiveness of metrics in indicating quality attributes in the context of SPLs. This paper presents an empirical investigation through two set of metrics regarding their power of indicating instabilities in evolving SPLs. A set of conventional metrics was confronted with a set of metrics we instantiated to capture important properties of SPLs. The software evolution history of two SPLs were analysed in our studies. These SPLs are implemented using two different programming techniques and all together they encompass 30 different versions under analysis. Our analysis confirmed that conventional metrics are not good indicators of instabilities in the context of evolving SPLs. The set of employed feature dependency metrics presented a high correlation with instabilities proving its value as indicator of SPL instabilities.}, 
keywords={software maintenance;software metrics;feature dependency metrics;software evolution history;software metrics;SPL source code;design instabilities;ripple effects;SPL design;metrics evaluation;software product lines;instabilities indicators;Measurement;Couplings;Programming;Stability analysis;Correlation;Software;Media;Software Product Line;Metrics;Experimentation;Stability;Feature Dependency}, 
doi={10.1109/WETSoM.2013.6619339}, 
ISSN={2327-0969}, 
month={May},}
@INPROCEEDINGS{7727245, 
author={Dayiheng Liu and J. Lv and Xiaofeng Qi and Jiangshu Wei}, 
booktitle={2016 International Joint Conference on Neural Networks (IJCNN)}, 
title={A neural words encoding model}, 
year={2016}, 
volume={}, 
number={}, 
pages={532-536}, 
abstract={This paper proposes a neural network model and learning algorithm that can be applied to encode words. The model realizes the function of words encoding and decoding which can be applied to text encryption/decryption and word-based compression. The model is based on Deep Belief Networks (DBNs) and it differs from traditional DBNs in that it is asymmetric structured and the output of it is a binary vector. With pre-training of multi-layer Restricted Boltzmann Machines (RBMs) and fine-tuning to reconstruct word set, the output of code layer can be used as a kind of representation code of words. We can change the number of neurons of code layer to control the length of representation code for different applications. This paper reports on experiments using English words of American National Corpus to train a neural words encoding model which can be used to encode/decode English words, realizing text encryption and data compression.}, 
keywords={belief networks;Boltzmann machines;cryptography;data compression;learning (artificial intelligence);text analysis;neural word encoding model;learning algorithm;decryption;word- based compression;deep belief networks;multilayer restricted Boltzmann machines;word set reconstruction;code layer;representation code;English words;American National Corpus;neural words encoding model;text encryption;data compression;Encoding;Neurons;Training;Training data;Decoding;Biological neural networks;Data models}, 
doi={10.1109/IJCNN.2016.7727245}, 
ISSN={2161-4407}, 
month={July},}
@INPROCEEDINGS{7977311, 
author={A. Rani and J. K. Chhabra}, 
booktitle={2017 3rd International Conference on Computational Intelligence Communication Technology (CICT)}, 
title={Prioritization of smelly classes: A two phase approach (Reducing refactoring efforts)}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Frequent changes in an object-oriented software system often result into a poor-quality and less maintainable design and the symptoms (known as Code Smells) causing that degradation, need to be corrected for which refactoring is one of the possible solutions. It is not feasible to refactor/ restructure each and every smelly class due to various constraints such as time and cost. Hence it is desirable to make an efficient approach of refactoring. Proposed scheme aims to save time (and cost) of refactoring by carrying out selective refactoring for high priority smelly classes. Prioritization is proposed to be done according to interaction level of each class with other classes. The proposed methodology works in two phases; first phase detects smelly classes using structural information of source code and second phase mines change history to prioritize smelly classes. This prioritization is used to carry out refactoring of more severe classes. This process helps in reducing efforts of refactoring and at the same time may result in avoiding refactoring chains. The proposed technique has been evaluated over a software consisting of 49 classes and results have been validated. The results clearly indicate that the proposed approach performs better and can be very useful for software maintainers in effective and efficient refactoring.}, 
keywords={object-oriented methods;software maintenance;smelly classes prioritization;two phase approach;refactoring efforts reduction;object-oriented software system;code smells;selective refactoring;software maintainers;History;Indexes;Measurement;Tools;Software systems;Couplings;code smells;refactoring;interaction among classes;object-oriented software maintenance}, 
doi={10.1109/CIACT.2017.7977311}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{6224285, 
author={J. Gil and M. Goldstein and D. Moshkovich}, 
booktitle={2012 9th IEEE Working Conference on Mining Software Repositories (MSR)}, 
title={An empirical investigation of changes in some software properties over time}, 
year={2012}, 
volume={}, 
number={}, 
pages={227-236}, 
abstract={Software metrics are easy to define, but not so easy to justify. It is hard to prove that a metric is valid, i.e., that measured numerical values imply anything on the vaguely defined, yet crucial software properties such as complexity and maintainability. This paper employs statistical analysis and tests to check some plausible assumptions on the behavior of software and metrics measured for this software in retrospective on its versions evolution history. Among those are the reliability assumption implicit in the application of any code metric, and the assumption that the magnitude of change, i.e., increase or decrease of its size, in a software artifact is correlated with changes to its version number. Putting a suite of 36 metrics to the trial, we confirm most of the assumptions on a large repository of software artifacts. Surprisingly, we show that a substantial portion of the reliability of some metrics can be observed even in random changes to architecture. Another surprising result is that Boolean-valued metrics tend to flip their values more often in minor software version increments than in major increments.}, 
keywords={Boolean functions;configuration management;software architecture;software metrics;statistical analysis;software properties over time;software metrics;numerical values;statistical analysis;statistical tests;versions evolution history;software artifact;version number;software architecture;Boolean-valued metrics;software version increments;Measurement;Software reliability;Taxonomy;Java;Software systems}, 
doi={10.1109/MSR.2012.6224285}, 
ISSN={2160-1860}, 
month={June},}
@INPROCEEDINGS{8447810, 
author={S. Prapulla and S. J. Bhat and G. Shobha}, 
booktitle={2017 2nd International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS)}, 
title={Framework for Detecting Metamorphic Malware Based on Opcode Feature Extraction}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={Malware is a PC program that is intended to enter and hinder PCs without proprietor's consent. There are different malware types such as key loggers, logic bomb, viruses, rootkits, Trojans, spywares, ransom wares, worms, backdoors, bots, etc. Metamorphic malwares are the malware type which can undergo change at runtime such that they change their code and signature's to bypass the security. Malware's are written in such a way that they get mutated frequently but the original algorithm remains the same. To distinguish a malware, systems like conduct based location, signature based identification and machine learning based procedures are utilized. The mark based discovery framework falls flat in the event that it experiences another obscure malware. In the event of conduct based identification framework, if the antivirus program distinguishes any odd conduct over web then it will produce alert flag, yet there is a high false positive rate as it is inclined to arrange kind records as malware. Machine learning methodology is proposed to detect the malware where feature extraction is based on opcodes in the executable file of the malware. Opcodes of the malware can be extracted into a text file using disassemblers like IDA pro. As success rate of a machine learning algorithm depends on correctness of the features extracted, a robust approach to extract the features is proposed. This model is expected to give success rate of more than 96% in detection of malware}, 
keywords={Malware;Feature extraction;Machine learning;Conferences;Robustness;Information technology;Security;metamorphic;malware;opcodes;feature extraction;machine learning}, 
doi={10.1109/CSITSS.2017.8447810}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6693152, 
author={M. Mirakhorli}, 
booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Preventing erosion of architectural tactics through their strategic implementation, preservation, and visualization}, 
year={2013}, 
volume={}, 
number={}, 
pages={762-765}, 
abstract={Nowadays, a successful software production is increasingly dependent on how the final deployed system addresses customers' and users' quality concerns such as security, reliability, availability, interoperability, performance and many other types of such requirements. In order to satisfy such quality concerns, software architects are accountable for devising and comparing various alternate solutions, assessing the trade-offs, and finally adopting strategic design decisions which optimize the degree to which each of the quality concerns is satisfied. Although designing and implementing a good architecture is necessary, it is not usually enough. Even a good architecture can deteriorate in subsequent releases and then fail to address those concerns for which it was initially designed. In this work, we present a novel traceability approach for automating the construction of traceabilty links for architectural tactics and utilizing those links to implement a change impact analysis infrastructure to mitigate the problem of architecture degradation. Our approach utilizes machine learning methods to detect tactic-related classes. The detected tactic-related classes are then mapped to a Tactic Traceability Pattern. We train our trace algorithm using code extracted from fifty performance-centric and safety-critical open source software systems and then evaluate it against a real case study.}, 
keywords={learning (artificial intelligence);safety-critical software;software architecture;software quality;architectural tactic;software production;security;reliability;interoperability;software architect;traceabilty link;change impact analysis infrastructure;machine learning;tactic-related class;tactic traceability pattern;performance-centric open source software system;safety-critical open source software system;Computer architecture;Software;Heart beat;Software architecture;Software reliability;Architecture;traceability;tactics;traceability patterns;machine learning}, 
doi={10.1109/ASE.2013.6693152}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7020273, 
author={S. Seshagiri and L. P. Goteti}, 
booktitle={2014 IEEE International Conference on MOOC, Innovation and Technology in Education (MITE)}, 
title={Bridging the gap between ABET outcomes and industry expectations — A case study on software engineering course}, 
year={2014}, 
volume={}, 
number={}, 
pages={210-214}, 
abstract={The role of ABET accreditation system is quite significant in providing guidance towards program and course design. The program outcomes encompassing through knowledge, skill and attitude play an important role towards competency development of the students. To improve the employability quotient of these fresh graduates, the alignment of respective outcomes along with the base lined expectations of IT industry is needed. In this context Software Engineering (SE) plays an important role in the transformation journey of the graduates to become entry level developers. It also helps to bridge the gap between academia and IT industry so that these new hires are productive as soon as possible. In this paper, the expectations of an IT industry from the new hires is described in conjunction with ABET educational outcomes highlighting pedagogical aspects that enable students develop these abilities. SE course is used as vehicle and an approach is presented integrating software code of ethics (recommended by ACM - IEEE), SE principles, pedagogical aspects and assessment instruments. Subsequently experiences from our corporate learning environment are highlighted.}, 
keywords={software engineering;ABET outcomes;industry expectations;software engineering course;ABET accreditation system;competency development;IT industry;SE principles;pedagogical aspects;corporate learning environment;Software engineering;Industries;Software;Instruments;Engineering education;Taxonomy;Engineering Education;Outcome based education;Pedagogy;Bloom's taxonomy}, 
doi={10.1109/MITE.2014.7020273}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6178887, 
author={S. Grant and J. R. Cordy and D. B. Skillicorn}, 
booktitle={2012 16th European Conference on Software Maintenance and Reengineering}, 
title={Using Topic Models to Support Software Maintenance}, 
year={2012}, 
volume={}, 
number={}, 
pages={403-408}, 
abstract={Our recent research has shown that the latent information found by commonly used topic models generally relates to the development history of a software system. While it is not always possible to associate these latent topics with human-oriented concepts, it is demonstrable that they identify historical maintenance relationships in source code. Specifically, when a developer makes a change to a software project, it is common for a significant part of that change to relate to a single latent topic. A significant conclusion can be drawn from this: latent topic models identify co-maintenance relationships with no supervision, and therefore topic models can be used to support the maintenance phase of software development.}, 
keywords={project management;software maintenance;latent topic models;software maintenance;software system;historical maintenance relationship identification;source code;software project;co-maintenance relationship identification;software development;History;Web services;Software systems;Maintenance engineering;Software maintenance;Visualization;software maintenance;topic models;program comprehension}, 
doi={10.1109/CSMR.2012.51}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{8094441, 
author={R. Yue and N. Meng and Q. Wang}, 
booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={A Characterization Study of Repeated Bug Fixes}, 
year={2017}, 
volume={}, 
number={}, 
pages={422-432}, 
abstract={Programmers always fix bugs when maintaining software. Previous studies showed that developers apply repeated bug fixes-similar or identical code changes-to multiple locations. Based on the observation, researchers built tools to identify code locations in need of similar changes, or to suggest similar bug fixes to multiple code fragments. However, some fundamental research questions, such as what are the characteristics of repeated bug fixes, are still unexplored. In this paper, we present a comprehensive empirical study with 341,856 bug fixes from 3 open source projects to investigate repeated fixes in terms of their frequency, edit locations, and semantic meanings. Specifically, we sampled bug reports and retrieved the corresponding fixing patches in version history. Then we chopped patches into smaller fixes (edit fragments). Among all the fixes related to a bug, we identified repeated fixes using clone detection, and put a fix and its repeated ones into one repeated-fix group. With these groups, we characterized the edit locations, and investigated the common bug patterns as well as common fixes.Our study on Eclipse JDT, Mozilla Firefox, and LibreOffice shows that (1) 15-20% of bugs involved repeated fixes; (2) 73-92% of repeated-fix groups were applied purely to code clones; and (3) 39% of manually examined groups focused on bugs relevant to additions or deletions of whole if-structures. These results deepened our understanding of repeated fixes. They enabled us to assess the effectiveness of existing tools, and will further provide insights for future research directions in automatic software maintenance and program repair.}, 
keywords={program debugging;public domain software;software maintenance;multiple locations;fixing patches;code clones;automatic software maintenance;program repair;clone detection;common fixes;common bug patterns;repeated-fix group;bug reports;edit locations;repeated bug fixes;Computer bugs;Tools;Cloning;Software;Maintenance engineering;Databases;Semantics;repeated bug fix;empirical study;fix pattern}, 
doi={10.1109/ICSME.2017.16}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7973442, 
author={E. Domazet and M. Gusev}, 
booktitle={2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)}, 
title={Parallelization of digital wavelet transformation of ECG signals}, 
year={2017}, 
volume={}, 
number={}, 
pages={318-323}, 
abstract={The advances in electronics and ICT industry for biomedical use have initiated a lot of new possibilities. However, these IoT solutions face the big data challenge where data comes with a certain velocity and huge quantities. In this paper, we analyze a situation where wearable ECG sensors stream continuous data to the servers. A server needs to receive these streams from a lot of sensors and needs to star various digital signal processing techniques initiating huge processing demands. Our focus in this paper is on optimizing the sequential Wavelet Transform filter. Due to the highly dependent structure of the transformation procedure we propose several optimization techniques for efficient parallelization. We set a hypothesis that optimizing the DWT initialization and processing part can yield a faster code. In this paper, we have provided several experiments to test the validity of this hypothesis by using OpenMP for parallelization. Our analysis shows that proposed techniques can optimize the sequential version of the code.}, 
keywords={application program interfaces;Big Data;electrocardiography;filtering theory;medical signal processing;optimisation;parallel algorithms;sensors;wavelet transforms;ICT industry;electronics industry;biomedical use;Big Data;IoT solutions;wearable ECG sensors;continuous data stream;servers;digital signal processing;sequential wavelet transform filter;optimization;DWT initialization;DWT processing;OpenMP;digital wavelet transformation parallelization;Discrete wavelet transforms;Electrocardiography;Algorithm design and analysis;Delays;Digital signal processing;Parallel processing;Wavelet Transform;ECG;Heart Signal;Parallelization;OpenMP}, 
doi={10.23919/MIPRO.2017.7973442}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8202307, 
author={B. Metka and M. Franzius and U. Bauer-Wersing}, 
booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Efficient navigation using slow feature gradients}, 
year={2017}, 
volume={}, 
number={}, 
pages={1311-1316}, 
abstract={A model of hierarchical Slow Feature Analysis (SFA) enables a mobile robot to learn a spatial representation of its environment directly from images captured during a random walk. After the unsupervised learning phase a subset of the resulting representations are orientation invariant and code for the position of the robot. Hence, they change monotonically over space even though the variation of the sensory signals received from the environment might change drastically e.g. during rotation on the spot. Furthermore, the property of spatial smoothness allows us to infer a navigation direction by taking the difference between the measurement at the current location and a measurement at a target location. In our work we investigated the use of slow feature representations, learned for a specific environment, for the purpose of navigation. We present a straightforward method for navigation using gradient descent on the difference between two points specified in slow feature space. Due to its slowness objective, the resulting slow feature representations implicitly encode information about static obstacles, allowing a mobile robot to efficiently circumnavigate them by simply following the steepest gradient in slow feature space.}, 
keywords={collision avoidance;feature extraction;gradient methods;image representation;mobile robots;random processes;robot programming;unsupervised learning;unsupervised learning;slow feature representations;image capture;robot position;static obstacles;robot learning;gradient descent;target location;navigation direction;spatial smoothness;sensory signals;invariant code;random walk;spatial representation;hierarchical Slow Feature Analysis;slow feature gradients;steepest gradient;mobile robot;slowness objective;slow feature space;Navigation;Training;Cameras;Robot vision systems;Robot kinematics;Mobile robots}, 
doi={10.1109/IROS.2017.8202307}, 
ISSN={2153-0866}, 
month={Sep.},}
@INPROCEEDINGS{7208599, 
author={M. Grobelny and I. Grobelna}, 
booktitle={2015 22nd International Conference Mixed Design of Integrated Circuits Systems (MIXDES)}, 
title={Logic controller design system supporting UML activity diagrams}, 
year={2015}, 
volume={}, 
number={}, 
pages={624-627}, 
abstract={The paper introduces a logic controller design system, called PNAD, supporting UML activity diagrams in version 2.x as a semi-formal specification technique. The system enables transformation of activity diagrams into control Petri nets, their formal verification using model checking technique and the nuXmv tool, generation of synthesizable code in hardware description language VHDL and generation of C code for microcontrollers. The benefits include the support for discrete event system development since the specification till prototype implementation. Additionally, reverse transformation from control Petri nets into UML activity diagrams is also possible. The internal representation of diagrams is based on XML files. The usage of proposed system is illustrated on an example of concrete production process.}, 
keywords={embedded systems;field programmable gate arrays;formal verification;hardware description languages;logic design;microcontrollers;Petri nets;program compilers;specification languages;Unified Modeling Language;FPGA-type structures;XML files;reverse transformation;discrete event system development;microcontrollers;C code generation;VHDL;hardware description language;synthesizable code generation;nuXmv tool;model checking technique;formal verification;control Petri nets;semiformal specification technique;PNAD;UML activity diagrams;logic controller design system;Unified modeling language;Petri nets;Concrete;Model checking;Control systems;Production;XML;activity diagrams;design;logic controllers;specification;system;UML}, 
doi={10.1109/MIXDES.2015.7208599}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7832846, 
author={R. A. Ashraf and R. Gioiosa and G. Kestor and R. F. DeMara and C. Cher and P. Bose}, 
booktitle={SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
title={Understanding the propagation of transient errors in HPC applications}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-12}, 
abstract={Resiliency of exascale systems has quickly become an important concern for the scientific community. Despite its importance, still much remains to be determined regarding how faults disseminate or at what rate do they impact HPC applications. The understanding of where and how fast faults propagate could lead to more efficient implementation of application-driven error detection and recovery. In this work, we propose a fault propagation framework to analyze how faults propagate in MPI applications and to understand their vulnerability to faults. We employ a combination of compiler-level code transformation and instrumentation, along with a runtime checker. Using the information provided by our framework, we employ machine learning technique to derive application fault propagation models that can be used to estimate the number of corrupted memory locations at runtime.}, 
keywords={learning (artificial intelligence);parallel processing;program processors;software fault tolerance;transient error propagation;HPC application;high performance computing;exascale systems;application-driven error detection;application-driven error recovery;fault propagation;compiler-level code transformation;instrumentation;runtime checker;machine learning technique;memory locations;application fault propagation models;Circuit faults;Transient analysis;Hardware;Resilience;Computational modeling;Measurement;Registers}, 
doi={10.1145/2807591.2807670}, 
ISSN={2167-4337}, 
month={Nov},}
@INPROCEEDINGS{7372003, 
author={S. Carino and J. H. Andrews}, 
booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Dynamically Testing GUIs Using Ant Colony Optimization (T)}, 
year={2015}, 
volume={}, 
number={}, 
pages={138-148}, 
abstract={In this paper we introduce a dynamic GUI test generator that incorporates ant colony optimization. We created two ant systems for generating tests. Our first ant system implements the normal ant colony optimization algorithm in order to traverse the GUI and find good event sequences. Our second ant system, called AntQ, implements the antq algorithm that incorporates Q-Learning, which is a behavioral reinforcement learning technique. Both systems use the same fitness function in order to determine good paths through the GUI. Our fitness function looks at the amount of change in the GUI state that each event causes. Events that have a larger impact on the GUI state will be favored in future tests. We compared our two ant systems to random selection. We ran experiments on six subject applications and report on the code coverage and fault finding abilities of all three algorithms.}, 
keywords={ant colony optimisation;graphical user interfaces;learning (artificial intelligence);program testing;dynamic GUI test generator;ant colony optimization algorithm;AntQ;Q-Learning;behavioral reinforcement learning technique;fitness function;code coverage;fault finding abilities;Graphical user interfaces;Testing;Ant colony optimization;Generators;Context;Heuristic algorithms;Computer architecture}, 
doi={10.1109/ASE.2015.70}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8237351, 
author={J. Dai and H. Qi and Y. Xiong and Y. Li and G. Zhang and H. Hu and Y. Wei}, 
booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
title={Deformable Convolutional Networks}, 
year={2017}, 
volume={}, 
number={}, 
pages={764-773}, 
abstract={Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets.}, 
keywords={computer vision;convolution;feedforward neural nets;image segmentation;learning (artificial intelligence);object detection;deformable convolutional networks;convolutional neural networks;geometric transformations;fixed geometric structures;transformation modeling capability;deformable convolution;deformable RoI pooling;spatial sampling locations;dense spatial transformation;deep CNN;standard back-propagation;Convolution;Kernel;Object detection;Standards;Feature extraction;Two dimensional displays}, 
doi={10.1109/ICCV.2017.89}, 
ISSN={2380-7504}, 
month={Oct},}
@INPROCEEDINGS{6619261, 
author={X. Ren and D. Ramanan}, 
booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Histograms of Sparse Codes for Object Detection}, 
year={2013}, 
volume={}, 
number={}, 
pages={3246-3253}, 
abstract={Object detection has seen huge progress in recent years, much thanks to the heavily-engineered Histograms of Oriented Gradients (HOG) features. Can we go beyond gradients and do better than HOG? We provide an affirmative answer by proposing and investigating a sparse representation for object detection, Histograms of Sparse Codes (HSC). We compute sparse codes with dictionaries learned from data using K-SVD, and aggregate per-pixel sparse codes to form local histograms. We intentionally keep true to the sliding window framework (with mixtures and parts) and only change the underlying features. To keep training (and testing) efficient, we apply dimension reduction by computing SVD on learned models, and adopt supervised training where latent positions of roots and parts are given externally e.g. from a HOG-based detector. By learning and using local representations that are much more expressive than gradients, we demonstrate large improvements over the state of the art on the PASCAL benchmark for both root-only and part-based models.}, 
keywords={image coding;image representation;learning (artificial intelligence);object detection;object detection;histogram of oriented gradient;HOG feature;sparse representation;histogram of sparse code;K-SVD;sliding window framework;dimension reduction;supervised training;learning;root only model;part-based model;Dictionaries;Feature extraction;Training;Object detection;Histograms;Computational modeling;Detectors;Object Detection;Sparse Coding;Feature Learning;Supervised Training}, 
doi={10.1109/CVPR.2013.417}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7832961, 
author={Q. Do and G. Yang and M. Che and D. Hui and J. Ridgeway}, 
booktitle={2016 IEEE/ACM International Conference on Mobile Software Engineering and Systems (MOBILESoft)}, 
title={Regression Test Selection for Android Applications}, 
year={2016}, 
volume={}, 
number={}, 
pages={27-28}, 
abstract={Mobile platform pervades human life, and much research in recent years has focused on improving the reliability of mobile applications on this platform, for example by applying automatic testing. However, researchers have primarily considered testing of single version of mobile applications. Although regression testing has been extensively studied for desktop applications, the approaches for desktop applications cannot be directly applied to mobile applications. Our approach leverages the combination of static impact analysis and dynamic code coverage information, and identifies a subset of test cases for re-execution on the modified app version. We implement our approach for Android apps, and illustrate its usefulness based on an Android application.}, 
keywords={Android (operating system);mobile computing;program diagnostics;program testing;regression test selection;Android applications;mobile platform;mobile application reliability;automatic testing;mobile application testing;desktop applications;static impact analysis;dynamic code coverage information;Androids;Humanoid robots;Mobile communication;Testing;Generators;Reliability;Mobile applications;Change impact analysis;coverage;control flow graph}, 
doi={10.1109/MobileSoft.2016.023}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7487305, 
author={C. Lea and R. Vidal and G. D. Hager}, 
booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Learning convolutional action primitives for fine-grained action recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={1642-1649}, 
abstract={Fine-grained action recognition is important for many applications of human-robot interaction, automated skill assessment, and surveillance. The goal is to segment and classify all actions occurring in a time series sequence. While recent recognition methods have shown strong performance in robotics applications, they often require hand-crafted features, use large amounts of domain knowledge, or employ overly simplistic representations of how objects change throughout an action. In this paper we present the Latent Convolutional Skip Chain Conditional Random Field (LC-SC-CRF). This time series model learns a set of interpretable and composable action primitives from sensor data. We apply our model to cooking tasks using accelerometer data from the University of Dundee 50 Salads dataset and to robotic surgery training tasks using robot kinematic data from the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our performance on 50 Salads and JIGSAWS are 18.0% and 5.3% higher than the state of the art, respectively. This model performs well without requiring hand-crafted features or intricate domain knowledge. The code and features have been made public.}, 
keywords={accelerometers;human-robot interaction;learning (artificial intelligence);manipulator kinematics;medical robotics;object recognition;robot vision;surgery;time series;convolutional action primitives learning;fine-grained action recognition;human-robot interaction;automated skill assessment;time series sequence;robotics applications;hand-crafted features;domain knowledge;latent convolutional skip chain conditional random field;LC-SC-CRF;cooking tasks;accelerometer data;University of Dundee 50 salads dataset;robotic surgery training tasks;robot kinematic data;JHU-ISI gesture and skill assessment working set;JIGSAWS;intricate domain knowledge;Hidden Markov models;Convolution;Robot sensing systems;Accelerometers;Surgery;Robot kinematics}, 
doi={10.1109/ICRA.2016.7487305}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6624017, 
author={S. Nadi and C. Dietrich and R. Tartler and R. C. Holt and D. Lohmann}, 
booktitle={2013 10th Working Conference on Mining Software Repositories (MSR)}, 
title={Linux variability anomalies: What causes them and how do they get fixed?}, 
year={2013}, 
volume={}, 
number={}, 
pages={111-120}, 
abstract={The Linux kernel is one of the largest configurable open source software systems implementing static variability. In Linux, variability is scattered over three different artifacts: source code files, Kconfig files, and Makefiles. Previous work detected inconsistencies between these artifacts that led to anomalies in the intended variability of Linux. We call these variability anomalies. However, there has been no work done to analyze how these variability anomalies are introduced in the first place, and how they get fixed. In this work, we provide an analysis of the causes and fixes of variability anomalies in Linux. We first perform an exploratory case study that uses an existing set of patches which solve variability anomalies to identify patterns for their causes. The observations we make from this dataset allow us to develop four research questions which we then answer in a confirmatory case study on the scope of the whole Linux kernel. We show that variability anomalies exist for several releases in the kernel before they get fixed, and that contrary to our initial suspicion, typos in feature names do not commonly cause these anomalies. Our results show that variability anomalies are often introduced through incomplete patches that change Kconfig definitions without properly propagating these changes to the rest of the system. Anomalies are then commonly fixed through changes to the code rather than to Kconfig files.}, 
keywords={Linux;program diagnostics;public domain software;Linux variability anomalies;Linux kernel;configurable open source software systems;static variability;source code files;KCONFIG files;Makefiles;incomplete patches;Linux;Kernel;Feature extraction;Educational institutions;History;Data mining;Software Variability;Variability Anomalies;Linux;Mining Software Repositories;GIT}, 
doi={10.1109/MSR.2013.6624017}, 
ISSN={2160-1860}, 
month={May},}
@ARTICLE{6155713, 
author={M. Hung and P. Chen and Y. Hwang and R. D. Ju and J. Lee}, 
journal={IEEE Transactions on Parallel and Distributed Systems}, 
title={Support of Probabilistic Pointer Analysis in the SSA Form}, 
year={2012}, 
volume={23}, 
number={12}, 
pages={2366-2379}, 
abstract={Probabilistic pointer analysis (PPA) is a compile-time analysis method that estimates the probability that a points-to relationship will hold at a particular program point. The results are useful for optimizing and parallelizing compilers, which need to quantitatively assess the profitability of transformations when performing aggressive optimizations and parallelization. This paper presents a PPA technique using the static single assignment (SSA) form. When computing the probabilistic points-to relationships of a specific pointer, a pointer relation graph (PRG) is first built to represent all of the possible points-to relationships of the pointer. The PRG is transformed by a sequence of reduction operations into a compact graph, from which the probabilistic points-to relationships of the pointer can be determined. In addition, PPA is further extended to interprocedural cases by considering function related statements. We have implemented our proposed scheme including static and profiling versions in the Open64 compiler, and performed experiments to obtain the accuracy and scalability. The static version estimates branch probabilities by assuming that every conditional is equally likely to be true or false, and that every loop executes 10 times before terminating. The profiling version measures branch probabilities dynamically from past program executions using a default workload provided with the benchmark. The average errors for selected benchmarks were 3.80 percent in the profiling version and 9.13 percent in the static version. Finally, SPEC CPU2006 is used to evaluate the scalability, and the result indicates that our scheme is sufficiently efficient in practical use. The average analysis time was 35.59 seconds for an average of 98,696 lines of code.}, 
keywords={graph theory;optimising compilers;probability;program diagnostics;probabilistic pointer analysis;PPA technique;SSA form;static single assignment form;compile-time analysis method;compiler parallelization;transformation profitability;probabilistic points-to relationships;pointer relation graph;PRG;reduction operations;compact graph;Open64 compiler;static version;profiling version;SPEC CPU2006;compiler optimization;Algorithm design and analysis;Probabilistic logic;Program processors;Benchmark testing;Complexity theory;Flow graphs;Compiler;pointer analysis;control flow graph (CFG);static single assignment (SSA) form}, 
doi={10.1109/TPDS.2012.73}, 
ISSN={1045-9219}, 
month={Dec},}
@INPROCEEDINGS{8578558, 
author={P. Wang and Q. Hu and Y. Zhang and C. Zhang and Y. Liu and J. Cheng}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={Two-Step Quantization for Low-bit Neural Networks}, 
year={2018}, 
volume={}, 
number={}, 
pages={4376-4384}, 
abstract={Every bit matters in the hardware design of quantized neural networks. However, extremely-low-bit representation usually causes large accuracy drop. Thus, how to train extremely-low-bit neural networks with high accuracy is of central importance. Most existing network quantization approaches learn transformations (low-bit weights) as well as encodings (low-bit activations) simultaneously. This tight coupling makes the optimization problem difficult, and thus prevents the network from learning optimal representations. In this paper, we propose a simple yet effective Two-Step Quantization (TSQ) framework, by decomposing the network quantization problem into two steps: code learning and transformation function learning based on the learned codes. For the first step, we propose the sparse quantization method for code learning. The second step can be formulated as a non-linear least square regression problem with low-bit constraints, which can be solved efficiently in an iterative manner. Extensive experiments on CIFAR-10 and ILSVRC-12 datasets demonstrate that the proposed TSQ is effective and outperforms the state-of-the-art by a large margin. Especially, for 2-bit activation and ternary weight quantization of AlexNet, the accuracy of our TSQ drops only about 0.5 points compared with the full-precision counterpart, outperforming current state-of-the-art by more than 5 points.}, 
keywords={learning (artificial intelligence);neural nets;object recognition;quantisation (signal);regression analysis;bit matters;quantized neural networks;extremely-low-bit representation;accuracy drop;extremely-low-bit neural networks;existing network quantization approaches;low-bit activations;optimization problem difficult;optimal representations;TSQ;network quantization problem;code learning;learned codes;sparse quantization method;nonlinear least square regression problem;low-bit constraints;2-bit activation;two-step quantization framework;Quantization (signal);Hardware;Biological neural networks;Optimization;Training;Acceleration}, 
doi={10.1109/CVPR.2018.00460}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{7107477, 
author={Ç. Biray and F. Buzluca}, 
booktitle={2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, 
title={A learning-based method for detecting defective classes in object-oriented systems}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Code or design problems in software classes reduce understandability, flexibility and reusability of the system. Performing maintenance activities on defective components such as adding new features, adapting to the changes, finding bugs, and correcting errors, is hard and consumes a lot of time. Unless the design defects are corrected by a refactoring process these error-prone classes will most likely generate new errors after later modifications. Therefore, these classes will have a high error frequency (EF), which is defined as the ratio between the number of errors and modifications. Early estimate of error-prone classes helps developers to focus on defective modules, thus reduces testing time and maintenance costs. In this paper, we propose a learning-based decision tree model for detecting error-prone classes with structural design defects. The main novelty in our approach is that we consider EFs and change counts (ChC) of classes to construct a proper data set for the training of the model. We built our training set that includes design metrics of classes by analyzing numerous releases of real-world software products and considering EFs of classes to mark them as error-prone or non-error-prone. We evaluated our method using two long-standing software solutions of Ericsson Turkey. We shared and discussed our findings with the development teams. The results show that, our approach succeeds in finding error-prone classes and it can be used to decrease the testing and maintenance costs.}, 
keywords={decision trees;object-oriented methods;software maintenance;software quality;defective classes;object-oriented systems;high error frequency;error-prone classes;learning-based decision tree model;structural design defects;change counts;ChC;real-world software products;long-standing software solutions;refactoring process;software quality;Measurement;Training;Software;Decision trees;Testing;Data models;Maintenance engineering;bug prediction;software defects;software quality;decision tree}, 
doi={10.1109/ICSTW.2015.7107477}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8425511, 
author={D. Feshbach and M. Glaser and M. Strout and D. G. Wonnacott}, 
booktitle={2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
title={Iterator-Based Optimization of Imperfectly-Nested Loops}, 
year={2018}, 
volume={}, 
number={}, 
pages={906-914}, 
abstract={Effective optimization of dense array codes often depends upon the selection of the appropriate execution order for the iterations of nested loops. Tools based on the Polyhedral Model have demonstrated dramatic success in performing such optimizations on many such codes, but others remain an area of active research, leaving programmers to optimize code in other ways. Bertolacci et. al demonstrated that programmer-defined iterators can be used to explore iteration-space reorderings, and that Cray's compiler for the Chapel language can optimize such codes to be competitive with polyhedral tools. This "iterator-based" approach allows programmers to explore iteration orderings not identified by automatic optimizers, but was only demonstrated for perfectly-nested loops, and lacked any system for warning about an iterator that would produce an incorrect result. We have now addressed these shortcomings of iterator-based loop optimization, and explored the use of our improved techniques to optimize the imperfectly-nested loops that form the core of Nussinov's algorithm for RNA secondary-structure prediction. Our C++ iterator provides performance that equals the fastest C code, several times faster than was achieved by using the same C compiler on the code with the original iteration ordering, or the code produced by the Pluto loop optimizer. Our Chapel iterators produce run-time that is competitive with the equivalent iterator-free Chapel code, though the Chapel performance does not equal that of the C/C++ code. We have also implemented an iterator that produces an incorrect-but-fast version of Nussinov's algorithm, and used this iterator to illustrate our approaches to error-detection. Manual application of our compile-time error-detection algorithm (which has yet to be integrated into a compiler) identifies this error, as does the run-time approach that we use for codes on which the static test proves inconclusive.}, 
keywords={C++ language;iterative methods;optimisation;program compilers;programmer-defined iterators;iteration-space reorderings;perfectly-nested loops;iterator-based loop optimization;imperfectly-nested loops;Pluto loop optimizer;Chapel iterators;equivalent iterator-free Chapel code;C/C++ code;dense array codes;Cray's compiler;Nussinov's algorithm;Tools;Optimization;C++ languages;Electronic mail;Jacobian matrices;Computational modeling;Prediction algorithms;Iterator;Loop transformation;Optimization}, 
doi={10.1109/IPDPSW.2018.00144}, 
ISSN={}, 
month={May},}
@ARTICLE{6451253, 
author={K. Lee and J. Lee and D. Shin and D. Yoo and H. Kim}, 
journal={IEEE Transactions on Industrial Electronics}, 
title={A Novel Grid Synchronization PLL Method Based on Adaptive Low-Pass Notch Filter for Grid-Connected PCS}, 
year={2014}, 
volume={61}, 
number={1}, 
pages={292-301}, 
abstract={The amount of distributed energy resources (DERs) has increased constantly worldwide. The power ratings of DERs have become considerably high, as required by the new grid code requirement. To follow the grid code and optimize the function of grid-connected inverters based on DERs, a phase-locked loop (PLL) is essential for detecting the grid phase angle accurately when the grid voltage is polluted by harmonics and imbalance. This paper proposes a novel low-pass notch filter PLL (LPN-PLL) control strategy to synchronize with the true phase angle of the grid instead of using a conventional synchronous reference frame PLL (SRF-PLL), which requires a d-q-axis transformation of three-phase voltage and a proportional-integral controller. The proposed LPN-PLL is an upgraded version of the PLL method using the fast Fourier transform concept (FFT-PLL) which is robust to the harmonics and imbalance of the grid voltage. The proposed PLL algorithm was compared with conventional SRF-PLL and FFT-PLL and was implemented digitally using a digital signal processor TMS320F28335. A 10-kW three-phase grid-connected inverter was set, and a verification experiment was performed, showing the high performance and robustness of the proposal under low-voltage ride-through operation.}, 
keywords={adaptive filters;digital signal processing chips;distributed power generation;fast Fourier transforms;invertors;low-pass filters;notch filters;phase locked loops;PI control;power generation control;power grids;power system harmonics;synchronisation;voltage control;grid synchronization PLL method;phase-locked loop;adaptive low-pass notch filter;grid-connected PCS;distributed energy resources;DER;power ratings;grid code requirement;grid phase angle detection;grid voltage pollution;harmonics;imbalance;LPN-PLL control strategy;d-q-axis transformation;proportional-integral controller;fast Fourier transform concept;FFT-PLL;TMS320F28335 digital signal processor;three-phase grid-connected inverter;low-voltage ride-through operation;power 10 kW;Phase locked loops;Harmonic analysis;Voltage fluctuations;Power harmonic filters;Equations;Mathematical model;Phase estimation;Digital signal processor (DSP);distributed energy resources (DERs);grid code;low-pass notch (LPN) filter;phase-locked loop (PLL)}, 
doi={10.1109/TIE.2013.2245622}, 
ISSN={0278-0046}, 
month={Jan},}
@INPROCEEDINGS{6224298, 
author={J. Park and M. Kim and B. Ray and D. Bae}, 
booktitle={2012 9th IEEE Working Conference on Mining Software Repositories (MSR)}, 
title={An empirical study of supplementary bug fixes}, 
year={2012}, 
volume={}, 
number={}, 
pages={40-49}, 
abstract={A recent study finds that errors of omission are harder for programmers to detect than errors of commission. While several change recommendation systems already exist to prevent or reduce omission errors during software development, there have been very few studies on why errors of omission occur in practice and how such errors could be prevented. In order to understand the characteristics of omission errors, this paper investigates a group of bugs that were fixed more than once in open source projects - those bugs whose initial patches were later considered incomplete and to which programmers applied supplementary patches. Our study on Eclipse JDT core, Eclipse SWT, and Mozilla shows that a significant portion of resolved bugs (22% to 33%) involves more than one fix attempt. Our manual inspection shows that the causes of omission errors are diverse, including missed porting changes, incorrect handling of conditional statements, or incomplete refactorings, etc. While many consider that missed updates to code clones often lead to omission errors, only a very small portion of supplementary patches (12% in JDT, 25% in SWT, and 9% in Mozilla) have a content similar to their initial patches. This implies that supplementary change locations cannot be predicted by code clone analysis alone. Furthermore, 14% to 15% of files in supplementary patches are beyond the scope of immediate neighbors of their initial patch locations - they did not overlap with the initial patch locations nor had direct structural dependencies on them (e.g. calls, accesses, subtyping relations, etc.). These results call for new types of omission error prevention approaches that complement existing change recommendation systems.}, 
keywords={inspection;program debugging;project management;public domain software;recommender systems;software maintenance;supplementary bug fixes;software development;open source project;supplementary patch;Eclipse JDT core;Eclipse SWT;Mozilla;manual inspection;conditional statement handling;code clone analysis;omission error prevention approach;change recommendation system;omission error detection;Computer bugs;Cloning;Dispersion;History;Databases;Entropy;Manuals;software evolution;empirical study;patches;bug fixes}, 
doi={10.1109/MSR.2012.6224298}, 
ISSN={2160-1860}, 
month={June},}
@INPROCEEDINGS{8574188, 
author={S. Rothschild}, 
booktitle={2018 IEEE International Symposium on Technologies for Homeland Security (HST)}, 
title={Secure Software by Design}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Based on the calculated cost of a lost record, Yahoo, who “lost” 3 billion records, would be in debt for 450 BILLION DOLLARS. What drives organizations to seek better methods to protect data? The cost of losing data can be high, and it will get higher. Large organizations are able to withstand the malware onslaught, small and mid-size companies have 50-50 chance of remaining in business. To reduce the damage caused by malware, organizations are investing in technology and research. Current research in supervised machine learning is promising. Small and mid-sized companies do not have security professionals to maintain and monitor them. Another area of research is “Honeypots” and “Red Flags”. These techniques may work in espionage, but “white hat testers” demonstrate that these traps are recognized and avoided. Organizations guilty of a data breach, even with clear evidence of negligence are seldom prosecuted. It is very rare that civil or criminal charges are brought against those negligent of reasonable efforts. Can the current environment change? New technologies will eventually be available for small and mid-sized organizations. Laws are changing to make senior management culpable for negligence in protecting sensitive data. Organizations need another way to protect against a data breach. An alternate, and easier strategy for fighting malware is to write software more difficult to hack. This research is identifying how current software practices, lessons learned from malware software, and a novel method to identify critical code, can reduce successful malware attacks. The objective of the research is to search for and identify critical sections in code that should be modified for reducing vulnerabilities. The critical application logic is identified and alternate designs are implemented making it more difficult for the malware author to locate and modify. This research examines easy processes to learn and apply. The work is applicable for all organization, but the existing focus is on helping small and mid-sized organizations. A goal is to reduce the complexity in designing more secure software. The primary considerations are that there are only small additional burdens on software designers and that management sees business value for supporting and requiring more secure software. Because small and mid-sized organizations are more tightly integrated into the supply chain, it the in the interest of large organization, government agencies and the public that these small and mid-sized organizations create more secure software. With an increasing shortage of cyber security professionals, the short-term alternative is to better train software developers for designing more secure software.}, 
keywords={invasive software;learning (artificial intelligence);small-to-medium enterprises;mid-sized organizations;malware attacks;software developers;critical application logic;small-sized organizations;Honeypots;Red Flags;malware software;sensitive data;data breach;supervised machine learning;cyber security professionals;software designers;secure software;Malware;Organizations;Machine learning;Law;malware defense, threat modeling, cyber security;secure coding}, 
doi={10.1109/THS.2018.8574188}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6598518, 
author={S. Yasutaka and S. Matsumoto and S. Saiki and M. Nakamura}, 
booktitle={2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing}, 
title={Visualizing Software Metrics with Service-Oriented Mining Software Repository for Reviewing Personal Process}, 
year={2013}, 
volume={}, 
number={}, 
pages={549-554}, 
abstract={We have proposed a framework named SO-MSR: service-oriented mining software repository, which applied service oriented architecture to MSR. Following the SO-MSR, we have developed a web service, named MetricsWebAPI, for metrics calculation from a variety of software repositories and a variety source codes. In this paper, we develop and propose Metrics Viewer, which is client of Metrics Viewer and is a web application to support personal process improvement. Metrics Viewer provides an interactive user interface for repository file exploring. Moreover the Metrics Viewer visualizes change of source code metrics to support overhead view of personal process. End user can improve their development activities based on software repository data without MSR specific knowledge by using Metrics Viewer. We have conducted a pilot study to evaluate the effect of proposed system for personal process improvement.}, 
keywords={application program interfaces;data mining;data visualisation;graphical user interfaces;interactive systems;personal computing;service-oriented architecture;software metrics;software process improvement;Web services;software metrics visualization;service-oriented mining software repository;SO-MSR;service-oriented architecture;Web service;MetricsWebAPI;MetricsViewer;Web application;personal process improvement;interactive user interface;repository file exploration;source code metrics;Software;Visualization;Writing;Data mining;History;Software metrics;MSR;Service-Oriented Architecture;Software Metrics;Visualization;SO-MSR;MetricsViewer}, 
doi={10.1109/SNPD.2013.96}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{8094418, 
author={Q. Huang and X. Xia and D. Lo}, 
booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Supervised vs Unsupervised Models: A Holistic Look at Effort-Aware Just-in-Time Defect Prediction}, 
year={2017}, 
volume={}, 
number={}, 
pages={159-170}, 
abstract={Effort-aware just-in-time (JIT) defect prediction aims at finding more defective software changes with limited code inspection cost. Traditionally, supervised models have been used; however, they require sufficient labelled training data, which is difficult to obtain, especially for new projects. Recently, Yang et al. proposed an unsupervised model (LT) and applied it to projects with rich historical bug data. Interestingly, they reported that, under the same inspection cost (i.e., 20 percent of the total lines of code modified by all changes), it could find more defective changes than a state-of-the-art supervised model (i.e., EALR). This is surprising as supervised models that benefit from historical data are expected to perform better than unsupervised ones. Their finding suggests that previous studies on defect prediction had made a simple problem too complex. Considering the potential high impact of Yang et al.'s work, in this paper, we perform a replication study and present the following new findings: (1) Under the same inspection budget, LT requires developers to inspect a large number of changes necessitating many more context switches. (2) Although LT finds more defective changes, many highly ranked changes are false alarms. These initial false alarms may negatively impact practitioners' patience and confidence. (3) LT does not outperform EALR when the harmonic mean of Recall and Precision (i.e., F1-score) is considered. Aside from highlighting the above findings, we propose a simple but improved supervised model called CBS. When compared with EALR, CBS detects about 15% more defective changes and also significantly improves Precision and F1-score. When compared with LT, CBS achieves similar results in terms of Recall, but it significantly reduces context switches and false alarms before first success. Finally, we also discuss the implications of our findings for practitioners and researchers.}, 
keywords={inspection;just-in-time;learning (artificial intelligence);program debugging;public domain software;software cost estimation;software maintenance;software metrics;inspection budget;unsupervised model;code inspection cost;LT;defective software;historical bug data;effort-aware just-in-time defect prediction;JIT defect prediction;CBS;EALR;harmonic mean;Predictive models;Inspection;Measurement;Computer bugs;Analytical models;Feature extraction;Software;Change Classification;Cost Effectiveness;Evaluation;Bias}, 
doi={10.1109/ICSME.2017.51}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{8185039, 
author={M. Gieles and P. E. R. Alexander and H. J. G. L. M. Lamers and H. Baumgardt}, 
journal={Monthly Notices of the Royal Astronomical Society}, 
title={A prescription and fast code for the long-term evolution of star clusters – II. Unbalanced and core evolution}, 
year={2014}, 
volume={437}, 
number={1}, 
pages={916-929}, 
abstract={We introduce version two of the fast star cluster evolution code Evolve Me A Cluster of StarS (EMACSS). The first version (Alexander and Gieles) assumed that cluster evolution is balanced for the majority of the life cycle, meaning that the rate of energy generation in the core of the cluster equals the diffusion rate of energy by two-body relaxation, which makes the code suitable for modelling clusters in weak tidal fields. In this new version, we extend the model to include an unbalanced phase of evolution to describe the pre-collapse evolution and the accompanying escape rate such that clusters in strong tidal fields can also be modelled. We also add a prescription for the evolution of the core radius and density and a related cluster concentration parameter. The model simultaneously solves a series of first-order ordinary differential equations for the rate of change of the core radius, half-mass radius and the number of member stars N. About two thousand integration steps in time are required to solve for the entire evolution of a star cluster and this number is approximately independent of N. We compare the model to the variation of these parameters following from a series of direct N-body calculations of single-mass clusters and find good agreement in the evolution of all parameters. Relevant time-scales, such as the total lifetimes and core collapse times, are reproduced with an accuracy of about 10 per cent for clusters with various initial half-mass radii (relative to their Jacobi radii) and a range of different initial N up to N = 65 536. The current version of EMACSS contains the basic physics that allows us to evolve several cluster properties for single-mass clusters in a simple and fast way. We intend to extend this framework to include more realistic initial conditions, such as a stellar mass spectrum and mass-loss from stars. The EMACSS code can be used in star cluster population studies and in models that consider the co-evolution of (globular) star clusters and large-scale structures.}, 
keywords={methods: numerical;stars: kinematics and dynamics;globular clusters: general;Galaxy: kinematics and dynamics;open clusters and associations: general;galaxies: star clusters: general}, 
doi={10.1093/mnras/stt1980}, 
ISSN={0035-8711}, 
month={Jan},}
@ARTICLE{7931599, 
author={Z. Chen and J. Lu and J. Feng and J. Zhou}, 
journal={IEEE Transactions on Multimedia}, 
title={Nonlinear Sparse Hashing}, 
year={2017}, 
volume={19}, 
number={9}, 
pages={1996-2009}, 
abstract={To facilitate fast similarity search, this paper proposes to encode the nonlinear similarity and image structure as compact binary codes. Rather than adopting single matrix as projection in the literature, we employ a nonlinear transformation in the form of multilayer neural network to generate binary codes to capture the local structure between data samples. Specifically, we train the network such that the quantization loss is minimized and the variance over all bits is maximized. In addition, we capture the salient structure of image samples at the abstract level with sparsity constraint and inherit the generalization power to unseen samples. Furthermore, we incorporate the supervisory label information into the learning procedure to take advantage of the manual label. To obtain the desired binary codes and the parameterized nonlinear transformation, we optimize the formulated objective problem over each variable with an iterative alternating method. To validate the efficacy of the proposed hashing approach, we conduct experiments on three widely used datasets, namely CIFAR10, MNIST, and SUN397, by comparing with several recent proposed hashing methods.}, 
keywords={binary codes;cryptography;image processing;iterative methods;neural nets;optimisation;search problems;sparse matrices;nonlinear sparse hashing;similarity search;nonlinear similarity;image structure;compact binary codes;single matrix;nonlinear transformation;multilayer neural network;binary code generation;quantization loss;objective problem optimization;iterative alternating method;CIFAR10;MNIST;SUN397;Binary codes;Encoding;Training;Neural networks;Semantics;Quantization (signal);Sparse matrices;Hashing;informative encoding;nonlinear transformation;sparse}, 
doi={10.1109/TMM.2017.2705918}, 
ISSN={1520-9210}, 
month={Sep.},}
@INPROCEEDINGS{8330212, 
author={Y. Liu and Y. Li and J. Guo and Y. Zhou and B. Xu}, 
booktitle={2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
title={Connecting software metrics across versions to predict defects}, 
year={2018}, 
volume={}, 
number={}, 
pages={232-243}, 
abstract={Accurate software defect prediction could help software practitioners allocate test resources to defect-prone modules effectively and efficiently. In the last decades, much effort has been devoted to build accurate defect prediction models, including developing quality defect predictors and modeling techniques. However, current widely used defect predictors such as code metrics and process metrics could not well describe how software modules change over the project evolution, which we believe is important for defect prediction. In order to deal with this problem, in this paper, we propose to use the Historical Version Sequence of Metrics (HVSM) in continuous software versions as defect predictors. Furthermore, we leverage Recurrent Neural Network (RNN), a popular modeling technique, to take HVSM as the input to build software prediction models. The experimental results show that, in most cases, the proposed HVSM-based RNN model has significantly better effort-aware ranking effectiveness than the commonly used baseline models.}, 
keywords={recurrent neural nets;software metrics;software quality;software reliability;defect-prone modules;quality defect predictors;code metrics;process metrics;HVSM;continuous software versions;RNN model;effort-aware ranking effectiveness;software metrics;software defect prediction models;software modules;historical version sequence of metrics;recurrent neural network;Measurement;Predictive models;Software;Recurrent neural networks;Training;Computer bugs;History}, 
doi={10.1109/SANER.2018.8330212}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6498458, 
author={T. F. Bissyandé and F. Thung and S. Wang and D. Lo and L. Jiang and L. Réveillère}, 
booktitle={2013 17th European Conference on Software Maintenance and Reengineering}, 
title={Empirical Evaluation of Bug Linking}, 
year={2013}, 
volume={}, 
number={}, 
pages={89-98}, 
abstract={To collect software bugs found by users, development teams often set up bug trackers using systems such as Bugzilla. Developers would then fix some of the bugs and commit corresponding code changes into version control systems such as svn or git. Unfortunately, the links between bug reports and code changes are missing for many software projects as the bug tracking and version control systems are often maintained separately. Yet, linking bug reports to fix commits is important as it could shed light into the nature of bug fixing processes and expose patterns in software management. Bug linking solutions, such as ReLink, have been proposed. The demonstration of their effectiveness however faces a number of issues, including a reliability issue with their ground truth datasets as well as the extent of their measurements. We propose in this study a benchmark for evaluating bug linking solutions. This benchmark includes a dataset of about 12,000 bug links from 10 programs. These true links between bug reports and their fixes have been provided during bug fixing processes. We designed a number of research questions, to assess both quantitatively and qualitatively the effectiveness of a bug linking tool. Finally, we apply this benchmark on ReLink to report the strengths and limitations of this bug linking tool.}, 
keywords={program debugging;software engineering;bug linking empirical evaluation;software bug collection;Bugzilla system;code change;version control system;software project;bug tracking system;software management;ReLink solution;ground truth dataset;bug linking tool;Joining processes;Benchmark testing;Software;Computer bugs;Information retrieval;Control systems;Training data;Bug Linking;empirical evaluation;benchmark;ReLink;missing links}, 
doi={10.1109/CSMR.2013.19}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{8374567, 
author={D. Jack and F. Maire and A. Eriksson and S. Shirazi}, 
booktitle={2017 International Conference on 3D Vision (3DV)}, 
title={Adversarially Parameterized Optimization for 3D Human Pose Estimation}, 
year={2017}, 
volume={}, 
number={}, 
pages={145-154}, 
abstract={We propose Adversarially Parameterized Optimization, a framework for learning low-dimensional feasible parameterizations of human poses and inferring 3D poses from 2D input. We train a Generative Adversarial Network to `imagine' feasible poses, and search this imagination space for a solution that is consistent with observations. The framework requires no scene/observation correspondences and enforces known geometric invariances without dataset augmentation. The algorithm can be configured at run time to take advantage of known values such as intrinsic/extrinsic camera parameters or target height when available without additional training. We demonstrate the framework by inferring 3D human poses from projected joint positions for both single frames and sequences. We show competitive results with extremely simple shallow network architectures and make the code publicly available.}, 
keywords={cameras;learning (artificial intelligence);neural net architecture;optimisation;pose estimation;adversarially parameterized optimization;3D human pose estimation;low-dimensional feasible parameterization learning;2D input;generative adversarial network;imagination space;geometric invariances;dataset augmentation;intrinsic camera parameter;extrinsic camera parameter;target height;shallow network architectures;Three-dimensional displays;Two dimensional displays;Gallium nitride;Pose estimation;Training;Generators;Optimization;GAN;Generative-Adversarial-Network;Human-Pose-Estimation;Non-rigid-Body-Transformation;Inverse-Graphics;Ill-Posed-Function-Inversion}, 
doi={10.1109/3DV.2017.00026}, 
ISSN={2475-7888}, 
month={Oct},}
@INPROCEEDINGS{7965122, 
author={A. Engelke and J. Weidendorfer}, 
booktitle={2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
title={Using LLVM for Optimized Lightweight Binary Re-Writing at Runtime}, 
year={2017}, 
volume={}, 
number={}, 
pages={785-794}, 
abstract={Providing new parallel programming models/abstractions as a set of library functions has the huge advantage that it allows for an relatively easy incremental porting path for legacy HPC applications, in contrast to the huge effort needed when novel concepts are only provided in new programming languages or language extensions. However, performance issues are to be expected with fine granular usage of library functions. In previous work, we argued that binary rewriting can bridge the gap by tightly coupling application and library functions at runtime. We showed that runtime specialization at the binary level, starting from a compiled, generic stencil code can help in approaching performance of manually written, statically compiled version. In this paper, we analyze the benefits of post-processing the re-written binary code using standard compiler optimizations as provided by LLVM. To this end, we present our approach for efficiently converting x86-64 binary code to LLVM-IR. Using the mentioned generic code for arbitrary 2d stencils, we present performance numbers with and without LLVM postprocessing. We find that we can now achieve the performance of variants specialized by hand.},
keywords={parallel programming;program compilers;rewriting systems;LLVM;optimized lightweight binary re-writing;parallel programming models;legacy HPC applications;runtime specialization;standard compiler optimizations;Registers;Optimization;Libraries;Runtime;Binary codes;Standards;Decoding;High Performance Computing;Dynamic Code Generation;Dynamic Optimization;Binary Transformation},
doi={10.1109/IPDPSW.2017.103}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{6693087, 
author={T. Jiang and L. Tan and S. Kim}, 
booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
title={Personalized defect prediction}, 
year={2013}, 
volume={}, 
number={}, 
pages={279-289}, 
abstract={Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction-building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java-the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.}, 
keywords={Java;Linux;program compilers;personalized defect prediction;separate prediction model;coding styles;commit frequencies;experience levels;different defect patterns;software defect prediction;C software projects;java software projects;Linux kernel;PostgreSQL;Xorg;Eclipse;Lucene;Jackrabbit;Predictive models;Vectors;Mars;Syntactics;Computer bugs;Training;Feature extraction;Change classification;machine learning;personalized defect prediction;software reliability}, 
doi={10.1109/ASE.2013.6693087}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{7743152, 
author={N. C. C. Brown and A. Altadmri and M. Kölling}, 
booktitle={2016 International Conference on Learning and Teaching in Computing and Engineering (LaTICE)}, 
title={Frame-Based Editing: Combining the Best of Blocks and Text Programming}, 
year={2016}, 
volume={}, 
number={}, 
pages={47-53}, 
abstract={Editing program code as text has several major weaknesses: syntax errors (such as mismatched braces) interrupt programmer flow and make automated tool support harder, boilerplate code templates have to be typed out, and programmers are responsible for layout. These issues have been known about for decades, but early attempts to address these issues, in the form of structured editors, produced unwieldy, hard-to-use tools which failed to catch on. Recently, however, block-based editors in education like Scratch and Snap! have demonstrated that modern graphical structured editors can provide great benefits for programming novices, including very young age groups. These editors become cumbersome for more advanced users, due to their unbending focus on mouse input for block creation and manipulation, and poor scaling of navigation and manipulation facilities to larger programs. Thus, after a few years, learners tend to move from Scratch to text-based editing. In this paper, we present the design and implementation of a novel way to edit programs: frame-based editing. Frame-based editing improves text-based editing by incorporating techniques from block-based editing, and thus provides a suitable follow-on from tools like Scratch. Frame-based editing retains the easy navigation and clearer display of textual code to support manipulation of complex programs, but fuses this with some of the structured editing capabilities that block programming has shown to be viable. The resulting system combines the advantages of text and structured blocks. Preliminary experiments suggest that frame-based editing enables faster program entry than blocks or text, while resulting in fewer syntax errors. We believe it provides an interesting future direction for program editing for learners at all levels of proficiency.}, 
keywords={programming;frame-based editing;block programming;text programming;program code editing;graphical structured editors;block-based editing;Syntactics;Programming profession;Mice;Keyboards;Education;Layout}, 
doi={10.1109/LaTiCE.2016.16}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{8014921, 
author={L. Beyer and S. Breuers and V. Kurin and B. Leibe}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
title={Towards a Principled Integration of Multi-camera Re-identification and Tracking Through Optimal Bayes Filters}, 
year={2017}, 
volume={}, 
number={}, 
pages={1444-1453}, 
abstract={With the rise of end-to-end learning through deep learning, person detectors and re-identification (ReID) models have recently become very strong. Multi-target multi-camera (MTMC) tracking has not fully gone through this transformation yet. We intend to take another step in this direction by presenting a theoretically principled way of integrating ReID with tracking formulated as an optimal Bayes filter. This conveniently side-steps the need for data-association and opens up a direct path from full images to the core of the tracker. While the results are still sub-par, we believe that this new, tight integration opens many interesting research opportunities and leads the way towards full end-to-end tracking from raw pixels. Code and models for all experiments are publicly available.}, 
keywords={Bayes methods;cameras;image filtering;learning (artificial intelligence);object detection;target tracking;multicamera re-identification;optimal Bayes filters;end-to-end learning;deep learning;person detectors;ReID;multitarget multicamera tracking;MTMC;end-to-end tracking;raw pixels;person re-identification;Computational modeling;Current measurement;Data models;Detectors;Mathematical model;Cameras;Tracking}, 
doi={10.1109/CVPRW.2017.187}, 
ISSN={2160-7516}, 
month={July},}
@INPROCEEDINGS{7051940, 
author={R. S. Durelli and D. S. M. Santibáñez and M. E. Delamaro and V. V. de Camargo}, 
booktitle={Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014)}, 
title={Towards a refactoring catalogue for knowledge discovery metamodel}, 
year={2014}, 
volume={}, 
number={}, 
pages={569-576}, 
abstract={Refactorings are a well known technique that assist developers in reformulating the overall structure of applications aiming to improve internal quality attributes while preserving their original behavior. One of the most conventional uses of refactorings are in reengineering processes, whose goal is to change the structure of legacy systems aiming to solve previously identified structural problems. Architecture-Driven Modernization (ADM) is the new generation of reengineering processes; relying just on models, rather than source code, as the main artifacts along the process. However, although ADM provides the general concepts for conducting model-driven modernizations, it does not provide instructions on how to create or apply refactorings in the Knowledge Discovery Metamodel (KDM) metamodel. This leads developers to create their own refactoring solutions, which are very hard to be reused in other contexts. One of the most well known and useful refactoring catalogue is the Fowler's one, but it was primarily proposed for source-code level. In order to fill this gap, in this paper we present a model-oriented version of the Fowler's Catalogue, so that it can be applied to KDM metamodel. In this paper we have focused on four categories of refactorings: (j') renaming, (w) moving features between objects, (iii) organizing data, and (iv) dealing with generalization. We have also developed an environment to support the application of our catalogue. To evaluate our solution we conducted an experiment using eight open source Java application. The results showed that our catalogue can be used to improve the cohesion and coupling of the legacy system.}, 
keywords={data mining;Java;software architecture;software maintenance;software quality;refactoring catalogue;knowledge discovery metamodel;quality attribute;reengineering process;legacy system;architecture-driven modernization;KDM metamodel;source-code level;Fowler Catalogue;open source Java application;Object oriented modeling;Software;Aging;Standards;Unified modeling language;Measurement;Feature extraction;Refactoring;KDM;ADM;Empirical Study}, 
doi={10.1109/IRI.2014.7051940}, 
ISSN={}, 
month={Aug},}
@ARTICLE{7809143, 
author={F. Sala and R. Gabrys and C. Schoeny and L. Dolecek}, 
journal={IEEE Transactions on Information Theory}, 
title={Exact Reconstruction From Insertions in Synchronization Codes}, 
year={2017}, 
volume={63}, 
number={4}, 
pages={2428-2445}, 
abstract={This paper studies problems in data reconstruction, an important area with numerous applications. In particular, we examine the reconstruction of binary and nonbinary sequences from synchronization (insertion/deletion-correcting) codes. These sequences have been corrupted by a fixed number of symbol insertions (larger than the minimum edit distance of the code), yielding a number of distinct traces to be used for reconstruction. We wish to know the minimum number of traces needed for exact reconstruction. This is a general version of a problem tackled by Levenshtein for uncoded sequences. We introduce an exact formula for the maximum number of common supersequences shared by sequences at a certain edit distance, yielding an upper bound on the number of distinct traces necessary to guarantee exact reconstruction. Without specific knowledge of the code words, this upper bound is tight. We apply our results to the famous single deletion/insertion-correcting Varshamov-Tenengolts (VT) codes and show that a significant number of VT code word pairs achieve the worst case number of outputs needed for exact reconstruction. We also consider extensions to other channels, such as adversarial deletion and insertion/deletion channels and probabilistic channels.}, 
keywords={binary sequences;error correction codes;synchronization codes;exact reconstruction;data reconstruction;nonbinary sequence reconstruction;single insertion codes;deletion-correcting codes;symbol insertions;uncoded sequences;common supersequences;insertion-correcting codes;Varshamov-Tenengolts codes;adversarial deletion;insertion-deletion channels;probabilistic channels;Probabilistic logic;Synchronization;Upper bound;Memory;DNA;Error correction codes;Electronic mail;Insertions and deletions;sequence reconstruction;edit distance;synchronization codes}, 
doi={10.1109/TIT.2017.2649493}, 
ISSN={0018-9448}, 
month={April},}
@INPROCEEDINGS{6483277, 
author={Y. Yoo and O. O. Koyluoglu and S. Vishwanath and I. Fiete}, 
booktitle={2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
title={Dynamic shift-map coding with side information at the decoder}, 
year={2012}, 
volume={}, 
number={}, 
pages={632-639}, 
abstract={Shift-map codes have been studied as joint source-channel codes for continuous sources. These codes are useful in delay-limited scenarios and also provide better tolerance to deviations of the signal-to-noise ratio (SNR) from a target SNR, compared to separate source and channel coding. This paper defines a generalized family of shift-map codes that share a strong connection with redundant residue number systems (RRNS), and are henceforth called RRNS-map codes. In the proposed coding scheme, side information about the source allows the decoder to consider only a fraction of the codebook for decoding, with no change in the encoding process. With an appropriately designed RRNS-map code, in this fraction of the codebook, the codewords are much better separated than the original codebook. As a result, RRNS-map codes achieve the same distortion in the mean square error sense as conventional shift-map codes without side information, but significantly outperform shift-map codes when side information is provided to the decoder. This coding scheme is ideally suited to applications where a simple and fixed encoding scheme is desired at the encoder, while the decoder is given access to side information about the source.}, 
keywords={combined source-channel coding;decoding;dynamic shift-map coding scheme;side information;decoder;joint source-channel codes;continuous source codes;signal-to-noise ratio;SNR;redundant residue number systems;RRNS-map codes;encoding process;mean square error;fixed encoding scheme;Decoding;Signal to noise ratio;Hypercubes;Joints;Channel coding}, 
doi={10.1109/Allerton.2012.6483277}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{6674540, 
author={T. Hilaire and B. Lopez}, 
booktitle={SiPS 2013 Proceedings}, 
title={Reliable implementation of linear filters with fixed-point arithmetic}, 
year={2013}, 
volume={}, 
number={}, 
pages={401-406}, 
abstract={This article deals with the implementation of linear filters or controllers with fixed-point arithmetic. The finite precision of the computations and the roundoff errors induced may have an important impact on the numerical behavior of the implemented system. More-over, the fixed-point transformation is a time consuming and error-prone task, specially with the objective of minimizing the quantization impact. Based on a formalism able to describe every structure of linear filters/controllers, this paper proposes an automatic method to generate fixed-point version of the inputs-to-outputs algorithm and an analysis of the global error added on the output. An example illustrates the approach.}, 
keywords={digital filters;fixed point arithmetic;fixed-point arithmetic;finite precision;fixed-point transformation;linear filters/controllers;inputs-to-outputs algorithm;error analysis;code generation;Transfer functions;Roundoff errors;Algorithm design and analysis;Hardware;Equations;Fixed-point arithmetic;Fixed-point arithmetic;error analysis;filter implementation;code generation}, 
doi={10.1109/SiPS.2013.6674540}, 
ISSN={2162-3562}, 
month={Oct},}
@INPROCEEDINGS{8498238, 
author={W. Snipes and S. Karlekar and R. Mo}, 
booktitle={2018 44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
title={A Case Study of the Effects of Architecture Debt on Software Evolution Effort}, 
year={2018}, 
volume={}, 
number={}, 
pages={400-403}, 
abstract={In large-scale software systems, the majority of defective files are architecturally connected, and the architecture connections usually exhibit design flaws, which are associated with higher change-proneness among files and higher maintenance costs. As software evolves with bug fixes, new features, or improvements, unresolved architecture design flaws can contribute to maintenance difficulties. The impact on effort due to architecture design flaws has been difficult to quantify and justify. In this paper, we conducted a case study where we identified flawed architecture relations and quantified their effects on maintenance activities. Using data from this project's source code and revision history, we identified file groups where files are architecturally connected and participated in flawed architecture designs, quantified the maintenance activities in the detected files, and assessed the penalty related to these files.}, 
keywords={program debugging;program testing;public domain software;software architecture;software maintenance;architecture debt;software evolution effort;large-scale software systems;defective files;architecture connections;bug fixes;unresolved architecture design flaws;maintenance difficulties;flawed architecture relations;maintenance activities;file groups;flawed architecture designs;detected files;Computer architecture;Maintenance engineering;History;Tools;Software systems;Computer bugs;Software Architecture;Software Maintenance;Technical Debt}, 
doi={10.1109/SEAA.2018.00071}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6750321, 
author={G. Mostafa and Y. Abedin}, 
booktitle={2013 2nd International Conference on Advances in Electrical Engineering (ICAEE)}, 
title={Development of a graphical user interface using Windows 7-C# to download Intel-Hex formatted file into the flash of 89S52 microcontroller}, 
year={2013}, 
volume={}, 
number={}, 
pages={136-141}, 
abstract={The 89S52 microcontroller (MCU) is very popular among the students, teachers, engineers, scientists and amateurs to build small and cost effective projects for learning purposes and or to incorporate it in a bigger project. In these projects, the chip remains in the system and occasionally need arises to change the program bytes of the code memory (flash) without removing the MCU from the holding instrument. To facilitate entering new codes into the flash without employing expensive commercial ROM Programmer, an interface circuit exists within the 89S52 known as `In-System Programming Interface (ISP)'. ISP Programming requires two components viz., (i) an interactive GUI (Graphical User) Interface (Fig. 7) at the IBMPC side, and (ii) an auxiliary communication controller (ACC) at the target MCU side (Fig. 1). The GUI interface transfers `control information (Chip Erase, chip Blank, Chip Write, Chip Read, Lock Security Bits and etc.)' and `Intel-Hex' formatted program bytes over COM port to the ACC. The ACC decodes the control information, extracts the program bytes and then configures the programming mode of the target MCU. It then activates the ISP interface as per `Serial Programming Instructions [1]' for writing the received program bytes into the flash of the target MCU. This paper has presented the development procedures of the GUI interface written using C# programming language, which is compatible with Windows 7 operating system. The GUI has been tested in an existing 89S52 based `CMCKIT: CISC Microcontroller Learning Kit (Fig. 1)' and found to be working as expected. The contents of this paper will encourage the interested readers to learn C# programming language, physical COM port hardware and programming, virtual COM port concept and finally creating new versions of GUI Interfaces for their own ISP Programmers.}, 
keywords={C++ language;computer science education;flash memories;graphical user interfaces;microcontrollers;operating systems (computers);graphical user interface;Windows 7-C#;Intel-Hex formatted file;flash;89S52 microcontroller;MCU;learning purposes;program bytes;code memory;interface circuit;in-system programming interface;ISP programming;interactive GUI;IBMPC side;auxiliary communication controller;ACC;GUI interface transfers;control information decoding;serial programming instructions;Windows 7 operating system;CMCKIT;CISC microcontroller learning kit;C# programming language learning;physical COM port hardware;virtual COM port concept;Graphical user interfaces;Ports (Computers);Programming;Hardware;Registers;Microcontrollers;Receivers;GUI Interface;ISP Interface;Virtual serial COM port;Intel-Hex Frame;89S52 MCU;C#}, 
doi={10.1109/ICAEE.2013.6750321}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7371961, 
author={L. R. Rodriguez and J. Lawall}, 
booktitle={2015 11th European Dependable Computing Conference (EDCC)}, 
title={Increasing Automation in the Backporting of Linux Drivers Using Coccinelle}, 
year={2015}, 
volume={}, 
number={}, 
pages={132-143}, 
abstract={Software is continually evolving, to fix bugs and add new features. Industry users, however, often value stability, and thus may not be able to update their code base to the latest versions. This raises the need to selectively backport new features to older software versions. Traditionally, backporting has been done by cluttering the backported code with preprocessor directives, to replace behaviors that are unsupported in an earlier version by appropriate workarounds. This approach however, involves writing a lot of error-prone backporting code, and results in implementations that are hard to read and maintain. We consider this issue in the context of the Linux kernel, for whicholder versions are in wide use. We present a new backporting strategy that relies on the use of a backporting compatability library and on code that is automatically generated using the program transformation tool Coccinelle. This approach reduces the amount of code that must be manually written, and thus can help the Linux kernel backporting effort scale while maintainingthe dependability of the backporting process.}, 
keywords={device drivers;Linux;software maintenance;automation;Linux drivers;Coccinelle;software versions;error-prone backporting code;backporting compatability library;Linux kernel backporting;Kernel;Linux;Libraries;Silicon;Semantics;Encoding;Automation;Linux;backports;program transformation}, 
doi={10.1109/EDCC.2015.23}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6337496, 
author={I. Ahmed and A. Zoranic and S. Javaid and G. G. R. III}, 
booktitle={2012 41st International Conference on Parallel Processing Workshops}, 
title={ModChecker: Kernel Module Integrity Checking in the Cloud Environment}, 
year={2012}, 
volume={}, 
number={}, 
pages={306-313}, 
abstract={Kernel modules are an integral part of most operating systems (OS) as they provide flexible ways of adding new functionalities (such as file system or hardware support) to the kernel without the need to recompile or reload the entire kernel. Aside from providing an interface between the user and the hardware, these modules maintain system security and reliability. Malicious kernel level exploits (e.g. code injections) provide a gateway to a system's privileged level where the attacker has access to an entire system. Such attacks may be detected by performing code integrity checks. Several commodity operating systems (such as Linux variants and MS Windows) maintain signatures of different pieces of kernel code in a database for code integrity checking purposes. However, it quickly becomes cumbersome and time consuming to maintain a database of legitimate dynamic changes in the code, such as regular module updates. In this paper we present Mod Checker, which checks in-memory kernel modules' code integrity in real time without maintaining a database of hashes. Our solution applies to virtual environments that have multiple virtual machines (VMs) running the same version of the operating system, an environment commonly found in large cloud servers. Mod Checker compares kernel module among a pool of VMs within a cloud. We thoroughly evaluate the effectiveness and runtime performance of Mod Checker and conclude that Mod Checker is able to detect any change in a kernel module's headers and executable content with minimal or no impact on the guest operating systems' performance.}, 
keywords={cloud computing;invasive software;Linux;operating system kernels;software maintenance;virtual machines;ModChecker;kernel module integrity checking;cloud environment;commodity operating systems;file system;hardware support;system security;system reliability;malicious kernel level;code integrity checking;Linux variants;MS Windows;regular module updates;virtual environments;multiple virtual machines;cloud servers;malware;Kernel;Virtual machining;Hardware;Malware;Libraries;Linux;Xen;cloud computing;kernel module;malware;code integrity;virtual machine}, 
doi={10.1109/ICPPW.2012.46}, 
ISSN={0190-3918}, 
month={Sep.},}
@INPROCEEDINGS{6311201, 
author={L. Reynoso and S. Romero and F. Romero}, 
booktitle={2012 IEEE 11th International Conference on Cognitive Informatics and Cognitive Computing}, 
title={Towards a formal model of the pedagogic discourse and the Zone of Proximal Development (ZPD) of Vygotsky}, 
year={2012}, 
volume={}, 
number={}, 
pages={510-517}, 
abstract={This article uses conceptual devices from different theories of communication, discourse analysis of Pêcheux and Zone of Proximal Development (ZPD) of Vygotsky, to present a formalization of a new pedagogical discourse based on the pedagogic transformation through a meaning-effect. The formalization of the main components of the pedagogic discourse takes into account the imaginary formations, languages and discursive process of teaching and learning, which allow us to orient ourselves towards producing a more diverse model. Finally technological artifacts created (new technological tools in education, virtual classrooms and new materials) become powerful tools daily in mediating our communication and our activities. Thus new forms of code -according to some communication models- seem to take on increasing significance. Is also described in this paper a proposal incorporating technological resources as elements in imaginary mediation and discursive composition.}, 
keywords={computer aided instruction;educational technology;teaching;virtual reality;formal model;pedagogic discourse;zone of proximal development;ZPD;Vygotsky;communication theories;pedagogic transformation;meaning-effect;teaching;learning;education;virtual classrooms;Production;Speech;Education;Context;Computational modeling;Vehicles;Materials;imaginary formations;cognition development;discourse;cognitive perception;learning model;communication}, 
doi={10.1109/ICCI-CC.2012.6311201}, 
ISSN={}, 
month={Aug},}
@ARTICLE{7862818, 
author={R. Zhao and B. Du and L. Zhang}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={Hyperspectral Anomaly Detection via a Sparsity Score Estimation Framework}, 
year={2017}, 
volume={55}, 
number={6}, 
pages={3208-3222}, 
abstract={Anomaly detection has become an important topic in hyperspectral imagery (HSI) analysis over the last 20 years. HSIs usually possess complexly cluttered spectral signals due to the complicated conditions of the land-cover distribution. This in turn makes it difficult to obtain an accurate background estimation to distinguish the anomaly targets. The sparse learning technique provides a way to obtain an implicit background representation with the learned dictionary and corresponding sparse codes. In this paper, we explore the background/anomaly information content for each atom of the learned dictionary, from an analysis based on the frequency of the dictionary atoms for HSI reconstruction. From this perspective, we propose a novel sparsity score estimation framework for hyperspectral anomaly detection. First, an overcomplete dictionary and the corresponding sparse code matrix are obtained from the HSI. The frequency of each dictionary atom for reconstruction, which is also called the atom usage probability, is then estimated from the sparse code matrix. Finally, the estimated frequencies are transformed to the sparsity score for each pixel, which can be seen as the degree of “anomalousness.” In the proposed detection framework, two strategies are proposed to enhance the diversity between the background and anomaly information in the learned dictionary: 1) dictionary-based background feature transformation and 2) dictionary iterative reweighting. A series of real-world HSI data sets is utilized to evaluate the performance of the proposed framework. The experimental results show that the proposed framework achieves a superior performance compared to some of the state-of-the-art anomaly detection methods.}, 
keywords={geophysical techniques;hyperspectral imaging;land cover;hyperspectral anomaly detection;sparsity score estimation framework;hyperspectral imagery analysis;complexly cluttered spectral signals;land-cover distribution;background estimation;anomaly targets;sparse learning technique;implicit background representation;learned dictionary;background-anomaly information;dictionary atom frequency;hyperspectral anomaly detection;sparse code matrix;atom usage probability;dictionary-based background feature transformation;dictionary iterative reweighting;real-world HSI data sets;anomaly detection methods;Dictionaries;Hyperspectral imaging;Detectors;Estimation;Sparse matrices;Encoding;Anomaly detection;dictionary enhancement;hyperspectral;K-SVD algorithm;negative log atom usage probability;sparse coding;sparsity score estimation}, 
doi={10.1109/TGRS.2017.2664658}, 
ISSN={0196-2892}, 
month={June},}
@INPROCEEDINGS{7070275, 
author={Y. Kim and L. Jiang}, 
booktitle={2015 48th Hawaii International Conference on System Sciences}, 
title={The Knowledge Accumulation and Transfer in Open-Source Software (OSS) Development}, 
year={2015}, 
volume={}, 
number={}, 
pages={3811-3820}, 
abstract={We examine the learning curves of individual software developers in Open-Source Software (OSS) Development. We collected the dataset of multi-year code change histories from the repositories for 20 open source software projects involving more than 200 developers. We build and estimate regression models to assess individual developers' learning progress (in reducing the likelihood they make a bug). Our estimation results show that developer's coding and indirect bug-fixing experiences do not decrease bug ratios while bug-fixing experience can lead to the decrease of bug ratio of learning progress. We also find that developer's coding and bug-fixing experiences in other projects do not decrease the developer's bug ratio in a focal project. We empirically confirm the moderating effects of bug types on learning progress. Developers exhibit learning effects for some simple bug types (e.g., Wrong literals) or bug types with many instances (e.g., Wrong if conditionals). The results may have managerial implications and provoke future research on project management about allocating resources on tasks that add new code versus tasks that debug and fix existing code.}, 
keywords={personnel;project management;public domain software;regression analysis;software development management;knowledge accumulation;knowledge transfer;open-source software development;individual software developers learning curves;open source software projects;regression models;developers bug-fixing experiences;developers coding experiences;OSS development;Computer bugs;Java;Encoding;Hidden Markov models;Open source software;Mathematical model;learning effects;knowledge transfer;software developer;open-source software}, 
doi={10.1109/HICSS.2015.458}, 
ISSN={1530-1605}, 
month={Jan},}
@INPROCEEDINGS{7332490, 
author={G. Balogh and G. Antal and Á. Beszédes and L. Vidács and T. Gyimóthy and Á. Z. Végh}, 
booktitle={2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={Identifying wasted effort in the field via developer interaction data}, 
year={2015}, 
volume={}, 
number={}, 
pages={391-400}, 
abstract={During software projects, several parts of the source code are usually re-written due to imperfect solutions before the code is released. This wasted effort is of central interest to the project management to assure on-time delivery. Although the amount of thrown-away code can be measured from version control systems, stakeholders are more interested in productivity dynamics that reflect the constant change in a software project. In this paper we present a field study of measuring the productivity of a medium-sized J2EE project. We propose a productivity analysis method where productivity is expressed through dynamic profiles - the so-called Micro-Productivity Profiles (MPPs). They can be used to characterize various constituents of software projects such as components, phases and teams. We collected detailed traces of developers' actions using an Eclipse IDE plug-in for seven months of software development throughout two milestones. We present and evaluate profiles of two important axes of the development process: by milestone and by application layers. MPPs can be an aid to take project control actions and help in planning future projects. Based on the experiments, project stakeholders identified several points to improve the development process. It is also acknowledged, that profiles show additional information compared to a naive diff-based approach.}, 
keywords={programming environments;project management;software engineering;software management;source code;software project management;medium-sized J2EE project;productivity analysis method;microproductivity profile;MPP;Eclipse IDE plugin;software development;Productivity;Software;User interfaces;Software measurement;Security;Project management;Stakeholders}, 
doi={10.1109/ICSM.2015.7332490}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8551388, 
author={A. Lakhotia and V. Notani and C. LeDoux}, 
booktitle={2018 International Conference On Cyber Situational Awareness, Data Analytics And Assessment (Cyber SA)}, 
title={Malware Economics and its Implication to Anti-Malware Situational Awareness}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Malware, like any other software, is developed iteratively and improved in incremental versions over a long period of time. Malware economics requires amortizing the cost of malware development over several attacks. Thus, the malware code persists through many incremental versions of the malware-albeit in a transformed and obfuscated state-while the classic indicators of attack, e.g., domain names, file names, and IP addresses, are parameterized and often change with each new version. Recent breakthroughs in automated malware analysis and code debofuscation make it possible to overcome the challenges imposed by code obfuscation and create new anti-malware tools that use the malware code itself as an immutable indicator in anti-malware defense. The resulting technologies can be used to provide situational awareness of the dynamic threat profile of an organization. A persistent adversary that intends to penetrate a particular organization will send morphed variants of the same malware to a large number of people in an organization. Such an attack campaign may be executed over weeks or months. By correlating malware generated from the same code base, one can detect such persistent campaigns against an organization using the malware blocked by an anti-virus. Results from the field demonstrate that this approach has promise in detecting targeted attacks while the attacks are in progress thus giving the defenders' enough time to take preventive actions.}, 
keywords={invasive software;malware economics;anti-malware situational;malware development;automated malware analysis;code debofuscation;code obfuscation;anti-malware tools;anti-malware defense;malware code;Malware;Organizations;Economics;Cyberattack}, 
doi={10.1109/CyberSA.2018.8551388}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{8453120, 
author={R. Wu and M. Wen and S. Cheung and H. Zhang}, 
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)}, 
title={[Journal First] ChangeLocator: Locate Crash-Inducing Changes Based on Crash Reports}, 
year={2018}, 
volume={}, 
number={}, 
pages={536-536}, 
abstract={Software crashes are severe manifestations of software bugs. Debugging crashing bugs is tedious and time-consuming. Understanding software changes that induce a crashing bug can provide useful contextual information for bug fixing and is highly demanded by developers. Locating the bug inducing changes is also useful for automatic program repair, since it narrows down the root causes and reduces the search space of bug fix location. However, currently there are no systematic studies on locating the software changes to a source code repository that induce a crashing bug reflected by a bucket of crash reports. To tackle this problem, we first conducted an empirical study on characterizing the bug inducing changes for crashing bugs (denoted as crashinducing changes). We also propose ChangeLocator, a method to automatically locate crash-inducing changes for a given bucket of crash reports. We base our approach on a learning model that uses features originated from our empirical study and train the model using the data from the historical fixed crashes. We evaluated ChangeLocator with six release versions of Netbeans project. The results show that it can locate the crash-inducing changes for 44.7%, 68.5%, and 74.5% of the bugs by examining only top 1, 5 and 10 changes in the recommended list, respectively. It significantly outperforms the existing state-of-the-art approach.}, 
keywords={program debugging;software maintenance;historical fixed crashes;crashinducing changes;bug fix location;bug inducing changes;crashing bug;software bugs;software crashes;crash reports;crash-inducing changes;Computer bugs;Software;Software engineering;Data models;Computer science;Maintenance engineering;crash-inducing change;software crash;crash stack;bug localization}, 
doi={}, 
ISSN={1558-1225}, 
month={May},}
@INPROCEEDINGS{7180065, 
author={B. Ray and M. Nagappan and C. Bird and N. Nagappan and T. Zimmermann}, 
booktitle={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories}, 
title={The Uniqueness of Changes: Characteristics and Applications}, 
year={2015}, 
volume={}, 
number={}, 
pages={34-44}, 
abstract={Changes in software development come in many forms. Some changes are frequent, idiomatic, or repetitive (e.g. Adding checks for nulls or logging important values) while others are unique. We hypothesize that unique changes are different from the more common similar (or non-unique) changes in important ways, they may require more expertise or represent code that is more complex or prone to mistakes. As such, these unique changes are worthy of study. In this paper, we present a definition of unique changes and provide a method for identifying them in software project history. Based on the results of applying our technique on the Linux kernel and two large projects at Microsoft, we present an empirical study of unique changes. We explore how prevalent unique changes are and investigate where they occur along the architecture of the project. We further investigate developers' contribution towards uniqueness of changes. We also describe potential applications of leveraging the uniqueness of change and implement two of those applications, evaluating the risk of changes based on uniqueness and providing change recommendations for non-unique changes.}, 
keywords={software architecture;software development;software project history;Linux kernel;project architecture;Linux;Software;Cloning;Context;History;Syntactics;Buildings}, 
doi={10.1109/MSR.2015.11}, 
ISSN={2160-1852}, 
month={May},}
@INPROCEEDINGS{6897234, 
author={S. Benedict}, 
booktitle={2014 Seventh International Conference on Contemporary Computing (IC3)}, 
title={Application of energy reduction techniques using niched pareto GA of energy analzyer for HPC applications}, 
year={2014}, 
volume={}, 
number={}, 
pages={559-564}, 
abstract={Energy consumption of High Performance Computing (HPC) architectures, on the path to exa-scale systems, is still a challenging problem among the HPC community owing to the technological issues, such as, power limitations of processor technologies, increased degree of parallelism (both in a node level and in a system level), and a hefty cost of communication which arises while executing applications on such architectures. In addition, the increased electrical billing and the other ensuing ecological hazards, including climate changes, have urged several researchers to focus much on framing solutions that address the energy consumption issues of future HPC systems. Reducing the energy consumption of HPC systems, however, is not an easy task due to its assorted nature of muddled up complicated issues that are tightly dependent on the performance of applications, the energy efficiency of hardware components, and the energy consumption of the compute center infrastructure. This paper presents Niched Pareto Genetic Algorithm (NPGA) based application of energy reduction techniques, namely, code version selection mechanism and compiler optimization switch selection mechanism, for HPC applications using Energy Analyzer tool. The proposed mechanism was tested with HPC applications, such as, MPI-C based HPCC benchmarks, Jacobi, PI, and matrix multiplication applications, on the HPCCLoud Research Laboratory of our premise. This paper could be of an interest to various researchers, namely, HPC application developers, performance analysis tool developers, environmentalist, and energy-aware hardware designers.}, 
keywords={computer centres;energy conservation;energy consumption;genetic algorithms;invoicing;Pareto optimisation;power aware computing;Niched Pareto GA;Niched Pareto genetic algorithm;energy analzyer tool;high performance computing architecture;exascale system;electrical billing;ecological hazards;climate change;energy consumption reduction technique;future HPC system architecture;hardware component energy efficiency;compute center infrastructure;NPGA;code version selection mechanism;compiler optimization switch selection mechanism;HPC cloud research laboratory;Jacobian matrices;Optical switches;Energy measurement;Monitoring;Communities;Benchmark testing;Energy tuning;HPC;Niched Pareto;Performance Analysis;Scientific Applications}, 
doi={10.1109/IC3.2014.6897234}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{6766392, 
author={M. Yusuf and A. El-Mahdy and E. Rohou}, 
booktitle={2013 Second International Japan-Egypt Conference on Electronics, Communications and Computers (JEC-ECC)}, 
title={On-stack replacement to improve JIT-based obfuscation a preliminary study}, 
year={2013}, 
volume={}, 
number={}, 
pages={94-99}, 
abstract={As more devices are connecting together, more effective security techniques are needed to protect running software from hackers. One possible security technique is to continuously change the binary code running of given software by recompiling it on the fly. This switching need to be done frequently, quickly, and randomly, not constrained by specific locations in code, to make it difficult for the hacker to track the behavior of the running code or predict its functionality. In our research we are working on a technique that recompiles speculatively and concurrently with current execution, and switches to the new compiled version dynamically, at arbitrary points. This paper presents an early analytical study augmented by experimental analysis on manually applying this technique on simple kernels, to study the concept in comparison with other similar techniques.}, 
keywords={binary codes;computer crime;concurrency control;program compilers;just-in-time compiler;hacking;binary code running;software protection;security techniques;JIT-based obfuscation;on-stack replacement;Switches;Optimization;Security;Measurement;Integrated circuits;Software;Complexity theory;on stack replacement;LLVM;code switching;obfuscation;security;recompilation}, 
doi={10.1109/JEC-ECC.2013.6766392}, 
ISSN={}, 
month={Dec},}
@ARTICLE{7736073, 
author={S. A. Carr and F. Logozzo and M. Payer}, 
journal={IEEE Transactions on Software Engineering}, 
title={Automatic Contract Insertion with CCBot}, 
year={2017}, 
volume={43}, 
number={8}, 
pages={701-714}, 
abstract={Existing static analysis tools require significant programmer effort. On large code bases, static analysis tools produce thousands of warnings. It is unrealistic to expect users to review such a massive list and to manually make changes for each warning. To address this issue we propose CCBot (short for CodeContracts Bot), a new tool that applies the results of static analysis to existing code through automatic code transformation. Specifically, CCBot instruments the code with method preconditions, postconditions, and object invariants which detect faults at runtime or statically using a static contract checker. The only configuration the programmer needs to perform is to give CCBot the file paths to code she wants instrumented. This allows the programmer to adopt contract-based static analysis with little effort. CCBot's instrumented version of the code is guaranteed to compile if the original code did. This guarantee means the programmer can deploy or test the instrumented code immediately without additional manual effort. The inserted contracts can detect common errors such as null pointer dereferences and out-of-bounds array accesses. CCBot is a robust large-scale tool with an open-source C# implementation. We have tested it on real world projects with tens of thousands of lines of code. We discuss several projects as case studies, highlighting undiscovered bugs found by CCBot, including 22 new contracts that were accepted by the project authors.}, 
keywords={C# language;program compilers;program diagnostics;program verification;software fault tolerance;automatic contract insertion;CCBot;static analysis tools;CodeContracts Bot;automatic code transformation;object invariants;fault detection;static contract checker;file paths;contract-based static analysis;null pointer dereferences;out-of-bounds array accesses;open-source C# implementation;contract-based verification;Contracts;C# languages;Instruments;Computer bugs;Reactive power;Semantics;Runtime;Contract-based verification;automated patching;assertions;class invariants}, 
doi={10.1109/TSE.2016.2625248}, 
ISSN={0098-5589}, 
month={Aug},}
@INPROCEEDINGS{6178870, 
author={H. C. Gall}, 
booktitle={2012 16th European Conference on Software Maintenance and Reengineering}, 
title={Keynote 1: LGTM - Software Sensing and Bug Smelling}, 
year={2012}, 
volume={}, 
number={}, 
pages={3-4}, 
abstract={Summary form only given. Looks Good To Me (LGTM) is a means to stamp an 'ok' on code reviews and then have the code moved to production. One key part of this process is a detailed quality analysis by checking code, looking for potential known bugs, and evaluating the design. As code bases are changed almost every day by many developers, we need to devise means for effective, regular, and focused analysis. In the recent past we have looked into approaches of software sensing and bug smelling. Sensing software is employed by audio-visual and multi-touch interfaces to a code base and its change history. Bug smelling leverages bug prediction on the levels of classes, methods, or change types. Combining both can lead to a more effective quality analysis for reviewing tasks such that the signing off by LGTM is facilitated. In this talk we will present approaches and tools such as SmellTagger or Coco Viz for software sensing and bug smelling. We will also discuss current limitations and potential new horizons for software evolution research.}, 
keywords={program debugging;software maintenance;user interfaces;LGTM;looks good to me;software sensing;bug smelling;quality analysis;code checking;design evaluation;audio-visual interfaces;multitouch interfaces;bug prediction;SmellTagger;Coco Viz;software evolution research;Software;Software engineering;Sensors;Informatics}, 
doi={10.1109/CSMR.2012.9}, 
ISSN={1534-5351}, 
month={March},}
@INPROCEEDINGS{8514350, 
author={B. Wicht and A. Fischer and J. Hennebert}, 
booktitle={2018 International Conference on High Performance Computing Simulation (HPCS)}, 
title={Seamless GPU Evaluation of Smart Expression Templates}, 
year={2018}, 
volume={}, 
number={}, 
pages={196-203}, 
abstract={Expression Templates is a technique allowing to write linear algebra code in C++ the same way it would be written on paper. It is also used extensively as a performance optimization technique, especially as the Smart Expression Templates form which allows for even higher performance. It has proved to be very efficient for computation on a Central Processing Unit (CPU). However, due to its design, it is not easily implemented on a Graphics Processing Unit (GPU). In this paper, we devise a set of techniques to allow the seamless evaluation of Smart Expression Templates on the GPU. The execution is transparent for the user of the library which still uses the matrices and vector as if it was on the CPU and profits from the performance and higher multi-processing capabilities of the GPU. We also show that the GPU version is significantly faster than the CPU version, without any change to the code of the user.}, 
keywords={C++ language;graphics processing units;matrix algebra;optimisation;parallel processing;software performance evaluation;CPU;seamless evaluation;GPU version;linear algebra code;performance optimization technique;central processing unit;graphics processing unit;GPU evaluation;multiprocessing capabilities;smart expression templates form;Graphics processing units;Kernel;Libraries;C++ languages;Runtime;Central Processing Unit;High performance computing}, 
doi={10.1109/HPCS.2018.00045}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{7203066, 
author={C. Vendome}, 
booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering}, 
title={A Large Scale Study of License Usage on GitHub}, 
year={2015}, 
volume={2}, 
number={}, 
pages={772-774}, 
abstract={The open source community relies upon licensing in order to govern the distribution, modification, and reuse of existing code. These licenses evolve to better suit the requirements of the development communities and to cope with unaddressed or new legal issues. In this paper, we report the results of a large empirical study conducted over the change history of 16,221 open source Java projects mined from Git Hub. Our study investigates how licensing usage and adoption changes over a period of ten years. We consider both the distribution of license usage within projects of a rapidly growing forge and the extent that new versions of licenses are introduced in these projects.}, 
keywords={Java;law;public domain software;software reusability;license usage;GitHub;open source community;licensing;code distribution;code modification;code reuse;development communities;legal issues;open source Java projects;license distribution;Licenses;Software;Conferences;Software engineering;Data mining;Law;Software Licenses;Mining Software Repositories;Empirical Studies}, 
doi={10.1109/ICSE.2015.245}, 
ISSN={0270-5257}, 
month={May},}
@INPROCEEDINGS{7361227, 
author={Y. Chen and P. S. Park}, 
booktitle={2015 SAI Intelligent Systems Conference (IntelliSys)}, 
title={A hybrid generative/discriminative model based object tracking primary exploration}, 
year={2015}, 
volume={}, 
number={}, 
pages={765-772}, 
abstract={Based on analysis and discussion of object representation, a hybrid model based tracking by detection algorithm is presented as yet a primary exploration. The whole system is made of a learning-detecting two phase loop. Object model is built on a general Haar-like feature space which is automatically generated and extracted by a special random projection. Our proposed algorithm involves two type of methods for object modeling, one is to learn a transformation matrix by Principal Component Analysis (PCA) as the multi-view appearance model of the target object, and the other is to learn a classifier by Fisher Linear Discriminant Analysis (FLD) as the classification between the foreground and the background. We extend the Fisher criterion to a multi-mode background situation, which is used to formulate features' discriminating power as feature weighting from the online captured positive/negative training data. In additionally, a two-stage detection is involved, in which all input samples firstly are tested by the learned FLD classifier to pick up candidates, then amongst candidates the maximum likelihood to the target template as the final detection result is searched for by PCA code matching. All generative model, discriminative model and target templates should online update due to appearance variation. A number of experiments illustrate that the proposed hybrid model based tracking algorithm does has advantages.},
keywords={Haar transforms;matrix algebra;object tracking;pattern matching;principal component analysis;PCA code matching;FLD classifier;positive-negative training data;multimode background situation;FLD;Fisher linear discriminant analysis;principal component analysis;transformation matrix;special random projection;Haar-like feature space;learning-detecting two phase loop;detection algorithm;hybrid model based tracking;object tracking primary exploration;hybrid generative-discriminative model;Principal component analysis;Target tracking;Feature extraction;Training;Covariance matrices;Analytical models;Training data;FLD;PCA;multi-mode background;hybrid model;online model updating;feature weighting}, 
doi={10.1109/IntelliSys.2015.7361227}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8578561, 
author={N. S. Detlefsen and O. Freifeld and S. Hauberg}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={Deep Diffeomorphic Transformer Networks}, 
year={2018}, 
volume={}, 
number={}, 
pages={4403-4412}, 
abstract={Spatial Transformer layers allow neural networks, at least in principle, to be invariant to large spatial transformations in image data. The model has, however, seen limited uptake as most practical implementations support only transformations that are too restricted, e.g. affine or homographic maps, and/or destructive maps, such as thin plate splines. We investigate the use of flexible diffeomorphic image transformations within such networks and demonstrate that significant performance gains can be attained over currently-used models. The learned transformations are found to be both simple and intuitive, thereby providing insights into individual problem domains. With the proposed framework, a standard convolutional neural network matches state-of-the-art results on face verification with only two extra lines of simple TensorFlow code.}, 
keywords={convolution;face recognition;feedforward neural nets;learning (artificial intelligence);standard convolutional neural network;neural networks;spatial transformations;image data;practical implementations;homographic maps;destructive maps;plate splines;transformation learning;spatial transformer layers;deep diffeomorphic transformer networks;diffeomorphic image transformations;face verification;Face;Neural networks;Computer architecture;Standards;Task analysis;Computational modeling;Kernel}, 
doi={10.1109/CVPR.2018.00463}, 
ISSN={2575-7075}, 
month={June},}
@ARTICLE{8023039, 
author={L. Veit and G. Pidpruzhnykova and A. Nieder}, 
journal={Journal of Cognitive Neuroscience}, 
title={Learning Recruits Neurons Representing Previously Established Associations in the Corvid Endbrain}, 
year={2017}, 
volume={29}, 
number={10}, 
pages={1712-1724}, 
abstract={<para>Crows quickly learn arbitrary associations. As a neuronal correlate of this behavior, single neurons in the corvid endbrain area nidopallium caudolaterale (NCL) change their response properties during association learning. In crows performing a delayed association task that required them to map both familiar and novel sample pictures to the same two choice pictures, NCL neurons established a common, prospective code for associations. Here, we report that neuronal tuning changes during learning were not distributed equally in the recorded population of NCL neurons. Instead, such learning-related changes relied almost exclusively on neurons which were already encoding familiar associations. Only in such neurons did behavioral improvements during learning of novel associations coincide with increasing selectivity over the learning process. The size and direction of selectivity for familiar and newly learned associations were highly correlated. These increases in selectivity for novel associations occurred only late in the delay period. Moreover, NCL neurons discriminated correct from erroneous trial outcome based on feedback signals at the end of the trial, particularly in newly learned associations. Our results indicate that task-relevant changes during association learning are not distributed within the population of corvid NCL neurons but rather are restricted to a specific group of association-selective neurons. Such association neurons in the multimodal cognitive integration area NCL likely play an important role during highly flexible behavior in corvids.</para>}, 
keywords={}, 
doi={10.1162/jocn_a_01152}, 
ISSN={0898-929X}, 
month={Oct},}
@INPROCEEDINGS{6507485, 
author={C. Reaño and A. J. Peña and F. Silla and J. Duato and R. Mayo and E. S. Quintana-Ortí}, 
booktitle={2012 19th International Conference on High Performance Computing}, 
title={CU2rCU: Towards the complete rCUDA remote GPU virtualization and sharing solution}, 
year={2012}, 
volume={}, 
number={}, 
pages={1-10}, 
abstract={GPUs are being increasingly embraced by the high performance computing and computational communities as an effective way of considerably reducing execution time by accelerating significant parts of their application codes. However, despite their extraordinary computing capabilities, the adoption of GPUs in current HPC clusters may present certain negative side-effects. In particular, to ease job scheduling in these platforms, a GPU is usually attached to every node of the cluster. In addition to increasing acquisition costs this favors that GPUs may frequently remain idle, as applications usually do not fully utilize them. On the other hand, idle GPUs consume non-negligible amounts of energy, which translates into very poor energy efficiency during idle cycles. rCUDA was recently developed as a software solution to address these concerns. Specifically, it is a middleware that allows transparently sharing a reduced number of GPUs among the nodes in a cluster. rCUDA thus increases the GPU-utilization rate, taking care of job scheduling. While the initial prototype versions of rCUDA demonstrated its functionality, they also revealed several concerns related with usability and performance. With respect to usability, in this paper we present a new component of the rCUDA suite that allows an automatic transformation of any CUDA source code, so that it can be effectively accommodated within this technology. In response to performance, we briefly show some interesting results, which will be deeply analyzed in future publications. The net outcome is a new version of rCUDA that allows, for any CUDA-compatible program, to use remote GPUs in a cluster with minimum overhead.}, 
keywords={graphics processing units;middleware;parallel processing;power aware computing;scheduling;virtualisation;CU2rCU;rCUDA remote GPU virtualization;sharing solution;high performance computing;computational communities;HPC clusters;energy efficiency;software solution;middleware;job scheduling;CUDA source code;CUDA-compatible program}, 
doi={10.1109/HiPC.2012.6507485}, 
ISSN={}, 
month={Dec},}
@ARTICLE{8341495, 
author={A. Ghoshal and J. Zhang and M. A. Roth and K. M. Xia and A. Y. Grama and S. Chaterji}, 
journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
title={A Distributed Classifier for MicroRNA Target Prediction with Validation Through TCGA Expression Data}, 
year={2018}, 
volume={15}, 
number={4}, 
pages={1037-1051}, 
abstract={Background: MicroRNAs (miRNAs) are approximately 22-nucleotide long regulatory RNA that mediate RNA interference by binding to cognate mRNA target regions. Here, we present a distributed kernel SVM-based binary classification scheme to predict miRNA targets. It captures the spatial profile of miRNA-mRNA interactions via smooth B-spline curves. This is accomplished separately for various input features, such as thermodynamic and sequence-based features. Further, we use a principled approach to uniformly model both canonical and non-canonical seed matches, using a novel seed enrichment metric. Finally, we verify our miRNA-mRNA pairings using an Elastic Net-based regression model on TCGA expression data for four cancer types to estimate the miRNAs that together regulate any given mRNA. Results: We present a suite of algorithms for miRNA target prediction, under the banner Avishkar, with superior prediction performance over the competition. Specifically, our final kernel SVM model, with an Apache Spark backend, achieves an average true positive rate (TPR) of more than 75 percent, when keeping the false positive rate of 20 percent, for non-canonical human miRNA target sites. This is an improvement of over 150 percent in the TPR for non-canonical sites, over the best-in-class algorithm. We are able to achieve such superior performance by representing the thermodynamic and sequence profiles of miRNA-mRNA interaction as curves, devising a novel seed enrichment metric, and learning an ensemble of miRNA family-specific kernel SVM classifiers. We provide an easy-to-use system for large-scale interactive analysis and prediction of miRNA targets. All operations in our system, namely candidate set generation, feature generation and transformation, training, prediction, and computing performance metrics are fully distributed and are scalable. Conclusions: We have developed an efficient SVM-based model for miRNA target prediction using recent CLIP-seq data, demonstrating superior performance, evaluated using ROC curves for different species (human or mouse), or different target types (canonical or non-canonical). We analyzed the agreement between the target pairings using CLIP-seq data and using expression data from four cancer types. To the best of our knowledge, we provide the first distributed framework for miRNA target prediction based on Apache Hadoop and Spark. Availability: All source code and sample data are publicly available at https://bitbucket.org/cellsandmachines/avishkar. Our scalable implementation of kernel SVM using Apache Spark, which can be used to solve large-scale non-linear binary classification problems, is available at https://bitbucket.org/cellsandmachines/kernelsvmspark.}, 
keywords={bioinformatics;learning (artificial intelligence);pattern classification;regression analysis;RNA;support vector machines;Elastic Net-based regression model;learning;nucleotide long regulatory RNA;miRNA family-specific kernel SVM classifiers;noncanonical human miRNA target sites;miRNA target prediction;miRNA-mRNA pairings;noncanonical seed matches;thermodynamic sequence-based features;miRNA-mRNA interaction;distributed kernel SVM-based binary classification scheme;mRNA target regions;TCGA expression data;MicroRNA target prediction;Cancer;Bioinformatics;Data models;Support vector machines;Computational modeling;Genomics;RNA;Support vector machines;binary classification;microRNA;CLIP-seq;non-canonical targets;kernel SVM;feature engineering;Apache Spark;ROC curves}, 
doi={10.1109/TCBB.2018.2828305}, 
ISSN={1545-5963}, 
month={July},}
@INPROCEEDINGS{6636731, 
author={J. Terzakis}, 
booktitle={2013 21st IEEE International Requirements Engineering Conference (RE)}, 
title={The impact of requirements on software quality across three product generations}, 
year={2013}, 
volume={}, 
number={}, 
pages={284-289}, 
abstract={In a previous case study, we presented data demonstrating the impact that a well-written and well-reviewed set of requirements had on software defects and other quality indicators between two generations of an Intel product. The first generation was coded from an unorganized collection of requirements that were reviewed infrequently and informally. In contrast, the second was developed based on a set of requirements stored in a Requirements Management database and formally reviewed at each revision. Quality indicators for the second software product all improved dramatically even with the increased complexity of the newer product. This paper will recap that study and then present data from a subsequent Intel case study revealing that quality enhancements continued on the third generation of the product. The third generation software was designed and coded using the final set of requirements from the second version as a starting point. Key product differentiators included changes to operate with a new Intel processor, the introduction of new hardware platforms and the addition of approximately fifty new features. Software development methodologies were nearly identical, with only the change to a continuous build process for source code check-in added. Despite the enhanced functionality and complexity in the third generation software, requirements defects, software defects, software sightings, feature commit vs. delivery (feature variance), defect closure efficiency rates, and number of days from project commit to customer release all improved from the second to the third generation of the software.}, 
keywords={formal specification;product development;software quality;software reliability;software defects;software quality indicators;Intel product;requirements management database;software product;quality enhancements;product generation;third generation software design;third generation software coding;product differentiators;Intel processor;hardware platforms;software development methodologies;source code;requirements defects;software sightings;feature commit;feature variance;delivery defect closure efficiency rates;feature commit rates;Software;Testing;Mobile communication;Complexity theory;Stability analysis;Indexes;Writing;Requirements specification;requirements defects;reviews;software defects;software quality;multi-generational software products}, 
doi={10.1109/RE.2013.6636731}, 
ISSN={1090-705X}, 
month={July},}
@ARTICLE{6133294, 
author={X. Ruan and Q. Yang and M. I. Alghamdi and S. Yin and X. Qin}, 
journal={IEEE Transactions on Dependable and Secure Computing}, 
title={ES-MPICH2: A Message Passing Interface with Enhanced Security}, 
year={2012}, 
volume={9}, 
number={3}, 
pages={361-374}, 
abstract={An increasing number of commodity clusters are connected to each other by public networks, which have become a potential threat to security sensitive parallel applications running on the clusters. To address this security issue, we developed a Message Passing Interface (MPI) implementation to preserve confidentiality of messages communicated among nodes of clusters in an unsecured network. We focus on M PI rather than other protocols, because M PI is one of the most popular communication protocols for parallel computing on clusters. Our MPI implementation-called ES-MPICH2-was built based on MPICH2 developed by the Argonne National Laboratory. Like MPICH2, ES-MPICH2 aims at supporting a large variety of computation and communication platforms like commodity clusters and high-speed networks. We integrated encryption and decryption algorithms into the MPICH2 library with the standard MPI interface and; thus, data confidentiality of MPI applications can be readily preserved without a need to change the source codes of the MPI applications. MPI-application programmers can fully configure any confidentiality services in MPICHI2, because a secured configuration file in ES-MPICH2 offers the programmers flexibility in choosing any cryptographic schemes and keys seamlessly incorporated in ES-MPICH2. We used the Sandia Micro Benchmark and Intel MPI Benchmark suites to evaluate and compare the performance of ES-MPICH2 with the original MPICH2 version. Our experiments show that overhead incurred by the confidentiality services in ES-MPICH2 is marginal for small messages. The security overhead in ES-MPICH2 becomes more pronounced with larger messages. Our results also show that security overhead can be significantly reduced in ES-MPICH2 by high-performance clusters. The executable binaries and source code of the ES-MPICH2 implementation are freely available at http:// www.eng.auburn.edu/~xqin/software/es-mpich2/.}, 
keywords={application program interfaces;computer network performance evaluation;computer network security;message authentication;message passing;parallel processing;private key cryptography;protocols;public key cryptography;workstation clusters;ES-MPICH2;message passing interface;enhanced security;commodity clusters;public networks;security sensitive parallel applications;message confidentiality preservation;unsecured network;communication protocols;parallel computing;Argonne National Laboratory;high-speed networks;integrated encryption algorithms;decryption algorithms;standard MPI interface;data confidentiality;cryptographic schemes;cryptographic keys;Intel MPI benchmark suites;Sandia Micro Benchmark suites;confidentiality services;source code;Sockets;Encryption;Libraries;Message passing;Three dimensional displays;Parallel computing;computer security;message passing interface;encryption.}, 
doi={10.1109/TDSC.2012.9}, 
ISSN={1545-5971}, 
month={May},}
@INPROCEEDINGS{6766582, 
author={J. I. Figueroa-Angulo and J. Savage-Carmona and E. Bribiesca-Correa and B. Escalante and R. S. Leder and L. E. Sucar}, 
booktitle={2013 16th International Conference on Advanced Robotics (ICAR)}, 
title={Recognition of arm activities based on Hidden Markov Models for natural interaction with service robots}, 
year={2013}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={This research presents a novel way of representing human motion and recognizing human activities from the skeleton output computed from RGB-D data from vision-based motion capture systems. The method uses a representation of the skeleton which is invariant to rotation and translation, based on Orthogonal Direction Change Chain Codes, as observations for a single Discrete Connected Hidden Markov Model formed by a set of multiple Hidden Markov Models for simple activities, which are merged using a grammar-based structure. The purpose of this research is to provide a service robot with the capability of human activity awareness, which can be used for action planning with implicit and indirect Human-Robot Interaction.}, 
keywords={hidden Markov models;human-robot interaction;image motion analysis;robot vision;service robots;arm activity recognition;discrete connected hidden Markov model;service robot;human motion;skeleton representation;RGB-D data;vision-based motion capture system;orthogonal direction change chain code;grammar-based structure;human activity awareness;action planning;human-robot interaction;Hidden Markov models;Joints;Image recognition;Cameras;Robot sensing systems;Communities;Hidden Markov Models;Activity Recognition;Motion Recognition;Human-Machine Interaction;Pattern Recognition;Machine Learning;Viterbi Path}, 
doi={10.1109/ICAR.2013.6766582}, 
ISSN={}, 
month={Nov},}
@ARTICLE{6265042, 
author={B. Shen and B. Nelson and S. Cheung and W. Tao}, 
journal={Computing in Science Engineering}, 
title={Improving NASA's Multiscale Modeling Framework for Tropical Cyclone Climate Study}, 
year={2013}, 
volume={15}, 
number={5}, 
pages={56-67}, 
abstract={One of the current challenges in tropical cyclone (TC) research is how to improve our understanding of TC interannual variability and the impact of climate change on TCs. Recent advances in global modeling, visualization, and supercomputing technologies at NASA show potential for such studies. In this article, the authors discuss recent scalability improvement to the multiscale modeling framework (MMF) that makes it feasible to perform long-term TC-resolving simulations. The MMF consists of the finite-volume general circulation model (fvGCM), supplemented by a copy of the Goddard cumulus ensemble model (GCE) at each of the fvGCM grid points, giving 13,104 GCE copies. The original fvGCM implementation has a 1D data decomposition; the revised MMF implementation retains the 1D decomposition for most of the code, but uses a 2D decomposition for the massive copies of GCEs. Because the vast majority of computation time in the MMF is spent computing the GCEs, this approach can achieve excellent speedup without incurring the cost of modifying the entire code. Intelligent process mapping allows differing numbers of processes to be assigned to each domain for load balancing. The revised parallel implementation shows highly promising scalability, obtaining a nearly 80-fold speedup by increasing the number of cores from 30 to 3,335.}, 
keywords={atmospheric movements;data handling;finite volume methods;geophysics computing;learning (artificial intelligence);meteorology;parallel processing;resource allocation;scalability;revised parallel implementation;load balancing;intelligent process mapping;2D data decomposition;1D data decomposition;GCE;Goddard cumulus ensemble model;fvGCM;finite-volume general circulation model;long-term TC-resolving simulations;MMF;supercomputing technology;visualization technology;global modeling technology;climate change;TC interannual variability;TC research;tropical cyclone climate study;National Aeronautics and Space Administration;NASA multiscale modeling framework;Atmospheric modeling;Clouds;Computational modeling;NASA;Meteorology;Hurricanes;Tropical cyclones;NASA;distributed programming;hurricane modeling;climate modeling;software;software engineering;scientific computing}, 
doi={10.1109/MCSE.2012.90}, 
ISSN={1521-9615}, 
month={Sep.},}
@ARTICLE{8213108, 
author={T. Zhang and H. Jiang and X. Luo and A. T. S. Chan}, 
journal={The Computer Journal}, 
title={A Literature Review of Research in Bug Resolution: Tasks, Challenges and Future Directions}, 
year={2016}, 
volume={59}, 
number={5}, 
pages={741-773}, 
abstract={Due to the increasing scale and complexity of software products, software maintenance especially on bug resolution has become a challenging task. Generally in large-scale software programs, developers depend on software artifacts (e.g., bug report, source code and change history) in bug repositories to complete the bug resolution task. However, a mountain of submitted bug reports every day increase the developers' workload. Therefore, ‘How to effectively resolve software defects by utilizing software artifacts?’ becomes a research hotspot in software maintenance. Considerable studies have been done on bug resolution by using multi-techniques, which cover data mining, machine learning and natural language processing. In this paper, we present a literature survey on tasks, challenges and future directions of bug resolution in software maintenance process. Our investigation concerns the most important phases in bug resolution, including bug understanding, bug triage and bug fixing. Moreover, we present the advantages and disadvantages of each study. Finally, based on the investigation and comparison results, we propose the future research directions of bug resolution.}, 
keywords={bug resolution;bug report;software maintenance;bug understanding;bug triage;bug fixing}, 
doi={10.1093/comjnl/bxv114}, 
ISSN={0010-4620}, 
month={May},}
@INPROCEEDINGS{7322814, 
author={P. P. Kute and P. Dakhole and P. Palsodkar}, 
booktitle={2015 International Conference on Communications and Signal Processing (ICCSP)}, 
title={Cross coupled digital NAND gate comparator based flash ADC}, 
year={2015}, 
volume={}, 
number={}, 
pages={1718-1721}, 
abstract={This paper demonstrates the Flash ADC which is constructed using digital 3-input cross coupled NAND gates. These cross coupled configuration of NAND gates form a comparator of flash ADC. It is latch comparator which operates on single phase clock Φ. The output of comparator is thermometer code. An encoder is constructed that encodes the thermometer code to binary as output of comparator. The design is simulated in 180nm technology and implement 4 bit flash ADC version. The Flash ADC is designed in TANNER S-EDIT 13.0.}, 
keywords={analogue-digital conversion;comparators (circuits);coupled circuits;flash memories;logic gates;cross coupled digital NAND gate comparator;cross coupled configuration;flash ADC comparator;latch comparator;single phase clock;thermometer code;Logic gates;Latches;Clocks;Decoding;Analog to digital convertor (ADC);Cross coupled;Clocked comparator}, 
doi={10.1109/ICCSP.2015.7322814}, 
ISSN={}, 
month={April},}
@ARTICLE{6866884, 
author={S. Jia and L. Shen and Q. Li}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={Gabor Feature-Based Collaborative Representation for Hyperspectral Imagery Classification}, 
year={2015}, 
volume={53}, 
number={2}, 
pages={1118-1129}, 
abstract={Sparse-representation-based classification (SRC) assigns a test sample to the class with minimum representation error via a sparse linear combination of all the training samples, which has successfully been applied to several pattern recognition problems. According to compressive sensing theory, the l<sub>1</sub>-norm minimization could yield the same sparse solution as the l<sub>0</sub> norm under certain conditions. However, the computational complexity of the l<sub>1</sub>-norm optimization process is often too high for large-scale high-dimensional data, such as hyperspectral imagery (HSI). To make matter worse, a large number of training data are required to cover the whole sample space, which is difficult to obtain for hyperspectral data in practice. Recent advances have revealed that it is the collaborative representation but not the l<sub>1</sub>-norm sparsity that makes the SRC scheme powerful. Therefore, in this paper, a 3-D Gabor feature-based collaborative representation (3GCR) approach is proposed for HSI classification. When 3-D Gabor transformation could significantly increase the discrimination power of material features, a nonparametric and effective l<sub>2</sub>-norm collaborative representation method is developed to calculate the coefficients. Due to the simplicity of the method, the computational cost has been substantially reduced; thus, all the extracted Gabor features can be directly utilized to code the test sample, which conversely makes the l<sub>2</sub>-norm collaborative representation robust to noise and greatly improves the classification accuracy. The extensive experiments on two real hyperspectral data sets have shown higher performance of the proposed 3GCR over the state-of-the-art methods in the literature, in terms of both the classifier complexity and generalization ability from very small training sets.}, 
keywords={compressed sensing;computational complexity;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image representation;learning (artificial intelligence);minimisation;hyperspectral imagery classification;sparse-representation-based classification;sparse linear combination;training sample;pattern recognition;l1-norm minimization;l0 norm minimization;l1-norm optimization;computational complexity;SRC scheme;3D Gabor feature-based collaborative representation approach;3GCR approach;HSI classification;3D Gabor transformation;l2-norm collaborative representation method;Gabor feature extraction;compressive sensing theory;Training;Collaboration;Feature extraction;Hyperspectral imaging;Optimization;Minimization;Collaborative representation;feature extraction;hyperspectral imagery (HSI) classification;Collaborative representation;feature extraction;hyperspectral imagery (HSI) classification}, 
doi={10.1109/TGRS.2014.2334608}, 
ISSN={0196-2892}, 
month={Feb},}
@INPROCEEDINGS{8418103, 
author={C. Bandirali and S. Lodi and G. Moro and A. Pagliarani and C. Sartori}, 
booktitle={2018 32nd International Conference on Advanced Information Networking and Applications Workshops (WAINA)}, 
title={Parallel Primitives for Vendor-Agnostic Implementation of Big Data Mining Algorithms}, 
year={2018}, 
volume={}, 
number={}, 
pages={396-401}, 
abstract={In the age of Big Data, scalable algorithm implementations as well as powerful computational resources are required. For data mining and data analytics the support of big data platforms is becoming increasingly important, since they provide algorithm implementations with all the resources needed for their execution. However, choosing the best platform might depend on several constraints, including but not limited to computational resources, storage resources, target tasks, service costs. Sometimes it may be necessary to switch from one platform to another depending on the constraints. As a consequence, it is desirable to reuse as much algorithm code as possible, so as to simplify the setup in new target platforms. Unfortunately each big data platform has its own peculiarity, especially to deal with parallelism. This impacts on algorithm implementation, which generally needs to be modified before being executed. This work introduces functional parallel primitives to define the parallelizable parts of algorithms in a uniform way, independent of the target platform. Primitives are then transformed by a compiler into skeletons, which are finally deployed on vendor-dependent frameworks. The procedure proposed aids not only in terms of code reuse but also in terms of parallelization, because programmer's expertise is not demanded. Indeed, it is the compiler that entirely manages and optimizes algorithm parallelization. The experiments performed show that the transformation process does not negatively affect algorithm performance.}, 
keywords={Big Data;data analysis;data mining;parallel algorithms;vendor-agnostic implementation;data analytics;storage resources;algorithm code;functional parallel primitives;vendor-dependent frameworks;algorithm parallelization;Big Data mining algorithms;compiler;Big Data;Skeleton;Data mining;Task analysis;Sparks;Machine learning algorithms;Data analysis;data mining;big data;big data analytics;big data platforms}, 
doi={10.1109/WAINA.2018.00118}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7516000, 
author={Z. U. Huda and R. Atre and A. Jannesari and F. Wolf}, 
booktitle={2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
title={Automatic Parallel Pattern Detection in the Algorithm Structure Design Space}, 
year={2016}, 
volume={}, 
number={}, 
pages={43-52}, 
abstract={Parallel design patterns have been developed to help programmers efficiently design and implement parallel applications. However, identifying a suitable parallel pattern for a specific code region in a sequential application is a difficult task. Transforming an application according to support structures applicable to these parallel patterns is also very challenging. In this paper, we present a novel approach to automatically find parallel patterns in the algorithm structure design space of sequential applications. In our approach, we classify code blocks in a region according to the appropriate supportstructure of the detected pattern. This classification eases the transformation of a sequential application into its parallel version. Weevaluated our approach on 17 applications from four different benchmark suites. Our method identified suitable algorithm structure patterns in the sequential applications. We confirmed our results by comparing them with the existing parallel versions of these applications. We also implemented the patterns we detected in cases in which parallel implementations were not available and achieved speedups of up to 14x.}, 
keywords={parallel programming;automatic parallel pattern detection;algorithm structure design space;parallel design patterns;parallel applications;code region;code blocks;algorithm structure patterns;sequential applications;Pipelines;Parallel processing;Positron emission tomography;Linear regression;Algorithm design and analysis;Benchmark testing;Software;parallelism;parallel patterns;task paralleism}, 
doi={10.1109/IPDPS.2016.60}, 
ISSN={1530-2075}, 
month={May},}
@INPROCEEDINGS{6698913, 
author={H. A. Nguyen and A. T. Nguyen and T. N. Nguyen}, 
booktitle={2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)}, 
title={Filtering noise in mixed-purpose fixing commits to improve defect prediction and localization}, 
year={2013}, 
volume={}, 
number={}, 
pages={138-147}, 
abstract={In open-source software projects, during fixing software faults, developers sometimes also perform other types of non-fixing code changes such as functionality enhancement, code restructuring/improving, or documentation. They commit non-fixing changes together with the fixing ones in the same transaction. We call them mixed-purpose fixing commits (MFCs). We have conducted an empirical study on MFCs in several popular open-source projects. Our results showed that MFCs are about 11%-39% of total fixing commits. In 3%-41% of MFCs, developers performed other change types without indicating them in the commit logs. Our study also showed that mining software repositories (MSR) approaches that rely on the recovery of the history of fixed/buggy files are affected by the noisy data where non-fixing changes in MFCs are considered as fixing ones. The results of our study motivated us to develop Cardo, a tool to identify MFCs and filter non-fixing changed files in the change sets of the fixing commits. It uses natural language processing to analyze the sentences in commit logs and program analysis to cluster the changes in the change sets to determine if a changed file is for non-fixing. Our empirical evaluation on several open-source projects showed that Cardo achieves on average 93% precision, and existing MSR approaches can be relatively improved up to 32% with data filtered by Cardo.}, 
keywords={program debugging;program diagnostics;public domain software;noise filtering;defect prediction;defect localization;mixed-purpose fixing commits;MFCs;open-source software projects;mining software repositories approaches;MSR approaches;fixed files;Cardo;nonfixing changed files filtering;natural language processing;commit logs;program analysis;buggy files;Noise;Feature extraction;History;Open source software;Documentation;Data mining}, 
doi={10.1109/ISSRE.2013.6698913}, 
ISSN={1071-9458}, 
month={Nov},}
@INPROCEEDINGS{7016370, 
author={M. Dixon and J. Lotze and M. Zubair}, 
booktitle={2014 Seventh Workshop on High Performance Computational Finance}, 
title={A Portable and Fast Stochastic Volatility Model Calibration Using Multi and Many-Core Processors}, 
year={2014}, 
volume={}, 
number={}, 
pages={23-28}, 
abstract={Financial markets change precipitously and on-demand pricing and risk models must be constantly recalibrated to reduce risk. However, certain classes of models are computationally intensive to robustly calibrate to intraday pricesstochastic volatility models being an archetypal example due to the non-convexity of the objective function. In order to accelerate this procedure through parallel implementation,nancial application developers are faced with an ever growing plethora of low-level high-performance computing frameworks such as OpenMP, OpenCL, CUDA, or SIMD intrinsics, and forced to make a trade-off between performance versus the portability,exibility and modularity of the code required to facilitate rapid in-house model development and productionization.This paper describes the acceleration of stochastic volatility model calibration on multi-core CPUs and GPUs using the Xcelerit platform. By adopting a simple dataow programming model, the Xcelerit platform enables the application developer to write sequential, high-level C++ code, without concern for low-level high-performance computing frameworks. This platform provides the portability,exibility and modularity required by application developers. Speedups of up to 30x and 293x are respectively achieved on an Intel Xeon CPU and NVIDIA Tesla K40 GPU, compared to a sequential CPU implementation. The Xcelerit platform implementation is further shown to be equivalent in performance to a low-level CUDA version. Overall, we are able to reduce the entire calibration process time of the sequential implementation from 6; 189 seconds to 183:8 and 17:8 seconds on the CPU and GPU respectively without requiring the developer to reimplement in low-level high performance computing frameworks.}, 
keywords={C++ language;data flow computing;financial data processing;graphics processing units;multiprocessing systems;parallel architectures;pricing;risk management;stock markets;fast stochastic volatility model calibration;multicore processors;many-core processors;financial markets;on-demand pricing model;risk model;risk reduction;objective function nonconvexity;low-level high-performance computing frameworks;OpenMP intrinsic;OpenCL intrinsic;CUDA intrinsic;SIMD intrinsic;multicore CPU;multicore GPU;Xcelerit platform;dataflow programming model;high-level C++ code;sequential code;NVIDIA Tesla K40 GPU;Intel Xeon CPU;Graphics processing units;Calibration;Computational modeling;Stochastic processes;Mathematical model;Optimization;Data models;Calibration; Stochastic Volatility; GPGPU; C++}, 
doi={10.1109/WHPCF.2014.12}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6224277, 
author={C. Sadowski and J. Yi and S. Kim}, 
booktitle={2012 9th IEEE Working Conference on Mining Software Repositories (MSR)}, 
title={The evolution of data races}, 
year={2012}, 
volume={}, 
number={}, 
pages={171-174}, 
abstract={Concurrency bugs are notoriously difficult to find and fix. Several prior empirical studies have identified the prevalence and challenges of concurrency bugs in open source projects, and several existing tools can be used to identify concurrency errors such as data races. However, little is known about how concurrency bugs evolve over time. In this paper, we examine the evolution of data races by analyzing samples of the committed code in two open source projects over a multi-year period. Specifically, we identify how the data races in these programs change over time.}, 
keywords={concurrency control;program diagnostics;public domain software;concurrency bugs;open source projects;concurrency error identification;data race evolution;Postal services;Concurrent computing;Computer bugs;Calendars;Detectors;XML;History}, 
doi={10.1109/MSR.2012.6224277}, 
ISSN={2160-1860}, 
month={June},}
@INPROCEEDINGS{7190558, 
author={C. w. Maina}, 
booktitle={2015 IST-Africa Conference}, 
title={Bioacoustic approaches to biodiversity monitoring and conservation in Kenya}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Kenya's rich biodiversity faces a number of threats including human encroachment, poaching and climate change. Since Kenya is a developing country, there is need to manage the sometimes competing interests of development, such as infrastructure development, and conservation. To achieve this, tools to effectively monitor the state of Kenya's various ecosystems are essential. In this paper we propose a biodiversity monitoring software tool that integrates acoustic indices of biodiversity, recognition of species of interest based on their vocalizations and acoustic census. This tool can be used by non-experts to determine the current state of their ecosystems by monitoring the state of bird species that serve as indicator taxa and whose abundance is related to the abundance of other terrestrial vertebrates including the “big five”. The tool we propose exploits state-of-the art advances in signal processing and machine learning to perform biodiversity monitoring, bird species detection and census in a joint framework. Using publicly available data we demonstrate how current acoustic indices of biodiversity can be improved by incorporating machine learning based audio segmentation algorithms. We also show how open source toolkits can be used to build bird species recognition systems. Code to reproduce the experiments in this paper is available on Github at https://github.com/ciiram/BirdPy.}, 
keywords={acoustic signal processing;bioacoustics;biology computing;ecology;bioacoustic approaches;biodiversity conservation;Kenya;human encroachment;poaching;climate change;infrastructure development;ecosystems;biodiversity monitoring software tool;species recognition;signal processing;machine learning;bird species detection;Birds;Biodiversity;Acoustics;Monitoring;Entropy;Audio recording;Feature extraction;Biodiversity;conservation;bird species recognition;open source software}, 
doi={10.1109/ISTAFRICA.2015.7190558}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{8437896, 
author={A. Rajagopalan and A. Thangaraj and S. Agrawal}, 
booktitle={2018 IEEE International Symposium on Information Theory (ISIT)}, 
title={Wiretap Polar Codes in Encryption Schemes Based on Learning with Errors Problem}, 
year={2018}, 
volume={}, 
number={}, 
pages={1146-1150}, 
abstract={The Learning with Errors (LWE) problem has been extensively studied in cryptography due to its strong hardness guarantees, efficiency and expressiveness in constructing advanced cryptographic primitives. In this work, we show that using polar codes in conjunction with LWE-based encryption yields several advantages. To begin, we demonstrate the obvious improvements in the efficiency or rate of information transmission in the LWE-based scheme by leveraging polar coding (with no change in the cryptographic security guarantee). Next, we integrate wiretap polar coding with LWE-based encryption to ensure provable semantic security over a wiretap channel in addition to cryptographic security based on the hardness of LWE. To the best of our knowledge this is the first wiretap code to have cryptographic security guarantees as well. Finally, we study the security of the private key used in LWE-based encryption with wiretap polar coding, and propose a key refresh method using random bits used in wiretap coding. Under a known-plaintext attack, we show that non-vanishing information-theoretic secrecy can be achieved for the key. We believe our approach is at least as interesting as our final results: our work combines cryptography and coding theory in a novel “non blackbox-way” which may be relevant to other scenarios as well.}, 
keywords={}, 
doi={10.1109/ISIT.2018.8437896}, 
ISSN={2157-8117}, 
month={June},}
@ARTICLE{7088591, 
author={C. Qian and Z. Xu}, 
journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
title={Robust Visual Tracking via Sparse Representation Under Subclass Discriminant Constraint}, 
year={2016}, 
volume={26}, 
number={7}, 
pages={1293-1307}, 
abstract={In this paper, we propose a method for visual tracking based on local sparse representation. Image patches from the object and the background are split into image blocks to construct local representations. Within the subclass discriminant framework, a discriminative subspace is learned to distinguish the object image blocks from the background image blocks while preserving their multimodal structure. A dictionary is constructed using the centers of the object subclasses. With this dictionary, sparse coding is implemented on the projected vectors corresponding to the image blocks, and the sparse coefficients are concatenated to obtain a local sparse code as the feature that represents the image patch. Considering the subclass discriminant constraint and the sparsity constraint imposed on the sparse coding, the subspace learning and sparse representation problems are converted into a joint optimization problem with respect to a transformation matrix and sparse coefficients. To enhance the tracking accuracy, two dictionaries are devised, one to incorporate the original observations of the target and the other to incorporate the latest observations, thereby providing two templates to characterize the appearance of the target. Histogram intersection over the local sparse codes provides an evaluation of the confidence. Finally, the candidate with the maximal confidence is selected as the object image patch. Compared with several state-of-the-art algorithms, our method demonstrates a superior performance when applied to challenging sequences.}, 
keywords={image coding;image representation;optimisation;sparse matrices;robust visual tracking;local sparse representation;subclass discriminant constraint;image patch;background image blocks;object image blocks;multimodal structure preservation;sparse coding;sparse coefficient concatenation;sparsity constraint;sparse coding;subspace learning;joint optimization problem;Dictionaries;Target tracking;Visualization;Training;Sparse matrices;Encoding;Optimization;Dictionary learning;sparse representation;subclass discriminant constraint (SDC);visual tracking}, 
doi={10.1109/TCSVT.2015.2424091}, 
ISSN={1051-8215}, 
month={July},}
@INPROCEEDINGS{7515485, 
author={B. Li and C. Vendome and M. Linares-Vásquez and D. Poshyvanyk and N. A. Kraft}, 
booktitle={2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)}, 
title={Automatically Documenting Unit Test Cases}, 
year={2016}, 
volume={}, 
number={}, 
pages={341-352}, 
abstract={Maintaining unit test cases is important during the maintenance and evolution of a software system. In particular, automatically documenting these unit test cases can ameliorate the burden on developers maintaining them. For instance, by relying on up-to-date documentation, developers can more easily identify test cases that relate to some new or modified functionality of the system. We surveyed 212 developers (both industrial and open-source) to understand their perspective towards writing, maintaining, and documenting unit test cases. In addition, we mined change histories of C# software systems and empirically found that unit test methods seldom had preceding comments and infrequently had inner comments, and both were rarely modified as those methods were modified. In order to support developers in maintaining unit test cases, we propose a novel approach - UnitTestScribe - that combines static analysis, natural language processing, backward slicing, and code summarization techniques to automatically generate natural language documentation of unit test cases. We evaluated UnitTestScribe on four subject systems by means of an online survey with industrial developers and graduate students. In general, participants indicated that UnitTestScribe descriptions are complete, concise, and easy to read.}, 
keywords={C# language;natural language processing;program slicing;program testing;software maintenance;system documentation;unit test cases automatic documentation;unit test case maintenance;software system maintenance;software system evolution;unit test case writing;C# software systems;UnitTestScribe;static analysis;natural language processing;backward slicing;code summarization techniques;natural language documentation;Software systems;Documentation;Open source software;Electronic mail;Maintenance engineering;History;unit test;summarization;natural language processing}, 
doi={10.1109/ICST.2016.30}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{8026092, 
author={M. Wagner and V. López and J. Morillo and C. Cavazzoni and F. Affinito and J. Giménez and J. Labarta}, 
booktitle={2017 46th International Conference on Parallel Processing Workshops (ICPPW)}, 
title={Performance Analysis and Optimization of the FFTXlib on the Intel Knights Landing Architecture}, 
year={2017}, 
volume={}, 
number={}, 
pages={243-250}, 
abstract={In this paper, we address the decreasing performance of the FFTXlib, the Fast Fourier Transformation (FFT) kernel of Quantum ESPRESSO, when scaling to a full KNL node. An increased performance in the FFTXlib will likewise increase the performance of the entire Quantum ESPRESSO code one of the most used plane-wave DFT codes in the community of material science. Our approach focuses on, first, overlapping computation and communication and, second, decreasing resource contention for higher compute efficiency. In order to achieve this we use the OmpSs programming model based on task dependencies. We allow overlapping of computation and communication by converting all steps of the FFT into tasks following a flow dependency. In the same way, we decrease resource contention by converting each FFT into an individual task that can be scheduled asynchronously. In both cases, multiple FFTs can be computed in parallel. The task-based optimizations are implemented in the FFTXlib and show up to 10% runtime reduction on the already highly optimized version. Since the task scheduling is done dynamically during execution by the parallel runtime, not statically by the user, it also frees the user from finding the ideal parallel configuration himself.}, 
keywords={fast Fourier transforms;parallel processing;resource allocation;software architecture;software performance evaluation;HPC;high performance computing;task-based optimizations;task dependencies;OmpSs programming model;resource contention;plane-wave DFT codes;Quantum ESPRESSO code;fast Fourier transformation;Intel Knights Landing architecture;FFTXlib;performance optimization;performance analysis;Tools;Runtime;Computer architecture;Kernel;Optimization;Discrete Fourier transforms;Programming;Performance Analysis;Tracing;Tools;KNL;Knights Landing;Xeon Phi;HPC;Extrae;Paraver;Quantum Espresso;FFTXlib}, 
doi={10.1109/ICPPW.2017.44}, 
ISSN={1530-2016}, 
month={Aug},}
@INPROCEEDINGS{8602848, 
author={A. Das and N. A. Touba}, 
booktitle={2018 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)}, 
title={Efficient Non-Binary Hamming Codes for Limited Magnitude Errors in MLC PCMs}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Emerging non-volatile main memories (e.g. phase change memories) have been the continuous focus of research currently. These memories provide an attractive alternative to DRAM with their high density and low cost. But the dominant error models in these memories are of limited magnitude caused by resistance drifts. Hamming codes have been used extensively to protect DRAM due to their low decoding latency and low redundancy as well. But with limited magnitude errors, traditional Hamming codes prove to be inefficient. This paper proposes a new systematic limited magnitude error correcting non-binary Hamming code specifically to address limited magnitude errors in multilevel cell memories storing multiple bits per cell. A general construction methodology is presented to correct errors of limited magnitude and is compared to existing schemes addressing limited magnitude errors in phase change memories. A syndrome analysis is done to show the reduction in total number of syndromes for limited magnitude error models. It is shown that the proposed codes provide better latency and complexity compared to existing limited magnitude error correcting non-binary Hamming codes. It is also shown that the proposed codes achieve better redundancy compared to the symbol extended version of binary Hamming codes.}, 
keywords={Parity check codes;Resistance;Phase change materials;Redundancy;Decoding;Error correction codes;Complexity theory}, 
doi={10.1109/DFT.2018.8602848}, 
ISSN={2377-7966}, 
month={Oct},}
@INPROCEEDINGS{8545874, 
author={A. A. Maliavko}, 
booktitle={2018 XIV International Scientific-Technical Conference on Actual Problems of Electronics Instrument Engineering (APEIE)}, 
title={The Lexical and Syntactic Analyzers of the Translator for the EI Language}, 
year={2018}, 
volume={}, 
number={}, 
pages={360-364}, 
abstract={This paper contains the task of implementation of functionally-imperative programming language EI, the composition and structure of the its translator, a brief description of the lexical and syntactic rules of the language, the algorithms and the basic functions of the lexical and syntactic analyzers. It describes fragments of formal definitions of lexic and syntax of the El-Ianguage that was used for the automated construction of lexical and syntactic analyzers of the compiler in C++ using the client-server package Webtranslab. It describes also the basic algorithms of lexical analyzer (scanner), which in this version of the compiler performs macroprocessing, files including, deleting all the insignificant character sequences and transformation of correct words of language into an internal representation - tokens. We consider the structure and functions of a parser that performs a descending analysis. This parser constructed in the form of a stack automaton with multi-states controlled by the current input token and the current state fields. The composition of the state cell of the automaton and the algorithms of its operation are described, as well as the mechanisms for performing the operations that are embedded in the grammar and intended to implement the functionality of the semantic analyzer and code generator. The current state of realization of two constituent parts of the translator for EI-language - scanner and parser - is described.}, 
keywords={C++ language;client-server systems;computational linguistics;functional programming;grammars;natural language processing;program compilers;programming languages;lexical analyzer;semantic analyzer;code generator;syntactic analyzers;syntactic rules;lexical rules;parser;macroprocessing;client-server package Webtranslab;C++;compiler;functionally-imperative programming language;EI language translator;Syntactics;Casting;Grammar;Semantics;Programming;C++ languages;Translator;lexical and syntactic analysis;regular expression systems;formal grammars}, 
doi={10.1109/APEIE.2018.8545874}, 
ISSN={2473-8573}, 
month={Oct},}
@INPROCEEDINGS{6297166, 
author={Fazal-e-Amin and A. K. Mahmood and A. Oxley}, 
booktitle={2012 International Conference on Computer Information Science (ICCIS)}, 
title={An evolutionary study of reusability in Open Source Software}, 
year={2012}, 
volume={2}, 
number={}, 
pages={967-972}, 
abstract={The phenomenon of evolution is closely related to Open Source Software (OSS) as there is a frequent release of versions. Improvements to the software are due to the enormous contributions made by developers. Software reusability is also seen as a necessary characteristic of OSS. In this paper, a conceptual model for reusability assessment is presented and reusability of the software is studied during evolution. The attributes of reusability of different versions are assessed and compared. The relationship between the attributes of reusability is analyzed. The experiment conducted in this paper is one of the potential applications of our proposed reusability assessment model. The study helps to understand the evolution of software from the perspective of reusability. The change in size (e.g. number of lines of code; number of methods/classes), maintainability index complexity, etc. are studied in different versions to analyze their effect on reusability of the software. The results justify the proposed reusability attribute model.}, 
keywords={configuration management;public domain software;software maintenance;software reusability;evolutionary study;software reusability;open source software;software evolution;OSS;software version;reusability assessment model;maintainability index complexity;reusability attribute model;Software;World Wide Web;Measurement;Software reusability;Evolution;Open Source Software}, 
doi={10.1109/ICCISci.2012.6297166}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7913021, 
author={K. Dhar and M. A. Rahman and M. A. Ullah}, 
booktitle={2017 International Conference on Electrical, Computer and Communication Engineering (ECCE)}, 
title={Interactive generalized keyboard driver for Bengali Braille Embosser}, 
year={2017}, 
volume={}, 
number={}, 
pages={850-854}, 
abstract={Bengali Braille Embosser is a very essential device for learning of visually impaired person in Bangladesh. This paper presents an interactive generalized keyboard driver for Bengali Braille Embosser. This driver is operated by single AVR microcontroller. It can interface any PC keyboard through PS2 interface protocol with the Braille Embosser. The driver is interactive and can communicate with visually impaired person by generating sound of Bengali alphabet after pressing related key of the keyboard. In this paper the Bijoy keyboard layout is used. This driver can stores the written text in memory (micro SD card) and playback the sound of the text also. The designed device supports 16 GB of micro SD storage. This keyboard driver serves the user to edit the written text without help of visual person. So, this device makes a visually impaired person independent of writing and reading. At the same time the device makes Braille code from the written text for printing the Braille text on the Braille paper.}, 
keywords={handicapped aids;keyboards;peripheral interfaces;interactive generalized keyboard driver;Bengali Braille embosser;visually impaired person learning;Bangladesh;AVR microcontroller;PC keyboard;PS2 interface protocol;Bengali alphabet;Bijoy keyboard layout;microSD card;Braille paper;Braille text printing;Keyboards;Microcontrollers;Protocols;Visualization;Presses;Pressing;Ports (Computers);Braille;Embosser;SPI bus;PC keyboard;micro SD}, 
doi={10.1109/ECACE.2017.7913021}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7969440, 
author={A. Ghannem and M. S. Hamdi and M. Kessentini and H. H. Ammar}, 
booktitle={2017 IEEE Congress on Evolutionary Computation (CEC)}, 
title={Search-based requirements traceability recovery: A multi-objective approach}, 
year={2017}, 
volume={}, 
number={}, 
pages={1183-1190}, 
abstract={Software systems nowadays are complex and difficult to maintain due to the necessity of continuous change and adaptation. One of the challenges in software maintenance is keeping requirements traceability up to date automatically. The process of generating requirements traceability is time-consuming and error-prone. Currently, most available tools do not support the automated recovery of traceability links. In some situations, companies accumulate the history of changes from past maintenance experiences. In this paper, we consider requirements traceability recovery as a multi objective search problem in which we seek to assign each requirement to one or many software elements (code elements, API documentation, and comments) by taking into account the recency of change, the frequency of change, and the semantic similarity between the description of the requirement and the software element. We use the Non-dominated Sorting Genetic Algorithm (NSGA-II) to find the best compromise between these three objectives. We report the results of our experiments on three open source projects.}, 
keywords={genetic algorithms;software maintenance;search-based requirements traceability recovery;multiobjective approach;software systems;software maintenance;requirements traceability;traceability links recovery;software elements;nondominated sorting genetic algorithm;Software;Semantics;Maintenance engineering;Tools;History;Frequency measurement;Search problems;Search based Software Engineering;Requirements Engineering;Requirements Traceability;NSGA-II;Pareto Front}, 
doi={10.1109/CEC.2017.7969440}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{7847646, 
author={S. Padmanabha and A. Lukefahr and R. Das and S. Mahlke}, 
booktitle={2013 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
title={Trace based phase prediction for tightly-coupled heterogeneous cores}, 
year={2013}, 
volume={}, 
number={}, 
pages={445-456}, 
abstract={Heterogeneous multicore systems are composed of multiple cores with varying energy and performance characteristics. A controller dynamically detects phase changes in applications and migrates execution onto the most efficient core that meets the performance requirements. In this paper, we show that existing techniques that react to performance changes break down at fine-grain intervals, as performance variations between consecutive intervals are high. We propose a predictive trace-based switching controller that predicts an upcoming phase change in a program and preemptively migrates execution onto a more suitable core. This prediction is based on a phase's individual history and the current program context. Our implementation detects re-peatable code sequences to build history, uses these histories to predict an phase change, and preemptively migrates execution to the most appropriate core. We compare our method to phase prediction schemes that track the frequency of code blocks touched during execution as well as traditional reactive controllers, and demonstrate significant increases in prediction accuracy at fine-granularities. For a big-little heterogeneous system that is comprised of a high performing out-of-order core (Big) and an energy-efficient, in-order core (Little), at granularities of 300 instructions, the trace based predictor can spend 28% of execution time on the Little, while targeting a maximum performance degradation of 5%. This translates to an increased energy savings of 15% on average over running only on Big, representing a 60% increase over existing techniques.}, 
keywords={multiprocessing systems;power aware computing;trace based phase prediction;heterogeneous multicore systems;predictive trace-based switching controller;repeatable code sequence detection;fine-grain intervals;Switches;Multicore processing;Program processors;Monitoring;Measurement;heterogeneous processors;fine-grained phase prediction;energy-efficiency}, 
doi={}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7541372, 
author={F. Sala and R. Gabrys and C. Schoeny and K. Mazooji and L. Dolecek}, 
booktitle={2016 IEEE International Symposium on Information Theory (ISIT)}, 
title={Exact sequence reconstruction for insertion-correcting codes}, 
year={2016}, 
volume={}, 
number={}, 
pages={615-619}, 
abstract={We study the problem of perfectly reconstructing sequences from traces. The sequences are codewords from a deletion/insertion-correcting code and the traces are the result of corruption by a fixed number of symbol insertions (larger than the minimum edit distance of the code.) This is the general version of a problem tackled by Levenshtein for uncoded sequences. We introduce an exact formula for the maximum number of common supersequences shared by sequences at a certain edit distance, yielding a tight upper bound on the number of distinct traces necessary to guarantee exact reconstruction. We apply our results to the famous single deletion/insertion-correcting Varshamov-Tenengolts (VT) codes and show that a significant number of VT codeword pairs achieve the worst-case number of outputs needed for exact reconstruction.}, 
keywords={error correction codes;sequential codes;sequence reconstruction;perfectly reconstructing sequences;codeword sequences;deletion-insertion-correcting code;fixed number;symbol insertions;uncoded sequences;distinct traces;exact reconstruction;Varshamov-Tenengolts codes;worst-case number;Information theory;Upper bound;Sequential analysis;Nickel;Indexes;Genomics;Insertions and Deletions;Insertion-Correcting Codes;Reconstruction;Combinatorics}, 
doi={10.1109/ISIT.2016.7541372}, 
ISSN={2157-8117}, 
month={July},}
@INPROCEEDINGS{6957226, 
author={E. Park and C. Kartsaklis and J. Cavazos}, 
booktitle={2014 43rd International Conference on Parallel Processing}, 
title={HERCULES: Strong Patterns towards More Intelligent Predictive Modeling}, 
year={2014}, 
volume={}, 
number={}, 
pages={172-181}, 
abstract={Recent work has shown that program analysis techniques to select meaningful code features of programs are important in the task of deciding the best compiler optimizations. Although, there are many successful state-of-the-art program analysis techniques, they often do not provide a simple method to extract the most expressive information about loops, especially when a target program is computationally intensive with complex loops and data dependencies. In this paper, we introduce a static technique to characterize a program using a pattern-driven system named HERCULES. This characterization technique not only helps a user to understand programs by searching pattern-of-interests, but also can be used for a predictive model that effectively selects the proper compiler optimizations. We formulated 35 loop patterns, then evaluated our characterization technique by comparing the predictive models constructed using HERCULES to three other state-of-the-art characterization methods. We show that our models outperform three state-of-the-art program characterization techniques on two multicore systems in selecting the best optimization combination from a given loop transformation space. We achieved up to 67% of the best possible speedup achievable with the optimization search space we evaluated.}, 
keywords={optimising compilers;program control structures;program diagnostics;search problems;HERCULES;intelligent predictive modeling;program analysis technique;code features;compiler optimization;data dependency;static technique;pattern-driven system;loop patterns;program characterization techniques;multicore systems;optimization combination;optimization search space;Optimization;Predictive models;Feature extraction;Arrays;Data mining;Radiation detectors;Pattern matching;compiler optimization;machine learning;predictive modeling;pattern-based program characterization}, 
doi={10.1109/ICPP.2014.26}, 
ISSN={0190-3918}, 
month={Sep.},}
@INPROCEEDINGS{7365725, 
author={O. Martens and A. Liimets and A. Kuusik}, 
booktitle={2015 Advances in Wireless and Optical Communications (RTUWO)}, 
title={Synchronization algorithm for RFID-reader and its implementation}, 
year={2015}, 
volume={}, 
number={}, 
pages={82-85}, 
abstract={An algorithm for synchronization of a decoder of a RFID receiver has been proposed, implemented and evaluated, by experimental setup. The solution can be used for UHF RFID (e.g. ISO18000-6 and EPC gen 2) readers using FM0, bi-phase or differential Manchester decoding. Goal of the proposed solution is the improved decoding of the received data in the presence of noise (so by using less power or working at longer distance) by innovative synchronization techniques. Synchronization problem (and challenge) is caused by non-precise timing of the binary signals sent by simple passive RFID-tags. Hardware prototype has been developed for testing and evaluating of the RFID-reader synchronization algorithm for decoding of the binary information. The developed algorithm has been implemented as C/C++ code for DSP and can be evaluated, benchmarked and developed further, with real-world experimental data log files acquired with the developed hardware setup (also on PC). The proposed algorithm is based on the finding of the best correlation of the expected binary preamble (header) waveform of the communication packet with corresponding part of the received noisy signal, as used in other known solutions, with variation of the possibly expected timing parameters, e.g by using matched correlation (FIR) filter-banks. The idea of the proposed solution is to use additionally to maximum correlation of the received signal with the reference preamble waveform also the “quadrature” version of the reference (derived from the ideal preamble) signal, as the “quadrature” correlation has at the best match clear zero value and has change around this “zero” point. So using this additional “Q” correlation channel gives the opportunity to find the exact timing base (“bit duration”) more precisely. The proposed algorithm and the results of the evaluation of the solution with real-world data are described. Also, potential future developments of the solution are discussed.}, 
keywords={binary codes;C++ language;decoding;radio receivers;radiofrequency identification;signal processing;synchronisation;telecommunication computing;UHF RFID-reader implementation;RFID receiver decoder synchronization algorithm;differential Manchester decoding;biphase decoding;FM0 decoding;hardware prototype;binary information decoding;C-C++ code;DSP;data log file;received noisy signal;Synchronization;Radiofrequency identification;Correlation;Signal processing algorithms;Decoding;Hardware}, 
doi={10.1109/RTUWO.2015.7365725}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6747229, 
author={S. Xie and F. Khomh and Y. Zou and I. Keivanloo}, 
booktitle={2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)}, 
title={An empirical study on the fault-proneness of clone migration in clone genealogies}, 
year={2014}, 
volume={}, 
number={}, 
pages={94-103}, 
abstract={Copy and paste activities create clone groups in software systems. The evolution of a clone group across the history of a software system is termed as clone genealogy. During the evolution of a clone group, developers may change the location of the code fragments in the clone group. The type of the clone group may also change (e.g., from Type-1 to Type-2). These two phenomena have been referred to as clone migration and clone mutation respectively. Previous studies have found that clone migration occur frequently in software systems, and suggested that clone migration can induce faults in a software system. In this paper, we examine how clone migration phenomena affect the risk for faults in clone segments, clone groups, and clone genealogies from three long-lived software systems JBoss, APACHE-ANT, and ARGOUML. Results show that: (1) migrated clone segments, clone groups, and clone genealogies are not equally fault-prone; (2) when a clone mutation occurs during a clone migration, the risk for faults in the migrated clone is increased; (3) migrating a clone that was not changed for a longer period of time is risky.}, 
keywords={software engineering;fault-proneness;clone migration;clone genealogies;copy-and-paste activities;software systems;clone genealogy;clone mutation;software faults;clone segments;JBoss;APACHE-ANT;ARGOUML;Cloning;Software systems;Context;Java;Fault diagnosis;History;Layout;Type of clones;clone genealogy;clone migration;clone mutation;fault-proneness}, 
doi={10.1109/CSMR-WCRE.2014.6747229}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{7020272, 
author={K. M. M. Rajashekharaiah and M. S. Patil and G. H. Joshi}, 
booktitle={2014 IEEE International Conference on MOOC, Innovation and Technology in Education (MITE)}, 
title={Impact of classification of lab assignments and problem solving approach in object oriented programming lab course: A case study}, 
year={2014}, 
volume={}, 
number={}, 
pages={205-209}, 
abstract={Information systems are growing in both size and complexity. Object orientation has emerged as a dominant paradigm in designing and constructing such large and complex information systems, which is being taught to undergraduate students of Computer Science and Engineering. The use of java technology to develop applications is found preferred one [1]. There exists a knowledge gap between industry expectations and students knowledge. Hence deep learning of java programming is essential attribute to reduce the gap [1]. This paper presents classification of lab assignments as demonstration, Exercises, structured Enquiry type and open ended types with an approach of using conceptual model as intermediate step before writing java code for a given problem statement. This classification of assignments/problem statements move the responsibility from a professor to student where the latter involvement is observed high in achieving the objectives set for each type in classification of assignments, hence helps to meet industry expectations. The conceptual model, drawing a class diagram using standard notations (diagrammatic representations) improves the students understanding of object oriented concepts [7], for a given lab assignment which in-turn eases the student effort in translating the class diagram into program code by following the syntax. The students will be graded during assessment as Grade-S, Grade-A, Grade-B, Grade-C, Grade-D, Grade-E, Grade-F and where Grade-S is highest and Grade-F is lowest. The results of assessment are analyzed for year 2013 and 2014 batches and it is observed that, there is increase in grade A in year 2014 compared to year 2013. The observed change is due to the above said approach. The result analysis of year 2013 and 2012 shows that, the grade F is increased little in results of year 2013, it is due to the current approach where the lab assignments are classified and students are expected to apply the concepts studied effectively and few students unable to follow the approach.}, 
keywords={computer science education;educational courses;educational technology;engineering education;information systems;Java;object-oriented programming;problem solving;lab assignments;problem solving;object oriented programming lab course;information systems;undergraduate students;computer science education;engineering education;Java technology;deep learning;Object oriented modeling;Java;Object oriented programming;Education;Unified modeling language;Programming profession;paradigm;dominant;shrink;classification;conceptual model;class diagrams;assignment}, 
doi={10.1109/MITE.2014.7020272}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6405302, 
author={F. Thung and D. Lo and L. Jiang and Lucia and F. Rahman and P. T. Devanbu}, 
booktitle={2012 28th IEEE International Conference on Software Maintenance (ICSM)}, 
title={When would this bug get reported?}, 
year={2012}, 
volume={}, 
number={}, 
pages={420-429}, 
abstract={Not all bugs in software would be experienced and reported by end users right away: Some bugs manifest themselves quickly and may be reported by users a few days after they get into the code base; others manifest many months or even years later, and may only be experienced and reported by a small number of users. We refer to the period of time between the time when a bug is introduced into code and the time when it is reported by a user as bug reporting latency. Knowledge of bug reporting latencies has an implication on prioritization of bug fixing activities-bugs with low reporting latencies may be fixed earlier than those with high latencies to shift debugging resources towards bugs highly concerning users. To investigate bug reporting latencies, we analyze bugs from three Java software systems: AspectJ, Rhino, and Lucene. We extract bug reporting data from their version control repositories and bug tracking systems, identify bug locations based on bug fixes, and back-trace bug introducing time based on change histories of the buggy code. Also, we remove non-essential changes, and most importantly, recover root causes of bugs from their treatments/fixes. We then calculate the bug reporting latencies, and find that bugs have diverse reporting latencies. Based on the calculated reporting latencies and features we extract from bugs, we build classification models that can predict whether a bug would be reported early (within 30 days) or later, which may be helpful for prioritizing bug fixing activities. Our evaluation on the three software systems shows that our bug reporting latency prediction models could achieve an AUC (Area Under the Receiving Operating Characteristics Curve) of 70.869%.}, 
keywords={aspect-oriented programming;Java;pattern classification;program debugging;software bugs;bug reporting latency prediction models;bug fixing activities;debugging resources;Java software systems;AspectJ;Rhino;Lucene;bug reporting data extraction;version control repositories;bug tracking systems;bug location identification;classification models;software systems;AUC;area under-the-receiving operating characteristics curve;back-trace bug;Computer bugs;Feature extraction;Software systems;Predictive models;Java;Databases;Conferences}, 
doi={10.1109/ICSM.2012.6405302}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{7181430, 
author={R. Minelli and A. Mocci and M. Lanza}, 
booktitle={2015 IEEE 23rd International Conference on Program Comprehension}, 
title={I Know What You Did Last Summer - An Investigation of How Developers Spend Their Time}, 
year={2015}, 
volume={}, 
number={}, 
pages={25-35}, 
abstract={Developing software is a complex mental activity, requiring extensive technical knowledge and abstraction capabilities. The tangible part of development is the use of tools to read, inspect, edit, and manipulate source code, usually through an IDE (integrated development environment). Common claims about software development include that program comprehension takes up half of the time of a developer, or that certain UI (user interface) paradigms of IDEs offer insufficient support to developers. Such claims are often based on anecdotal evidence, throwing up the question of whether they can be corroborated on more solid grounds. We present an in-depth analysis of how developers spend their time, based on a fine-grained IDE interaction dataset consisting of ca. 740 development sessions by 18 developers, amounting to 200 hours of development time and 5 million of IDE events. We propose an inference model of development activities to precisely measure the time spent in editing, navigating and searching for artifacts, interacting with the UI of the IDE, and performing corollary activities, such as inspection and debugging. We report several interesting findings which in part confirm and reinforce some common claims, but also disconfirm other beliefs about software development.}, 
keywords={software engineering;software development;IDE;fine-grained IDE interaction dataset;user interface;Mice;Keyboards;Navigation;Browsers;Inspection;Software;History;interaction data;empirical study;program understanding;user interface}, 
doi={10.1109/ICPC.2015.12}, 
ISSN={1092-8138}, 
month={May},}
@INPROCEEDINGS{6956577, 
author={L. Xing and X. Pan and R. Wang and K. Yuan and X. Wang}, 
booktitle={2014 IEEE Symposium on Security and Privacy}, 
title={Upgrading Your Android, Elevating My Malware: Privilege Escalation through Mobile OS Updating}, 
year={2014}, 
volume={}, 
number={}, 
pages={393-408}, 
abstract={Android is a fast evolving system, with new updates coming out one after another. These updates often completely overhaul a running system, replacing and adding tens of thousands of files across Android's complex architecture, in the presence of critical user data and applications (apps for short). To avoid accidental damages to such data and existing apps, the upgrade process involves complicated program logic, whose security implications, however, are less known. In this paper, we report the first systematic study on the Android updating mechanism, focusing on its Package Management Service (PMS). Our research brought to light a new type of security-critical vulnerabilities, called Pileup flaws, through which a malicious app can strategically declare a set of privileges and attributes on a low-version operating system (OS) and wait until it is upgraded to escalate its privileges on the new system. Specifically, we found that by exploiting the Pileup vulnerabilities, the app can not only acquire a set of newly added system and signature permissions but also determine their settings (e.g., protection levels), and it can further substitute for new system apps, contaminate their data (e.g., cache, cookies of Android default browser) to steal sensitive user information or change security configurations, and prevent installation of critical system services. We systematically analyzed the source code of PMS using a program verification tool and confirmed the presence of those security flaws on all Android official versions and over 3000 customized versions. Our research also identified hundreds of exploit opportunities the adversary can leverage over thousands of devices across different device manufacturers, carriers and countries. To mitigate this threat without endangering user data and apps during an upgrade, we also developed a new detection service, called SecUP, which deploys a scanner on the user's device to capture the malicious apps designed to exploit Pileup vulnerabilities, based upon the vulnerability-related information automatically collected from newly released Android OS images.}, 
keywords={Android (operating system);formal verification;invasive software;mobile computing;malware;mobile OS updating;operating systems;package management service;PMS;security-critical vulnerability;Pileup flaws;low-version operating system;security configuration;user information;program verification tool;malicious apps;Android OS images;Androids;Humanoid robots;Smart phones;Security;Mobile communication;Google;Registers;Android;OS update;Privilege Escalation;Package Manager Service}, 
doi={10.1109/SP.2014.32}, 
ISSN={1081-6011}, 
month={May},}
@INPROCEEDINGS{6648001, 
author={G. Buttazzo and G. Lipari}, 
booktitle={2013 IEEE 18th Conference on Emerging Technologies Factory Automation (ETFA)}, 
title={Ptask: An educational C library for programming real-time systems on Linux}, 
year={2013}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={When learning real-time programming, the novice is faced with many technical difficulties due to low-level C libraries that require considerable programming effort even for implementing a simple periodic task. For example, the POSIX Real-Time standard only provides a low level notion of thread, hence programmers usually build higher level code on top of the POSIX API, every time re-inventing the wheel. In this paper we present a simple C library that simplifies realtime programming in Linux by hiding low-level details of task creation, allocation and synchronization, and provides utilities for more high-level functionalities, like support for mode-change and adaptive systems. The library is released as open-source and it is currently being employed to teach real-time programming in university courses in embedded systems.}, 
keywords={application program interfaces;C language;computer science education;educational courses;educational institutions;embedded systems;Linux;programming;public domain software;synchronisation;educational C library;real-time system programming learning;Linux;Ptask;low-level C libraries;POSIX real-time standard;higher level code;POSIX API;task creation;task allocation;task synchronization;high-level functionalities;adaptive systems;open-source library;university courses;embedded systems;Libraries;Real-time systems;Linux;Processor scheduling;Clocks;Programming profession}, 
doi={10.1109/ETFA.2013.6648001}, 
ISSN={1946-0740}, 
month={Sep.},}
@INPROCEEDINGS{7996544, 
author={Y. Zhang and G. Yang and L. Hu and H. Wen and J. Wu}, 
booktitle={2017 IEEE International Conference on Communications (ICC)}, 
title={Dot-product based preference preserved hashing for fast collaborative filtering}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Recommendation is widely used to deal with information overloading by suggesting items based on historical information of users. One of the most popular recommendation techniques is matrix factorization (MF), in which the preferences of users are estimated by dot products of their real latent factors between users and items. Although MF can achieve high recommendation accuracy, it suffers from efficiency issues when making preferences ranking in real space. Hash retrieval technique can be applied to recommender systems to speed up preferences ranking. Due to the existence of discrete constraints in learning hash codes, it is possible to exploit a two-stage learning procedure according to most existing methods. This two-stage procedure consists of relaxed optimization by discarding discrete constraints and subsequent binary quantization. However, existing methods have not been able to well handle the change of dot product arising from quantization. To this end, we propose a dot-product based preference preserved hashing method, which quantizes both norm and cosine similarity in dot product respectively. We also design an algorithm to optimize the bit length for norm quantization. Based on the evaluation to several datasets, the proposed framework shows consistent superiority to the competing baselines even though only using shorter binary code.}, 
keywords={collaborative filtering;information retrieval;learning (artificial intelligence);matrix decomposition;recommender systems;dot-product based preference preserved hashing method;collaborative filtering;recommendation techniques;matrix factorization;hash retrieval technique;recommender systems;hash codes;binary quantization;norm quantization;Quantization (signal);Binary codes;Predictive models;Mathematical model;Big Data;Recommender systems;Optimization;Recommender system;preference ranking;hash codes;norm quantization}, 
doi={10.1109/ICC.2017.7996544}, 
ISSN={1938-1883}, 
month={May},}
@INPROCEEDINGS{7573860, 
author={F. Zhang and P. Di and H. Zhou and X. Liao and J. Xue}, 
booktitle={2016 45th International Conference on Parallel Processing (ICPP)}, 
title={RegTT: Accelerating Tree Traversals on GPUs by Exploiting Regularities}, 
year={2016}, 
volume={}, 
number={}, 
pages={562-571}, 
abstract={Tree traversals are widely used irregular applications. Given a tree traversal algorithm, where a single tree is traversed by multiple queries (with truncation), its efficient parallelization on GPUs is hindered by branch divergence, load imbalance and memory-access irregularity, as the nodes and their visitation orders differ greatly under different queries. We leverage a key insight made on several truncation-induced tree traversal regularities to enable as many threads in the same warp as possible to visit the same node simultaneously, thereby enhancing both GPU resource utilization and memory coalescing at the same time. We introduce a new parallelization approach, RegTT, to orchestrate an efficient execution of a tree traversal algorithm on GPUs by starting with BFT (Breadth-First Traversal), then reordering the queries being processed (based on their truncation histories), and finally, switching to DFT (Depth-First Traversal). RegTT is general (without relying on domain-specific knowledge) and automatic (as a source-code transformation). For a set of five representative benchmarks used, RegTT outperforms the state-of-the-art by 1.66x on average.}, 
keywords={graphics processing units;parallel processing;query processing;storage management;tree data structures;REGTT;tree traversals;parallelization;memory-access irregularity;load imbalance;branch divergence;GPU resource utilization;memory coalescing;BFT;breadth-first traversal;query processing;DFT;depth-first traversal;Instruction sets;Graphics processing units;Resource management;Discrete Fourier transforms;History;Algorithm design and analysis;Switches;automatic parallelization;GPU computing;irregular application}, 
doi={10.1109/ICPP.2016.71}, 
ISSN={2332-5690}, 
month={Aug},}
@INPROCEEDINGS{8595234, 
author={M. Lamothe and W. Shang}, 
booktitle={2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)}, 
title={Exploring the Use of Automated API Migrating Techniques in Practice: An Experience Report on Android}, 
year={2018}, 
volume={}, 
number={}, 
pages={503-514}, 
abstract={In recent years, open source software libraries have allowed developers to build robust applications by consuming freely available application program interfaces (API). However, when these APIs evolve, consumers are left with the difficult task of migration. Studies on API migration often assume that software documentation lacks explicit information for migration guidance and is impractical for API consumers. Past research has shown that it is possible to present migration suggestions based on historical code-change information. On the other hand, research approaches with optimistic views of documentation have also observed positive results. Yet, the assumptions made by prior approaches have not been evaluated on large scale practical systems, leading to a need to affirm their validity. This paper reports our recent practical experience migrating the use of Android APIs in FDroid apps when leveraging approaches based on documentation and historical code changes. Our experiences suggest that migration through historical code-changes presents various challenges and that API documentation is undervalued. In particular, the majority of migrations from removed or deprecated Android APIs to newly added APIs can be suggested by a simple keyword search in the documentation. More importantly, during our practice, we experienced that the challenges of API migration lie beyond migration suggestions, in aspects such as coping with parameter type changes in new API. Future research may aim to design automated approaches to address the challenges that are documented in this experience report.}, 
keywords={Documentation;Software;Tools;Data mining;Task analysis;Keyword search;History;Android API;API migration;Mining Software Repositories;Software evolution}, 
doi={}, 
ISSN={2574-3864}, 
month={May},}
@INPROCEEDINGS{8301720, 
author={L. Jones and D. Christman and S. Banescu and M. Carlisle}, 
booktitle={2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)}, 
title={ByteWise: A case study in neural network obfuscation identification}, 
year={2018}, 
volume={}, 
number={}, 
pages={155-164}, 
abstract={Researchers taking advantage of recent advancements in neural networks have made leaps in many fields such as image recognition, natural language processing, and speech recognition. However, little work has been done with neural networks in the field of binary analysis. Recently, researchers have used neural networks to recognize function boundaries in binaries, using only the bytes of the programs as features. In this paper, we extend their work to detect the bytes of bogus basic blocks added in the dead branches of opaque predicates. We perform a case study using the bogus control flow transformation offered by Obfuscator-LLVM. We detect the bytes of bogus basic blocks with a 94% F1 score. This information can be used to prune code for static reverse engineering. We believe this line of research will yield optimized triage, reverse engineering tools, and malware detection based on obfuscation identification using neural networks.}, 
keywords={invasive software;neural nets;program diagnostics;reverse engineering;neural network obfuscation identification;bogus basic blocks;ByteWise;image recognition;natural language processing;speech recognition;binary analysis;function boundaries;opaque predicates;Obfuscator-LLVM;F1 score;static reverse engineering;malware detection;Malware;Biological neural networks;Reverse engineering;Measurement;Computer crime;Semantics;obfuscation identification;neural networks;software obfuscation}, 
doi={10.1109/CCWC.2018.8301720}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{6976074, 
author={H. W. Alomari and M. L. Collard and J. I. Maletic}, 
booktitle={2014 IEEE International Conference on Software Maintenance and Evolution}, 
title={A Slice-Based Estimation Approach for Maintenance Effort}, 
year={2014}, 
volume={}, 
number={}, 
pages={81-90}, 
abstract={Program slicing is used as a basis for an approach to estimate maintenance effort. A case study of the GNU Linux kernel with over 900 versions spanning 17 years of history is presented. For each version a system dictionary is built using a lightweight slicing approach and encodes the forward decomposition static slice profiles for all variables in all the files in the system. Changes to the system are then modeled at the behavioral level using the difference between the system dictionaries of two versions. The three different granularities of slice (i.e., line, function, and file) are analyzed. We use a direct extension of srcML to represent computed change information. The retrieved information reflects the fact that additional knowledge of the differences can be automatically derived to help maintainers understand code changes. We consider the hypotheses: (1) The structured format helps create traceability links between the changes and other software artifacts. (2) This model is predictive of maintenance effort. The results demonstrate that the approach accurately predicts effort in a scalable manner.}, 
keywords={information retrieval;Linux;operating system kernels;program slicing;software maintenance;maintenance effort estimation;GNU Linux kernel;lightweight slicing approach;forward decomposition static slice profiles;system dictionaries;slice granularities;srcML;computed change information;information retrieval;structured format;traceability links;software artifacts;Maintenance engineering;Dictionaries;Estimation;Open source software;Linux;Encoding;effort estimation;program slicing;software metrics;software maintenance}, 
doi={10.1109/ICSME.2014.30}, 
ISSN={1063-6773}, 
month={Sep.},}
@INPROCEEDINGS{6387239, 
author={J. C. A. Guadarrama and J. R. M. Romero and M. Romero and J. H. Camacho}, 
booktitle={2012 11th Mexican International Conference on Artificial Intelligence}, 
title={Implementing a Knowledge Bases Debugger}, 
year={2012}, 
volume={}, 
number={}, 
pages={9-14}, 
abstract={Knowledge representation is an important topic in common-sense reasoning and Artificial Intelligence, and one of the earliest techniques to represent it is by means of knowledge bases encoded into logic clauses. Encoding knowledge, however, is prone to typos and other kinds of consistency mistakes, which may yield incorrect results or even internal contradictions with conflicting information from other parts of the same code. In order to overcome such situations, we propose a logic-programming system to debug knowledge bases. The system has a strong theoretical framework on knowledge representation and reasoning, and a suggested on-line prototype where one can test logic programs. Such logic programs may have, of course, conflicting information and the system shall prompt the user where the possible source of conflict is. Besides, the system can be employed to identify conflicts of the knowledge base itself and upcoming new information, it can also be used to locate the source of conflict from a given inherently inconsistent static knowledge base. This paper describes an implementation of a declarative version of the system that has been characterised to debug knowledge bases in a semantical formalism. Some of the key components of such implementation are existing solvers, so this paper focuses on how to use them and why they work, towards an implemented a fully-fledged system.}, 
keywords={common-sense reasoning;knowledge based systems;knowledge representation;logic programming;program debugging;programming language semantics;knowledge bases debugger;knowledge representation;common-sense reasoning;artificial intelligence;logic clause;knowledge encoding;logic programming system;semantics;inconsistent static knowledge base;Knowledge based systems;Semantics;Debugging;Proposals;Cognition;Logic programming;Answer Set Programming;Knowledge Representation;program transformation;implementation;non-monotonic reasoning;belief revision}, 
doi={10.1109/MICAI.2012.20}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8530133, 
author={J. Grabner and R. Decker and T. Artner and M. Bernhart and T. Grechenig}, 
booktitle={2018 IEEE Working Conference on Software Visualization (VISSOFT)}, 
title={Combining and Visualizing Time-Oriented Data from the Software Engineering Toolset}, 
year={2018}, 
volume={}, 
number={}, 
pages={76-86}, 
abstract={The simultaneous use of more than two different data sources from the software engineering toolset is still uncommon in the research areas of software evolution and visualization. In our work, we address this research gap by the design and evaluation of three interactive visualizations which combine the data from the version control (VCS), the issue tracking (ITS), and the continuous integration (CI) system. After analyzing the information needs of a project team and describing the available data, we selected change impact, code ownership, and activity peaks as our visualization topics. Then, we adapted suitable visualization techniques from the literature to meet our design requirements. After implementation, we evaluated our visualizations by conducting a usability test with ten senior software engineers. On the system usability scale (SUS), our visualizations achieved the rating "good" from the participants. A scenario success rate of 88% and the qualitative user feedback has provided evidence for the benefits of visualizing combined data from the VCS, ITS, and CI system.}, 
keywords={configuration management;data visualisation;formal specification;software engineering;interactive visualizations;continuous integration system;design requirements;system usability scale;time-oriented data;software engineering toolset;software evolution;software visualization;version control system;VCS;issue tracking system;ITS;CI system;usability test;Data visualization;Software;Software engineering;Data mining;Ontologies;Pipelines;Biological system modeling;Data visualization;Data mining;Software engineering;Mining software repositories}, 
doi={10.1109/VISSOFT.2018.00016}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{6176624, 
author={I. Oz and H. R. Topcuoglu and M. Kandemir and O. Tosun}, 
booktitle={2012 Design, Automation Test in Europe Conference Exhibition (DATE)}, 
title={Performance-reliability tradeoff analysis for multithreaded applications}, 
year={2012}, 
volume={}, 
number={}, 
pages={893-898}, 
abstract={Modern architectures become more susceptible to transient errors with the scale down of circuits. This makes reliability an increasingly critical concern in computer systems. In general, there is a tradeoff between system reliability and performance of multithreaded applications running on multicore architectures. In this paper, we conduct a performance-reliability analysis for different parallel versions of three data-intensive applications including FFT, Jacobi Kernel, and Water Simulation. We measure the performance of these programs by counting execution clock cycles, while the system reliability is measured by Thread Vulnerability Factor (TVF) which is a recently-proposed metric. TVF measures the vulnerability of a thread to hardware faults at a high level. We carry out experiments by executing parallel implementations on multicore architectures and collect data about the performance and vulnerability. Our experimental evaluation indicates that the choice is clear for FFT application and Jacobi Kernel. Transpose algorithm for FFT application results in less than 5% performance loss while the vulnerability increases by 20% compared to binary-exchange algorithm. Unrolled Jacobi code reduces execution time up to 50% with no significant change on vulnerability values. However, the tradeoff is more interesting for Water Simulation where nsquared version reduces the vulnerability values significantly by worsening the performance with similar rates compared to faster but more vulnerable spatial version.}, 
keywords={fast Fourier transforms;integrated circuit reliability;Jacobian matrices;microprocessor chips;performance-reliability tradeoff analysis;multithreaded applications;transient errors;computer systems;multicore architectures;Jacobi Kernel;water simulation;execution clock cycles;thread vulnerability factor;TVF;FFT application;unrolled Jacobi code;chip multiprocessors;Instruction sets;Reliability;Jacobian matrices;Measurement;Registers;Hardware;Multicore processing;Multi-Core Architectures and Support;Reliable Parallel;Distributed Algorithms}, 
doi={10.1109/DATE.2012.6176624}, 
ISSN={1558-1101}, 
month={March},}
@INPROCEEDINGS{7332447, 
author={D. Piorkowski and S. D. Fleming and C. Scaffidi and M. Burnett and I. Kwan and A. Z. Henley and J. Macbeth and C. Hill and A. Horvath}, 
booktitle={2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
title={To fix or to learn? How production bias affects developers' information foraging during debugging}, 
year={2015}, 
volume={}, 
number={}, 
pages={11-20}, 
abstract={Developers performing maintenance activities must balance their efforts to learn the code vs. their efforts to actually change it. This balancing act is consistent with the “production bias” that, according to Carroll's minimalist learning theory, generally affects software users during everyday tasks. This suggests that developers' focus on efficiency should have marked effects on how they forage for the information they think they need to fix bugs. To investigate how developers balance fixing versus learning during debugging, we conducted the first empirical investigation of the interplay between production bias and information foraging. Our theory-based study involved 11 participants: half tasked with fixing a bug, and half tasked with learning enough to help someone else fix it. Despite the subtlety of difference between their tasks, participants foraged remarkably differently-making foraging decisions from different types of “patches,” with different types of information, and succeeding with different foraging tactics.}, 
keywords={program debugging;software maintenance;production bias;developer information foraging;debugging;maintenance activities;Carroll minimalist learning theory;bug fixing;Production;Encoding;Debugging;Maintenance engineering;Software;Navigation;Interviews;Debugging;information foraging;theory meets tools}, 
doi={10.1109/ICSM.2015.7332447}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{8376846, 
author={M. Z. Ejaz and K. Khurshid and Z. Abbas and M. A. Aizaz and A. Nawaz}, 
booktitle={2018 Advances in Science and Engineering Technology International Conferences (ASET)}, 
title={A novel image encoding and communication technique of B/W images for IOT, robotics and drones using (15, 11) reed solomon scheme}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In the modern age of IOT and Robotics, different intelligent entities like robots, drones, IOT nodes or smart vehicles need fast and error-free communication of data, which is predominantly in the form of images. The medium used for communication of this data is mainly wireless and it can be short distance, medium distance, long haul or even satellite links crossing the ionosphere layers. Different wireless mediums incorporate different types of noises in the images being transmitted by Drones, Robots or IOT Nodes. For better analysis and then performing subsequent action on the basis of these received images using artificial intelligence, machine learning or machine vision, it is imperative that the images transmitted are encoded and recovered as fast and as error-free as possible. Normal conventional methods use different image correction algorithms for detection and correction of errors in images. Reed Solomon codes, which are normally used for error detection and correction at data link layer in TCP/IP protocol stack, have a high probability of signal correction and are highly efficient due to their burst error detection and correction capabilities. The RS codes can be implemented where there is a large number of input symbols and noise duration is relatively small as compared to the code word. Sometimes at the receiver end, we get images which are partially corrupted and only half or some part of them is visible. Most of the filters used for image reconstruction insert the approximated bits in place of the corrupted bits by using some algorithms but if only partial part of the image is corrupted, no filter will be able to recover the images properly as it will also change the bits in the non-corrupted part of the image. We have proposed a novel approach of using RS codes for the detection and correction of errors in the images. This novel technique can be used over a variety of applications including robotics, drones, IOT nodes, smart vehicles using wireless and satellite communication, which include the transfer of images and decision making on the basis of the content of the images.}, 
keywords={error correction codes;error detection;image coding;Internet of Things;mobile robots;Reed-Solomon codes;robot vision;image encoding;drones;Reed-Solomon codes;communication technique;blaack and white images;IoT;Internet of Things;robot vision;error detection;error correction;Robots;Decoding;Encoding;Drones;Reed-Solomon codes;Wireless communication;Mathematical model;Reed Solomon;Image reconstruction;Encoding;Decoding;Internet of Things;Drones;Robotics;Smart Vehicles;Wireless Communication;Satellite Communications;Artificial Intelligence;Machine Learning;Machine Vision}, 
doi={10.1109/ICASET.2018.8376846}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{6982626, 
author={R. Gopinath and C. Jensen and A. Groce}, 
booktitle={2014 IEEE 25th International Symposium on Software Reliability Engineering}, 
title={Mutations: How Close are they to Real Faults?}, 
year={2014}, 
volume={}, 
number={}, 
pages={189-200}, 
abstract={Mutation analysis is often used to compare the effectiveness of different test suites or testing techniques. One of the main assumptions underlying this technique is the Competent Programmer Hypothesis, which proposes that programs are very close to a correct version, or that the difference between current and correct code for each fault is very small. Researchers have assumed on the basis of the Competent Programmer Hypothesis that the faults produced by mutation analysis are similar to real faults. While there exists some evidence that supports this assumption, these studies are based on analysis of a limited and potentially non-representative set of programs and are hence not conclusive. In this paper, we separately investigate the characteristics of bug-fixes and other changes in a very large set of randomly selected projects using four different programming languages. Our analysis suggests that a typical fault involves about three to four tokens, and is seldom equivalent to any traditional mutation operator. We also find the most frequently occurring syntactical patterns, and identify the factors that affect the real bug-fix change distribution. Our analysis suggests that different languages have different distributions, which in turn suggests that operators optimal in one language may not be optimal for others. Moreover, our results suggest that mutation analysis stands in need of better empirical support of the connection between mutant detection and detection of actual program faults in a larger body of real programs.}, 
keywords={program debugging;program testing;programming languages;software fault tolerance;mutation analysis;test suites;testing techniques;competent programmer hypothesis;bug-fix characteristics;programming languages;mutation operator;syntactical patterns;bug-fix change distribution;mutant detection;program fault detection;Java;Software;Computer bugs;Accuracy;Sociology;Statistics;mutation analysis;software testing}, 
doi={10.1109/ISSRE.2014.40}, 
ISSN={1071-9458}, 
month={Nov},}
@INPROCEEDINGS{7437736, 
author={F. Handani and S. Rochimah}, 
booktitle={2015 International Conference on Information Technology Systems and Innovation (ICITSI)}, 
title={Relationship between features volatility and software architecture design stability in object-oriented software: Preliminary analysis}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Software architecture is the core structure of a system. Software architecture describes the functionality and the size of system to be built. Software architecture is illustrated as packages diagram, class diagram or Enterprise Architecture diagram. To make a robust software, it's important to know quality of architecture. Architecture Quality is reflected in its design. There are various topics of research on the quality aspect of the architectural design, from enviroment adaption of architectural design to design stability maintenance. The concept of reuse elements of the system is one of the topics to maintain the stability of the software design. Aversano and Constantinou introduce the method of measuring the stability of the architectural design by taking into account external and internal elements of architecture built. Both just look at the number of packets that undergo additions and deletions to the pair versions. Quantitative research to assess an architectural stability by looking at environmental factors needed to complete measurement. Before implementing this factor, it is necessary to measure the relationship between variables the stability and environmental factors. We introduced a quantitative analysis of the mechanisms related to the extent to which the relationship between features volatility and architecture stability. Architecture design stability is measured by metrics Constantinou, and the calculation of features volatility depend on change of features from consecutive version. We applied this analysis into one project. The source code in the repository extracted to be converted into data according to metrics Constantinou, then the results are validated by experts selected. Datasets that have been validated measured by metrics and measurable correlation with Pearson-Product-Moment analysis.}, 
keywords={object-oriented methods;software architecture;Pearson-product-moment analysis;quantitative analysis;environmental factors;architecture quality;enterprise architecture diagram;class diagram;packages diagram;object-oriented software;software architecture design stability maintenance;volatility;Stability analysis;Measurement;Computer architecture;Software;Feature extraction;Software architecture;Statistical analysis;correlation analysis;software architecture stability;features volatility;Pearson-Product Moment}, 
doi={10.1109/ICITSI.2015.7437736}, 
ISSN={}, 
month={Nov},}
@ARTICLE{8565895, 
author={T. d. F. Pereira and A. Anjos and S. Marcel}, 
journal={IEEE Transactions on Information Forensics and Security}, 
title={Heterogeneous Face Recognition Using Domain Specific Units}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={The task of Heterogeneous Face Recognition consists in matching face images that are sensed in different domains, such as sketches to photographs (visual spectra images), thermal images to photographs or near-infrared images to photographs. In this work we suggest that high level features of Deep Convolutional Neural Networks trained on visual spectra images are potentially domain independent and can be used to encode faces sensed in different image domains. A generic framework for Heterogeneous Face Recognition is proposed by adapting Deep Convolutional Neural Networks low level features in, so called, “Domain Specific Units”. The adaptation using Domain Specific Units allow the learning of shallow feature detectors specific for each new image domain. Furthermore, it handles its transformation to a generic face space shared between all image domains. Experiments carried out with four different face databases covering three different image domains show substantial improvements, in terms of recognition rate, surpassing the state-of-the-art for most of them. This work is made reproducible: all the source code, scores and trained models of this approach are made publicly available.}, 
keywords={Face;Databases;Face recognition;Feature extraction;Protocols;Visualization;Image recognition;Face Recognition;Heterogeneous Face Recognition;Reproducible Research;Domain Adaptation;Deep Neural Networks}, 
doi={10.1109/TIFS.2018.2885284}, 
ISSN={1556-6013}, 
month={},}
@ARTICLE{7895279, 
author={Z. Chi and J. Xuan and Z. Ren and X. Xie and H. Guo}, 
journal={IEEE Computational Intelligence Magazine}, 
title={Multi-Level Random Walk for Software Test Suite Reduction}, 
year={2017}, 
volume={12}, 
number={2}, 
pages={24-33}, 
abstract={Software testing is important and time-consuming. A test suite, i.e., a set of test cases, plays a key role in validating the expected program behavior. In modern test-driven development, a test suite pushes the development progress. Software evolves over time; its test suite is executed to detect whether a new code change adds bugs to the existing code. Executing all test cases after each code change is unnecessary and may be impossible due to the limited development cycle. On the one hand, multiple test cases may focus on an identical piece of code; then several test cases cannot detect extra bugs. On the other hand, even executing a test suite once in a large project takes around one hour [1]; frequent code changes require much time for conducting testing. For instance, in Hadoop, a framework of distributed computing, 2,847 version commits are accepted within one year from September 2014 with a peak of 135 commits in one week [2].}, 
keywords={computational complexity;integer programming;linear programming;program testing;public domain software;random processes;multilevel random walk;software test suite reduction;software development cycle;NP-hard problem;software engineering;test execution;test requirements;search based software engineering;computational intelligence methods;multilevel optimization algorithm;local optima;large-scale open source projects;integer linear programming;ILP;Software testing;Algorithm design and analysis;Software engineering;Search methods;Heuristic algorithms;Computer bugs;Computational intelligence;Software product lines;Software algorithms}, 
doi={10.1109/MCI.2017.2670460}, 
ISSN={1556-603X}, 
month={May},}
@INPROCEEDINGS{7529891, 
author={C. Rasmussen and M. Sottile and S. Rasmussen and D. Nagle and W. Dumas}, 
booktitle={2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
title={CAFe: Coarray Fortran Extensions for Heterogeneous Computing}, 
year={2016}, 
volume={}, 
number={}, 
pages={357-365}, 
abstract={Emerging hybrid accelerator architectures are often proposed for inclusion as components in an exascale machine, not only for performance reasons but also to reduce total power consumption. Unfortunately, programmers of these architectures face a daunting and steep learning curve that frequently requires learning a new language (e.g., OpenCL) or adopting a new programming model. Furthermore, the distributed (and frequently multi-level) nature of the memory organization of clusters of these machines provides an additional level of complexity. This paper presents preliminary work examining how Fortran coarray syntax can be extended to provide simpler access to accelerator architectures. This programming model integrates the Partitioned Global Address Space (PGAS) features of Fortran with some of the more task-oriented constructs in OpenMP 4.0 and OpenACC. It also includes the potential for compiler-based transformations targeting the Open Community Runtime (OCR) environment. We demonstrate these CoArray Fortran extensions (CAFe) by implementing a multigrid Laplacian solver and transforming this high-level code to a mixture of standard coarray Fortran and OpenCL kernels.}, 
keywords={FORTRAN;parallel programming;CAFe;coarray Fortran extension;heterogeneous computing;hybrid accelerator architecture;exascale machine;partitioned global address space;PGAS feature;OpenMP 4.0;OpenACC;compiler-based transformation;open community runtime;multigrid Laplacian solver;OpenCL kernel;Hardware;Resource management;Programming;Syntactics;Computer architecture;Parallel processing;Semantics;distributed memory parallelism;domain specific language}, 
doi={10.1109/IPDPSW.2016.140}, 
ISSN={}, 
month={May},}
@INPROCEEDINGS{7017960, 
author={J. A. Ang and R. F. Barrett and R. E. Benner and D. Burke and C. Chan and J. Cook and D. Donofrio and S. D. Hammond and K. S. Hemmert and S. M. Kelly and H. Le and V. J. Leung and D. R. Resnick and A. F. Rodrigues and J. Shalf and D. Stark and D. Unat and N. J. Wright}, 
booktitle={2014 Hardware-Software Co-Design for High Performance Computing}, 
title={Abstract Machine Models and Proxy Architectures for Exascale Computing}, 
year={2014}, 
volume={}, 
number={}, 
pages={25-32}, 
abstract={To achieve exascale computing, fundamental hardware architectures must change. This will significantly impact scientific applications that run on current high performance computing (HPC) systems, many of which codify years of scientific domain knowledge and refinements for contemporary computer systems. To adapt to exascale architectures, developers must be able to reason about new hardware and determine what programming models and algorithms will provide the best blend of performance and energy efficiency in the future. An abstract machine model is designed to expose to the application developers and system software only the aspects of the machine that are important or relevant to performance and code structure. These models are intended as communication aids between application developers and hardware architects during the co-design process. A proxy architecture is a parameterized version of an abstract machine model, with parameters added to elucidate potential speeds and capacities of key hardware components. These more detailed architectural models enable discussion among the developers of analytic models and simulators and computer hardware architects and they allow for application performance analysis, system software development, and hardware optimization opportunities. In this paper, we present a set of abstract machine models and show how they might be used to help software developers prepare for exascale. We then apply parameters to one of these models to demonstrate how a proxy architecture can enable a more concrete exploration of how well application codes map onto future architectures.}, 
keywords={optimisation;parallel processing;power aware computing;proxy architectures;exascale computing;fundamental hardware architectures;scientific applications;high performance computing systems;HPC;contemporary computer systems;programming models;energy efficiency;abstract machine model;system software;code structure;hardware architects;codesign process;computer hardware architects;application performance analysis;system software development;hardware optimization opportunities;proxy architecture;application codes;Multicore processing;Computational modeling;Hardware;Instruction sets;System-on-chip;Abstracts}, 
doi={10.1109/Co-HPC.2014.4}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8260159, 
author={A. G. Paspatis and G. C. Konstantopoulos and M. Mayfield and V. C. Nikolaidis}, 
booktitle={2017 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe)}, 
title={Current-limiting droop controller with fault-ride-through capability for grid-tied inverters}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In this paper, the recently proposed current-limiting droop (CLD) controller for grid-connected inverters is enhanced in order to comply with the Fault-Ride-Through (FRT) requirements set by the Grid Code under grid voltage sags. The proposed version of the CLD extends the operation of the original CLD by fully utilizing the power capacity of the inverter under grid faults. It is analytically proven that during a grid fault, the inverter current increases but never violates a given maximum value. Based on this property, an FRT algorithm is proposed and embedded into the proposed control design to support the voltage of the grid. In contrast to the existing FRT algorithms that change the desired values of both the real and reactive power, the proposed method maximizes only the reactive power to support the grid voltage and the real power automatically drops due to the inherent current-limiting property. Extensive simulations are presented to compare the proposed control approach with the original CLD under a faulty grid.}, 
keywords={invertors;power generation faults;power grids;power supply quality;reactive power;voltage control;grid voltage sags;grid fault;FRT algorithm;reactive power;faulty grid;grid code;current-limiting droop controller;fault-ride-through capability;grid-tied inverters;grid-connected inverters;FRT requirements;inverter power capacity;inherent current-limiting property;Inverters;Circuit faults;Reactive power;Voltage control;Power quality;Guidelines;Control design;Inverter;droop control;current-limiting property;fault-ride-through;voltage sags}, 
doi={10.1109/ISGTEurope.2017.8260159}, 
ISSN={}, 
month={Sep.},}
@ARTICLE{7362043, 
author={A. Painsky and S. Rosset and M. Feder}, 
journal={IEEE Transactions on Information Theory}, 
title={Generalized Independent Component Analysis Over Finite Alphabets}, 
year={2016}, 
volume={62}, 
number={2}, 
pages={1038-1053}, 
abstract={Independent component analysis (ICA) is a statistical method for transforming an observable multi-dimensional random vector into components that are as statistically independent as possible from each other. Usually, the ICA framework assumes a model according to which the observations are generated (such as a linear transformation with additive noise). ICA over finite fields is a special case of ICA in which both the observations and the independent components are over a finite alphabet. In this paper, we consider a generalization of this framework in which an observation vector is decomposed to its independent components (as much as possible) with no prior assumption on the way it was generated. This generalization is also known as Barlow's minimal redundancy representation problem and is considered an open problem. We propose several theorems and show that this hard problem can be accurately solved with a branch and bound search tree algorithm, or tightly approximated with a series of linear problems. Our contribution provides the first efficient set of solutions to Barlow's problem. The minimal redundancy representation (also known as factorial code) has many applications, mainly in the fields of neural networks and deep learning. The binary ICA is also shown to have applications in several domains, including medical diagnosis, multi-cluster assignment, network tomography, and internet resource management. In this paper, we show that this formulation further applies to multiple disciplines in source coding, such as predictive coding, distributed source coding, and coding of large alphabet sources.}, 
keywords={independent component analysis;search problems;source coding;tree searching;trees (mathematics);generalized independent component analysis;finite alphabets;ICA;multidimensional random vector;Barlow's minimal redundancy representation problem;branch and bound search tree algorithm;neural networks;factorial code;medical diagnosis;multicluster assignment;network tomography;internet resource management;source coding;predictive coding;distributed source coding;large alphabet sources;Redundancy;Entropy;Source coding;Complexity theory;Independent component analysis;Independent Component Analysis;BICA;ICA over Galois Field;Blind Source Separation;Minimal Redundancy Representation;Minimum Entropy Codes;factorial Codes;Predictive Coding;Distributed Source Coding;Neural Networks;Independent component analysis;BICA;ICA over galois field;blind source separation;minimal redundancy representation;minimum entropy codes;factorial codes;predictive coding;distributed source coding;neural networks}, 
doi={10.1109/TIT.2015.2510657}, 
ISSN={0018-9448}, 
month={Feb},}
@INPROCEEDINGS{8578295, 
author={H. Wu and S. Zheng and J. Zhang and K. Huang}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={Fast End-to-End Trainable Guided Filter}, 
year={2018}, 
volume={}, 
number={}, 
pages={1838-1847}, 
abstract={Image processing and pixel-wise dense prediction have been advanced by harnessing the capabilities of deep learning. One central issue of deep learning is the limited capacity to handle joint upsampling. We present a deep learning building block for joint upsampling, namely guided filtering layer. This layer aims at efficiently generating the high-resolution output given the corresponding low-resolution one and a high-resolution guidance map. The proposed layer is composed of a guided filter, which is reformulated as a fully differentiable block. To this end, we show that a guided filter can be expressed as a group of spatial varying linear transformation matrices. This layer could be integrated with the convolutional neural networks (CNNs) and jointly optimized through end-to-end training. To further take advantage of end-to-end training, we plug in a trainable transformation function that generates task-specific guidance maps. By integrating the CNNs and the proposed layer, we form deep guided filtering networks. The proposed networks are evaluated on five advanced image processing tasks. Experiments on MIT-Adobe FiveK Dataset demonstrate that the proposed approach runs 10-100Ã— faster and achieves the state-of-the-art performance. We also show that the proposed guided filtering layer helps to improve the performance of multiple pixel-wise dense prediction tasks. The code is available at https://github.com/wuhuikai/DeepGuidedFilter.}, 
keywords={Task analysis;Training;Computer vision;Image resolution;Filtering}, 
doi={10.1109/CVPR.2018.00197}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{6676939, 
author={R. K. Saha and C. K. Roy and K. A. Schneider}, 
booktitle={2013 IEEE International Conference on Software Maintenance}, 
title={gCad: A Near-Miss Clone Genealogy Extractor to Support Clone Evolution Analysis}, 
year={2013}, 
volume={}, 
number={}, 
pages={488-491}, 
abstract={Understanding the evolution of code clones is important for both developers and researchers to understand the maintenance implications of clones and to design robust clone management systems. Generally, a study of clone evolution starts with extracting clone genealogies across multiple versions of a program and classifying them according to their change patterns. Although these tasks are straightforward for exact clones, extracting the history of near-miss clones and classifying their change patterns automatically is challenging due to the potential diverse variety of clone fragments even in the same clone class. In this tool demonstration paper we describe the design and implementation of a near-miss clone genealogy extractor, gCad, that can extract and classify both exact and near-miss clone genealogies. Developers and researchers can compute a wide range of popular metrics regarding clone evolution by simply post processing the gCad results. gCad scales well to large subject systems, works for different granularities of clones, and adapts easily to popular clone detection tools.}, 
keywords={pattern classification;program diagnostics;software maintenance;software metrics;software tools;clone genealogy classification;metrics;clone detection tools;clone evolution;exact clone genealogy extractor;clone evolution analysis;near-miss clone genealogy extractor;gCad;Cloning;XML;Measurement;Maintenance engineering;Detectors;Educational institutions;Type-3 clones;clone genealogy;clone evolution}, 
doi={10.1109/ICSM.2013.79}, 
ISSN={1063-6773}, 
month={Sep.},}
@ARTICLE{8502855, 
author={Y. Chen and S. Wang and S. Lu and K. Sankaralingam}, 
journal={IEEE Transactions on Parallel and Distributed Systems}, 
title={Applying Transactional Memory for Concurrency-Bug Failure Recovery in Production Runs}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={Concurrency bugs widely exist and severely threaten system availability. Techniques that help recover from concurrency-bug failures during production runs are highly desired. This paper proposes BugTM, an approach that applies transactional memory techniques for concurrency-bug recovery in production runs. Requiring no knowledge about where are concurrency bugs, BugTM uses static analysis and code transformation to enable BugTM-transformed software to recover from a concurrency-bug failure by rolling back and re-executing the recent history of a failure thread. BugTM is instantiated as three schemes that have different trade-offs in performance and recovery capability: BugTMH uses existing hardware transactional memory (HTM) support, BugTMS leverages software transactional memory techniques, and BugTMHS is a software-hardware hybrid design. BugTM greatly improves the recovery capability of state-of-the-art techniques with low run-time overhead and no changes to OS or hardware, while guarantees not to introduce new bugs.}, 
keywords={Computer bugs;Software;Concurrent computing;Message systems;Production;Hardware;Checkpointing;Concurrency Bugs;Transactional Memory;Failure Recovery;Software Availability}, 
doi={10.1109/TPDS.2018.2877656}, 
ISSN={1045-9219}, 
month={},}
@ARTICLE{8207321, 
author={H. Jia-Jia and W. Jing-Jing and Q. Sheng-Bang}, 
journal={Publications of the Astronomical Society of Japan}, 
title={V524 Monocerotis: A Marginal Contact Binary with a Cyclic Period Variation}, 
year={2012}, 
volume={64}, 
number={4}, 
pages={85-85}, 
abstract={We have secured and analyzed three-color light curves of V524 Monocerotis with the 2003 version of the Wilson–Devinney code. We confirmed that V524 Mon is a shallow W-type contact binary system with a mass ratio of$q$$=$2.099 and a degree of contact factor of$f$$=$7.7%. Based on the new eight times of the light minima and those published by previous investigators, we find that the orbital period of the binary shows a long-term decrease ($dp/dt$$=$$-$1.52$\times$10$^{-10}$), while it undergoes a cyclic oscillation ($T_{3}$$=$23.93 yr,$A_{3}$$=$0.0082 d). The long-term period decrease can be explained by mass transfer from the primary to the secondary. The cyclic change, explained as the light-travel time effect, reveals the presence of a tertiary companion. The marginal contact configuration and the continuous period decrease both suggest that the system may be a newly formed contact binary.}, 
keywords={binaries: close;binaries: eclipsing;stars: evolution;stars: individual (V524 Monocerotis)}, 
doi={10.1093/pasj/64.4.85}, 
ISSN={0004-6264}, 
month={Aug},}
@INPROCEEDINGS{6482087, 
author={S. A. M. A. Junid and N. M. Tahir and Z. A. Majid and A. K. Halim and K. K. M. Shariff}, 
booktitle={2012 International Symposium on Computer Applications and Industrial Electronics (ISCAIE)}, 
title={Improved data minimization technique in reducing memory space complexity for DNA local alignment accelerator application}, 
year={2012}, 
volume={}, 
number={}, 
pages={153-156}, 
abstract={Improved data minimization technique to optimize the length of DNA sequence and alignment result characters representation is presented in this paper. The primary objective is to improve and optimize data representation for DNA sequences alignment and result character. The proposed design change in algorithm and architecture is presented in this paper. Algorithm design based on binary equivalent method is used to obtain the optimal size of characters representation. The code is written, compiled and simulated using Altera Quartus II Version 9.0 EDA tools. Verilog Hardware Description Language (HDL) and Altera Cyclone II EP2C35 FPGA are used as coding language and target device respectively. In addition, the structural modelling technique is used to reduce the design complexity. Simulation result showed that the improved data minimization technique takes 50% more memory compared to previous work, but it covers 6 DNA sequences and alignment result characters.}, 
keywords={data handling;data structures;DNA;hardware description languages;medical computing;data minimization technique;memory space complexity reduction;DNA local alignment accelerator application;characters representation;data representation;DNA sequences alignment;algorithm design;binary equivalent method;Altera Quartus II;EDA tools;Verilog Hardware Description Language;HDL;Altera Cyclone II EP2C35 FPGA;coding language;structural modelling technique;design complexity;DNA;Minimization;Computer architecture;Algorithm design and analysis;Hardware design languages;Field programmable gate arrays;Complexity theory;Data minimization;DNA sequences alignment;FPGA}, 
doi={10.1109/ISCAIE.2012.6482087}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{7169446, 
author={M. d. Jong and A. v. Deursen}, 
booktitle={2015 IEEE/ACM 3rd International Workshop on Release Engineering}, 
title={Continuous Deployment and Schema Evolution in SQL Databases}, 
year={2015}, 
volume={}, 
number={}, 
pages={16-19}, 
abstract={Continuous Deployment is an important enabler of rapid delivery of business value and early end user feedback. While frequent code deployment is well understood, the impact of frequent change on persistent data is less understood and supported. SQL schema evolutions in particular can make it expensive to deploy a new version, and may even lead to downtime if schema changes can only be applied by blocking operations. In this paper we study the problem of continuous deployment in the presence of database schema evolution in more detail. We identify a number of shortcomings to existing solutions and tools, mostly related to avoidable downtime and support for foreign keys. We propose a novel approach to address these problems, and provide an open source implementation. Initial evaluation suggests the approach is effective and sufficiently efficient.}, 
keywords={public domain software;SQL;continuous deployment;SQL database schema evolution;open source implementation;Databases;Software;Switches;Synchronization;Production;Hard disks;Vehicles;Continuous Deployment;SQL databases;schema evolution}, 
doi={10.1109/RELENG.2015.14}, 
ISSN={}, 
month={May},}
@ARTICLE{8182976, 
author={S. Zola and A. Baran and B. Debski and D. Jableka}, 
journal={Monthly Notices of the Royal Astronomical Society}, 
title={The phase smearing effect in the light curves of contact binaries observed by the Kepler mission and the determination of the parameters of 17 contact systems}, 
year={2016}, 
volume={466}, 
number={2}, 
pages={2488-2495}, 
abstract={The Kepler mission observations, taken in the long cadence mode, have a time resolution of about 30 min. In this paper, we investigate how the long cadence binning influences the shapes of the light curves of eclipsing binaries. A simulated light curve of a contact binary exhibiting a flat-bottom secondary minimum was applied for this purpose. We found that the binning caused a change in the variation amplitude and the shape of the minima. We modelled the simulated light curves corresponding to periods between 0.2 and 2 d using a code that does not account for binning and we derived the parameters. It turned out that only when the binary period is close to or longer than about 1.5 d are the solutions derived with such a code accurate. Rigorous modelling of systems with shorter periods requires the use of codes that do account for phase smearing due to long exposure times. We selected a sample of contact binaries observed by the Kepler mission, exhibiting a flat-bottom secondary minimum and showing no intrinsic activity. We solved the light curves of the sample with the most recent (2015) version of the Wilson–Devinney code and we derived the system parameters. The best models that we derived indicate that most of the systems in our sample have a deep contact configuration and that 13 out of 17 required the addition of a third light for good fits. Our results suggest that 13 systems could have tertiary companions.}, 
keywords={binaries: eclipsing;stars: fundamental parameters}, 
doi={10.1093/mnras/stw3138}, 
ISSN={0035-8711}, 
month={Aug},}
@INPROCEEDINGS{6906759, 
author={H. Ulusoy and M. Kantarcioglu and E. Pattuk and K. Hamlen}, 
booktitle={2014 IEEE International Congress on Big Data}, 
title={Vigiles: Fine-Grained Access Control for MapReduce Systems}, 
year={2014}, 
volume={}, 
number={}, 
pages={40-47}, 
abstract={Security concerns surrounding the rise of Big Data systems have stimulated myriad new Big Data security models and implementations over the past few years. A significant disadvantage shared by most of these implementations is that they customize the underlying system source code to enforce new policies, making the customizations difficult to maintain as these layers evolve over time (e.g., over version updates). This paper demonstrates how a broad class of safety policies, including fine-grained access control policies at the level of key-value data pairs rather than files, can be elegantly enforced on MapReduce clouds with minimal overhead and without any change to the system or OS implementations. The approach realizes policy enforcement as a middleware layer that rewrites the cloud's front-end API with reference monitors. After rewriting, the jobs run on input data authorized by fine-grained access control policies, allowing them to be safely executed without additional system-level controls. Detailed empirical studies show that this more modular approach exhibits just 1% overhead compared to a less modular implementation that customizes MapReduce directly to enforce the same policies.}, 
keywords={authorisation;Big Data;cloud computing;middleware;Vigiles;MapReduce systems;fine-grained access control policies;key-value data pairs;MapReduce clouds;middleware layer;front-end API;reference monitors;Big Data;Access control;Data models;Big data;Programming;Computational modeling;Java;Access Control;MapReduce;Security}, 
doi={10.1109/BigData.Congress.2014.16}, 
ISSN={2379-7703}, 
month={June},}
@ARTICLE{8212376, 
author={H. Haghi and A. H. Zonoozi and S. Taghavi}, 
journal={Monthly Notices of the Royal Astronomical Society}, 
title={Galactic orbital motions of star clusters: static versus semicosmological time-dependent Galactic potentials}, 
year={2015}, 
volume={450}, 
number={3}, 
pages={2812-2821}, 
abstract={In order to understand the orbital history of Galactic halo objects, such as globular clusters, authors usually assume a static potential for our Galaxy with parameters that appear at the present day. According to the standard paradigm of galaxy formation, galaxies grow through a continuous accretion of fresh gas and a hierarchical merging with smaller galaxies from high redshift to the present day. This implies that the mass and size of disc, bulge, and halo change with time. We investigate the effect of assuming a live Galactic potential on the orbital history of halo objects and its consequences on their internal evolution. We numerically integrate backwards the equations of motion of different test objects located in different Galactocentric distances in both static and time-dependent Galactic potentials in order to see if it is possible to discriminate between them. We show that in a live potential, the birth of the objects, 13 Gyr ago, would have occurred at significantly larger Galactocentric distances, compared to the objects orbiting in a static potential. Based on the direct N-body calculations of star clusters carried out with collisional N-body code, nbody6, we also discuss the consequences of the time-dependence of a Galactic potential on the early- and long-term evolution of star clusters in a simple way, by comparing the evolution of two star clusters embedded in galactic models, which represent the galaxy at present and 12 Gyr ago, respectively. We show that assuming a static potential over a Hubble time for our Galaxy as it is often done, leads to an enhancement of mass-loss, an overestimation of the dissolution rates of globular clusters, an underestimation of the final size of star clusters, and a shallower stellar mass function.}, 
keywords={methods: numerical;galaxies: evolution;galaxies: star clusters: general}, 
doi={10.1093/mnras/stv827}, 
ISSN={0035-8711}, 
month={May},}
@ARTICLE{8566145, 
author={J. Cong and Z. Fang and M. Huang and P. Wei and D. Wu and C. H. Yu}, 
journal={Proceedings of the IEEE}, 
title={Customizable Computing—From Single Chip to Datacenters}, 
year={2019}, 
volume={107}, 
number={1}, 
pages={185-203}, 
abstract={Since its establishment in 2009, the Center for Domain-Specific Computing (CDSC) has focused on customizable computing. We believe that future computing systems will be customizable with extensive use of accelerators, as custom-designed accelerators often provide 10–100X performance/energy efficiency over the general-purpose processors. Such an accelerator-rich architecture presents a fundamental departure from the classical von Neumann architecture, which emphasizes efficient sharing of the executions of different instructions on a common pipeline, providing an elegant solution when the computing resource is scarce. In contrast, the accelerator-rich architecture features heterogeneity and customization for energy efficiency; this is better suited for energy-constrained designs where the silicon resource is abundant and spatial computing is favored—which has been the case with the end of Dennard scaling. Currently, customizable computing has garnered great interest; for example, this is evident by Intel’s $17 billion acquisition of Altera in 2015 and Amazon’s introduction of field-programmable gate-arrays (FPGAs) in its AWS public cloud. In this paper, we present an overview of the research programs and accomplishments of CDSC on customizable computing, from single chip to server node and to datacenters, with extensive use of composable accelerators and FPGAs. We highlight our successes in several application domains, such as medical imaging, machine learning, and computational genomics. In addition to architecture innovations, an equally important research dimension enables automation for customized computing. This includes automated compilation for combining source-code-level transformation for high-level synthesis with efficient parameterized architecture template generations, and efficient runtime support for scheduling and transparent resource management for integration of FPGAs for datacenter-scale acceleration with support to the existing programming interfaces, such as MapReduce, Hadoop, and Spark, for large-scale distributed computation. We will present the latest progress in these areas, and also discuss the challenges and opportunities ahead.}, 
keywords={Computer architecture;Program processors;Transistors;Cloud computing;Field programmable gate arrays;Biomedical imaging;Cloud computing;Accelerator-rich architecture;CPU-FPGA;customizable computing;FPGA cloud;specialized acceleration}, 
doi={10.1109/JPROC.2018.2876372}, 
ISSN={0018-9219}, 
month={Jan},}
@ARTICLE{8175605, 
author={E. Carlesi and A. Knebe and G. Yepes and S. Gottlöber and J. B. Jiménez and A. L. Maroto}, 
journal={Monthly Notices of the Royal Astronomical Society}, 
title={N-body simulations with a cosmic vector for dark energy}, 
year={2012}, 
volume={424}, 
number={1}, 
pages={699-715}, 
abstract={We present the results of a series of cosmological N-body simulations of a vector dark energy (VDE) model, performed using a suitably modified version of the publicly available gadget-2 code. The set-ups of our simulations were calibrated pursuing a twofold aim: (1) to analyse the large-scale distribution of massive objects and (2) to determine the properties of halo structure in this different framework. We observe that structure formation is enhanced in VDE, since the mass function at high redshift is boosted up to a factor of 10 with respect to Λ cold dark matter (ΛCDM), possibly alleviating tensions with the observations of massive clusters at high redshifts and early reionization epoch. Significant differences can also be found for the value of the growth factor, which in VDE shows a completely different behaviour, and in the distribution of voids, which in this cosmology are on average smaller and less abundant. We further studied the structure of dark matter haloes more massive than 5 × 1013h−1M⊙, finding that no substantial difference emerges when comparing spin parameter, shape, triaxiality and profiles of structures evolved under different cosmological pictures. Nevertheless, minor differences can be found in the concentration–mass relation and the two-point correlation function, both showing different amplitudes and steeper slopes. Using an additional series of simulations of a ΛCDM scenario with the same$\Omega _{\rm M}$and σ8used in the VDE cosmology, we have been able to establish whether the modifications induced in the new cosmological picture were due to the particular nature of the dynamical dark energy or a straightforward consequence of the cosmological parameters. On large scales, the dynamical effects of the cosmic vector field can be seen in the peculiar evolution of the cluster number density function with redshift, in the shape of the mass function, in the distribution of voids and on the characteristic form of the growth index γ(z). On smaller scales, internal properties of haloes are almost unaffected by the change of cosmology, since no statistical difference can be observed in the characteristics of halo profiles, spin parameters, shapes and triaxialities. Only halo masses and concentrations show a substantial increase, which can, however, be attributed to the change in the cosmological parameters.}, 
keywords={galaxies: haloes;cosmology: theory;dark matter}, 
doi={10.1111/j.1365-2966.2012.21258.x}, 
ISSN={0035-8711}, 
month={July},}
@ARTICLE{8176182, 
author={H. Yajima and Y. Li and Q. Zhu and T. Abel}, 
journal={Monthly Notices of the Royal Astronomical Society}, 
title={ART2: coupling Lyα line and multi-wavelength continuum radiative transfer}, 
year={2012}, 
volume={424}, 
number={2}, 
pages={884-901}, 
abstract={Narrow-band Lyα line and broad-band continuum have played important roles in the discovery of high-redshift galaxies in recent years. Hence, it is crucial to study the radiative transfer of both Lyα and continuum photons in the context of galaxy formation and evolution in order to understand the nature of distant galaxies. Here, we present a three-dimensional Monte Carlo radiative transfer code, All-wavelength Radiative Transfer with Adaptive Refinement Tree (ART2), which couples Lyα line and multi-wavelength continuum, for the study of panchromatic properties of galaxies and interstellar medium. This code is based on the original version of Li et al., and features three essential modules: continuum emission from X-ray to radio, Lyα emission from both recombination and collisional excitation, and ionization of neutral hydrogen. The coupling of these three modules, together with an adaptive refinement grid, enables a self-consistent and accurate calculation of the Lyα properties, which depend strongly on the UV continuum, ionization structure and dust content of the object. Moreover, it efficiently produces multi-wavelength properties, such as the spectral energy distribution and images, for direct comparison with multi-band observations. As an example, we apply ART2to a cosmological simulation that includes both star formation and black hole growth, and study in detail a sample of massive galaxies at redshifts z = 3.1–10.2. We find that these galaxies are Lyα emitters (LAEs), whose Lyα emission traces the dense gas region, and that their Lyα lines show a shape characteristic of gas inflow. Furthermore, the Lyα properties, including photon escape fraction, emergent luminosity and equivalent width, change with time and environment. Our results suggest that LAEs evolve with redshift, and that early LAEs such as the most distant one detected at z ∼ 8.6 may be dwarf galaxies with a high star formation rate fuelled by infall of cold gas, and a low Lyα escape fraction.}, 
keywords={line: profiles;radiative transfer;dust, extinction;galaxies: evolution;galaxies: formation;galaxies: high-redshift}, 
doi={10.1111/j.1365-2966.2012.21228.x}, 
ISSN={0035-8711}, 
month={Aug},}
@INPROCEEDINGS{7551406, 
author={M. M. U. Alam and A. Muzahid}, 
booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)}, 
title={Production-Run Software Failure Diagnosis via Adaptive Communication Tracking}, 
year={2016}, 
volume={}, 
number={}, 
pages={354-366}, 
abstract={Software failure diagnosis techniques work either by sampling some events at production-run time or by using some bug detection algorithms. Some of the techniques require the failure to be reproduced multiple times. The ones that do not require such, are not adaptive enough when the execution platform, environment or code changes. We propose ACT, a diagnosis technique for production-run failures, that uses the machine intelligence of neural hardware. ACT learns some invariants (e.g., data communication invariants) on-the-fly using the neural hardware and records any potential violation of them. Since ACT can learn invariants on-the-fly, it can adapt to any change in execution setting or code. Since it records only the potentially violated invariants, the postprocessing phase can pinpoint the root cause fairly accurately without requiring to observe the failure again. ACT works seamlessly for many sequential and concurrency bugs. The paper provides a detailed design and implementation of ACT in a typical multiprocessor system. It uses a three stage pipeline for partially configurable one hidden layer neural networks. We have evaluated ACT on a variety of programs from popular benchmarks as well as open source programs. ACT diagnoses failures caused by 16 bugs from these programs with accurate ranking. Compared to existing learning and sampling based approaches, ACT has better diagnostic ability. For the default configuration, ACT has an average execution overhead of 8.2%.}, 
keywords={neural nets;program debugging;program diagnostics;software fault tolerance;production-run software failure diagnosis;adaptive communication tracking;production-run time;bug detection algorithms;ACT;machine intelligence;neural hardware;data communication invariants;sequential bugs;concurrency bugs;multiprocessor system;three stage pipeline;hidden layer neural networks;open source programs;Computer bugs;Neurons;Hardware;Software;Concurrent computing;Biological neural networks;Training;Concurrency bugs;Sequential bugs;Failures;Dependence;Neural hardware}, 
doi={10.1109/ISCA.2016.39}, 
ISSN={1063-6897}, 
month={June},}
@INPROCEEDINGS{8578856, 
author={S. Hu and M. Feng and R. M. H. Nguyen and G. H. Lee}, 
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
title={CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization}, 
year={2018}, 
volume={}, 
number={}, 
pages={7258-7267}, 
abstract={The problem of localization on a geo-referenced aerial/satellite map given a query ground view image remains challenging due to the drastic change in viewpoint that causes traditional image descriptors based matching to fail. We leverage on the recent success of deep learning to propose the CVM-Net for the cross-view image-based ground-to-aerial geo-localization task. Specifically, our network is based on the Siamese architecture to do metric learning for the matching task. We first use the fully convolutional layers to extract local image features, which are then encoded into global image descriptors using the powerful NetVLAD. As part of the training procedure, we also introduce a simple yet effective weighted soft margin ranking loss function that not only speeds up the training convergence but also improves the final matching accuracy. Experimental results show that our proposed network significantly outperforms the state-of-the-art approaches on two existing benchmarking datasets. Our code and models are publicly available on the project website1.}, 
keywords={Satellites;Training;Feature extraction;Task analysis;Image retrieval;Image matching;Measurement}, 
doi={10.1109/CVPR.2018.00758}, 
ISSN={2575-7075}, 
month={June},}
@INPROCEEDINGS{8564544, 
author={J. Villarroel-Ramos and S. Sanchez-Gordon and S. Luján-Mora}, 
booktitle={2018 International Conference on Information Systems and Computer Science (INCISCOS)}, 
title={Architectural Metamodel for Requirements of Images Accessibility in Online Editors}, 
year={2018}, 
volume={}, 
number={}, 
pages={312-319}, 
abstract={In this study, the authors envision a solution for online editors with accessibility considerations for images. To do this, an architectural metamodel with three levels is proposed: environment, guidelines and information system components. A set of 20 requirements was identified. This set was used to assess the accessibility level of a sample of 12 online platforms. The sample included different types of online platforms: massive open online courses, learning management systems, content management systems and social networking services. After analyzing the results, the authors found that in terms of management of images, the most accessible platforms were Moodle, Sakai, ATutor and MiriadaX. In addition, an insert/edit image interface with accessibility features was prototyped. The validation of this interface showed compliance with the Authoring Tool Accessibility Guidelines (ATAG) 2.0 and the HTML code generated complied with the Web Content Accessibility Guidelines (WCAG) 2.1.}, 
keywords={Guidelines;Tools;Visualization;Information systems;Browsers;User interfaces;Computer science;Architecture, ATAG, Blindness, Design, Images Accessibility, Information System, Low Vision Users, Requirements Metamodel, UAAG, WCAG}, 
doi={10.1109/INCISCOS.2018.00052}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8441108, 
author={M. M. Rahman and P. Galanakou and G. Kalantzis}, 
booktitle={2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)}, 
title={A Fast GPU Point-cloud Registration Algorithm}, 
year={2018}, 
volume={}, 
number={}, 
pages={111-116}, 
abstract={The purpose of point cloud registration is to find a 3D rigid body transformation so that the 3D coordinates of the point cloud at different angles can be correctly matched. With the current advances of high definition (HD) 3D cameras, several applications have emerged utilizing stereoscopic cameras. For real time objects tracking, fast point cloud registration calculations are required. In the current study we have considered two methods: a standard Singular Value Decomposition (SVD) and a truncated SVD (TSVD) point cloud registration algorithm. The registration process can be summarized in the following steps; the centroids of the chosen point datasets are first found, then they both are aligned to the origin, and then the optimal rotation and translation are determined based on the SVD or on the TSVD technique. Our strategy was firstly to identify the major computational bottlenecks of our code, and secondly parallelize them on the GPU accordingly. Performance tests were conducted on three GPU cards in comparison to a serial version of the algorithm executed on a CPU. Performance comparisons are also conducted between the parallel SVD and the parallel TSVD in order to test the computational efficiency of them on GPU cards. The studies indicated that there is no computational benefit from the parallization of the simple SVD on GPU. On the contrary, there is computational advantage of the parallel TSVD, but it varies with the GPU architecture. Speedup factors were recorded for every registration steps for all GPU cards. The step 2 of registration process was the most computational expensive task for the algorithm, and when it was parallelized on K40m card gave a maximum speed up of ~100 for the maximum number of pixels, while for other resolution sizes the performance of K40m decreased dramatically. The GTX1080Ti card achieved the highest speed up of ~150 for block 2 calculations, for 8K resolution. In overall, for the full registration process GTX1080Ti indicated a linear increase of speedup factors versus the number of pixels, fact that renders it is the most suitable GPU card with respect to the other GPU cards used for the specific application.}, 
keywords={Three-dimensional displays;Matrix decomposition;Graphics processing units;Covariance matrices;Symmetric matrices;Standards;Principal component analysis;point-cloud registration;Singular Value Decomposition (SVD);Truncated SVD;GPU;speedup factors}, 
doi={10.1109/SNPD.2018.8441108}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{6757640, 
author={V. Chiriseri and S. Winberg and S. Rajan}, 
booktitle={2013 Africon}, 
title={RHINO Cluster Control and management system}, 
year={2013}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={The objective of this paper is to present the RHINO ARM API Cluster Control System (RAACMS) that will enable a user to access and control networked Reconfigurable Hardware Interface for computing and radio (RHINO) platforms. The framework is designed to run on a reconfigurable platforms consisting of FPGA and an ARM processor connected in a cluster. This system is built around a client-server design, and includes an API on the ARM and control PC, that enables users to execute their code on the control PC and control and change variables on a cluster of RHINO platforms. This paper will present the design and implementation of the RAACMS on the cluster of RHINOs at the University of Cape Town. Tests are performed on a prototype version of the framework. The conclusions discuss uses of the systems, together with further plans for improving the framework.}, 
keywords={application program interfaces;client-server systems;field programmable gate arrays;microprocessor chips;software radio;telecommunication control;telecommunication network management;RHINO cluster control and management system;RHINO ARM API cluster control system;RAACMS;reconfigurable hardware interface for computing radio;RHINO platforms;reconfigurable platforms;FPGA;ARM processor;client-server design;control PC;University of Cape Town;Field programmable gate arrays;Servers;Hardware;Registers;Prototypes;Computers;Educational institutions;signal processing;cluster management system;field programmable gate arrays;radio astronomy}, 
doi={10.1109/AFRCON.2013.6757640}, 
ISSN={2153-0033}, 
month={Sep.},}
@INPROCEEDINGS{7824035, 
author={Z. Najam and M. Y. Qadri and S. Najam}, 
booktitle={2016 6th International Conference on Intelligent and Advanced Systems (ICIAS)}, 
title={Real-time implementation of DVFS enhanced LEON3 MPSoC on FPGA}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In the field of embedded system there exists a trade off between power/performance optimization, hence many heuristics and techniques were presented at various development levels such as hardware software co-design, schedulers and optimal code compilation. This paper presents an enhanced version of LEON3 architecture which includes support for run-time management of supply voltage and processor operating frequency. This enhancement can be useful to implement various DVFS driving algorithms in LEON3 architecture aiming to leverage power consumption and throughput. Frequency scaling on the fly is based on dynamically reconfigurable clock synthesis feature available in Xilinx FPGA Virtex-4 or higher. The implementation of DVFS in LEON3 architecture is driven and controlled by general-purpose I/O port attached to advanced peripheral bus (APB) to change processor frequency on the fly during the execution of application programs. The work is implemented as a prototype on a Xilinx FPGA platform and incurs very small hardware overheads.}, 
keywords={electric potential;embedded systems;field programmable gate arrays;input-output programs;microprocessor chips;power aware computing;DVFS;embedded system;power optimization;performance optimization;LEON3 architecture;run time management;supply voltage;processor operating frequency;power consumption;reconfigurable clock synthesis;Xilinx FPGA Virtex;I/O port;advanced peripheral bus;APB;Clocks;Heuristic algorithms;Computer architecture;Field programmable gate arrays;Hardware;IP networks;Power demand;DVFS;GPIO;LEON3;APB;Xilinx FPGA}, 
doi={10.1109/ICIAS.2016.7824035}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8190524, 
author={D. M. de Souza and M. Kölling and E. F. Barbosa}, 
booktitle={2017 IEEE Frontiers in Education Conference (FIE)}, 
title={Most common fixes students use to improve the correctness of their programs}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Teach students how to program is the main goal of most introductory CS courses. In fact, programming is one of the basic skills a professional in CS should have. However, there are many difficulties students face when they are learning how to program and, consequently, it is common introductory programming courses have high dropout rates. The purpose of this paper is to identify and discuss the most common fixes students use to improve the correctness of their programs. The findings can be useful to help students to produce more correct programs and highlight issues about possible difficulties they are having. To do so, we used the BLACKBOX data collection, which stores the actions of the BLUEJ programming environment users. The main idea was to observe the modifications students did in their source codes that made a failed JUNIT test case become succeeded. The results suggest the majority of fixes students use in their source codes are related either to the change of expressions or to the restructuring of code, reflecting difficulties in logic and problem solving among students.}, 
keywords={computer science education;educational courses;program testing;teaching;introductory CS courses;introductory programming courses;students teaching;BLACKBOX data collection;fixes;source codes;BLUEJ programming environment users;Programming profession;Computer bugs;Data collection;Testing;Java;Electronic mail}, 
doi={10.1109/FIE.2017.8190524}, 
ISSN={}, 
month={Oct},}
@ARTICLE{8048828, 
author={S. Cass}, 
journal={IEEE Spectrum}, 
title={Wearable tech for halloween - The gemma MO's embedded python lets you change your code on the fly [Resources_Tools]}, 
year={2017}, 
volume={54}, 
number={10}, 
pages={15-16}, 
abstract={Halloween is approaching, and with it a global parade of costumes. So I thought this would be the perfect time to try out a new wearable microcontroller from Adafruit Industries: the Gemma M0. Adafruit has been putting out wearable microcontrollers for several years. These differ from conventional controllers, such as the Arduino Uno, in that the wearables are typically more compact and use pads with large through holes for input and output, instead of pins. These holes make it easy to sew boards to fabric or tie conductive thread to the pads. What makes the Gemma M0 particularly interesting is that it runs CircuitPython, Adafruit's modified version of the Python language designed for embedded devices. (At this point, I should note that Limor Fried, the founder of Adafruit, is a member of IEEE Spectrum's editorial advisory board, but she played no role in the origination of this article.).}, 
keywords={microcontrollers;wearable computers;wearable tech;halloween;Adafruit Industries;Gemma M0;wearable microcontrollers;Arduino Uno;CircuitPython;Python language;embedded devices;Limor Fried;IEEE Spectrum;editorial advisory board}, 
doi={10.1109/MSPEC.2017.8048828}, 
ISSN={0018-9235}, 
month={October},}
@INPROCEEDINGS{7740300, 
author={}, 
booktitle={2016 6th International Conference on IT Convergence and Security (ICITCS)}, 
title={Table of contents}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={The following topics are dealt with: power-aware data structure; memory allocation techniques; EPS-based motion recognition systems; secure distance bounding protocol; TH-UWB; cooperative spectrum sensing scheme; femtocell network; video quality; unequal loss protection; Wi-Fi based broadcasting system; example-based retrieval system; human motion data; astronaut virtual training system; layout familiarization training; stereo image correction; 3D optical microscope; UHF RFID; BLE; stereo-based tag association; medical image segmentation; sensitive adaptive thresholding; interactive event recognition; semantic video understanding; Bgslibrary algorithms; traffic surveillance video; CUDA-based acceleration techniques; image filtering; image-based ship detection; AR navigation; augmented reality; vehicle information; head-up display: LiDAR data; classifier performance; people detection; wavelet transform; max-min energy-efficiency optimization; wireless powered communication network; harvest-then-transmit protocol; ARM64bit Server; WeChat text messages service flow traffic classification; machine learning technique; load balancing; WSN; novel Markov decision process based routing algorithm; repulsion-propulsion firefly algorithm; sentence based mathematical problem solving approach; ontology modeling; sentiment analysis; HARN algorithm; real-time road surface condition determination algorithm; automatic weather system; kinematic constraint method; human gesture recognition; weighted dynamic time warping; IT demand governance; business goal structuring notation; software requirement specification; AOP-based approach;decision support system; proactive flood control; context-aware user interface field classification; common vocabulary set; maritime equipment; e-navigation services; genetic algorithm; strategic information systems planning; speech enhancement; ES information; phase-error based filters; holistic service orchestration; distributed micro data center; hierarchical cluster network; wellness sports industry; secure agent based architecture; resource allocation; cloud computing; distributed multi-platform context-aware user interface; parallel prime number labeling; XML data; MapReduce; metadata extension; data presentations; aspect-oriented user interfaces design integration; Angular 2 framework; energy impact; Web user interface technology; mobile devices; JIT compilation-based unified SQL query optimization system; partial materialization; data integration; SQL-on-Hadoop engines; z-transform based encryption algorithm; FARIS; fast and memory-efficient URL filter; domain specific machine; synchronized blind audio watermarking; multilevel DWT; windowed vector modulation; packet length covert channel capacity estimation; flexible authentication protocol; WBAN; SMS-based mobile botnet detection module; full-duplex jamming attack; active eavesdropping; Big Data security analysis; complex security requirements patterns; SSH attacks; SSL/TLS nonintrusive proxy; JSON data; neural stegoclassifier; polymorphic malware detection; linguistic based steganography; lexical substitution; syntactical transformation; holistic-based feature extraction; error correcting code biometric template protection technique; network based IMSI catcher detection; Internet of Things environment; light-weight API-call safety checking; automotive control software; SmartDriver; project management software; model-based testing; exploratory testing; automated ECG beat classification system and convolutional neural networks.}, 
keywords={application program interfaces;aspect-oriented programming;astronomy computing;audio watermarking;augmented reality;Big Data;biometrics (access control);body area networks;channel capacity;cloud computing;computer based training;cooperative communication;cryptographic protocols;data integration;data structures;decision support systems;discrete wavelet transforms;driver information systems;electrocardiography;electronic messaging;energy conservation;error correction codes;feature extraction;femtocellular radio;formal specification;genetic algorithms;gesture recognition;image filtering;image segmentation;information systems;Internet of Things;invasive software;learning (artificial intelligence);Markov processes;medical image processing;meta data;neural nets;object detection;ontologies (artificial intelligence);optical radar;parallel programming;power aware computing;problem solving;program testing;query processing;radiofrequency identification;resource allocation;sentiment analysis;signal classification;signal detection;speech enhancement;sport;SQL;stereo image processing;storage management;strategic planning;ubiquitous computing;ultra wideband communication;user interfaces;video surveillance;vocabulary;wireless LAN;wireless sensor networks;XML;power-aware data structure;memory allocation techniques;EPS-based motion recognition systems;secure distance bounding protocol;TH-UWB;cooperative spectrum sensing scheme;femtocell network;video quality;unequal loss protection;Wi-Fi based broadcasting system;example-based retrieval system;human motion data;astronaut virtual training system;layout familiarization training;stereo image correction;3D optical microscope;UHF RFID;BLE;stereo-based tag association;medical image segmentation;sensitive adaptive thresholding;interactive event recognition;semantic video understanding;Bgslibrary algorithms;traffic surveillance video;CUDA-based acceleration techniques;image filtering;image-based ship detection;AR navigation;augmented reality;vehicle information;head-up display;LiDAR data;classifier performance;people detection;wavelet transform;max-min energy-efficiency optimization;wireless powered communication network;harvest-then-transmit protocol;ARM64bit Server;WeChat text messages service flow traffic classification;machine learning technique;load balancing;WSN;Markov decision process based routing algorithm;repulsion-propulsion firefly algorithm;holistic service orchestration;distributed micro data center;hierarchical cluster network;wellness sports industry;secure agent based architecture;resource allocation;cloud computing;distributed multi-platform context-aware user interface;parallel prime number labeling;XML data;MapReduce;metadata extension;data presentations;aspect-oriented user interfaces design integration;Angular 2 framework;energy impact;Web user interface technology;mobile devices;JIT compilation-based unified SQL query optimization system;partial materialization;data integration;SQL-on-Hadoop engines;z-transform based encryption algorithm;FARIS;fast and memory-efficient URL filter;domain specific machine;synchronized blind audio watermarking;multilevel DWT;windowed vector modulation;packet length covert channel capacity estimation;flexible authentication protocol;WBAN;SMS-based mobile botnet detection module;phase-error based filters;ES information;speech enhancement;strategic information systems planning;genetic algorithm;e-navigation services;maritime equipment;common vocabulary set;context-aware user interface field classification;proactive flood control;decision support system;AOP-based approach;software requirement specification;business goal structuring notation;IT demand governance;weighted dynamic time warping;human gesture recognition;kinematic constraint method;automatic weather system;real-time road surface condition determination algorithm;HARN algorithm;sentiment analysis;ontology modeling;sentence based mathematical problem solving approach;full-duplex jamming attack;active eavesdropping;Big Data security analysis;complex security requirements patterns;SSH attacks;SSL/TLS nonintrusive proxy;JSON data;neural stegoclassifier;polymorphic malware detection;linguistic based steganography;lexical substitution;syntactical transformation;holistic-based feature extraction;error correcting code biometric template protection;network based IMSI catcher detection;convolutional neural networks;automated ECG beat classification system;exploratory testing;model-based testing;project management software;SmartDriver;automotive control Software;light-weight API-call safety checking;Internet of Things environment}, 
doi={10.1109/ICITCS.2016.7740300}, 
ISSN={}, 
month={Sep.},}
@INPROCEEDINGS{7422340, 
author={}, 
booktitle={2015 Signal Processing and Intelligent Systems Conference (SPIS)}, 
title={[Title page]}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-1}, 
abstract={The following topics are dealt with: robust beamforming; tax fraud detection algorithm; multipath effect; intelligent stock trading systems; region based histogram analysis strategy; automatic drug-drug interaction extraction; dynamic feature weighting; bias protein primary sequence; protein structure prediction; edge histogram analysis; sentiment analysis; community question answering systems; user-friendly visual secret sharing; multiple-query image retrieval; singular Lorenz measure method; KNN-scatter search optimization algorithm; VoIP network; quaternion-based salient region detection; wavelet image denoising based spatial noise estimation; subspace-based speech enhancement; high speed vehicle application; INS-GPS navigation system; color image segmentation; virtual machine placement; traffic density estimation; interference binary channel; QoS-aware resource allocation; LTE networks; Petri net based transformation method; objectionable image recognition; convolutional neural nets; semisupervised intrusion detection; online Laplacian twin support vector machine; secure echo steganography; compressed sensing DOA estimation; DV-Hop localization algorithm; wireless sensor networks; salient object detection; global contrast graph; speaker-independent isolated Persian digit recognition; enhanced vector quantization algorithm; dictionary learning; parallel secure turbo code; and traffic sign recognition.}, 
keywords={compressed sensing;data mining;image processing;image retrieval;Long Term Evolution;resource allocation;speech processing;steganography;virtual machines;wireless sensor networks;traffic density estimation;interference binary channel;QoS-aware resource allocation;LTE networks;Petri net based transformation method;objectionable image recognition;convolutional neural nets;semisupervised intrusion detection;online Laplacian twin support vector machine;secure echo steganography;compressed sensing DOA estimation;DV-Hop localization algorithm;wireless sensor networks;salient object detection;global contrast graph;speaker-independent isolated Persian digit recognition;enhanced vector quantization algorithm;dictionary learning;parallel secure turbo code;traffic sign recognition;virtual machine placement;color image segmentation;INS-GPS navigation system;high speed vehicle application;subspace-based speech enhancement;wavelet image denoising based spatial noise estimation;quaternion-based salient region detection;VoIP network;KNN-scatter search optimization algorithm;singular Lorenz measure method;multiple-query image retrieval;user-friendly visual secret sharing;community question answering systems;sentiment analysis;edge histogram analysis;protein structure prediction;bias protein primary sequence;dynamic feature weighting;automatic drug-drug interaction extraction;region based histogram analysis strategy;intelligent stock trading systems;multipath effect;tax fraud detection algorithm;robust beamforming}, 
doi={10.1109/SPIS.2015.7422340}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6745506, 
author={}, 
booktitle={2013 Fourth International Conference on e-Learning "Best Practices in Management, Design and Development of e-Courses: Standards of Excellence and Creativity"}, 
title={[Copyright notice]}, 
year={2013}, 
volume={}, 
number={}, 
pages={iv-iv}, 
abstract={Copyright (c) 2013 by The Institute of Electrical and Electronics Engineers, Inc. All rights reserved. Copyright and Reprint Permissions: Abstracting is permitted with credit to the source. Libraries may photocopy beyond the limits of US copyright law, for private use of patrons, those articles in this volume that carry a code at the bottom of the first page, provided that the per-copy fee indicated in the code is paid through the Copyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923. Other copying, reprint, or republication requests should be addressed to: IEEE Copyrights Manager, IEEE Service Center, 445 Hoes Lane, P.O. Box 133, Piscataway, NJ 08855-1331. The papers in this book comprise the proceedings of the meeting mentioned on the cover and title page. They reflect the authors' opinions and, in the interests of timely dissemination, are published as presented and without change. Their inclusion in this publication does not necessarily constitute endorsement by the editors, the IEEE Computer Society, or the Institute of Electrical and Electronics Engineers, Inc. IEEE Computer Society Order Number E5036. BMS Part Number CFP13BAH-ART. ISBN 978-0-7695-5036-7.}, 
keywords={}, 
doi={10.1109/ECONF.2013.4}, 
ISSN={}, 
month={May},}