@article{WANG20131070,
title = "GENXICC2.1: An improved version of GENXICC for hadronic production of doubly heavy baryons",
journal = "Computer Physics Communications",
volume = "184",
number = "3",
pages = "1070 - 1074",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2012.10.022",
url = "http://www.sciencedirect.com/science/article/pii/S0010465512003657",
author = "Xian-You Wang and Xing-Gang Wu",
keywords = "Hadronic production, Doubly heavy baryon",
abstract = "We present an improved version of GENXICC, which is a generator for hadronic production of the doubly heavy baryons Ξcc, Ξbc and Ξbb and has been introduced by C.H. Chang, J.X. Wang and X.G. Wu [Comput. Phys. Commun. 177 (2007) 467; Comput. Phys. Commun. 181 (2010) 1144]. In comparison with the previous GENXICC versions, we update the program in order to generate the unweighted baryon events more effectively under various simulation environments, whose distributions are now generated according to the probability proportional to the integrand. One Les Houches Event (LHE) common block has been added to produce a standard LHE data file that contains useful information of the doubly heavy baryon and its accompanying partons. Such LHE data can be conveniently imported into PYTHIA to do further hadronization and decay simulation, especially, the color-flow problem can be solved with PYTHIA8.0.
NEW VERSION PROGRAM SUMMARY
Title of program: GENXICC2.1 Program obtained from: CPC Program Library Reference to original program: GENXICC Reference in CPC: Comput. Phys. Commun. 177, 467 (2007); Comput. Phys. Commun. 181, 1144 (2010) Does the new version supersede the old program: No Computer: Any LINUX based on PC with FORTRAN 77 or FORTRAN 90 and GNU C compiler as well Operating systems: LINUX Programming language used: FORTRAN 77/90 Memory required to execute with typical data: About 2.0 MB No. of bytes in distributed program: About 2 MB, including PYTHIA6.4 Distribution format: .tar.gz Nature of physical problem: Hadronic production of doubly heavy baryons Ξcc, Ξbc and Ξbb. Method of solution: The upgraded version with a proper interface to PYTHIA can generate full production and decay events, either weighted or unweighted, conveniently and effectively. Especially, the unweighted events are generated by using an improved hit-and-miss approach. Reasons for new version: Responding to the feedback from users of CMS and LHCb groups at the Large Hadron Collider, and based on the recent improvements of PYTHIA on the color-flow problem, we improve the efficiency for generating the unweighted events, and also improve the color-flow part for further hadronization. Especially, an interface has been added to import the output production events into a suitable form for PYTHIA8.0 simulation, in which the color-flow during the simulation can be correctly set. Typical running time: It depends on which option is chosen to match PYTHIA when generating the full events and also on which mechanism is chosen to generate the events. Typically, for the dominant gluon–gluon fusion mechanism to generate the mixed events via the intermediate diquarks in (cc)[3S1]3̄ and (cc)[1S0]6 states, setting IDWTUP=3 and unwght =.true., it takes 30 min to generate 105 unweighted events on a 2.27 GHz Intel Xeon E5520 processor machine; setting IDWTUP=3 and unwght =.false. or IDWTUP=1 and IGENERATE=0, it only needs 2 min to generate the 105 baryon events (the fastest way, for theoretical purposes only). As a comparison, for previous GENXICC versions, if setting IDWTUP=1 and IGENERATE=1, it takes about 22 hours to generate 1000 unweighted events. Keywords: Event generator; Doubly heavy baryons; Hadronic production. Summary of the changes (improvements): (1) The scheme for generating unweighted events has been improved; (2) One Les Houches Event (LHE) common block has been added to record the standard LHE data in order to be the correct input for PYTHIA8.0 for later simulation; (3) We present the code for connecting GENXICC to PYTHIA8.0, where three color-flows have to be correctly set for later simulation. More specifically, we present the changes together with their detailed explanations in the following: •Unweighted events generation. For theoretical studies, e.g. to derive the total baryon production cross-section or various differential distributions, one can directly use the fastest way, e.g. setting the PYTHIA parameter IDWTUP=3 and unwght =.false. or setting IDWTUP=1 and IGENERATE=0 (in these cases, xmaxup should be set as 0), to generate the baryon events [1]. By using GENXICC [2], [3] in this way, some interesting properties for hadronic production of Ξcc, Ξbc and Ξbb have been found in the literature, cf. Refs. [4], [5], [6]. While, for the events simulation in detector conditions, it is necessary to get the unweighted events. In previous GENXICC versions, the unweighted events are generated by setting IDWGTUP=1 and IGENERATE=1; i.e., the events are generated according to PYTHIA’s inner mechanism, the so-called hit-and-miss approach (von Neumann algorithm), to reject those unsatisfied events and output the allowed events. But, as is well-known, the original hit-and-miss approach is really time-consuming. Some alterations must be made to improve its efficiency.As an intermediate step, in BCVEGPY2.1a [7] we have suggested a practical trick to increase the efficiency of generating unweighted events (BCVEGPY is a generator for hadronic production Bc mesons [8]). In this trick, other than choosing the maximum differential cross-section as a reference weight in the hit-and-miss approach, we directly select an effective differential cross-section, which is smaller than the maximum one, as the reference weight [7]. This treatment can greatly improve the generation efficiency without affecting the total cross-section of the process. However, in using this trick to generate unweighted events such as for CMS detector simulation, one will incidentally find a false peak in the Bc–pt distributions. This is caused by the fact that sometimes the same event will be stored a (false) large number of times in the hit-and-miss process. Then, we are facing a dilemma: such a false peak can be avoided by raising the effective reference weight to a value approaching the maximum weight, but, conversely, a larger reference weight will surely lead to a much longer running time.One observes that by using the VEGAS algorithm [9], the SPRING-BASES program [10] performs the integration in using the BASES subroutines and generates events with a probability proportional to the integrand in using the SPRING subroutines. After each iteration of VEGAS running, the integration result and the maximum value of the function will be stored in a file for each cell of the adaptive mesh. In the generation stage, a cell is chosen with a probability proportional to the corresponding integral, and then a point in the cell is generated using the hit-and-miss approach. This method is highly efficient, but it has the disadvantage that the required amount of storage space grows exponentially with the integration dimension.Next, in the POWHEG program [11] the authors have developed a new method MINT [12] to replace the SPRING-BASES package. This MINT package also use the VEGAS algorithm to perform the integration. What’s different is that it does not store the value of the integral but stores the upper bound value for each cell. The multidimensional stepwise function that equals the upper bound of the function to be integrated in each cell is in fact an upper bound for the whole function, which is the wanted upper bound for BCVEGPY2.1a or PYTHIA. So, the program is to find the upper bound grid for those cells. And next, by using again the hit-and-miss technique in each cell, one can generate the points according to the original distribution.Based on these methods, as a further improvement, we present an ultimate solution to generate unweighted events in the present new GENXICC version. We adopt the MINT algorithm but with certain alterations to do the simulation. For this purpose, we change the VEGAS subroutine as follows. Three new variables have been added in the original VEGAS subroutine, where xint is the integral value for the integrand fxn after a ndim-dimensional integration, the xmax array records the upper bounding envelope of the integrand in all cells, imode is a flag:vegas(fxn,ndim,ncall,itmx,nprn,xint,xmax,imode)–When called with imode=0, vegas performs the integration over the integrand fxn, and stores the answer in a common block parameter vegsec.–xmax stands for a (nvegbin,ndim) dimensional array, where nvegbin denotes the bin number for each coordinate, and ndim stands for the integration dimension. When called with imode=1, vegas will first initialize all the elements of xmax to be xint1/ndim, where xint equals the value of vegsec that has been derived from a previous VEGAS running with imode=0. During the following sampling iteration, when the calculated integral value is larger than the initial xmax(nvegbin,ndim) value in a specific cell, then the value of xmax(nvegbin,ndim) for this cell will be increased by a fixed factor f=1+1/10ndim. After a sufficiently large number of calls, the values of xmax(nvegbin,ndim) will be stabilized for all cells. Such a final xmax array will be stored in the same grid file as that of the importance sampling function in order to do the final simulation. Comparing to the previous GENXICC versions, in doing the initialization (subroutine evntinit), we will call vegas twice with imode=0 and imode=1 accordingly to generate the upper bound grid xmax and also a more precise importance sampling function. Practically, the user can directly use the existing grid file derived by previous VEGAS running to generate events by setting methodevnt=2 or methodevnt=3 without running VEGAS again, which is the same as in the older GENXICC versions. Once the xmax array has been set up in the previous steps, one can call the subroutine gen to generate events. For this purpose, three options for calling the gen subroutine are programmed: where jmode=0 is to initialize a step-wise function xmmm which is described in [12]. And jmode=3 is to print out the generation statistics. The calling of the gen subroutine with jmode=1 has been implemented into the UPEVNT subroutine to generate events according to the probability proportional to the integrand. Each event produced needs several iterations with a three step procedure as follows: 1.Calculate the upper bounding function by generating a set of step-wise functions, each of them associated with a specific coordinate (dimension).2.Call the phase_gen subroutine to generate a random phase-space point and calculate the integral.3.Judge whether such a point be kept or not by using the hit-and-miss approach with the help of the upper bounding function. In VEGAS the integral together with its numerical error are related to the sampling numbers ncall and the iteration times itmx. So, to generate full events, we suggest the user do a test run first in order to find the effective and time-saving parameters for VEGAS. Furthermore, to validate the program, we use the same default parameters as the input for the program to generate mixed events via the intermediate diquark in (cc)[3S1]3̄ and (cc)[1S0]6 states, and the same for the other two doubly heavy baryons Ξbc and Ξbb. Fig. 1Comparison of the normalized Ξcc transverse momentum (PT) and rapidity (y) distributions derived by IDWTUP=3 (events) and IDWTUP=1 (differential cross-sections), which are represented by a solid line and dotted line respectively. As a cross-check, we derive the unweighted Ξcc event distributions by setting IDWTUP=3 andunwght =.true., and the weighted Ξcc differential distributions by setting IDWTUP=1 and IGENERATE=0, respectively, which are shown in Fig. 1. The two distributions after proper normalization agree well with each other. This demonstrates that our present scheme for unweighted events is correct. •Color-flow problem. Within the framework of non-relativistic QCD (NRQCD) [13], the production of the ΞQQ′ baryon can be factorized into two steps. The first step is to produce two free heavy-quark pairs QQ̄ and Q′Q̄′, which is perturbatively calculable. The second step is to make the two heavy quarks Q and Q′ into a bounding diquark (QQ′) in [3S1] (or [1S0]) spin state and in 3̄ (or 6) color state accordingly; then it will be hadronized into a ΞQQ′ baryon by grabbing a light quark u, d or s (plus a suitable number of gluons), whose probability is described by a non-perturbative NRQCD matrix element. More explicitly, the intermediate diquarks in Ξcc and Ξbb have two spin-and-color configurations [3S1]3̄ and [1S0]6; while for the intermediate diquark (bc) in Ξbc, there are four spin-and-color configurations Ξbc[3S1]3̄, Ξbc[3S1]6, Ξbc[1S0]3̄, and Ξbc[1S0]6.Since a baryon is constructed by three valence quarks, under the standard color-flow decomposition, there must be three different color-flow lines ending at a baryon [14], [3]. It is different from the case of mesons, where the color-flow lines of the quark and anti-quark inside a meson are continued. The previous PYTHIA6.4 can only generate full events with two or less independent color-flow lines, thus in GENXICC2.0, we adopt a ‘cheating method’ to generate the events. That is, by using the fact that 3⨂3=6⨁3̄ and 3⨂3̄=8⨁1 in the general QCD SU(3) color space [3]: –We combine any two of the color-flow lines ending with two quarks into one anti-color-flow line ending with one anti-quark with a color 3̄ that is different from the two quarks (the third color with respect to those of the two quarks);–Secondly, such an anti-color-flow line obtained by the combination may be continued (connected) to the remaining quark’s color-flow line in the baryon;–Finally, as a consequence, the color-flow lines ending at a baryon become ‘joined without ends’ at all, which is the requirement of the color-singlet bound state. However we should point out that due to approximation and simplification with the ‘cheating method’, the obtained information about the ‘tiny jets’, corresponding to the soft anti-quark and soft gluon(s) produced in fragmentation of the doubly heavy diquark, may not be very reliable. When the experimental analyzer uses the generator to simulate the baryon decay and other parton hardronization, they are still facing the color-flow rearrangement error; sometimes, PYTHIA will present an error message to show that the color-flow rearrangement is wrong during the parton’s evolution process, and then it will stop running. To generate full events of the doubly heavy baryons smoothly, the best way is to improve PYTHIA with proper treatment of the color-flow lines ending at the baryon. Fortunately, such an improvement has been done in its newest version PYTHIA8.0. Based on the suggestion from Peter Skands, we find that the further event simulation can be implemented in PYTHIA8.0 correctly even with the previously generated Les Houches Event (LHE) files [15]. As has been described in Ref. [16], the read-in of an external generator’s LHE files generated by PYTHIA6.4 is simply technically less sophisticated and less able to deal with junctions, even though the physics implementation of junction fragmentation is in principle the same. As a solution, PYTHIA8.0 improves the treatment of these LHE files. We adopt the same trick as that of BCVEGPY2.1a [7] to generate and record the data; i.e. two subroutines have been introduced in the file pythialheinit.F. One of the subroutine XICC_PYUPIN is used to fill the HEPRUP common block with information on the incoming beams and the allowed processes, and optionally stores that information on file. Another subroutine XICC_WRITE_LHE is used to store event information in the HEPEUP common block. And these two subroutines are called by the main program xicc.F to generate the LHE file that records the momentum and color information for the events [17]. More specifically, in the main program, the subroutine UPEVNT will be called for generating the baryon events, which is used to call the program to generate the baryon with a probability proportional to the importance sampling function. Here, one can also use the PYTHIA subroutine PYEVNT for the purpose, but one should at the same time switch off the hadronization, the initial and final state parton shower and so on, in order to avoid the color-flow rearrangement error. That generated baryon information together with the information of the accompanying partons will be stored in the Les Houches common block and will be exported to a LHE file “GENXICC.lhe”. And then, such a LHE file can be used when necessary by PYTHIA8.0 to do the following simulations. Here, to successfully simulate the baryon’s production and decay, the user needs to install PYTHIA8 [15] following the instructions on its official web-site: http://home.thep.lu.se/~torbjorn/Pythia.html. For using our generator, the user can use the following command to compile the configuration file, where $(PYTHIA8) stands for the PYTHIA8.0 installation directory. For convenience, we put an example configuration file in the package for generating the full baryon production and decay events in PYTHIA8.0, which is placed in the main folder of the program and is named as “genxicc.cc”."
}
@article{BUTLER2015763,
title = "Cancer Incidence Among US Medicare ESRD Patients Receiving Hemodialysis, 1996-2009",
journal = "American Journal of Kidney Diseases",
volume = "65",
number = "5",
pages = "763 - 772",
year = "2015",
issn = "0272-6386",
doi = "https://doi.org/10.1053/j.ajkd.2014.12.013",
url = "http://www.sciencedirect.com/science/article/pii/S0272638615000177",
author = "Anne M. Butler and Andrew F. Olshan and Abhijit V. Kshirsagar and Jessie K. Edwards and Matthew E. Nielsen and Stephanie B. Wheeler and M. Alan Brookhart",
keywords = "Hemodialysis, malignancy, carcinoma, tumor, cancer incidence, cancer risk factor, end-stage renal disease (ESRD), chronic kidney failure, US Renal Data System (USRDS), diagnostic code, claims-based cancer definition",
abstract = "Background
Patients with end-stage renal disease (ESRD) receiving dialysis have been reported to have increased risk of cancer. However, contemporary cancer burden estimates in this population are sparse and do not account for the high competing risk of death characteristic of dialysis patients.
Study Design
Retrospective cohort study.
Setting & Participants
US adult patients enrolled in Medicare’s ESRD program who received in-center hemodialysis.
Factors
Demographic/clinical characteristics.
Outcomes
For overall and site-specific cancers identified using claims-based definitions, we calculated annual incidence rates (1996-2009). We estimated 5-year cumulative incidence since dialysis therapy initiation using competing-risk methods.
Results
We observed a constant rate of incident cancers for all sites combined, from 3,923 to 3,860 cases per 100,000 person-years (annual percentage change, 0.1; 95% CI, −0.4 to 0.6). Rates for some common site-specific cancers increased (ie, kidney/renal pelvis) and decreased (ie, colon/rectum, lung/bronchus, pancreas, and other sites). Of 482,510 incident hemodialysis patients, cancer was diagnosed in 37,128 within 5 years after dialysis therapy initiation. The 5-year cumulative incidence of any cancer was 9.48% (95% CI, 9.39%-9.57%) and was higher for certain subgroups: older age, males, nonwhites, non-Hispanics, nondiabetes primary ESRD cause, recent dialysis therapy initiation, and history of transplantation evaluation. Among blacks and whites, we observed 35,767 cases compared with 25,194 expected cases if the study population had experienced rates observed in the US general population (standardized incidence ratio [SIR], 1.42; 95% CI, 1.41-1.43). Risk was most elevated for cancers of the kidney/renal pelvis (SIR, 4.03; 95% CI, 3.88-4.19) and bladder (SIR, 1.57; 95% CI, 1.51-1.64).
Limitations
Claims-based cancer definitions have not been validated in the ESRD population. Information for cancer risk factors was not available in our data source.
Conclusions
These results suggest a high burden of cancer in the dialysis population compared to the US general population, with varying patterns of cancer incidence in subgroups."
}
@article{DAI201421,
title = "3D numerical modeling using smoothed particle hydrodynamics of flow-like landslide propagation triggered by the 2008 Wenchuan earthquake",
journal = "Engineering Geology",
volume = "180",
pages = "21 - 33",
year = "2014",
note = "Special Issue on The geological and geotechnical hazards of the 2008 Wenchuan earthquake, China: Part I",
issn = "0013-7952",
doi = "https://doi.org/10.1016/j.enggeo.2014.03.018",
url = "http://www.sciencedirect.com/science/article/pii/S0013795214000714",
author = "Zili Dai and Yu Huang and Hualin Cheng and Qiang Xu",
keywords = "Flow-like landslides, Wenchuan earthquake, Smoothed particle hydrodynamics, Three-dimensional numerical simulation, Hazard assessment",
abstract = "The extremely strong Wenchuan earthquake triggered thousands of landslides in Sichuan Province, China. Flow-like landslides, such as the Tangjiashan, Wangjiayan, and Donghekou landslides were among the most destructive, causing many casualties and serious economic damage. It is therefore important to identify the flow mechanisms and investigate the specific characteristics of these flow-like landslides. This paper presents a three-dimensional model based on smoothed particle hydrodynamics (SPH) to simulate rapid landslide motion across 3D terrain. The Navier–Stokes equations in a CFD framework are used as governing equations, and artificial viscosity is incorporated into the pressure terms in the momentum equation to dissipate energy to avoid numerical oscillation and particle penetration, thus improving the stability of the numerical results. A non-Newtonian fluid model, the Bingham model, has proven suitable for describing the relationship between the shear strain rate and the shear stress in highly deformed soil materials. In the proposed model it is used as the constitutive model to describe the fluidization characteristics of flow-like landslides combined with the Mohr–Coulomb yield criterion. The model incorporates a no-slip boundary condition, to consider the effect of a solid boundary on slope movement. Ghost particles are created and assigned an artificial velocity. The viscous force caused by the solid boundary is calculated using the relative velocities between the fluid and the boundary particles. Open Multiprocessing (OpenMP), an API for multi-platform shared-memory parallel programming, is used to improve the efficiency of the SPH code running. To show the validity of the proposed approach, a benchmark problem of 3D dam break was simulated. The calculated distances of the surge front at different times agree well with the test results. Numerical modeling of the propagation of the Tangjiashan, Wangjiayan, and Donghekou landslides was performed by the application of SPH models to real flow-like landslides. The whole flow processes of these flow-like landslides across the 3D terrain are represented. The landslides change direction, split or join in, and spread or contract in their flow path in response to the local topography. Time-history curves of the velocity and displacement were obtained to analyze the movement characteristics of the landslide mass. The shapes of the deposition zones after slide occurrence were investigated. Comparisons of the SPH simulated geometry and the surveyed landslide configurations for the Tangjiashan, Wangjiayan and Donghekou landslides were conducted, and show a high degree of similarity. This indicates that the proposed 3D SPH model can accurately represent the evolution of the final slide shape. The prediction of the fluidization characteristics of earthquake-induced flow-like landslides can notably reduce sudden loss of life, as it provides a means for mapping hazardous areas, for estimating the hazard intensity, and for identification and design of appropriate protective measures."
}
@article{CLARK2015466,
title = "Challenges in essential tremor genetics",
journal = "Revue Neurologique",
volume = "171",
number = "6",
pages = "466 - 474",
year = "2015",
issn = "0035-3787",
doi = "https://doi.org/10.1016/j.neurol.2015.02.015",
url = "http://www.sciencedirect.com/science/article/pii/S0035378715007389",
author = "L.N. Clark and E.D. Louis",
keywords = "Essential Tremor, Genetics, Mendelian, Complex Disease, Tremblement essentiel, Génétique",
abstract = "The field of essential tremor (ET) genetics remains extremely challenging. The relative lack of progress in understanding the genetic etiology of ET, however, does not reflect the lack of a genetic contribution, but rather, the presence of substantial phenotypic and genotypic heterogeneity. A meticulous approach to phenotyping is important for genetic research in ET. The only tool for phenotyping is the clinical history and examination. There is currently no ET-specific serum or imaging biomarker or defining neuropathological feature (e.g., a protein aggregate specific to ET) that can be used for phenotyping, and there is considerable clinical overlap with other disorders such as Parkinson's disease (PD) and dystonia. These issues greatly complicate phenotyping; thus, in some studies, as many as 30–50% of cases labeled as “ET” have later been found to carry other diagnoses (e.g., dystonia, PD) rather than ET. A cursory approach to phenotyping (e.g., merely defining ET as an “action tremor”) is likely a major issue in some family studies of ET, and this as well as lack of standardized phenotyping across studies and patient centers is likely to be a major contributor to the relative lack of success of genome wide association studies (GWAS). To dissect the genetic architecture of ET, whole genome sequencing (WGS) in carefully characterized and well-phenotyped discovery and replication datasets of large case-control and familial cohorts will likely be of value. This will allow specific hypotheses about the mode of inheritance and genetic architecture to be tested. There are a number of approaches that still remain unexplored in ET genetics, including the contribution of copy number variants (CNVs), ‘uncommon’ moderate effect alleles, ‘rare’ variant large effect alleles (including Mendelian and complex/polygenic modes of inheritance), de novo and gonadal mosaicism, epigenetic changes and non-coding variation. Using these approaches is likely to yield new ET genes.
Résumé
La génétique du tremblement essentiel reste un défi majeur. Néanmoins, le relatif manque de progrès dans la compréhension des étiologies génétiques du tremblement essentiel ne reflète pas l’absence de contribution de la génétique dans cette pathologie mais plutôt la présence d’une forte hétérogénéité à la fois phénotypique et génotypique. Une approche phénotypique méticuleuse est primordiale pour mettre en évidence les bases génétiques du tremblement essentiel. Le seul outil à disposition pour phénotyper le tremblement essentiel est l’histoire clinique et l’examen neurologique. Il n’y a actuellement pas de biomarqueur sérique ou d’imagerie qui soit spécifique du tremblement essentiel ni de caractéristique neuropathologique (notamment l’agrégation d’une protéine spécifique du tremblement essentiel) qui puisse être utilisée pour le phénotypage. De même, il y a un chevauchement clinique considérable entre le tremblement essentiel et d’autres pathologies telles que la maladie de Parkinson et la dystonie. Ces problèmes compliquent considérablement le phénotypage. Ainsi, dans certaines études, 30 à 50 % des cas considérés comme un tremblement essentiel ont finalement un autre diagnostic qu’il s’agisse d’une dystonie ou d’une maladie de Parkinson. Une approche plus précise du phénotypage, basée notamment sur la définition du tremblement essentiel comme un tremblement d’action, est probablement un point clé dans l’étude de familles atteintes de tremblement essentiel. De même, l’absence de phénotypage standardisé dans les différentes études et centres évaluateurs contribue vraisemblablement à l’échec relatif des études génétiques d’association de type GWAS. Pour mettre en évidence la base génétique du tremblement essentiel, les approches de type séquençage à haut débit du génome portant sur des patients et des familles examinés de façon attentive et bien phénotypée suivies d’une réplication des résultats obtenus dans de larges cohortes de cas–témoins et de familles pourraient être la clé du succès. Cela pourrait permettre de tester des hypothèses particulières concernant le mode de transmission du tremblement essentiel et ses bases génétiques. De multiples approches n’ont pas encore été explorées concernant la génétique du tremblement essentiel incluant la contribution des variations du nombre de copies (CNV), le mosaïcisme gonadique et le mosaïcisme de novo, les allèles peu fréquents d’effet modéré, les allèles rares d’effet marqué (incluant les modes de transmission mendélienne et complexe, voire polygénique), les modifications épigénétiques et les variations de l’ADN non codant. L’usage de ces approches pourrait nous permettre d’identifier enfin des gènes responsables du tremblement essentiel."
}
@article{TORRESFREIRE2013143,
title = "As empresas olham além de seus muros para inovar?",
journal = "RAI Revista de Administração e Inovação",
volume = "10",
number = "3",
pages = "143 - 164",
year = "2013",
issn = "1809-2039",
doi = "https://doi.org/10.5773/rai.v10i3.896",
url = "http://www.sciencedirect.com/science/article/pii/S1809203916302637",
author = "Carlos Torres-Freire and Frederico Henriques",
keywords = "Inovação, Indústria, Difusão de Conhecimento, Interações Sociais, Brasil. Códigos do Journal of Economic Literature: L20, O30, Innovation, Manufacturing, Knowledge Diffusion, Social Interactions, Brazil. Journal of Economic Literature codes: L20, O30",
abstract = "RESUMO
Este artigo trata da influência de atores como universidades, centros de pesquisa, consultorias, fornecedores, clientes e concorrentes em processos de inovação de firmas industriais. Com as transformações na economia nas últimas três décadas, os fluxos de informação e a difusão de conhecimento passaram a ser mais cada vez mais importantes para a qualidade do sistema produtivo e para a competitividade das empresas. Tais atores externos às firmas conformam redes que criam conexões para processos de aprendizado e transferência de conhecimento. A hipótese é que o desempenho inovador de empresas industriais brasileiras tem relação direta com o grau de interação destas com agentes como universidades, centros de pesquisa, fornecedores, clientes e consultorias. A análise de entrevistas com 106 empresários – realizadas na Pesquisa de Atitudes Empresariais para Desenvolvimento e Inovação (PAEDI – CEBRAP/IPEA) – indica uma relação entre atitude inovadora mais forte e grau de interação: empresas com alto grau de interação com atores externos tendem a ser aquelas mais inovadoras.
ABSTRACT
This paper analyses the influence of universities, research institutes, laboratories, consultancies, suppliers and clients on the innovation processes held at firms. These external actors tend to outline networks considered essential for learning and innovation processes. Changes in the systems of production during the last 30 years made information flows and knowledge diffusion stand out as core factors for competitiveness. The main hypothesis of this paper is that the innovative performance of Brazilian manufacturing firms is related to the levels of interaction that companies develop with such external actors. Analysis based on interviews with entrepreneurs and CEOs of 106 Brazilian companies – from the Research of Entrepreneurs Attitudes for Development and Innovation (PAEDI) – show a relationship between innovative attitude and interactional level: firms with higher levels of interaction with “external actors” tend to be the ones who area more innovative."
}
@article{MAXIMUS20161101,
title = "Defining operative mortality: Impact on outcome reporting",
journal = "The Journal of Thoracic and Cardiovascular Surgery",
volume = "151",
number = "4",
pages = "1101 - 1110",
year = "2016",
issn = "0022-5223",
doi = "https://doi.org/10.1016/j.jtcvs.2015.10.062",
url = "http://www.sciencedirect.com/science/article/pii/S002252231502125X",
author = "Steven Maximus and Jeffrey C. Milliken and Beate Danielsen and Junaid Khan and Richard Shemin and Joseph S. Carey",
keywords = "cardiac surgery, operative mortality, outcome reporting, CABG, PCI",
abstract = "Objective
Death is an important outcome of procedural interventions. The death rate, or mortality rate, is subject to variability by definition. The Society of Thoracic Surgeons Adult Cardiac Surgery Database definition of “operative” mortality originally included all in-hospital deaths and deaths occurring within 30 days of the procedure. In recent versions of the Society of Thoracic Surgeons Adult Cardiac Surgery Database, “in-hospital” has been modified to include “patients transferred to other acute care facilities,” and “deaths within 30 days unless clearly unrelated to the procedure” has been changed to “deaths within 30 days regardless of cause.” This study addresses the impact of these redefinitions on outcome reporting.
Methods
The California Office of Statewide Health Planning and Development hospitalized patient discharge database was queried for the year 2009, the most recent year that data files could be linked to the vital statistics death files to include all-cause mortality. Isolated coronary artery bypass grafting, isolated valve, coronary artery bypass grafting valve, and percutaneous coronary intervention procedures were identified by International Classification of Diseases, Ninth Edition, Clinical Modification procedure codes. Percutaneous coronary intervention procedures were further divided into acute coronary syndrome (percutaneous coronary intervention acute coronary syndrome) and all other percutaneous coronary intervention (percutaneous coronary intervention no acute coronary syndrome). Deaths were counted by 5 methods depending on the time and place of occurrence: (1) in-hospital or during the index hospitalization; (2) in-hospital + connected hospitalization, defined as a transfer to another acute care facility on the same day or within 24 hours of discharge; (3) in-hospital + 30 day, death during index hospitalization or within 30 days after the procedure; (4) in-hospital + connected + 30 day readmission, death during index hospitalization, transfer to acute care facility, or deaths during readmission within 30 days; and (5) in-hospital + connected + 30 day. To study the impact of these operative mortality definitions, we examined 5 different methods to track mortality and performed 2 separate analyses. The first analysis did not exclude any patients, and the second analysis excluded any patient who could not be accurately tracked after hospital discharge.
Results
In the first analysis with no patients excluded, a total of 17% (117/697) of surgical deaths and 31% (409/1324) of percutaneous coronary intervention deaths were counted after the original hospitalization. The highest percentage of posthospital deaths occurred after elective percutaneous coronary intervention: 45% (135/301). In surgical patients, the highest percentage of posthospital deaths occurred in coronary artery bypass grafting procedures: 20% (57/284). In the second analysis, with untrackable patients excluded, hospital deaths included 12% (161/1324) for percutaneous coronary intervention compared with 4% (30/697) for surgical procedures.
Conclusions
A significant percentage of procedural deaths occur after transfer or discharge from the index hospital. This is especially evident in the percutaneous coronary intervention group. These findings illustrate the importance of the definition of “operative” mortality and the need to ensure accuracy in the reporting of data to voluntary clinical registries, such as the Society of Thoracic Surgeons Adult Cardiac Surgery Database and National Cardiovascular Data Registry."
}
@article{OKEBE2016348,
title = "The Gametocytocidal Efficacy of Different Single Doses of Primaquine with Dihydroartemisinin-piperaquine in Asymptomatic Parasite Carriers in The Gambia: A Randomized Controlled Trial",
journal = "EBioMedicine",
volume = "13",
pages = "348 - 355",
year = "2016",
issn = "2352-3964",
doi = "https://doi.org/10.1016/j.ebiom.2016.10.032",
url = "http://www.sciencedirect.com/science/article/pii/S2352396416304935",
author = "Joseph Okebe and Teun Bousema and Muna Affara and Gian Luca Di Tanna and Edgard Dabira and Abdoulaye Gaye and Frank Sanya-Isijola and Henry Badji and Simon Correa and Davis Nwakanma and Jean-Pierre Van Geertruyden and Chris Drakeley and Umberto D'Alessandro",
keywords = "Asymptomatic infection, Malaria, Primaquine, Plasmodium falciparum, Infectivity, Gametocyte carriage, Efficacy, Randomized trial",
abstract = "Background
Asymptomatic low-density gametocyte carriers represent the majority of malaria-infected individuals. However, the impact of recommended treatment with single low dose of primaquine and an artemisinin-based combination therapy to reduce transmission in this group is unknown.
Methods
This was a four-arm, open label, randomized controlled trial comparing the effect of dihydroartemisinin-piperaquine (DHAP) alone or combined with single dose of primaquine (PQ) at 0.20mg/kg, 0.40mg/kg, or 0.75mg/kg on Plasmodium falciparum gametocytaemia, infectiousness to mosquitoes and hemoglobin change in asymptomatic, malaria-infected, glucose-6-phosphate dehydrogenase (G6PD) normal individuals. Randomization was done using a computer-generated sequence of uneven block sizes with codes concealed in sequentially numbered opaque envelopes. The primary endpoint was the prevalence of P. falciparum gametocytemia at day 7 of follow-up determined by quantitative nucleic acid sequence based assay and analysis was by intention to treat. The trial has been concluded (registration number: NCT01838902; https://clinicaltrials.gov/ct2/show/NCT01838902).
Results
A total of 694 asymptomatic, malaria-infected individuals were enrolled. Gametocyte prevalence at day 7 was 37.0% (54/146; 95% CI 29.2–45.4), 19.0% (27/142; 95% CI 12.9–26.4), 17.2% (25/145; 95% CI 11.0–23.5) and 10.6% (15/141; 95% CI 6.1–16.9) in the DHAP alone, 0.20mg/kg, 0.40mg/kg, and 0.75mg/kg PQ arms, respectively. The main adverse events reported include headache (130/471, 27.6%), cough (73/471, 15.5%), history of fever (61/471, 13.0%) and abdominal pain (57/471, 12.1%). There were five serious adverse events however, none was related to the interventions.
Interpretation
A single course of PQ significantly reduces gametocyte carriage in malaria-infected asymptomatic, G6PD-normal individuals without increasing the risk of clinical anemia. The limited number of successful mosquito infections suggests that post-treatment transmission potential in this asymptomatic population is low."
}
@article{ARIZA2013372,
title = "Cuidado de enfermería al paciente en postoperatorio temprano de revascularización miocárdica",
journal = "Revista Colombiana de Cardiología",
volume = "20",
number = "6",
pages = "372 - 380",
year = "2013",
issn = "0120-5633",
doi = "https://doi.org/10.1016/S0120-5633(13)70087-1",
url = "http://www.sciencedirect.com/science/article/pii/S0120563313700871",
author = "Claudia Ariza",
keywords = "adultos, cuidado, cirugía de revascularización coronaria, adults, care, coronary bypass surgery",
abstract = "La etapa temprana postoperatoria después de un injerto de bypass de arteria coronaria, sigue siendo una de las fases más críticas para los pacientes que recibieron intervención quirúrgica del corazón. Se pretendió diseñar una propuesta de cuidado de enfermería con base en los problemas que presentan los pacientes en esta fase del proceso de recuperación (48–96 horas), a partir de la descripción e interpretación de los eventos clínicos que requieren cuidados de enfermería (ECRCE) y situaciones que requieren cuidado de enfermería (SRCE). Los ECRCE incluyeron datos cuantitativos en los sistemas neurológico, cardiovascular y respiratorio, y en piel. Estos datos se recolectaron a través de la Hoja de Registro de Información (HRI), historia clínica e información general, diagnósticos y datos relacionados con la cirugía. Por otro lado, los datos cualitativos, que incluyen bienestar, logros, razonamiento, beneficio, complacencia, creencias y valores, sufrimiento, agobio y pesadumbre se investigaron a través de una entrevista semiestructurada a profundidad. Los datos cuantitativos (ECRCE) se analizaron mediante el modelo de Rasch, estadística descriptiva y análisis de correspondencias múltiple y el método de clasificación (cluster analysis). Los datos cualitativos (SRCE) fueron codificados y agrupados en categorías, y luego analizados e interpretados en consecuencia. El análisis mostró que los cambios que se presentan inmediatamente después de la cirugía, permanecen a lo largo de todo el proceso de recuperación. La identificación de éstos permitió la elaboración de una propuesta de cuidado de enfermería a los pacientes durante by pass aorto-coronario (CABG).
Postoperative early stage after grafting of coronary artery bypass, remains one of the most critical phases for the patients undergoing heart surgery. We intended to design a nursing proposal based on the problems presented by patients at this stage of the recovery process (48–96 hours), from the description and interpretation of clinical events requiring nursing care (CERNC) and situations that require nursing care (SRNC). The CERNC included quantitative data on the neurological, cardiovascular and respiratory systems and in the skin. These data were collected through Record Sheet Information (RSI), medical history and general information, diagnostics and surgery-related data. Secondly, qualitative data, which include welfare, achievements, reasoning, benefit, complacency, beliefs and values, suffering, anxiety and grief were investigated through in-depth semi-structured interviews. Quantitative data (CERNC) were analyzed using the Rasch model, descriptive statistics and multiple correspondence analysis and classification method (cluster analysis). Qualitative data (SRNC) were coded and grouped into categories, and then analyzed and interpreted accordingly. The analysis showed that the changes that occur immediately after surgery remain along the whole recovery process. The identification of these led to the drafting of a proposed nursing care to patients during CABG."
}
@article{TANNER20151405,
title = "Management of Pulmonary Nodules by Community Pulmonologists: A Multicenter Observational Study",
journal = "Chest",
volume = "148",
number = "6",
pages = "1405 - 1414",
year = "2015",
issn = "0012-3692",
doi = "https://doi.org/10.1378/chest.15-0630",
url = "http://www.sciencedirect.com/science/article/pii/S0012369215501037",
author = "Nichole T. Tanner and Jyoti Aggarwal and Michael K. Gould and Paul Kearney and Gregory Diette and Anil Vachani and Kenneth C. Fang and Gerard A. Silvestri",
abstract = "BACKGROUND
Pulmonary nodules (PNs) are a common reason for referral to pulmonologists. The majority of data for the evaluation and management of PNs is derived from studies performed in academic medical centers. Little is known about the prevalence and diagnosis of PNs, the use of diagnostic testing, or the management of PNs by community pulmonologists.
METHODS
This multicenter observational record review evaluated 377 patients aged 40 to 89 years referred to 18 geographically diverse community pulmonary practices for intermediate PNs (8-20 mm). Study measures included the prevalence of malignancy, procedure/test use, and nodule pretest probability of malignancy as calculated by two previously validated models. The relationship between calculated pretest probability and management decisions was evaluated.
RESULTS
The prevalence of malignancy was 25% (n = 94). Nearly one-half of the patients (46%, n = 175) had surveillance alone. Biopsy was performed on 125 patients (33.2%). A total of 77 patients (20.4%) underwent surgery, of whom 35% (n = 27) had benign disease. PET scan was used in 141 patients (37%). The false-positive rate for PET scan was 39% (95% CI, 27.1%-52.1%). Pretest probability of malignancy calculations showed that 9.5% (n = 36) were at a low risk, 79.6% (n = 300) were at a moderate risk, and 10.8% (n = 41) were at a high risk of malignancy. The rate of surgical resection was similar among the three groups (17%, 21%, 17%, respectively; P = .69).
CONCLUSIONS
A substantial fraction of intermediate-sized nodules referred to pulmonologists ultimately prove to be lung cancer. Despite advances in imaging and nonsurgical biopsy techniques, invasive sampling of low-risk nodules and surgical resection of benign nodules remain common, suggesting a lack of adherence to guidelines for the management of PNs.
Materials and Methods
This was a multicenter, community-based, retrospective observational study of patients with PNs, ranging from 8 to 20 mm in diameter, presenting to 18 geographically representative outpatient pulmonary clinics across the United States. The study was approved at 15 sites by a central institutional review board and at three sites by local institutional review board approval.
Site Selection
Four hundred forty sites were identified based on investigator databases and claims data from a large insurance carrier whose coverage population was representative of the overall US population. Of these, 77 sites expressed interest in participating, and 48 sites went on to sign confidentiality agreements. Of these, 17 did not request additional information, leaving 31 sites undergoing qualification review. Eighteen outpatient pulmonary clinics were chosen to participate based on the following criteria: (1) management of patients with PNs, (2) availability of medical records, and (3) ability to perform data abstraction. In addition, investigators targeted enrollment of geographically diverse patients to limit the potential bias associated with differences in practice patterns and to account for variation in disease prevalence (eg, endemic mycoses) that could alter management decisions.
Patient Selection
Patients were identified by querying databases (eg, billing and scheduling systems) using five International Classification of Diseases, Ninth Revision, Clinical Modification codes for PN (793.1, 786.6, 518.89, 519.8, 519.9) to ensure homogeneity in patient identification and inclusion.17 Manual chart abstraction was then used to identify those who met the criteria. To minimize selection bias, the sites were not permitted to use additional codes during database query to identify patients. To ensure a systematic sample, patient eligibility was determined by examining consecutively referred patients to the site. Inclusion criteria included age ≥ 40 years and ≤ 89 years at the time of nodule finding, presentation to a pulmonologist, nodule size 8 to 20 mm, and definitive diagnosis ascertained by tissue diagnosis or radiographic follow-up for 2 years. Exclusion criteria included chest CT scan performed > 60 days prior to the initial visit, prior diagnosis of any cancer within 2 years of nodule detection, or incomplete chart data. Patients were categorized into three groups by the most invasive procedure performed during management, as follows: surveillance (serial imaging), biopsy (CT scan-guided transthoracic needle aspiration [TTNA] or bronchoscopy), or surgery (including mediastinoscopy, video-assisted thorascopic surgery, and/or thoracotomy).
Data Collection
Clinical data were abstracted retrospectively by designated study staff into an electronic data capture system from initial consultation through establishment of a definitive diagnosis (ie, pathology results) or a minimum 2-year follow-up. Data included patient demographic and clinical characteristics, PN characteristics, imaging tests, invasive testing, and surgery. PET scan reports were reviewed and abstracted where available for a subset of patients. To adjudicate a PET scan report as positive or negative, an algorithm was developed that prioritized the following components of the report from highest to lowest: final radiology impression, description of findings, and standard uptake values (SUVs) (e-Fig 1). PET scanning was defined as negative if the report included any of the following statements: no evidence of malignancy, no 18F-fluorodeoxyglucose uptake or hypermetabolic activity, or an SUV of > 2.5. A positive PET scan was defined as a report that included any of the following statements: concern or suggestion of malignancy, findings that described increased 18F-fluorodeoxyglucose uptake or hypermetabolic activity, or an SUV ≥ 2.5. In the 27 cases in which the findings were discordant with the final impression, adjudication was performed by two independent pulmonologists and agreement was reached in all cases. Data quality was ensured through ongoing site monitoring. Programmed edit checks were built into the electronic data capture system and at the conclusion of chart abstraction, each site provided access to a random 10% sample of deidentified patient records for review.
Pretest Probability of Malignancy
Two previously developed and validated models9, 10 were used to estimate the pCA in each patient. Model accuracy was determined by comparing the pCA with the final diagnosis. Receiver operating characteristic curves and the area under the curve were generated with 95% CIs. The pCA was calculated for each patient and categorized into three groups (< 5%, 5% to < 65%, and ≥ 65%). Procedure use by group was examined.
Data Analysis
χ2 or analysis of variance tests were used to compare subgroups, and P values > .05 were considered significant. A nodule was classified as benign based on confirmed benign pathology or the absence of radiographic change as determined by the managing physician during surveillance for at least 2 years. Multivariate logistic regression was performed to identify factors associated with the use of an invasive diagnostic procedure. All statistical analyses were performed using SAS/STAT, version 9.3 (SAS Institute inc)."
}
@article{LITTERA201731,
title = "Effect of turbulence intensity on PM emission of heavy duty diesel trucks - Wind tunnel studies",
journal = "Atmospheric Environment",
volume = "162",
pages = "31 - 44",
year = "2017",
issn = "1352-2310",
doi = "https://doi.org/10.1016/j.atmosenv.2017.05.013",
url = "http://www.sciencedirect.com/science/article/pii/S135223101730314X",
author = "D. Littera and A. Cozzolini and M. Besch and D. Carder and M. Gautam",
keywords = "Diesel exhaust emissions, Heavy duty diesel truck, Exhaust aftertreatment, Particulate matter, Wind tunnel, Turbulence intensity",
abstract = "Stringent emission regulations have forced drastic technological improvements in diesel aftertreatment systems, particularly in reducing Particulate Matter (PM) emissions. The formation and evolution of PM from modern engines are more sensitive to overall changes in the dilution process, such as rapidity of mixing, background PM present in the air. These technological advancements were made in controlled laboratory environments compliant with measurement standards (i.e. Code of Federal Regulation CFR in the USA) and are not fully representative of real-world emissions from these engines or vehicles. In light of this, a specifically designed and built wind tunnel by West Virginia University (WVU) is used for the study of the exhaust plume of a heavy-duty diesel vehicle, providing a better insight in the dilution process and the representative nanoparticles emissions in a real-world scenario. The subsonic environmental wind tunnel is capable of accommodating a full-sized heavy-duty truck and generating wind speeds in excess of 50mph. A three-dimensional gantry system allows spanning the test section and sample regions in the plume with accuracy of less than 5 mm. The gantry system is equipped with engine exhaust gas analyzers and PM sizing instruments. The investigation involves three different heavy-duty Class-8 diesel vehicles representative of three emission regulation standards, namely a US-EPA 2007 compliant, a US-EPA 2010 compliant, and a baseline vehicle without any aftertreatment technologies as a pre US-EPA 2007, respectively. The testing procedure includes three different vehicle speeds: idling, 20mph, and 35mph. The vehicles were tested on WVU's medium-duty chassis dynamometer, with the load applied to the truck reflecting the road load equation at the corresponding vehicle test speeds. Wind tunnel wind speed and vehicle speed were maintained in close proximity to one another during the entire test. Results show that the cross-sectional plume area increases with increase in distance away from tailpipe. Also indicating the cooling and dilution of the exhaust begins at close vicinity to the tailpipe. The rate of cooling and dilution are greatest in early stages of the dilution process for the areas with high turbulence intensity (TI), where strong mixing phenomena occurs, leading to the formation of a predominant nucleation mode. On the other hand, the core of the plume observes a slower cooling and dilution rate. This difference is reflected in the PM formation and evolution of these two distinct regions, as shown by the particle size distributions and number concentrations. Continuous mixing will tend to mellow those differences, but its “final” result is related to the dilution history."
}
@article{TAGELDIN2015939,
title = "Adverse reactions among patients being treated for multi-drug resistant tuberculosis at Abbassia Chest Hospital",
journal = "Egyptian Journal of Chest Diseases and Tuberculosis",
volume = "64",
number = "4",
pages = "939 - 952",
year = "2015",
issn = "0422-7638",
doi = "https://doi.org/10.1016/j.ejcdt.2015.03.004",
url = "http://www.sciencedirect.com/science/article/pii/S042276381520115X",
author = "Mohammad A. Tag El Din and Ashraf A. El Maraghy and Abdel Hay R. Abdel Hay",
keywords = "Multi-drug resistant tuberculosis, Abbassia Chest Hospital, Adverse reactions",
abstract = "Background
Pulmonary tuberculosis is a major cause of morbidity and mortality worldwide, resulting in the greatest number of deaths due to any one single infectious agent. Drug resistance threatens global tuberculosis control efforts.
Objective
The aim of this study was to assess adverse reactions of second-line TB drugs in patients treated for MDR-TB at Abbassia Chest Hospital from 1st of January 2009 to 1st of January 2012.
Subjects and methods
This study included 107 patients admitted at Abbassia Chest Hospital; during the period from January 2009 to January 2012. The patients were resistant to at least Rifampicin and INH. All patients’ files were analyzed and the following data were discussed: meticulous history taking, complete clinical examination, drug susceptibility testing, and initial laboratory investigations, adverse reactions were determined by clinical criteria and/or laboratory data, severity code, management of side effects and fate of treatment.
Results
72.9% of the patients were males and 27.1% were females. The mean of age was 37.1years. The special habits detected among the studied cases were tobacco smoking, drug addiction and alcohol intake. According to type of resistance, acquired resistance was 95.3% and primary resistance was 4.7%. The most common co-morbidities associated with MDR-TB in the studied cases were diabetes (29.9%) and chronic obstructive lung disease (11.2%). Side effects of drugs were; 57% GIT manifestations, 53.3% peripheral neuritis, hypokalemia 26.2%, irritable bowel syndrome 22.4%, ototoxicity 17.8%, skin reaction 10.3%, hypothyroidism 10.3%, hepatotoxicity 9.3%, hypoalbuminemia 5.6%, depression 3.7%, arthritis 0.9%, gynecomastia 2.8%, hyponatremia 5.6%, hypomagnesaemia 1.9%, dizziness 0.9%, nephrotoxicity 3.7%. Most of the drugs’ side effects started to appear within the first 3months of treatment. The frequency of nephrotoxicity, hepatotoxicity and hypoalbuminemia were significantly higher in diabetic than in non-diabetic cases. Elevations of liver enzymes began from the 3rd month after treatment and these elevations became statistically significant beginning from the 6th month. Also, elevations of creatinine levels began from the 3rd month after treatment and became statistically significant beginning from the 6th month, while there were no significant changes in potassium levels among the studied cases all through the follow up period. It was noticed that highly significant gain of body weight started from the 3rd month after treatment. 92.5% of the studied cases were cured, 6.5% died and 0.9% was defaulter. The predictors of patients’ outcome were sputum conversion, number of previous TB treatment and associated co-morbidities.
Conclusions
There is a relation between both tobacco smoking and drug addiction, and MDR TB. The most common type of resistance is acquired resistance because of lack of adherence to treatment or inappropriate treatment. The most common co-morbidities associated with MDR TB are diabetes and chronic obstructive lung diseases. The most important predictors of patients’ outcome are sputum conversion, number of previous TB treatment and presence of co-morbidities."
}
@article{BAUDOIN2013335,
title = "L’équipe mobile de soins palliatifs en service de neurologie : présentation de deux groupes interdisciplinaires et pluriprofessionnels fruits d’un projet commun de diffusion de la démarche palliative et d’aide à la réflexion éthique",
journal = "Revue Neurologique",
volume = "169",
number = "4",
pages = "335 - 344",
year = "2013",
issn = "0035-3787",
doi = "https://doi.org/10.1016/j.neurol.2012.10.013",
url = "http://www.sciencedirect.com/science/article/pii/S0035378713000350",
author = "D. Baudoin and S. Krebs",
keywords = "Neurologie, Éthique médicale, Droit des patients, Équipe de soin en soins palliatifs centrée sur le patient, Équipe de santé interdisciplinaire, Neurology, Medical ethics, Patient rights, Patient palliative care team, Interdisciplinary health team",
abstract = "Résumé
Une équipe mobile de soins palliatifs (EMSP) et un service de neurologie apprennent ensemble à faire face aux situations de fin de vie. Après l’historique de la rencontre entre les deux équipes, est décrit le travail clinique de l’EMSP auprès du patient, de sa famille et/ou auprès de l’équipe de neurologie. Au centre des prises en charge neurologiques palliatives complexes se trouvent le groupe ressource soins palliatifs et la réunion de concertation éthique. Ces deux outils interdisciplinaires et pluriprofessionnels sont décrits dans leur fonctionnement comme dans leurs objectifs. Ils créent des moments forts de la vie d’une équipe. Ils fondent sa cohésion, permettent une culture commune, une convergence des représentations individuelles lors des prises de décision et préviennent le burnout. Puis sont détaillées les difficultés d’une décision de pose de gastrostomie chez un homme atteint de paralysie supranucléaire progressive, la souffrance d’une équipe soignante qui prend en charge une femme jeune atteinte de Creutzfeldt-Jakob et pour finir les allers-retours entre vie et mort de Mme H. après un accident vasculaire cérébral. Les découpages théoriques, illustrés de vignettes cliniques, permettent la transmission de pratiques utilisables dans les équipes de neurologie.
This article describes how a mobile team of palliative care and a department of neurology learned to cope with many complex end-of-life situations. After a brief introduction to inter-team cooperation, clinical work of the mobile team with patients and families and its cooperation with the neurology team are presented. The specificity of supportive care in neurology is also analyzed. Two interdisciplinary and multi-professional tools – the Palliative Care Resource Group and the Ethics Consultation Group – are described, with their activities and their goals. The Palliative Care Resource Group is a specific entity whose identity lies at the crossroads between commonly recognized organizational units: clinic staff, clinical practice, ethical or organizational analysis groups (Balint, 1960), discussion groups (Rusznievski, 1999), training groups. It has several objectives: 1) create a robust conceptual environment enabling the pursuit of palliative care practices without relying on the empty paradigm of stereotypical actions; if suffering cannot be avoided, psychic development and transformation can be promoted; 2) attempt to prevent caregiver burnout; 3) help support and strengthen the collective dimension of the team, learning a mode of care which goes beyond the execution of coded actions; 4) enhance the primary dimension of care, i.e. taking care, especially in clinical situations where conventional wisdom declares that “nothing more can be done.”; 5) promote group work so new ideas arising from the different teams influence the behavior of all caregivers. The Ethics Consultation Group organizes its work in several steps. The first step is discernment, clearly identifying the question at hand with the clinical staff. This is followed by a consultation between the clinical team, the patient, the family and the referring physician to arrive at a motivated decision, respecting the competent patient's opinion. The final step is an evaluation of the decision and its consequences. The Ethical Consultation Group, which meets at a scheduled time at a set place, unites the different members of the neurology and palliative care teams who come to a common decision. These specific moments have an important impact on team cohesion, creating a common culture and a convergence of individual representations about making difficult decisions. Specific clinical cases are described to illustrate some of the difficulties encountered in palliative care decision-making. These cases provide insight about the decision to create a palliative care gastrostomy for a man with progressive supranuclear palsy, the suffering experienced by a medical team caring for a young woman with Creutzfeldt-Jacob encephalopathy, or a woman's experience with the post-stroke life-and-death seesaw. Theoretical divisions, illustrated with clinical stories, can be useful touchstones for neurology teams."
}
@article{AUXEMERY2012373,
title = "L’état de stress post-traumatique comme conséquence de l’interaction entre une susceptibilité génétique individuelle, un évènement traumatogène et un contexte social",
journal = "L'Encéphale",
volume = "38",
number = "5",
pages = "373 - 380",
year = "2012",
issn = "0013-7006",
doi = "https://doi.org/10.1016/j.encep.2011.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0013700611002284",
author = "Y. Auxéméry",
keywords = "État de stress post-traumatique, Traumatisme psychique, État de stress aigu, Débriefing, Comorbidités, Interaction gènes environnement, Psychopathologie, Anthropologie, Prévention, Traitement, Posttraumatic stress disorder, Psychic trauma, Acute stress disorder, Debriefing, Comorbidites, Gene-by-environment interaction, Psychopathology, Anthropology, Prevention, Treatment",
abstract = "Résumé
Introduction
Un état de stress post-traumatique (ESPT) ne s’installe jamais par hasard : l’intrication de facteurs de risque intrinsèques (individuels) et extrinsèques (évènement traumatique) témoigne d’un support génétique interactif au trouble. Toute situation dramatique peut être le lieu d’un trauma, non nécessairement, mais en lien avec la manière dont l’individu a investi l’évènement. Parmi les sujets confrontés à la même situation stressante, seuls quelques uns souffriront d’un ESPT. Pour ces derniers, la thématique des répétitions est très différente d’un sujet à un autre, venant témoigner de la singularité de l’évènement vécu pour chacun d’entre eux. Comme témoignage d’une interaction entre l’homme et son environnement, le stress est une réaction biologique aspécifique de l’organisme, mais réaction déclenchée par un ressenti subjectif. L’ESPT en tant que diagnostic causalement attribué s’intègre parfaitement dans le modèle interactif gène×environnement.
Revue de la littérature
Les sujets présentant le génotype S/S codant pour le transporteur de la sérotonine déclenchent un ESPT pour un niveau d’exposition traumatique moindre que leurs homologues L/L. Mais l’interaction entre le génome et son environnement est plus complexe qu’une simple implication : une association de facteurs environnementaux intervient. Considérant la voie dopaminergique, l’allèle A1 codant pour le récepteur dopaminergique de type 2 est associé à une comorbidité sévère de l’ESPT avec présence de troubles somatiques, d’anxiété, d’altération sociale et de dépression. S’intéressant à la neuromodulation noradrénergique, une interaction entre le polymorphisme du gène GABRA2 et la survenue d’un ESPT est décrite tandis qu’une interaction entre le nombre d’évènements traumatiques et le polymorphisme Val(158)Met du gène codant pour la catécholamine-o-méthyltransférase a également été retrouvée. Au niveau neuroendocrinien, le gène codant pour la protéine FKBPR, co-chaperonne de la hsp90 qui lie le récepteur aux glucocorticoïdes, a été étudié selon quatre polymorphismes qui interviennent comme cofacteur en interaction avec l’origine ethnique et les expériences stressantes. Ce polymorphisme mono-nucléotidique interagit avec la sévérité de traumatismes infantiles pour prédire le niveau d’ESPT ultérieur retrouvé à l’âge adulte, ce dernier étant secondaire à un autre évènement de vie traumatisant.
Discussion
Aucune étude neurobiologique n’a pour l’instant décrit de marqueur biologique qui destinerait a priori et immanquablement un sujet à structurer un ESPT en réaction à une situation de stress. Différemment, l’étude psychopathologique retrouve a posteriori que tel sujet a nécessairement construit un syndrome de répétition traumatique en fonction de la concordance de données signifiantes relatives à son histoire. L’évènement vient frapper un refoulement ou une impasse biographique antérieure et dont la thématique interroge les fondamentaux de la culture humaine dans son émancipation d’avec la nature. Une proposition thérapeutique constitue alors par excellence un facteur environnemental lequel peut être ou protecteur ou délétère. La prise en charge aiguë par la technique du débriefing anglophone est discutée alors que la technique francophone est toujours en cours d’évaluation. Bien que ces données restent à confirmer, les benzodiazépines paraissent délétères en post-immédiat alors que l’usage de propranolol serait protecteur. À la phase chronique, la prise en charge pharmacologique n’est pas consensuelle même si les ISRS et les IRSNa sont régulièrement prescrits. S’éloignant de la pharmacopée pour rejoindre une perspective psychothérapique à l’orientation dynamique et interactive, ce qui a fait trauma n’est pas simplement l’évènement stressant en temps que tel, mais sa rencontre avec l’homme qui se trouvait là, prêt à accueillir ce trauma et qui ne s’en détache plus. La réparation du sujet psychotraumatisé ne peut simplement s’établir sur un statut passif de victime stressée, statut qui nuirait à la réflexion et à la reconstruction. Alors que la confrontation à la mort s’apparentait à l’insensé, le sujet interrogera les déterminants psychotraumatiques de son histoire biographique pour y réinscrire son évènement dramatique au sein d’une quête singulière de sens. Une telle restructuration se construit via l’intersubjectivité de la relation clinique, laquelle intervient au sein d’un contexte social. L’ESPT est une pathologie qui interagit avec le contexte sociétal : d’une part, le trauma s’établit via la remise en cause brutale de valeurs sociales qui semblaient immuables et, d’autre part, le concept clinique et nosographique d’ESPT est changeant avec l’évolution des sociétés.
Conclusion
Un ESPT ne survient jamais par hasard, les conditions de possibilité du trauma sont établies par des déterminants génétiques et psychologiques s’intégrant de façon interactive au cœur d’un contexte social. Après l’inflation d’un intérêt psychotraumatique dans les publications internationales depuis les années 1980, une lutte contre la survictimisation semble désormais s’installer. L’évolution des techniques de génétique et de neuro-imagerie est en cours de supplanter les études psychométriques en termes de fiabilité et de validité : peut-être faut-il voir dans cette évolution sociale les changements de demain concernant la clinique de l’ESPT et son traitement.
Summary
Introduction
Why are some individuals more likely than others to develop a posttraumatic stress disorder (PTSD) in the face of similar levels of trauma exposure? Monitoring the traumatic process combining the antecedents, the determinants of the psychic trauma and the acute symptoms can clarify the causes of the final onset of a chronic repetition syndrome. Epidemiologic research has clarified risk factors that increase the likelihood of PTSD after exposure to a potentially traumatic event. PTSD is an interaction between a subject, a traumatogenic factor and a social context. With each epidemiological, psychopathological and more particularly neurogenetic study, we will expand on the impact of these interactions on the therapeutic treatment of psycho-traumatised persons.
Literature findings
Most studies have shown that unrelated to the traumatic event, additional risk factors for developing PTSD include younger age at the time of the trauma, female gender, lower social economic statuts, lack of social support, premorbid personality characteristics and preexisting anxiety or depressive disorders increase the risk of PTSD. The psychic trauma is firmly attached to the repetition and the previous traumas are as many risks of developing a subsequent PTSD in the wake of a new trauma: PTSD in adults may represent a prolonged symptomatic reaction to prior traumatic assault, child abuse and childhood adversities. Related to the traumatic event, the organic pain, the traumatic brain injury, but also the sight of blood can lead to a trauma being considered as more serious or more harmful to life. It is useful to recognize the acute reactions of exhaustion stress as they can guide both the pharmacotherapeutic and the psychotherapeutic treatment thanks to debriefings. Even though the majority of people with acute stress disorder subsequently develop PTSD, the current data indicate that too many people can develop PTSD without initially displaying acute stress disorder. Though peritraumatic dissociation and peritraumatic distress have emerged as the strongest predictors for PTSD and have to be treated as soon as possible with the debriefing or the pharmacology; initial evidence suggests the potential benefits of early intervention, shortly after the trauma, and psychological debriefing has received increasing interest from the scientific community. However the Anglo-Saxon techniques (such as Critical Incident Stress Debriefing also known as the Mitchell model) are in total contrast with the French approach. In the first case the emotional response is controlled to ensure the pursuit of the group action, whilst in the second case the debriefing concerns patients with acute symptoms in order to prevent the development of a PTSD structuring of the latter. The facts, emotions and thoughts are not partitioned but inter-linked, thus enabling a fragmentation of the traumatic experience. In the face of the annihilation experienced, speech production by the subject is restored linking the person to the human community, once abandoned. However, debate continues on the efficacy of single session debriefing in the prevention of PTSD. At the time of the acute stress reactions, benzodiazepines are contraindicated at this stage as they promote dissociation and ulterior revivals. On the other hand, treatment with propranolol could be proposed: a two or three week course of propranolol begun in the aftermath of a traumatic event can reduce subsequent PTSD symptoms.
Discussion
A genetic polymorphism is evidently at work in the development of a PTSD via the regulation of the expression of genes of interest to the serotoninergic system and the adrenocorticotropic axis. The 5-HTTLPR (promoter region of SLC6A4 witch encodes the serotonin transporter) constitutes a genetic candidate region that may modulate emotional responses to traumatic events. The interaction between variation at the 5HTTLPR and stressful life events could predict depression and PTSD. Considering the dopaminergic pathway, the A1 allele coding the type 2 dopaminergic receptor is associated with a severe comorbidity of PTSD with the presence of somatic disorders, anxiety, social change and depression. For noradrenergic neuromodulation, an interaction between the polymorphism of gene GABRA2 and the occurrence of PTSD is described whereas an interaction between the number of traumatic events and Val(158)Met polymorphism of the gene coding for catecholamine-o-methyltransferase has also been found. The role of polymorphisms in FKBP5 (a co-chaperone of hsp 90 which binds to the glucocorticoid receptor) in predicting PTSD too, with a gene-by-environment point of view. These gene-by-environment studies are needed to focus more on distinct endophenotypes and influences from environmental factors. If several candidate genes are involved, a weighting of susceptibility to such and such a neurological regulation system will imply various endophenotypes. According to the monoamine predominantly incriminated, PTSD can take on a more hyper-vegetative clinical expression linked with noradrenergic overuse. Differently, avoidance behaviour and the depressive aspect invoke more a modification of the serotoninergic modulation whilst posttraumatic psychotic reactions question the role of dopaminergic pathways. Neuroscientific discoveries interesting the biological support of PTSD can thus modify our view of the conception of the disorder in relation to different therapeutic prospects.
Conclusion
Chronic PTSD can manifest itself in different clinical forms. The repetition syndrome can appear a long time after the traumatic event, following a paucisymptomatic latency period, which can last several years or even decades. The absence of complaints from the patient is common, the latter suffering in silence. Often other comorbid disorders and other complaints arise sooner than the clinical picture. Thus a depressive episode characterised as drug-seeking behaviour is frequently encountered. The therapeutic accompaniment traditionally combines a pharmacological and a psychotherapeutic treatment even if recommendations are rare. A posttraumatic stress disorder is never just a coincidence. The different stages of the evolution and the establishment of a PTSD are the expression of an interaction between the outside and the inner self. Despite a known progression of the posttraumatic stress disorder, this deleterious evolution is far from being a foregone conclusion. On the contrary, several levels of prevention are possible at each stage of its structuration to propose treatments to subjects who are vulnerable and/or present symptoms. No neurobiological study has yet found a biological marker, which would apparently and inevitably destine a subject to structure, a posttraumatic stress disorder in reaction to a stress. Conversely, the psychopathological study finds afterwards that a particular subject has necessarily built a traumatic repetition syndrome according to the concordance of significant data relative to his/her history. The event strikes a repression or an anterior biographical deadlock and of which the thematic questions the fundamentals of human culture in its emancipation with nature, like the question of death and its consequences: bereavement, parentality, transgenerational transmission and organicity often linked to the illness. A therapeutic proposal constitutes an environmental factor par excellence which can be either protective or deleterious. If the traumatic repetition syndrome has been known since Antiquity, the birth of PTSD has followed the chronology of the DSM according to the sociopolitical contexts encountered. A PTSD does not occur by chance: the conditions of possibility of the trauma are established by genetic and psychological determinants interactively integrated at the heart of a social context. After the increase in a psychotraumatic interest in international publications since the 1980s, a fight against over-victimisation seems to be setting in. The advances in genetic and neuroimaging techniques are in the process of superseding psychometric studies in terms of reliability and validity; maybe we should see in this social evolution the changes of tomorrow concerning the clinical of PTSD and its treatment. The healing of the psycho-traumatised subject cannot just be established on the passive status of victim, which would be detrimental to reflection and ultimately reconstruction: the rebirth of the subject will require active commitment, which could distract from the deadly repetition. Whilst the confrontation with death resembled nonsense, the subject will question the psychotraumatic determinants of his/her life history to reinstate this tragic event within a search for meaning. Such restructuring is built on the intersubjectivity of the clinical relationship, which occurs within a social context. PTSD is a pathology which interacts with the societal context: on the one hand the trauma is established on the brutal reconsideration of social values which seem immutable and on the other hand, the clinical and nosographical concept of PTSD is changing with the evolution of society."
}
@article{PERLETH2014192,
title = "Die Gesundheitsuntersuchung: Vom Gesetz zur Richtlinie des Gemeinsamen Bundesausschusses (G-BA)",
journal = "Zeitschrift für Evidenz, Fortbildung und Qualität im Gesundheitswesen",
volume = "108",
number = "4",
pages = "192 - 195",
year = "2014",
note = "Ist die Gesundheitsuntersuchung nach §25 SGB V noch zeitgemäß?",
issn = "1865-9217",
doi = "https://doi.org/10.1016/j.zefq.2014.02.002",
url = "http://www.sciencedirect.com/science/article/pii/S1865921714000336",
author = "Matthias Perleth and Katja Matthias",
keywords = "Gesundheitsuntersuchung, Sekundärprävention, Gemeinsamer Bundesausschuss, Richtlinie, Sozialgesetzbuch V, Periodic health examination, secondary prevention, German Federal Joint Committee, directive, Social Code Book V",
abstract = "Zusammenfassung
Seit 1989 gibt es in Deutschland die Gesundheitsuntersuchung ab dem Alter von 35 Jahren zur Früherkennung der „Volkskrankheiten“ (insbesondere Herz-Kreislauf- und Nierenerkrankungen sowie Diabetes) mittels Anamnese, körperlicher Untersuchung, Blut- und Urintests sowie Beratung. Die entsprechende Richtlinie des Gemeinsamen Bundesausschusses (G-BA) wurde insgesamt sechsmal geändert, allerdings erfolgte nur einmal eine inhaltliche Änderung (1999 Streichung von Harnsäure, Kreatinin und Ruhe-EKG). Mehrmals wurden aber auch mögliche weitere Untersuchungen nicht in die Richtlinie aufgenommen, nachdem die Bewertung keinen Nutzen zeigen konnte (z.B. Glaukomscreening). Mitte der 1990er Jahre erfolgten mehrere Evaluationen, die zeigten, dass durchaus bisher nicht bekannte Diagnosen gestellt und Maßnahmen wie Ernährungsberatung eingeleitet wurden. Einen Nutzen im Sinne von vermiedenen unerwünschten Ereignissen (wie bspw. Herzinfarkte) konnten die Evaluationen aus methodischen Gründen aber nicht belegen. Kritik an der Gesundheitsuntersuchung ist nicht neu, insbesondere der fehlende Nutzenbeleg der in der Richtlinie geregelten Maßnahmen wird angemahnt. Ein Gesetzentwurf der letzten Bundesregierung mit einem Änderungsvorschlag für die Gesundheitsuntersuchung wurde allerdings im Bundesrat abgelehnt.
Summary
Since 1989 a periodic health examination beginning at the age of 35 for the early detection of “common diseases” (especially cardiovascular and kidney diseases as well as diabetes) by means of history-taking, physical examination, blood and urine tests and counselling has been available in Germany. Altogether, the respective directive of the Federal Joint Committee (G-BA) was revised six times, but a substantive change took place only once (i. e., the cancellation of uric acid, creatinine, and resting ECG in 1999). However, additional examinations (e.g., glaucoma screening) were not added to the health check after systematic assessments of the evidence were completed. In the mid-1990s, several evaluations were performed which showed that new diagnoses were established in a significant proportion of patients, and measures were initiated such as nutrition counselling. A patient-relevant benefit in terms of avoided adverse events (such as heart attacks) could, however, not be demonstrated due to methodological reasons. Criticism of the health examination is not new, in particular concerning the lack of evidence of benefit for the diagnostic procedures of the health examination. A draft law issued by the former Federal Government proposing an amendment to the health examination has recently been rejected in the Bundesrat (upper house of the German parliament)."
}
@article{PROCTOR2012722,
title = "Compatibility of current DSM-IV and proposed DSM-5 diagnostic criteria for cocaine use disorders",
journal = "Addictive Behaviors",
volume = "37",
number = "6",
pages = "722 - 728",
year = "2012",
issn = "0306-4603",
doi = "https://doi.org/10.1016/j.addbeh.2012.02.010",
url = "http://www.sciencedirect.com/science/article/pii/S0306460312000639",
author = "Steven L. Proctor and Albert M. Kopak and Norman G. Hoffmann",
keywords = "DSM-5, Cocaine, Cocaine use disorders, State prison, Inmates",
abstract = "Objectives
The present study examined the compatibility of the current DSM-IV and proposed DSM-5 diagnostic criteria for cocaine use disorders (CUD) among state prison inmates, and evaluated the diagnostic utility of the proposed criteria in accounting for DSM-IV “diagnostic orphans” (i.e., individuals who meet one or two of the diagnostic criteria for substance dependence yet fail to report indications of substance abuse).
Method
Data were derived from routine clinical assessments of adult male inmates (N=6871) recently admitted to the Minnesota Department of Corrections state prison system from 2000 to 2003. An automated (i.e., computer-prompted) version of the Substance Use Disorder Diagnostic Schedule-IV (SUDDS-IV; Hoffmann & Harrison, 1995) was administered to all inmates as part of routine assessments. DSM-IV and DSM-5 criteria were coded using proposed guidelines.
Results
The past 12-month prevalence of DSM-IV CUDs was 12.7% (Abuse, 3.8%, Dependence, 8.9%), while 11.0% met past 12-month DSM-5 criteria for a CUD (Moderate [MCUD], 1.7%; Severe [SCUD], 9.3%). When DSM-5 criteria were applied, 11.8% of the DSM-IV diagnostic orphans received a MCUD diagnosis. The vast majority of those with no diagnosis (99.6%) continued to have no diagnosis, and a similar proportion who met dependence criteria (98.4%) met SCUD criteria of the proposed DSM-5. Most of the variation in diagnostic classifications was accounted for by those with a current abuse diagnosis.
Conclusions
The proposed DSM-5 criteria perform similarly to DSM-IV criteria in terms of the observed past 12-month CUD prevalence and diagnostic classifications. The proposed criteria appear to account for diagnostic orphans that may warrant a diagnosis. DSM-IV abuse cases were most affected when DSM-5 criteria were applied. Additional criteria, beyond those included in the proposed DSM-5 changes, concerning use to relieve emotional stress and preoccupation with use were frequently endorsed by those with a proposed DSM-5 diagnosis."
}
@article{ALONSOAREVALO201451,
title = "Uso y aplicación de herramientas 2.0 en los servicios, producción, organización y difusión de la información en la biblioteca universitaria",
journal = "Investigación Bibliotecológica: Archivonomía, Bibliotecología e Información",
volume = "28",
number = "64",
pages = "51 - 74",
year = "2014",
issn = "0187-358X",
doi = "https://doi.org/10.1016/S0187-358X(14)70909-8",
url = "http://www.sciencedirect.com/science/article/pii/S0187358X14709098",
author = "Julio Alonso Arévalo and José Antonio Cordón García and Raquel Gómez Díaz and Belén García-Delgado Giménez",
keywords = "España, Biblioteca 2.0, Bibliotecas académicas, Cambio sociotécnico, , Código abierto, Identidad digital, Spain, Library 2.0, Academic Libraries, Sociotechnical Change, Freeware, Open Code, Digital Identity",
abstract = "Resumen
El presente estudio tiene como objetivo presentar un análisis sobre el uso de herramientas 2.0 en bibliotecas universitarias, con el fin de entender los patrones de uso como una forma de proporcionar información a los usuarios y mejorar la visibilidad de la entidad a través de su marca digital. El análisis, basado en la experiencia profesional, muestra cómo la integración de diferentes herramientas 2.0 permite potenciar el servicio de información que ofrece la biblioteca universitaria al tiempo que favorece la creación de comunidades de aprendizaje y fomenta los mecanismos de comunicación entre los usuarios. La integración de los distintos tipos de herramientas de la web social permite articular un sistema de información, donde gracias a sistemas como los canales RSS (Really Simple Syndication) se facilita la recopilación de información, la cual es organizada y normalizada con los gestores de referencias sociales y difundida entre los usuarios a través de los blogs, redes sociales y listas de distribución, lo que permite darle una mayor visibilidad a la institución, pero sobre todo ofrecer mejores servicios a los usuarios. El valor del artículo radica en que la experiencia de un centro concreto permite su réplica y/o adaptación a otros centros en los que se podrán implantar servicios similares, puesto que los servicios proporcionados se basan en herramientas de la web 2.0, freeware o de código abierto.
The object of this study is to present an analysis on the use of 2.0 tools in university libraries, with the goal of grasping usage patterns of the information provided to users, while enhancing the visibility of the entity’s digital brand. Based on professional experience, the analysis shows how the integration of diverse 2.0 tools can improve the information services offered by the university library, while improving communication mechanisms among users. This encourages the creation of learning communities. The integration of diverse social networking tools allows for the articulation of an information system, in which, thanks to RSS feeds channels, information is organized and standardized by social reference managers and subsequently disseminated among users through blogs, social networks, distribution lists; thereby enhancing the institution’s visibility and, above all, providing better services to users. This article provides a valuable portrait of an information center, using 2.0 web tools, freeware and open code application; whose experience can be replicated and/or adapted in other centers, where similar services are offered"
}
@article{HU2013448,
title = "An overview of medical informatics education in China",
journal = "International Journal of Medical Informatics",
volume = "82",
number = "5",
pages = "448 - 466",
year = "2013",
issn = "1386-5056",
doi = "https://doi.org/10.1016/j.ijmedinf.2012.04.011",
url = "http://www.sciencedirect.com/science/article/pii/S138650561200086X",
author = "Dehua Hu and Zhenling Sun and Houqing Li",
keywords = "Medical informatics, Education, China, Undergraduate education, Graduate education, Doctoral education, Main courses, Specialty",
abstract = "Objective
To outline the history of medical informatics education in the People's Republic of China, systematically analyze the current status of medical informatics education at different academic levels (bachelor's, master's, and doctoral), and suggest reasonable strategies for the further development of the field in China.
Method
The development of medical informatics education was divided into three stages, defined by changes in the specialty's name. Systematic searches of websites for material related to the specialty of medical informatics were then conducted. For undergraduate education, the websites surveyed included the website of the Ministry of Education of the People's Republic of China (MOE) and those of universities or colleges identified using the baidu.com search engine. For postgraduate education, the websites included China's Graduate Admissions Information Network (CGAIN) and the websites of the universities or their schools or faculties. Specialties were selected on the basis of three criteria: (1) for undergraduate education, the name of specialty or program was medical informatics or medical information or information management and information system; for postgraduate education, medical informatics or medical information; (2) the specialty was approved and listed by the MOE; (3) the specialty was set up by a medical college or medical university, or a school of medicine of a comprehensive university. The information abstracted from the websites included the year of program approval and listing, the university/college, discipline catalog, discipline, specialty, specialty code, objectives, and main courses.
Results and conclusions
A total of 55 program offerings for undergraduate education, 27 for master's-level education, and 5 for PhD-level education in medical informatics were identified and assessed in China. The results indicate that medical informatics education, a specialty rooted in medical library and information science education in China, has grown significantly in that country over the past 10 years. Frequent changes in the specialty's name and an unclear identity have hampered the visibility of this educational specialty and impeded its development. There is a noticeable imbalance in the distribution of degree programs in medical informatics in different disciplines, with the majority falling under information management. There is also an uneven distribution of the specialty settings of medical informatics at the various academic levels (bachelor's, master's, and doctoral). In addition, the objectives and curriculum design of medical informatics education differ from one university to another and also from those of foreign universities or colleges. It is recommended that China (1) treat medical informatics as a priority “must-have” discipline to build in China, (2) establish its own independent, balanced degree programs, (3) set up a specialty of “medical informatics” under the “medicine” category, (4) explore curriculum integration with international medical informatics education, and (5) establish and improve medical informatics education system."
}
@article{FUJINAMI2015326,
title = "Clinical and Molecular Characteristics of Childhood-Onset Stargardt Disease",
journal = "Ophthalmology",
volume = "122",
number = "2",
pages = "326 - 334",
year = "2015",
issn = "0161-6420",
doi = "https://doi.org/10.1016/j.ophtha.2014.08.012",
url = "http://www.sciencedirect.com/science/article/pii/S0161642014007349",
author = "Kaoru Fujinami and Jana Zernant and Ravinder K. Chana and Genevieve A. Wright and Kazushige Tsunoda and Yoko Ozawa and Kazuo Tsubota and Anthony G. Robson and Graham E. Holder and Rando Allikmets and Michel Michaelides and Anthony T. Moore",
abstract = "Purpose
To describe the clinical and molecular characteristics of patients with childhood-onset Stargardt disease (STGD).
Design
Retrospective case series.
Participants
Forty-two patients who were diagnosed with STGD in childhood at a single institution between January 2001 and January 2012.
Methods
A detailed history and a comprehensive ophthalmic examination were undertaken, including color fundus photography, autofluorescence imaging, spectral-domain optical coherence tomography (SD-OCT), and pattern and full-field electroretinograms. The entire coding region and splice sites of ABCA4 were screened using a next-generation, sequencing-based strategy. The molecular genetic findings of childhood-onset STGD patients were compared with those of adult-onset patients.
Main Outcome Measures
Clinical, imaging, electrophysiologic, and molecular genetic findings.
Results
The median ages of onset and the median age at baseline examination were 8.5 (range, 3–16) and 12.0 years (range, 7-16), respectively. The median baseline logarithm of the minimum angle of resolution visual acuity was 0.74. At baseline, 26 of 39 patients (67%) with available photographs had macular atrophy with macular/peripheral flecks; 11 (28%) had macular atrophy without flecks; 1 (2.5%) had numerous flecks without macular atrophy; and 1 (2.5%) had a normal fundus appearance. Flecks were not identified at baseline in 12 patients (31%). SD-OCT detected foveal outer retinal disruption in all 21 patients with available images. Electrophysiologic assessment demonstrated retinal dysfunction confined to the macula in 9 patients (36%), macular and generalized cone dysfunction in 1 subject (4%), and macular and generalized cone and rod dysfunction in 15 individuals (60%). At least 1 disease-causing ABCA4 variant was identified in 38 patients (90%), including 13 novel variants; ≥2 variants were identified in 34 patients (81%). Patients with childhood-onset STGD more frequently harbored 2 deleterious variants (18% vs 5%) compared with patients with adult-onset STGD.
Conclusions
Childhood-onset STGD is associated with severe visual loss, early morphologic changes, and often generalized retinal dysfunction, despite often having less severe fundus abnormalities on examination. One third of children do not have flecks at presentation. The relatively high proportion of deleterious ABCA4 variants supports the hypothesis that earlier onset disease is often owing to more severe variants in ABCA4 than those found in adult-onset disease."
}
@article{CARRIERI2012570,
title = "Modeling and experimental validation of mass transfer from carbonated beverages in polyethylene terephthalate bottles",
journal = "Journal of Food Engineering",
volume = "108",
number = "4",
pages = "570 - 578",
year = "2012",
issn = "0260-8774",
doi = "https://doi.org/10.1016/j.jfoodeng.2011.09.001",
url = "http://www.sciencedirect.com/science/article/pii/S0260877411004717",
author = "Gabriella Carrieri and Maria Valeria De Bonis and Gianpaolo Ruocco",
keywords = "Mass transfer, Beverage packaging, Computational fluid dynamics",
abstract = "Mass transfer and related shelf life assessment is an important issue in the beverage industry. Product change due to mass transfer is at stake and, with it, its consumer value and consideration. Carbonation loss takes place at the product/package interface, and to the environment through the package itself. In this paper a joint experimental/computational approach has been exploited: the CO2 loss through the polyethylene terephthalate barrier has been computed by means of a multidimensional finite element code, while actual measurements have been carried out to validate the computations. Residual carbonation histories are validated and presented for a variety of thermal regimes and for two different bottles carrying the same capacity. The paper highlights on the combination of bottle weight, initial carbonation and storage temperature, indicating the operational set for the longest shelf life within the explored cases. Lighter bottles can be used with no inference on shelf life, while the carbonic loss depends non-linearly on initial carbonation and temperature increment. The associated concentration maps help infer on the importance of polyethylene terephthalate thickness uniformity. It is then demonstrated that the model carries the flexibility of a general tool, applicable to any carbonated beverage at any storage condition."
}
@article{CASPERSON20121029,
title = "IBAR: Interacting boson model calculations for large system sizes",
journal = "Computer Physics Communications",
volume = "183",
number = "4",
pages = "1029 - 1035",
year = "2012",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2011.12.024",
url = "http://www.sciencedirect.com/science/article/pii/S0010465511004127",
author = "R.J. Casperson",
keywords = "Interacting boson model, Quantum phase transitions, Reduced matrix elements, Nuclear structure",
abstract = "Scaling the system size of the interacting boson model-1 (IBM-1) into the realm of hundreds of bosons has many interesting applications in the field of nuclear structure, most notably quantum phase transitions in nuclei. We introduce IBAR, a new software package for calculating the eigenvalues and eigenvectors of the IBM-1 Hamiltonian, for large numbers of bosons. Energies and wavefunctions of the nuclear states, as well as transition strengths between them, are calculated using these values. Numerical errors in the recursive calculation of reduced matrix elements of the d-boson creation operator are reduced by using an arbitrary precision mathematical library. This software has been tested for up to 1000 bosons using comparisons to analytic expressions. Comparisons have also been made to the code PHINT for smaller system sizes.
Program summary
Program title: IBAR Catalogue identifier: AELI_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AELI_v1_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: GNU General Public License version 3 No. of lines in distributed program, including test data, etc.: 28 734 No. of bytes in distributed program, including test data, etc.: 4 104 467 Distribution format: tar.gz Programming language: C++ Computer: Any computer system with a C++ compiler Operating system: Tested under Linux RAM: 150 MB for 1000 boson calculations with angular momenta of up to L=4 Classification: 17.18, 17.20 External routines: ARPACK (http://www.caam.rice.edu/software/ARPACK/) Nature of problem: Construction and diagonalization of large Hamiltonian matrices, using reduced matrix elements of the d-boson creation operator. Solution method: Reduced matrix elements of the d-boson creation operator have been stored in data files at machine precision, after being recursively calculated with higher than machine precision. The Hamiltonian matrix is calculated and diagonalized, and the requested transition strengths are calculated using the eigenvectors. Restrictions: The 1000 boson coefficients for L=0 and L=20 have been included in the IBAR distribution and the 7.3 GB of data that make up the remaining coefficients for L=21 to L=2000 are available upon request. Running time: If the provided example is changed to include 100 bosons, the calculation requires about 1 second of run time. For 1000 bosons, the calculation requires about 9 minutes of run time."
}
@article{MURTHY2012113,
title = "Effect of the New Standards for Case Logging on Resident Operative Volume: Doing Better Cases or Better Numbers?",
journal = "Journal of Surgical Education",
volume = "69",
number = "1",
pages = "113 - 117",
year = "2012",
issn = "1931-7204",
doi = "https://doi.org/10.1016/j.jsurg.2011.10.010",
url = "http://www.sciencedirect.com/science/article/pii/S193172041100300X",
author = "Raghav Murthy and Alex Shepard and Andrew Swartz and Ann Woodward and Craig Reickert and Mathilda Horst and Ilan Rubinfeld",
keywords = "surgery, surgical education, graduate medical education, case logs, resident case logs, competency, Patient Care, Practice Based Learning and Improvement, System Based Practice",
abstract = "Objectives
The Accreditation Council for Graduate Medical Education (ACGME) modified the designation of major (index) operative cases to include those previously considered “minor.” This study assessed the potential effect of these changes on resident operative experience.
Methods
With Institutional Review Board approval, we analyzed National Surgical Quality Improvement Program participant use files for 2005–2008 for general and vascular surgery cases. Primary CPT case coding was mapped to the ACGME major case category using both the old and new classification schemes. The variables were analyzed using χ2 analysis in SPSS IBM 19 (IBM, Armonk, New York).
Results
A total of 576,019 cases were reviewed. Major cases as defined by the new classification represented an increasing proportion of the cases each year, rising from 88.3% in 2005 to 95% by 2008 (p < 0.001). Major cases as defined by the old scheme decreased from 71% in 2005 to 62% by 2008 (p < 0.001). The cases covered by a resident dropped from 82% in 2005 to 61% in 2008 (p < 0.001). When comparing the new to the old scheme, 364,366 (63.3%) cases were considered major and 30,587 (5.3%) were minor by both standards; 7089 (1.2%) cases previously classified as major were changed to minor, whereas 173,977 (30.2%) (p < 0.001) previously classified as minor were now major. This latter group showed top procedures to include excision of breast lesion (22,175 [12.7%]), laparoscopic gastric bypass (18,825 [10.8%]), ventral hernia repair (14,732 [8.5%]), and appendectomy (10,190 [5.9%]). Of these newly designated major cases, the proportion not covered by residents increased from 22% in 2005 to 44% in 2007 and 2008 (p < 0.001).
Conclusions
Although some operative cases newly classified as major are technically advanced procedures (eg, Roux-en-Y gastric bypass), other cases are not (eg, breast lesion excision), which raises the issue as to whether the major case category has been diluted by less demanding case types. The implications of these findings may suggest preservation of case volumes at the expense of case quality."
}
@article{MIZUNO2014606,
title = "La théorie du néo-organodynamisme : une nouvelle classification pratique des maladies mentales",
journal = "Annales Médico-psychologiques, revue psychiatrique",
volume = "172",
number = "8",
pages = "606 - 614",
year = "2014",
issn = "0003-4487",
doi = "https://doi.org/10.1016/j.amp.2012.04.004",
url = "http://www.sciencedirect.com/science/article/pii/S0003448712002569",
author = "Tatsuo Mizuno",
keywords = "Blonansérine, Classification des maladies mentales, Néo-organodynamisme, Organodynamisme, Voie accessoire, Accessory pathway, Blonanserin, Classification of mental illness, Neo-organodynamism, Organodynamism",
abstract = "Résumé
La classification traditionnelle des maladies mentales et celle d’Henri Ey y sont discutées et un néo-organodynamisme est présenté. La phase aiguë d’une maladie mentale n’est sans doute qu’une modification temporaire de la maladie de base. La nature de cette dernière peut se décrire par l’importance de la dégénérescence organique. Alors qu’un état psychotique constitue le trouble de base, une phase agrégative aiguë se manifesterait en proportion du degré d’excitations anormales du réseau neuronal. En s’appuyant sur une pluralité d’états mentaux, il est possible de représenter avec précision l’état des patients psychiatriques, qui varie de façon néo-organodynamique avec l’évolution. De la même façon, grâce à la classification proposée, il devient possible de procéder à une recherche clinique comparative des effets des médicaments et des modifications biochimiques. Le concept de « voie accessoire » est introduit. Les contraintes et tensions mentales, ainsi que les actes rituels, promeuvent une construction et une réorganisation du réseau neuronal, qui se fige sous l’effet des répétitions. L’encodage de ce nouveau réseau neuronal dans le subconscient crée une dérivation qui finit par se fixer en tant que « voie accessoire », à l’instar du faisceau de Kent dans le syndrome de Wolff-Parkinson-White. Lorsque la « voie sous-conductrice » est dominante, le patient ne peut plus prendre en considération quoi que ce soit, y compris lui-même. In fine, l’histoire du psychisme est celle d’une hiérarchisation mentale, construite par des voies accessoires et l’encodage dans le subconscient des états mentaux associés.
Background
Dogmatically conflicting psychological and biological theories make the foundations of psychiatry fragile.
Methods
Traditional and Henri Ey's classifications of mental illness are discussed and the neo-organodynamism is proposed. The principle of the accessory pathway is also proposed.
Results
The acute phase of disease may only be a temporary modification of the basic mental illness, the nature of the latter can be expressed in terms of the depth of the organic degeneration. The psychotic state consists of the basic disorder and its acute aggregative phase that is in proportion to the degree of abnormal excitations of neural network. In my classification, mental illness is classified from a normal stage N to X. The degree of acute aggravation is staged from 0 to 7. The twelve stages of organic degeneration multiplied by the eight stages of acute aggravation results in a product of 96 mental stages. By applying this, we can accurately represent the conditions of psychiatric patients that change neo-organodynamically over time. At the same time, clinical comparative research, such as the effects of medicine and of biochemical changes at one time becomes possible. The mental automatism, the mechanism of acute psychosis and treatment concept are discussed with this theory. And this theory may explain what the mental illness is, what the delusion is and what the endogenous psychosis is. Mental pressures or ritual acts promote construction and the reorganization of neural network, and it is fixed by repeating itself. Coding to the subconsciousness of a new neural network namely bypass will be made, and it is immobilized as an accessory pathway like the bundle of Kent of the Wolff–Parkinson–White syndrome. This is the principle of the accessory pathway. When the accessory pathway is dominant, the patient cannot consider anything like himself. This condition shows the low level of his psychic energy (psychasthénie). The history of a mind hierarchy that is constructed with the accessory pathways may be a history of mind. And the history of a mind hierarchy should be what the psychic body is.
Treatment concept
From the viewpoint of the neo-organodynamism, the conception of organic degeneration is consisted of 1) tissue degeneration literally and 2) generation of accessory pathway by the reorganization of the synapse. The symptoms may be irreversible in the case of the tissue degenerations. But the symptoms that are induced by the accessory pathway may be reversible. If the conductivity of the main pathway becomes dominant as before, the symptom by the accessory pathway has a possibility of the improvement. But the accessory pathway will be remained even if the conductivity of the main pathway becomes dominant as before. One's usual personality is defined as a predominant state of the main pathway. Healing is to return to one's usual personality from the accessory pathway predominant state. In the organic degenerations that are the essence of mental illness, rehabilitation (psychotherapy, cognitive behavioral therapy, mental counseling, etc.) is essential to recuperate lost functions. For the positive symptoms, which are the secondary effects of organic degenerations, medication that focuses on the synaptic neurotransmitters will be effective. Theoretically, the anti-epileptics may be might be designated as the fundamental treatment for the acute aggravation phase because of the abnormal firings of the neuronal network.
Conclusions
Neo-organodynamism is proposed as a minimum clinical classification of mental illness for psychiatrists. By using this, we can understand that the psychosis is not the special disease, but everyone has the possibility of onset. Neo-organodynamism will eliminate the prejudice against mental illness."
}
@article{OGBURN2014244,
title = "A finite difference construction of the spheroidal wave functions",
journal = "Computer Physics Communications",
volume = "185",
number = "1",
pages = "244 - 253",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.07.024",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513002610",
author = "Daniel X. Ogburn and Colin L. Waters and Murray D. Sciffer and Jeff A. Hogan and Paul C. Abbott",
keywords = "Slepian function, Spheroidal wave function, Spectral concentration problem, Spheroidal cap harmonics, Basis function expansion, Finite difference method, Sturm–Liouville problem, Compressed sensing",
abstract = "A fast and simple finite difference algorithm for computing the spheroidal wave functions is described. The resulting eigenvalues and eigenfunctions for real and complex spheroidal bandwidth parameter, c, agree with those in the literature from four to more than eleven significant figures. The validity of this algorithm in the extreme parameter regime, up to c2=1014, is demonstrated. Furthermore, the algorithm generates the spheroidal functions for complex order m. The coefficients of the differential equation can be simply modified so that the algorithm may solve any second-order differential equation in Sturm–Liouville form. The prolate spheroidal functions and the spectral concentration problem in relation to band-limited and time-limited signals is discussed. We review the properties of these eigenfunctions in the context of Sturm–Liouville theory and the implications for a finite difference algorithm. A number of new suggestions for data fitting using prolate spheroidal wave functions with a heuristic for optimally choosing the value of c and the number of basis functions are described.
Program summary
Program title: SWF_8thOrder Catalogue identifier: AEQE_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEQE_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 1081 No. of bytes in distributed program, including test data, etc.: 160,312 Distribution format: tar.gz Programming language: Matlab R2009b. Computer: Designed to run on any computer capable of running Matlab 2009b with at least 2 GB of RAM in order to handle moderate grid sizes. With an appropriate change of syntax, the program may be easily adapted to Maple, Mathematica, Fortran, IDL and any other software capable of performing the diagonalization of large matrices. Operating system: Any operating system which will run Matlab, Mathematica, Fortran or any other language capable of performing large matrix diagonalizations. Has the code been vectorized or parallelized?: Tested with dual core and quad core systems. The algorithm will also work with single core systems. RAM: 1372 MB total used by Matlab for a grid with 4001 points, 41 MB used to store eigenfunctions, grid and spectrum arrays (4001 points). More RAM is used for larger grids. For example, with 8000 grid points, 162 MB of RAM is required to store the eigenfunction and eigenvalue arrays. Classification: 4.3. External routines: The program uses Matlab’s internal ‘eig’ routine. Nature of problem: The problem is to construct the angular eigenfunctions of the Laplacian in three dimensional, spheroidal coordinates. These are the prolate, oblate and generalized spheroidal wave functions and to compute the corresponding eigenvalues. Equivalently, the task can be seen as generating the angular functions which arise when solving the Helmholtz wave equation by separation of variables in three dimensional, spheroidal coordinates: [∂η(1−η2)∂η+λlm(c)−c2η2−m21−η2]Slm=0. This task often arises in the solution of problems with axial symmetry although setting c=0 restores spherical symmetry. More generally, the coefficient function handles, CD1E and CD2E, can be redefined by the user to match the coefficients of the 2nd and 1st derivatives of any second order Sturm–Liouville type equation, for example, the harmonics of a tri-axial ellipsoid. Hence this algorithm and manuscript provide a foundation for solving a range of problems that have application beyond the spheroidal problems considered here. Solution method: The method of solution is the ‘finite difference method’. The spatial grid is discretized into N points, N−2 of which comprise the ‘interior grid’ and 2 points are the boundary points. The spheroidal differential operator is discretized which arises from separation of variables of the Laplacian in three dimensional, spheroidal coordinates on the interior grid using 8th order finite difference formulas. The boundary conditions for the spheroidal wave functions are implemented implicitly via finite difference operators which relate the boundary points back to the interior points. This is done using sliding off-centered differences at 8th order. The discretization of the Laplace operator and implicit implementation of the boundary conditions leads to a discretized eigenvalue problem. The eigenvectors give the discretized eigenfunctions of the spheroidal differential operator and the eigenvalues give the spectrum of the differential operator. Points at the boundary are reconstructed using forward and backward difference operators. The eigenfunctions are numerically normalized using a 6th order “Boole’s Rule” integration procedure. Restrictions: The current version of this algorithm implements the option of both Dirichlet and Neumann boundary conditions, which is chosen by the user by the “BC” switch. Mixed boundary conditions can also be implemented by the user, by modifying the boundary condition ‘IF’ statements. When solving with a very large concentration parameter, |c|, one must use a large number of grid points which would result in longer computational times and require more RAM. Unusual features: This program solves for the angular eigenfunctions of the spheroidal wave equation in three dimensions. Due to the finite difference approach, one is able to solve over non-standard domains such as spheroidal caps or spheroidal belts. The program is capable of solving for the generalized spheroidal wavefunctions with complex geometric parameter c. Given a sufficiently large number of grid points, the program can generate spheroidal eigenfunctions and eigenvalues for extreme parameter regimes, for example, for |c|∼108 with a grid of 20,000 points. Furthermore, the program can generate spheroidal wavefunctions for non-integer and complex order parameter, m, which may correspond to some analytic continuation of the spheroidal wave functions. In particular, for real, non-integer values of m, this corresponds to an axially symmetric ellipsoid with a defect angle where the 2π periodicity symmetry about the rotation axis becomes 2πα periodicity, where α is some defect factor. Additional comments: Main User-Input Parameters: The main input parameters are located at the top of the code. The parameter C2 sets the value of the square of the spheroidal concentration parameter, c2. The parameter m is the order parameter which is typically an integer such as for the Legendre functions, but can also be non-integer and complex valued. The switch BC, sets the boundary conditions for the differential equation. A value BC=0 gives Dirichlet boundary conditions and BC=1 gives Neumann boundary conditions. The values of MinTheta and MaxTheta set the minimum and maximum values of the angular coordinate, θ, which specify the domain of the differential equation. The parameter Npts sets the number of grid points, with larger grids resulting in more accurate eigenfunctions and spectra. General comments: To obtain spheroidal harmonics the eigenfunctions need to be multiplied by a complex exponential factor eimφ or sinusoidal functions for real solutions. The eigenfunctions generated by this program may be normalized numerically. Since normalization conventions differ by application, normalization is left for the user to implement. For this purpose, we have encoded a plain, 6th order Boole rule unit normalization which is easy to modify. Finally, we note that if the user modifies the function handles, CD1E and CD2E then the program will solve any 2nd order ordinary differential equation with Dirichlet or Neumann boundary conditions. Non-homogeneous terms can be added by modifying the entries in the finite difference matrix where c2 and m2 appear. Running time: On a laptop with an Intel Core i3-2350M CPU (2.30 GHz), with 4.00 GB of RAM, the program takes 149 s for a grid with 4000 points. Running time increases for larger grid sizes. For example, increasing the grid size to 8000 points increased the run time to 1138 s on the same machine."
}
@article{GRANEROMOYA2016649,
title = "Dificultades de las enfermeras de atención primaria en los procesos de planificación anticipada de las decisiones: un estudio cualitativo",
journal = "Atención Primaria",
volume = "48",
number = "10",
pages = "649 - 656",
year = "2016",
issn = "0212-6567",
doi = "https://doi.org/10.1016/j.aprim.2016.01.008",
url = "http://www.sciencedirect.com/science/article/pii/S0212656716300683",
author = "Nani Granero-Moya and Antonio Frías-Osuna and Inés M. Barrio-Cantalejo and Antonio Jesús Ramos-Morcillo",
keywords = "Planificación anticipada de las decisiones, Cuidados al final de la vida, Enfermeras de atención primaria, Investigación cualitativa, Bioética, Advance care planning, End of life care, Primary care nurses, Qualitative research, Bioethics",
abstract = "Resumen
Objetivo
Conocer las dificultades que encuentran las enfermeras de atención primaria para promover procesos de planificación anticipada de las decisiones con personas en el final de la vida.
Diseño
Estudio cualitativo fenomenológico.
Emplazamiento
Área de Gestión Sanitaria Norte de Jaén.
Participantes
Enfermeras de atención primaria.
Método
Muestreo intencional. Realización de 14 entrevistas en profundidad hasta la saturación de los discursos. Análisis de contenido en 4 etapas: transcripción de datos, codificación, obtención de resultados y verificación de conclusiones. Uso de N-Vivo como apoyo al análisis. Triangulación de resultados entre investigadores.
Resultados
Dificultades referidas a los profesionales: falta de conocimiento sobre el tema, falta de habilidades de comunicación o de experiencia y presencia de emociones negativas. En la institución sanitaria, la falta de tiempo y las interferencias con otros profesionales suponen una barrera. También la actitud del propio paciente o su familia es vista como una traba ya que pocos hablan sobre el final de la vida. Finalmente, nuestra sociedad evita las conversaciones abiertas sobre temas relacionados con la muerte.
Conclusiones
Es necesario el aprendizaje de los profesionales sobre planificación anticipada de decisiones, su entrenamiento en habilidades comunicativas y su educación afectiva. Los gestores sanitarios han de tener en cuenta el hecho de que las intervenciones para planificar anticipadamente decisiones sanitarias precisan formación, tiempo y atención continuada. En tanto no acontezca un cambio cultural, persistirá un modelo evasivo para afrontar el final de la vida.
Objective
To know the primary care nurses’ difficulties to promote advance care planning process with patients in the end of life.
Design
Phenomenological qualitative methodology.
Location
Health Management Area North of Jaén.
Participants
Primary care nurses.
Method
Purposive sampling. Fourteen in-depth interviews were conducted until the speeches saturation. Content analysis in four steps: transcription, coding, obtaining results and conclusions verification. Supported whit the software Nvivo 8. Triangulation of results between researchers.
Results
Professionals’ difficulties: Lack of knowledge about the topic, lack of communication skills, lack of experience and presence of negative emotions. In the health institution lack of time and interference with other professionals is a barrier. Also the patient's attitude and the family are identified as an obstacle because few people speak about the end of life. Finally, our society prevents open discussion about issues related to death.
Conclusions
Professional learning about advanced care planning, training in communication skills and emotional education are necessary. Health managers should consider the fact that early interventions for planning health decisions require training, time and continued attention. If a cultural change does not happen, an evasive way to face the end of life will persist."
}
@article{REMY2014763,
title = "Biotherapies for Parkinson disease",
journal = "Revue Neurologique",
volume = "170",
number = "12",
pages = "763 - 769",
year = "2014",
issn = "0035-3787",
doi = "https://doi.org/10.1016/j.neurol.2014.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S0035378714010108",
author = "P. Remy",
keywords = "Graft, Transplantation, Gene therapy, Viral vector, Review, Greffe, La transplantation, La thérapie génique, Vecteurs viraux, Avis",
abstract = "The clinical use of biotherapies in Parkinson disease already has 30years history. The transplantation of dopamine fetal cells in the striatum of advanced patients has proved to be relevant in some patients but randomized efficacy trials in the US have provided disappointing results. However, cell therapies might come back on stage with the use of stem cells in the future. Gene therapy is a more recent strategy relying on viral vectors able to transduce genes coding either for the enzymes that can increase neurotransmitters production or genes for trophic factors. Several approaches have been developed in PD and have been experimented in patients. Although, some of the studies have evidenced insufficient clinical benefit, other programs, such as those using dopamine replacement techniques are promising. We find fresh hope in this field that might be the future of PD treatment. It remains however that advanced PD might not be the ideal condition to properly benefit from biotherapies and there is a need of studies at earlier stages of the disease, a time where major change in the disease course might be expected.
Résumé
Les premiers essais de biothérapie dans la maladie de Parkinson datent maintenant d’il y a 30ans. Les greffes de cellules fœtales dans le striatum de patients à un stade avancé de la maladie ont montré qu’elles pouvaient améliorer certains patients. Néanmoins, les essais contrôlés réalisés aux États-Unis ont été décevants. La thérapie cellulaire pourrait toutefois revenir au premier plan avec l’émergence prochaine des cellules souches. La thérapie génique est plus récente et repose sur l’utilisation de vecteurs viraux capables de transmettre des gènes codant soit pour des enzymes permettant la fabrication de neurotransmetteurs, soit pour des facteurs trophiques. Plusieurs approches différentes ont déjà été expérimentées chez les parkinsoniens. Si certaines de ces stratégies ont apporté des bénéfices insuffisants, il semble que les techniques visant à faire produire de la dopamine soient prometteuses. Elles pourraient révolutionner le champ thérapeutique de la maladie. Il n’en demeure pas moins que les formes avancées de maladie de Parkinson ne sont peut-être pas le stade idéal pour bénéficier de ces biothérapies. Nous avons besoin d’essais à des stades plus précoces qui pourraient s’avérer en particulier bénéfiques sur l’évolution de la maladie."
}