@article{DONOFRIO2014666,
title = "Clinical Utility of Screening Laboratory Tests in Pediatric Psychiatric Patients Presenting to the Emergency Department for Medical Clearance",
journal = "Annals of Emergency Medicine",
volume = "63",
number = "6",
pages = "666 - 675.e3",
year = "2014",
issn = "0196-0644",
doi = "https://doi.org/10.1016/j.annemergmed.2013.10.011",
url = "http://www.sciencedirect.com/science/article/pii/S0196064413014856",
author = "J. Joelle Donofrio and Genevieve Santillanes and Bradley D. McCammack and Chun Nok Lam and Michael D. Menchine and Amy H. Kaji and Ilene A. Claudius",
abstract = "Study objective
We assess whether screening laboratory tests obtained to medically clear pediatric psychiatric patients altered management or disposition.
Methods
This was a retrospective chart review of consecutive patients younger than 18 years and presenting to an academic pediatric emergency department for medical clearance of an acute psychiatric emergency potentially requiring an involuntary hold (danger to self, danger to others, grave disability) from July 2009 to December 2010. Patients were identified by discharge diagnosis codes. History and physical examination and screening laboratory tests were reviewed for changes in management or disposition. Further analysis compared length of stay according to type of laboratory test performed. To avoid missing patients presenting with or for evaluation of an involuntary hold for whom an organic cause was diagnosed, charts with psychiatric chief complaints were reviewed for the same period.
Results
One thousand eighty-two visits resulting in 13,725 individual laboratory tests were analyzed. Of 871 visits with laboratory tests performed, abnormal laboratory tests were associated with 7 disposition changes (0.8%) and 50 management changes (5.7%) not associated with a disposition change. Twenty-five patients with noncontributory history and physical examination results had management changes, all non-urgent. One patient with a noncontributory history and physical examination result had a disposition-changing laboratory result, a positive urine pregnancy test. Patients who had any screening test performed had a longer length of stay than patients without testing (117 minutes longer; 95% confidence interval 109.7 to 124.4 minutes). In charts reviewed according to chief complaint, no patient was found to have an organic cause of their symptoms according to only screening tests.
Conclusion
Screening laboratory tests resulted in few management and disposition changes in patients with noncontributory history and physical examination results but were associated with increased length of stay."
}
@article{EUGSTER20131490,
title = "Safe uniform proxies for Java",
journal = "Science of Computer Programming",
volume = "78",
number = "9",
pages = "1490 - 1520",
year = "2013",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2012.10.012",
url = "http://www.sciencedirect.com/science/article/pii/S0167642312002043",
author = "Patrick Eugster",
keywords = "Proxy, Object, Java, Byte-code, Transformation, Safety",
abstract = "The proxy abstraction has a long-lasting tradition in object-oriented programming. From design patterns to inherent programming language support, and from remote method invocations to simple forms of behavioral reflection, incarnations as well as applications of proxies are innumerable. Since version 1.3, Java has supported the concept of a dynamic proxy. Such an object conforms to a set of types specified by the program and can be used wherever an expression of any of these types is expected, yet it reifies invocations performed on it. This ability has allowed dynamic proxies to be used to implement paradigms such as behavioral reflection, structural conformance, or multi-methods. Alas, these proxies are only available “for interfaces”. The case of creating dynamic proxies for a set of types including a class has not been addressed, meaning that it is currently not possible to create a dynamic proxy that conforms to an application-defined class type. This weakness strongly limits any application of dynamic proxies beyond the inherent limitations of proxies, which have motivated deeper programming language support for features such as behavioral reflection. In this paper, we unfold the current support for dynamic proxies in Java, assessing it in the light of a set of generic criteria for proxy implementations. We present an approach to supporting dynamic proxies “for classes”, consisting in transformations performed on classes at load-time, including a generic scheme for enforcing encapsulation upon field accesses. These transformations seamlessly extend the scope of the current support for dynamic proxies from the programmer’s perspective. We argue for the safety of our transformations, and discuss the precise benefits and costs of our extension in terms of the criteria introduced through an implementation of future method invocations balancing safety and transparency."
}
@article{FIORITO2013307,
title = "Development of time-dependent reaction rates to optimise predictor–corrector algorithm in ALEPH burn-up code",
journal = "Annals of Nuclear Energy",
volume = "62",
pages = "307 - 315",
year = "2013",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2013.05.046",
url = "http://www.sciencedirect.com/science/article/pii/S0306454913003113",
author = "L. Fiorito and A. Stankovskiy and G. Van den Eynde and P.E. Labeau",
keywords = "Reaction rate, Burn-up code, Linear interpolation",
abstract = "Shells coupling Monte-Carlo transport and deterministic depletion codes are extensively used in the nuclear field to simulate material changes throughout irradiation. The dynamic behaviour of the phenomenon is described by the system of coupled ordinary differential equations, with generally a stiff matrix of coefficients that current codes keep constant in time along each burn-up interval. The matrix coefficients represent decay constants and microscopic reaction rates of the numerous nuclides involved in the calculations. For a typical burn-up problem, their determination consumes most of the required computational time while only a small fraction is spent by the depletion solver. Predictor–corrector methods have been implemented to guarantee more accurate results, but not much has been done to overcome the running time issue. This work presents a unique and innovative feature of the ALEPH Monte-Carlo burn-up code which optimises the depletion algorithm by using time-dependent matrix coefficients. Linear polynomials interpolate the evolution of the matrix coefficients along a few consecutive time steps. Then, trend curves are constructed and used to extrapolate the effective reaction rates in the following intervals, thus reducing the total required computational time spent in the neutronic calculations. This technique has been implemented in the version 2 of the ALEPH Monte-Carlo burn-up code and validated against the REBUS experimental benchmark. The results revealed a considerable computational time saving without any drawbacks in the accuracy."
}
@article{SWEET201810,
title = "Fuel performance simulation of iron-chrome-aluminum (FeCrAl) cladding during steady-state LWR operation",
journal = "Nuclear Engineering and Design",
volume = "328",
pages = "10 - 26",
year = "2018",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2017.11.043",
url = "http://www.sciencedirect.com/science/article/pii/S0029549317305654",
author = "R.T. Sweet and N.M. George and G.I. Maldonado and K.A. Terrani and B.D. Wirth",
keywords = "BISON, Fuel performance, Fuel modeling, Simulation, Accident tolerant fuels, FeCrAl",
abstract = "Alternative cladding materials have been proposed to replace the currently used zirconium (Zr)-based alloys, in order to improve the accident tolerance of light water reactor (LWR) fuel. Of these materials, there is a particular focus on iron-chromium-aluminum (FeCrAl) alloys that exhibit much slower oxidation kinetics in high-temperature steam than Zr-alloys. This behavior should decrease the energy release due to oxidation and allow the cladding to remain integral longer in the presence of high temperature steam, making accident mitigation more likely. Within the development of these alloys, suitability for normal operation must also be demonstrated. This article is focused on modeling the integral thermo-mechanical performance of FeCrAl clad UO2 fuel during normal reactor operation. Finite element analysis has been performed to assess commercially available FeCrAl alloys (namely Alkrothal 720 and APMT) as a candidate fuel cladding replacement for Zr-alloys, using the MOOSE-based fuel performance code BISON. These simulations identify the effects of the mechanical-stress and irradiation responses of FeCrAl and provide a comparison with Zr-alloys. In comparing these cladding materials, fuel rods have been simulated for normal reactor operation and simple steady-state operation. Normal reactor operating conditions target the cladding performance over the rod lifetime (∼4 cycles) for the highest-power rod in the highest-power fuel assembly under reactor power maneuvering. These power histories and axial temperature profiles input into BISON were generated from a neutronics study on full-core reactivity equivalence for FeCrAl using the 3D full core simulator NESTLE. The fuel rod designs and operating conditions used here are based on the Peach Bottom BWR with representative GE-12/14 fuel geometries, and design consideration was given to minimize the neutronic penalty of the FeCrAl cladding by changing fuel enrichment and cladding thickness. Individual sensitivity analyses of the fuel and cladding creep responses were also performed, which indicated the influence of compliance for each material, separately, on the stress state of the fuel cladding. These parametric analyses are performed using steady-state operating conditions such as a simple axial power profile, a constant cladding surface temperature, and a constant fuel power history."
}
@article{DAVIS201861,
title = "Functional coding variation in the presynaptic dopamine transporter associated with neuropsychiatric disorders drives enhanced motivation and context-dependent impulsivity in mice",
journal = "Behavioural Brain Research",
volume = "337",
pages = "61 - 69",
year = "2018",
issn = "0166-4328",
doi = "https://doi.org/10.1016/j.bbr.2017.09.043",
url = "http://www.sciencedirect.com/science/article/pii/S0166432817311312",
author = "Gwynne L. Davis and Adele Stewart and Gregg D. Stanwood and Raajaram Gowrishankar and Maureen K. Hahn and Randy D. Blakely",
keywords = "Attention-deficit hyperactivity disorder, Motivation, Impulsivity, Dopamine transporter, Transgenic model, Instrumental learning",
abstract = "Recent genetic analyses have provided evidence that clinical commonalities associated with different psychiatric diagnoses often have shared mechanistic underpinnings. The development of animal models expressing functional genetic variation attributed to multiple disorders offers a salient opportunity to capture molecular, circuit and behavioral alterations underlying this hypothesis. In keeping with studies suggesting dopaminergic contributions to attention-deficit hyperactivity disorder (ADHD), bipolar disorder (BPD) and autism spectrum disorder (ASD), subjects with these diagnoses have been found to express a rare, functional coding substitution in the dopamine (DA) transporter (DAT), Ala559Val. We developed DAT Val559 knock-in mice as a construct valid model of dopaminergic alterations that drive multiple clinical phenotypes, and here evaluate the impact of lifelong expression of the variant on impulsivity and motivation utilizing the 5- choice serial reaction time task (5-CSRTT) and Go/NoGo as well as tests of time estimation (peak interval analysis), reward salience (sucrose preference), and motivation (progressive ratio test). Our findings indicate that the DAT Val559 variant induces impulsivity behaviors that are dependent upon the reward context, with increased impulsive action observed when mice are required to delay responding for a reward, whereas mice are able to withhold responding if there is a probability of reward for a correct rejection. Utilizing peak interval and progressive ratio tests, we provide evidence that impulsivity is likely driven by an enhanced motivational phenotype that also may drive faster task acquisition in operant tasks. These data provide critical validation that DAT, and more generally, DA signaling perturbations can drive impulsivity that can manifest in specific contexts and not others, and may rely on motivational alterations, which may also drive increased maladaptive reward seeking."
}
@article{FERGUSON2018241,
title = "‘If no-one stops me, I'll make the mistake again’: Changing prescribing behaviours through feedback; A Perceptual Control Theory perspective’",
journal = "Research in Social and Administrative Pharmacy",
volume = "14",
number = "3",
pages = "241 - 247",
year = "2018",
issn = "1551-7411",
doi = "https://doi.org/10.1016/j.sapharm.2017.03.001",
url = "http://www.sciencedirect.com/science/article/pii/S1551741116305575",
author = "Jane Ferguson and Chris Keyworth and Mary P. Tully",
keywords = "Prescribing errors, Feedback, Perceptual Control Theory, Qualitative, Behaviour change",
abstract = "Background
Doctors at all levels make prescribing errors which can prolong patients' hospital stay, increase the risk of death, and place a significant financial burden on the health system. Doctors have previously reported receiving little or no feedback on their prescribing errors. The effectiveness of feedback in modifying future practice varies widely, depending on how feedback is delivered. To date there is little evidence about why and how feedback interventions do or do not work. Behavioural theories can be used to evaluate this process and provide explanatory accounts to inform recommendations for future interventions.
Objective
To explore the experiences of prescribers receiving different methods of feedback about their prescribing errors. Perceptual Control Theory (PCT) was used as a theoretical framework to explain which aspects of feedback were most likely to influence prescribing behaviour.
Methods
A secondary analysis of 31 semi-structured qualitative interviews with junior doctors who had taken part one of three studies in which they received feedback on their prescribing errors. A hybrid approach to analysis involved inductive thematic analysis, and deductive a priori template of codes using PCT as a framework to guide data analysis and interpretation.
Results
Feedback was most useful for learning and most likely to influence future prescribing behaviour when it was timely, and provided a comprehensive, contextualised benchmark to which participants could compare their prescribing behaviours and current level of knowledge. Group discussions and completing directly-observed prescribing event forms were thought most likely to impact future prescribing; email feedback alone was perceived as least effective in changing prescribing behaviour.
Conclusion
Feedback has the potential to change future prescribing behaviour. Behaviour change can only take place if prescribers are made aware of these discrepancies, either via providing appropriate reference values or benchmarks before mistakes are made, or by providing timely and comprehensive feedback after mistakes are made."
}
@article{HUGHES201758,
title = "Understanding policy persistence—The case of police drug detection dog policy in NSW, Australia",
journal = "International Journal of Drug Policy",
volume = "44",
pages = "58 - 68",
year = "2017",
issn = "0955-3959",
doi = "https://doi.org/10.1016/j.drugpo.2017.03.007",
url = "http://www.sciencedirect.com/science/article/pii/S0955395917300816",
author = "Caitlin E. Hughes and Alison Ritter and Kari Lancaster and Robert Hoppe",
keywords = "Advocacy coalition framework, Policing, Drug detection dogs, Policy persistence, Policy change, Australia",
abstract = "Background
Significant research attention has been given to understanding the processes of drug policy reform. However, there has been surprisingly little analysis of the persistence of policy in the face of opposition and evidence of ineffectiveness. In this article we analysed just such a case – police drug detection dog policy in NSW, Australia. We sought to identify factors which may account for the continuation of this policy, in spite of counter-evidence and concerted advocacy.
Methods
The analysis was conducted using the Advocacy Coalition Framework (ACF). We collated documents relating to NSW drug detection dog policy from 1995 to 2016, including parliamentary records (NSW Parliament Hansard), government and institutional reports, legislation, police procedures, books, media, and academic publications. Texts were then read, coded and classified against the core dimensions of the ACF, including subsystem actors and coalitions, their belief systems and resources and venues employed for policy debate.
Results
Three coalitions were identified as competing in the policy subsystem: security/law and order, civil liberties and harm reduction. Factors that aided policy stability were the continued dominance of the security/law and order coalition since they introduced the drug dog policy; a power imbalance enabling the ruling coalition to limit when and where the policy was discussed; and a highly adversarial policy subsystem. In this context even technical knowledge that dogs infringed civil liberties and increased risks of overdose were readily downplayed, leading to only incremental changes in implementation rather than policy cessation or wholesale revision.
Conclusion
The analysis provides new insights into why the accumulation of new evidence and advocacy efforts can be insufficient to drive significant policy change. It poses a challenge for the evidence-based paradigm suggesting that in highly adversarial policy subsystems new evidence is unlikely to generate policy change without broader subsystem change, such as reducing the adversarial nature and/or providing new avenues for cross-coalition learning."
}
@article{SINGH2015146,
title = "Isolation, characterization and functional analysis of full length p53 cDNA from Bubalus bubalis",
journal = "Gene",
volume = "568",
number = "2",
pages = "146 - 154",
year = "2015",
issn = "0378-1119",
doi = "https://doi.org/10.1016/j.gene.2015.05.047",
url = "http://www.sciencedirect.com/science/article/pii/S0378111915006198",
author = "Minu Singh and Suruchi Aggarwal and Ashok K. Mohanty and Tapas Mukhopadhyay",
keywords = "Cloning, Water buffalo, Tumor suppressor, p21, mdm2",
abstract = "p53 plays a pivotal role in maintaining the genomic integrity of the cell and has an important role in cellular transformation. We isolated and cloned a full length p53 cDNA (Bp53) from water buffalo in expression vectors designed to generate tagged proteins with FLAG or GFP. Bp53 was found to be 1161 nucleotide long and codes for 386 amino acid residues with 79% homology with human p53 containing 393 amino acids. Although Bp53 has some inherent differences in amino acid composition in different functional domains as compared to human p53 but the total electrostatic charge of amino acids has been maintained. Bp53 cDNA was transiently transfected in a p53 null human NSCLC cell line and as expected, it was predominantly localized in the nucleus. Besides, Bp53 effectively transactivates a number of target genes similar to human p53 and exerts most of its anti-tumorigenic potential in culture as observed in clonogenic and cell viability assays. Like human p53 mutants, core domain mutant version of Bp53 was found to be mis-localized to cytoplasm with diminished tumor suppressor activity. However, Bp53 appeared to be more sensitive to mdm2 mediated degradation and as a result, this protein was less stable as compared to human p53. For the first time we have characterized a functionally efficient wild-type p53 from buffalo having lower stability than human p53 and thus, buffalo p53 could be used as a model system for further insight to the molecular basis of wild-type p53 instability."
}
@article{JARVELA201639,
title = "How do types of interaction and phases of self-regulated learning set a stage for collaborative engagement?",
journal = "Learning and Instruction",
volume = "43",
pages = "39 - 51",
year = "2016",
note = "Special Issue: Student engagement and learning: theoretical and methodological advances",
issn = "0959-4752",
doi = "https://doi.org/10.1016/j.learninstruc.2016.01.005",
url = "http://www.sciencedirect.com/science/article/pii/S0959475216300056",
author = "Sanna Järvelä and Hanna Järvenoja and Jonna Malmberg and Jaana Isohätälä and Márta Sobocinski",
keywords = "Engagement, Self-regulated learning, Collaborative learning, Videodata, Cognitive interaction, Socioemotional interaction",
abstract = "This study investigates how self-regulated learning phases are related to collaborative engagement in two different collaborative task conditions. It integrates SRL theory and the concept of engagement, including interaction in collaboration, as key characteristics of engagement. Forty-four second-year teacher education students worked in groups during a 7-week math didactic course. We collected 84 h of video recordings and coded the group's cognitive and socioemotional interaction and three phases of self-regulation within interaction, including forethought, performance and reflection. After that we analyzed the relationship between the interaction types representing collaborative engagement and SRL phases within two learning tasks. The results show that collaborative engagement did not differ between teacher-led and student-led tasks in terms of the interaction types. However, the results showed that the SRL phases occurred differently within cognitive and socioemotional interaction types when the two task conditions were compared. Findings concerning teacher-led tasks showed invariance in the occurrence of SRL phases across the task and highlighted the relationship between socioemotional interaction and the forethought phase. Additionally, findings concerning the student-led tasks showed systematic changes in the distribution of phases of SRL across sessions in all interaction types. Our results' theoretical and methodological implications for collaborative engagement research are discussed."
}
@article{TODD2016497,
title = "Initial uncertainty impacts statistical learning in sound sequence processing",
journal = "Journal of Physiology-Paris",
volume = "110",
number = "4, Part B",
pages = "497 - 507",
year = "2016",
note = "Role of synchronized physiological and interpersonal rhythms in typical and atypical development",
issn = "0928-4257",
doi = "https://doi.org/10.1016/j.jphysparis.2017.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S0928425717300013",
author = "Juanita Todd and Alexander Provost and Lisa Whitson and Daniel Mullens",
keywords = "Auditory evoked potentials, Sequential learning, Predictive coding, Mismatch negativity, Primacy bias",
abstract = "This paper features two studies confirming a lasting impact of first learning on how subsequent experience is weighted in early relevance-filtering processes. In both studies participants were exposed to sequences of sound that contained a regular pattern on two different timescales. Regular patterning in sound is readily detected by the auditory system and used to form “prediction models” that define the most likely properties of sound to be encountered in a given context. The presence and strength of these prediction models is inferred from changes in automatically elicited components of auditory evoked potentials. Both studies employed sound sequences that contained both a local and longer-term pattern. The local pattern was defined by a regular repeating pure tone occasionally interrupted by a rare deviating tone (p=0.125) that was physically different (a 30msvs. 60ms duration difference in one condition and a 1000Hz vs. 1500Hz frequency difference in the other). The longer-term pattern was defined by the rate at which the two tones alternated probabilities (i.e., the tone that was first rare became common and the tone that was first common became rare). There was no task related to the tones and participants were asked to ignore them while focussing attention on a movie with subtitles. Auditory-evoked potentials revealed long lasting modulatory influences based on whether the tone was initially encountered as rare and unpredictable or common and predictable. The results are interpreted as evidence that probability (or indeed predictability) assigns a differential information-value to the two tones that in turn affects the extent to which prediction models are updated and imposed. These effects are exposed for both common and rare occurrences of the tones. The studies contribute to a body of work that reveals that probabilistic information is not faithfully represented in these early evoked potentials and instead exposes that predictability (or conversely uncertainty) may trigger value-based learning modulations even in task-irrelevant incidental learning."
}
@article{OKUYAN2014442,
title = "BilKristal 2.0: A tool for pattern information extraction from crystal structures",
journal = "Computer Physics Communications",
volume = "185",
number = "1",
pages = "442 - 443",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.09.020",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513003214",
author = "Erhan Okuyan and Uğur Güdükbay",
keywords = "Crystal, Crystallography, Material science, Pattern recognition, Primitive vectors, Basis vectors, Space group, Symmetry",
abstract = "We present a revised version of the BilKristal tool of Okuyan et al. (2007). We converted the development environment into Microsoft Visual Studio 2005 in order to resolve compatibility issues. We added multi-core CPU support and improvements are made to graphics functions in order to improve performance. Discovered bugs are fixed and exporting functionality to a material visualization tool is added.
New version program summary
Program title: BilKristal 2.0. Catalogue identifier: ADYU_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ADYU_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 364263 No. of bytes in distributed program, including test data, etc.: 9135815 Distribution format: tar.gz Programming language: C, C++, Microsoft.NET Framework 2.0 and OpenGL Libraries. Computer: Personal computer with Windows operating system. Operating system: Windows XP or higher. Has the code been vectorized or parallelized?: Multi-core CPU support included. RAM: 20–60 Megabytes. Catalogue identifier of previous version: ADYU_v1_0 Journal reference of previous version: Comput. Phys. Comm. 176 (2007) 486 Classification: 8. External routines: Microsoft.NET Framework 2.0. For the visualization tool, the graphics card driver should also support OpenGL. Does the new version supercede the previous version?: Yes Nature of problem: Determining the crystal structure parameters of a material is a very important issue in crystallography. Knowing the crystal structure parameters helps the understanding of the physical behavior of a material. For complex structures, particularly for materials which also contain local symmetry as well as global symmetry, obtaining crystal parameters can be very hard. Solution method: The tool extracts crystal parameters such as primitive vectors and basis vectors and identifies the space group from the atomic coordinates of crystal structures. Reasons for new version: Additional features, resolved compatibility issues with the new development environments, performance optimizations, minor bug corrections. Summary of revisions:•Capability to export to MaterialVis tool [1] is added. The tool can export the unit cell information extracted from the crystal structure, the raw atomic coordinates and atomic radii into a data file (.dat) that the MaterialVis tool can process.•Compatibility issues with Microsoft Visual Studio 2005 up to 2010 are resolved. The original code was developed using Microsoft Visual Studio 2003. However, newer Visual Studio versions were not able to convert and compile the code. Due to the changes in the .NET framework, the converted project produced many errors. In this work, the project is converted into a Visual Studio 2005 project and compilation errors are resolved. We also tested the code with Visual Studio 2010 and the project was successfully converted and compiled.•Multi-Core CPU support is added. In recent years, multi-core CPUs have become very common. We added the multi-core CPU support in order to utilize the computational capabilities of additional CPU cores. This significantly improves the performance.•The visualization interface is improved. In particular, the sphere drawing functionality is replaced with an efficient and high quality version that utilizes GPU acceleration.•For some cases, the fractional coordinates of some of the calculated basis vectors were not all in the [0,1) range, but at coordinate 1.0 for some axes. These cases were corrected by translating these basis vectors into the [0,1) range.Restrictions: Assumptions are explained in [2]. However, none of them can be considered as a restriction on the complexity of the problem. Running time: The tool was able to process input files with more than a million atoms in less than 20 s on a PC with an Athlon quad-core CPU at 3.2 GHz using the default parameter values. References: [1] Erhan Okuyan, Ugur Güdükbay, MaterialVis: Crystal and Amorphous Material Visualization Tool Using Direct Volume and Surface Rendering Techniques (Program Summary), Computer Physics Communications, Submitted. [2] Erhan Okuyan, Ugur Güdükbay, and Oguz Gülseren, Pattern Information Extraction from Crystal Structures, Computer Physics Communications, 176 (2007) 486."
}
@article{ROMANO201862,
title = "SPIRITuS: a SimPle Information Retrieval regressIon Test Selection approach",
journal = "Information and Software Technology",
volume = "99",
pages = "62 - 80",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.03.004",
url = "http://www.sciencedirect.com/science/article/pii/S0950584918300405",
author = "Simone Romano and Giuseppe Scanniello and Giuliano Antoniol and Alessandro Marchetto",
keywords = "SPIRITuS, Regression test case selection, Regression testing",
abstract = "Context:Regression Test case Selection (RTS) approaches aim at selecting only those test cases of a test suite that exercise changed parts of the System Under Test (SUT) or parts affected by changes. Objective:We present SPIRITuS (SimPle Information Retrieval regressIon Test Selection approach). It uses method code coverage information and a Vector Space Model to select test cases to be run. In a nutshell, the extent of a lexical modification to a method is used to decide if a test case has to be selected. The main design goals of SPIRITuS are to be: (i) easy to adapt to different programming languages and (ii) tunable via an easy to understand threshold. Method:To assess SPIRITuS, we conducted a large experiment on 389 faulty versions of 14 open-source programs implemented in Java. We were mainly interested in investigating the tradeoff between the number of selected test cases from the original test suite and fault detection effectiveness. We also compared SPIRITuS against well-known RTS approaches. Results:SPIRITuS selects a number of test cases significantly smaller than the number of test cases the other approaches select at the price of a slight reduction in fault detection capability. Conclusions:SPIRITuS can be considered a viable competitor of existing test case selection approaches especially when the average number of test cases covering a modified method increases (such information can be easily derived before test case selection takes place)."
}
@article{PIQUETTE2015678,
title = "Balancing care and teaching during clinical activities: 2 contexts, 2 strategies",
journal = "Journal of Critical Care",
volume = "30",
number = "4",
pages = "678 - 684",
year = "2015",
issn = "0883-9441",
doi = "https://doi.org/10.1016/j.jcrc.2015.03.002",
url = "http://www.sciencedirect.com/science/article/pii/S0883944115000842",
author = "Dominique Piquette and Carol-Anne Moulton and Vicki R. LeBlanc",
keywords = "Clinical supervision, Clinical learning, Bedside teaching, Multidisciplinary rounds, Medical crises, Critical care",
abstract = "Purpose
The goal of this study was to better understand how clinical supervisors integrate teaching interactions with medical trainees into 2 types of clinical activities in the critical care setting: multidisciplinary rounds and medical crises.
Methods
We conducted a qualitative, observational study based on an ethnographic approach. We observed the teaching interactions among clinical supervisors and medical trainees during 12 multidisciplinary rounds and 74 medical crises in 2 academic hospitals. Grounded theory methods (theoretical sampling and saturation, inductive thematic coding, and constant comparison) were used to analyze data.
Results
Two models of integration of teaching interactions into clinical activities are described: the in series model, typical of multidisciplinary rounds and characterized by well-structured learning bubbles uninterrupted by patient care, and the in parallel model, common during medical crises and involving multiple, short learning flashes intricately related to and frequently interrupted by patient care. By adopting a model over the other, supervisors appeared to adapt to 2 contexts that differed in terms of priority, supervisor's understanding of events, and social context of interactions. Each model presented complementary opportunities and limitations for learning.
Conclusions
Modern views of medical apprenticeship and clinical teaching need to take into account the specific clinical context in which learning occurs. Teaching interactions that differ in structure and content in response to changing clinical circumstances could impact learning in unique ways. Learning outcomes resulting from different models of integration of teaching into clinical activities need to be further explored."
}
@article{MELLO20151113,
title = "A Scalable Population Code for Time in the Striatum",
journal = "Current Biology",
volume = "25",
number = "9",
pages = "1113 - 1122",
year = "2015",
issn = "0960-9822",
doi = "https://doi.org/10.1016/j.cub.2015.02.036",
url = "http://www.sciencedirect.com/science/article/pii/S0960982215002055",
author = "Gustavo B.M. Mello and Sofia Soares and Joseph J. Paton",
abstract = "Summary
To guide behavior and learn from its consequences, the brain must represent time over many scales. Yet, the neural signals used to encode time in the seconds-to-minute range are not known. The striatum is a major input area of the basal ganglia associated with learning and motor function. Previous studies have also shown that the striatum is necessary for normal timing behavior. To address how striatal signals might be involved in timing, we recorded from striatal neurons in rats performing an interval timing task. We found that neurons fired at delays spanning tens of seconds and that this pattern of responding reflected the interaction between time and the animals’ ongoing sensorimotor state. Surprisingly, cells rescaled responses in time when intervals changed, indicating that striatal populations encoded relative time. Moreover, time estimates decoded from activity predicted timing behavior as animals adjusted to new intervals, and disrupting striatal function led to a decrease in timing performance. These results suggest that striatal activity forms a scalable population code for time, providing timing signals that animals use to guide their actions."
}
@article{CAYZAC20153200,
title = "Altered hippocampal information coding and network synchrony in APP-PS1 mice",
journal = "Neurobiology of Aging",
volume = "36",
number = "12",
pages = "3200 - 3213",
year = "2015",
issn = "0197-4580",
doi = "https://doi.org/10.1016/j.neurobiolaging.2015.08.023",
url = "http://www.sciencedirect.com/science/article/pii/S0197458015004406",
author = "Sebastien Cayzac and Nicole Mons and Antonin Ginguay and Bernadette Allinquant and Yannick Jeantet and Yoon H. Cho",
keywords = "β-amyloid, Place cell, Synaptic plasticity, Theta synchrony, Ripple oscillation, c-Fos, Cognition, APP-PS1 mice, Alzheimer's disease",
abstract = "β-amyloid is hypothesized to harm neural function and cognitive abilities by perturbing synaptic transmission and plasticity in Alzheimer's disease (AD). To assess the impact of this pathology on hippocampal neurons' ability to encode flexibly environmental information across learning, we performed electrophysiological recordings of CA1 hippocampal unit activity in AD transgenic mice as they acquired an action-reward association in a spatially defined environment; the behavioral task enabled the precise timing of discrete and intentional behaviors of the animal. We found that the proportion of behavioral task-sensitive cells in wild-type (WT) mice typically increased, whereas the proportion of place cells decreased with learning. In AD mice, this learning-dependent change of cell-discharge patterns was absent, and cells exhibited similar firings from the beginning to firings attained at the late learning stage in wild-type cells. These inflexible hippocampal representations of task and space throughout learning are accompanied by remarkable alterations of local oscillatory activity in the theta and ultra-fast ripple frequencies as well as learning abilities. The present data offer new insights into the in vivo cellular and network processes by which β-amyloid and other AD mutations may exert its harmful effects to produce cognitive and behavioral impairments in early stage of AD."
}
@article{BURGER201427,
title = "Parallel flow routing in SWMM 5",
journal = "Environmental Modelling & Software",
volume = "53",
pages = "27 - 34",
year = "2014",
issn = "1364-8152",
doi = "https://doi.org/10.1016/j.envsoft.2013.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S1364815213002831",
author = "G. Burger and R. Sitzenfrei and M. Kleidorfer and W. Rauch",
keywords = "Multi-core, OpenMP, Parallel computing, Storm Water Management Model, Urban drainage modeling",
abstract = "The hydrodynamic rainfall-runoff and urban drainage simulation model SWMM (Storm Water Management Model) is a state of the art software tool applied likewise in research and practice. In order to reduce the computational burden of long simulation runs and to use the extra power of modern multi-core computers, a parallel version of SWMM is presented herein. The challenge has been to modify the software in such minimal way that the resulting code enhancement may find its way into the commercial and non-commercial software tools that depend on SWMM for its calculation engine. A pragmatic approach to identify and enhance only the critical parts of the software in terms of run-time was chosen in order to keep the code changes as low as possible. The enhanced software was first tested for coherence against the original code and then benchmarked on four different input scenarios ranging from a very small village to a medium sized urban area. For the investigated sewer systems a speedup of six to ten times on a twelve core system was realized, thus decreasing the execution time to an acceptable level even for tedious system analysis."
}
@article{OTERODELAROZA20141007,
title = "Critic2: A program for real-space analysis of quantum chemical interactions in solids",
journal = "Computer Physics Communications",
volume = "185",
number = "3",
pages = "1007 - 1018",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.10.026",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513003718",
author = "A. Otero-de-la-Roza and Erin R. Johnson and Víctor Luaña",
keywords = "Quantum Theory of Atoms in Molecules (QTAIM), Quantum Chemical Topology (QCT), Non-covalent interactions (NCI) index, Atomic charges, Electron density",
abstract = "We present critic2, a program for the analysis of quantum-mechanical atomic and molecular interactions in periodic solids. This code, a greatly improved version of the previous critic program (Otero-de-la Roza et al., 2009), can: (i) find critical points of the electron density and related scalar fields such as the electron localization function (ELF), Laplacian, … (ii) integrate atomic properties in the framework of Bader’s Atoms-in-Molecules theory (QTAIM), (iii) visualize non-covalent interactions in crystals using the non-covalent interactions (NCI) index, (iv) generate relevant graphical representations including lines, planes, gradient paths, contour plots, atomic basins, … and (v) perform transformations between file formats describing scalar fields and crystal structures. Critic2 can interface with the output produced by a variety of electronic structure programs including WIEN2k, elk, PI, abinit, Quantum ESPRESSO, VASP, Gaussian, and, in general, any other code capable of writing the scalar field under study to a three-dimensional grid. Critic2 is parallelized, completely documented (including illustrative test cases) and publicly available under the GNU General Public License.
Program summary
Program title: CRITIC2 Catalogue identifier: AECB_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AECB_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: yes No. of lines in distributed program, including test data, etc.: 11686949 No. of bytes in distributed program, including test data, etc.: 337020731 Distribution format: tar.gz Programming language: Fortran 77 and 90. Computer: Workstations. Operating system: Unix, GNU/Linux. Has the code been vectorized or parallelized?: Shared-memory parallelization can be used for most tasks. Classification: 7.3. Catalogue identifier of previous version: AECB_v1_0 Journal reference of previous version: Comput. Phys. Comm. 180 (2009) 157 Nature of problem: Analysis of quantum-chemical interactions in periodic solids by means of atoms-in-molecules and related formalisms. Solution method: Critical point search using Newton’s algorithm, atomic basin integration using bisection, qtree and grid-based algorithms, diverse graphical representations and computation of the non-covalent interactions index on a three-dimensional grid. Additional comments: !!!!! The distribution file for this program is over 330 Mbytes and therefore is not delivered directly when download or Email is requested. Instead a html file giving details of how the program can be obtained is sent. !!!!! Running time: Variable, depending on the crystal and the source of the underlying scalar field."
}
@article{FU2016135,
title = "Tuning for software analytics: Is it really necessary?",
journal = "Information and Software Technology",
volume = "76",
pages = "135 - 146",
year = "2016",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2016.04.017",
url = "http://www.sciencedirect.com/science/article/pii/S0950584916300738",
author = "Wei Fu and Tim Menzies and Xipeng Shen",
keywords = "Defect prediction, CART, Random forest, Differential evolution, Search-based software engineering",
abstract = "Context: Data miners have been widely used in software engineering to, say, generate defect predictors from static code measures. Such static code defect predictors perform well compared to manual methods, and they are easy to use and useful to use. But one of the “black arts” of data mining is setting the tunings that control the miner. Objective: We seek simple, automatic, and very effective method for finding those tunings. Method: For each experiment with different data sets (from open source JAVA systems), we ran differential evolution as an optimizer to explore the tuning space (as a first step) then tested the tunings using hold-out data. Results: Contrary to our prior expectations, we found these tunings were remarkably simple: it only required tens, not thousands, of attempts to obtain very good results. For example, when learning software defect predictors, this method can quickly find tunings that alter detection precision from 0% to 60%. Conclusion: Since (1) the improvements are so large, and (2) the tuning is so simple, we need to change standard methods in software analytics. At least for defect prediction, it is no longer enough to just run a data miner and present the result without conducting a tuning optimization study. The implication for other kinds of analytics is now an open and pressing issue."
}
@article{AZIZOGLU20181777,
title = "Work hardening during alternating load directions of 316L SS",
journal = "Procedia Manufacturing",
volume = "15",
pages = "1777 - 1784",
year = "2018",
note = "Proceedings of the 17th International Conference on Metal Forming METAL FORMING 2018 September 16 – 19, 2018, Loisir Hotel Toyohashi, Toyohashi, Japan",
issn = "2351-9789",
doi = "https://doi.org/10.1016/j.promfg.2018.07.246",
url = "http://www.sciencedirect.com/science/article/pii/S2351978918309417",
author = "Yağız Azizoğlu and Mattias Gärdsback and Akinori Yamanaka and Toshihiko Kuwabara and Lars-Erik Lindgren",
keywords = "Finite element method (FEM), Isotropic, kinematic hardening, Cyclic plasticity, Chaboche model, Uniaxial / Multiaxial loading, Cold pilgering, Tube forming",
abstract = "Understanding and modelling the plastic behavior of a material are essential for simulation and design of metal forming processes. Cold pilgering of tubes is a process with very complex strain history with alternating loading direction. This makes evaluation of the work hardening challenging. Cold deformation applied in a single direction predominantly exhibit work hardening, while changes of the loading direction may even cause softening in other directions. The influence of alternating loading directions on work hardening has been experimentally investigated for 316L stainless steel (SS). Cubic specimens were cut out from the preform of the tube. The specimens are subjected to uniaxial compressions in alternating directions along two perpendicular axes. From the results, a cyclic elastic-plastic constitutive model based on a Chaboche-type approach is calibrated and implemented in the commercial finite element code MSC.Marc."
}
@article{GARDNER2017174,
title = "Using guitar learning to probe the Action Observation Network's response to visuomotor familiarity",
journal = "NeuroImage",
volume = "156",
pages = "174 - 189",
year = "2017",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2017.04.060",
url = "http://www.sciencedirect.com/science/article/pii/S1053811917303804",
author = "Tom Gardner and Aidas Aglinskas and Emily S. Cross",
keywords = "Action Observation Network, Familiarity, Direct matching, Predictive coding, ROI regression, fMRI, Neural efficiency, Music learning, Guitar",
abstract = "Watching other people move elicits engagement of a collection of sensorimotor brain regions collectively termed the Action Observation Network (AON). An extensive literature documents more robust AON responses when observing or executing familiar compared to unfamiliar actions, as well as a positive correlation between amplitude of AON response and an observer's familiarity with an observed or executed movement. On the other hand, emerging evidence shows patterns of AON activity counter to these findings, whereby in some circumstances, unfamiliar actions lead to greater AON engagement than familiar actions. In an attempt to reconcile these conflicting findings, some have proposed that the relationship between AON response amplitude and action familiarity is nonlinear in nature. In the present study, we used an elaborate guitar training intervention to probe the relationship between movement familiarity and AON engagement during action execution and action observation tasks. Participants underwent fMRI scanning while executing one set of guitar sequences with a scanner-compatible bass guitar and observing a second set of sequences. Participants then acquired further physical practice or observational experience with half of these stimuli outside the scanner across 3 days. Participants then returned for an identical scanning session, wherein they executed and observed equal numbers of familiar (trained) and unfamiliar (untrained) guitar sequences. Via region of interest analyses, we extracted activity within AON regions engaged during both scanning sessions, and then fit linear, quadratic and cubic regression models to these data. The data best support the cubic regression models, suggesting that the response profile within key sensorimotor brain regions associated with the AON respond to action familiarity in a nonlinear manner. Moreover, by probing the subjective nature of the prediction error signal, we show results consistent with a predictive coding account of AON engagement during action observation and execution that also takes into account effects of changes in neural efficiency."
}
@article{WICKENS2018110,
title = "Multiple “Lower BAC” offenders: Characteristics and response to remedial interventions",
journal = "Accident Analysis & Prevention",
volume = "115",
pages = "110 - 117",
year = "2018",
issn = "0001-4575",
doi = "https://doi.org/10.1016/j.aap.2018.02.019",
url = "http://www.sciencedirect.com/science/article/pii/S0001457518300812",
author = "Christine M. Wickens and Rosely Flam-Zalcman and Gina Stoduto and Chloe Docherty and Rita K. Thomas and Tara Marie Watson and Justin Matheson and Kamna Mehra and Robert E. Mann",
keywords = "Drink-driving, Remedial measures, Blood alcohol concentration, Recidivism",
abstract = "Background
In recent years, there has been increasing attention to “lower BAC” drinking drivers, typically those whose blood alcohol content (BAC) is under the legal limits defined in criminal law. In 2009, legislation was enacted in Ontario, Canada that enabled police to issue roadside license suspensions to individuals caught driving with BAC between 0.05% and 0.08%, known as the “warn range”. Multiple warn range (MWR) offenders are required to attend the Back on Track (BOT) remedial measures program. This study aimed to provide: (1) a preliminary characterization of MWR drivers charged under warn range legislation; and (2) an initial assessment of outcomes associated with BOT participation among MWR offenders.
Methods
A subsample of 727 MWR offenders was drawn from program records, and compared to samples of 3597 first-time Criminal Code (CC) offenders (those caught driving with a BAC of 0.08% or higher) and 359 second-time CC offenders. To provide an initial assessment of outcomes associated with BOT participation, another subsample consisted of 394 MWR participants from whom pre- and post-workshop questionnaires were collected and successfully matched using probabilistic matching processes.
Results
Similarities in demographic profile and driving history between MWR and first-time CC participants were apparent. MWR offenders scored higher on risk of problem drinking and drink-driving recidivism than either of the CC offender groups. Second-time CC offenders scored higher on these measures than first-time CC offenders. Following BOT participation, MWR participants demonstrated positive change including improved knowledge of and intentions to avoid drink-driving.
Conclusions
MWR offenders share a similar demographic profile to that of first-time CC offenders and they report significantly higher risk of problem drinking and recidivism. MWR offenders may include high-functioning problem drinkers who are likely to continue drink-driving and who may escalate to a CC drink-driving offense. Like CC offenders, MWR offenders benefited from BOT participation."
}
@article{PILKIW201857,
title = "Neural representations of time-linked memory",
journal = "Neurobiology of Learning and Memory",
volume = "153",
pages = "57 - 70",
year = "2018",
note = "MCCS 2018: Time and Memory",
issn = "1074-7427",
doi = "https://doi.org/10.1016/j.nlm.2018.03.024",
url = "http://www.sciencedirect.com/science/article/pii/S1074742718300789",
author = "Maryna Pilkiw and Kaori Takehara-Nishiuchi",
keywords = "Time, Episodic memory, Temporal association, Neural code, Single-unit activity, Ensemble decoding",
abstract = "Many cognitive processes, such as episodic memory and decision making, rely on the ability to form associations between two events that occur separately in time. The formation of such temporal associations depends on neural representations of three types of information: what has been presented (trace holding), what will follow (temporal expectation), and when the following event will occur (explicit timing). The present review seeks to link these representations with firing patterns of single neurons recorded while rodents and non-human primates associate stimuli, outcomes, and motor responses over time intervals. Across these studies, two distinct firing patterns were observed in the hippocampus, neocortex, and striatum: some neurons change firing rates during or shortly after the stimulus presentation and sustain the firing rate stably or sidlingly during the subsequent intervals (tonic firings). Other neurons transiently change firing rates during a specific moment within the time intervals (phasic firings), and as a group, they form a sequential firing pattern that covers the entire interval. Clever task designs used in some of these studies collectively provide evidence that both tonic and phasic firing responses represent trace holding, temporal expectation, and explicit timing. Subsequently, we applied machine-learning based classification approaches to the two firing patterns within the same dataset collected from rat medial prefrontal cortex during trace eyeblink conditioning. This quantitative analysis revealed that phasic-firing patterns showed greater selectivity for stimulus identity and temporal position than tonic-firing patterns. Our summary illuminates distributed neural representations of temporal association in the forebrain and generates several ideas for future investigations."
}
@article{WOJNOWICZ2016130,
title = "Wavelet decomposition of software entropy reveals symptoms of malicious code",
journal = "Journal of Innovation in Digital Ecosystems",
volume = "3",
number = "2",
pages = "130 - 140",
year = "2016",
issn = "2352-6645",
doi = "https://doi.org/10.1016/j.jides.2016.10.009",
url = "http://www.sciencedirect.com/science/article/pii/S2352664516300220",
author = "Michael Wojnowicz and Glenn Chisholm and Matt Wolff and Xuan Zhao",
keywords = "Wavelet decomposition, Structural entropy, Malware detection, Parasitic malware, Machine learning",
abstract = "Sophisticated malware authors can sneak hidden malicious contents into portable executable files, and this contents can be hard to detect, especially if encrypted or compressed. However, when an executable file switches between contents regimes (e.g., native, encrypted, compressed, text, and padding), there are corresponding shifts in the file’s representation as an entropy signal. In this paper, we develop a method for automatically quantifying the extent to which patterned variations in a file’s entropy signal make it “suspicious”. In Experiment 1, we use wavelet transforms to define a Suspiciously Structured Entropic Change Score (SSECS), a scalar feature that quantifies the suspiciousness of a file based on its distribution of entropic energy across multiple levels of spatial resolution. Based on this single feature, it was possible to raise predictive accuracy on a malware detection task from 50.0% to 68.7%, even though the single feature was applied to a heterogeneous corpus of malware discovered “in the wild”. In Experiment 2, we describe how wavelet-based decompositions of software entropy can be applied to a parasitic malware detection task involving large numbers of samples and features. By extracting only string and entropy features (with wavelet decompositions) from software samples, we are able to obtain almost 99% detection of parasitic malware with fewer than 1% false positives on good files. Moreover, the addition of wavelet-based features uniformly improved detection performance across plausible false positive rates, both in a strings-only model (e.g., from 80.90% to 82.97%) and a strings-plus-entropy model (e.g. from 92.10% to 94.74%, and from 98.63% to 98.90%). Overall, wavelet decomposition of software entropy can be useful for machine learning models for detecting malware based on extracting millions of features from executable files."
}
@article{GREENBERG201516,
title = "Using concept maps to provide an integrative framework for teaching the cost or managerial accounting course",
journal = "Journal of Accounting Education",
volume = "33",
number = "1",
pages = "16 - 35",
year = "2015",
issn = "0748-5751",
doi = "https://doi.org/10.1016/j.jaccedu.2014.11.001",
url = "http://www.sciencedirect.com/science/article/pii/S0748575114000967",
author = "Rochelle Kaplan Greenberg and Neil A. Wilner",
keywords = "Concept maps, Product cost, Cost systems, Assimilation learning theory",
abstract = "Accounting students often perceive their beginning cost or managerial course as lacking a framework with which they can organize the material. They have usually completed a beginning Financial Accounting course recently where the accounting equation provides such a framework. This makes the lack of a framework in their next course even more problematical. This paper provides a framework for integrating topics in the cost/managerial course to enhance learning. The framework uses hierarchical concept maps as the integrating mechanism. Concept maps provide a visual presentation method based on the theories of learning and knowledge. Assimilation learning theory posits that the difference between meaningful learning and rote learning depends upon whether or not the new information is integrated with, and connected to, existing knowledge. Meaningful learning takes place most easily when broader concepts are presented first and detailed ones that provide support are furnished later. Hierarchical concept maps are organized in such a fashion, with the more general, inclusive concepts at the top of the map, and progressively more specific concepts arranged below them. Concept maps are a way to develop logical thinking and study skills by revealing connections and helping students see how individual ideas form a larger whole. The paper provides a detailed discussion of how to proceed with the concept maps as building blocks in the course. This can help the instructor organize their presentation in a logical manner aimed at enhancing student learning. We also provide a comprehensive numerical example to reinforce the process over the course of the semester."
}
@article{BLANC2015180,
title = "Hepatocellular adenoma management: Call for shared guidelines and multidisciplinary approach",
journal = "Clinics and Research in Hepatology and Gastroenterology",
volume = "39",
number = "2",
pages = "180 - 187",
year = "2015",
issn = "2210-7401",
doi = "https://doi.org/10.1016/j.clinre.2014.10.003",
url = "http://www.sciencedirect.com/science/article/pii/S2210740114002472",
author = "Jean Frédéric Blanc and Nora Frulio and Laurence Chiche and Christine Sempoux and Laurence Annet and Catherine Hubert and Annette S.H. Gouw and Koert P. de Jong and Paulette Bioulac-Sage and Charles Balabaud",
abstract = "Summary
Hepatocellular adenomas are rare benign nodules developed mainly in women taking oral contraceptives. They are solitary or multiple. Their size is highly variable. There is no consensus in the literature for their management except that once their size exceeds 5cm nodules are taken out to prevent 2 major complications: bleeding and malignant transformation. There are exceptions particularly in men where it is recommended to remove smaller nodules. Since the beginning of this century, major scientific contributions have unveiled the heterogeneity of the disease. HCA are composed of four major subtypes. HNF1A (coding for hepatocyte nuclear factor 1a) inactivating mutations (H-HCA); inflammatory adenomas (IHCA); the β-catenin-mutated HCAs (β-HCA) and unclassified HCA (UHCA) occurring in 30–40%, 40–50%, 10–15% and 10% of all HCA, respectively. Half of β-HCAs are also inflammatory (β-IHCA). Importantly, β-catenin mutations are associated with a high risk of malignant transformation. HCA subtypes can be identified on liver tissue, including biopsies using specific immunomarkers with a good correspondence with molecular data. Recent data has shown that TERT promoter mutation was a late event in the malignant transformation of β-HCA, β-IHCA. Furthermore, in addition to β-catenin exon 3 mutations, other mutations do exist (exon 7 and 8) with a lower risk of malignant transformation. With these new scientific informations, we have the tools to better know the natural history of the different subtypes, in terms of growth, disappearance, bleeding, malignant transformation and to investigate HCA in diseased livers (vascular diseases, alcoholic cirrhosis). A better knowledge of HCA should lead to a more rational management of HCA. This can be done only if the different subspecialties, including hepatologists, liver pathologists, radiologists and surgeons work altogether in close relationship with molecular biologists. It is a long way to go."
}
@article{WEN201660,
title = "Influence of SKF38393 on changes of gene profile in rat prefrontal cortex during chronic paradoxical sleep deprivation",
journal = "Behavioural Brain Research",
volume = "304",
pages = "60 - 66",
year = "2016",
issn = "0166-4328",
doi = "https://doi.org/10.1016/j.bbr.2016.02.002",
url = "http://www.sciencedirect.com/science/article/pii/S0166432816300596",
author = "Xiaosa Wen and Xinmin Chen and Si Chen and Yue Tan and Fei Rong and Jiangbo Zhu and Wenling Ma",
keywords = "Chronic paradoxical sleep deprivation, Prefrontal cortex, D1 receptor, SKF38393, Signaling pathway, Gene chip",
abstract = "Chronic paradoxical sleep deprivation (CSD) can induce dramatic physiological and neurofunctional changes in rats, including decreased body weight, reduced learning and memory, and declined locomotor function. SKF38393, a dopamine D1 receptor agonist, can reverse the above damages. However, the mechanism of CSD syndrome and reversal role of SKF38393 remains largely unexplained. To preliminarily elucidate the mechanism of the neural dysfunction caused by CSD, in the present study we use gene chips to examine the expression profile of more than 28,000 transcripts in the prefrontal cortex (PFC). Rats were sleep deprived by modified multi-platform method for 3 weeks. Totally 59 transcripts showed differential expressions in CSD group in contrast to controls; they included transcripts coding for caffeine metabolism, circadian rhythm, drug metabolism and some amino acid metabolism pathway. Among the 59 transcripts, 39 increased their expression and 20 decreased. Two transcripts can be specifically reversed with SKF38393, one of them is Homer1, which is related to 20 functional classifications and coding for Glutamatergic synapse pathway. Our findings in the present study indicate that long-term sleep deprivation may trigger the changes of some certain functions and pathways in the PFC, and lead to the dysfunction of this advanced neuron, and the activation of D1 receptor by SKF38393 might ameliorate these changes via modulation of some transcripts such as Homer1, which is involved in the Ca2+ pathway and MAPK pathway related to Glutamatergic synapse pathway."
}
@article{LEIGHTLEY201817,
title = "Integrating electronic healthcare records of armed forces personnel: Developing a framework for evaluating health outcomes in England, Scotland and Wales",
journal = "International Journal of Medical Informatics",
volume = "113",
pages = "17 - 25",
year = "2018",
issn = "1386-5056",
doi = "https://doi.org/10.1016/j.ijmedinf.2018.02.012",
url = "http://www.sciencedirect.com/science/article/pii/S138650561830090X",
author = "Daniel Leightley and Zoe Chui and Margaret Jones and Sabine Landau and Paul McCrone and Richard D. Hayes and Simon Wessely and Nicola T. Fear and Laura Goodwin",
keywords = "Hospital episode statistics, Electronic health records, Hospital admission, Secondary care, Big data, Data linkage",
abstract = "Background
Electronic Healthcare Records (EHRs) are created to capture summaries of care and contact made to healthcare services. EHRs offer a means to analyse admissions to hospitals for epidemiological research. In the United Kingdom (UK), England, Scotland and Wales maintain separate data stores, which are administered and managed exclusively by devolved Government. This independence results in harmonisation challenges, not least lack of uniformity, making it difficult to evaluate care, diagnoses and treatment across the UK. To overcome this lack of uniformity, it is important to develop methods to integrate EHRs to provide a multi-nation dataset of health.
Objective
To develop and describe a method which integrates the EHRs of Armed Forces personnel in England, Scotland and Wales based on variable commonality to produce a multi-nation dataset of secondary health care.
Methods
An Armed Forces cohort was used to extract and integrate three EHR datasets, using commonality as the linkage point. This was achieved by evaluating and combining variables which shared the same characteristics. EHRs representing Accident and Emergency (A&E), Admitted Patient Care (APC) and Outpatient care were combined to create a patient-level history spanning three nations. Patient-level EHRs were examined to ascertain admission differences, common diagnoses and record completeness.
Results
A total of 6,336 Armed Forces personnel were matched, of which 5,460 personnel had 7,510 A&E visits, 9,316 APC episodes and 45,005 Outpatient appointments. We observed full completeness for diagnoses in APC, whereas Outpatient admissions were sparsely coded; with 88% of diagnoses coded as “Unknown/unspecified cause of morbidity”. In addition, A&E records were sporadically coded; we found five coding systems for identifying reason for admission.
Conclusion
At present, EHRs are designed to monitor the cost of treatment, enable administrative oversight, and are not currently suited to epidemiological research. However, only small changes may be needed to take advantage of what should be a highly cost-effective means of delivering important research for the benefit of the NHS."
}
@article{DOBRY2018374,
title = "Calibration of non-linear effective stress code for seismic analysis of excess pore pressures and liquefaction in the free field",
journal = "Soil Dynamics and Earthquake Engineering",
volume = "107",
pages = "374 - 389",
year = "2018",
issn = "0267-7261",
doi = "https://doi.org/10.1016/j.soildyn.2018.01.029",
url = "http://www.sciencedirect.com/science/article/pii/S0267726117300301",
author = "R. Dobry and W. El-Sekelly and T. Abdoun",
abstract = "The paper presents numerical predictions of excess pore pressure, liquefaction and settlement response of four centrifuge model tests of 6m uniform deposits of saturated clean Ottawa sand, placed by dry pluviation and having a relative density ranging from 38% to 66%. The deposits were subjected to 1D uniform base shaking consisting of 10–15 cycles of peak acceleration ranging from 0.04 to 0.12g. All predictions were conducted with the nonlinear effective stress numerical code Dmod2000. Significant effort was spent in calibrating Dmod2000 by matching the pore pressure and settlement measurements of the first shaking (S1) of a series of shakings conducted in centrifuge Experiment 3. This resulted in very good predictions of both pore pressures and settlement measured in this shaking S1. The exercise showed the importance for realistic simulations of having the correct soil compressibility and permeability. This calibrated version of Dmod2000 was used for a good pore pressure prediction of the preshaken deposit in the same Experiment 3 (S36), by modifying only one parameter in the undrained pore pressure model; and also well predicted pore pressure responses in Tests FFV3 and PFV1, without any change in the parameters of Dmod2000 except for use of the new input motions (Type B predictions). The experimental and numerical results showed that both cyclic shear stress/strains and upward water flow determine together the pore pressure buildup and liquefaction phenomena. The soil response is partially drained rather than undrained, and pore pressure dissipation does take place during shaking both before and after liquefaction occurs."
}
@article{FENG2017101,
title = "An evolution model for elliptic-cylindrical void in viscous materials considering the evolvements of void shape and orientation",
journal = "Mechanics of Materials",
volume = "112",
pages = "101 - 113",
year = "2017",
issn = "0167-6636",
doi = "https://doi.org/10.1016/j.mechmat.2017.06.002",
url = "http://www.sciencedirect.com/science/article/pii/S0167663617304027",
author = "Chao Feng and Zhenshan Cui and Xiaoqing Shang and Mingxiang Liu",
keywords = "Void evolution, Shape and orientation, Representative volume element, Large ingot, Viscous materials",
abstract = "The evolution behavior of the internal voids influences the mechanical property of the materials significantly. Considering the void deformation and rotation, the evolution behavior of the elliptic-cylindrical void in power-law viscous materials was investigated by using the representative volume element (RVE) model. The rigid visco-plastic finite element (FE) method was applied to calculate the velocity field in the RVE under different loading conditions, and the instantaneous changing rate of the void radius and orientation were determined by evaluating the evolving of the void profiles at the instant. The calculated results show the deviatoric stress takes an important role in the void radius evolution, and the shear stress influences the change of the void orientation significantly. Based on the investigation, a void evolution model was established to relate the changing rates of the void radius and orientation to the void aspect ratio and the macroscopic stress/strain conditions. This model was incorporated into the FE code to predict the evolvements of void radius and orientation in each step of the deformation history. The predictions agree well with the results of the numerical simulations containing embedded void geometries in the mesh, which demonstrates that this model is capable to evaluate the void evolution behavior under large deformation. As an application, this model was used to predict the closure behavior of the void defects in the large ingot during the hot forging process."
}
@article{BUSHUYEV201361,
title = "Proactive Program Management for Development National Finance System in Turbulence Environment",
journal = "Procedia - Social and Behavioral Sciences",
volume = "74",
pages = "61 - 70",
year = "2013",
note = "Selected papers from the 26th IPMA (International Project Management Association), World Congress, Crete, Greece, 2012",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2013.03.044",
url = "http://www.sciencedirect.com/science/article/pii/S1877042813004734",
author = "Sergey Bushuyev and Ruslan Jaroshenko",
keywords = "Public finance system, The keys of success, Reforming, Implementation, The finance policy paradigm",
abstract = "The global financial crisis has become a powerful impulse for the initiation a number of development programs in public finance system in many countries. Every time when we come to large-scale changes (such as those associated with the project “Reboot the public finance system of Ukraine”) it is difficult to determine all assumptions and driving forces for changes or to assess their impact. The program of innovation development, in this case carrying out the “reboot” of public finance, is being developed and implemented by the Ministry of Finance of Ukraine on the basis of evaluation of its positive impact on economic development, with effective interaction with the environment in order to create innovative products such as the Budget Code, Tax Code, Virtual University, distance learning courses for public officials, system of independent assessment of the competence of staff within the regulated processes. Products and results of the program are evaluated according to profile of values, specified in the program of President of Ukraine and supported by the people of Ukraine. The necessity of the success program management in the reformation of complicated systems, especially the finance system of Ukraine is discussed in the report. The authors suggest seven keys of success methodology, which make it possible to elaborate the strategy of reforming the domestic finance system in turbulence environment. The issues of measuring the force of the impact of innovation and change on the organizational system remain open and will be discussed."
}
@article{CERRI2018274,
title = "The Bartolomeo Ammannati’s Fountain: an artifact in progress",
journal = "Procedia Structural Integrity",
volume = "11",
pages = "274 - 281",
year = "2018",
note = "XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018",
issn = "2452-3216",
doi = "https://doi.org/10.1016/j.prostr.2018.11.036",
url = "http://www.sciencedirect.com/science/article/pii/S2452321618301379",
author = "Giada Cerri and Giacomo Pirazzoli and Giorgio Verdiani and Marco Tanganelli and Vieri Cardinali and Stefania Viti",
keywords = "Juno fountain, rigid block analysis, sculptures by Ammannati, Bargello Museum, seismic assessment of artifacts",
abstract = "Artifacts are not only fundamental evidences of our history and culture, but they are even entities having a proper life. The present research focuses on Bartolomeo Ammannati’s Juno Fountain (1555) – a Late-Renaissance masterpiece whose eventful story made it moving around from its planned site, the Sala Grande in Florentine Palazzo Vecchio, to Pratolino Park, then to Boboli Garden. Finally, current fragments re-assembling and museography staging under the vaults of the National Museum of Bargello court in Florence has been set up a few years ago on the 5th centenary of Ammannati’s birthdate – after careful historical research about the many vicissitudes of the Fountain. Although there isn’t any location change expected for this Ammannati’s artwork, investigations and researches are going on. Namely, the seismic performance of the reconstructed Fountain is to be checked with reference to the seismic hazard of the site, as provided by the Italian Code classification. To this objective, the previously done laser scanning which allowed a three-dimensional digital modeling to help re-assembling the Fountain, has been now adopted to perform the structural analysis. Consequently, a structural evaluation to check the setting’s seismic behavior is currently under process. The research, developed by joining different knowledges and fields, is an example of the importance of a multidisciplinary approach for preserving artifacts and museums’ collections."
}
@article{BLECHL2013496,
title = "Variant high-molecular-weight glutenin subunits arising from biolistic transformation of wheat",
journal = "Journal of Cereal Science",
volume = "57",
number = "3",
pages = "496 - 503",
year = "2013",
issn = "0733-5210",
doi = "https://doi.org/10.1016/j.jcs.2013.02.005",
url = "http://www.sciencedirect.com/science/article/pii/S0733521013000362",
author = "Ann E. Blechl and William H. Vensel",
keywords = "Transgenic, 1Dx5, 1Dy10, Tandem mass spectrometry",
abstract = "Genetic transformation via the biolistic method has been used to introduce genes encoding natural and novel high-molecular-weight glutenin subunits (HMW-GS) into wheat. The appearance of new seed proteins of sizes not predicted by the transgene coding sequences was noted in some experiments. In this report, the identities of thirteen of these novel proteins were determined by tandem mass spectrometry (MS/MS). Seven different proteins larger than and two proteins smaller than the native protein were shown to contain peptides from 1Dx5. A novel protein found in some progeny of crosses between a transgenic plant and Great Plains winter wheats was larger than but contained several peptides from 1Dy10. In one line, a protein larger than and a protein smaller than HMW-GS each contained peptides from the N- and C-terminus of 1Dx5 and from the repeat region of 1Dy10. In a sixth transgenic line, the native Bx7 gene was apparently replaced by a gene that encodes a larger version of 1Bx7. The variant proteins accumulate in the polymeric protein fraction, indicating that they can form inter-molecular disulfide bonds. These results show that novel proteins found in some transformants are encoded by altered versions of either the transforming or endogenous HMW-GS genes."
}
@article{LU2012338,
title = "Empirical performance model-driven data layout optimization and library call selection for tensor contraction expressions",
journal = "Journal of Parallel and Distributed Computing",
volume = "72",
number = "3",
pages = "338 - 352",
year = "2012",
issn = "0743-7315",
doi = "https://doi.org/10.1016/j.jpdc.2011.09.006",
url = "http://www.sciencedirect.com/science/article/pii/S0743731511002255",
author = "Qingda Lu and Xiaoyang Gao and Sriram Krishnamoorthy and Gerald Baumgartner and J. Ramanujam and P. Sadayappan",
keywords = "Data layout optimization, Library call selection, Compiler optimization, Tensor contractions",
abstract = "Empirical optimizers like ATLAS have been very effective in optimizing computational kernels in libraries. The best choice of parameters such as tile size and degree of loop unrolling is determined in ATLAS by executing different versions of the computation. In contrast, optimizing compilers use a model-driven approach to program transformation. While the model-driven approach of optimizing compilers is generally orders of magnitude faster than ATLAS-like library generators, its effectiveness can be limited by the accuracy of the performance models used. In this paper, we describe an approach where a class of computations is modeled in terms of constituent operations that are empirically measured, thereby allowing modeling of the overall execution time. The performance model with empirically determined cost components is used to select library calls and choose data layout transformations in the context of the Tensor Contraction Engine, a compiler for a high-level domain-specific language for expressing computational models in quantum chemistry. The effectiveness of the approach is demonstrated through experimental measurements on representative computations from quantum chemistry."
}
@article{SHARIYAT2018157,
title = "A new analytical solution and novel energy formulations for non-linear eccentric impact analysis of composite multi-layer/sandwich plates resting on point supports",
journal = "Thin-Walled Structures",
volume = "127",
pages = "157 - 168",
year = "2018",
issn = "0263-8231",
doi = "https://doi.org/10.1016/j.tws.2018.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S0263823117313575",
author = "M. Shariyat and M. Roshanfar",
keywords = "Analytical approach, Multi-layered/Sandwich plate, Point support, Eccentric impact, Energy approaches",
abstract = "In the present research, an analytical solution based on a new idea of superposition of two kinematic descriptions is presented for dynamic response analysis of a multi-layer/sandwich composite plate with point supports subjected to an eccentric low-velocity impact. Direct and virtual-work-based novel energy formulations are proposed for the problem that take into account the potential energy of the indentation region. The nonlinear governing equations of motions are found based on minimization of the total potential energy of the whole mass-plate system, including work of the inertia forces, employing Ritz technique and transformation of the time-dependent nonlinear system of governing equations to a non-linear algebraic one through a novel concept. In contrast to the available researches, influence of the lower layers on the stiffness of the contact region is incorporated. Time-dependent responses of a sandwich composite plate with simply supported edges are compared with those of a plate resting on point supports. Verification of the results has been accomplished based on results of ABAQUS computer code. In the results section, the significant effects of the point supports (in comparison to the complete edge supports), initial velocity of the indenter, aspect ratio of the plate, and material properties of the layers on time histories of the contact force and lateral deflection of the plate are investigated."
}
@article{FAN2017399,
title = "A dynamic framework based on local Zernike moment and motion history image for facial expression recognition",
journal = "Pattern Recognition",
volume = "64",
pages = "399 - 406",
year = "2017",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2016.12.002",
url = "http://www.sciencedirect.com/science/article/pii/S0031320316303892",
author = "Xijian Fan and Tardi Tjahjadi",
keywords = "Zernike moment, Facial expression, Motion history image, Entropy, Feature extraction",
abstract = "A dynamic descriptor facilitates robust recognition of facial expressions in video sequences. The current two main approaches to the recognition are basic emotion recognition and recognition based on facial action coding system (FACS) action units. In this paper we focus on basic emotion recognition and propose a spatio-temporal feature based on local Zernike moment in the spatial domain using motion change frequency. We also design a dynamic feature comprising motion history image and entropy. To recognise a facial expression, a weighting strategy based on the latter feature and sub-division of the image frame is applied to the former to enhance the dynamic information of facial expression, and followed by the application of the classical support vector machine. Experiments on the CK+ and MMI datasets using leave-one-out cross validation scheme demonstrate that the integrated framework achieves a better performance than using individual descriptor separately. Compared with six state-of-arts methods, the proposed framework demonstrates a superior performance."
}
@article{HERRERAIBATA201520,
title = "Mapping chemical structure-activity information of HAART-drug cocktails over complex networks of AIDS epidemiology and socioeconomic data of U.S. counties",
journal = "Biosystems",
volume = "132-133",
pages = "20 - 34",
year = "2015",
issn = "0303-2647",
doi = "https://doi.org/10.1016/j.biosystems.2015.04.007",
url = "http://www.sciencedirect.com/science/article/pii/S0303264715000659",
author = "Diana María Herrera-Ibatá and Alejandro Pazos and Ricardo Alfredo Orbegozo-Medina and Francisco Javier Romero-Durán and Humberto González-Díaz",
keywords = "Urban influence code, AIDS epidemiology, Box–Jenkins operators, Shannon entropy, Information theory",
abstract = "Using computational algorithms to design tailored drug cocktails for highly active antiretroviral therapy (HAART) on specific populations is a goal of major importance for both pharmaceutical industry and public health policy institutions. New combinations of compounds need to be predicted in order to design HAART cocktails. On the one hand, there are the biomolecular factors related to the drugs in the cocktail (experimental measure, chemical structure, drug target, assay organisms, etc.); on the other hand, there are the socioeconomic factors of the specific population (income inequalities, employment levels, fiscal pressure, education, migration, population structure, etc.) to study the relationship between the socioeconomic status and the disease. In this context, machine learning algorithms, able to seek models for problems with multi-source data, have to be used. In this work, the first artificial neural network (ANN) model is proposed for the prediction of HAART cocktails, to halt AIDS on epidemic networks of U.S. counties using information indices that codify both biomolecular and several socioeconomic factors. The data was obtained from at least three major sources. The first dataset included assays of anti-HIV chemical compounds released to ChEMBL. The second dataset is the AIDSVu database of Emory University. AIDSVu compiled AIDS prevalence for >2300 U.S. counties. The third data set included socioeconomic data from the U.S. Census Bureau. Three scales or levels were employed to group the counties according to the location or population structure codes: state, rural urban continuum code (RUCC) and urban influence code (UIC). An analysis of >130,000 pairs (network links) was performed, corresponding to AIDS prevalence in 2310 counties in U.S. vs. drug cocktails made up of combinations of ChEMBL results for 21,582 unique drugs, 9 viral or human protein targets, 4856 protocols, and 10 possible experimental measures. The best model found with the original data was a linear neural network (LNN) with AUROC>0.80 and accuracy, specificity, and sensitivity≈77% in training and external validation series. The change of the spatial and population structure scale (State, UIC, or RUCC codes) does not affect the quality of the model. Unbalance was detected in all the models found comparing positive/negative cases and linear/non-linear model accuracy ratios. Using synthetic minority over-sampling technique (SMOTE), data pre-processing and machine-learning algorithms implemented into the WEKA software, more balanced models were found. In particular, a multilayer perceptron (MLP) with AUROC=97.4% and precision, recall, and F-measure >90% was found."
}
@article{ANDICS2013351,
title = "Mean-based neural coding of voices",
journal = "NeuroImage",
volume = "79",
pages = "351 - 360",
year = "2013",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2013.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S1053811913004928",
author = "Attila Andics and James M. McQueen and Karl Magnus Petersson",
keywords = "fMRI, Inferior frontal cortex, Prototype-centered representations, Superior temporal sulcus, Voice identity learning",
abstract = "The social significance of recognizing the person who talks to us is obvious, but the neural mechanisms that mediate talker identification are unclear. Regions along the bilateral superior temporal sulcus (STS) and the inferior frontal cortex (IFC) of the human brain are selective for voices, and they are sensitive to rapid voice changes. Although it has been proposed that voice recognition is supported by prototype-centered voice representations, the involvement of these category-selective cortical regions in the neural coding of such “mean voices” has not previously been demonstrated. Using fMRI in combination with a voice identity learning paradigm, we show that voice-selective regions are involved in the mean-based coding of voice identities. Voice typicality is encoded on a supra-individual level in the right STS along a stimulus-dependent, identity-independent (i.e., voice-acoustic) dimension, and on an intra-individual level in the right IFC along a stimulus-independent, identity-dependent (i.e., voice identity) dimension. Voice recognition therefore entails at least two anatomically separable stages, each characterized by neural mechanisms that reference the central tendencies of voice categories."
}
@article{PEPPER2014337,
title = "The Effect of New Duty Hours on Resident Academic Performance and Adult Resuscitation Outcomes",
journal = "The American Journal of Medicine",
volume = "127",
number = "4",
pages = "337 - 342",
year = "2014",
issn = "0002-9343",
doi = "https://doi.org/10.1016/j.amjmed.2013.12.007",
url = "http://www.sciencedirect.com/science/article/pii/S0002934313010759",
author = "Dominique J. Pepper and Michelle Schweinfurth and Vincent E. Herrin",
keywords = "ACGME, Emergency, Outcome, Resident, Resuscitation",
abstract = "Background
From July 2011, the Accreditation Council for Graduate Medical Education implemented new resident duty hours throughout the US. This study aimed to determine whether changes to call schedules due to these new duty hours achieved the intended goals of excellent patient care and improved resident learning.
Methods
We conducted a retrospective cohort study at an academic hospital. For patient outcomes, we used the hospital registry for code blues and rapid responses to compare the proportion of deaths and transfers to an intensive care unit (July 2010 to June 2011; July 2011 to June 2012). For resident learning, we compared delta percentage scores for annual in-service training examinations (2009 to 2010; 2010 to 2011; 2011 to 2012).
Results
We recorded 187 code blues and 469 rapid responses during the 2-year period: 48 (7.3%) deaths, 374 (57.0%) transfers to the intensive care unit, and 234 (35.7%) stabilizations on the floor. Of all transfers to the intensive care unit, those due to a code blue decreased after implementation of the new duty hours (36% [63/174] vs 25% [49/200], P = .02; adjusted odds ratio = 0.59; 95% confidence interval, 0.37-0.92). The median (interquartile range) delta percentage scores for annual in-service training examinations decreased significantly from the first time-period (2009 to 2010: 7 [4-11]) to the third time-period (2011 to 2012: 5 [2-8], P = .02).
Conclusion
We observed a reduced proportion of transfers to the intensive care unit with a code blue after implementation of new resident duty hours. Resident academic performance experienced a small but significant decrease in in-service training examination delta percentage score. We need large, multicenter studies to corroborate these findings."
}
@article{RODGER20147005,
title = "Application of a Fuzzy Feasibility Bayesian Probabilistic Estimation of supply chain backorder aging, unfilled backorders, and customer wait time using stochastic simulation with Markov blankets",
journal = "Expert Systems with Applications",
volume = "41",
number = "16",
pages = "7005 - 7022",
year = "2014",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2014.05.012",
url = "http://www.sciencedirect.com/science/article/pii/S0957417414002929",
author = "James A. Rodger",
keywords = "Supply chain, Decision support system, Bayesian network, Backorder, Fuzzy logic, Markov blanket",
abstract = "Because supply chains are complex systems prone to uncertainty, statistical analysis is a useful tool for capturing their dynamics. Using data on acquisition history and data from case study reports, we used regression analysis to predict backorder aging using National Item Identification Numbers (NIINs) as unique identifiers. More than 56,000 NIINs were identified and used in the analysis. Bayesian analysis was then used to further investigate the NIIN component variables. The results indicated that it is statistically feasible to predict whether an individual NIIN has the propensity to become a backordered item. This paper describes the structure of a Bayesian network from a real-world supply chain data set and then determines a posterior probability distribution for backorders using a stochastic simulation based on Markov blankets. Fuzzy clustering was used to produce a funnel diagram that demonstrates that the Acquisition Advice Code, Acquisition Method Suffix Code, Acquisition Method Code, and Controlled Inventory Item Code backorder performance metric of a trigger group dimension may change dramatically with variations in administrative lead time, production lead time, unit price, quantity ordered, and stock. Triggers must be updated regularly and smoothly to keep up with the changing state of the supply chain backorder trigger clusters of market sensitiveness, collaborative process integration, information drivers, and flexibility."
}
@article{ZHU2014292,
title = "TFOx: A versatile kinetic Monte Carlo program for simulations of island growth in three dimensions",
journal = "Computational Materials Science",
volume = "91",
pages = "292 - 302",
year = "2014",
issn = "0927-0256",
doi = "https://doi.org/10.1016/j.commatsci.2014.04.053",
url = "http://www.sciencedirect.com/science/article/pii/S092702561400295X",
author = "Qing Zhu and Chris Fleck and Wissam A. Saidi and Alan McGaughey and Judith C. Yang",
keywords = "Kinetic Monte Carlo, Film oxidation, Three dimensional, Potential gradient, Ehrlich–Schwöbel barrier",
abstract = "A three dimensional (3D) kinetic Monte Carlo (KMC) code has been developed that simulates the general behavior of the 3D irreversible nucleation and growth of epitaxial islands, as motivated by experimental observations of oxide nuclei formation and growth during the early stages of copper oxidation. This package was originally a versatile two dimensional (2D) KMC code [Thin Film Oxidation (TFOx)] that considered a variety of elementary steps, including deposition, adsorption, surface diffusion, aggregation, desorption, and substrate-mediated indirect interactions between static adatoms. We extended TFOx to describe 3D island growth. This new version of TFOx is composed of a C++ console program and Python graphical user interface (GUI), such that parameterized simulation, parallel execution, and 3D growth capabilities are feasible. We examined the effects of the potential gradient and the Ehrlich–Schwöbel barrier and found that the 3D island morphology is significantly influenced by the incorporation of these two factors."
}
@article{OLTEANU20141089,
title = "Working in the Second Life Environment - A Way for Enhancing Students’ Collaboration",
journal = "Procedia - Social and Behavioral Sciences",
volume = "141",
pages = "1089 - 1094",
year = "2014",
note = "4th World Conference on Learning Teaching and Educational Leadership (WCLTA-2013)",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2014.05.183",
url = "http://www.sciencedirect.com/science/article/pii/S1877042814036076",
author = "Radu Lucian Olteanu and Mihai Bîzoi and Gabriel Gorghiu and Ana-Maria Suduc",
keywords = "Technology-enhanced learning, creative collaboration, virtual environments, student's profile, on-line course, KA3-ICT project;",
abstract = "At present, virtual world environments have a huge potential on changing the manner in which people can interact, navigate on Internet, and make business. Due to the fact that the virtual world environments become more pervasive, it is important for the researchers to be deeply involved in the understanding of those spaces. The paper describes the main aspects met by tutors and students on working in the Second Life environment, during the on-line course “Designing Technology-Enhanced Learning”, organized in the frame of the European LLP-KA3 project: “Enabling Creative Collaboration through Supportive Technologies” (code 511733-LLP-1-2010-1-FI-KA3-KA3MP). Second Life was chosen due to the fact that provides a strong collaborative environment equipped with modern features: voice interactions, chat and instant messenger, expanding the ways of communication and the possibilities for collaboration. For many Romanian students who participated to the course, it was a unique experience - they were involved in the collaborative work, together with their colleagues from Finland, Estonia and Norway, but also engaged in learning processes that enhanced creative collaboration."
}
@article{ZHANG201537,
title = "New photometric investigation of eclipsing binary NSVS 10653195",
journal = "New Astronomy",
volume = "41",
pages = "37 - 41",
year = "2015",
issn = "1384-1076",
doi = "https://doi.org/10.1016/j.newast.2015.04.009",
url = "http://www.sciencedirect.com/science/article/pii/S1384107615000512",
author = "B. Zhang and S.-B. Qian and W.-P. Liao and J. Zhang and N.-P. Liu and J.-J. Wang",
keywords = "Binary, Eclipsing binary, Orbital period, Light curve",
abstract = "New CCD photometric light curves of the low-mass binary system NSVS 10653195 are presented. Our complete B, V, Rc and Ic-band light curves show a remarkable out-eclipsing distortion. This phenomenon suggests that the components of the system may be active. The photometric solutions with star-spot were derived by using the 2013 version of the Wilson–Devinney (WD) code. Based on all available times of light minimum, we analyzed the orbital period changes. The O–C diagram reveals that the period of NSVS 10653195 is decreasing at a rate of dP/dt=−2.79×10−7 days yr−1, which is probably caused by angular momentum loss."
}
@article{TANNER20166121,
title = "Modeling and simulation of air-assist atomizers with applications to food sprays",
journal = "Applied Mathematical Modelling",
volume = "40",
number = "11",
pages = "6121 - 6133",
year = "2016",
issn = "0307-904X",
doi = "https://doi.org/10.1016/j.apm.2016.01.048",
url = "http://www.sciencedirect.com/science/article/pii/S0307904X16300440",
author = "Franz X. Tanner and Kathleen Feigl and Ossi Kaario and Erich J. Windhab",
keywords = "Air-assist atomization, Drop breakup modeling, CAB model, Food sprays, Emulsions",
abstract = "The Cascade Atomization and Drop Breakup (CAB) model has been developed originally for pressure atomizers. In this study, the CAB model is modified to accommodate air-assist atomization. The modifications include a change in the product drop distributions, namely, the uniform distribution used in the original CAB model is replaced with a χ-squared distribution with the same average drop size. The second modification addresses the air-assist atomization process. This process is modeled by estimating the Weber number due to the increased relative velocity caused by the air flow. Depending on the value of the Weber number this leads to a catastrophic, or a stripping (sheet-thinning), or a bag breakup. The model changes are validated with experimental data obtained from two different air-assist atomizers using an oil-in-water emulsion. The simulations were performed with a modified version of the KIVA-3 CFD code, and they showed good agreement with the experimental data."
}
@article{SINAIE2014465,
title = "Mechanical properties of cyclically-damaged structural mild steel at elevated temperatures",
journal = "Construction and Building Materials",
volume = "52",
pages = "465 - 472",
year = "2014",
issn = "0950-0618",
doi = "https://doi.org/10.1016/j.conbuildmat.2013.11.042",
url = "http://www.sciencedirect.com/science/article/pii/S095006181301074X",
author = "S. Sinaie and A. Heidarpour and X.L. Zhao",
keywords = "Mild steel, Elevated temperature, Cyclic loading, Mechanical properties, Post-earthquake fire",
abstract = "The mechanical response of a structural element not only depends on the inherent properties of the materials which constitute the element, but also on the history of any loads it had been previously subjected to. An important instance of this is the response of steel structures under post-earthquake fire. This research aims to investigate the potential changes that the mechanical properties of structural grade mild steel experience under such a loading sequence. The experimental results presented in this paper indicate that a prior history of cyclic loading significantly affects the proceeding ductility and strength of grade 300 steel at high temperatures. This implies that any history of cyclic loading should be included in the post-earthquake fire-resistant design of structures."
}
@article{SMITH2014211,
title = "Scuba diving tourism with critically endangered grey nurse sharks (Carcharias taurus) off eastern Australia: Tourist demographics, shark behaviour and diver compliance",
journal = "Tourism Management",
volume = "45",
pages = "211 - 225",
year = "2014",
issn = "0261-5177",
doi = "https://doi.org/10.1016/j.tourman.2014.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0261517714000958",
author = "K.R. Smith and C. Scarpaci and M.J. Scarr and N.M. Otway",
keywords = "Marine management, Shark, , Critically endangered, Impact, Behaviour, Regulations, Code of conduct",
abstract = "Guidelines and a national code of conduct were implemented to manage scuba diving tourism with the critically endangered grey nurse shark (Carcharias taurus) along the Australian east coast. The demographics of diving tourists, swimming behaviour of grey nurse sharks at various life-history stages and compliance of divers to the guidelines/code of conduct were simultaneously assessed during diver–shark interactions at four sites from March 2011 to February 2012. Milling was the most frequent swimming behaviour observed and no significant changes occurred with the number of divers or distance to sharks. Divers exhibited 100% compliance with all guidelines investigated. Satisfactory compliance may have been attributable to guideline clarity, the ease of establishing diver–shark interactions, stakeholder involvement in management processes and diver perceptions of sharks. Similar sampling of group and individual shark behaviour should be done to further enhance the understanding of the beneficial and adverse impacts of this marine wildlife tourism sector."
}
@article{PELLISSIER201853,
title = "Divergence of insulin superfamily ligands, receptors and Igf binding proteins in marine versus freshwater stickleback: Evidence of selection in known and novel genes",
journal = "Comparative Biochemistry and Physiology Part D: Genomics and Proteomics",
volume = "25",
pages = "53 - 61",
year = "2018",
issn = "1744-117X",
doi = "https://doi.org/10.1016/j.cbd.2017.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S1744117X17300898",
author = "Tim Pellissier and Hend Al Nafea and Sara V. Good",
abstract = "Three-spine stickleback (Gasterosteus aculeatus) is a teleost model for understanding genetic, physiological and morphological changes accompanying freshwater (FW) adaptation. There is growing evidence that the insulin superfamily plays important roles in traits involved in marine and FW adaptation. We performed a candidate gene analysis to look for evidence of selection on 33 insulin superfamily ligand-receptor genes and insulin-like growth factor binding proteins (Igfbp's) in stickleback. Using genotype data from 11 marine and 10 FW populations, we calculated the number of SNPs per site in regulatory and intronic regions, the number of synonymous and nonsynonymous mutations in coding regions, Wright's fixation index (Fst), and performed t-tests to identify SNPs with divergent genotype frequencies between marine/FW versus Atlantic/Pacific populations. Next, we analysed genome-wide transcriptome data from eight tissues to assess differential gene expression. Two Igfbp's (Igfbp2a and Igfbp5a) show evidence of divergent adaptation between life-history types, and a cluster of nonsynonymous mutations in Igfbp5a exhibit high Fst in exons apparently alternatively spliced in gill. We find evidence of selection on the relaxin family ligand-receptor gene pair, Insl3-Rxfp2, known to be involved in male spermatogenesis and bone metabolism, and in the 5′ regulatory region of Igf2. We also confirmed the gene and coding sequence of two unannotated relaxin family ligands. These analyses underscore the utility of candidate gene studies and indicate directions for further exploration of the function of insulin superfamily genes in FW adaptation."
}
@article{ALHASROUTY201842,
title = "Adaptive multicast streaming for videoconferences on software-defined networks",
journal = "Computer Communications",
volume = "132",
pages = "42 - 55",
year = "2018",
issn = "0140-3664",
doi = "https://doi.org/10.1016/j.comcom.2018.09.009",
url = "http://www.sciencedirect.com/science/article/pii/S0140366418304286",
author = "Christelle Al Hasrouty and Mohamed Lamine Lamali and Vincent Autefage and Cristian Olariu and Damien Magoni and John Murphy",
keywords = "software defined networking, Videoconference, Multicast, Scalable video coding, Quality of service",
abstract = "Real-time applications, such as video conferences, have strong Quality of Service requirements for ensuring a decent Quality of Experience. Nowadays, most of these conferences are performed over wireless devices. Thus, an appropriate management of both heterogeneous mobile devices and network dynamics is necessary. Software Defined Networking enables the use of multicasting and stream layering inside the network nodes, two techniques able to enhance the quality of live video streams. In this paper, we propose two algorithms for building and maintaining multicast sessions in a software-defined network. The first algorithm sets up the initial multicast trees for a given call. It optimally places the stream layer adaptation function inside the core network in order to minimize the bandwidth consumption. This algorithm has two versions: the first one, based on shortest path trees is minimizing the latency, while the second one, based on spanning trees is minimizing the bandwidth consumption. The second algorithm adapts the multicast trees according to the network changes occurring during a call. It does not recompute the trees, but only relocates the stream layer adaptation functions. It requires very low computation at the controller, thus making our proposal fast and highly reactive. Extensive simulation results confirm the efficiency of our solution in terms of processing time and bandwidth savings compared to existing solutions such as multiple unicast connections, Multipoint Control Unit solutions and application layer multicast."
}
@article{ZHINING2017103,
title = "The first photometric investigation of the neglected short period binary DY CVn",
journal = "New Astronomy",
volume = "54",
pages = "103 - 108",
year = "2017",
issn = "1384-1076",
doi = "https://doi.org/10.1016/j.newast.2017.01.010",
url = "http://www.sciencedirect.com/science/article/pii/S1384107616301889",
author = "Qu ZhiNing and Jiang LinQiao and Liu Jie and Hu YanFei and Yuan YuQuan",
keywords = "Stars: binaries: close, Stars: binaries : eclipsing, Stars: individual (DY CVn)",
abstract = "We present new photometric observations of the short period binary DY CVn. By using the Wilson-Devinney code of 2013 version, we found that DY CVn is a shallow contact binary (f=13.2%) with a high mass ratio of q=1.251. It is a W-subtype contact system where the more massive component is about 113 K cooler than the less massive one. From 44 available CCD times of light minimum collected from the references and 3 new ones in the present paper, the variation of the orbital period is studied. It is found that the O−C diagram shows a cyclic variation, and the most plausible explanation for this cyclic change is the light-travel time effect via a third body, since an outer third companion could play an important role for its formation by removing angular momentum from the central binary."
}
@article{CHAUDHARY2018104,
title = "Studies on jatropha oil pool fire",
journal = "Thermal Science and Engineering Progress",
volume = "6",
pages = "104 - 127",
year = "2018",
issn = "2451-9049",
doi = "https://doi.org/10.1016/j.tsep.2018.03.010",
url = "http://www.sciencedirect.com/science/article/pii/S2451904917304365",
author = "Avinash Chaudhary and Akhilesh Gupta and Surendra Kumar",
keywords = "Compartment fire, Jatropha oil pool fire, CFD modeling, Heat release rate",
abstract = "Fire experiments with jatropha oil are performed in a cubical compartment of volume 64 m3 to explore fire development and induced thermal environment. A door opening of size 2 m height and 1 m width is made on the front wall for ventilation purpose. Three fire experiments, with full door, half door and quarter door ventilation conditions, are conducted. Heat release rate (HRR), mass loss rate (MLR), profiles of flame temperature, room corner temperature, door temperature with velocity profiles and heat flux at different locations have been recorded. It is found that reducing the door ventilation from full door to quarter door results in change in average mass loss rate from 8.42 g/s to 6.82 g/s and fire remains overventilated type. Experiment performed under full door and half door ventilation are then simulated using CFD code, Fire Dynamics Simulator (FDS, version 6.2.0). Based on the simulations results of full door, half door ventilation recommended mesh size is 0.05 m–0.07 m and corresponding value of D∗/dx is in the range of 8.34 to 12."
}
@article{BARROS2019366,
title = "Seismic design of low-rise buildings based on frequent earthquake response spectrum",
journal = "Journal of Building Engineering",
volume = "21",
pages = "366 - 372",
year = "2019",
issn = "2352-7102",
doi = "https://doi.org/10.1016/j.jobe.2018.11.005",
url = "http://www.sciencedirect.com/science/article/pii/S235271021830665X",
author = "José Barros and Hernán Santa-María",
keywords = "Structural design, Performance-based seismic design, Immediate occupancy, Service level earthquake",
abstract = "Special provisions for the design of concrete frame structures to fulfill the immediate occupancy performance level for frequent (or low magnitude, or service) earthquakes are unavailable, or it is believed that drift control assures this compliance. This paper evaluates a different procedure for the structural design that guarantees an appropriate behavior for frequent (or service) earthquakes and for rare (or design) earthquakes, limited to short period concrete frame structures up to two stories. A two-story building with the typical architecture of a school is proposed as the study case. In order to compare the design procedure behavior, special moment frame (SMF) and intermediate moment frame (IMF) designs have been developed, as ASCE/SEI 7–16 and ACI 318-14 suggests in the regulations that the Ecuadorian code (NEC-2015) presents. An additional building is designed with the proposed procedure, for an earthquake with a return period of 43 years. To evaluate the behavior of the proposed structures, FEMA P-695 recommendations were followed, and non-linear models were developed using pushover and time history analyses. Results show for the buildings that were designed with the current regulations excessive demands in the non-structural elements expected for the frequent earthquake and that the proposed design methodology satisfies the behavior levels required by the regulations, without producing an abrupt change in the existing design procedure."
}
@article{KOLODZIEJ2015563,
title = "carlomat_3.0, an automatic tool for the electron–positron annihilation into hadrons at low energies",
journal = "Computer Physics Communications",
volume = "196",
pages = "563 - 568",
year = "2015",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2015.06.013",
url = "http://www.sciencedirect.com/science/article/pii/S0010465515002581",
author = "Karol Kołodziej",
keywords = "Automation of calculations, Monte Carlo programs, Electron–positron annihilation to hadrons at low energies, Effective models",
abstract = "A new version of carlomat that allows to generate automatically the Monte Carlo programs dedicated to the description of the processes e+e−→hadrons at low center-of-mass energies is presented. The program has been substantially modified in order to incorporate the photon–vector meson mixing terms and to make possible computation of the helicity amplitudes involving the Feynman interaction vertices of new tensor structures, like those predicted by the Resonance Chiral Theory or Hidden Local Symmetry model, and the effective Lagrangian of the electromagnetic interaction of the nucleons. Moreover, a number of new options have been introduced in the program in order to enable a better control over the effective models implemented. In particular, they offer a possibility to determine the dominant production mechanisms of the final state chosen by the user.
Program summary
Program title: carlomat, version 3.0 Catalogue identifier: AEDQ_v3_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDQ_v3_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 71596 No. of bytes in distributed program, including test data, etc.: 2804246 Distribution format: tar.gz Programming language: Fortran 90/95. Computer: All. Operating system: Linux. Catalogue identifier of previous version: AEDQ_v2_0 Journal reference of previous version: Comput. Phys. Comm. 185(2014)323 Classification: 4.4, 11.2, 11.6. Does the new version supersede the previous version?: Yes Nature of problem: Predictions for reactions of low energy e+e−-annihilation into final states containing pions, kaons, light vector mesons, one or more photons and light fermion pairs within the Standard Model and effective models inspired by the Resonance Chiral Theory or Hidden Local Symmetry model. Description of the electromagnetic production of nucleon pairs within the effective Lagrangian approach. Solution method: As in former versions, a program for the Monte Carlo (MC) simulation of e+e−→ hadrons at low energies is generated in a fully automatic way for a user specified process. However, the user is supposed to select a number of options and adjust arbitrary parameters in the main part of the MC computation program in order to obtain possibly the best description of experimental data. To this end, the user can also easily supplement her/his own formulae for s-dependent vector meson widths or running couplings by appropriately modifying corresponding subroutines. Reasons for new version: Processes of e+e−→ hadrons in the energy range below the J/Ψ threshold cannot be described in the framework of perturbative quantum chromodynamics. The scalar electrodynamics which has been implemented in carlomat 2.0 [1] does not provide a satisfactory description either. The most promising theoretical frameworks in this context are the Resonance Chiral Theory or Hidden Local Symmetry model which, among others, involve the photon–vector meson mixing and a number of vertices of rather complicated Lorentz tensor structure that is not present in the Standard Model or scalar QED. Already at low energies, the hadronic final states may consist of several particles, such as pions, kaons, or nucleons which can be accompanied by one or more photons, or light fermion pairs such as e+e−, or μ+μ−. The number of Feynman diagrams of such multiparticle reactions grows substantially with increasing numbers of interaction vertices and mixing terms of the effective models. Therefore, it is highly desirable to automatize the calculations. At the same time, new program options should provide the user with an easy way of implementing her/his own changes in the program in order to better fit the experimental data. Summary of revisions: The code generation part of the program has been substantially modified in order to incorporate the photon–vector meson mixing and calls to new subroutines for computation of the helicity amplitudes of the building blocks and complete Feynman diagrams which contain new interaction vertices and mixing terms. The subroutine library of carlomat has been extended to make possible computation of the helicity amplitudes involving the Feynman interaction vertices of new Lorentz tensor structures. Many subroutines have been modified in order to incorporate the q2-dependent couplings and vector meson widths. A number of options have been introduced in order to give a better control of the effective model implemented. Restrictions: As in previous versions of the program the number of particles is limited to 12 which exceeds typical numbers of particles of the exclusive low energy e+e−-annihilation processes. However, in the presence of photon–vector meson mixing, the Feynman diagrams proliferate, for example, with currently implemented Feynman rules, there are 90672 diagrams of e+e−→3(π+π−). Hence, the compilation time of generated code may become very long already for processes with a smaller number of the final state particles. Many couplings of the effective models are not known with good enough precision and must be adjusted in consecutive runs of the program in order to obtain a satisfactory description of the experimental data. Running time: Depends on the selected process. Typical running time for the code generation varies from a fraction of a second for, e.g.,  e+e−→π+π−K+K− to about 2 min for e+e−→3(π+π−). It may become substantially longer for processes with more particles in the final state. The execution time necessary to produce the appended test output files for e+e−→π+π−μ+μ−γ and e+e−→π+π−π+π−γ was 13s and 4s, respectively. The code generation for both processes took a fraction of a second time for each process. References:[1]K. Kolodziej, Comput. Phys. Commun. 185 (2014) 323."
}
@article{DENG2018662,
title = "A directional global sparse model for single image rain removal",
journal = "Applied Mathematical Modelling",
volume = "59",
pages = "662 - 679",
year = "2018",
issn = "0307-904X",
doi = "https://doi.org/10.1016/j.apm.2018.03.001",
url = "http://www.sciencedirect.com/science/article/pii/S0307904X18301069",
author = "Liang-Jian Deng and Ting-Zhu Huang and Xi-Le Zhao and Tai-Xiang Jiang",
keywords = "Single image rain removal, Directional sparse model, Alternating direction method of multipliers",
abstract = "Rain removal from a single image is an important issue in the fields of outdoor vision. Rain, a kind of bad weather that is often seen, usually causes complex local intensity changes in images and has negative impact on vision performance. Many existing rain removal approaches have been proposed recently, such as some dictionary learning-based methods and layer decomposition-based methods. Although these methods can improve the visibility of rain images, they fail to consider the intrinsic directional and structural information of rain streaks, thus usually leave undesired rain streaks or change the background intensity of rain-free region significantly. In the paper, we propose a simple but efficient method to remove rain streaks from a single rainy image. The proposed method formulates a global sparse model that involves three sparse terms by considering the intrinsic directional and structural knowledge of rain streaks, as well as the property of image background information. We employ alternating direction method of multipliers (ADMM) to solve the proposed convex model which guarantees the global optimal solution. Results on a variety of synthetic and real rainy images demonstrate that the proposed method outperforms two recent state-of-the-art rain removal methods. Moreover, the proposed method needs no training and requires much less computation significantly."
}
@article{PRZEWOZNICZEK201615,
title = "Active Multi-Population Pattern Searching Algorithm for flow optimization in computer networks – The novel coevolution schema combined with linkage learning",
journal = "Information Sciences",
volume = "355-356",
pages = "15 - 36",
year = "2016",
issn = "0020-0255",
doi = "https://doi.org/10.1016/j.ins.2016.02.048",
url = "http://www.sciencedirect.com/science/article/pii/S002002551630130X",
author = "Michal Przewozniczek",
keywords = "Evolutionary algorithms, Messy GA, Gene patterns, Linkage learning, Connection-oriented networks, Coevolution",
abstract = "The main objective of this paper is to propose an effective evolutionary method for solving the problem of working paths optimization in survivable MPLS network. The paper focuses on existing network, in which only network flow can be optimized to provide network survivability using the local repair strategy. The problem is NP-complete, the solution space of the test cases is large and many genes are required to code the potential solution. Recently, the MuPPetS method (Multi-Population Pattern Searching Algorithm for Flow Assignment) was proposed and seems to be a promising tool for tackling high-dimensional, hard optimization problems. The MuPPetS is a linkage learning method that minimizes the negative effects of typical EA bottlenecks, e.g., preconvergence and significant effectiveness dropdown caused by an increasing number of genes in the chromosome. In comparison to other evolutionary methods, the MuPPetS was shown to be effective and capable of solving GA-hard problems. Therefore, the proposed MuPPetS-FuN method (Multi-Population Pattern Searching Algorithm for Flow Assignment in Non-bifurcated Commodity Flow) is based on MuPPetS. The additional objective of this paper is to propose changes to general MuPPetS framework to increase its effectiveness via better subpopulation number control strategy."
}
@article{THAKUR2012365,
title = "Information Extraction from the Un-Structured Document using Grammatical Inference and Alignment Similarity",
journal = "Procedia Technology",
volume = "4",
pages = "365 - 369",
year = "2012",
note = "2nd International Conference on Computer, Communication, Control and Information Technology( C3IT-2012) on February 25 - 26, 2012",
issn = "2212-0173",
doi = "https://doi.org/10.1016/j.protcy.2012.05.056",
url = "http://www.sciencedirect.com/science/article/pii/S2212017312003350",
author = "Ramesh Thakur and Suresh Jain and Narendra S. Chaudhari and Rahul Singhai",
keywords = "Alignment profile, Learning systems, Knowledge discovery, Sequence Mining, Grammar Inference",
abstract = "Huge amount of information is available in un-structured (text) documents. Knowledge discovery in un-structured document has been recognized as promising task in the recent years. Since un-structured document is typically formatted for human viewing, it varies widely from document to document. Frequent changes made to their formatting further causes difficulty in construction of a global schema. So, Discovery of interesting rules form it is complex and tedious process. Most of the existing system uses hand-coded wrappers to extract information, which is monotonous and time consuming. In this paper we propose a novel and hybrid approach of learning (context-free) grammar rules that are based on alignment between texts. Also it automatically discovers the grammar rules using grammatical inference of repeated pattern present in un-structured (text) document. The generated rules can be used to infer the attribute value pairs from the unstructured text document."
}
@article{DEALBAAPARICIO201813,
title = "FSK-Lab – An open source food safety model integration tool",
journal = "Microbial Risk Analysis",
volume = "10",
pages = "13 - 19",
year = "2018",
note = "Special issue on 10th International Conference on Predictive Modelling in Food: Interdisciplinary Approaches and Decision-Making Tools in Microbial Risk Analysis",
issn = "2352-3522",
doi = "https://doi.org/10.1016/j.mran.2018.09.001",
url = "http://www.sciencedirect.com/science/article/pii/S2352352218300136",
author = "Miguel de Alba Aparicio and Tasja Buschhardt and Ahmad Swaid and Lars Valentin and Octavio Mesa-Varona and Taras Günther and Carolina Plaza-Rodriguez and Matthias Filter",
keywords = "Information exchange format, FSK-ML, Modelling, Open-source software, FoodRisk-Labs, Microbial risk assessment",
abstract = "In the last decades a large number of models have been developed in the quantitative microbial risk assessment (QMRA) and predictive microbiology (PM) domains. These models were generated with different scripting languages (e.g. R, MATLAB), commercial tools (e.g. @Risk) or even proprietary software tools (e.g. FSSP, FDA-iRISK). The heterogeneity in software tools used to generate models together with the lack of a harmonized model exchange format has made the (re)-use of existing models in different software tools or simulation environments very difficult. The adoption of a harmonized information exchange format called Food Safety Knowledge Markup Language (FSK-ML) would be a solution to this challenge. FSK-ML defines a framework for encoding all relevant data, metadata and model scripts in a machine-readable format. A specific feature of FSK-ML is that it allows the user to provide model scripts in different scripting languages (e.g. R, Perl, Python or MATLAB). Model metadata can be provided in accordance with the metadata schema and controlled vocabularies proposed from the Risk Assessment Knowledge Integration Platform (RAKIP) community. In order to achieve a broad adoption of FSK-ML by the scientific community it is however of extraordinary importance to provide support in terms of easy to use software solutions. In fact, it has to be as simple as possible for the end-user to create, export, import or modify standard-compliant files. Food Safety Knowledge Lab (FSK-Lab) represents such a user-friendly software tool that can create, read (import), write (export), execute and combine FSK-ML compliant objects. All metadata needed to annotate a model can also be entered and edited through FSK-Lab. It also allows generating (export) files that comply with FSK-ML and that carry the file extension “.fskx”. This ensures that all information on a model is contained in this information exchange file and that the user does not have to write and compile FSK-ML files “by hand”. FSK-Lab extends the open source Konstanz Information Miner (KNIME) data analytics platform (URL: www.knime.org), which is a graphical programming framework allowing users to create data analysis workflows from building blocks (so called nodes). Within FSK-Lab, each node has a specific task e.g. model creation can be performed with the “FSK Creator” node. As a KNIME extension, FSK-Lab allows the user to execute and integrate code from several programming languages, like Java, R, Python. All FSK-Lab software code is freely available under the GNU public license version 3."
}
@article{MATON20138,
title = "Making semantic waves: A key to cumulative knowledge-building",
journal = "Linguistics and Education",
volume = "24",
number = "1",
pages = "8 - 22",
year = "2013",
note = "Cumulative knowledge-building in secondary schooling",
issn = "0898-5898",
doi = "https://doi.org/10.1016/j.linged.2012.11.005",
url = "http://www.sciencedirect.com/science/article/pii/S0898589812000678",
author = "Karl Maton",
keywords = "Social realism, Legitimation Code Theory, Knowledge-building, Cumulative teaching, Semantic gravity, Semantic density",
abstract = "The paper begins by arguing that knowledge-blindness in educational research represents a serious obstacle to understanding knowledge-building. It then offers sociological concepts from Legitimation Code Theory – ‘semantic gravity’ and ‘semantic density’ – that systematically conceptualize one set of organizing principles underlying knowledge practices. Brought together as ‘semantic profiles’, these allow changes in the context-dependence and condensation of meaning of knowledge practices to be traced over time. These concepts are used to analyze passages of classroom practice from secondary school lessons in Biology and History. The analysis suggests that ‘semantic waves’, where knowledge is transformed between relatively decontextualized, condensed meanings and context-dependent, simplified meanings, offer a means of enabling cumulative classroom practice. How these concepts are being widely used to explore organizing principles of diverse practices in education and beyond is discussed, revealing the widespread, complex and suggestive nature of ‘semantic waves’ and their implications for cumulative knowledge-building."
}
@article{HAYDEN201841,
title = "Long-term impact of intensive lifestyle intervention on cognitive function assessed with the National Institutes of Health Toolbox: The Look AHEAD study",
journal = "Alzheimer's & Dementia: Diagnosis, Assessment & Disease Monitoring",
volume = "10",
pages = "41 - 48",
year = "2018",
issn = "2352-8729",
doi = "https://doi.org/10.1016/j.dadm.2017.09.002",
url = "http://www.sciencedirect.com/science/article/pii/S2352872917300544",
author = "Kathleen M. Hayden and Laura D. Baker and George Bray and Raymond Carvajal and Kathryn Demos-McDermott and Andrea L. Hergenroeder and James O. Hill and Edward Horton and John M. Jakicic and Karen C. Johnson and Rebecca H. Neiberg and Stephen R. Rapp and Thomas A. Wadden and Michael E. Miller",
keywords = "Diabetes mellitus, Obesity, Cognition, Body mass index, Randomized controlled trial, Weight loss, Neuropsychological tests, Aged",
abstract = "Introduction
This study sought to determine whether 10 years of assignment to intensive lifestyle intervention (ILI) relative to diabetes support and education leads to better cognition. We examine intervention effects overall and among clinical subgroups, and report correlations between computer-administered and interviewer-administered cognitive batteries.
Methods
The Action for Health in Diabetes (Look AHEAD) was a 16-site randomized controlled trial with overweight/obese individuals (aged 45–76) who had type 2 diabetes. The NIH Toolbox Cognition Battery tests developed to measure cognition across the lifespan were used to evaluate cognition. Results were compared with standard paper-and-pencil tests. The Toolbox and paper-and-pencil tests were administered an average of 10.9 years after randomization to 1002 participants.
Results
Toolbox measures significantly correlated with interviewer-administered measures, with the strongest correlations between the Toolbox Fluid Cognition Composite and Trails B (r = −0.64, P < .0001) and Digit Symbol Coding (r = 0.63, P < .0001), and between the Toolbox Dimensional Change Card Sort (r = 0.55, P < .0001) and the Digit Symbol Coding test. Overall, ILI and diabetes support and education groups had similar adjusted mean cognitive outcomes (P > .05 for all). Subgroup analyses identified different intervention effects within baseline body mass index groups for Picture Sequence Memory (P = .01), within baseline cardiovascular disease groups for Picture Vocabulary (P = .01) and Fluid Cognition Composite (P = .02) measures, and within baseline age groups for Picture Vocabulary (P = .02).
Discussion
Correlations between Toolbox and interviewer-administered outcomes provide a measure of internal validity. Findings suggest no overall effect of the intervention on cognition and that an ILI resulting in weight loss may have negative implications for cognition in individuals aged ≥60, with previous history of cardiovascular disease, and those with body mass index ≥40."
}
@article{BERGMANN2015176,
title = "Algorithmic choices in WARP – A framework for continuous energy Monte Carlo neutron transport in general 3D geometries on GPUs",
journal = "Annals of Nuclear Energy",
volume = "77",
pages = "176 - 193",
year = "2015",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2014.10.039",
url = "http://www.sciencedirect.com/science/article/pii/S0306454914005787",
author = "Ryan M. Bergmann and Jasmina L. Vujić",
keywords = "Monte Carlo, Neutron transport, GPU, CUDA, CUDPP, OptiX",
abstract = "In recent supercomputers, general purpose graphics processing units (GPGPUs) are a significant faction of the supercomputer’s total computational power. GPGPUs have different architectures compared to central processing units (CPUs), and for Monte Carlo neutron transport codes used in nuclear engineering to take advantage of these coprocessor cards, transport algorithms must be changed to execute efficiently on them. WARP is a continuous energy Monte Carlo neutron transport code that has been written to do this. The main thrust of WARP is to adapt previous event-based transport algorithms to the new GPU hardware; the algorithmic choices for all parts of which are presented in this paper. It is found that remapping history data references increases the GPU processing rate when histories start to complete. The main reason for this is that completed data are eliminated from the address space, threads are kept busy, and memory bandwidth is not wasted on checking completed data. Remapping also allows the interaction kernels to be launched concurrently, improving efficiency. The OptiX ray tracing framework and CUDPP library are used for geometry representation and parallel dataset-side operations, ensuring high performance and reliability."
}
@article{TEKIAN2019288,
title = "What do quantitative ratings and qualitative comments tell us about general surgery residents’ progress toward independent practice? Evidence from a 5-year longitudinal cohort",
journal = "The American Journal of Surgery",
volume = "217",
number = "2",
pages = "288 - 295",
year = "2019",
issn = "0002-9610",
doi = "https://doi.org/10.1016/j.amjsurg.2018.09.031",
url = "http://www.sciencedirect.com/science/article/pii/S0002961018305300",
author = "Ara Tekian and Martin Borhani and Sarette Tilton and Eric Abasolo and Yoon Soo Park",
keywords = "Next accreditation system, Qualitative feedback, Learning trajectories, Surgery residency program",
abstract = "Background
This study examines the alignment of quantitative and qualitative assessment data in end-of-rotation evaluations using longitudinal cohorts of residents progressing throughout the five-year general surgery residency.
Methods
Rotation evaluation data were extracted for 171 residents who trained between July 2011 and July 2016. Data included 6069 rotation evaluations forms completed by 38 faculty members and 164 peer-residents. Qualitative comments mapped to general surgery milestones were coded for positive/negative feedback and relevance.
Results
Quantitative evaluation scores were significantly correlated with positive/negative feedback, r = 0.52 and relevance, r = −0.20, p < .001. Themes included feedback on leadership, teaching contribution, medical knowledge, work ethic, patient-care, and ability to work in a team-based setting. Faculty comments focused on technical and clinical abilities; comments from peers focused on professionalism and interpersonal relationships.
Conclusions
We found differences in themes emphasized as residents progressed. These findings underscore improving our understanding of how faculty synthesize assessment data."
}
@article{BALUJA2013956,
title = "Neighborhood Preserving Codes for Assigning Point Labels: Applications to Stochastic Search",
journal = "Procedia Computer Science",
volume = "18",
pages = "956 - 965",
year = "2013",
note = "2013 International Conference on Computational Science",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2013.05.261",
url = "http://www.sciencedirect.com/science/article/pii/S1877050913004043",
author = "Shumeet Baluja and Michele Covell",
keywords = "Search Representation, Gray Code, Graph Propagation, Adsorption, Graph Labeling, Genetic Algorithms",
abstract = "Selecting a good representation of a solution-space is vital to solving any search and optimization problem. In particular, once regions of high performance are found, having the property that small changes in the candidate solution correspond to searching nearby neighborhoods provides the ability to perform effective local optimization. To achieve this, it is common for stochastic search algorithms, such as stochastic hillclimbing, evolutionary algorithms (including genetic algorithms), and simulated annealing, to employ Gray Codes for encoding ordinal points or discretized real numbers. In this paper, we present a novel method to label similar and/or close points within arbitrary graphs with small Hamming distances. The resultant point labels can be seen as an approximate high-dimensional variant of Gray Codes with standard Gray Codes as a subset of the labels found here. The labeling procedure is applicable to any task in which the solution requires the search algorithm to select a small subset of items out of many. Such tasks include vertex selection in graphs, knapsack-constrained item selection, bin packing, prototype selection for machine learning, and numerous scheduling problems, to name a few."
}
@article{ITOGA2016175,
title = "Ventral pallidal coding of a learned taste aversion",
journal = "Behavioural Brain Research",
volume = "300",
pages = "175 - 183",
year = "2016",
issn = "0166-4328",
doi = "https://doi.org/10.1016/j.bbr.2015.11.024",
url = "http://www.sciencedirect.com/science/article/pii/S0166432815302977",
author = "Christy A. Itoga and Kent C. Berridge and J. Wayne Aldridge",
keywords = "Taste aversion, Hedonic value, Ventral pallidum",
abstract = "The hedonic value of a sweet food reward, or how much a taste is ‘liked’, has been suggested to be encoded by neuronal firing in the posterior ventral pallidum (VP). Hedonic impact can be altered by psychological manipulations, such as taste aversion conditioning, which can make an initially pleasant sweet taste become perceived as disgusting. Pairing nausea-inducing LiCl injection as a Pavlovian unconditioned stimulus (UCS) with a novel taste that is normally palatable as the predictive conditioned stimulus (CS+) suffices to induce a learned taste aversion that changes orofacial ‘liking’ responses to that sweet taste (e.g., lateral tongue protrusions) to ‘disgust’ reactions (e.g., gapes) in rats. We used two different sweet tastes of similar initial palatability (a sucrose solution and a polycose/saccharin solution, CS± assignment was counterbalanced across groups) to produce a discriminative conditioned aversion. Only one of those tastes (arbitrarily assigned and designated as CS+) was associatively paired with LiCl injections as UCS to form a conditioned aversion. The other taste (CS−) was paired with mere vehicle injections to remain relatively palatable as a control sweet taste. We recorded the neural activity in VP in response to each taste, before and after aversion training. We found that the safe and positively hedonic taste always elicited excitatory increases in firing rate of VP neurons. By contrast, aversion learning reversed the VP response to the ‘disgusting’ CS+ taste from initial excitation into a conditioned decrease in neuronal firing rate after training. Such neuronal coding of hedonic impact by VP circuitry may contribute both to normal pleasure and disgust, and disruptions of VP coding could result in affective disorders, addictions and eating disorders."
}
@article{AHMED2012523,
title = "Identifying best practice guidelines for debriefing in surgery: a tri-continental study",
journal = "The American Journal of Surgery",
volume = "203",
number = "4",
pages = "523 - 529",
year = "2012",
issn = "0002-9610",
doi = "https://doi.org/10.1016/j.amjsurg.2011.09.024",
url = "http://www.sciencedirect.com/science/article/pii/S0002961012000244",
author = "Maria Ahmed and Nick Sevdalis and John Paige and Ram Paragi-Gururaja and Debra Nestel and Sonal Arora",
keywords = "Debriefing, Feedback, Surgery, Simulation, Surgical education, Workplace-based learning, Qualitative",
abstract = "Background
Changes in surgical training have decreased opportunities for experiential learning in the operating room (OR). With this decrease, a commensurate increase in debriefing-dependent simulation-based activities has occurred. Effective debriefing could optimize learning from both simulated and real clinical encounters.
Methods
Thirty-three semistructured interviews with surgeons, anesthesiologists, and OR nurses from the United Kingdom, United States, and Australia identified the goals of debriefing, core components of an effective debrief, and solutions to its effective implementation. Interviews were audiotaped, transcribed, and coded using emergent theme analysis.
Results
Core components of an effective debrief include having the appropriate approach, establishing a learning environment, learner engagement, managing learner reaction, reflection, analysis, diagnosis, and application to real clinical practice. Solutions to enhance practice involve promotion of a debriefing culture within the surgical community with protected time to conduct a structured debriefing.
Conclusions
A need exists to enhance surgical training through regular structured debriefing. Identifying the key components of an effective debrief is a first step toward improving practice and embedding a debriefing culture within the OR."
}
@article{ZAGHI20142151,
title = "OFF, Open source Finite volume Fluid dynamics code: A free, high-order solver based on parallel, modular, object-oriented Fortran API",
journal = "Computer Physics Communications",
volume = "185",
number = "7",
pages = "2151 - 2194",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2014.04.005",
url = "http://www.sciencedirect.com/science/article/pii/S0010465514001283",
author = "S. Zaghi",
keywords = "CFD, Finite volume scheme, Riemann’s Problem solver, WENO, OOP Fortran, MPI, OpenMP",
abstract = "OFF, an open source (free software) code for performing fluid dynamics simulations, is presented. The aim of OFF is to solve, numerically, the unsteady (and steady) compressible Navier–Stokes equations of fluid dynamics by means of finite volume techniques: the research background is mainly focused on high-order (WENO) schemes for multi-fluids, multi-phase flows over complex geometries. To this purpose a highly modular, object-oriented application program interface (API) has been developed. In particular, the concepts of data encapsulation and inheritance available within Fortran language (from standard 2003) have been stressed in order to represent each fluid dynamics “entity” (e.g. the conservative variables of a finite volume, its geometry, etc…) by a single object so that a large variety of computational libraries can be easily (and efficiently) developed upon these objects. The main features of OFF can be summarized as follows: Programming LanguageOFF is written in standard (compliant) Fortran 2003; its design is highly modular in order to enhance simplicity of use and maintenance without compromising the efficiency; Parallel Frameworks Supported the development of OFF has been also targeted to maximize the computational efficiency: the code is designed to run on shared-memory multi-cores workstations and distributed-memory clusters of shared-memory nodes (supercomputers); the code’s parallelization is based on Open Multiprocessing (OpenMP) and Message Passing Interface (MPI) paradigms; Usability, Maintenance and Enhancement in order to improve the usability, maintenance and enhancement of the code also the documentation has been carefully taken into account; the documentation is built upon comprehensive comments placed directly into the source files (no external documentation files needed): these comments are parsed by means of doxygen free software producing high quality html and latex documentation pages; the distributed versioning system referred as git has been adopted in order to facilitate the collaborative maintenance and improvement of the code; CopyrightsOFF is a free software that anyone can use, copy, distribute, study, change and improve under the GNU Public License version 3. The present paper is a manifesto of OFF code and presents the currently implemented features and ongoing developments. This work is focused on the computational techniques adopted and a detailed description of the main API characteristics is reported. OFF capabilities are demonstrated by means of one and two dimensional examples and a three dimensional real application.
Program summary
Program title:OFF Catalogue identifier: AESV_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AESV_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public Licence, version 3 No. of lines in distributed program, including test data, etc.: 60466 No. of bytes in distributed program, including test data, etc.: 595575 Distribution format: tar.gz Programming language: Fortran (standard 2003 or newer); developed and tested with Intel Fortran Compiler v. 12.x or newer. Computer: Designed for shared-memory multi-cores workstations and for hybrid distributed/shared-memory supercomputers, but any computer system with a Fortran (2003+) compiler is suited. Operating system: Designed for POSIX architecture and tested on GNU/Linux one. Has the code been vectorized or parallelized?: Hybrid parallelization by means of MPI library and OpenMP paradigm, tested on up to 256 processors. RAM: [1 MB; 1 GB] x core, simulation-dependent Classification: 4.3, 4.10, 12. External routines: The proprietary library [1] must be linked for producing binary outputs in Tecplot Inc. format; the MPI library [2] must be linked for running on distribute-memory parallel systems Nature of problem: Numerical solution of the Compressible Navier–Stokes equations for multi-fluids multi-phase flows in complex geometries Solution method: Fully-conservative Finite Volume scheme based on very high-order WENO Positivity-Preserving reconstruction technique and Strong Stability Preserving high-order Runge–Kutta time integration. Structured multi-block general curvilinear and body-fitted grids constitute the underlining numerical grids. MPI and OpenMP paradigms are used for parallel computations. Pre-processing tool to deal with commercial meshing softwares is provided as well as post-processing one for numerical results visualization Restrictions: At present, OFF is validated for simulating inviscid and single-phase flows (viscous fluxes computation and fully coupled Eulerian /Lagrangian schemes have still to be validated, but they are already implemented); modern Fortran compiler is mandatory Unusual features: OFF is a complex CFD software strongly based on Object-Oriented Programming paradigm by means of modern Fortran standard (2003 or higher) Additional comments: OFF project adopts Git [3], a free and open source distributed version control system. A public repository dedicated to OFF project [4] has been created on github, a web-based hosting service for software development projects using git versioning system. Finally, a comprehensive documentation [5] is provided parsing source code comments by means of doxygen software [6] Running time: The running time depends on the available computation resources, the complexity of the problem and the selected accuracy of the numerical scheme. Simple one dimensional problems require few minutes on personal workstation-like systems whereas complex three dimensional problems needing high accuracy (e.g. DNS) could require weeks on supercomputers. OFF has proven to have a good scalability on small clusters (e.g. CASPUR facilities, namely on Matrix, a GNU/Linux cluster composed by 320 nodes each one constituted by a dual Opteron quadcore at 2.1GHz with 16/32 GB of RAM) References:[1]TecIO Library, Tecplot Inc. proprietary library for I/O binary files in Tecplot format, http://www.tecplot.com/downloads/tecio-library/.[2]The Message Passing Interface (MPI) standard, a library specification for message-passing, proposed as a standard by a broadly based committee of vendors, implementors, and users, http://www.mcs.anl.gov/research/projects/mpi/.[3]Git, a free and open source distributed version control system, http://git-scm.com/.[4]Github, a web-based hosting service for software development projects using git versioning system, https://github.com.[5]Ocial OFF documentation, http://szaghi.github.com/OFF/index.html.[6]Doxygen, a documentation system for many programming languages, http://www.stack.nl/dimitri/doxygen."
}
@article{MA201583,
title = "Two dimensional hashing for visual tracking",
journal = "Computer Vision and Image Understanding",
volume = "135",
pages = "83 - 94",
year = "2015",
issn = "1077-3142",
doi = "https://doi.org/10.1016/j.cviu.2015.01.003",
url = "http://www.sciencedirect.com/science/article/pii/S107731421500017X",
author = "Chao Ma and Chuancai Liu",
keywords = "Hashing, Tracking, Appearance model, Incremental learning",
abstract = "Appearance model is a key part of tracking algorithms. To attain robustness, many complex appearance models are proposed to capture discriminative information of object. However, such models are difficult to maintain accurately and efficiently. In this paper, we observe that hashing techniques can be used to represent object by compact binary code which is efficient for processing. However, during tracking, online updating hash functions is still inefficient with large number of samples. To deal with this bottleneck, a novel hashing method called two dimensional hashing is proposed. In our tracker, samples and templates are hashed to binary matrices, and the hamming distance is used to measure confidence of candidate samples. In addition, the designed incremental learning model is applied to update hash functions for both adapting situation change and saving training time. Experiments on our tracker and other eight state-of-the-art trackers demonstrate that the proposed algorithm is more robust in dealing with various types of scenarios."
}
@article{LEE2018,
title = "Incidence Trends of Gastroenteropancreatic Neuroendocrine Tumors in the United States",
journal = "Clinical Gastroenterology and Hepatology",
year = "2018",
issn = "1542-3565",
doi = "https://doi.org/10.1016/j.cgh.2018.12.017",
url = "http://www.sciencedirect.com/science/article/pii/S1542356518313934",
author = "Mi Ri Lee and Cynthia Harris and Kiwoon Joshua Baeg and Anne Aronson and Juan P. Wisnivesky and Michelle Kang Kim",
keywords = "SEER, gastrointestinal cancer, epidemiology, carcinoid, pancreas, small bowel",
abstract = "Background & Aims
Although multiple studies have reported an increasing incidence of gastroenteropancreatic neuroendocrine tumors (GEP-NETs) over the past decades, there are limited national data on recent trends. Using a population-based registry, we evaluated GEP-NET incidence trends in the United States population from 1975 through 2012, based on age, calendar year at diagnosis, and year of birth.
Methods
GEP-NET cases from 1975 through 2012 were identified from the most recent version of the Surveillance, Epidemiology, and End Results registry using histologic and site codes. We calculated overall annual incidence, age-adjusted incidence (number of cases per 100,000), annual percent change (APC), and average APC by 5-year age intervals. We also evaluated the incidence rates by age, period, and birth year cohorts.
Results
We identified 22,744 patients with GEP-NETs. In adults 25–39 years old, GEP-NET incidence rates decreased from the mid-1970s to the early 1980s, then increased until 2012. In adults ages 40 years and older or young adults ages 15–24 years, incidence rates generally increased continuously from 1975 through 2012. Adults ages 40–69 years had the most rapid increases in average APC (approximately 4%–6% per year). Overall incidence rates were highest in adults 70–84 years old. Since the inception of the Surveillance, Epidemiology, and End Results registry, GEP-NET incidence has increased in consecutive birth cohorts.
Conclusion
The incidence of GEP-NET continues to increase—particularly in older adults. More recent generations have had higher GEP-NET incidence rates than more distant generations."
}
@article{PETRY201422,
title = "First results of operational ionospheric dynamics prediction for the Brazilian Space Weather program",
journal = "Advances in Space Research",
volume = "54",
number = "1",
pages = "22 - 36",
year = "2014",
issn = "0273-1177",
doi = "https://doi.org/10.1016/j.asr.2014.03.017",
url = "http://www.sciencedirect.com/science/article/pii/S0273117714001793",
author = "Adriano Petry and Jonas Rodrigues de Souza and Haroldo Fraga de Campos Velho and André Grahl Pereira and Graham John Bailey",
keywords = "Ionosphere, Space weather, Total electron content, Operational system",
abstract = "It is shown the development and preliminary results of operational ionosphere dynamics prediction system for the Brazilian Space Weather program. The system is based on the Sheffield University Plasmasphere–Ionosphere Model (SUPIM), a physics-based model computer code describing the distribution of ionization within the Earth mid to equatorial latitude ionosphere and plasmasphere, during geomagnetically quiet periods. The model outputs are given in a 2-dimensional plane aligned with Earth magnetic field lines, with fixed magnetic longitude coordinate. The code was adapted to provide the output in geographical coordinates. It was made referring to the Earth’s magnetic field as an eccentric dipole, using the approximation based on International Geomagnetic Reference Field (IGRF-11). During the system operation, several simulation runs are performed at different longitudes. The original code would not be able to run all simulations serially in reasonable time. So, a parallel version for the code was developed for enhancing the performance. After preliminary tests, it was frequently observed code instability, when negative ion temperatures or concentrations prevented the code from continuing its processing. After a detailed analysis, it was verified that most of these problems occurred due to concentration estimation of simulation points located at high altitudes, typically over 4000km of altitude. In order to force convergence, an artificial exponential decay for ion–neutral collisional frequency was used above mentioned altitudes. This approach shown no significant difference from original code output, but improved substantially the code stability. In order to make operational system even more stable, the initial altitude and initial ion concentration values used on exponential decay equation are changed when convergence is not achieved, within pre-defined values. When all code runs end, the longitude of every point is then compared with its original reference station longitude, and differences are compensated by changing the simulation point time slot, in a temporal adjustment optimization. Then, an approximate neighbor searching technique was developed to obtain the ion concentration values in a regularly spaced grid, using inverse distance weighting (IDW) interpolation. A 3D grid containing ion and electron concentrations is generated for every hour of simulated day. Its spatial resolution is 1° of latitude per 1° of longitude per 10km of altitude. The vertical total electron content (VTEC) is calculated from the grid, and plotted in a geographic map. An important feature that was implemented in the system is the capacity of combining observational data and simulation outputs to obtain more appropriate initial conditions to the ionosphere prediction. Newtonian relaxation method was used for this data assimilation process, where ionosonde data from four different locations in South America was used to improve the system accuracy. The whole process runs every day and predicts the VTEC values for South America region with almost 24h ahead."
}
@article{DIMOVSKI20181,
title = "Variability abstractions for lifted analyses",
journal = "Science of Computer Programming",
volume = "159",
pages = "1 - 27",
year = "2018",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2017.12.012",
url = "http://www.sciencedirect.com/science/article/pii/S0167642318300583",
author = "Aleksandar S. Dimovski and Claus Brabrand and Andrzej Wasowski",
keywords = "Program families, Static analysis, Abstract interpretation",
abstract = "Family-based (lifted) static analysis for “highly configurable programs” (program families) is capable of analyzing all variants at once without generating any of them explicitly. It takes as input only the common code base, which encodes all variants of a program family, and produces precise analysis results corresponding to all variants. However, the computational cost of the lifted analysis still depends inherently on the number of variants, which is in the worst case exponential in the number of statically configurable options (features). For a large number of features, the lifted analysis may be too costly or even infeasible. In this work, we introduce variability abstractions defined as Galois connections, which simplify variability away from program families based on #ifdef-s. Then, we use abstract interpretation as a formal method for the calculational-based derivation of abstracted lifted analyses, which are sound by construction. Our approach for abstracting lifted analysis is orthogonal to the particular program analysis chosen as a client. While a single program analysis operates on program states and depends on language-specific constructs, the lifted analysis assumes that a single program analysis already exists and lifts its results to all variants of the analyzed program family. Variability abstractions aim to reduce this variability-specific component of the lifted analysis, which handles variability and #ifdef-s. Furthermore, given the “orthogonality” of variability abstractions to the rest of the analysis (its language-specific component), we can implement abstractions as a preprocessor. In particular, given an abstraction we define a syntactic transformation, which translates any program family into an abstracted version of it, such that the analysis of the abstracted program family coincides with the corresponding abstracted analysis of the original program family. We have implemented the proposed approach, and we evaluate its practicality on three Java benchmarks. The evaluation shows that abstractions yield significant performance gains, especially for families with higher variability."
}
@article{XIE201713,
title = "An intelligent system for estimating frog community calling activity and species richness",
journal = "Ecological Indicators",
volume = "82",
pages = "13 - 22",
year = "2017",
issn = "1470-160X",
doi = "https://doi.org/10.1016/j.ecolind.2017.06.015",
url = "http://www.sciencedirect.com/science/article/pii/S1470160X17303588",
author = "Jie Xie and Michael Towsey and Mingying Zhu and Jinglan Zhang and Paul Roe",
keywords = "Frog community calling activity, Frog species richness, Acoustic event detection, Multi-label learning",
abstract = "Over the past decade, dramatic declines in frog populations have been noticed worldwide. To examine this decline, monitoring frogs is becoming increasingly important. Compared to traditional field survey methods, recent advances in acoustic sensor technology have greatly extended spatial and temporal scales for monitoring animal populations. In this paper, we examine the problem of monitoring frog populations by analysing acoustic sensor data, where the population is reflected by community calling activity and species richness. Specifically, a novel acoustic event detection (AED) algorithm is first proposed to filter out those recordings without frog calls. Then, multi-label learning is used to classify each individual recording with six acoustic features: linear predictive coding coefficients, Mel-frequency cepstral coefficients, linear-frequency cepstral coefficients, acoustic complexity index, acoustic diversity index, and acoustic evenness index. Next, frog community calling activity and species richness are estimated by accumulating the results of AED and multi-label learning, respectively. Finally, ordinary least squares regression (OLS) is conducted to reveal the relationship between frog populations (frog calling activity and species richness) and weather variables (maximum temperature and rainfall). Experimental results demonstrate that our proposed intelligent system can significantly facilitate the effort to estimate frog community calling activity and species richness with comparable accuracies. The statistical results of OLS indicate that rainfall pattern has a lagged impact on frog community calling activity (significant in the first day after rainy day) and species richness (significant in the fourth day after rainy day). Temperature is shown to affect species richness but is less likely to change calling activity."
}
@article{PITA201396,
title = "Hurricane vulnerability modeling: Development and future trends",
journal = "Journal of Wind Engineering and Industrial Aerodynamics",
volume = "114",
pages = "96 - 105",
year = "2013",
issn = "0167-6105",
doi = "https://doi.org/10.1016/j.jweia.2012.12.004",
url = "http://www.sciencedirect.com/science/article/pii/S0167610512002851",
author = "Gonzalo L. Pita and Jean-Paul Pinelli and Kurtis R. Gurley and Shahid Hamid",
keywords = "Vulnerability, Catastrophe modeling, Uncertainty, Loss models",
abstract = "Catastrophe models help to evaluate the vulnerability of the building stock exposed to a hazard. This paper presents a history of the hurricane risk models in Florida, and discusses their relationship to the building codes. The first models were econometric, and failed to predict the insured building losses produced by hurricane Andrew. This led to a change in the loss projection paradigm and to the advent of modern catastrophe modeling. Advantages and challenges of the current methodologies are discussed, including the quality of input, validation, uncertainty, and scope of the outputs. The paper concludes with a brief overview of current and future research in vulnerability modeling."
}
@article{ACAR2016861,
title = "Soundscapes of Digital Morphogenesis in Architecture which Created from Musical Algorithm",
journal = "Procedia - Social and Behavioral Sciences",
volume = "216",
pages = "861 - 873",
year = "2016",
note = "Urban Planning and Architectural Design for Sustainable Development (UPADSD)",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2015.12.083",
url = "http://www.sciencedirect.com/science/article/pii/S1877042815062631",
author = "Didem Acar",
keywords = "Transcoding, Acoustic, Computational Design, Transdisciplinary framework, Architectural design",
abstract = "Music and architecture have made use of mathematical proportions throughout the history for the purpose of creating acoustic and visual forms. The reason for this is the aesthetic pursuit of both disciplines since centuries. Mathematics is one of the most important factors that influence aesthetic results. While forming their abstract aesthetic compositions the musicians use the musical notes that have definite frequency values. Each of these frequency values are defined by one integer. Every classical music artist uses the fractal sequencing of these frequencies. On the other hand we encounter hundreds of silent formats which are produced using mathematical ideas. In this context if we think of the interdisciplinary interaction between music and architecture no form is ever silent. In this study, the intersection of two disciplines will be examined in the perspective of architecture; a stumper and interrogative start for pursuit of architectural forms of the present day with the transformation of auditory forms to visual forms will be made; and a basis will be provided to be able to discuss the innovations that the spaces, structures and auditory experiences which can be formed by obtaining musical codes bring."
}
@article{MIRARCHI2012511,
title = "TRIAD III: Nationwide Assessment of Living Wills and Do Not Resuscitate Orders",
journal = "The Journal of Emergency Medicine",
volume = "42",
number = "5",
pages = "511 - 520",
year = "2012",
issn = "0736-4679",
doi = "https://doi.org/10.1016/j.jemermed.2011.07.015",
url = "http://www.sciencedirect.com/science/article/pii/S0736467911008535",
author = "Ferdinando L. Mirarchi and Erin Costello and Justin Puller and Timothy Cooney and Nathan Kottkamp",
keywords = "living will, interpretation, DNR, code status, patient safety",
abstract = "Background
Concern exists that living wills are misinterpreted and may result in compromised patient safety.
Objective
To determine whether adding code status to a living will improves understanding and treatment decisions.
Methods
An Internet survey was conducted of General Surgery, and Family, Internal, and Emergency Medicine residencies between May and December 2009. The survey posed a fictitious living will with and without additional clarification in the form of code status. An emergent patient care scenario was then presented that included medical history and signs/symptoms. Respondents were asked to assign a code status and choose appropriate intervention. Questions were formatted as dichotomous responses. Correct response rate was based on legal statute. Significance of changes in response due to the addition of either clinical context (past medical history/signs/symptoms) or code status was assessed by contingency table analysis.
Results
Seven hundred sixty-eight faculty and residents at accredited training centers in 34 states responded. At baseline, 22% denoted “full code” as the code status for a typical living will, and 36% equated “full care” with a code status DNR. Adding clinical context improved correct responses by 21%. Specifying code status further improved correct interpretation by 28% to 34%. Treatment decisions were either improved 12–17% by adding code status (“Full Code,” “Hospice Care”) or worsened 22% (“DNR”).
Conclusion
Misunderstanding of advance directives is a nationwide problem. Addition of code status may help to resolve the problem. Further research is required to ensure safety, understanding, and appropriate care to patients."
}
@article{NOCERA20181281,
title = "NSQIP Analysis of Axillary Lymph Node Dissection Rates for Breast Cancer: Implications for Resident and Fellow Participation",
journal = "Journal of Surgical Education",
volume = "75",
number = "5",
pages = "1281 - 1286",
year = "2018",
issn = "1931-7204",
doi = "https://doi.org/10.1016/j.jsurg.2018.02.020",
url = "http://www.sciencedirect.com/science/article/pii/S1931720417306736",
author = "Nadia F. Nocera and Bryan J. Pyfer and Lucy M. De La Cruz and Abhishek Chatterjee and Paul T. Thiruchelvam and Carla S. Fisher",
keywords = "breast cancer, axillary lymph node dissection, surgery, resident education, Practice-Based Learning and Improvement",
abstract = "Introduction
Management of the axilla in invasive breast cancer (IBC) has shifted away from more radical surgery such as axillary lymph node dissection (ALND), towards less invasive procedures, such as sentinel lymph node biopsy. Because of this shift, we hypothesize that there has been a national downward trend in ALND procedures, subsequently impacting surgical trainee exposure to this procedure using the ACS-NSQIP database to evaluate this.
Methods
Women with IBC were identified in the ACS-NSQIP database from 2007 to 2014. Procedures including ALND were identified using CPT codes. This number was divided by total cases, given a varying number of participating institutions each year. Next, cases involving resident participation were identified and divided by training level: junior (post graduate year–[PGY] 1-2), senior (PGY 3-5) and fellow (PGY ≥ 6). Two tailed z tests were used to compare proportions, with significance determined when p < 0.05.
Results
A total of 128,372 women were identified with IBC with 36,844 ALND. ALND rates decreased by an average of 2.43% yearly from 2007 to 2014. Resident participation significantly drops in 2011, from 49.3% before to 29.4% after (p < 0.01). Junior residents experienced a significant decrease in participation rate (43.3%–32.2%, p < 0.05). Senior residents and fellows experienced an upward trend in their participation, although not significant (51.2%–56.3%, p = 0.35, and 5.6%–11.6%, p = 0.056, respectively).
Conclusions
Using the ACS-NSQIP database, we demonstrate the downward trend in rate of ALND for IBC with subsequent decrease in resident participation. Junior residents experienced a significant decrease in their participation with no significant change for senior or fellow-level trainees. Awareness of this trend is important when creating future surgical curriculum changes for general surgery and fellowship training programs."
}
@article{HILDEBRANDT2015131,
title = "Testing the disgust conditioning theory of food-avoidance in adolescents with recent onset anorexia nervosa",
journal = "Behaviour Research and Therapy",
volume = "71",
pages = "131 - 138",
year = "2015",
issn = "0005-7967",
doi = "https://doi.org/10.1016/j.brat.2015.06.008",
url = "http://www.sciencedirect.com/science/article/pii/S0005796715001072",
author = "Tom Hildebrandt and Andrew Grotzinger and Marianne Reddan and Rebecca Greif and Ifat Levy and Wayne Goodman and Daniela Schiller",
keywords = "Anorexia nervosa, Reversal learning, Electromyography, Emotion, Food learning",
abstract = "Anorexia nervosa is characterized by chronic food avoidance that is resistant to change. Disgust conditioning offers one potential unexplored mechanism for explaining this behavioral disturbance because of its specific role in facilitating food avoidance in adaptive situations. A food based reversal learning paradigm was used to study response flexibility in 14 adolescent females with restricting subtype anorexia nervosa (AN-R) and 15 healthy control (HC) participants. Expectancy ratings were coded as a behavioral measure of flexibility and electromyography recordings from the levator labii (disgust), zygomaticus major (pleasure), and corrugator (general negative affect) provided psychophysiological measures of emotion. Response inflexibility was higher for participants with AN-R, as evidenced by lower extinction and updated expectancy ratings during reversal. EMG responses to food stimuli were predictive of both extinction and new learning. Among AN-R patients, disgust specific responses to food were associated with impaired extinction, as were elevated pleasure responses to the cued absence of food. Disgust conditioning appears to influence food learning in acutely ill patients with AN-R and may be maintained by counter-regulatory acquisition of a pleasure response to food avoidance and an aversive response to food presence. Developing strategies to target disgust may improve existing interventions for patients with AN."
}
@article{BOYLE2016178,
title = "An update to the systematic literature review of empirical evidence of the impacts and outcomes of computer games and serious games",
journal = "Computers & Education",
volume = "94",
pages = "178 - 192",
year = "2016",
issn = "0360-1315",
doi = "https://doi.org/10.1016/j.compedu.2015.11.003",
url = "http://www.sciencedirect.com/science/article/pii/S0360131515300750",
author = "Elizabeth A. Boyle and Thomas Hainey and Thomas M. Connolly and Grant Gray and Jeffrey Earp and Michela Ott and Theodore Lim and Manuel Ninaus and Claudia Ribeiro and João Pereira",
keywords = "Computer games, Serious games, Entertainment, Engagement, Learning, Systematic literature review",
abstract = "Continuing interest in digital games indicated that it would be useful to update Connolly et al.'s (2012) systematic literature review of empirical evidence about the positive impacts and outcomes of games. Since a large number of papers was identified in the period from 2009 to 2014, the current review focused on 143 papers that provided higher quality evidence about the positive outcomes of games. Connolly et al.'s multidimensional analysis of games and their outcomes provided a useful framework for organising the varied research in this area. The most frequently occurring outcome reported for games for learning was knowledge acquisition, while entertainment games addressed a broader range of affective, behaviour change, perceptual and cognitive and physiological outcomes. Games for learning were found across varied topics with STEM subjects and health the most popular. Future research on digital games would benefit from a systematic programme of experimental work, examining in detail which game features are most effective in promoting engagement and supporting learning."
}
@article{TOBIN2015330,
title = "The Impact of Pseudostuttering Experiences on SLT students’ Learning",
journal = "Procedia - Social and Behavioral Sciences",
volume = "193",
pages = "330",
year = "2015",
note = "10th Oxford Dysfluency Conference, ODC 2014, 17 - 20 July, 2014, Oxford, United kingdom",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2015.03.301",
url = "http://www.sciencedirect.com/science/article/pii/S1877042815020947",
author = "L. Tobin and R. Lyons",
keywords = "Pseudo-stuttering, Clinical, Education, Students",
abstract = "Studies have shown that a significant number of Speech and Language Therapystudents report feeling less competent in the area of fluency. In light of this, a small number ofresearchers have reported that participation in a pseudo-stuttering exercise can enhancefluency modules and be a beneficial learning experience for Speech and Language Therapystudents. However, there is limited research to support this claim.
Aim
To investigate the impact of a pseudo-stuttering experience on students’ learning and theimplications for future clinical practice. This study used a qualitative methodology to facilitate an in-depth exploration of theexperiences of the participants. Two focus groups were conducted in which a total of 16 SLTstudents discussed their experiences. The focus groups were transcribed and coded with theaid of NVivo9 software. The data was analysed using principles of grounded theory. Analysis of the data identified three themes relating to the students’ experiences ofpseudo-stuttering: Stepping into their shoes, Learning from experience and Implications forclinical practice. The students reported that their experience was not truly reflective of a PWS'sexperience. Nonetheless, they considered it as an opportunity to “Step into someone else'sshoes” which resulted in reports of increased empathy for PWS by some. Empathy has beenshown to facilitate the development of positive client-clinician relationships which in turn impactson therapy outcomes. The participants identified practical changes they will make to clinicalpractice following the experience. The findings suggest that participation in a pseudo-stuttering exercise can be avaluable learning tool for SLT students. Furthermore, this study considered important ethicalissues that previous research has overlooked."
}
@article{SAINUDIIN20181,
title = "Full likelihood inference from the site frequency spectrum based on the optimal tree resolution",
journal = "Theoretical Population Biology",
volume = "124",
pages = "1 - 15",
year = "2018",
issn = "0040-5809",
doi = "https://doi.org/10.1016/j.tpb.2018.07.002",
url = "http://www.sciencedirect.com/science/article/pii/S004058091730165X",
author = "Raazesh Sainudiin and Amandine Véber",
keywords = "Importance sampler, Semi-parametric estimation, Optimal tree resolution, Controlled Markov process on hidden genealogical trees",
abstract = "We develop a novel importance sampler to compute the full likelihood function of a demographic or structural scenario given the site frequency spectrum (SFS) at a locus free of intra-locus recombination. This sampler, instead of representing the hidden genealogy of a sample of individuals by a labelled binary tree, uses the minimal level of information about such a tree that is needed for the likelihood of the SFS and thus takes advantage of the huge reduction in the size of the state space that needs to be integrated. We assume that the population may have demographically changed and may be non-panmictically structured, as reflected by the branch lengths and the topology of the genealogical tree of the sample, respectively. We also assume that mutations conform to the infinitely-many-sites model. We achieve this by a controlled Markov process that generates ‘particles’ in the hidden space of SFS histories which are always compatible with the observed SFS. To produce the particles, we use Aldous’ Beta-splitting model for a one parameter family of prior distributions over genealogical topologies or shapes (including that of the Kingman coalescent) and allow the branch lengths or epoch times to have a parametric family of priors specified by a model of demography (including exponential growth and bottleneck models). Assuming independence across unlinked loci, we can estimate the likelihood of a population scenario based on a large collection of independent SFS by an importance sampling scheme, using the (unconditional) distribution of the genealogies under this scenario when the latter is available. When it is not available, we instead compute the joint likelihood of the tree balance parameter β assuming that the tree topology follows Aldous’ Beta-splitting model, and of the demographic scenario determining the distribution of the inter-coalescence times or epoch times in the genealogy of a sample, in order to at least distinguish different equivalence classes of population scenarios leading to different tree balances and epoch times. Simulation studies are conducted to demonstrate the capabilities of the approach with publicly available code."
}
@article{WEBB2014472,
title = "Assessing Competency in Practice-Based Learning: A Foundation for Milestones in Learning Portfolio Entries",
journal = "Journal of Surgical Education",
volume = "71",
number = "4",
pages = "472 - 479",
year = "2014",
issn = "1931-7204",
doi = "https://doi.org/10.1016/j.jsurg.2014.01.019",
url = "http://www.sciencedirect.com/science/article/pii/S1931720414000488",
author = "Travis P. Webb and Taylor R. Merkley and Thomas J. Wade and Deborah Simpson and Rachel Yudkowsky and Ilene Harris",
keywords = "practice-based learning, residency, milestones, competency, learning portfolios, Practice-Based Learning and Improvement, Medical Knowledge",
abstract = "Background
Graduate medical education is undergoing a dramatic shift toward competency-based assessment of learners. Competency assessment requires clear definitions of competency and validated assessment methods. The purpose of this study is to identify criteria used by surgical educators to judge competence in Practice-Based Learning and Improvement (PBL&I) as demonstrated in learning portfolios.
Methods
A total of 6 surgical learning and instructional portfolio entries served as documents to be assessed by 3 senior surgical educators. These faculty members were asked to rate and then identify criteria used to assess PBL&I competency. Individual interviews and group discussions were conducted, recorded, and transcribed to serve as the study dataset. Analysis was performed using qualitative methodology to identify themes for the purpose of defining competence in PBL&I. The assessment themes derived are presented with narrative examples to describe the progression of competency.
Results
The collaborative coding process resulted in identification of 7 themes associated with competency in PBL&I related to surgical learning and instructional portfolio entries: (1) self-awareness regarding effect of actions; (2) identification and thorough description of learning goals; (3) cases used as catalyst for reflection; (4) reconceptualization with appropriate use and critique of cited literature; (5) communication skills/completeness of entry template; (6) description of future behavioral change; and (7) engagement in process—identifies as personally relevant.
Conclusions
The identified themes are consistent with and complement other criteria emerging from reflective practice literature and experiential learning theory. This study provides a foundation for further development of a tool for assessing learner portfolios consistent with the Accreditation Council for Graduate Medical Education’s Next Accreditation System requirements."
}
@article{VANGEEPURAM2015532,
title = "Use of Focus Groups to Inform a Youth Diabetes Prevention Model",
journal = "Journal of Nutrition Education and Behavior",
volume = "47",
number = "6",
pages = "532 - 539.e1",
year = "2015",
issn = "1499-4046",
doi = "https://doi.org/10.1016/j.jneb.2015.08.006",
url = "http://www.sciencedirect.com/science/article/pii/S1499404615006314",
author = "Nita Vangeepuram and Jane Carmona and Guedy Arniella and Carol R. Horowitz and Deborah Burnet",
keywords = "youth, diabetes prevention, diet, physical activity, focus groups",
abstract = "Objective
To explore minority adolescents’ perceptions of their diabetes risk, barriers and facilitators to adopting lifestyle changes, and ideas for adapting a youth diabetes prevention model.
Methods
The study was conducted at collaborating community sites in East Harlem, NY. Trained moderators facilitated focus groups, which were audio taped and transcribed. Participants were 21 Latino and African American adolescents aged 14–18 years with a family history of diabetes and no reported personal history of diabetes. The phenomenon of interest was youth input in adapting a diabetes prevention model. Two researchers independently coded transcripts, identified major themes, compared findings, and resolved differences through discussion and consensus.
Results
Dominant themes included (1) the impact of diabetes on quality of life within adolescents’ personal networks; (2) conflict between changing diet and activity and their current lifestyle; (3) lifestyle choices being dictated by cost, mood, body image, and environment, not health; and (4) family, social, and environmental pressures reinforcing sedentary behaviors and unhealthy diets.
Conclusions and Implications
Themes from youth focus groups were framed in the context of an existing youth diabetes prevention conceptual model, with results informing expansion of the model and identification and organization of potential intervention components."
}
@article{DOUBT2017767,
title = "“It Has Changed”: Understanding Change in a Parenting Program in South Africa",
journal = "Annals of Global Health",
volume = "83",
number = "5",
pages = "767 - 776",
year = "2017",
note = "Adolescent Health and Medicine",
issn = "2214-9996",
doi = "https://doi.org/10.1016/j.aogh.2017.10.021",
url = "http://www.sciencedirect.com/science/article/pii/S2214999617306732",
author = "Jenny Doubt and Rachel Bray and Heidi Loening-Voysey and Lucie Cluver and Jasmina Byrne and Divane Nzima and Barnaby King and Yulia Shenderovich and Janina Steinert and Sally Medley",
keywords = "adolescence, child maltreatment, parenting programs, qualitative study, South Africa",
abstract = "Background
Poor parenting that leads to child maltreatment during adolescence presents a major public health burden. Research from high-income countries indicates that evidence-based parenting program interventions can reduce child maltreatment. Much less is known, however, about how beneficiaries of these programs experience this process of change. Understanding the process that brings about change in child maltreatment practices is essential to understanding intervention mechanisms of change. This is particularly important given the current scale-up of parenting programs across low- and middle-income countries.
Objectives
This study aimed to provide insight into how caregivers and adolescents attending a parenting program in South Africa perceived changes associated with abuse reduction.
Methods
Semi-structured interviews were conducted with caregivers and adolescents (n = 42) after the intervention, as well as observations of sessions (n = 9) and focus group discussions (n = 240 people). Participants were adolescents between the ages of 10-18 and their primary caregiver residing in peri-urban and rural program clusters in the Eastern Cape Province of South Africa. Data were coded in Atlas.ti, and thematic content analysis was conducted.
Findings
Based on participant perceptions, the Sinovuyo Teen parenting program workshops catalyzed change into practice by creating an environment that was conducive to learning alternatives. It did so through prioritizing a process of mutual respect, openness, and being valued by others, giving legitimacy to a respectful reciprocity and new ways of spending time together that enabled caregivers and teenagers to shift and normalize more positive behaviors. This in turn led to reductions in physical and verbal abuse.
Conclusions
This study's findings may be of use to policymakers and practitioners who need to understand how parenting programs support parents and teenagers in increasing positive parenting approaches and changing potentially harmful practices. It additionally highlights the importance of assessing the experiences of both parents and teenagers attending such programs."
}
@article{DARGUSH2016360,
title = "Contact modeling in boundary element analysis including the simulation of thermomechanical wear",
journal = "Tribology International",
volume = "100",
pages = "360 - 370",
year = "2016",
note = "42nd Leeds-Lyon Symposium on Tribology- Surfaces and Interfaces: Mysteries at Different Scales",
issn = "0301-679X",
doi = "https://doi.org/10.1016/j.triboint.2016.04.001",
url = "http://www.sciencedirect.com/science/article/pii/S0301679X16300470",
author = "Gary F. Dargush and Andres Soom",
keywords = "Contact mechanics, Surface roughness, Local wear model, Boundary element method",
abstract = "Computational mechanics codes include contact or over-closure models to account for contact stiffness, which can be related to surface roughness and nominal pressure from Greenwood and Williamson. We present results of wear simulation studies with adjustable contact parameters for a ring-on-ring sliding configuration. A boundary element code is used to solve the non-linear contact problem with an axisymmetric thermoelastic representation of the rings, along with a localized Archard wear model. Parametric studies indicate the extent to which the wear tracks can be affected by the roughness and wear history. The effects range from minor to moderately significant, with areas of maximum wear shifting radially. Contact parameters influence pressure, temperature and wear distributions, with stiffer contacts resulting in greater localization."
}
@article{BLOMFORS2018394,
title = "Engineering bond model for corroded reinforcement",
journal = "Engineering Structures",
volume = "156",
pages = "394 - 410",
year = "2018",
issn = "0141-0296",
doi = "https://doi.org/10.1016/j.engstruct.2017.11.030",
url = "http://www.sciencedirect.com/science/article/pii/S0141029617306442",
author = "Mattias Blomfors and Kamyab Zandi and Karin Lundgren and Dario Coronelli",
keywords = "Corrosion, Reinforced concrete, Anchorage, Bond, Assessment",
abstract = "Corrosion of the reinforcement in concrete structures affects their structural capacity. This problem affects many existing concrete bridges and climate change is expected to worsen the situation in future. At the same time, assessment engineers lack simple and reliable calculation methods for assessing the structural capacity of structures damaged by corrosion. This paper further develops an existing model for assessing the anchorage capacity of corroded reinforcement. The new version is based on the local bond stress-slip relationships from fib Model Code 2010 and has been modified to account for corrosion. The model is verified against a database containing the results from nearly 500 bond tests and by comparison with an empirical model from the literature. The results show that the inherent scatter among bond tests is large, even within groups of similar confinement and corrosion level. Nevertheless, the assessment model that has been developed can represent the degradation of anchorage capacity due to corrosion reasonably well. This new development of the model is shown to represent the experimental data better than the previous version; it yields similar results to an empirical model in the literature. In contrast to many empirical models, the model developed here represents physical behaviour and shows the full local bond stress-slip relationship. Using this assessment model will increase the ability of professional engineers to estimate the anchorage capacity of corroded concrete structures."
}
@article{SAHAR201928,
title = "Towards energy aware object-oriented development of android applications",
journal = "Sustainable Computing: Informatics and Systems",
volume = "21",
pages = "28 - 46",
year = "2019",
issn = "2210-5379",
doi = "https://doi.org/10.1016/j.suscom.2018.10.005",
url = "http://www.sciencedirect.com/science/article/pii/S2210537918302014",
author = "Hareem Sahar and Abdul A. Bangash and Mirza O. Beg",
keywords = "Energy consumption, Mining software repositories, Software metrics, Sustainable software, Static analysis, Energy-aware development",
abstract = "Energy consumption has become a concern for developers due to the increasing complexity of applications that are to run on devices with limited battery power. Developers want to develop energy efficient applications however existing tools do not bridge the gap between understanding where energy is consumed and suggesting how the code can be modified in order to reduce energy consumption. A generalized method to relate software structure with its energy consumption is hence desirable. Previous attempts to relate change in object-oriented structure to its effects on energy consumption have been inconclusive. In this paper, we proposed a methodology to relate software structural information represented as metrics to energy consumption. Employing our methodology we empirically validated three Object Oriented (OO) metric suites; the Abreus Metrics (MOOD), Chidamber and Kemerer (CK) metrics and Martin's package metric suite and determine their relationship with energy consumption. Our results show that software structural metrics can be reliably related to energy consumption behavior of programs using a total of 63 releases from seven open-source iteratively developed android applications."
}
@article{JEANTET2012354,
title = "Evolution of hippocampal spatial representation over time in mice",
journal = "Neurobiology of Learning and Memory",
volume = "98",
number = "4",
pages = "354 - 360",
year = "2012",
issn = "1074-7427",
doi = "https://doi.org/10.1016/j.nlm.2012.10.004",
url = "http://www.sciencedirect.com/science/article/pii/S1074742712001232",
author = "Yannick Jeantet and Yoon H. Cho",
keywords = "Place cell, Long-term plasticity, Long-term stability, Memory consolidation, Attention, Behavior",
abstract = "To investigate the intriguing and paradoxical contrast between the time-limited role of the hippocampus in memory consolidation and its permanent contribution to spatial memory as revealed by place cell activity, we carefully monitored the temporal evolution of the same set of place cells in normal naïve mice throughout their familiarization to a spatial context and their consolidation of memory about space. Over six daily recording sessions, despite their widely reported stability, we observed gradual changes in hippocampal place fields and cell firing patterns. These changes were interpreted in terms of both improvement and impoverishment of spatial codes: improvement due to intrinsic place cell plasticity, and impoverishment as a consequence of attentional filtering of allocentric spatial information reaching the hippocampus due to the procedural behavioral requirements of the task, or to hippocampal disengagement as learning progresses."
}
@article{TETTEY2014369,
title = "Effects of different insulation materials on primary energy and CO2 emission of a multi-storey residential building",
journal = "Energy and Buildings",
volume = "82",
pages = "369 - 377",
year = "2014",
issn = "0378-7788",
doi = "https://doi.org/10.1016/j.enbuild.2014.07.009",
url = "http://www.sciencedirect.com/science/article/pii/S037877881400543X",
author = "Uniben Yao Ayikoe Tettey and Ambrose Dodoo and Leif Gustavsson",
keywords = "Insulation material, Primary energy, CO emission, Energy-efficiency, Fossil fuel, Residential building",
abstract = "In this study, we analyzed the implications of various insulation materials on the primary energy and CO2 emission for material production of a residential building. We modeled changes to the original design of the building to achieve reference buildings to energy-efficiency levels of the Swedish building code of 2012 or the Swedish Passivhus 2012 criteria. We varied the insulation materials in different parts of the reference buildings from mineral rock wool to glass wool, cellulose fiber, expanded polystyrene or foam glass. We compared the primary energy use and CO2 emission from material production of functionally equivalent reference and optimum versions of the building. The results showed a reduction of about 6–7% in primary energy use and 6–8% in CO2 emission when the insulation material in the reference buildings is changed from rock wool to cellulose fiber in the optimum versions. Also, the total fossil fuel use for only insulation material production was reduced by about 39%. This study suggests that enhancing material production technologies by reducing fossil fuel-use and increasing renewable energy sources, as well as careful material choice with renewable-based raw materials can contribute significantly in reducing primary energy use and GHG emission in the building sector."
}
@article{ROHDE2012412,
title = "Development and verification of the coupled 3D neutron kinetics/thermal-hydraulics code DYN3D-HTR for the simulation of transients in block-type HTGR",
journal = "Nuclear Engineering and Design",
volume = "251",
pages = "412 - 422",
year = "2012",
note = "5th International Topical Meeting on High Temperature Reactor Technology (HTR 2010)",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2011.09.051",
url = "http://www.sciencedirect.com/science/article/pii/S0029549311008454",
author = "U. Rohde and S. Baier and S. Duerigen and E. Fridman and S. Kliem and B. Merk",
abstract = "DYN3D is a nodal diffusion code for 3D steady-state and transient analysis of Light Water Reactor (LWR) cores with hexagonal or square fuel element geometry. In addition to the neutron kinetics, it comprises of a thermal-hydraulics model for flow in parallel coolant channels. Macroscopic cross section data libraries generated with variation of burn-up, reactor poisons concentrations and thermal-hydraulic feedback parameters are linked to the code. Two-group and multi-groups versions of the code are available. Currently, at the Helmholtz-Zentrum Dresden-Rossendorf (HZDR), the DYN3D code is being extended and adopted for the application to block-type High Temperature Gas-Cooled Reactors (HTGRs). In this paper, we give an overview of the latest developments of DYN3D concerning block-type HTGR. The simplified P3 (SP3) transport approximation is implemented into the multi-group DYN3D code to take anisotropy of the neutron flux and heterogeneity of the core more precisely into account. The SP3 method previously implemented into DYN3D for square fuel element geometry of LWR is being extended for hexagonal geometry of the graphite blocks, where the hexagons are subdivided into triangular nodes to be able to perform a systematic mesh refinement. One of the main challenges in cross section generation for the HTGR core calculations is the treatment of the so-called “double heterogeneity”. The modified Reactivity-Equivalent Physical Transformation (RPT) approach is applied in order to eliminate the double-heterogeneity of HTGR fuel elements in the deterministic lattice calculations. The main steps of the RPT method are described. The use of the method for the cross section generation of a simplified HTGR core including its verification is presented. A 3D heat conduction module coupled with a channel-type coolant flow model is implemented to take the temperature reactivity feedback to neutronics physically correctly into account. It is shown that there is significant redistribution of the produced heat by heat conduction between the graphite blocks."
}
@article{LOSH201440,
title = "From Authorship to Authoring: Critical Literacy, Expert Users, and Proprietary Software",
journal = "Computers and Composition",
volume = "33",
pages = "40 - 49",
year = "2014",
issn = "8755-4615",
doi = "https://doi.org/10.1016/j.compcom.2014.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S875546151400036X",
author = "Elizabeth Losh",
keywords = "Digital literacy, Software studies, Computer graphics",
abstract = "This essay argues that new authoring environments fundamentally change the authorship paradigm. Four kinds of critical literacy may be useful to produce computational media that responds appropriately to the larger rhetorical context of software culture. These critical literacies include not only writing code, but also learning a range of user interfaces, participating in design practices for debugging programs, and recognizing the norms of digital labor workflows and systems of credit."
}
@article{GANDELLI2018,
title = "A novel OpenSees element for single curved surface sliding isolators",
journal = "Soil Dynamics and Earthquake Engineering",
year = "2018",
issn = "0267-7261",
doi = "https://doi.org/10.1016/j.soildyn.2018.01.044",
url = "http://www.sciencedirect.com/science/article/pii/S0267726117303330",
author = "E. Gandelli and M. Penati and V. Quaglini and G. Lomiento and E. Miglio and G.M. Benzoni",
keywords = "Curved Surface Slider, Static coefficient of friction, Heating effect on friction, OpenSees, Finite element bearing element",
abstract = "The increasing use of curved surface sliding bearings as seismic isolators benefits from the improvement of analytical models that can accurately capture their experimental performance and enhance the predictive capability of nonlinear response history analyses. The mathematical formulation proposed in this paper aims at addressing the variability of the coefficient of friction based on experimental data that can be retrieved from prototype tests on Curved Surface Sliders. The formulation accounts for variation in the coefficient of friction with the instantaneous change of axial load and sliding velocity at the contact interface, and the accumulated heat due to cyclic motion; furthermore, it incorporates new features such as the static friction developed in the transition from the pre-sliding phase to the dynamic sliding condition. The proposed model has been coded in the object-oriented finite element software OpenSees by modifying the standard SingleFPSimple3d element that describes the force – displacement relationship of a bearing comprising one concave sliding surface and a spherical articulation. The main novelties of the new CSSBearing_BVNC element are inclusion of the static friction before the breakaway and degradation of kinetic friction induced by the heat developed during the motion of the articulated slider. The primary assumptions in the development of the friction model and the verification of the newly developed element are validated by agreement with available data. A case study helps to demonstrate the improved prediction capability of the new bearing element over its standard counterpart when applied to real situations, such as estimating a +50% increase in isolator displacement, superstructure drift and base shear demand under high intensity earthquakes, and possible non-activation of the sliding isolators under weak or medium intensity earthquakes."
}
@article{APEL201672,
title = "Improved absolute calibration of LOPES measurements and its impact on the comparison with REAS 3.11 and CoREAS simulations",
journal = "Astroparticle Physics",
volume = "75",
pages = "72 - 74",
year = "2016",
issn = "0927-6505",
doi = "https://doi.org/10.1016/j.astropartphys.2015.09.002",
url = "http://www.sciencedirect.com/science/article/pii/S0927650515001309",
author = "W.D. Apel and J.C. Arteaga-Velázquez and L. Bähren and K. Bekk and M. Bertaina and P.L. Biermann and J. Blümer and H. Bozdog and I.M. Brancus and E. Cantoni and A. Chiavassa and K. Daumiller and V. de Souza and F. Di Pierro and P. Doll and R. Engel and H. Falcke and B. Fuchs and H. Gemmeke and C. Grupen and A. Haungs and D. Heck and R. Hiller and J.R. Hörandel and A. Horneffer and D. Huber and T. Huege and P.G. Isar and K.-H. Kampert and D. Kang and O. Krömer and J. Kuijpers and K. Link and P. Łuczak and M. Ludwig and H.J. Mathes and M. Melissas and C. Morello and S. Nehls and J. Oehlschläger and N. Palmieri and T. Pierog and J. Rautenberg and H. Rebel and M. Roth and C. Rühle and A. Saftoiu and H. Schieler and A. Schmidt and S. Schoo and F.G. Schröder and O. Sima and G. Toma and G.C. Trinchero and A. Weindl and J. Wochele and J. Zabierowski and J.A. Zensus",
keywords = "Cosmic rays, Extensive air showers, Radio emission, LOPES, Absolute calibration",
abstract = "LOPES was a digital antenna array detecting the radio emission of cosmic-ray air showers. The calibration of the absolute amplitude scale of the measurements was done using an external, commercial reference source, which emits a frequency comb with defined amplitudes. Recently, we obtained improved reference values by the manufacturer of the reference source, which significantly changed the absolute calibration of LOPES. We reanalyzed previously published LOPES measurements, studying the impact of the changed calibration. The main effect is an overall decrease of the LOPES amplitude scale by a factor of 2.6 ± 0.2, affecting all previously published values for measurements of the electric-field strength. This results in a major change in the conclusion of the paper ‘Comparing LOPES measurements of air-shower radio emission with REAS 3.11 and CoREAS simulations’ published by Apel et al. (2013) : With the revised calibration, LOPES measurements now are compatible with CoREAS simulations, but in tension with REAS 3.11 simulations. Since CoREAS is the latest version of the simulation code incorporating the current state of knowledge on the radio emission of air showers, this new result indicates that the absolute amplitude prediction of current simulations now is in agreement with experimental data."
}
@article{GHOLAMPOUR2015206,
title = "Steganographic schemes with multiple q-ary changes per block of pixels",
journal = "Signal Processing",
volume = "108",
pages = "206 - 219",
year = "2015",
issn = "0165-1684",
doi = "https://doi.org/10.1016/j.sigpro.2014.09.008",
url = "http://www.sciencedirect.com/science/article/pii/S0165168414004186",
author = "Iman Gholampour and Khashayar Khosravi",
keywords = "Difference sets, Wet paper coding, Embedding efficiency, Embedding rate, Sum and Difference Covering Sets",
abstract = "A family of matrix embedding steganographic schemes for digital images is investigated. The target schemes are applied to blocks of n pixels in a cover image. In every block, at most m pixels are allowed to change with q-ary steps. We have derived some upper bounds on the embedding efficiency of these schemes for different values on m. It is also shown that these upper bounds approach the general upper bound on the embedding efficiency of q-ary steganography. For the case of q=3, we have shown that there is no feasible optimum member of the family for m=2, although for m=1, a well-known example exists. Instead, for m=2, a new close-to-bound scheme in the family is presented which exploits difference sets and wet paper coding in two separate channels. The proposed scheme outperforms other known members of the family, including the original Exploiting Modification Direction (EMD), several improved EMD versions, and two-change Sum and Difference Covering Set (SDCS) constructions. Some experiments are also conducted on an image dataset to compare the proposed scheme with other state-of-the-art Matrix embedding schemes. According to the experimental results, the performance of the proposed scheme is close to the state-of-the-arts at various embedding rates."
}
@article{POLEDNAK2012133,
title = "Recent decline in the U.S. death rate from myeloproliferative neoplasms, 1999–2006",
journal = "Cancer Epidemiology",
volume = "36",
number = "2",
pages = "133 - 136",
year = "2012",
issn = "1877-7821",
doi = "https://doi.org/10.1016/j.canep.2011.05.016",
url = "http://www.sciencedirect.com/science/article/pii/S1877782111000981",
author = "Anthony P. Polednak",
keywords = "Cancer mortality, Cancer surveillance, Multiple causes of death, Myeloproliferative neoplasms",
abstract = "Background: Myeloproliferative neoplasms (MPNs) are classified as neoplasms of uncertain or unknown behavior in the International Classification of Diseases (ICD) Version 10 and can contribute to risk of death from complications (especially thrombosis). Methods: U.S age-standardized death rates using ICD-Version 10 codes relevant to classical MPN (i.e., polycythemia vera, essential thrombocythemia, and “chronic myeloproliferative disease”) were examined for 1999–2006. The underlying cause of death and also all causes (“multiple causes” or “mentions”) coded on death certificates were considered. Trends were assessed by using percentage change (PC) in rate between 1999 and 2006, and annual percentage change (APC) estimated from linear regression. Results: The decline in death rates was large for MPN, whether based only the underlying cause (PC=−19.7%, APC=−3.4%) or on the substantially higher rates based on any cause (PC=−24.1%, APC=−3.8%), and was consistent by gender and age group (<65 and 65+ years). For deaths with MPN coded as other than the underlying cause, cardiovascular diseases were the most common underlying cause and the ASR for these deaths declined substantially (PC=−40.0%). Conclusions: Use of the underlying cause of death in surveillance will considerably underestimate MPN-related mortality rates in the population. Studies are needed on treatment in random samples of MPN patients from population-based cancer registries. Continued surveillance of MPN-related mortality rates in the population is needed in view of recent attempts (including the use of aspirin) to control cardiovascular complications of MPN."
}
@article{GORGHIU2013528,
title = "Consulting the Educational Actors - What Do Romanian Science Teachers Really Need?",
journal = "Procedia - Social and Behavioral Sciences",
volume = "83",
pages = "528 - 534",
year = "2013",
note = "2nd World Conference on Educational Technology Research",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2013.06.101",
url = "http://www.sciencedirect.com/science/article/pii/S1877042813011683",
author = "Gabriel Gorghiu and Laura Monica Gorghiu and Luminiţa Mihaela Drăghicescu",
keywords = "training needs analysis, teacher professional development, lifelong learning, in-service training, Inquiry-based Science Education, PROFILES Project ;",
abstract = "All the recent changes made in the European education system - and also in the Romanian one - claim from the teaching staff to be actively involved in specific lifelong learning actions. On the other hand, schools must proceed to a clear self-assessment of their educational offer, and also to assess the extent on meeting new challenges and identifying the actual training needs.However, for designing effective training programs, it must accurately be assessed the real in-service teachers’ training needs. In this respect, the training needs analysis helps on determining the proper ways that can enhance the teachers’ professional development, schools performances and quality assurance standards. This paper presents the main results of the training needs analysis performed in the frame of the FP7 European Research Project “PROFILES - Professional Reflection-Oriented Focus on Inquiry-based Learning and Education through Science” (code: 5.2.2.1-SiS-2010- 2.2.1-266589), with the view to illustrate an accurate image of the actual professional level of Science teachers, but also to consider the conclusions of analysis as a basis for designing a specific training program that targets to improve the teaching activities, through promoting reflection-oriented teaching, pedagogical and scientific competences, Inquiry-based Science Education (IBSE) and other related approaches which can be implemented in the educational environment."
}
@article{ZANOTTI2012106,
title = "Martensitic–Austenitic phase transformation of Ni–Ti SMAs: Thermal properties",
journal = "Intermetallics",
volume = "24",
pages = "106 - 114",
year = "2012",
issn = "0966-9795",
doi = "https://doi.org/10.1016/j.intermet.2012.01.026",
url = "http://www.sciencedirect.com/science/article/pii/S0966979512000489",
author = "C. Zanotti and P. Giuliani and A. Chrysanthou",
keywords = "A. Intermetallics, miscellaneous, B. Thermal properties, B. Martensitic transformations",
abstract = "The main scope of this work concerns the definition of the thermal conductivity temperature dependence of fully dense NiTi SMAs in the temperature range where the Martensitic–Austenitic phase transformation occurs. The methodology used to evaluate this thermal property is based on an experimental-numerical approach that requires the definition of the heat capacity temperature dependence and the knowledge of the latent heat of transformation. The experimental work is based on the capability to heat the cylindrical NiTi samples uniformly on one side and to impose a variety of initial heating rates ranging from 0.1 to 5K/s. Laser radiant energy was used as the heating source and the temperature history of the top and bottom NiTi sample surfaces were recorded using thermocouples. The numerical code considered the sample as a solid with a constant density and with thermal properties that were dependent on temperature. The heat capacity and latent heat of transformation were defined on the basis of the thermal analysis data, while the convection heat exchange coefficient was estimated from knowledge of the experimental configuration, lateral sample surface and the temperature field of the gas surrounding the sample. The results indicated that the thermal conductivity generally increased with temperature, but a minimum in the temperature range defining the Martensitic–Austenitic transformation has been pointed out. The higher thermal conductivity value of the Austenite phase is correlated with its electronic structure."
}
@article{GRIFFITHS201451,
title = "Phenomena identification and ranking table for thermal-hydraulic phenomena during a small-break LOCA with loss of high pressure injection",
journal = "Progress in Nuclear Energy",
volume = "73",
pages = "51 - 63",
year = "2014",
issn = "0149-1970",
doi = "https://doi.org/10.1016/j.pnucene.2014.01.008",
url = "http://www.sciencedirect.com/science/article/pii/S0149197014000109",
author = "M.J. Griffiths and J.P. Schlegel and T. Hibiki and M. Ishii and I. Kinoshita and Y. Yoshida",
keywords = "PIRT, LOCA, Small break, CSAU, Reactor safety",
abstract = "Currently Appendix K of 10 CFR 50 is used in the United States to evaluate models for the emergency cooling systems of light-water reactors. To assure that these models are accurate enough to ensure that the cooling systems are satisfactory, code scalability, applicability and uncertainty methodologies are used to evaluate the uncertainty in system analysis code predictions due to the various models. One cornerstone of this methodology is the development of the Phenomenon Identification and Ranking Table (PIRT), which summarizes the thermal-hydraulic phenomenon associate with a particular accident scenario and ranks their importance in determining the effectiveness of core cooling and the parts of the accident for which the phenomenon may be important. In this paper the PIRT developed by the Institute for Nuclear Safety Systems for a small-break LOCA with loss of high-pressure emergency coolant injection is analyzed in detail and several modifications are proposed based on a mechanistic understanding of the phenomenon involved. The resulting PIRT should provide a more accurate guide for model evaluation and development in advanced thermal-hydraulic system analysis codes."
}
@article{PITMAN201723,
title = "Coherent backscattering effect in spectra of icy satellites and its modeling using multi-sphere T-matrix (MSTM) code for layers of particles",
journal = "Planetary and Space Science",
volume = "149",
pages = "23 - 31",
year = "2017",
note = "Special Issue: Cosmic Dust IX",
issn = "0032-0633",
doi = "https://doi.org/10.1016/j.pss.2017.08.005",
url = "http://www.sciencedirect.com/science/article/pii/S0032063316304457",
author = "Karly M. Pitman and Ludmilla Kolokolova and Anne J. Verbiscer and Daniel W. Mackowski and Emily C.S. Joseph",
keywords = "Coherent backscattering effect,  VIMS, Ground-based, Infrared spectra, T-matrix modeling",
abstract = "The coherent backscattering effect (CBE), the constructive interference of light scattering in particulate surfaces (e.g., regolith), manifests as a non-linear increase in reflectance, or opposition surge, and a narrow negative polarization feature at small solar phase angles. Due to a strong dependence of the amplitude and angular width of this opposition surge on the absorptive characteristics of the surface material, CBE also produces phase-angle-dependent variations in the near-infrared spectra. In this paper we present a survey of such variations in the spectra of icy satellites of Saturn obtained by the Cassini spacecraft's Visual and Infrared Mapping Spectrometer (VIMS) and in the ground-based spectra of Oberon, a satellite of Uranus, obtained with TripleSpec, a cross-dispersed near-infrared spectrometer on the Astrophysical Research Consortium 3.5-m telescope located at the Apache Point Observatory near Sunspot, New Mexico. The paper also presents computer modeling of the saturnian satellite spectra and their phase-angle variations using the most recent version of the Multi-Sphere T-Matrix (MSTM) code developed to simulate light scattering by layers of randomly distributed spherical particles. The modeling allowed us not only to reproduce the observed effects but also to estimate characteristics of the icy particles that cover the surfaces of Rhea, Dione, and Tethys."
}
@article{BERNA2016563,
title = "Enhancement of the SPARC90 code to pool scrubbing events under jet injection regime",
journal = "Nuclear Engineering and Design",
volume = "300",
pages = "563 - 577",
year = "2016",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2016.02.027",
url = "http://www.sciencedirect.com/science/article/pii/S0029549316000881",
author = "C. Berna and A. Escrivá and J.L. Muñoz-Cobo and L.E. Herranz",
keywords = "L. Safety and risk analysis",
abstract = "Submerged gaseous jets may have an outstanding relevance in many industrial processes and may be of particular significance in severe nuclear accident scenarios, like in the Fukushima accident. Even though pool scrubbing has been traditionally associated with low injection velocities, there are a number of potential scenarios in which fission product trapping in aqueous ponds might also occur under jet injection regime (like SGTR meltdown sequences in PWRs and SBO ones in BWRs). The SPARC90 code was developed to determine the fission product trapping in pools during severe accidents. The code assumes that carrier gas arrives at the water ponds at low or moderate velocities and it forms a big bubble that eventually detaches from the injection pipe. However, particle laden gases may enter the water at very high velocities resulting in a submerged gas jet instead. This work presents the fundamentals, major hypotheses and changes introduced into the code in order to estimate particle removal during gas injection in pools under the jet regime (SPARC90-Jet). A simplified and reliable approach to submerged jet hydrodynamics has been implemented on the basis of updated equations for jet hydrodynamics and aerosol removal, so that gas–liquid and droplet-particles interactions are described. The code modifications have been validated as far as possible. However, no suitable hydrodynamic tests have been found in the literature, so that an indirect validation has been conducted through comparisons against data from pool scrubbing experiments. Besides, this validation has been forcefully limited since very few pool scrubbing tests are available in the jet injection regime (i.e., ACE, LACE, POSEIDON II and RCA). But nevertheless, a considerable improvement in the estimation of the Decontamination Factor (DF) has been reached, as well as it has been proven that sizes of aerosol particles and submergencies are factors of major influence, however there is still a long road ahead. We have extended the SPARC90 capabilities to study jet discharge processes, then the new SPARC90-Jet version is able to study globular and jet discharge processes, i.e. pool discharges under low and high velocity conditions. Therefore, the work here presented should be understood as a promising first step toward an effective code extension to the jet regime."
}
@article{YIN2018444,
title = "Numerical analysis of experimental studies of methane hydrate dissociation induced by depressurization in a sandy porous medium",
journal = "Applied Energy",
volume = "230",
pages = "444 - 459",
year = "2018",
issn = "0306-2619",
doi = "https://doi.org/10.1016/j.apenergy.2018.08.115",
url = "http://www.sciencedirect.com/science/article/pii/S0306261918312856",
author = "Zhenyuan Yin and George Moridis and Zheng Rong Chong and Hoon Kiang Tan and Praveen Linga",
keywords = "Methane hydrate, Depressurization, Kinetic reaction, Sandy porous medium, Heterogeneous, TOUGH+Hydrate v1.5",
abstract = "Methane Hydrates (MHs) are a promising energy source abundantly available in nature. Understanding the complex processes of MH formation and dissociation is critical for the development of safe and efficient technologies for energy recovery. Many laboratory and numerical studies have investigated these processes using synthesized MH-bearing sediments. A near-universal issue encountered in these studies is the spatial heterogeneous hydrate distribution in the testing apparatus. In the absence of direct observations (e.g. using X-ray computed tomography) coupled with real time production data, the common assumption made in almost all numerical studies is a homogeneous distribution of the various phases. In an earlier study (Yin et al., 2018) that involved the numerical description of a set of experiments on MH-formation in sandy medium using the excess water method, we showed that spatially heterogeneous phase distribution is inevitable and significant. In the present study, we use as a starting point the results and observations at the end of the MH formation and seek to numerically reproduce the laboratory experiments of depressurization-induced dissociation of the spatially-heterogeneous MH distribution. This numerical study faithfully reproduces the geometry of the laboratory apparatus, the initial and boundary conditions of the system, and the parameters of the dissociation stimulus, capturing accurately all stages of the experimental process. Using inverse modelling (history-matching) that minimized deviations between the experimental observations and numerical predictions, we determined the values of all the important flow, thermal, and kinetic parameters that control the system behaviour, which yielded simulation results that were in excellent agreement with the measurements of key monitored variables, i.e. pressure, temperature, cumulative production of gas and water over time. We determined that at the onset of depressurization (when the pressure drop – the driving force of dissociation – is at its maximum), the rate of MH dissociation approaches that of an equilibrium reaction and is limited by the heat transfer from the system surroundings. As the effect of depressurization declines over time, the dissociation reaction becomes kinetically limited despite significant heat inflows from the boundaries, which lead to localized temperature increases in the reactor."
}
@article{KESSOUS2019349,
title = "Smoking during pregnancy as a possible risk factor for pediatric neoplasms in the offspring: A population-based cohort study",
journal = "Addictive Behaviors",
volume = "90",
pages = "349 - 353",
year = "2019",
issn = "0306-4603",
doi = "https://doi.org/10.1016/j.addbeh.2018.11.039",
url = "http://www.sciencedirect.com/science/article/pii/S0306460318311638",
author = "Roy Kessous and Tamar Wainstock and Eyal Sheiner",
keywords = "Maternal smoking in pregnancy, Intrauterine environment, Childhood malignancy, Benign childhood tumors",
abstract = "Objective
The aim of this study was to evaluate the association between maternal smoking during pregnancy and future risk of childhood neoplasm risk.
Study design
A population based cohort analysis comparing the risk for long-term childhood neoplasms in children born (1991–2014) to mothers that smoked during pregnancy vs. those that did not. Childhood neoplasms were pre-defined based on ICD-9 codes, as recorded in the hospital medical files. Children with congenital malformations and multiple gestations were excluded from the analysis. Kaplan-Meier survival curves were constructed to compare cumulative oncological morbidity over time. Cox proportional hazards model was used to control for confounders.
Results
241,273 infants met the inclusion criteria; out of those 2841 were born to mothers that smoked during pregnancy. Offspring to smoking mothers had higher incidence of benign (OR 1.6, 95%CI 1.02–2.58; p value = .038) but not malignant tumors. Total cumulative neoplasm incidence was significantly higher in smoking women (Log Rank = 0.001) but no significant difference in the incidence of malignant tumors was noted (Log Rank = 0.834). In a Cox regression model controlling for maternal confounders; a history of maternal smoking during pregnancy remained independently associated only with increased risk for benign tumors (adjusted HR 2.5, 95%CI 1.57–3.83, p = .001).
Conclusion
Maternal smoking during pregnancy is associated with increased long-term risk for benign but not malignant tumors. This is important when counseling mothers regarding potential future risks and recommended lifestyle modifications. Despite this large population study with long follow-up, childhood malignancies are rare, and clarifying the possible association may require further studies."
}
@article{LIEFFERS2018229,
title = "Experiences and Perceptions of Adults Accessing Publicly Available Nutrition Behavior-Change Mobile Apps for Weight Management",
journal = "Journal of the Academy of Nutrition and Dietetics",
volume = "118",
number = "2",
pages = "229 - 239.e3",
year = "2018",
issn = "2212-2672",
doi = "https://doi.org/10.1016/j.jand.2017.04.015",
url = "http://www.sciencedirect.com/science/article/pii/S2212267217304197",
author = "Jessica R.L. Lieffers and Jose F. Arocha and Kelly Grindrod and Rhona M. Hanning",
keywords = "Weight management, Mobile applications, Adults, Nutrition, Qualitative research",
abstract = "Background
Nutrition mobile apps have become accessible and popular weight-management tools available to the general public. To date, much of the research has focused on quantitative outcomes with these tools (eg, weight loss); little is known about user experiences and perceptions of these tools when used outside of a research trial environment.
Objective
Our aim was to understand the experiences and perceptions of adult volunteers who have used publicly available mobile apps to support nutrition behavior change for weight management.
Design
We conducted one-on-one semi-structured interviews with individuals who reported using nutrition mobile apps for weight management outside of a research setting.
Participants/setting
Twenty-four healthy adults (n=19 females, n=5 males) who had used publicly available nutrition mobile apps for weight management for ≥1 week within the past 3 to 4 months were recruited from the community in southern Ontario and Edmonton, Canada, using different methods (eg, social media, posters, and word of mouth).
Qualitative data analysis
Interviews were audiorecorded, transcribed verbatim, and transcripts were verified against recordings. Data were coded inductively and organized into categories using NVivo, version 10 (QSR International).
Results
Participants used nutrition apps for various amounts of time (mean=approximately 14 months). Varied nutrition apps were used; however, MyFitnessPal was the most common. In the interviews, the following four categories of experiences with nutrition apps became apparent: food data entry (database, data entry methods, portion size, and complex foods); accountability, feedback, and progress (goal setting, accountability, monitoring, and feedback); technical and app-related factors; and personal factors (self-motivation, privacy, knowledge, and obsession). Most participants used apps without professional or dietitian support.
Conclusions
This work reveals that numerous factors affect use and ongoing adherence to use of nutrition mobile apps. These data are relevant to professionals looking to better assist individuals using these tools, as well as developers looking to develop new and improved apps."
}
@article{JAMAL201420,
title = "Modelling gravel beach dynamics with XBeach",
journal = "Coastal Engineering",
volume = "89",
pages = "20 - 29",
year = "2014",
issn = "0378-3839",
doi = "https://doi.org/10.1016/j.coastaleng.2014.03.006",
url = "http://www.sciencedirect.com/science/article/pii/S0378383914000593",
author = "M.H. Jamal and D.J. Simmonds and V. Magar",
keywords = "Numerical modelling, Infiltration, Sediment transport, Accretion, Berm, XBeach model",
abstract = "Numerical cross-shore profile evolution models have been good at predicting beach erosion during storm conditions, but have difficulty in predicting the accretion of the beach during calm periods. This paper describes the progress made in modifying and applying the public domain XBeach code to the prediction and explanation of the observed behaviour of coarse-grained beaches in the laboratory and the field under accretive conditions. The paper outlines in details the changes made to the original code (version 12), including the introduction of a new morphological module based upon Soulsby's sediment transport equation for waves and currents, and the incorporation of Packwood's infiltration approach in the unsaturated area of the swash region. The competence of this modified model during calm conditions for describing the steepening of the profile, and the growth of the beach berm is demonstrated. Preliminary results on the behaviour of the beach subject to both waves and tides are presented. Good agreement is found between the model simulations and large-scale laboratory measurements, as well as field observations from a composite beach in the UK. The reasons for the model's capabilities are discussed."
}
@article{PETROV20131192,
title = "Radar Emitter Signals Recognition and Classification with Feedforward Networks",
journal = "Procedia Computer Science",
volume = "22",
pages = "1192 - 1200",
year = "2013",
note = "17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems - KES2013",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2013.09.206",
url = "http://www.sciencedirect.com/science/article/pii/S187705091300999X",
author = "Nedyalko Petrov and Ivan Jordanov and Jon Roe",
keywords = "radar signals recognition, emitter identification, feedforward neural networks, classification",
abstract = "A possible application of neural networks for timely and reliable recognition of radar signal emitters is investigated. In particular, a large data set of intercepted generic radar signal samples is used for investigating and evaluating several neural network topologies, training parameters, input and output coding and machine learning facilitating data transformations. Three case studies are discussed, where in the first two the radar signals are classified in two broad classes – with civil or military application, based on patterns in their pulse train characteristics and in the third one trained to distinguish between several more specific radar functions. Very competitive results of about 82%, 84% and 67% are achieved on the testing data sets."
}