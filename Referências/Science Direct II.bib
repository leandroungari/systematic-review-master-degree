@article{CHUA2017109,
title = "How are e-leadership practices in implementing a school virtual learning environment enhanced? A grounded model study",
journal = "Computers & Education",
volume = "109",
pages = "109 - 121",
year = "2017",
issn = "0360-1315",
doi = "https://doi.org/10.1016/j.compedu.2017.02.012",
url = "http://www.sciencedirect.com/science/article/pii/S0360131517300441",
author = "Yan Piaw Chua and Yee Pei Chua",
keywords = "Computer-mediated communication, Interactive learning environments, Learning communities, Secondary education",
abstract = "E-leadership is defined as a social influence process mediated by information and communication technology to produce change in behavior and performance with individuals and groups in an organization. This study investigates e-leadership practices among users of a school virtual learning environment. It was performed in two stages. First, semi-structured interviews with school administrators, teachers, students, parents and school software experts were conducted. The qualitative data collected from the interviews were coded and analyzed using open and axial coding procedures. As a result, an e-leadership model emerged from the data that consisted of eight themes: e-leadership quality with seven core factors, namely, readiness, practices, strategies, support, culture, needs and obstacles. Second, the validity and reliability of the model were further ascertained with a quantitative survey study involving 320 school administrators. The findings of this study established a grounded model for e-leadership practices in schools."
}
@article{CATALA20131930,
title = "A meta-model for dataflow-based rules in smart environments: Evaluating user comprehension and performance",
journal = "Science of Computer Programming",
volume = "78",
number = "10",
pages = "1930 - 1950",
year = "2013",
note = "Special section on Language Descriptions Tools and Applications (LDTA’08 & ’09) & Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2012.06.010",
url = "http://www.sciencedirect.com/science/article/pii/S0167642312001232",
author = "Alejandro Catala and Patricia Pons and Javier Jaen and Jose A. Mocholi and Elena Navarro",
keywords = "Ambient intelligence, Customization, Dataflow, Visual language, Rule, Event based, Non-expert programmer, Smart home",
abstract = "A considerable part of the behavior in smart environments relies on event-driven and rule specification. Rules are the mechanism most often used to enable user customization of the environment. However, the expressiveness of the rules available to users in editing and other tools is usually either limited or the available rule editing interfaces are not designed for end-users with low skills in programming. This means we have to look for interaction techniques and new ways to define user customization rules. This paper describes a generic and flexible meta-model to support expressive rules enhanced with data flow expressions that will graphically support the definition of rules without writing code. An empirical study was conducted on the ease of understanding of the visual data flow expressions, which are the key elements in our rule proposal. The visual dataflow language was compared to its corresponding textual version in terms of comprehension and ease of learning by teenagers in exercises involving calculations, modifications, writing and detecting equivalences in expressions in both languages. Although the subjects had some previous experience in editing mathematical expressions on spreadsheets, the study found their performance with visual dataflows to be significantly better in calculation and modification exercises. This makes our dataflow approach a promising mechanism for expressing user-customized reactive behavior in Ambient Intelligence (AmI) environments. The performance of the rule matching processor was validated by means of two stress tests to ensure that the meta-model approach adopted would be able to scale up with the number of types and instances in the space."
}
@article{POLI201836,
title = "TORBEAM 2.0, a paraxial beam tracing code for electron-cyclotron beams in fusion plasmas for extended physics applications",
journal = "Computer Physics Communications",
volume = "225",
pages = "36 - 46",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2017.12.018",
url = "http://www.sciencedirect.com/science/article/pii/S001046551730423X",
author = "E. Poli and A. Bock and M. Lochbrunner and O. Maj and M. Reich and A. Snicker and A. Stegmeir and F. Volpe and N. Bertelli and R. Bilato and G.D. Conway and D. Farina and F. Felici and L. Figini and R. Fischer and C. Galperti and T. Happel and Y.R. Lin-Liu and N.B. Marushchenko and U. Mszanowski and F.M. Poli and J. Stober and E. Westerhof and R. Zille and A.G. Peeters and G.V. Pereverzev",
keywords = "Plasma physics, Magnetic confinement, Wave–plasma interactions, Electron cyclotron waves, Paraxial beam tracing",
abstract = "The paraxial WKB code TORBEAM (Poli, 2001) is widely used for the description of electron-cyclotron waves in fusion plasmas, retaining diffraction effects through the solution of a set of ordinary differential equations. With respect to its original form, the code has undergone significant transformations and extensions, in terms of both the physical model and the spectrum of applications. The code has been rewritten in Fortran 90 and transformed into a library, which can be called from within different (not necessarily Fortran-based) workflows. The models for both absorption and current drive have been extended, including e.g. fully-relativistic calculation of the absorption coefficient, momentum conservation in electron–electron collisions and the contribution of more than one harmonic to current drive. The code can be run also for reflectometry applications, with relativistic corrections for the electron mass. Formulas that provide the coupling between the reflected beam and the receiver have been developed. Accelerated versions of the code are available, with the reduced physics goal of inferring the location of maximum absorption (including or not the total driven current) for a given setting of the launcher mirrors. Optionally, plasma volumes within given flux surfaces and corresponding values of minimum and maximum magnetic field can be provided externally to speed up the calculation of full driven-current profiles. These can be employed in real-time control algorithms or for fast data analysis."
}
@article{ELUSZKIEWICZ201731,
title = "A fast code for channel limb radiances with gas absorption and scattering in a spherical atmosphere",
journal = "Journal of Quantitative Spectroscopy and Radiative Transfer",
volume = "193",
pages = "31 - 39",
year = "2017",
issn = "0022-4073",
doi = "https://doi.org/10.1016/j.jqsrt.2017.02.010",
url = "http://www.sciencedirect.com/science/article/pii/S0022407317301152",
author = "Janusz Eluszkiewicz and Gennady Uymin and David Flittner and Karen Cady-Pereira and Eli Mlawer and John Henderson and Jean-Luc Moncet and Thomas Nehrkorn and Michael Wolff",
keywords = "Radiative transfer, Limb sounding, Mars",
abstract = "We present a radiative transfer code capable of accurately and rapidly computing channel limb radiances in the presence of gaseous absorption and scattering in a spherical atmosphere. The code has been prototyped for the Mars Climate Sounder measuring limb radiances in the thermal part of the spectrum (200–900cm−1) where absorption by carbon dioxide and water vapor and absorption and scattering by dust and water ice particles are important. The code relies on three main components: 1) The Gauss Seidel Spherical Radiative Transfer Model (GSSRTM) for scattering, 2) The Planetary Line-By-Line Radiative Transfer Model (P-LBLRTM) for gas opacity, and 3) The Optimal Spectral Sampling (OSS) for selecting a limited number of spectral points to simulate channel radiances and thus achieving a substantial increase in speed. The accuracy of the code has been evaluated against brute-force line-by-line calculations performed on the NASA Pleiades supercomputer, with satisfactory results. Additional improvements in both accuracy and speed are attainable through incremental changes to the basic approach presented in this paper, which would further support the use of this code for real-time retrievals and data assimilation. Both newly developed codes, GSSRTM/OSS for MCS and P-LBLRTM, are available for additional testing and user feedback."
}
@article{SOOT20151941,
title = "Reflection on Teaching: A Way to Learn from Practice",
journal = "Procedia - Social and Behavioral Sciences",
volume = "191",
pages = "1941 - 1946",
year = "2015",
note = "The Proceedings of 6th World Conference on educational Sciences",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2015.04.591",
url = "http://www.sciencedirect.com/science/article/pii/S1877042815028517",
author = "Anu Sööt and Ele Viskus",
keywords = "Reflection, core reflection, teacher education, higher education ;",
abstract = "Developing students’ reflection on their learning is currently one of the major learning goals in higher education. Today's students need to be prepared to function in the rapidly changing world of professional practice. In line with the above, reflection is currently a key concept in teacher education. The purpose of the present study is to support student teachers’ reflection. More specifically, to find out what kind of problematic situations students face in their practical teaching and which levels of activity they report in reflection when using a reduced version of the guided reflection procedure. The analysis is based on 34 written individual reports of the student teachers from a university in Estonia. Data was analysed using qualitative content analysis method, the employed coding scheme was developed based on Korthagen & Vasalos (2005) model of core reflection. The majority of problems were brought out in connection with the students themselves. Finding solutions to the problematic situations showed that reflections were made on all levels of the onion model (Korthagen & Vasalos, 2005). The most frequent level of reflection was the level of beliefs, followed by environment, behaviour, competencies, identity and mission."
}
@article{PETERSEN2012698,
title = "Local electron tomography using angular variations of surface tangents: Stomo version 2",
journal = "Computer Physics Communications",
volume = "183",
number = "3",
pages = "698 - 704",
year = "2012",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2011.11.008",
url = "http://www.sciencedirect.com/science/article/pii/S0010465511003705",
author = "T.C. Petersen and S.P. Ringer",
keywords = "Electron tomography, Morphology, Differential geometry, Atom probe tomography",
abstract = "In a recent publication, we investigated the prospect of measuring the outer three-dimensional (3D) shapes of nano-scale atom probe specimens from tilt-series of images collected in the transmission electron microscope. For this purpose alone, an algorithm and simplified reconstruction theory were developed to circumvent issues that arise in commercial “back-projection” computations in this context. In our approach, we give up the difficult task of computing the complete 3D continuum structure and instead seek only the 3D morphology of internal and external scattering interfaces. These interfaces can be described as embedded 2D surfaces projected onto each image in a tilt series. Curves and other features in the images are interpreted as inscribed sets of tangent lines, which intersect the scattering interfaces at unknown locations along the direction of the incident electron beam. Smooth angular variations of the tangent line abscissa are used to compute the surface tangent intersections and hence the 3D morphology as a “point cloud”. We have published the explicit details of our alternative algorithm along with the source code entitled “stomo_version_1”. For this work, we have further modified the code to efficiently handle rectangular image sets, perform much faster tangent-line “edge detection” and smoother tilt-axis image alignment using simple bi-linear interpolation. We have also adapted the algorithm to detect tangent lines as “ridges”, based upon 2nd order partial derivatives of the image intensity; the magnitude and orientation of which is described by a Hessian matrix. Ridges are more appropriate descriptors for tangent-line curves in phase contrast images outlined by Fresnel fringes or absorption contrast data from fine-scale objects. Improved accuracy, efficiency and speed for “stomo_version_2” is demonstrated in this paper using both high resolution electron tomography data of a nano-sized atom probe tip and simulated absorption-contrast images.
Program summary
Program title: STOMO version 2 Catalogue identifier: AEFS_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEFS_v2_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 2854 No. of bytes in distributed program, including test data, etc.: 23 559 Distribution format: tar.gz Programming language: C/C++ Computer: PC Operating system: Windows XP RAM: Scales as the product of experimental image dimensions multiplied by the number of points chosen by the user in polynomial fitting. Typical runs require between 50 Mb and 100 Mb of RAM. Supplementary material: Sample output files, for the test run provided, are available. Classification: 7.4, 14 Catalogue identifier of previous version: AEFS_v1_0 Journal reference of previous version: Comput. Phys. Comm. 181 (2010) 676 Does the new version supersede the previous version?: Yes Nature of problem: A local electron tomography algorithm of specimens for which conventional back projection may fail and or data for which there is a limited angular range (which would otherwise cause significant ‘missing-wedge’ artefacts). The algorithm does not solve the tomography back projection problem but rather locally reconstructs the 3D morphology of surfaces defined by varied scattering densities. Solution method: Local reconstruction is effected using image-analysis edge and ridge detection computations on experimental tilt series to measure smooth angular variations of surface tangent-line intersections, which generate point clouds decorating the embedded and or external scattering surfaces of a specimen. Reasons for new version: The new version was coded to cater for rectangular images in experimental tilt-series, ensure smoother image rotations, provide ridge detection (suitable for sensing phase-contrast Fresnel fringes and other fine-scale structures), faster/larger kernel edge detection and also greatly reduce RAM usage. Specimen surface normals are also explicitly computed from tangent-line and edge intersections, providing new information for potential use in point cloud rendering. Hysteresis thresholding implemented in the version 1 edge-detection algorithm provided only sparse edge-linking. Version 2 now implements edge tracking using recursion to fully link the edges during hysteresis thresholding. Furthermore in version 1 the minimum number of fitted polynomial points (specified in the input file) was not correctly imposed, which has been fixed for version 2. Most of these changes increase the accuracy of 3d morphology surface-tomography reconstructions by facilitating the use of more/finer tilt angles and experimental images of increased spatial-resolution. The ridge detection was incorporated to specifically improve the reconstruction of internal specimen morphology. Summary of revisions:•Included Hessian() function to compute 2nd order spatial derivatives of image intensities (operates in the same fashion as the previous and existing Sobel() function).•Changed convolve_Gaussian() function to alternatively use successive 1D convolutions (rather than cumbersome 2D summations implemented in version 1), resulting in a large increase in computational speed without any loss in accuracy. The convolution kernel size was hence widened to three times the full width half maximum of the Gaussian filter to improve scale-space selection accuracy.•A ridge detection option was included to compute edge maps sensitive to ridges, rather than edges, using elements from a Hessian matrix; the eigenvalues of which were used to define ridge direction for Canny-type hysteresis thresholding. Function edge_detect_Canny() was also altered to pass the gradient-direction maps (from either Hessian or Sobel based operators) in and out of scope for computation of surface normals; thereby enabling the output of both point-cloud and corresponding unstructured vector-field surface descriptors.•Function rotate_imgs() was changed to incorporate basic bi-linear interpolation for improved tilt-axis alignment of the entire tilt series in exp_data.dat. Smoother and more accurate edge maps are thereby produced.•Algorithm convert_point_cloud_to_tomogram() was created to output the tomogram 3d_imgs.dat in a more memory efficient manner. The function shell_sort(), adapted from numerical recipes in C, was also coded for this purpose.•The new function compute_xyz() was coded to calculate point-clouds and tomogram surface normals using information from single tilt images, as opposed to the entire stack. This function is hence used iteratively throughout the reconstruction as each tilt image is analysed in succession.•The new function reconstruct_local() is the heart of stomo_version_2.cpp. the main() source code in stomo_version_1.cpp has been rewritten here to process experimental images and edge maps one at a time, using a buffered 3d array of dimensions dictated solely by the number of tilt images required for the local SVD fit of the angular variations. These changes (along with similar iterative file writing) have been made to vastly reduce memory usage and hence allow higher spatial and angular resolution data sets to be analysed without recourse to high performance computing resources.•The input file has been simplified by removing the ‘slices’ and ‘channels’ settings (used in version 1 for crude image binning), which are now equal to the respective numbers of image rows and columns.•Every summation over image rows and columns has been checked to enable the analysis of rectangular images without error. For images of specimens with high aspect-ratios, such as narrow tips, these fixes allow significant reductions in computation time and memory usage.•Some arrays in the source code were not appropriately zeroed in version 1, causing reconstruction artefacts in some cases. These problems have now been fixed.•Fixed an if-statement to correctly impose the minimum number of fitted polynomial points, thereby reducing noise in the reconstructed data.•Implemented proper edge linking in the hysteresis thresholding code for Canny edge detection. Restrictions: The input experimental tilt-series of images must be registered with respect to a common single tilt axis with known orientation and position. Running time: For high quality reconstruction, 2–5 min."
}
@article{REZAEEHAJIDEHI2018208,
title = "Gradient-enhanced model and its micromorphic regularization for simulation of Lüders-like bands in shape memory alloys",
journal = "International Journal of Solids and Structures",
volume = "135",
pages = "208 - 218",
year = "2018",
issn = "0020-7683",
doi = "https://doi.org/10.1016/j.ijsolstr.2017.11.021",
url = "http://www.sciencedirect.com/science/article/pii/S0020768317305231",
author = "Mohsen Rezaee Hajidehi and Stanisław Stupkiewicz",
keywords = "Martensite, Phase transformation, Micromorphic model, Strain localization, Thermomechanical coupling",
abstract = "Shape memory alloys, notably NiTi, often exhibit softening pseudoelastic response that results in formation and propagation of Lüders-like bands upon loading, for instance, in uniaxial tension. A common approach to modelling softening and strain localization is to resort to gradient-enhanced formulations that are capable of restoring well-posedness of the boundary-value problem. This approach is also followed in the present paper by introducing a gradient-enhancement into a simple one-dimensional model of pseudoelasticity. In order to facilitate computational treatment, a micromorphic-type regularization of the gradient-enhanced model is subsequently performed. The formulation employs the incremental energy minimization framework that is combined with the augmented Lagrangian treatment of the resulting non-smooth minimization problem. A thermomechanically coupled model is also formulated and implemented in a finite-element code. The effect of the loading rate on the localization pattern in a NiTi wire under tension is studied, and the features predicted by the model show a good agreement with the experimental observations. Additionally, an analytical solution is provided for a propagating interface (macroscopic transformation front) both for the gradient-enhanced model and for its micromorphic version."
}
@article{LI2018285,
title = "A novel prediction method for down-hole working conditions of the beam pumping unit based on 8-directions chain codes and online sequential extreme learning machine",
journal = "Journal of Petroleum Science and Engineering",
volume = "160",
pages = "285 - 301",
year = "2018",
issn = "0920-4105",
doi = "https://doi.org/10.1016/j.petrol.2017.10.052",
url = "http://www.sciencedirect.com/science/article/pii/S092041051730832X",
author = "Kun Li and Ying Han and Tong Wang",
keywords = "Beam pumping unit, Working conditions prediction, Time series analysis, 8-Directions chain codes, OS-ELM, Grey interval relational degree",
abstract = "In the oilfield operation, the beam pumping unit is a very important artificial lift method. As the down-hole parts work at hundred and thousand meters underground, they are hard to be found immediately when failures come out. If we can predict down-hole working conditions and master its continuous operation states in time, great improvement of the oil well production will be developed. In this paper, a novel down-hole working conditions prediction method for the beam pumping unit based on the chaos time series prediction is proposed. First, curve contour of the dynamometer card is redrawn by 8-directions chain codes, and then eight feature vectors are extracted to construct eight feature vector time series; then, the online sequential extreme learning machine (OS-ELM) method is used to build the prediction model, which can realize fast updating with dynamic work condition changes; finally, the grey interval relational degree between the predicted feature vectors and feature vectors of each fault type is calculated to determine the predicted fault type. Actual production data of an oil well are used for example verification, and both online diagnosis and offline diagnosis illustrate the effectiveness of the method."
}
@article{KAMIYAMA2016138,
title = "A neural mechanism of dynamic gating of task-relevant information by top-down influence in primary visual cortex",
journal = "Biosystems",
volume = "150",
pages = "138 - 148",
year = "2016",
issn = "0303-2647",
doi = "https://doi.org/10.1016/j.biosystems.2016.09.009",
url = "http://www.sciencedirect.com/science/article/pii/S0303264716302398",
author = "Akikazu Kamiyama and Kazuhisa Fujita and Yoshiki Kashimori",
keywords = "Top-down influence, Primary visual cortex, Perceptual learning, Neural model",
abstract = "Visual recognition involves bidirectional information flow, which consists of bottom-up information coding from retina and top-down information coding from higher visual areas. Recent studies have demonstrated the involvement of early visual areas such as primary visual area (V1) in recognition and memory formation. V1 neurons are not passive transformers of sensory inputs but work as adaptive processor, changing their function according to behavioral context. Top-down signals affect tuning property of V1 neurons and contribute to the gating of sensory information relevant to behavior. However, little is known about the neuronal mechanism underlying the gating of task-relevant information in V1. To address this issue, we focus on task-dependent tuning modulations of V1 neurons in two tasks of perceptual learning. We develop a model of the V1, which receives feedforward input from lateral geniculate nucleus and top-down input from a higher visual area. We show here that the change in a balance between excitation and inhibition in V1 connectivity is necessary for gating task-relevant information in V1. The balance change well accounts for the modulations of tuning characteristic and temporal properties of V1 neuronal responses. We also show that the balance change of V1 connectivity is shaped by top-down signals with temporal correlations reflecting the perceptual strategies of the two tasks. We propose a learning mechanism by which synaptic balance is modulated. To conclude, top-down signal changes the synaptic balance between excitation and inhibition in V1 connectivity, enabling early visual area such as V1 to gate context-dependent information under multiple task performances."
}
@article{OUNI201518,
title = "Improving multi-objective code-smells correction using development history",
journal = "Journal of Systems and Software",
volume = "105",
pages = "18 - 39",
year = "2015",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2015.03.040",
url = "http://www.sciencedirect.com/science/article/pii/S0164121215000631",
author = "Ali Ouni and Marouane Kessentini and Houari Sahraoui and Katsuro Inoue and Mohamed Salah Hamdi",
keywords = "Search-based software engineering, Refactoring, Code-smells",
abstract = "One of the widely used techniques to improve the quality of software systems is refactoring. Software refactoring improves the internal structure of the system while preserving its external behavior. These two concerns drive the existing approaches to refactoring automation. However, recent studies demonstrated that these concerns are not enough to produce correct and consistent refactoring solutions. In addition to quality improvement and behavior preservation, studies consider, among others, construct semantics preservation and minimization of changes. From another perspective, development history was proven as a powerful source of knowledge in many maintenance tasks. Still, development history is not widely explored in the context of automated software refactoring. In this paper, we use the development history collected from existing software projects to propose new refactoring solutions taking into account context similarity with situations seen in the past. We propose a multi-objective optimization-based approach to find good refactoring sequences that (1) minimize the number of code-smells, and (2) maximize the use of development history while (3) preserving the construct semantics. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between these three objectives. We evaluate our approach using a benchmark composed of five medium and large-size open-source systems and four types of code-smells (Blob, spaghetti code, functional decomposition, and data class). Our experimental results show the effectiveness of our approach, compared to three different state-of-the-art approaches, with more than 85% of code-smells fixed and 86% of suggested refactorings semantically coherent when the change history is used."
}
@article{TODD201841,
title = "Initial Uncertainty Impacts Statistical Learning in Sound Sequence Processing",
journal = "Neuroscience",
volume = "389",
pages = "41 - 53",
year = "2018",
note = "Sensory Sequence Processing in the Brain",
issn = "0306-4522",
doi = "https://doi.org/10.1016/j.neuroscience.2018.05.011",
url = "http://www.sciencedirect.com/science/article/pii/S030645221830352X",
author = "Juanita Todd and Alexander Provost and Lisa Whitson and Daniel Mullens",
keywords = "Auditory evoked potentials, Sequential learning, Predictive coding, Mismatch negativity, Primacy bias",
abstract = "This paper features two studies confirming a lasting impact of first learning on how subsequent experience is weighted in early relevance-filtering processes. In both studies participants were exposed to sequences of sound that contained a regular pattern on two different timescales. Regular patterning in sound is readily detected by the auditory system and used to form “prediction models” that define the most likely properties of sound to be encountered in a given context. The presence and strength of these prediction models is inferred from changes in automatically elicited components of auditory evoked potentials. Both studies employed sound sequences that contained both a local and longer-term pattern. The local pattern was defined by a regular repeating pure tone occasionally interrupted by a rare deviating tone (p=0.125) that was physically different (a 30msvs. 60ms duration difference in one condition and a 1000Hz vs. 1500Hz frequency difference in the other). The longer-term pattern was defined by the rate at which the two tones alternated probabilities (i.e., the tone that was first rare became common and the tone that was first common became rare). There was no task related to the tones and participants were asked to ignore them while focussing attention on a movie with subtitles. Auditory-evoked potentials revealed long lasting modulatory influences based on whether the tone was initially encountered as rare and unpredictable or common and predictable. The results are interpreted as evidence that probability (or indeed predictability) assigns a differential information-value to the two tones that in turn affects the extent to which prediction models are updated and imposed. These effects are exposed for both common and rare occurrences of the tones. The studies contribute to a body of work that reveals that probabilistic information is not faithfully represented in these early evoked potentials and instead exposes that predictability (or conversely uncertainty) may trigger value-based learning modulations even in task-irrelevant incidental learning."
}
@article{JAAFAR2017311,
title = "Analyzing software evolution and quality by extracting Asynchrony change patterns",
journal = "Journal of Systems and Software",
volume = "131",
pages = "311 - 322",
year = "2017",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.05.047",
url = "http://www.sciencedirect.com/science/article/pii/S0164121217300948",
author = "Fehmi Jaafar and Angela Lozano and Yann-Gaël Guéhéneuc and Kim Mens",
keywords = "Change patterns, Anti-patterns, Clones, Fault-proneness, Software quality",
abstract = "Change patterns describe two or more files were often changed together during the development or the maintenance of software systems. Several studies have been presented to detect change patterns and to analyze their types and their impact on software quality. In this context, we introduced the Asynchrony change pattern to describes a set of files that always change together in the same change periods, regardless developers who maintained them. In this paper, we investigate the impact of Asynchrony change pattern on design and code smells such as anti-patterns and code clones. Concretely, we conduct an empirical study by detecting Asynchrony change patterns, anti-patterns and code clones occurrences on 22 versions of four software systems and analyzing their fault-proneness. Results show that cloned files that follow the same Asynchrony change patterns have significantly increased fault-proneness with respect to other clones, and that anti-patterns following the same Asynchrony change pattern can be up to five times more risky in terms of fault-proneness as compared to other anti-patterns. Asynchrony change patterns thus seem to be strong indicators of fault-proneness for clones and anti-patterns."
}
@article{PENG2016143,
title = "Semi-supervised subspace learning with L2graph",
journal = "Neurocomputing",
volume = "208",
pages = "143 - 152",
year = "2016",
note = "SI: BridgingSemantic",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2015.11.112",
url = "http://www.sciencedirect.com/science/article/pii/S092523121630443X",
author = "Xi Peng and Miaolong Yuan and Zhiding Yu and Wei Yun Yau and Lei Zhang",
keywords = "Bio-inspired feature learning, Automatic subspace learning, Dimension reduction, Graph embedding",
abstract = "Subspace learning aims to learn a projection matrix from a given training set so that a transformation of raw data to a low-dimensional representation can be obtained. In practice, the labels of some training samples are available, which can be used to improve the discrimination of low-dimensional representation. In this paper, we propose a semi-supervised learning method which is inspired by the biological observation of similar inputs having similar codes (SISC), i.e., the same collection of cortical columns of the mammal׳s visual cortex is always activated by the similar stimuli. More specifically, we propose a mathematical formulation of SISC which minimizes the distance among the data points with the same label while maximizing the separability between different subjects in the projection space. The proposed method, namely, semi-supervised L2graph (SeL2graph) has two advantages: (1) unlike the classical dimension reduction methods such as principle component analysis, SeL2graph can automatically determine the dimension of feature space. This remarkably reduces the effort to find an optimal feature dimension for a good performance; and (2) it fully exploits the prior knowledge carried by the labeled samples and thus the obtained features are with higher discrimination and compactness. Extensive experiments show that the proposed method outperforms 7 subspace learning algorithms on 15 data sets with respect to classification accuracy, computational efficiency, and robustness to noises and disguises."
}
@article{JOHANSON2017109,
title = "Modeling polyp activity of Paragorgia arborea using supervised learning",
journal = "Ecological Informatics",
volume = "39",
pages = "109 - 118",
year = "2017",
issn = "1574-9541",
doi = "https://doi.org/10.1016/j.ecoinf.2017.02.007",
url = "http://www.sciencedirect.com/science/article/pii/S1574954116301406",
author = "Arne N. Johanson and Sascha Flögel and Wolf-Christian Dullo and Peter Linke and Wilhelm Hasselbring",
abstract = "While the distribution patterns of cold-water corals, such as Paragorgia arborea, have received increasing attention in recent studies, little is known about their in situ activity patterns. In this paper, we examine polyp activity in P. arborea using machine learning techniques to analyze high-resolution time series data and photographs obtained from an autonomous lander cluster deployed in the Stjernsund, Norway. An interactive illustration of the models derived in this paper is provided online as supplementary material. We find that the best predictor of the degree of extension of the coral polyps is current direction with a lag of three hours. Other variables that are not directly associated with water currents, such as temperature and salinity, offer much less information concerning polyp activity. Interestingly, the degree of polyp extension can be predicted more reliably by sampling the laminar flows in the water column above the measurement site than by sampling the more turbulent flows in the direct vicinity of the corals. Our results show that the activity patterns of the P. arborea polyps are governed by the strong tidal current regime of the Stjernsund. It appears that P. arborea does not react to shorter changes in the ambient current regime but instead adjusts its behavior in accordance with the large-scale pattern of the tidal cycle itself in order to optimize nutrient uptake."
}
@article{HARMAN2015378,
title = "Case-Based Learning Facilitates Critical Thinking in Undergraduate Nutrition Education: Students Describe the Big Picture",
journal = "Journal of the Academy of Nutrition and Dietetics",
volume = "115",
number = "3",
pages = "378 - 388",
year = "2015",
issn = "2212-2672",
doi = "https://doi.org/10.1016/j.jand.2014.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S2212267214013677",
author = "Tara Harman and Brenda Bertrand and Annette Greer and Arianna Pettus and Jill Jennings and Elizabeth Wall-Bassett and Oyinlola Toyin Babatunde",
keywords = "Dietetics, Teaching strategy, Case studies, Case method, Problem-based learning",
abstract = "Background
The vision of dietetics professions is based on interdependent education, credentialing, and practice. Case-based learning is a method of problem-based learning that is designed to heighten higher-order thinking. Case-based learning can assist students to connect education and specialized practice while developing professional skills for entry-level practice in nutrition and dietetics.
Objective
This study examined student perspectives of their learning after immersion into case-based learning in nutrition courses.
Design
The theoretical frameworks of phenomenology and Bloom’s Taxonomy of Educational Objectives triangulated the design of this qualitative study.
Participants/setting
Data were drawn from 426 written responses and three focus group discussions among 85 students from three upper-level undergraduate nutrition courses.
Main outcome measures
Coding served to deconstruct the essence of respondent meaning given to case-based learning as a learning method. The analysis of the coding was the constructive stage that led to configuration of themes and theoretical practice pathways about student learning.
Results
Four leading themes emerged. Story or Scenario represents the ways that students described case-based learning, changes in student thought processes to accommodate case-based learning are illustrated in Method of Learning, higher cognitive learning that was achieved from case-based learning is represented in Problem Solving, and Future Practice details how students explained perceived professional competency gains from case-based learning.
Conclusions
The skills that students acquired are consistent with those identified as essential to professional practice. In addition, the common concept of Big Picture was iterated throughout the themes and demonstrated that case-based learning prepares students for multifaceted problems that they are likely to encounter in professional practice."
}
@article{REFICE2012293,
title = "SIGNUM: A Matlab, TIN-based landscape evolution model",
journal = "Computers & Geosciences",
volume = "45",
pages = "293 - 303",
year = "2012",
issn = "0098-3004",
doi = "https://doi.org/10.1016/j.cageo.2011.11.013",
url = "http://www.sciencedirect.com/science/article/pii/S009830041100392X",
author = "A. Refice and E. Giachetta and D. Capolongo",
keywords = "Landscape evolution model, Matlab, TIN, Morphodynamics",
abstract = "Several numerical landscape evolution models (LEMs) have been developed to date, and many are available as open source codes. Most are written in efficient programming languages such as Fortran or C, but often require additional code efforts to plug in to more user-friendly data analysis and/or visualization tools to ease interpretation and scientific insight. In this paper, we present an effort to port a common core of accepted physical principles governing landscape evolution directly into a high-level language and data analysis environment such as Matlab. SIGNUM (acronym for Simple Integrated Geomorphological Numerical Model) is an independent and self-contained Matlab, TIN-based landscape evolution model, built to simulate topography development at various space and time scales. SIGNUM is presently capable of simulating hillslope processes such as linear and nonlinear diffusion, fluvial incision into bedrock, spatially varying surface uplift which can be used to simulate changes in base level, thrust and faulting, as well as effects of climate changes. Although based on accepted and well-known processes and algorithms in its present version, it is built with a modular structure, which allows to easily modify and upgrade the simulated physical processes to suite virtually any user needs. The code is conceived as an open-source project, and is thus an ideal tool for both research and didactic purposes, thanks to the high-level nature of the Matlab environment and its popularity among the scientific community. In this paper the simulation code is presented together with some simple examples of surface evolution, and guidelines for development of new modules and algorithms are proposed."
}
@article{JEANNE2013352,
title = "Associative Learning Enhances Population Coding by Inverting Interneuronal Correlation Patterns",
journal = "Neuron",
volume = "78",
number = "2",
pages = "352 - 363",
year = "2013",
issn = "0896-6273",
doi = "https://doi.org/10.1016/j.neuron.2013.02.023",
url = "http://www.sciencedirect.com/science/article/pii/S0896627313001815",
author = "James M. Jeanne and Tatyana O. Sharpee and Timothy Q. Gentner",
abstract = "Summary
Learning-dependent cortical encoding has been well described in single neurons. But behaviorally relevant sensory signals drive the coordinated activity of millions of cortical neurons; whether learning produces stimulus-specific changes in population codes is unknown. Because the pattern of firing rate correlations between neurons—an emergent property of neural populations—can significantly impact encoding fidelity, we hypothesize that it is a target for learning. Using an associative learning procedure, we manipulated the behavioral relevance of natural acoustic signals and examined the evoked spiking activity in auditory cortical neurons in songbirds. We show that learning produces stimulus-specific changes in the pattern of interneuronal correlations that enhance the ability of neural populations to recognize signals relevant for behavior. This learning-dependent enhancement increases with population size. The results identify the pattern of interneuronal correlation in neural populations as a target of learning that can selectively enhance the representations of specific sensory signals."
}
@article{KHATCHADOURIAN201756,
title = "Detecting broken pointcuts using structural commonality and degree of interest",
journal = "Science of Computer Programming",
volume = "150",
pages = "56 - 74",
year = "2017",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2017.06.011",
url = "http://www.sciencedirect.com/science/article/pii/S0167642317301326",
author = "Raffi Khatchadourian and Awais Rashid and Hidehiko Masuhara and Takuya Watanabe",
keywords = "Software development environments, Software maintenance, Software tools",
abstract = "Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base-code can lead to join points incorrectly falling in or out of the scope of pointcuts. Deciding which pointcuts have broken due to base-code changes is a daunting venture, especially in large and complex systems. We present an automated approach that recommends pointcuts that are likely to require modification due to a particular base-code change, as well as ones that do not. Our hypothesis is that join points selected by a pointcut exhibit common structural characteristics. Patterns describing such commonality are used to recommend pointcuts that have potentially broken with a degree of confidence as the developer is typing. The approach is implemented as an extension to the popular Mylyn Eclipse IDE plug-in, which maintains focused contexts of entities relevant to the task at hand using a Degree of Interest (DOI) model. We show that it is accurate in revealing broken pointcuts by applying it to multiple versions of several open source projects and evaluating the quality of the recommendations produced against actual modifications. We found that our tool made broken pointcuts 2.14 times more interesting in the DOI model than unbroken ones, with a p-value under 0.1, indicating a significant difference in final DOI value between the two kinds of pointcuts (i.e., broken and unbroken)."
}
@article{GREGOR201874,
title = "Trimming and gluing Gray codes",
journal = "Theoretical Computer Science",
volume = "714",
pages = "74 - 95",
year = "2018",
issn = "0304-3975",
doi = "https://doi.org/10.1016/j.tcs.2017.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0304397517308678",
author = "Petr Gregor and Torsten Mütze",
keywords = "Gray code, Subset, Combination, Loopless algorithm, Hypercube",
abstract = "We consider the algorithmic problem of generating each subset of [n]:={1,2,…,n} whose size is in some interval [k,l], 0≤k≤l≤n, exactly once (cyclically) by repeatedly adding or removing a single element, or by exchanging a single element. For k=0 and l=n this is the classical problem of generating all 2n subsets of [n] by element additions/removals, and for k=l this is the classical problem of generating all (nk) subsets of [n] by element exchanges. We prove the existence of such cyclic minimum-change enumerations for a large range of values n, k, and l, improving upon and generalizing several previous results. For all these existential results we provide optimal algorithms to compute the corresponding Gray codes in constant O(1) time per generated set and O(n) space. Rephrased in terms of graph theory, our results establish the existence of (almost) Hamilton cycles in the subgraph of the n-dimensional cube Qn induced by all levels [k,l]. We reduce all remaining open cases to a generalized version of the middle levels conjecture, which asserts that the subgraph of Q2k+1 induced by all levels [k−c,k+1+c], c∈{0,1,…,k}, has a Hamilton cycle. We also prove an approximate version of this generalized conjecture, showing that this graph has a cycle that visits a (1−o(1))-fraction of all vertices."
}
@article{RINALDI2017121,
title = "Changes in interlimb coordination during walking and grasping task in older adult fallers and non-fallers",
journal = "Human Movement Science",
volume = "55",
pages = "121 - 137",
year = "2017",
issn = "0167-9457",
doi = "https://doi.org/10.1016/j.humov.2017.08.002",
url = "http://www.sciencedirect.com/science/article/pii/S0167945717305687",
author = "Natalia Madalena Rinaldi and Richard van Emmerik and Renato Moraes",
keywords = "Coordination, Grasping, Walking, Older adults, Falls",
abstract = "The aim of this study was to investigate interlimb coordination in young and older adults with and without a history of falls during the combined task of walking and prehension with different levels of manual task difficulty. Participants walked on a pathway and grasped a dowel. A vector coding technique evaluated coordination patterns. The coordination pattern was not affected by the difficulty level of the manual task. Older adults seemed to prioritize the movement of the right shoulder to grasp the dowel and then ‘froze’ the movement of the other joint (left shoulder) not directly involved in the grasping task. The preference to pick up the dowel in the double support phase and the increase in right shoulder phase made by older adults with a history of falls suggests an even greater decoupling between walking and prehension."
}
@article{MAZER20181520,
title = "Evaluating Surgical Coaching: A Mixed Methods Approach Reveals More Than Surveys Alone",
journal = "Journal of Surgical Education",
volume = "75",
number = "6",
pages = "1520 - 1525",
year = "2018",
issn = "1931-7204",
doi = "https://doi.org/10.1016/j.jsurg.2018.03.009",
url = "http://www.sciencedirect.com/science/article/pii/S1931720418300382",
author = "Laura M. Mazer and Yue-Yung Hu and Alexander F. Arriaga and Caprice C. Greenberg and Stuart R. Lipsitz and Atul A. Gawande and Douglas S. Smink and Steven J. Yule",
keywords = "surgical coaching, surgical resident education, educational assessment, technical skill training, nontechnical skill training, dialogue analysis, Patient Care, Medical Knowledge, Practice Based Learning and Improvement",
abstract = "Objective
Traditionally, surgical educators have relied upon participant survey data for the evaluation of educational interventions. However, the ability of such subjective data to completely evaluate an intervention is limited. Our objective was to compare resident and attending surgeons’ self-assessments of coaching sessions from surveys with independent observations from analysis of intraoperative and postoperative coaching transcripts.
Design
Senior residents were video-recorded operating. Each was then coached by the operative attending in a 1:1 video review session. Teaching points made in the operating room (OR) and in post-OR coaching sessions were coded by independent observers using dialogue analysis then compared using t-tests. Participants were surveyed regarding the degree of teaching dedicated to specific topics and perceived changes in teaching level, resident comfort, educational assessments, and feedback provision between the OR and the post-OR coaching sessions.
Setting
A single, large, urban, tertiary-care academic institution.
Participants
Ten PGY4 to 5 general surgery residents and 10 attending surgeons.
Results
Although the reported experiences of teaching and coaching sessions by residents and faculty were similar (Pearson correlation coefficient = 0.88), these differed significantly from independent observations. Observers found that residents initiated a greater proportion of teaching points and had more educational needs assessments during coaching, compared to the OR. However, neither residents nor attendings reported a change between the 2 environments with regard to needs assessments nor comfort with asking questions or making suggestions. The only metric on which residents, attendings, and observers agreed was the provision of feedback.
Conclusions
Participants’ perspectives, although considered highly reliable by traditional metrics, rarely aligned with analysis of the associated transcripts from independent observers. Independent observation showed a distinct benefit of coaching in terms of frequency and type of learning points. These findings highlight the importance of seeking different perspectives, data sources, and methodologies when evaluating clinical education interventions. Surgical education can benefit from increased use of dialogue analyses performed by independent observers, which may represent a viewpoint distinct from that obtained by survey methodology."
}
@article{ACOSTA201625,
title = "Extending Paralldroid with object oriented annotations",
journal = "Parallel Computing",
volume = "57",
pages = "25 - 36",
year = "2016",
issn = "0167-8191",
doi = "https://doi.org/10.1016/j.parco.2016.04.003",
url = "http://www.sciencedirect.com/science/article/pii/S0167819116300126",
author = "Alejandro Acosta and Sergio Afonso and Francisco Almeida",
keywords = "Renderscript, Source-to-source transformation, Android",
abstract = "The popularity of the handheld systems ( smartphones, tablets , ...) and their computational capability open new challenges in terms of the efficient use of such devices. The heterogeneity of these SoCs and MPSoCs demands very specific knowledge of the devices, involving a very high learning curve for the programmers. To ease the development task we build Paralldroid, a framework oriented to general purpose programmers for mobile devices. Paralldroid unifies the Android programming models and allows for the automatic generation of parallel code. Sections of code to be optimized in a Java program can be annotated using Paralldroid annotations. Paralldroid automatically generates the native C or Renderscript code required to take advantage of the underlying parallel platform (GPU included). The code generated by Paralldroid offers a good performance with a very low cost of development, contributing to increased productivity when developing efficient code."
}
@article{BILODID201534,
title = "Spectral history model in DYN3D: Verification against coupled Monte-Carlo thermal-hydraulic code BGCore",
journal = "Annals of Nuclear Energy",
volume = "81",
pages = "34 - 40",
year = "2015",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2015.03.030",
url = "http://www.sciencedirect.com/science/article/pii/S0306454915001644",
author = "Y. Bilodid and D. Kotlyar and M. Margulis and E. Fridman and E. Shwageraus",
keywords = "History effects, Spectral history, Coupled Monte Carlo, DYN3D, BGCore",
abstract = "This research focuses on the verification of a recently developed methodology accounting for spectral history effects in 3D full core nodal simulations. The traditional deterministic core simulation procedure includes two stages: (1) generation of homogenized macroscopic cross section sets and (2) application of these sets to obtain a full 3D core solution with nodal codes. The standard approach adopts the branch methodology in which the branches represent all expected combinations of operational conditions as a function of burnup (main branch). The main branch is produced for constant, usually averaged, operating conditions (e.g. coolant density). As a result, the spectral history effects that associated with coolant density variation are not taken into account properly. Number of methods to solve this problem (such as micro-depletion and spectral indexes) were developed and implemented in modern nodal codes. Recently, we proposed a new and robust method to account for history effects. The methodology was implemented in DYN3D and involves modification of the few-group cross section sets. The method utilizes the local Pu-239 concentration as an indicator of spectral history. The method was verified for PWR and VVER applications. However, the spectrum variation in BWR core is more pronounced due to the stronger coolant density change. The purpose of the current work is investigating the applicability of the method to BWR analysis. The proposed methodology was verified against recently developed BGCore system, which couples Monte Carlo neutron transport with depletion and thermal-hydraulic solvers and thus capable of providing a reference solution for 3D simulations. The results clearly show that neglecting the spectral history effects leads to a very large deviation (e.g. 1700pcm in multiplication factor) from the reference solution. Application of the Pu-correction method results in a very good agreement between DYN3D and BGCore on the order of 200pcm in kinf."
}
@article{THOMPSON2018e240,
title = "Does a Written Tool to Guide Structured Debriefing Improve Discourse? Implications for Interprofessional Team Simulation",
journal = "Journal of Surgical Education",
volume = "75",
number = "6",
pages = "e240 - e245",
year = "2018",
issn = "1931-7204",
doi = "https://doi.org/10.1016/j.jsurg.2018.07.001",
url = "http://www.sciencedirect.com/science/article/pii/S1931720418301867",
author = "Ryan Thompson and Sarah Sullivan and Krystle Campbell and Ingie Osman and Brianna Statz and Hee Soo Jung",
keywords = "simulation, debriefing, discourse, interprofessional, Interpersonal and Communication Skills, Practice-Based Learning and Improvement, Systems-Based Practice",
abstract = "PURPOSE
Timely debriefing following a simulated event supports learners in critically reflecting on their performance and areas for improvement. Content of debriefing has been shown to affect learner skill acquisition and retention. The use of good judgment statements from debriefing facilitators is considered superior to judgmental or nonjudgmental statements. Ideally, the majority of the conversation will consist of learner self-reflection and focused facilitation rather than directive performance feedback. We hypothesized that the introduction of a written tool to help facilitate high-quality debriefing techniques could improve the ratio of judgmental, nonjudgmental, and good judgment statements from facilitators, as well as shift the percentage of talk in the debrief away from directive performance feedback and toward self-assessment and focused facilitation.
METHODS
The University of Wisconsin Joint Trauma Simulation Program is an interdisciplinary project to improve quality of trauma care through simulation. Simulations use teams of five trauma trainees: two surgery residents, an emergency medicine resident, and two nurses. Three faculty members conducted the scenarios and debriefings. Debriefings were video recorded. Videos were transcribed and dialogue analyzed according to the teaching/learning strategy used in each turn of talk. Discourse was coded into three categories: (1) learner self-assessment; (2) focused facilitation; and (3) directive performance feedback. Each facilitation statement was coded as either (1) judgmental; (2) nonjudgmental, or (3) good judgment. The TEAM Debrief Tool is a written guide designed to help facilitators adhere to best practices, with example structure and phrasing, similar to the Promoting Excellence and Reflective Learning in Simulation tool. Pre- and post-implementation analysis was completed to assess for efficacy of the tool.
RESULTS
Seven videos before the implementation of the tool and seven videos after implementation were analyzed. The percentage of learner self-assessment increased significantly with tool use (7.23% vs 24.99%, p = 0.00004), and directive performance feedback decreased significantly (56.13% vs 32.75%, p = 0.0042). There was no significant change in the percentage of talk using focused facilitation. After implementation of the tool, there was a significant decrease in use of the nonjudgmental debriefing style (60.63% vs 37.31%, p = 0.00017), and a significant increase in the use of good judgment debriefing (38.77% vs 59.82%, p = 0.00038). There was also a slight increase in judgmental debriefing (0.60% vs 2.87%, p = 0.0027).
CONCLUSIONS
The discourse in our interprofessional trauma simulation debriefings unaided by a written debriefing tool skewed heavily toward direct performance feedback, with a preponderance of nonjudgmental statements. After introduction of the tool, dialogue shifted significantly toward learner self-assessment, and there was a large increase in utilization of debriefing with good judgment. This shift toward higher quality debriefing styles demonstrates the utility of such a tool in the debriefing of interprofessional simulations."
}
@article{DILLON201867,
title = "Using simulation with interprofessional team training to improve RRT/code performance",
journal = "Journal of Interprofessional Education & Practice",
volume = "11",
pages = "67 - 72",
year = "2018",
issn = "2405-4526",
doi = "https://doi.org/10.1016/j.xjep.2018.01.002",
url = "http://www.sciencedirect.com/science/article/pii/S2405452617302082",
author = "Patricia Dillon and Helene Moriarty and Gregg Lipschik",
keywords = "Simulation, Interprofessional collaboration, Quality improvement",
abstract = "Early recognition of and response to changes in patients’ conditions are a National Patient Safety Goal. Rapid Response Teams (RRTs) are one safety strategy aimed at early recognition of signs and symptoms of clinical deterioration and reduction in rates of cardiopulmonary arrest and death in hospitalized patients. Mock codes and RRTs are another strategy for improving outcomes.The Corporal Michael J. Crescenz VA Medical Center(CMCVAMC) used data from the American Heart Association National Registry of Cardio-Pulmonary Resuscitation to create an interprofessional, collaborative program using simulation. The program included: review of emergency responses and hands-on sessions with crash cart equipment, airway management, and BLS skills, followed by a mock RRT and Code with debriefing. Participants in this quality improvement initiative were nurses, physicians, anesthetists, pharmacists, and respiratory therapists. They evaluated the simulation as a positive learning experience. Staff and patient outcomes were improved after the program. The program engaged staff and promoted interprofessional collaboration that may ultimately improve the quality of patient care."
}
@article{ZHAO201879,
title = "Maximal granularity structure and generalized multi-view discriminant analysis for person re-identification",
journal = "Pattern Recognition",
volume = "79",
pages = "79 - 96",
year = "2018",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2018.01.033",
url = "http://www.sciencedirect.com/science/article/pii/S003132031830044X",
author = "Cairong Zhao and Xuekuan Wang and Duoqian Miao and Hanli Wang and Weishi Zheng and Yong Xu and David Zhang",
keywords = "Person re-identification, Maximal granularity structure descriptor, Generalized multi-view discriminant analysis, Representation consistency",
abstract = "This paper proposes a novel descriptor called Maximal Granularity Structure Descriptor (MGSD) for feature representation and an effective metric learning method called Generalized Multi-view Discriminant Analysis based on representation consistency (GMDA-RC) for person re-identification (Re-ID). The proposed descriptor of MGSD captures rich local structural information from overlapping macro-pixels in an image, analyzes the horizontal occurrence of multi-granularity and maximizes the occurrence to extract a robust representation for viewpoint changes. As a result, the proposed descriptor of MGSD can obtain rich person appearance whilst being robust against different condition changes. Besides, considering multi-view information, we present a new GMDA-RC for different views, inspired by the observation that different views share similar data structures. The proposed metric learning method of GMDA-RC seeks multiple discriminant common spaces for multiple views by jointly learning multiple view-specific linear transforms. Finally, we evaluate the proposed method of (MGSD+GMDA-RC) on three publicly available person Re-ID datasets: VIPeR, CUHK-01 and Wide Area Re-ID dataset (WARD). For the VIPeR and CUHK-01, the experimental results show that our method significantly outperforms the state-of-the-art methods, achieving the rank-1 matching rates of 67.09%, 70.61%, and the improvements of 17.41%, 5.34%, respectively. For the WARD, we consider different pairwise camera views (camera 1–2, camera 1–3, camera 2–3) and our method can achieve the rank-1 matching rates of 64.33%, 59.42%, 70.32%, increasing of 5.68%, 11.04%, 9.06% compared with the state-of-the-art methods, respectively."
}
@article{WALTERCOSTA2018143,
title = "Temporal ordering of substitutions in RNA evolution: Uncovering the structural evolution of the Human Accelerated Region 1",
journal = "Journal of Theoretical Biology",
volume = "438",
pages = "143 - 150",
year = "2018",
issn = "0022-5193",
doi = "https://doi.org/10.1016/j.jtbi.2017.11.015",
url = "http://www.sciencedirect.com/science/article/pii/S0022519317305222",
author = "Maria Beatriz Walter Costa and Christian Höner zu Siederdissen and Dan Tulpan and Peter F. Stadler and Katja Nowick",
keywords = "Human evolution, Computational modeling, Dynamic programming, Non-coding RNA, Secondary structure, Data visualisation",
abstract = "The Human Accelerated Region 1 (HAR1) is the most rapidly evolving region in the human genome. It is part of two overlapping long non-coding RNAs, has a length of only 118 nucleotides and features 18 human specific changes compared to an ancestral sequence that is extremely well conserved across non-human primates. The human HAR1 forms a stable secondary structure that is strikingly different from the one in chimpanzee as well as other closely related species, again emphasizing its human-specific evolutionary history. This suggests that positive selection has acted to stabilize human-specific features in the ensemble of HAR1 secondary structures. To investigate the evolutionary history of the human HAR1 structure, we developed a computational model that evaluates the relative likelihood of evolutionary trajectories as a probabilistic version of a Hamiltonian path problem. The model predicts that the most likely last step in turning the ancestral primate HAR1 into the human HAR1 was exactly the substitution that distinguishes the modern human HAR1 sequence from that of Denisovan, an archaic human, providing independent support for our model. The MutationOrder software is available for download and can be applied to other instances of RNA structure evolution."
}
@article{GLYNN2017108,
title = "Dedicated Educational Nursing Unit: Clinical Instructors Role Perceptions and Learning Needs",
journal = "Journal of Professional Nursing",
volume = "33",
number = "2",
pages = "108 - 112",
year = "2017",
issn = "8755-7223",
doi = "https://doi.org/10.1016/j.profnurs.2016.08.005",
url = "http://www.sciencedirect.com/science/article/pii/S875572231630120X",
author = "Donna M. Glynn and Cecilia McVey and Judith Wendt and Bonnie Russell",
keywords = "Dedicated education unit, Clinical instructors, Academic practice partnerships, Perceived learning needs",
abstract = "Over the past decade, health care leaders have called for a radical transformation in health care and nursing education. Patient care has become complex, demanding succinct interprofessional communication and collaboration to optimize the care of the patient, and the nurse at the bedside is the optimal leader at the point of care. Assistance with the clinical reasoning and critical thinking with nursing students is pivotal for successful patient outcomes. The expert clinical nurse at the bedside is the premier faculty to guide the young practitioner in the care of the patient. A dedicated educational unit (DEU) is an example of an academic–practice partnership designed to provide students with a positive clinical learning environment. The purpose of this qualitative research study was to identify the role perceptions of staff nurse's participating as clinical instructors on a DEU and the perceived educational learning needs of the experienced staff nurses. After Veterans Affairs Boston Healthcare System Institutional Review Board approval, a total of 8 nurses serving in the role of clinical instructor on a DEU participated in the study. Content analyses were used to code and synthesize common theses from the interviews. The themes that emerged related to role perception were mentoring, ensuring competency with basic skills and tasks, and development of critical thinking in nursing clinical education. The themes related to perceived learning needs of staff nurses related to the role of clinical instructor were the need for clear objectives from the academic affiliate, more coordination and acknowledgement by the academic affiliate, and addition education in dealing with students with diverse learning needs and accommodations."
}
@article{SHCHUROVSKAYA2018436,
title = "Validation of the MCU-PTR computational model of beryllium poisoning using selected experiments at the IRT-T research reactor",
journal = "Annals of Nuclear Energy",
volume = "113",
pages = "436 - 445",
year = "2018",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2017.11.046",
url = "http://www.sciencedirect.com/science/article/pii/S030645491730436X",
author = "M.V. Shchurovskaya and V.P. Alferov and N.I. Geraskin and A.I. Radaev and A.G. Naymushin and Yu.B. Chertkov and M.N. Anikin and I.I. Lebedev",
keywords = "Research reactor, Beryllium reflector, Reactivity, He, Monte Carlo code, Validation of neutronics codes",
abstract = "This paper presents the results of the validation of the MCU-PTR code computational model of beryllium poisoning caused by 3He and 6Li buildup against selected experiments at the IRT-T research reactor in the National Research Tomsk Polytechnic University. Calculations including the reflector irradiation history modeling for the entire reactor lifetime were performed using the continuous energy Monte Carlo code MCU–PTR. The measured reactivity loss (∼3.2%Δk/k) after the reactor shutdown period of 672 days and the reactivity increase (0.9%Δk/k) after replacement of the part of the reflector beryllium blocks by fresh blocks were compared with the calculated results. The impact of the parameters of beryllium poisoning model on the calculated results was investigated. The calculation underestimates the reactivity loss caused by 3He buildup during the 672-day shutdown by approximately 0.5%Δk/k."
}
@article{SALMINEN201775,
title = "Mitochondrial genotype modulates mtDNA copy number and organismal phenotype in Drosophila",
journal = "Mitochondrion",
volume = "34",
pages = "75 - 83",
year = "2017",
issn = "1567-7249",
doi = "https://doi.org/10.1016/j.mito.2017.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S1567724916300757",
author = "Tiina S. Salminen and Marcos T. Oliveira and Giuseppe Cannino and Päivi Lillsunde and Howard T. Jacobs and Laurie S. Kaguni",
keywords = "Mitochondrial DNA, Haplotype, Cybrid, Respiration, Supercomplexes",
abstract = "We evaluated the role of natural mitochondrial DNA (mtDNA) variation on mtDNA copy number, biochemical features and life history traits in Drosophila cybrid strains. We demonstrate the effects of both coding region and non-coding A+T region variation on mtDNA copy number, and demonstrate that copy number correlates with mitochondrial biochemistry and metabolically important traits such as development time. For example, high mtDNA copy number correlates with longer development times. Our findings support the hypothesis that mtDNA copy number is modulated by mtDNA genome variation and suggest that it affects OXPHOS efficiency through changes in the organization of the respiratory membrane complexes to influence organismal phenotype."
}
@article{VENTSISLAVOVA2018246,
title = "The hazard prediction test: A comparison of free-response and multiple-choice formats",
journal = "Safety Science",
volume = "109",
pages = "246 - 255",
year = "2018",
issn = "0925-7535",
doi = "https://doi.org/10.1016/j.ssci.2018.06.004",
url = "http://www.sciencedirect.com/science/article/pii/S0925753518301188",
author = "Petya Ventsislavova and David Crundall",
keywords = "Hazard perception, Hazard prediction, Multiple-choice format, Driving",
abstract = "Hazard perception skill is often related to lower crash risk, and the hazard perception test has been widely employed to measure this ability in drivers. An increasingly popular test-variant is the hazard prediction test: driving videos are occluded immediately prior to a hazard and participants are asked to predict how the situation will develop. Early versions of this test asked participants to provide a free-response answer which was subsequently coded. Later versions, however, have used a multiple-choice format where participants are provided with four options presented on screen. While the benefits of a multiple-choice format are obvious in terms of providing immediate feedback without relying on subjective coding, it is unclear whether this change in format affects the discriminative validity of the test. For the current study, a free-response test and a multiple-choice test were created using the same video clips. The free-response test (experiment 1) was found to successfully discriminate between novice and experienced drivers, with the latter predicting more hazards correctly. The answers provided by participants in Experiment 1 were then used to generate the options for a multiple-choice test (experiment 2). This second test was also found to discriminate between novice and experienced drivers, and a comparison between the two tests failed to reveal an advantage for one over the other. Despite this, correlations between prediction accuracy and both years of post-license driving, and annual mileage, were only significant for the multiple-choice test. The results suggest that the multiple-choice format is not only time- and cost-efficient, but is ostensibly as good as the free-response test in discriminating between driver groups."
}
@article{DROBNY2017278,
title = "F-TRIDYN: A Binary Collision Approximation code for simulating ion interactions with rough surfaces",
journal = "Journal of Nuclear Materials",
volume = "494",
pages = "278 - 283",
year = "2017",
issn = "0022-3115",
doi = "https://doi.org/10.1016/j.jnucmat.2017.07.037",
url = "http://www.sciencedirect.com/science/article/pii/S002231151730301X",
author = "Jon Drobny and Alyssa Hayes and Davide Curreli and David N. Ruzic",
keywords = "Binary Collision Approximation, Fractal, Surface roughness, Ion-solid interactions, Plasma material interactions",
abstract = "Fractal TRIDYN (F-TRIDYN) is a modified version of the widely used Monte Carlo, Binary Collision Approximation code TRIDYN that includes an explicit model of surface roughness and additional output modes for coupling to plasma edge and material codes. Surface roughness plays an important role in ion irradiation processes such as sputtering; roughness can significantly increase the angle of maximum sputtering and change the maximum observed sputtering yield by a factor of 2 or more. The complete effect of surface roughness on sputtering and other ion irradiation phenomena is not completely understood. Many rough surfaces can be consistently and realistically modeled by fractals, using the fractal dimension and fractal length scale as the sole input parameters. F-TRIDYN includes a robust fractal surface algorithm that is more computationally efficient than those in previous fractal codes and which reproduces available experimental sputtering data from rough surfaces. Fractals provide a compelling path toward a complete and concise understanding of the effect that surface geometry plays on the behavior of plasma-facing materials. F-TRIDYN is a flexible code for simulating ion-solid interactions and coupling to plasma and material codes for multiscale modeling."
}
@article{RYSSENS2015175,
title = "Solution of the Skyrme–HF+BCS equation on a 3D mesh, II: A new version of the Ev8 code",
journal = "Computer Physics Communications",
volume = "187",
pages = "175 - 194",
year = "2015",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2014.10.001",
url = "http://www.sciencedirect.com/science/article/pii/S0010465514003361",
author = "W. Ryssens and V. Hellemans and M. Bender and P.-H. Heenen",
keywords = "Self-consistent mean field, Hartree–Fock, Hartree–Fock+BCS, Skyrme interaction, Quadrupole deformation",
abstract = "We describe a new version of the Ev8 code that solves the nuclear Skyrme–Hartree–Fock+BCS problem using a 3-dimensional cartesian mesh. Several new features have been implemented with respect to the earlier version published in 2005. In particular, the numerical accuracy has been improved for a given mesh size by (i) implementing a new solver to determine the Coulomb potential for protons, and (ii) implementing a more precise method to calculate the derivatives on a mesh that had already been implemented earlier in our beyond-mean-field codes. The code has been made very flexible to enable the use of a large variety of Skyrme energy density functionals that have been introduced in the last years. Finally, the treatment of the constraints that can be introduced in the mean-field equations has been improved. The code Ev8 is today the tool of choice to study the variation of the energy of a nucleus from its ground state to very elongated or triaxial deformations with a well-controlled accuracy.
Program summary
Program title: Ev8 Catalogue identifier: ADWA_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ADWA_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 29956 No. of bytes in distributed program, including test data, etc.: 235072 Distribution format: tar.gz Programming language: FORTRAN-90. Computer: AMD Opteron 6274, AMD Opteron 6134, AMD Opteron 2378, Intel Core i7-4700HQ. Operating system: Unix, Linux, OS X. RAM: On the order of 64 megabytes for the examples provided. Classification: 17.22. Catalogue identifier of previous version: ADWA_v1_0 Journal reference of previous version: Comput. Phys. Comm. 171(2005)49 Does the new version supersede the previous version?: Yes, but when used in the same conditions both codes give the same result. Nature of problem: By means of the Hartree–Fock+BCS method for Skyrme-type energy density functionals, Ev8 allows the study of the evolution of the binding energy of even–even atomic nuclei for various shapes determined by the most general quadrupole and monopole constraints. Solution method: The program expands the single-particle wave-functions on a 3D Cartesian mesh. The nonlinear mean-field equations are solved by the imaginary time step method. A quadratic constraint is used to obtain states corresponding to given values of the monopole and quadrupole operators. Reasons for new version: The code has been generalized in several directions. The main changes concern the energy density functional that is more general than previously (including now tensor terms) and the accuracy of the final result that has been significantly improved by a new algorithm to determine the Coulomb potential. Several other changes should make the code more user friendly than it was before. Summary of revisions: 1. Skyrme energy functionals with tensor terms; 2. Improved accuracy for calculating derivatives; 3. Improved accuracy for solving Coulomb problem; 4. Improvement of the numerics of constraints; Restrictions: Ev8 assumes time-reversal invariance and nuclear shapes exhibiting three plane-reflection symmetries. Pairing correlations are treated at the BCS level of approximation. Running time: A few minutes for the examples provided, which concern rather heavy nuclei in modest boxes with an initial guess of Nilsson wavefunctions."
}
@article{PETITPAS201821451,
title = "Simulation of boil-off losses during transfer at a LH2 based hydrogen refueling station",
journal = "International Journal of Hydrogen Energy",
volume = "43",
number = "46",
pages = "21451 - 21463",
year = "2018",
issn = "0360-3199",
doi = "https://doi.org/10.1016/j.ijhydene.2018.09.132",
url = "http://www.sciencedirect.com/science/article/pii/S0360319918330234",
author = "Guillaume Petitpas",
abstract = "Losses along the LH2 pathway are intrinsic to the utilization of a cryogenic fluid. They occur when the fluid is transferred between 2 vessels (liquefaction plant to trailer, trailer to station storage, station storage to pump or compressor, then possibly onto fuel cell electric vehicles …) and when it is warmed up due to heat transfer with the environment. Those losses can be estimated with good accuracy using thermodynamic models based on the conservation of mass and energy, provided that the thermodynamic states are correctly described. Indeed, the fluid undergoes various changes as it moves along the entire pathway (2 phase transition, sub-cooled liquid phase, super-heated warming, non-uniform temperature distributions across the saturation film) and accurate equations of state and 2 phase behavior implementations are essential. The balances of mass and energy during the various dynamics processes then enable to quantify the boil-off losses. In this work, a MATLAB code previously developed by NASA to simulate rocket loading is used as the basis for a LH2 transfer model. This code implements complex physical phenomena such as the competition between condensation and evaporation and the convection vs. conduction heat transfer as a function of the relative temperatures on both sides of the saturated film. The original code was modified to consider real gas equations of state, and some semi-empirical relationships, such as between the heat of vaporization and the critical temperature, were replaced by a REFPROP equivalent expression, assumed to be more accurate. Non-constant liquid temperature equations were added to simulate sub-cooled conditions. The model shows that under environmental heat transfer only the liquid phase of a LH2 vessel would experience cooling, while the boil-off is mainly a result of evaporation from the saturation film onto the vapor phase. Under the conditions assumed for this work, it was also concluded that the actual LH2 density was lower than the corresponding saturation density given by the working pressure of the vessel. During a bottom fill transfer, for example from a LH2 trailer to an on-site stationary vessel, it is shown that the boil-off losses are due to the compression of the vapor phase (“pdV” force). The model indicates that the magnitude of those losses is not dependent on the regulated pressure in the receiving vessel but is rather a function of the initial pressure in the vessel, amounting to more than 12% of losses for a vessel initially at 100 psia. At last, the model is used to estimate the amount of vapor H2 vented when depressurizing a LH2 trailer following a LH2 delivery."
}
@article{SCHNITER201894,
title = "Information transmission and the oral tradition: Evidence of a late-life service niche for Tsimane Amerindians",
journal = "Evolution and Human Behavior",
volume = "39",
number = "1",
pages = "94 - 105",
year = "2018",
issn = "1090-5138",
doi = "https://doi.org/10.1016/j.evolhumbehav.2017.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S1090513817301411",
author = "Eric Schniter and Nathaniel T. Wilcox and Bret A. Beheim and Hillard S. Kaplan and Michael Gurven",
keywords = "Oral tradition, Information transmission, Storytelling, Expertise, Development, Life history theory",
abstract = "Storytelling can affect wellbeing and fitness by transmitting information and reinforcing cultural codes of conduct. Despite their potential importance, the development and timing of storytelling skills, and the transmission of story knowledge have received minimal attention in studies of subsistence societies that more often focus on food production skills. Here we examine how storytelling and patterns of information transmission among Tsimane forager-horticulturalists are predicted by the changing age profiles of storytellers’ abilities and accumulated experience. We find that storytelling skills are most developed among older adults who demonstrate superior knowledge of traditional stories and who report telling stories most. We find that the important information transmitted via storytelling typically flows from older to younger generations, and stories are primarily learned from older same-sex relatives, especially grandparents. Our findings suggest that the oral tradition provides a specialized late-life service niche for Tsimane adults who have accumulated important experience and knowledge relevant to foraging and sociality, but have lost comparative advantage in other productive domains. These findings may help extend our understanding of the evolved human life history by illustrating how changes in embodied capital predict the development of information transmission services in a forager-horticulturalist economy."
}
@article{HENDRAWAN2015597,
title = "Visualizing Time-based Weighted Coupling Using Particle Swarm Optimization to Aid Program Comprehension",
journal = "Procedia Computer Science",
volume = "72",
pages = "597 - 604",
year = "2015",
note = "The Third Information Systems International Conference 2015",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.12.168",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915036297",
author = "Rully Agus Hendrawan and Katsuhisa Maruyama",
keywords = "source code visualization, program comprehension, logical coupling, particle swarm optimization, association mining",
abstract = "By knowing software coupling, developers can get better view of the software quality and improve their productivity in development and maintenance. This paper presents a method to visualize coupling network that are often very complex, using heuristic approach based on particle swarming optimization. Each node is placed randomly and assigned with initial speed. Node that are coupled together will be attracted each other and trying to get closer until they reach a particular distance. This distance is determined from the coupling value of two nodes. A closely related nodes will move closer until reaching a short distance. On each iteration, node position is dynamically updated based on attraction and repulsive force around them. Thus, gradually forming a near best solution of logical coupling graph. The coupling values are measured by mining the association rule from changes history. A software development project sometimes can be very active, updates happen within minutes. Sometimes it becomes slow with weekly or even monthly updates. Time-based weighted analysis method was used to accommodate these time sensitive situations. A co-change in a short duration will be weighted more than co-changes that happen in longer duration."
}
@article{HARMON2017218,
title = "Food Insecurity Experience: Building Empathy in Future Food and Nutrition Professionals",
journal = "Journal of Nutrition Education and Behavior",
volume = "49",
number = "3",
pages = "218 - 227.e1",
year = "2017",
issn = "1499-4046",
doi = "https://doi.org/10.1016/j.jneb.2016.10.023",
url = "http://www.sciencedirect.com/science/article/pii/S1499404616308818",
author = "Alison Harmon and Kara Landolfi and Carmen Byker Shanks and Leanna Hansen and Laura Iverson and Melody Anacker",
keywords = "community nutrition students, food insecurity, dietetics education, food assistance, transformational learning theory",
abstract = "Objective
To assess changes in empathy in students completing a food insecurity experience.
Design
Mixed methods; quantitative data from survey in years 1 and 2; qualitative data extracted from students' workbooks in years 2–5. This study was conducted over 10 weeks annually for 5 years.
Setting
Northwest US land-grant university.
Participants
Students enrolled in a community nutrition course who chose to complete the food insecurity exercise. Total included 58 students in quantitative analysis in years 1 and 2 and 119 in qualitative analysis, years 2–5.
Intervention(s)
The intervention was a food insecurity experience in which participants spent no more than $3/d on food for 5 days ($15 total) while striving for a nutritious diet and reflecting on their experience.
Main Outcome Measures
Empathy scores measured by Likert scales; participant responses and reflections recorded in workbook journals.
Analysis
Comparison of means across time using paired t tests (P < .05); coding and sorting themes from workbook journals.
Results
Quantitative findings indicated that both classroom content and experiential exercises were important for enhancing empathy about food insecurity. Empathy scores increased from time I to time II and from time I to time III. Qualitative reflections among participants included terms such as guilt, empathy, compassion, and raised consciousness about food insecurity.
Conclusions and Implications
Experiential and transformational learning to develop empathy can take place in a 5-day food insecurity experience during a typical university-level community nutrition course. This intervention can be tested for applications in other contexts."
}
@article{VDOVIN2018257,
title = "A coupled approach for vehicle brake cooling performance simulations",
journal = "International Journal of Thermal Sciences",
volume = "132",
pages = "257 - 266",
year = "2018",
issn = "1290-0729",
doi = "https://doi.org/10.1016/j.ijthermalsci.2018.05.016",
url = "http://www.sciencedirect.com/science/article/pii/S1290072917316149",
author = "Alexey Vdovin and Mats Gustafsson and Simone Sebben",
keywords = "Brake cooling, Alpine descent, CFD, Brake thermal management",
abstract = "Advances of CFD methods together with the constant growth of computer capacity enables simulations of complex coupled fluid and thermal problems. One such problem is the evaluation of brake cooling performance. The brake system is a critical component for passenger vehicles and ensuring correct brake operation under all possible load scenarios is a safety issue. An accurate prediction of convection, conduction and radiation heat fluxes for such a complicated system is challenging from modelling as well as numerical efficiency perspectives. This study describes a simulation procedure developed to numerically predict brake system component temperatures during a downhill brake performance test. Such tests have stages of forced and natural convection, and therefore, the airflow is influenced by the temperature changes within the system. For the numerical simulation, a coupled approach is utilized by combining aerodynamic and thermal codes. The aerodynamic code computes the convective heat transfer using a fully-detailed vehicle model in the virtual wind tunnel. The thermal code then uses this data and combines it with conduction and radiation calculations to give an accurate prediction of the component temperatures, which are subsequently used for airflow recalculation. The procedure is described in considerable detail for most parts of the setup. The calculated temperature history results are validated against experimental data and show good agreement. The method allows detailed investigations of distribution and direction of the heat fluxes inside the system, and of how these fluxes are affected by changes in material properties as well as changes in parts within or outside the brake system. For instance, it is shown that convection and especially convection from the inner vanes is the main contributor for the heat dissipation from the brake disc. Finally, some examples of how changing the vehicle design affects the brake cooling performance are also discussed."
}
@article{TETTEY2017846,
title = "Energy use implications of different design strategies for multi-storey residential buildings under future climates",
journal = "Energy",
volume = "138",
pages = "846 - 860",
year = "2017",
issn = "0360-5442",
doi = "https://doi.org/10.1016/j.energy.2017.07.123",
url = "http://www.sciencedirect.com/science/article/pii/S0360544217313063",
author = "Uniben Yao Ayikoe Tettey and Ambrose Dodoo and Leif Gustavsson",
keywords = "Climate change, Representative concentration pathways (RCPs), Design strategies and overheating control measures, Space heating and cooling, Primary energy, Residential building",
abstract = "The effects of climate change on the final and primary energy use of versions of a multi-storey residential building have been analysed. The building versions are designed to the Swedish building code (BBR 2015) and passive house criteria (Passive 2012) with different design and overheating control strategies under different climate scenarios. Future climate datasets are based on Representative Concentration Pathway scenarios for 2050–2059 and 2090–2099. The analysis showed that strategies giving the lowest space heating and cooling demands for the Passive 2012 building version remained the same under all climate scenarios. In contrast, strategies giving the lowest space heating and cooling demands for the BBR 2015 version varied, as cooling demand became more significant under future climate scenarios. Cooling demand was more dominant than heating for the Passive 2012 building version under future climate scenarios. Household equipment and technical installations based on best available technology gave the biggest reduction in total primary energy use among considered strategies. Overall, annual total operation primary energy decreased by 37–54% for the building versions when all strategies are implemented under the considered climate scenarios. This study shows that appropriate design strategies could result in significant primary energy savings for low-energy buildings under changing climates."
}
@article{TKACHENKO201646,
title = "Centrality bias measure for high density QR code module recognition",
journal = "Signal Processing: Image Communication",
volume = "41",
pages = "46 - 60",
year = "2016",
issn = "0923-5965",
doi = "https://doi.org/10.1016/j.image.2015.11.007",
url = "http://www.sciencedirect.com/science/article/pii/S0923596515002040",
author = "Iuliia Tkachenko and William Puech and Olivier Strauss and Jean-Marc Gaudin and Christophe Destruel and Christian Guichard",
keywords = "High density QR code, Module recognition, Module centrality bias, Weighted mean squared error",
abstract = "High density bar codes are very popular today due to large storage capacity and small code area. But unfortunately, high density versions of QR codes are not being used due to reading problems with most smartphones and flatbed scanner QR code applications. Due to frequent changes and small sizes of the black and white modules, there are reading problems in the QR code binarization and tilt correction processes. The binarization method sets the global or local threshold, and binarizes each pixel separately, that is why they are sensitive to print-and-scan distortion and luminosity. In this paper, we focus on the recognition of high density QR codes. We propose to use the centrality bias of each module to improve the module recognition results. This measure has been used for proposed classification methods and for standard QR code recognition methods. In both cases, the recognition rate was improved, as confirmed by the experimental results."
}
@article{SIGMUND2018110,
title = "Electronic stopping in oxides beyond Bragg additivity",
journal = "Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms",
volume = "415",
pages = "110 - 116",
year = "2018",
issn = "0168-583X",
doi = "https://doi.org/10.1016/j.nimb.2017.11.023",
url = "http://www.sciencedirect.com/science/article/pii/S0168583X17309862",
author = "P. Sigmund and A. Schinner",
keywords = "Stopping power, Compounds, Bragg additivity, PASS code, Light ions, Heavy ions",
abstract = "We present stopping cross sections calculated by our PASS code for several ions in metal oxides and SiO2 over a wide energy range. Input takes into account changes in the valence structure by assigning two additional electrons to the 2p shell of oxygen and removing the appropriate number of electrons from the outer shells of the metal atom. Results are compared with tabulated experimental values and with two versions of Bragg’s additivity rule. Calculated stopping cross sections are applied in testing a recently-proposed scaling rule, which relates the stopping cross section to the number of oxygen atoms per molecule."
}
@article{PALMER20131437,
title = "Mapping Abbreviated Injury Scale data from 1990 to 1998 versions: A stepping-stone in the contemporary evaluation of trauma",
journal = "Injury",
volume = "44",
number = "11",
pages = "1437 - 1442",
year = "2013",
issn = "0020-1383",
doi = "https://doi.org/10.1016/j.injury.2012.08.033",
url = "http://www.sciencedirect.com/science/article/pii/S0020138312003439",
author = "Cameron S. Palmer and Jacelle Lang and Glen Russell and Natalie Dallow and Kathy Harvey and Belinda Gabbe and Peter Cameron",
keywords = "Abbreviated Injury Scale, Injury Severity Score, Injury scoring, Anatomic injury, Trauma registry, Kappa, Data mapping",
abstract = "Introduction
Many trauma registries have used the 1990 revision of the Abbreviated Injury Scale (AIS; AIS90) to code injuries sustained by trauma patients. Due to changes made to the AIS codeset since its release, AIS90-coded data lacks currency in the assessment of injury severity. The ability to map between the 1998 revision of AIS (AIS98) and the current (2008) AIS version (AIS08) already exists. The development of a map for transforming AIS90-coded data into AIS98 would therefore enable contemporary injury severity estimates to be derived from AIS90-coded data.
Methods
Differences between the AIS90 and AIS98 codesets were identified, and AIS98 maps were generated for AIS90 codes which changed or were not present in AIS98. The effectiveness of this map in describing the severity of trauma using AIS90 and AIS98 was evaluated using a large state registry dataset, which coded injury data using AIS90 over several years. Changes in Injury Severity Scores (ISS) calculated using AIS90 and mapped AIS98 codesets were assessed using three distinct methods.
Results
Forty-nine codes (out of 1312) from the AIS90 codeset changed or were not present in AIS98. Twenty-four codes required the assignment of maps to AIS98 equivalents. AIS90-coded data from 78,075 trauma cases were used to evaluate the map. Agreement in calculated ISS between coded AIS90 data and mapped AIS98 data was very high (kappa=0.971). The ISS changed in 1902 cases (2.4%), and the mean difference in ISS across all cases was 0.006 points. The number of cases classified as major trauma using AIS98 decreased by 0.8% compared with AIS90. A total of 3102 cases (4.0%) sustained at least one AIS90 injury which required mapping to AIS98.
Conclusions
This study identified the differences between the AIS90 and AIS98 codesets, and generated maps for the conversion process. In practice, the differences between AIS90- and AIS98-coded data were very small. As a result, AIS90-coded data can be mapped to the current AIS version (AIS08) via AIS98, with little apparent impact on the functional accuracy of the mapped dataset produced."
}
@article{GYORGY2017238,
title = "Investigation on the potential use of thorium as fuel for the Sodium-cooled Fast Reactor",
journal = "Annals of Nuclear Energy",
volume = "103",
pages = "238 - 250",
year = "2017",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2017.01.030",
url = "http://www.sciencedirect.com/science/article/pii/S0306454916309690",
author = "H. György and Sz. Czifrus",
keywords = "Burnup, Gen IV, SFR, Thorium",
abstract = "Generation IV reactors are planned to be an evolutionary step in the history of nuclear power plants. Although they have many advantageous properties, five of the six concepts are designed for the utilization of uranium–plutonium fuel. In this paper, the Sodium-cooled Fast Reactor is investigated regarding the potential application of thorium. The basis of the investigation is the European concept of SFR, which is studied with thorium-containing fuel compositions. Two different approaches are presented with the results of several full-core burnup calculations performed by the Monte Carlo code Serpent 2. The presented results are the multiplication factor changes, delayed neutron fractions, fuel temperature and void coefficients and fissile isotope vectors as well. The results can help to determine how 233U could be produced in this type of reactors and how it could be used as alternative fuel in SFR."
}
@article{TREVORS2017116,
title = "Exploring the relations between epistemic beliefs, emotions, and learning from texts",
journal = "Contemporary Educational Psychology",
volume = "48",
pages = "116 - 132",
year = "2017",
issn = "0361-476X",
doi = "https://doi.org/10.1016/j.cedpsych.2016.10.001",
url = "http://www.sciencedirect.com/science/article/pii/S0361476X16300455",
author = "Gregory J. Trevors and Krista R. Muis and Reinhard Pekrun and Gale M. Sinatra and Marloes M.L. Muijselaar",
keywords = "Multiple conflicting documents, Epistemic beliefs, Emotions, Controversial knowledge, Text-mining",
abstract = "Conflicting claims about important socio-scientific debates are proliferating in contemporary society. It is therefore important to understand the individual characteristics that predict learning from conflicting claims. We explored individuals’ beliefs about the nature of knowledge and knowing (i.e., epistemic beliefs) and their emotions as potentially interrelated sets of learner characteristics that predict learning in such contexts. Undergraduate university students (N=282) self-reported their topic-specific epistemic beliefs and were given three conflicting texts about climate change to read. Immediately after each of the three texts, participants self-reported the emotions they experienced. Following reading and self-report, participants wrote summaries of the conflicting texts. Text-mining and human coding were applied to summaries to construct two indices of learning from conflicting texts that reflected which source’s information is privileged in memory. Results from structural equation modeling revealed that epistemic beliefs were consistent in their predictions of emotions, which in turn variously predicted different learning outcomes. In particular, a belief that knowledge is justified by inquiry predicted surprise and curiosity, which at times facilitated learning. In contrast, confusion, predicted by passive reliance on external sources, related to impaired memory of conflicting content. Theoretical and methodological implications are discussed for research on the relations between epistemic beliefs, emotions, and learning about controversial topics."
}
@article{LAN2013149,
title = "Adaptively post-encoding multiple description video coding",
journal = "Neurocomputing",
volume = "101",
pages = "149 - 160",
year = "2013",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2012.08.015",
url = "http://www.sciencedirect.com/science/article/pii/S0925231212006546",
author = "Xuguang Lan and Meng Yang and Yuan Yuan and Songlin Zhao and Nanning Zheng",
keywords = "Multiple description coding, Scalable wavelet coding, Lagrangian rate allocation, Post-encoding",
abstract = "In video coding and transmission, the network conditions are crucial, i.e., bandwidth, delay and other factors. However, how to effectively take these factors into account is still a challenge, as heterogeneous networks are dynamic, and therefore it is difficult to predict their changes. This paper first reports a post-encoding of Scalable Multiple Description Coding (SMDC), which is towards self-adaptive video delivery under different conditions of the network. The proposed scheme contains three major steps: (1) spatiotemporal wavelet transformation of input video sequence; (2) context-based adaptive binary arithmetic coding; and (3) rate allocation under the conditions of the network and analysis of the principle relationship of the rate–distortion slope ratio with packet-loss probability in network links. The performance of the SMDC is compared with that of scalable video coding and scalable H.264, and SMDC is demonstrated to effectively optimize the video delivery to adapt to the dynamics of heterogeneous networks. Part of this work has been published on Data Compression Conference 2009 as a short abstract version [37]."
}
@article{LU2016749,
title = "Face recognition algorithm based on discriminative dictionary learning and sparse representation",
journal = "Neurocomputing",
volume = "174",
pages = "749 - 755",
year = "2016",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2015.09.091",
url = "http://www.sciencedirect.com/science/article/pii/S0925231215014241",
author = "Zhenyu Lu and  Linghua Zhang",
keywords = "Face recognition, Regularized sparse representation, Uniform local binary pattern, Gabor filtering, Dictionary learning",
abstract = "In order to overcome the defect that the face recognition (FR) rate is greatly reduced in the existing uncontrolled environments such as the change of illumination, occlusion, and posture, etc, Face recognition algorithm based on discriminative dictionary learning and regularized robust coding was proposed. In this proposed algorithm, the Gabor amplitude images of a face image are obtained via using Gabor filter at first, then we extract the uniform local binary histogram and use Fisher criterion to gain a new dictionary, finally the test image is classified as the existing class via sparse representation Coding. The experimental results obtained from Extended Yale B databases and AR databases show that the proposed algorithm has higher face recognition rate in the existing uncontrolled environments in comparison with K-SVD, LC-K-SVD, FDDL and so on."
}
@article{GDALYAHU2012121,
title = "Associative Fear Learning Enhances Sparse Network Coding in Primary Sensory Cortex",
journal = "Neuron",
volume = "75",
number = "1",
pages = "121 - 132",
year = "2012",
issn = "0896-6273",
doi = "https://doi.org/10.1016/j.neuron.2012.04.035",
url = "http://www.sciencedirect.com/science/article/pii/S0896627312004394",
author = "Amos Gdalyahu and Elaine Tring and Pierre-Olivier Polack and Robin Gruver and Peyman Golshani and Michael S. Fanselow and Alcino J. Silva and Joshua T. Trachtenberg",
abstract = "Summary
Several models of associative learning predict that stimulus processing changes during association formation. How associative learning reconfigures neural circuits in primary sensory cortex to “learn” associative attributes of a stimulus remains unknown. Using 2-photon in vivo calcium imaging to measure responses of networks of neurons in primary somatosensory cortex, we discovered that associative fear learning, in which whisker stimulation is paired with foot shock, enhances sparse population coding and robustness of the conditional stimulus, yet decreases total network activity. Fewer cortical neurons responded to stimulation of the trained whisker than in controls, yet their response strength was enhanced. These responses were not observed in mice exposed to a nonassociative learning procedure. Our results define how the cortical representation of a sensory stimulus is shaped by associative fear learning. These changes are proposed to enhance efficient sensory processing after associative learning."
}
@article{GOLDENBERG201749,
title = "Shifting attitudes towards research and evidence-based medicine within the naturopathic medical community: The power of people, money and acceptance",
journal = "Advances in Integrative Medicine",
volume = "4",
number = "2",
pages = "49 - 55",
year = "2017",
issn = "2212-9588",
doi = "https://doi.org/10.1016/j.aimed.2017.08.003",
url = "http://www.sciencedirect.com/science/article/pii/S2212958817300952",
author = "Joshua Z. Goldenberg and Bonnie S. Burlingham and Jane Guiltinan and Erica B. Oberg",
keywords = "Evidence-based medicine, Naturopathic medicine, Qualitative",
abstract = "Background
Naturopathic medicine (NM) is a distinct system of primary health care often considered a form of complementary and alternative medicine. Evidence-based medicine (EBM) is an approach to medicine which has gained increasing prominence over the past 2 decades. Like other health professions, as the influence of EBM grew in the global medical community, NM had to discover, explore and take stock of it through the lenses of its own unique culture, history and values. We conducted a phenomenological qualitative research study to explore attitudes toward EBM, probe for evidence of cultural change, and to investigate the drivers of said change within the naturopathic medical community.
Methods
Participants were selected by purposive sampling and interviews were based on semi-structured questionnaires focusing on the participants’ perceptions of research, EBM, and their relationship to the field of naturopathic medicine. All interviews were transcribed and then coded independently and in duplicate by two investigators who assigned thematic codes to relevant excerpts. Themes and a concept map were identified, reviewed, and analyzed by investigators. Atlas.ti (version 6.2) software was used for coding and concept mapping.
Results
Seventeen interviews were conducted of which 15 were available for transcription and ranged in length from 17 to 55 minutes. A total of 34 codes were identified, which we aggregated into three themes: (1) a spectrum of EBM definitions, (2) attitudes towards research and EBM, and (3) drivers of change. Interviewees used a spectrum of definitions for EBM which informed their reported attitudes toward it. While current attitudes toward research and EBM were generally described as favorable, “spectrums,” “subgroups,” or even “factions” were described representing a continuum of attitudes within the naturopathic community. Overall, the interviewees described a rapid cultural shift in attitudes from hesitancy to the cautious embrace of research and EBM. Numerous promoters of this cultural change were described with the majority of interviewees emphasizing the importance of influential people within the profession, research and EBM funding, and the desire for acceptance from the larger medical community.
Discussion
As a profession which developed from vitalism on the margins of the larger medical community, naturopathic medicine has grown rapidly in size and influence and, as it has entered new non-vitalistic (mainstream) practice environments, it has incorporated new peers and new role models. Research and EBM acculturation may represent a flashpoint example of a professional adolescence for naturopathic medicine. In which case a relevant question becomes, will the profession be able to adequately integrate the values of its youth/roots with the values of the EBM-driven medical community?"
}
@article{JUNG201882,
title = "Transformation of Arden Syntax's medical logic modules into ArdenML for a business rules management system",
journal = "Artificial Intelligence in Medicine",
volume = "92",
pages = "82 - 87",
year = "2018",
note = "Special Issue on Arden Syntax",
issn = "0933-3657",
doi = "https://doi.org/10.1016/j.artmed.2016.03.005",
url = "http://www.sciencedirect.com/science/article/pii/S0933365716301130",
author = "Chai Young Jung and Jong-Ye Choi and Seong Jik Jeong and Kyunghee Cho and Yong Duk Koo and Jin Hee Bae and Sukil Kim",
keywords = "XML, XSLT, Clinical decision support system, Arden Syntax, ArdenML, Blaze advisor",
abstract = "Introduction
Arden Syntax is a Health Level Seven International (HL7) standard language that is used for representing medical knowledge as logic statements. Arden Syntax Markup Language (ArdenML) is a new representation of Arden Syntax based on XML. Compilers are required to execute medical logic modules (MLMs) in the hospital environment. However, ArdenML may also replace the compiler. The purpose of this study is to demonstrate that MLMs, encoded in ArdenML, can be transformed into a commercial rule engine format through an XSLT stylesheet and made executable in a target system.
Methods
The target rule engine selected was Blaze Advisor. We developed an XSLT stylesheet to transform MLMs in ArdenML into Structured Rules Language (SRL) in Blaze Advisor, through a comparison of syntax between the two languages. The stylesheet was then refined recursively, by building and applying rules collected from the billing and coding guidelines of the Korean health insurance service. Two nurse coders collected and verified the rules and two information technology (IT) specialists encoded the MLMs and built the XSLT stylesheet. Finally, the stylesheet was validated by importing the MLMs into Blaze Advisor and applying them to claims data.
Results
The language comparison revealed that Blaze Advisor requires the declaration of variables with explicit types. We used both integer and real numbers for numeric types in ArdenML. “IF∼THEN” statements and assignment statements in ArdenML become rules in Blaze Advisor. We designed an XSLT stylesheet to solve this issue. In addition, we maintained the order of rule execution in the transformed rules, and added two small programs to support variable declarations and action statements. A total of 1489 rules were reviewed during this study, of which 324 rules were collected. We removed duplicate rules and encoded 241 unique MLMs in ArdenML, which were successfully transformed into SRL and imported to Blaze Advisor via the XSLT stylesheet. When applied to 73,841 outpatients’ insurance claims data, the review result was the same as that of the legacy system.
Conclusion
We have demonstrated that ArdenML can replace a compiler for transforming MLMs into commercial rule engine format. While the proposed XSLT stylesheet requires refinement for general use, we anticipate that the development of further XSLT stylesheets will support various rule engines."
}
@article{HALDER2017421,
title = "JaSTA-2: Second version of the Java Superposition T-matrix Application",
journal = "Computer Physics Communications",
volume = "221",
pages = "421 - 422",
year = "2017",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2017.08.020",
url = "http://www.sciencedirect.com/science/article/pii/S0010465517302667",
author = "Prithish Halder and Himadri Sekhar Das",
keywords = "Java, Cosmic dust aggregates, Light scattering, Polarization, Extinction, Superposition T-matrix code",
abstract = "In this article, we announce the development of a new version of the Java Superposition T-matrix App (JaSTA-2), to study the light scattering properties of porous aggregate particles. It has been developed using Netbeans 7.1.2, which is a java integrated development environment (IDE). The JaSTA uses double precision superposition T-matrix codes for multi-sphere clusters in random orientation, developed by Mackowski and Mischenko (1996). The new version consists of two options as part of the input parameters: (i) single wavelength and (ii) multiple wavelengths. The first option (which retains the applicability of older version of JaSTA) calculates the light scattering properties of aggregates of spheres for a single wavelength at a given instant of time whereas the second option can execute the code for a multiple numbers of wavelengths in a single run. JaSTA-2 provides convenient and quicker data analysis which can be used in diverse fields like Planetary Science, Atmospheric Physics, Nanoscience, etc. This version of the software is developed for Linux platform only, and it can be operated over all the cores of a processor using the multi-threading option.
New version program summary
Program Title: Java superposition T-matrix Application: version - 2.0 Program Files doi:http://dx.doi.org/10.17632/bbtjj8kd74.1 Licensing provisions: GPLv3 Programming language: Fortran, Java. External routines/libraries: jfreechart-1.0.14 [1] (free plotting library for java), j3d-jre-1.5.2 [2] StdDraw3D (3D visualization) Subprograms used: spline.f90, splinek.f90, wavesort.f90 Journal reference of previous version: Comput. Phys. Commun., 2014, 185, 2369 Does the new version supersede the previous version?: No. Nature of problem: Light scattering properties of cosmic dust aggregates Solution method: Java application based on Mackowski and Mishchenko’s Superposition T-Matrix code (1996). Reasons for the new version: The earlier version was mainly developed to calculate the optical properties of cosmic dust aggregates for a single wavelength in vacuum. The user had to calculate multiple times for different wavelengths to analyze the variation of different scattering parameters (e.g., phase function, polarization, the extinction efficiency, the absorption efficiency, the scattering efficiency, etc.) of aggregate particles for a range of wavelengths which was quite time-consuming. Therefore we have developed the new version of JaSTA with an ability to calculate the scattering parameters for a wide range of wavelength. In this new version, we have introduced the multi-threading option to distribute the multi-wavelength calculations to the maximum number of processing cores present in a computer. Hence the calculation time decreases considerably. Summary of revisions: Java superposition T-matrix App (JaSTA) [3] is a Java swing application developed to study the light scattering properties of cosmic dust aggregates based on the Mackowski & Mischenko’s Superposition T-matrix (STM) code [4]. This application calculates the polarization and other scattering matrix elements along with extinction, absorption, and scattering efficiencies for a single wavelength. JaSTA was solely devoted to the light scattering properties of cosmic dust aggregates. Cosmic dust includes comet dust, interplanetary dust, and interstellar dust. Many investigators studied the light scattering properties of comet dust [5]–[10], interplanetary dust [11], interstellar dust [12] using the superposition T-matrix code. JaSTA provided a much better platform for the STM code as a packaged software which can calculate light scattering properties of cosmic dust aggregates for a single wavelength in the vacuum with a click of a button and saves the results in a database so that the saved data can be re-utilized. Recently JaSTA has been used to compute the orientation-averaged scattering matrix elements for fractal aggregates of black carbon aerosols [13]. The interesting feature of the new version of JaSTA is that it can calculate the extinction efficiency at different wavelengths for a given size in a single run which is applicable in the study of interstellar extinction by aggregates at different wavelengths. To analyze the wavelength dependence of extinction one had to execute the calculation for several wavelengths by changing the wavelength and refractive indices in the input for each run in older version of JaSTA. This was very much time consuming and cumbersome. Hence this major drawback led us to rethink and re-establish JaSTA with the multi-wavelength option. JaSTA-2 is the second version of JaSTA aimed to provide the multi-wavelength facility along with the default single wavelength option. JaSTA-2 comes with some other two major updates: cubic spline interpolation of refractive index with wavelengths and multi-threading option for faster calculation. It is developed using Netbeans IDE and is available only for Linux OS. It uses jFreechart-1.0.14 java library to plot various graphs. The multi-wavelength feature will help us to extract the dependence of extinction efficiency on wavelength. Further information on the new features and applicability of JaSTA-2 are provided in the documentation file ‘ JaSTA-2_doc.pdf’, and ‘ readme.txt’ file which are available in the software package. [1]http://www.jfree.org/index.html[2]https://java3d.java.net/[3]P. Halder, A. Chakraborty, P. Deb Roy & H.S. Das, Computer Physics Communications, 185 (2014) 2369-2379.[4]D.W. Mackowski & M.I. Mischenko, J. Opt. Soc. Am. Am., 13 (1996) 2266-2278.[5]H. Kimura, L. Kolokolova & I. Mann, A&A, 407 (2003) L5-L8.[6]H. Kimura, L. Kolokolova & I. Mann, A&A, 449 (2006) 1243-1254.[7]H.S. Das, S.R. Das, T. Paul, A. Suklabaidya & A.K. Sen, MNRAS, 389 (2008) 787-791.[8]H.S. Das, S.R. Das & A.K Sen, MNRAS, 390 (2008) 1195-1199.[9]H.S. Das, A. Suklabaidya, S. Datta Majumder & A.K. Sen, Research in A&A, 10 (2010) 355-362[10]H.S. Das, D. Paul, A. Suklabaidya & A.K. Sen, MNRAS, 416 (2011) 94-100.[11]J. Lasue, A.C. Levasseur-Regourd, N. Fray & H. Cottin, A&A, 473 (2007) 641-649.[12]M. A. Iati, A. Giusto, R. Saija, F. Borghese, P. Denti, C. Cecchi-Pestellini & S. Aielo, ApJ, 615 (2004), 286.[13]A. Pandey & R. K. Chakrabarty, Optics Letters, 41 (2016) 3351-3354."
}
@article{SMITH2018263,
title = "Learning to know, be, do, and live together with in the cross-cultural experiences of immigrant teacher educators",
journal = "Teaching and Teacher Education",
volume = "69",
pages = "263 - 274",
year = "2018",
issn = "0742-051X",
doi = "https://doi.org/10.1016/j.tate.2017.10.018",
url = "http://www.sciencedirect.com/science/article/pii/S0742051X1730375X",
author = "Patriann Smith",
keywords = "Immigrant, International, Multicultural teacher education, Cultural diversity, Cross-cultural, Black, Caribbean, Globalization",
abstract = "This study examined three Afro-Caribbean immigrant teacher educators whose learning based on reflections about their experiences with teachers in the United States revealed how they developed knowledge beyond practice in their learning to know, do, be and live together with others. The educators' learning reflected the processes of observation, reflection, awareness, requesting student feedback in the moment, and the passing of time that resulted in adjustment to their body language, changes in their expectations of students, a modification in their communication, code-switching and sensitivity. Implications based on the study for the new kind of teacher educator are subsequently addressed."
}
@article{UZAKGIDER2015357,
title = "Learning-based approach for layered adaptive video streaming over SDN",
journal = "Computer Networks",
volume = "92",
pages = "357 - 368",
year = "2015",
note = "Software Defined Networks and Virtualization",
issn = "1389-1286",
doi = "https://doi.org/10.1016/j.comnet.2015.09.027",
url = "http://www.sciencedirect.com/science/article/pii/S1389128615003473",
author = "Tuba Uzakgider and Cihat Cetinkaya and Muge Sayit",
keywords = "OpenFlow, Scalable video coding, Reinforcement learning, QoE",
abstract = "Software-defined networking is a recently emerging paradigm that decouples the control and data planes of computer networks. It allows for the implementation of application-specific routing algorithms, and the advantages that the SDN architecture enables can be used to enhance the performance of multimedia communication applications. In this paper, we propose an adaptive video streaming system with a learning-based approach, running over SDN. In the proposed video streaming system, we use a novel learning model to determine the optimal time to re-route the traffic flows and to change the bitrate of the video. The learning model aims to minimize the packet loss rate, quality changes and controller cost while adapting the flow routes and video quality. We have tested the performance of the learning-based approach by comparing it to traditional Internet routing and the greedy approach. The results show that the proposed system significantly outperforms the traditional Internet routing approach and the greedy approach in terms of quality of experience (QoE) and network cost under different network scenarios."
}
@article{GRIESHEIMER201529,
title = "MC21 v.6.0 – A continuous-energy Monte Carlo particle transport code with integrated reactor feedback capabilities",
journal = "Annals of Nuclear Energy",
volume = "82",
pages = "29 - 40",
year = "2015",
note = "Joint International Conference on Supercomputing in Nuclear Applications and Monte Carlo 2013, SNA + MC 2013. Pluri- and Trans-disciplinarity, Towards New Modeling and Numerical Simulation Paradigms",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2014.08.020",
url = "http://www.sciencedirect.com/science/article/pii/S0306454914004058",
author = "D.P. Griesheimer and D.F. Gill and B.R. Nease and T.M. Sutton and M.H. Stedry and P.S. Dobreff and D.C. Carpenter and T.H. Trumbull and E. Caro and H. Joo and D.L. Millman",
keywords = "MC21, Monte Carlo, Reactor calculations",
abstract = "MC21 is a continuous-energy Monte Carlo radiation transport code for the calculation of the steady-state spatial distributions of reaction rates in three-dimensional models. The code supports neutron and photon transport in fixed source problems, as well as iterated-fission-source (eigenvalue) neutron transport problems. MC21 has been designed and optimized to support large-scale problems in reactor physics, shielding, and criticality analysis applications. The code also supports many in-line reactor feedback effects, including depletion, thermal feedback, xenon feedback, eigenvalue search, and neutron and photon heating. MC21 uses continuous-energy neutron/nucleus interaction physics over the range from 10−5eV to 20MeV. The code treats all common neutron scattering mechanisms, including fast-range elastic and non-elastic scattering, and thermal- and epithermal-range scattering from molecules and crystalline materials. For photon transport, MC21 uses continuous-energy interaction physics over the energy range from 1keV to 100GeV. The code treats all common photon interaction mechanisms, including Compton scattering, pair production, and photoelectric interactions. All of the nuclear data required by MC21 is provided by the NDEX system of codes, which extracts and processes data from EPDL-, ENDF-, and ACE-formatted source files. For geometry representation, MC21 employs a flexible constructive solid geometry system that allows users to create spatial cells from first- and second-order surfaces. The system also allows models to be built up as hierarchical collections of previously defined spatial cells, with interior detail provided by grids and template overlays. Results are collected by a generalized tally capability which allows users to edit integral flux and reaction rate information. Results can be collected over the entire problem or within specific regions of interest through the use of phase filters that control which particles are allowed to score each tally. The tally system has been optimized to maintain a high level of efficiency, even as the number of edit regions becomes very large."
}
@article{MONTALTO20151315,
title = "A linear approach for sparse coding by a two-layer neural network",
journal = "Neurocomputing",
volume = "149",
pages = "1315 - 1323",
year = "2015",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2014.08.066",
url = "http://www.sciencedirect.com/science/article/pii/S0925231214011254",
author = "Alessandro Montalto and Giovanni Tessitore and Roberto Prevete",
keywords = "Neural networks, Sparse coding, Linear approach, Encoder–decoder",
abstract = "Many approaches to transform classification problems from non-linear to linear by feature transformation have been recently presented in the literature. These notably include sparse coding methods and deep neural networks. However, many of these approaches require the repeated application of a learning process upon the presentation of unseen data input vectors, or else involve the use of large numbers of parameters and hyper-parameters, which must be chosen through cross-validation, thus increasing running time dramatically. In this paper, we propose and experimentally investigate a new approach for the purpose of overcoming limitations of both kinds. The proposed approach makes use of a linear auto-associative network (called SCNN) with just one hidden layer. The combination of this architecture with a specific error function to be minimized enables one to learn a linear encoder computing a sparse code which turns out to be as similar as possible to the sparse coding that one obtains by re-training the neural network. Importantly, the linearity of SCNN and the choice of the error function allow one to achieve reduced running time in the learning phase. The proposed architecture is evaluated on the basis of two standard machine learning tasks. Its performances are compared with those of recently proposed non-linear auto-associative neural networks. The overall results suggest that linear encoders can be profitably used to obtain sparse data representations in the context of machine learning problems, provided that an appropriate error function is used during the learning phase."
}
@article{DREINER20132604,
title = "Full 1-loop calculation of BR(Bs,d0→ℓℓ̄) in models beyond the MSSM with SARAH  and SPheno",
journal = "Computer Physics Communications",
volume = "184",
number = "11",
pages = "2604 - 2617",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.06.021",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513002282",
author = "H. Dreiner and K. Nickel and W. Porod and F. Staub",
keywords = "Supersymmetry, SARAH, SPheno, Neutral B-meson decays",
abstract = "We present the possibility of calculating the quark flavor changing neutral current decays Bs0→ℓℓ̄ and Bd0→ℓℓ̄ for a large variety of supersymmetric models. For this purpose, the complete one-loop calculation has been implemented in a generic form in the Mathematica package SARAH. This information is used by SARAH  to generate Fortran source code for SPheno  for a numerical evaluation of these processes in a given model. We comment also on the possibility to use this setup for non-supersymmetric models.
Program summary
Program title: SARAH Catalogue identifier: AEIB_v2_1 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIB_v2_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 467777 No. of bytes in distributed program, including test data, etc.: 3927691 Distribution format: tar.gz Programming language: Mathematica, Fortran. Computer: All computers which can run Mathematica and SPheno. Operating system: Linux, MacOS. Classification: 11.1, 11.6. Catalogue identifier of previous version: AEIB_v2_0 Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 1792 Does the new version supercede the previous version?: Yes, the new version includes all known features of previous versions but provides also the new features mentioned below. Nature of problem: Models beyond the SM can have new contributions to the decays of neutral B-mesons. For a precise prediction of the corresponding branching ratios a full 1-loop calculation including all possible wave, penguin and box diagrams is necessary. This usually requires a big effort and public codes for these calculations so far only support a few selected models. Solution method: The implementation of a new model in SARAH is easy and straightforward. In addition, SARAH is already delivered with many different supersymmetric and a few non-supersymmetric models. As a first step, SARAH derives the analytical expressions for masses, interactions and renormalization group equations for the given model. Furthermore, SARAH checks for all possible tree- and 1-loop diagrams which can contribute to the B-meson decays into two leptons. This information is exported to Fortran source code which can afterwards be compiled with SPheno. This generates a fully functional spectrum generator: besides the mass spectrum, sparticle and Higgs decays the new SPheno modules also calculate precision observables like the B-meson decays based on the parameters chosen by the user. Reasons for new version: The possible decays of neutral B-mesons into two leptons are constraining models beyond the Standard Model. SARAH allows, in the new version, the production of SPheno source code to calculate those decays at full 1-loop for a large variety of models. Summary of revisions: Full 1-loop calculation of Bs,d0→ℓℓ̄ for any model which can be implemented in SARAH. Restrictions: SARAH can only calculate the renormalization group equations for a supersymmetric model. Hence, for a non-supersymmetric model it is not possible to calculate the running parameters which enter the calculation. These have to be provided by the user as input. In addition, the effects of chiral resummation as well as next-to-leading order QCD corrections known for the MSSM are not included. Unusual features: This is the first public tool which allows a full 1-loop calculation of B-meson decays in more complicated models than the minimal or next-to-minimal supersymmetric standard model. Any new contribution in a renormalizable model stemming from an extended matter content or gauge sector is taken into account. Running time: Measured CPU time for the evaluation of the MSSM using a Lenovo Thinkpad X220 with i7 processor (2.53 GHz). Calculating the complete Lagrangian: 9 s. Calculating all vertices: 51 s. Output of the UFO model files: 49 s."
}
@article{SEIDL201789,
title = "Generative software product line development using variability-aware design patterns",
journal = "Computer Languages, Systems & Structures",
volume = "48",
pages = "89 - 111",
year = "2017",
note = "Special Issue on the 14th International Conference on Generative Programming: Concepts & Experiences (GPCE'15)",
issn = "1477-8424",
doi = "https://doi.org/10.1016/j.cl.2016.08.006",
url = "http://www.sciencedirect.com/science/article/pii/S1477842415300609",
author = "Christoph Seidl and Sven Schuster and Ina Schaefer",
abstract = "Software Product Lines (SPLs) are an approach to reuse in-the-large that models a set of closely related software systems in terms of commonalities and variabilities. Design patterns are best practices for addressing recurring design problems in object-oriented source code. In the practice of implementing SPL, instances of certain design patterns are employed to handle variability, which makes these “variability-aware design patterns” a best practice for SPL design. However, currently there is no dedicated method for proactively developing SPLs using design patterns suitable for realizing variable functionality. In this paper, we present a method to perform generative SPL development with design patterns. We use role models to capture design patterns and their relation to a variability model. We further allow mapping of individual design pattern roles to (parts of) implementation elements to be generated (e.g., classes, methods) and check the conformance of the realization with the specification of the pattern. We provide definitions for the variability-aware versions of the design patterns Observer, Strategy, Template Method and Composite. Furthermore, we support generation of realizations in Java, C++ and UML class diagrams utilizing annotative, compositional and transformational variability realization mechanisms. Hence, we support proactive development of SPLs using design patterns to apply best practices for the realization of variability. We realize our concepts within the Eclipse IDE and demonstrate them within a case study."
}
@article{SCHLAFFKE2017429,
title = "Dynamic changes of resting state connectivity related to the acquisition of a lexico-semantic skill",
journal = "NeuroImage",
volume = "146",
pages = "429 - 437",
year = "2017",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2016.08.065",
url = "http://www.sciencedirect.com/science/article/pii/S1053811916304578",
author = "L. Schlaffke and L. Schweizer and N.N. Rüther and R. Luerding and M. Tegenthoff and C. Bellebaum and T. Schmidt-Wilcke",
keywords = "Resting state – fMRI, Resting state connectivity, Learning, Neuroplasticity, Language",
abstract = "The brain undergoes adaptive changes during learning. Spontaneous neural activity has been proposed to play an important role in acquiring new information and/or improve the interaction of task related brain regions. A promising approach is the investigation of resting state functional connectivity (rs-fc) and resting state networks, which rely on the detection of interregional correlations of spontaneous BOLD fluctuations. Using Morse Code (MC) as a model to investigate neural correlates of lexico-semantic learning we sought to identify patterns in rs-fc that predict learning success and/or undergo dynamic changes during a 10-day training period. Thirty-five participants were trained to decode twelve letters of MC. Rs-fMRI data were collected before and after the training period and rs-fc analyses were performed using a group independent component analysis. Baseline connectivity between the language-network (LANG) and the anterior-salience-network (ASN) predicted learning success and learning was associated with an increase in LANG – ASN connectivity. Furthermore, a disconnection between the default mode network (DMN) and the ASN as well as the left fusiform gyrus, which is critically involved in MC deciphering, was observed. Our findings demonstrate that rs-fc can undergo behaviorally relevant changes within 10 training days, reflecting a learning dependent modulation of interference between task specific networks."
}
@article{MENDIVELSO2018,
title = "A brief history of parameterized matching problems",
journal = "Discrete Applied Mathematics",
year = "2018",
issn = "0166-218X",
doi = "https://doi.org/10.1016/j.dam.2018.07.017",
url = "http://www.sciencedirect.com/science/article/pii/S0166218X18304049",
author = "Juan Mendivelso and Sharma V. Thankachan and Yoan Pinzón",
keywords = "Parameterized matching, String matching, Software maintenance, Function matching",
abstract = "Parameterized pattern matching is a string searching variant that was initially defined to detect duplicate code but later proved to support several other applications. In particular, two equal-length strings X andY are a parameterized-match if there exists a bijective function g for which every text symbol in X is equal to g(Y). Baker was the first researcher to have addressed this problem (Baker, 1993) and, since then, many others have followed her work. She did, indeed, open up a wide field of extensive research. Over the years, many variants and extensions that have been pursued include: parameterized matching under edit and Hamming distances, parameterized multi-pattern matching, two dimensional parameterized matching, structural matching, function matching, and the very recent developments in succinct and streaming models. This accelerated research could only be justified by the usefulness of its practical applications such as in software maintenance, image processing and bioinformatics to name some. Even though the problem was posed about 25 years ago, research on parameterized matching is still very active. Its extensive study over the years and its current relevance motivate us to review the most notorious contributions as road map for current and future research."
}
@article{DUFVA201697,
title = "Metaphors of code—Structuring and broadening the discussion on teaching children to code",
journal = "Thinking Skills and Creativity",
volume = "22",
pages = "97 - 110",
year = "2016",
issn = "1871-1871",
doi = "https://doi.org/10.1016/j.tsc.2016.09.004",
url = "http://www.sciencedirect.com/science/article/pii/S1871187116301055",
author = "Tomi Dufva and Mikko Dufva",
keywords = "Code, Code literacy, Metaphors, Education, Programming, Teaching programming, Pedagogy, Media literacy",
abstract = "Digital technology has become embedded into our daily lives. Code is at the heart of this technology. The way code is perceived influences the way our everyday interaction with digital technologies is perceived: is it an objective exchange of ones and zeros, or a value- laden power struggle between white male programmers and those who think they are users, when they are, in fact, the product being sold. Understanding the nature of code thus enables the imagination and exploration of the present state and alternative future developments of digital technologies. A wider imagination is especially important for developing basic education so that it provides the capabilities for coping with these developments. Currently, the discussion has been mainly on the technical details of code. We study how to broaden this narrow view in order to support the design of more comprehensive and future-proof education around code and coding. We approach the concept of code through nine different metaphors from the existing literature on systems thinking and organisational studies. The metaphors we use are machine, organism, brain, flux and transformation, culture, political system, psychic prison, instrument of domination and carnival. We describe their epistemological backgrounds and give examples of how code is perceived through each of them. We then use the metaphors in order to suggest different complementary ways that ICT could be taught in schools. The metaphors illustrate different contexts and help to interpret the discussions related to developments in digital technologies such as free software movement, democratization of information and internet of things. They also help to identify the dominant views and the tensions between the views. We propose that the systematic use of metaphors described in this paper would be a useful tool for broadening and structuring the dialogue about teaching children to code."
}
@article{ROLLS201820,
title = "Non-accidental properties, metric invariance, and encoding by neurons in a model of ventral stream visual object recognition, VisNet",
journal = "Neurobiology of Learning and Memory",
volume = "152",
pages = "20 - 31",
year = "2018",
issn = "1074-7427",
doi = "https://doi.org/10.1016/j.nlm.2018.04.017",
url = "http://www.sciencedirect.com/science/article/pii/S1074742718301023",
author = "Edmund T. Rolls and W. Patrick C. Mills",
keywords = "Visual object recognition, Non-accidental properties, Visual coding, Unsupervised learning, Invariant representations, Inferior temporal visual cortex, VisNet, Trace learning rule, Slow learning",
abstract = "When objects transform into different views, some properties are maintained, such as whether the edges are convex or concave, and these non-accidental properties are likely to be important in view-invariant object recognition. The metric properties, such as the degree of curvature, may change with different views, and are less likely to be useful in object recognition. It is shown that in a model of invariant visual object recognition in the ventral visual stream, VisNet, non-accidental properties are encoded much more than metric properties by neurons. Moreover, it is shown how with the temporal trace rule training in VisNet, non-accidental properties of objects become encoded by neurons, and how metric properties are treated invariantly. We also show how VisNet can generalize between different objects if they have the same non-accidental property, because the metric properties are likely to overlap. VisNet is a 4-layer unsupervised model of visual object recognition trained by competitive learning that utilizes a temporal trace learning rule to implement the learning of invariance using views that occur close together in time. A second crucial property of this model of object recognition is, when neurons in the level corresponding to the inferior temporal visual cortex respond selectively to objects, whether neurons in the intermediate layers can respond to combinations of features that may be parts of two or more objects. In an investigation using the four sides of a square presented in every possible combination, it was shown that even though different layer 4 neurons are tuned to encode each feature or feature combination orthogonally, neurons in the intermediate layers can respond to features or feature combinations present is several objects. This property is an important part of the way in which high capacity can be achieved in the four-layer ventral visual cortical pathway. These findings concerning non-accidental properties and the use of neurons in intermediate layers of the hierarchy help to emphasise fundamental underlying principles of the computations that may be implemented in the ventral cortical visual stream used in object recognition."
}
@article{RAO2014216,
title = "Assessment of ASSERT-PV for prediction of critical heat flux in CANDU bundles",
journal = "Nuclear Engineering and Design",
volume = "276",
pages = "216 - 227",
year = "2014",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2014.06.004",
url = "http://www.sciencedirect.com/science/article/pii/S0029549314003422",
author = "Y.F. Rao and Z. Cheng and G.M. Waddington",
abstract = "Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The recently released ASSERT-PV 3.2 provides enhanced models for improved predictions of flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against five full-scale CANDU bundle experiments conducted in 1990s and in 2009 by Stern Laboratories (SL), using 28-, 37- and 43-element (CANFLEX) bundles. A total of 15 CHF test series with varying pressure-tube creep and/or bearing-pad height were analyzed. The SL experiments encompassed the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for CANDU reactors. Code predictions of channel dryout power and axial and radial CHF locations were compared against measurements from the SL CHF tests to quantify the code prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. Furthermore, the sensitivity studies evaluated the contribution of each CHF model change or enhancement to the improvement in CHF prediction. Overall, the assessment demonstrated significant improvement in prediction of channel dryout power and axial and radial CHF locations in horizontal fuel channels containing CANDU bundles."
}
@article{MITTERER2015116,
title = "Letters don’t matter: No effect of orthography on the perception of conversational speech",
journal = "Journal of Memory and Language",
volume = "85",
pages = "116 - 134",
year = "2015",
issn = "0749-596X",
doi = "https://doi.org/10.1016/j.jml.2015.08.005",
url = "http://www.sciencedirect.com/science/article/pii/S0749596X15001011",
author = "Holger Mitterer and Eva Reinisch",
keywords = "Spoken-word recognition, Visual-word recognition, Phonological reduction",
abstract = "It has been claimed that learning to read changes the way we perceive speech, with detrimental effects for words with sound–spelling inconsistencies. Because conversational speech is peppered with segment deletions and alterations that lead to sound–spelling inconsistencies, such an influence would seriously hinder the perception of conversational speech. We hence tested whether the orthographic coding of a segment influences its deletion costs in perception. German glottal stop, a segment that is canonically present but not orthographically coded, allows such a test. The effects of glottal-stop deletion in German were compared to deletion of /h/ in German (grapheme: h) and deletion of glottal stop in Maltese (grapheme: q) in an implicit task with conversational speech and explicit task with careful speech. All segment deletions led to similar reduction costs in the implicit task, while an orthographic effect, with larger effects for orthographically coded segments, emerged in the explicit task. These results suggest that learning to read does not influence how we process speech but mainly how we think about it."
}
@article{FIGUEIREDO2019328,
title = "Study of the influence of atmospheric air climatic parameters on the air kerma measurements in low energy X reference radiation fields",
journal = "Radiation Physics and Chemistry",
volume = "155",
pages = "328 - 331",
year = "2019",
note = "IRRMA-10",
issn = "0969-806X",
doi = "https://doi.org/10.1016/j.radphyschem.2018.08.031",
url = "http://www.sciencedirect.com/science/article/pii/S0969806X17308101",
author = "M.T.T. Figueiredo and M.A.S. Lacerda and T.A. da Silva",
keywords = "Low energy X-ray beam, X-ray dosimetry, Monte Carlo simulation",
abstract = "The control of climatic conditions in a radiation metrology laboratory is very important. Air pressure, temperature and humidity affect the value of the air density, and, consequently, alter the absorption of the photon radiation. Then, the values of the air kerma Ka, conversion coefficients from Ka to the dose equivalent quantities (hp,K(10,α) and h*K(10)) and their product (Hp(10) and H*(10)) are affected by climatic changes. For low energy X radiation fields, changes in climatic conditions are more critical. The International Organization for Standardization (ISO), via the ISO 4037-4, specifies corrections for air density for all quantities defined in 10 mm depth in tissue for nominal tube potentials varying from 10 kV to 30 kV (inclusive). In this work, we utilized monte carlo method to evaluate the influence of atmospheric air climate parameters on the air kerma measurements, for the ISO low energies, series N and L. Simulations were performed using the MCNPX code version 2.7.d, running under MPI (Message Passing Interface) on a computational cluster. We simulated the air with different humidity levels, and consequently, different densities and elemental compositions. The ISO 4037 reference beams of the Dosimeters Calibration Laboratory of the Nuclear Technology Development Center (LCD/CDTN) were utilized to validate the Monte Carlo simulations."
}
@article{ROCHA2018,
title = "Automatic parallelization of recursive functions with rewriting rules",
journal = "Science of Computer Programming",
year = "2018",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2018.01.004",
url = "http://www.sciencedirect.com/science/article/pii/S0167642318300066",
author = "Rodrigo C.O. Rocha and Luís F.W. Góes and Fernando M.Q. Pereira",
keywords = "Recursive functions, Parallel computing, Functional programming, Algebraic framework, Abstract algebra",
abstract = "Functional programming languages, since their early days, have been regarded as the holy grail of parallelism. And, in fact, the absence of race conditions, coupled with algorithmic skeletons such as map and reduce, have given developers the opportunity to write many different techniques aimed at the automatic parallelization of programs. However, there are many functional programs that are still difficult to parallelize. This difficulty stems from many factors, including the complex syntax of recursive functions. This paper provides new equipment to deal with this problem. Such instrument consists of an insight, plus a code transformation that is enabled by this insight. Concerning the first contribution, we demonstrate that many recursive functions can be rewritten as a combination of associative operations. We group such functions into two categories, which involve monoid and semiring operations. Each of these categories admits a parallel implementation. To demonstrate the effectiveness of this idea, we have implemented an automatic code rewriting tool for Haskell, and have used it to convert six well-known recursive functions to algorithms that run in parallel. Our tool is totally automatic, and it is able to deliver non-trivial speedups upon the sequential version of the programs that it receives. In particular, the automatically generated parallel code delivers good scalability when varying the number of threads or the input size."
}
@article{MARGOLIS2017333,
title = "Student Reflection Papers on a Global Clinical Experience: A Qualitative Study",
journal = "Annals of Global Health",
volume = "83",
number = "2",
pages = "333 - 338",
year = "2017",
note = "Current Topics in Global Health",
issn = "2214-9996",
doi = "https://doi.org/10.1016/j.aogh.2017.04.006",
url = "http://www.sciencedirect.com/science/article/pii/S2214999617306045",
author = "Carmi Z. Margolis and Robert M. Rohrbaugh and Luisa Tsang and Jennifer Fleischer and Mark J. Graham and Anne Kellett and Janet P. Hafler",
keywords = "global health education, reflection papers, qualitative research, medical student education, international health",
abstract = "Background
Many of the 70,000 graduating US medical students [per year] have reported participating in a global health activity at some stage of medical school. This case study design provided a method for understanding the student's experience that included student’s learning about culture, health disparities, exposure and reaction to a range of diseases actually encountered. The broad diversity of themes among students indicated that the GCE provided a flexible, personalized experience. We need to understand the student’s experience in order to help design appropriate curricular experiences [and valid student assessment].
Objective
Our research aim was to analyze medical student reflection papers to understand how they viewed their Global Clinical Experience (GCE).
Methods
A qualitative case study design was used to analyze student reflection papers. All 28 students who participated in a GCE from 2008-2010 and in 2014-2015 and submitted a reflection paper on completion of the GCE were eligible to participate in the study. One student did not submit a reflection paper and was not included in the study.
Findings
All 27 papers were coded by paragraph for reflection and for themes. System of Care/Range of Care was mentioned most often, Aids to Adjustment Process was mentioned least. The theme, “Diseases,” referred to any mention of a disease in the reflection papers, and 44 diseases were mentioned in the papers. The analysis for depth of reflection yielded the following data: Observation, 81/248 paragraphs; Observation and Interpretation, 130/248 paragraphs; and Observation, Interpretation, and Suggestions for change, 36/248 paragraphs; 9 reflection papers contained 27 separate accounts of a transformational experience.
Conclusions
This study provided a method for understanding the student's experience that included student’s learning about culture, health disparities, and exposure and reaction to a range of diseases actually encountered. The broad diversity of themes among students indicated that the GCE provided a flexible, personalized experience. How we might design a curriculum to facilitate transformational learning experiences needs further research."
}
@article{LEANDRO201959,
title = "Thermal hydraulic model of the molten salt reactor experiment with the NEAMS system analysis module",
journal = "Annals of Nuclear Energy",
volume = "126",
pages = "59 - 67",
year = "2019",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2018.10.060",
url = "http://www.sciencedirect.com/science/article/pii/S0306454918305887",
author = "Adrian M. Leandro and Florent Heidet and Rui Hu and Nicholas R. Brown",
keywords = "Molten Salt Reactor Experiment (MSRE), System Analysis Module (SAM), FLiBe, Molten Salt Reactors (MSR)",
abstract = "System analysis codes have a long history of providing best-estimate and conservative safety analysis for both light water and advanced reactor technologies, including molten salt reactors. As interest continues to expand with advanced reactor concepts, system analysis codes will need revisions to accommodate the behavior of these technologies. Legacy system analysis codes will need to be updated to the latest numerical techniques to shorten execution time and increase the accuracy of results. One example of a modern system analysis code that already encompasses these characteristics is the System Analysis Module (SAM). One key objective of this paper was to review available information for system code modeling of the Molten Salt Reactor Experiment (MSRE) from sources in the open literature and collect the information from these open sources in one place for the first time. This supports the potential objective of developing an open specification for system code analysis for MSRE steady state and transients with and without reactor kinetics. Data from actual MSRE tests will serve as the basis for code-to-code comparison exercises, including the MSRE zero power physics tests, the fuel pump start-up and coast down tests, and the natural circulation transient. The objective is to produce a code-to-code benchmark with a standardized set of comparison problems, recognizing the limitations of the original data. To demonstrate an initial application of this objective and the usefulness of compiling this open data, two Molten Salt Reactor Experiment (MSRE)-related models were developed to evaluate SAM for liquid fueled molten salt reactors. One model was the SAM MSRE hydraulic mockup, which provided experimental data for pressure drop measurements. The second model was the complete MSRE primary loop. The MSRE primary loop model incorporated a fluoride salt fuel/coolant with heat transfer in both the core and heat exchanger. For both the hydraulic mockup and MSRE primary loop models, a holistic 1-D system description was built using open documentation, an open description that can be readily modified and applied for any system analysis code. SAM results for the pressure drop of the hydraulic mockup model were within 6% with measurements. Coolant temperatures for the primary loop model matched the expected axial change in temperature from historical calculations. Using alternative coolant properties obtained from the literature, corresponding to salts with different actinide contents, returned similar trends in core temperature profiles. A thermal hydraulic demonstration of a loss-of-flow transient showed the importance of coupling SAM thermal hydraulic analysis to neutronics. This coupling is essential for simulating MSR transients with system analysis codes."
}
@article{GONTCHAR2018414,
title = "DFMDEF18: A C-code for the double folding interaction potential of a spherical nucleus with deformed nucleus",
journal = "Computer Physics Communications",
volume = "222",
pages = "414 - 417",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2017.09.008",
url = "http://www.sciencedirect.com/science/article/pii/S0010465517302953",
author = "I.I. Gontchar and M.V. Chushnyakova",
keywords = "Nucleus–nucleus collision, Coulomb barrier, Double folding model, Density dependent  forces, M3Y-interaction",
abstract = "This is a new version of the DFMDEF code published earlier. The new version is designed to obtain the nucleus–nucleus potential between two nuclei (one of which may be deformed) by using the double folding model (DFM). In particular the code enables to find the Coulomb barrier. The new version allows the user to employ his (her) own charge, proton, and neutron density distributions. The main functionalities of the original code (the nucleus–nucleus potential as a function of the incident angle and the distance between the centers of mass of colliding nuclei; the Coulomb barrier characteristics) have not been modified.
New version program summary
Program title: DFMDEF18 Program Files doi:http://dx.doi.org/10.17632/y9pwd2p24z.1 Licensing provisions: CC0 1.0 Programming language: C Journal reference of previous version: Comp. Phys. Comm. 184 (2013) 172 Does the new version supersede the previous version? Yes Nature of problem: The code calculates in a semimicroscopic way the bare interaction potential between two colliding nuclei one of which can be deformed and axially symmetric. The potential is evaluated as a function of the center of mass distance and the angle between the axis of symmetry and the beam direction. The heights and the positions of the Coulomb barriers are found. Dependence of the barrier parameters upon the characteristics of the effective NN forces (like, e.g. the range of the exchange part of the nuclear term) as well as upon the parameters of the density distributions can be investigated. Method of solution: The nucleus–nucleus potential is calculated using the double folding model with the Coulomb and the effective M3Y NN interactions. For the direct parts of the Coulomb and the nuclear terms, the Fourier transform method is used. In order to calculate the exchange parts, the density matrix expansion method is applied. Reason for new version: Many users asked us how to implement their own density distributions in the code. Now this option has been added. Summary of revisions: 1. Additional features of DFMDEF18:Projectile and target densities as input files In the DFMDEF [1, 2], only the Woods–Saxon profile for the charge, proton, and neutron density distributions was used. In the new version, DFMDEF18, the user can use the same profile, but there is an option to provide six input files <inp_rhoP_Z.c>, <inp_rhoP_N.c>, <inp_rhoP_q.c>, <inp_rhoT_Z.c>, <inp_rhoT_N.c>, and <inp_rhoT_q.c>similar to what was done in [3]. In these files the proton (_Z), neutron (_N), and charge (_q) density distributions are defined for the projectile (P) and target (T) nuclei as functions of the distance from the nucleus center (r) and the zenith angle (the_deg). We will refer to this set of six files as to “rho-input files”. The technical explanation of the rho-input files might be found in file <Program_changes.txt>(subsection 2.11). The values of the densities required for producing the interaction potential are found by means of the following interpolating polynomial. In order to avoid the overlap in notations for the zenith angle, instead of the traditional θ, we use here Θ. (1)ρr,Θ=ρr0,Θ0∗r−r1r0−r1∗r−r2r0−r2∗Θ−Θ1Θ0−Θ1∗Θ−Θ2Θ0−Θ2+ρr1,Θ0∗r−r0r1−r0∗r−r2r1−r2∗Θ−Θ1Θ0−Θ1∗Θ−Θ2Θ0−Θ2+ρr2,Θ0∗r−r1r2−r1∗r−r0r2−r0∗Θ−Θ1Θ0−Θ1∗Θ−Θ2Θ0−Θ2+ρr0,Θ1∗r−r1r0−r1∗r−r2r0−r2∗Θ−Θ0Θ1−Θ0∗Θ−Θ2Θ1−Θ2+ρr0,Θ2∗r−r1r0−r1∗r−r2r0−r2∗Θ−Θ1Θ2−Θ1∗Θ−Θ0Θ2−Θ0+ρr1,Θ1∗r−r0r1−r0∗r−r2r1−r2∗Θ−Θ0Θ1−Θ0∗Θ−Θ2Θ1−Θ2+ρr1,Θ2∗r−r0r1−r0∗r−r2r1−r2∗Θ−Θ1Θ2−Θ1∗Θ−Θ0Θ2−Θ0+ρr2,Θ1∗r−r1r2−r1∗r−r0r2−r0∗Θ−Θ0Θ1−Θ0∗Θ−Θ2Θ1−Θ2+ρr2,Θ2∗r−r1r2−r1∗r−r0r2−r0∗Θ−Θ1Θ2−Θ1∗Θ−Θ0Θ2−Θ0.Thus the density at the point with arbitrary coordinates r,Θ is determined by the nine values of the density ρri,Θi from the tables provided by user. The values r0,r1,r2 (Θ0,Θ1,Θ2) are the closest ones to the necessary r (Θ), and the interval r0÷r2 (Θ0÷Θ2) always contains the value ofr (Θ). This interpolation works rather accurately. However, due to the inevitable numerical errors in the derivatives entering the kF (see Eq. (26) in Ref. [1]) the value of kF2 sometimes becomes negative. Therefore in the present code we keep only zero-order term calculating kF as follows (2)kF=1.5π2ρAr→1∕3.The quality of this approximation is illustrated by Fig. 1. In Fig. 1 a the Coulomb barrier heights obtained using the earlier published code [1] (line with symbols, Vb1) and the present code (line without symbols, Vbp) are shown. One sees that two codes produce very close results. To quantify this statement we present in Fig. 1 b the fractional difference between the barrier heights (3)ε=Vb1Vbp−1.Since the fractional difference does not exceed 0.5% we believe the approximation (2) can beaccepted. The original version of the code [1] allowed performing the calculations of the strong (nuclear) term of the nucleus–nucleus interaction potential in two different ways. Namely, the phenomenological Woods–Saxon parametrization [4, 5] and semimicroscopic double folding calculations could be used. In order to use the Woods–Saxon parametrization it is necessary to know the deformation parameters of the target nucleus. If the code uses the rho-input files for the densities provided by the user, such parameters can be absent. Therefore the Woods–Saxon parametrization for the potential is removed from the present version of the code. The Woods–Saxon approximation of the calculated DFM nuclear term of the potential is also removed for the same reason. Thus in the present code only the options concerning the double folding calculations remain. 2. The program The code consists now of 8 files and one header file. It reads the data from 8 input files and prints the results into two output files. For specific details regarding the changes in each source file see the file <Program_changes.txt>. The main input file has been split into two files: <inp_dfpdef.c> and <inp_dens.c>. Their description as well as the description of the output file might be also found in the file <Program_changes.txt>. References[1]I. I. Gontchar, M. V. Chushnyakova, Comput. Phys. Commun. 184 (2013) 172.[2]I. I. Gontchar, D. J. Hinde, M. Dasgupta, C. R. Morton, J. O. Newton, Phys. Rev. C 73 (2006) 034610.[3]I. I. Gontchar, M. V. Chushnyakova, Comput. Phys. Commun. 206 (2016) 97.[4]I.I. Gontchar, M. Dasgupta, D. J. Hinde, R. D. Butt, A. Mukherjee, Phys. Rev. C 65 (2002) 034610.[5]I.I. Gontchar, M. Dasgupta, D. J. Hinde, J. O. Newton, Phys. At. Nucl. 69 (2006) 1428Appendix TEST RUN OUTPUT Input file <inp_dfpdef.c>  Output file <out_dfmsph.c> "
}
@article{BUTYKAI2017507,
title = "PFMCal : Photonic force microscopy calibration extended for its application in high-frequency microrheology",
journal = "Computer Physics Communications",
volume = "220",
pages = "507 - 508",
year = "2017",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2017.07.019",
url = "http://www.sciencedirect.com/science/article/pii/S0010465517302394",
author = "A. Butykai and P. Domínguez-García and F.M. Mor and R. Gaál and L. Forró and S. Jeney",
keywords = "Calibration of optical tweezers, Brownian motion, Power spectral density (PSD), Mean square displacement (MSD), Velocity autocorrelation function (VAF)",
abstract = "The present document is an update of the previously published MatLab code for the calibration of optical tweezers in the high-resolution detection of the Brownian motion of non-spherical probes [1]. In this instance, an alternative version of the original code, based on the same physical theory [2], but focused on the automation of the calibration of measurements using spherical probes, is outlined. The new added code is useful for high-frequency microrheology studies, where the probe radius is known but the viscosity of the surrounding fluid maybe not. This extended calibration methodology is automatic, without the need of a user’s interface. A code for calibration by means of thermal noise analysis [3] is also included; this is a method that can be applied when using viscoelastic fluids if the trap stiffness is previously estimated [4]. The new code can be executed in MatLab and using GNU Octave.
New version program summary
Program Title: PFMCal Program Files doi:http://dx.doi.org/10.17632/s59f3gz729.1 Licensing provisions: GPLv3 Programming language: MatLab 2016a (MathWorks Inc.) and GNU Octave 4.0 Operating system: Linux and Windows. Supplementary material: A new document README.pdf includes basic running instructions for the new code. Journal reference of previous version: Computer Physics Communications, 196 (2015) 599 Does the new version supersede the previous version?: No. It adds alternative but compatible code while providing similar calibration factors. Nature of problem (approx. 50–250 words): The original code uses a MatLab-provided user’s interface, which is not available in GNU Octave, and cannot be used outside of a proprietary software as MatLab. Besides, the process of calibration when using spherical probes needs an automatic method when calibrating big amounts of different data focused to microrheology. Solution method (approx. 50–250 words): The new code can be executed in the latest version of MatLab and using GNU Octave, a free and open-source alternative to MatLab. This code generates an automatic calibration process which requires only to write the input data in the main script. Additionally, we include a calibration method based on thermal noise statistics, which can be used with viscoelastic fluids if the trap stiffness is previously estimated. Reasons for the new version: This version extends the functionality of PFMCal for the particular case of spherical probes and unknown fluid viscosities. The extended code is automatic, works in different operating systems and it is compatible with GNU Octave. Summary of revisions: The original MatLab program in the previous version, which is executed by PFMCal.m, is not changed. Here, we have added two additional main archives named PFMCal_auto.m and PFMCal_histo.m, which implement automatic calculations of the calibration process and calibration through Boltzmann statistics, respectively. The process of calibration using this code for spherical beads is described in the README.pdf file provided in the new code submission. Here, we obtain different calibration factors, β (given in μm/V), according to [2], related to two statistical quantities: the mean-squared displacement (MSD), βMSD, and the velocity autocorrelation function (VAF), βVAF. Using that methodology, the trap stiffness, k, and the zero-shear viscosity of the fluid, η, can be calculated if the value of the particle’s radius, a, is previously known. For comparison, we include in the extended code the method of calibration using the corner frequency of the power-spectral density (PSD) [5], providing a calibration factor βPSD. Besides, with the prior estimation of the trap stiffness, along with the known value of the particle’s radius, we can use thermal noise statistics to obtain calibration factors, β, according to the quadratic form of the optical potential, βE, and related to the Gaussian distribution of the bead’s positions, βσ2. This method has been demonstrated to be applicable to the calibration of optical tweezers when using non-Newtonian viscoelastic polymeric liquids [4]. An example of the results using this calibration process is summarized in Table 1. Using the data provided in the new code submission, for water and acetone fluids, we calculate all the calibration factors by using the original PFMCal.m and by the new non-GUI code PFMCal_auto.m and PFMCal_histo.m. Regarding the new code, PFMCal_auto.m returns η, k, βMSD, βVAF and βPSD, while PFMCal_histo.m provides βσ2 and βE. Table 1 shows how we obtain the expected viscosity of the two fluids at this temperature and how the different methods provide good agreement between trap stiffnesses and calibration factors. Additional comments including Restrictions and Unusual features (approx. 50–250 words): The original code, PFMCal.m, runs under MatLab using the Statistics Toolbox. The extended code, PFMCal_auto.m and PFMCal_histo.m, can be executed without modification using MatLab or GNU Octave. The code has been tested in Linux and Windows operating systems. References[1]A. Butykai, F. Mor, R. Gaál, P. Domínguez-García, L. Forró, J. S., Calibration of optical tweezers with non-spherical probes via high-resolution detection of brownian motion, Comput. Phys. Commun. 196 (2015) 599–610.[2]M. Grimm, T. Franosch, S. Jeney, High-resolution detection of brownian motion for quantitative optical tweezers experiments, Phys. Rev. E 86 (2012) 021912.[3]E.-L. Florin, A. Pralle, E. H. K. Stelzer, J. K. H. Hörber, Photonic force microscope calibration by thermal noise analysis, Appl. Phys. A 66 (1998) S75–S78.[4]P. Domínguez-García, L. Forró, S. Jeney, Interplay between optical, viscous, and elastic forces on an optically trapped brownian particle immersed in a viscoelastic fluid, Appl. Phys. Lett. 109 (14) (2016) 143702.[5]K. Berg-Sørensen, H. Flyvbjerg, Power spectrum analysis for optical tweezers, Rev. Sci. Instrum. 75 (3) (2004) 594–612."
}
@article{TANANA201643,
title = "A Comparison of Natural Language Processing Methods for Automated Coding of Motivational Interviewing",
journal = "Journal of Substance Abuse Treatment",
volume = "65",
pages = "43 - 50",
year = "2016",
note = "Motivational Interviewing in Substance Use Treatment",
issn = "0740-5472",
doi = "https://doi.org/10.1016/j.jsat.2016.01.006",
url = "http://www.sciencedirect.com/science/article/pii/S0740547216000222",
author = "Michael Tanana and Kevin A. Hallgren and Zac E. Imel and David C. Atkins and Vivek Srikumar",
keywords = "Behavioral coding, Discrete sentence feature model, Machine learning, Motivational interviewing, Natural language processing, Recursive neural network, Treatment integrity",
abstract = "Motivational interviewing (MI) is an efficacious treatment for substance use disorders and other problem behaviors. Studies on MI fidelity and mechanisms of change typically use human raters to code therapy sessions, which requires considerable time, training, and financial costs. Natural language processing techniques have recently been utilized for coding MI sessions using machine learning techniques, rather than human coders, and preliminary results have suggested these methods hold promise. The current study extends this previous work by introducing two natural language processing models for automatically coding MI sessions via computer. The two models differ in the way they semantically represent session content, utilizing either 1) simple discrete sentence features (DSF model) and 2) more complex recursive neural networks (RNN model). Utterance- and session-level predictions from these models were compared to ratings provided by human coders using a large sample of MI sessions (N=341 sessions; 78,977 clinician and client talk turns) from 6 MI studies. Results show that the DSF model generally had slightly better performance compared to the RNN model. The DSF model had “good” or higher utterance-level agreement with human coders (Cohen's kappa>0.60) for open and closed questions, affirm, giving information, and follow/neutral (all therapist codes); considerably higher agreement was obtained for session-level indices, and many estimates were competitive with human-to-human agreement. However, there was poor agreement for client change talk, client sustain talk, and therapist MI-inconsistent behaviors. Natural language processing methods provide accurate representations of human derived behavioral codes and could offer substantial improvements to the efficiency and scale in which MI mechanisms of change research and fidelity monitoring are conducted."
}
@article{AHMADI20173,
title = "How can a recurrent neurodynamic predictive coding model cope with fluctuation in temporal patterns? Robotic experiments on imitative interaction",
journal = "Neural Networks",
volume = "92",
pages = "3 - 16",
year = "2017",
note = "Advances in Cognitive Engineering Using Neural Networks",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2017.02.015",
url = "http://www.sciencedirect.com/science/article/pii/S0893608017300618",
author = "Ahmadreza Ahmadi and Jun Tani",
keywords = "Neuro-robotics, Predictive coding, Recurrent neural networks, Synchronized imitation, Time-warping, Error regression",
abstract = "The current paper examines how a recurrent neural network (RNN) model using a dynamic predictive coding scheme can cope with fluctuations in temporal patterns through generalization in learning. The conjecture driving this present inquiry is that a RNN model with multiple timescales (MTRNN) learns by extracting patterns of change from observed temporal patterns, developing an internal dynamic structure such that variance in initial internal states account for modulations in corresponding observed patterns. We trained a MTRNN with low-dimensional temporal patterns, and assessed performance on an imitation task employing these patterns. Analysis reveals that imitating fluctuated patterns consists in inferring optimal internal states by error regression. The model was then tested through humanoid robotic experiments requiring imitative interaction with human subjects. Results show that spontaneous and lively interaction can be achieved as the model successfully copes with fluctuations naturally occurring in human movement patterns."
}
@article{GATES201780,
title = "Experimental fatigue crack growth behavior and predictions under multiaxial variable amplitude service loading histories",
journal = "Engineering Fracture Mechanics",
volume = "174",
pages = "80 - 103",
year = "2017",
note = "Special Issue on Multiaxial Fracture 2016",
issn = "0013-7944",
doi = "https://doi.org/10.1016/j.engfracmech.2016.11.023",
url = "http://www.sciencedirect.com/science/article/pii/S0013794416306403",
author = "Nicholas R. Gates and Ali Fatemi",
keywords = "Fatigue crack growth, Multiaxial, Variable amplitude, FASTRAN, UniGrow",
abstract = "This study presents experimental crack growth data for naturally initiated fatigue cracks in tubular specimens of 2024-T3 aluminum alloy subjected to both uniaxial and multiaxial variable amplitude flight loading spectra. Experimental crack growth behavior is compared to predictions based on two state-of-the-art analysis codes: UniGrow and FASTRAN. UniGrow is based on the idea that residual stress distributions surrounding the crack tip are responsible for causing load sequence effects in variable amplitude crack growth, while FASTRAN attributes these effects to varying degrees of plasticity induced closure in the crack wake. For variable amplitude fatigue tests performed under pure axial nominal loading conditions, both UniGrow and FASTRAN analyses were found to produce conservative growth life predictions despite good agreement with constant amplitude crack growth data. For variable amplitude torsion and combined axial-torsion crack growth analyses, however, the conservatism in growth life predictions was found to reduce. This is attributed to multiaxial nominal stress state effects, such as T-stress and mixed-mode crack growth, which are not accounted for in either UniGrow or FASTRAN, but were observed in constant amplitude crack growth tests to increase experimental crack growth rates. Additionally, by comparing experimental crack growth lives between tests performed using full and edited versions of the same loading history, it was found that a 94% reduction in loading history length resulted in differences in experimental crack growth life of less than a factor of 5. Since cracks in this study were initiated naturally, the effect of initial crack geometry assumptions on crack growth predictions was also investigated."
}
@article{CARDALL2017247,
title = "GenASiS   Basics: Object-oriented utilitarian functionality for large-scale physics simulations (Version 2)",
journal = "Computer Physics Communications",
volume = "214",
pages = "247 - 248",
year = "2017",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2016.12.019",
url = "http://www.sciencedirect.com/science/article/pii/S0010465517300097",
author = "Christian Y. Cardall and Reuben D. Budiardja",
keywords = "Simulation framework, Object-oriented programming, Fortran 2003",
abstract = "GenASiSBasics provides Fortran 2003 classes furnishing extensible object-oriented utilitarian functionality for large-scale physics simulations on distributed memory supercomputers. This functionality includes physical units and constants; display to the screen or standard output device; message passing; I/O to disk; and runtime parameter management and usage statistics. This revision–Version 2 of Basics–makes mostly minor additions to functionality and includes some simplifying name changes.
New version program summary
Program Title: SineWaveAdvection, SawtoothWaveAdvection, and RiemannProblem (fluid dynamics example problems illustrating GenASiSBasics); ArgonEquilibrium and ClusterFormation (molecular dynamics example problems illustrating GenASiSBasics) Program Files doi:http://dx.doi.org/10.17632/6w9ygpygmc.1 Licensing provisions: GPLv3 Programming language: Fortran 2003 (tested with gfortran 6.1.0, Intel Fortran 16.0.3, Cray Compiler 8.5.3) Journal reference of previous version: Computer Physics Communications, 196 (2015) 506 Does the new version supersede the previous version?: Yes Reasons for the new version: This version makes mostly minor additions to functionality and includes some simplifying name changes. Summary of revisions: Several additions to functionality are minor. Two new singleton objects are KIND_SMALL and KIND_TINY, for smaller sized numbers than those specified by the previously available KIND_DEFAULT. The class MeasuredValueForm can now handle some more complicated cases of unit string processing. The numerical values in the CONSTANT singleton have been updated to 2016 values [3], and CONSTANT and UNIT contain a few additional members. A new class TimerForm can be used to track the wall time occupied by various segments of code. The PROGRAM_HEADER singleton now contains an array member of this new class. With calls like the user can initialize their own timers; on return iMyTimer contains the index of the newly initialized timer. The calls and should surround the block of code to be timed. The information displayed by calling the ShowStatistics method of PROGRAM_HEADER includes data from all initialized timers, including one for overall execution time which is present by default. The code now expects to be compiled with OpenMP, typically by applying compiler flags. Strictly speaking this is only required for the PROGRAM_HEADER singleton, which queries the number of threads via a library call. In GenASiSBasics, OpenMP directives (which appear as comments as far as the Fortran 2003 standard is concerned) are only used in the Clear and Copy commands. There have been a number of name changes, mostly for simplification and consistency. These include the classes in ArrayArrays, where for example ArrayInteger_1D_Form is now simply Integer_1D_Form. Similar streamlining changes have been made to MessagePassing classes: IncomingMessageArrayRealForm is now MessageIncoming_1D_R_Form, for instance. The class VariableGroupArrayMetadata is now VariableGroup_1D_Form. The name ParametersStreamForm has been changed by one character (deletion of an s) to ParameterStreamForm. The member Selected of VariableGroupForm has been changed to iaSelected, where the prefix ia is a conventional prefix we use for an array of array indices. The interface and functionality of the SetGrid member of StructuredGridImageForm have been modified so as not to include boundary cells exterior to the computational domain, which prevented display of the computational domain in 3D plots with VisIt [4] unless a Box operator was applied. See the fluid dynamics examples for the modified usage. Finally, version 4.10 of the Silo library [2] introduced an include file named silo_f9x.inc, which the FileSystem classes of GenASiSBasics now expect to be available instead of silo.inc. Nature of problem: By way of illustrating GenASiSBasics functionality, solve example fluid dynamics and molecular dynamics problems. Solution method: For fluid dynamics examples, finite-volume. For molecular dynamics examples, leapfrog and velocity-Verlet integration. External routines/libraries: MPI [1] and Silo [2] Additional comments including Restrictions and Unusual features: The example problems named above are not ends in themselves, but serve to illustrate our object-oriented approach and the functionality available though GenASiSBasics. In addition to these more substantial examples, we provide individual unit test programs for each of the classes comprised by GenASiSBasics. GenASiSBasics is available in the CPC Program Library and also at https://github.com/GenASiS. [1]http://www.mcs.anl.gov/mpi/[2]https://wci.llnl.gov/simulation/computer-codes/silo[3]C. Patrignani et al. (Particle Data Group), Chin. Phys. C 40 (2016) 100001[4]https://wci.llnl.gov/simulation/computer-codes/visit"
}
@article{CHENG2014239,
title = "Assessment of ASSERT-PV for prediction of post-dryout heat transfer in CANDU bundles",
journal = "Nuclear Engineering and Design",
volume = "278",
pages = "239 - 248",
year = "2014",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2014.07.031",
url = "http://www.sciencedirect.com/science/article/pii/S002954931400435X",
author = "Z. Cheng and Y.F. Rao and G.M. Waddington",
abstract = "Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The recently released ASSERT-PV 3.2 provides enhanced models for improved predictions of subchannel flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against PDO tests performed during five full-size CANDU bundle experiments conducted between 1992 and 2009 by Stern Laboratories (SL), using 28-, 37- and 43-element bundles. A total of 10 PDO test series with varying pressure-tube creep and/or bearing-pad height were analyzed. The SL experiments encompassed the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for existing CANDU reactors. Code predictions of maximum PDO fuel-sheath temperature were compared against measurements from the SL PDO tests to quantify the code's prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. Furthermore, separate-effects sensitivity studies quantified the contribution of each PDO model change or enhancement to the improvement in PDO heat transfer prediction. Overall, the assessment demonstrated significant improvement in prediction of PDO sheath temperature in horizontal fuel channels containing CANDU bundles."
}
@article{STEPSYS20143062,
title = "HOTB: High precision parallel code for calculation of four-particle harmonic oscillator transformation brackets",
journal = "Computer Physics Communications",
volume = "185",
number = "11",
pages = "3062 - 3064",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2014.06.025",
url = "http://www.sciencedirect.com/science/article/pii/S0010465514002306",
author = "A. Stepšys and S. Mickevicius and D. Germanas and R.K. Kalinauskas",
keywords = "Mathematical methods in physics, Algebraic methods, Nuclear shell model",
abstract = "This new version of the HOTB program for calculation of the three and four particle harmonic oscillator transformation brackets provides some enhancements and corrections to the earlier version (Germanas et al., 2010) [1]. In particular, new version allows calculations of harmonic oscillator transformation brackets be performed in parallel using MPI parallel communication standard. Moreover, higher precision of intermediate calculations using GNU Quadruple Precision and arbitrary precision library FMLib [2] is done. A package of Fortran code is presented. Calculation time of large matrices can be significantly reduced using effective parallel code. Use of Higher Precision methods in intermediate calculations increases the stability of algorithms and extends the validity of used algorithms for larger input values.
New version program summary
Title of program: HOTB_MPI Catalogue identifier: AEFQ_v4_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEFQ_v4_0.html Program obtainable from: CPC Program Library, Queen’s University of Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 Number of lines in programs, including test data, etc.: 1711 Number of bytes in distributed programs, including test data, etc.: 11667 Distribution format: tar.gz Program language used: FORTRAN 90 with MPI extensions for parallelism Computer: Any computer with FORTRAN 90 compiler Operating system: Windows, Linux, FreeBSD, True64 Unix Has the code been vectorized of parallelized?: Yes, parallelism using MPI extensions. Number of CPUs used: up to 999 RAM(per CPU core): Depending on allocated binomial and trinomial matrices and use of precision; at least 500 MB Catalogue identifier of previous version: AEFQ_v1_0 Journal reference of previous version: Comput. Phys. Comm. 181, Issue 2, (2010) 420–425 Does the new version supersede the previous version? Yes Nature of problem: Calculation of matrices of three-particle harmonic oscillator brackets (3HOB) and four-particle harmonic oscillator brackets (4HOB) in a more effective way, which allows us to calculate matrix of the brackets up to a few hundred times more rapidly and more accurate than in a previous version. Solution method: Using external parallelization libraries and mutable precision we created a pack of numerical codes based on the methods of compact expressions of the three and four-particle harmonics oscillator brackets 3HOB, 4HOB, presented in [3]. Restrictions: For double precision version calculations can be done up to harmonic oscillator (HO) energy quanta e=28. For quadruple precision mantissa is equal to approximately 34 decimal digits, therefore calculations can be done up to HO energy quanta to e=52. Running time: The running time depends on the harmonic oscillator energy quanta, cluster size and the precision of intermediate calculations. More information on Table 1 for 3HOB and Table 2 for 4HOB. Reasons for a new version: The new program version expands the limits of harmonic oscillator energy quanta and gives shorter calculation time. Summary of revisions: 1.Additional features of new code HOTB_MPI: (a)Extend the limits of calculation of HOB First version was able to produce harmonic oscillator transformation brackets for three and four particles if E≤HO energy quanta. With this version of our program, if quadruple or arbitrary precision functions are being used, it is possible to calculate three and four particle harmonic oscillator transformation brackets for greater values of energy and momenta, while sustaining tolerable margin of error.(b)Calculation time As the code of previous version of program was redone using parallelism paradigma, it is now possible to reduce the calculation time of transformation matrices significantly, depending on the size of computing cluster, as the dimensions of matrices are growing very rapidly according to the energy and momenta values.2.Modifications or corrections to HOTB:New program HOTB_MPI is written in the FORTRAN 90 language, according to formulas described in [3]. In total there are six files: HOTB_mpi.f90, data_module.f90, matrix_tools.f90, dp_module.f90, qd_module.f90, and fm_module.f90. File HOTB_mpi.f90 contains main program for executing calculations. File data_module.f90 contains different kind of precision matrices for storage of binomial and trinomial values and also additional parameters.Detailed descriptions of parameters used by functions and subroutines are located in file README.txt.File matrix_tools.f90 contains functions needed to determine the dimensions of matrix, creation of state array and parallel calculation of desired matrix. (a)matrix_tools.f90i.subroutinematrix_4HOB_dimensionCalculates the dimension of 4HOB matrix.ii.subroutinematrix_3HOB_dimensionCalculates the dimension of 3HOB matrix,iii.subroutinematrix_3HOBCalculates the global state array which is used in parallel calculation of 3HOB matrix.iv.subroutinematrix_4HOBCalculates the global state array which is used in parallel calculation of 4HOB matrix.v.subroutinestate_array_3HOBCreates state array for 3HOB matrix output.vi.subroutinestate_array_4HOBCreates state array for 4HOB matrix output.vii.subroutinecalculate_3HOBPerforms parallel calculations of 3HOB matrix.viii.subroutinecalculate_4HOBPerforms parallel calculations of 4HOB matrix.*_module.f90 files contain modules for calculations respectively to the precision that is going to be used. We describe only functions of module dp_module.f90 as other modules are created replicating the structure of dp_module, except arbitrary precision module, which has several functions which will be described below.The naming convention for qd_module and fm_module is the same except that respectively qd_ and fm_ prefixes are added to function names. (a)dp_module.f90i.double precision functiondp_4HOBCalculates matrix element for 4HOB.ii.subroutinedp_binomFills the array of binomial coefficients.iii.subroutinedp_trinomFills the array of trinomial coefficients.iv.integer functiontriFunction for triangle condition testing.v.double precision functiondp_c6jFunction for 6-j coefficient calculation.vi.double precision functiondp_c9jFunction for 9-j coefficient calculationvii.double precision functiondp_kl0Function for Clebsch–Gordan coefficient with zero projection calculation.viii.double precision functiondp_gFunction for gamma element calculation.ix.double precision functiondp_3HOBCalculates three particle harmonic oscillator transformation bracket.Additional functions located in fm_module.f90, which are required for arbitrary precision calculation: (a)fm_module.f90i.type(fm) functionbinomasFunction for calculation of binomial value using FMLIB function Binomial.ii.type(fm) functionaccess_binomFunction for accessing triangular binomial matrix fm_bin.iii.type(fm) functioncheck_binFunction for checking if required binomial value is located in matrix fm_bin. If not, the value is calculated using FMLIB function Binomial.iv.subroutinewrite_binomFunction for writing calculated binomial value to triangular matrix fm_bin."
}
@article{BARAM2017182,
title = "Developmental metaplasticity in neural circuit codes of firing and structure",
journal = "Neural Networks",
volume = "85",
pages = "182 - 196",
year = "2017",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2016.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S0893608016301356",
author = "Yoram Baram",
keywords = "Developmental metaplasticity, Cortical plasticity, Neural circuit firing codes, Neural circuit connectivity codes, Neural firing time constants, Neural plasticity time constants",
abstract = "Firing-rate dynamics have been hypothesized to mediate inter-neural information transfer in the brain. While the Hebbian paradigm, relating learning and memory to firing activity, has put synaptic efficacy variation at the center of cortical plasticity, we suggest that the external expression of plasticity by changes in the firing-rate dynamics represents a more general notion of plasticity. Hypothesizing that time constants of plasticity and firing dynamics increase with age, and employing the filtering property of the neuron, we obtain the elementary code of global attractors associated with the firing-rate dynamics in each developmental stage. We define a neural circuit connectivity code as an indivisible set of circuit structures generated by membrane and synapse activation and silencing. Synchronous firing patterns under parameter uniformity, and asynchronous circuit firing are shown to be driven, respectively, by membrane and synapse silencing and reactivation, and maintained by the neuronal filtering property. Analytic, graphical and simulation representation of the discrete iteration maps and of the global attractor codes of neural firing rate are found to be consistent with previous empirical neurobiological findings, which have lacked, however, a specific correspondence between firing modes, time constants, circuit connectivity and cortical developmental stages."
}
@article{SALVES2017336,
title = "A method to localize faults in concurrent C programs",
journal = "Journal of Systems and Software",
volume = "132",
pages = "336 - 352",
year = "2017",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.03.010",
url = "http://www.sciencedirect.com/science/article/pii/S0164121217300614",
author = "Erickson H. da S. Alves and Lucas C. Cordeiro and Eddie B. de L. Filho",
keywords = "Concurrent software, Bounded model checking, Fault localization, Non-determinism, Sequentialization",
abstract = "We describe a new approach to localize faults in concurrent programs, which is based on bounded model checking and sequentialization techniques. The main novelty is the idea of reproducing a faulty behavior, in a sequential version of a concurrent program. In order to pinpoint faulty lines, we analyze counterexamples generated by a model checker, to the new instrumented sequential program, and search for a diagnostic value, which corresponds to actual lines in a program. This approach is useful to improve debugging processes for concurrent programs, since it tells which line should be corrected and what values lead to a successful execution. We implemented this approach as a code-to-code transformation from concurrent into non-deterministic sequential programs, which are used as inputs to existing verification tools. Experimental results show that our approach is effective and capable of identifying faults in our benchmark set, which was extracted from the SV-COMP 2016 suite."
}
@article{OXBURGH20141027,
title = "Dr TIM: Ray-tracer TIM, with additional specialist scientific capabilities",
journal = "Computer Physics Communications",
volume = "185",
number = "3",
pages = "1027 - 1037",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.10.031",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513003767",
author = "Stephen Oxburgh and Tomáš Tyc and Johannes Courtial",
keywords = "Ray tracing, Lorentz transform, Geometrical optics, Perfect imaging, METATOYs",
abstract = "We describe several extensions to TIM, a raytracing program for ray-optics research. These include relativistic raytracing; simulation of the external appearance of Eaton lenses, Luneburg lenses and generalised focusing gradient-index lens (GGRIN) lenses, which are types of perfect imaging devices; raytracing through interfaces between spaces with different optical metrics; and refraction with generalised confocal lenslet arrays, which are particularly versatile METATOYs.
Program summary
Program title: TIM Catalogue identifier: AEKY_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEKY_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licencing provisions: GNU General Public License No. of lines in distributed program, including test data, etc.: 106905 No. of bytes in distributed program, including test data, etc.: 6327715 Distribution format: tar.gz Programming language: Java. Computer: Any computer capable of running the Java Virtual Machine (JVM) 1.6. Operating system: Any, developed under Mac OS X Version 10.6 and 10.8.3. RAM: Typically 130 MB (interactive version running under Mac OS X Version 10.8.3) Classification: 14, 18. Catalogue identifier of previous version: AEKY_v1_0 Journal reference of previous version: Comput. Phys. Comm. 183(2012)711 External routines: JAMA [1] (source code included) Does the new version supersede the previous version?: Yes Nature of problem: Visualisation of scenes that include scene objects that create wave-optically forbidden light-ray fields. Solution method: Ray tracing. Reasons for new version: Significant extension of the capabilities (see Summary of revisions), as demanded by our research. Summary of revisions: Added capabilities include the simulation of different types of camera moving at relativistic speeds relative to the scene; visualisation of the external appearance of generalised focusing gradient-index (GGRIN) lenses, including Maxwell fisheye, Eaton and Luneburg lenses; calculation of refraction at the interface between spaces with different optical metrics; and handling of generalised confocal lenslet arrays (gCLAs), a new type of METATOY. Unusual features: Specifically designed to visualise wave-optically forbidden light-ray fields; can visualise ray trajectories and geometric optic transformations; can simulate photos taken with different types of camera moving at relativistic speeds, interfaces between spaces with different optical metrics, the view through METATOYs and generalised focusing gradient-index lenses; can create anaglyphs (for viewing with coloured “3D glasses”), HDMI-1.4a standard 3D images, and random-dot autostereograms of the scene; integrable into web pages. Running time: Problem-dependent; typically seconds for a simple scene. References: [1] JAMA: A Java Matrix Package, http://math.nist.gov/javanumerics/jama/"
}
@article{KALSOOM20171301,
title = "Inquiry into sustainability issues by preservice teachers: A pedagogy to enhance sustainability consciousness",
journal = "Journal of Cleaner Production",
volume = "164",
pages = "1301 - 1311",
year = "2017",
issn = "0959-6526",
doi = "https://doi.org/10.1016/j.jclepro.2017.07.047",
url = "http://www.sciencedirect.com/science/article/pii/S0959652617314750",
author = "Qudsia Kalsoom and Afifa Khanam",
keywords = "Education for sustainable development, Sustainability consciousness, ESD pedagogy, Inquiry based learning, Undergraduate research, Higher education",
abstract = "Education for sustainable development (ESD) is transformative education aiming at developing participants’ understanding of sustainability issues and transforming their attitudes and behaviours regarding environment, society and economy. Sustainability consciousness, an expected outcome of ESD, is a complex of cognitive and affective learning. Development of sustainability consciousness requires transformative learning experiences. The study presented in this paper employed action research to enhance sustainability consciousness of the preservice teachers through inquiry based learning. The study was done in the Institute of Education, Lahore College for Women University, Pakistan. The study integrated sustainability education in an existing course entitled ‘Research Methods in Education’. The course is included in the final year of B.Ed. (Honours) programme. Outcome of the ESD-integration was measured in terms of change in the sustainability consciousness of the preservice teachers. The Action Research project engaged 27 preservice teachers in inquiry-based learning (empirical investigations and research based discussions) for a period of 11 weeks. The participants investigated sustainability issues collaboratively. To investigate the change in participants’ sustainability consciousness through inquiry based learning, researchers collected through pre- and post-tests, interviews and observation. Quantitative data were analyzed through paired t-test while qualitative data through thematic coding. The data indicate that empirical investigations into sustainability issues by the presevice teachers and research-based discussions enhanced preservice teachers’ sustainability consciousness. This highlights the transformative potential of inquiry based learning. Moreover, it indicates that sustainability education can be successfully integrated in the course of ‘research methods in education’. The findings suggest that teacher education programmes and other university programmes may employ inquiry based learning as a vehicle to enhance sustainability consciousness of the undergraduate students."
}
@article{GAVARESHKI20125379,
title = "The Role of Education, Educational Processes, and Education Culture on the Development of Virtual Learning in Iran",
journal = "Procedia - Social and Behavioral Sciences",
volume = "46",
pages = "5379 - 5381",
year = "2012",
note = "4th WORLD CONFERENCE ON EDUCATIONAL SCIENCES (WCES-2012) 02-05 February 2012 Barcelona, Spain",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2012.06.442",
url = "http://www.sciencedirect.com/science/article/pii/S1877042812021787",
author = "Mojdeh Naderzadeh Gavareshki and F. Haddadian and Mc. HassanzadehKalleh",
keywords = "education culture, virtual learning, grounded theory",
abstract = "The aim of this study was to investigate the effect of education and education culture on the development of virtual learning in Iran. The qualitative method with a grounded theory approach was developed. Virtual students as well as their teachers teaching in Tehran, Iran were chosen as the population. The research instrument for the data collection was a thorough unstructured interview. Based on the systematic grounded approach, the data were encoded through three different coding systems: open coding (in two levels), axial coding, and selective coding and finally the grounded theory is explained. The results showed attention to the influence of technology as well as its entrance to the world of education; such a thing that leads to changes in current traditional teaching and finally the combination of virtual and traditional education."
}
@article{LUO201952,
title = "A novel algorithm for longitudinal track-bridge interactions considering loading history and using a verified mechanical model of fasteners",
journal = "Engineering Structures",
volume = "183",
pages = "52 - 68",
year = "2019",
issn = "0141-0296",
doi = "https://doi.org/10.1016/j.engstruct.2019.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S0141029618323708",
author = "Jun Luo and Zhiping Zeng",
keywords = "Track–bridge interaction, Loading history, Dahl friction model, Principle of minimum potential energy, Ritz method",
abstract = "The mechanical behavior of fasteners has considerable influence on track–bridge interaction (TBI) analysis, and a consensus has been gradually reached that their loading history and nonlinear characteristics should be taken into account. In this study, we present a new variation pattern of the longitudinal resistance of the fasteners considering loading history, which was experimentally verified based on the Dahl friction model. The model’s mechanical behavior was more consistent with measured results compared with previously proposed Ruge’s model and the double-spring model. According to the new model, an algorithm for TBI analysis considering loading history was derived based on the Ritz method and the principle of minimum potential energy, which was applied in a case study of an N-span, simply-supported girder bridge in a high-speed railway with a typical loading sequence: (1) the seasonal temperature change of the bridge; (2) the bending of the bridge structure under a vertical train load; (3) the braking of the train. Additionally, some significant conclusions were obtained by comparing the numerical results and adopting the mechanical parameters of the fasteners specified in various codes, according to both the linear superposition method (LSM) and loading history method (LHM)."
}
@article{CHEN20151038,
title = "A binary differential evolution algorithm learning from explored solutions",
journal = "Neurocomputing",
volume = "149",
pages = "1038 - 1047",
year = "2015",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2014.07.030",
url = "http://www.sciencedirect.com/science/article/pii/S0925231214009679",
author = "Yu Chen and Weicheng Xie and Xiufen Zou",
keywords = "Binary differential evolution algorithm, Convergence in probability, Renewal metric, Refinement metric",
abstract = "Although real-coded differential evolution (DE) algorithms can perform well on continuous optimization problems (CoOPs), designing an efficient binary-coded DE algorithm is still a challenging task. Inspired by the learning mechanism in particle swarm optimization (PSO) algorithms, we propose a binary learning differential evolution (BLDE) algorithm that can efficiently locate the global optimal solutions by learning from the last population. Then, we theoretically prove the global convergence of BLDE, and compare it with some existing binary-coded evolutionary algorithms (EAs) via numerical experiments. Numerical results show that BLDE is competitive with the compared EAs. Further study is performed via the change curves of a renewal metric and a refinement metric to investigate why BLDE cannot outperform some compared EAs for several selected benchmark problems. Finally, we employ BLDE in solving the unit commitment problem (UCP) in power systems to show its applicability to practical problems."
}
@article{HOSCHELE2014528,
title = "MT: A Mathematica package to compute convolutions",
journal = "Computer Physics Communications",
volume = "185",
number = "2",
pages = "528 - 539",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.10.007",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513003354",
author = "Maik Höschele and Jens Hoff and Alexey Pak and Matthias Steinhauser and Takahiro Ueda",
keywords = "Infra-red singularities, Mellin transform, Convolutions",
abstract = "We introduce the Mathematica package MT which can be used to compute, both analytically and numerically, convolutions involving harmonic polylogarithms, polynomials or generalized functions. As applications contributions to next-to-next-to-next-to leading order Higgs boson production and the Drell–Yan process are discussed.
Program summary
Title of program:MT Available from:http://www-ttp.physik.uni-karlsruhe.de/Progdata/ttp13/ttp13-27/ Computer for which the program is designed and others on which it is operable: Any computer where Mathematica version 6 or higher is running. Operating system or monitor under which the program has been tested: Linux No. of bytes in distributed program including test data etc.: approximately 50000 bytes, and tables of approximately 60 megabytes Distribution format: source code Keywords: Convolution of partonic cross sections and splitting functions, Mellin transformation, harmonic sums, harmonic polylogarithms, Higgs boson production, Drell–Yan process Nature of physical problem: For the treatment of collinear divergences connected to initial-state radiation it is necessary to consider convolutions of partonic cross sections with splitting functions. MT can be used to compute such convolutions. Method of solution: MT is implemented in Mathematica and we provide several functions in order to perform transformations to Mellin space, manipulations of the expressions, and inverse Mellin transformations. Restrictions on the complexity of the problem: In case the weight of the input quantities is too high the tables for the (inverse) Mellin transforms have to be extended. In the current implementation the tables contain expressions up to weight eight, code for the generation of tables of even higher weight is provided, too. MT can only handle convolutions of expressions involving harmonic polylogarithms, plus distributions and polynomials in the partonic variable x. Typical running time: In general the run time for the individual operations is at most of the order of a few minutes (depending on the speed and memory of the computer)."
}
@article{LINDQVIST2017141,
title = "The relationship between counselors' technical skills, clients' in-session verbal responses, and outcome in smoking cessation treatment",
journal = "Journal of Substance Abuse Treatment",
volume = "77",
pages = "141 - 149",
year = "2017",
issn = "0740-5472",
doi = "https://doi.org/10.1016/j.jsat.2017.02.004",
url = "http://www.sciencedirect.com/science/article/pii/S0740547216302070",
author = "Helena Lindqvist and Lars Forsberg and Pia Enebrink and Gerhard Andersson and Ingvar Rosendahl",
keywords = "Smoking cessation, Motivational Interviewing, Therapeutic process, Mediation, Change talk, Sustain talk",
abstract = "Background
The technical component of Motivational Interviewing (MI) posits that client language mediates the relationship between counselor techniques and subsequent client behavioral outcomes. The purpose of this study was to examine this hypothesized technical component of MI in smoking cessation treatment in more depth.
Method
Secondary analysis of 106 first treatment sessions, derived from the Swedish National Tobacco Quitline, and previously rated using the Motivational Interviewing Sequential Code for Observing Process Exchanges (MI-SCOPE) Coder's Manual and the Motivational Interviewing Treatment Integrity code (MITI) Manual, version 3.1. The outcome measure was self-reported 6-month continuous abstinence at 12-month follow-up.
Results
Sequential analyses indicated that clients were significantly more likely than expected by chance to argue for change (change talk) following MI-consistent behaviors and questions and reflections favoring change. Conversely, clients were more likely to argue against change (sustain talk) following questions and reflections favoring status-quo. Parallel mediation analysis revealed that a counselor technique (reflections of client sustain talk) had an indirect effect on smoking outcome at follow-up through client language mediators.
Conclusions
The study makes a significant contribution to our understanding of how MI works in smoking cessation treatment and adds further empirical support for the hypothesized technical component in MI. The results emphasize the importance of counselors avoiding unintentional reinforcement of sustain talk and underline the need for a greater emphasis on the direction of questions and reflections in MI trainings and fidelity measures."
}
@article{FORSYTH20181492,
title = "Machine Learning Methods to Extract Documentation of Breast Cancer Symptoms From Electronic Health Records",
journal = "Journal of Pain and Symptom Management",
volume = "55",
number = "6",
pages = "1492 - 1499",
year = "2018",
issn = "0885-3924",
doi = "https://doi.org/10.1016/j.jpainsymman.2018.02.016",
url = "http://www.sciencedirect.com/science/article/pii/S0885392418300824",
author = "Alexander W. Forsyth and Regina Barzilay and Kevin S. Hughes and Dickson Lui and Karl A. Lorenz and Andrea Enzinger and James A. Tulsky and Charlotta Lindvall",
keywords = "Machine learning, natural language processing, patient-reported symptoms, electronic health record, breast cancer, palliative care",
abstract = "Context
Clinicians document cancer patients' symptoms in free-text format within electronic health record visit notes. Although symptoms are critically important to quality of life and often herald clinical status changes, computational methods to assess the trajectory of symptoms over time are woefully underdeveloped.
Objectives
To create machine learning algorithms capable of extracting patient-reported symptoms from free-text electronic health record notes.
Methods
The data set included 103,564 sentences obtained from the electronic clinical notes of 2695 breast cancer patients receiving paclitaxel-containing chemotherapy at two academic cancer centers between May 1996 and May 2015. We manually annotated 10,000 sentences and trained a conditional random field model to predict words indicating an active symptom (positive label), absence of a symptom (negative label), or no symptom at all (neutral label). Sentences labeled by human coder were divided into training, validation, and test data sets. Final model performance was determined on 20% test data unused in model development or tuning.
Results
The final model achieved precision of 0.82, 0.86, and 0.99 and recall of 0.56, 0.69, and 1.00 for positive, negative, and neutral symptom labels, respectively. The most common positive symptoms were pain, fatigue, and nausea. Machine-based labeling of 103,564 sentences took two minutes.
Conclusion
We demonstrate the potential of machine learning to gather, track, and analyze symptoms experienced by cancer patients during chemotherapy. Although our initial model requires further optimization to improve the performance, further model building may yield machine learning methods suitable to be deployed in routine clinical care, quality improvement, and research applications."
}
@article{MIKKOLA201338,
title = "Implementation of an efficient logarithmic-Hamiltonian three-body code",
journal = "New Astronomy",
volume = "20",
pages = "38 - 41",
year = "2013",
issn = "1384-1076",
doi = "https://doi.org/10.1016/j.newast.2012.09.004",
url = "http://www.sciencedirect.com/science/article/pii/S1384107612000917",
author = "Seppo Mikkola and Kiyotaka Tanikawa",
keywords = "Three-body problem, Numerical integration",
abstract = "The numerical integration of the gravitational few-body problem using the logarithmic-Hamiltonian leapfrog algorithm has been found to produce highly accurate results, especially when combined with the extrapolation method. We describe in detail an implementation of the algorithm to the three-body problem. The considered algorithm has proved itself in numerous simulations and is believed to have led to one of the simplest well working codes for the problem. The advantage of the present code is its relative simplicity since no coordinate transformations are required and therefore adaptations to special purposes are straightforward."
}
@article{CAMPBELL2016143,
title = "Non-linearity issues and multiple ionization satellites in the PIXE portion of spectra from the Mars alpha particle X-ray spectrometer",
journal = "Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms",
volume = "383",
pages = "143 - 151",
year = "2016",
issn = "0168-583X",
doi = "https://doi.org/10.1016/j.nimb.2016.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S0168583X16302993",
author = "John L. Campbell and Christopher M. Heirwegh and Brianna Ganly",
keywords = "Mars Science Laboratory, Alpha particle X-ray spectrometer, XRF, PIXE, Non-linearity, Multiple ionization satellites",
abstract = "Spectra from the laboratory and flight versions of the Curiosity rover’s alpha particle X-ray spectrometer were fitted with an in-house version of GUPIX, revealing departures from linear behavior of the energy-channel relationships in the low X-ray energy region where alpha particle PIXE is the dominant excitation mechanism. The apparent energy shifts for the lightest elements present were attributed in part to multiple ionization satellites and in part to issues within the detector and/or the pulse processing chain. No specific issue was identified, but the second of these options was considered to be the more probable. Approximate corrections were derived and then applied within the GUAPX code which is designed specifically for quantitative evaluation of APXS spectra. The quality of fit was significantly improved. The peak areas of the light elements Na, Mg, Al and Si were changed by only a few percent in most spectra. The changes for elements with higher atomic number were generally smaller, with a few exceptions. Overall, the percentage peak area changes are much smaller than the overall uncertainties in derived concentrations, which are largely attributable to the effects of rock heterogeneity. The magnitude of the satellite contributions suggests the need to incorporate these routinely in accelerator-based PIXE using helium beams."
}
@article{KAVULURU2015155,
title = "An empirical evaluation of supervised learning approaches in assigning diagnosis codes to electronic medical records",
journal = "Artificial Intelligence in Medicine",
volume = "65",
number = "2",
pages = "155 - 166",
year = "2015",
note = "Intelligent healthcare informatics in big data era",
issn = "0933-3657",
doi = "https://doi.org/10.1016/j.artmed.2015.04.007",
url = "http://www.sciencedirect.com/science/article/pii/S0933365715000482",
author = "Ramakanth Kavuluru and Anthony Rios and Yuan Lu",
keywords = "Multi-label text classification, Learning to rank, Label calibration, Diagnosis code assignment",
abstract = "Background
Diagnosis codes are assigned to medical records in healthcare facilities by trained coders by reviewing all physician authored documents associated with a patient's visit. This is a necessary and complex task involving coders adhering to coding guidelines and coding all assignable codes. With the popularity of electronic medical records (EMRs), computational approaches to code assignment have been proposed in the recent years. However, most efforts have focused on single and often short clinical narratives, while realistic scenarios warrant full EMR level analysis for code assignment.
Objective
We evaluate supervised learning approaches to automatically assign international classification of diseases (ninth revision) – clinical modification (ICD-9-CM) codes to EMRs by experimenting with a large realistic EMR dataset. The overall goal is to identify methods that offer superior performance in this task when considering such datasets.
Methods
We use a dataset of 71,463 EMRs corresponding to in-patient visits with discharge date falling in a two year period (2011–2012) from the University of Kentucky (UKY) Medical Center. We curate a smaller subset of this dataset and also use a third gold standard dataset of radiology reports. We conduct experiments using different problem transformation approaches with feature and data selection components and employing suitable label calibration and ranking methods with novel features involving code co-occurrence frequencies and latent code associations.
Results
Over all codes with at least 50 training examples we obtain a micro F-score of 0.48. On the set of codes that occur at least in 1% of the two year dataset, we achieve a micro F-score of 0.54. For the smaller radiology report dataset, the classifier chaining approach yields best results. For the smaller subset of the UKY dataset, feature selection, data selection, and label calibration offer best performance.
Conclusions
We show that datasets at different scale (size of the EMRs, number of distinct codes) and with different characteristics warrant different learning approaches. For shorter narratives pertaining to a particular medical subdomain (e.g., radiology, pathology), classifier chaining is ideal given the codes are highly related with each other. For realistic in-patient full EMRs, feature and data selection methods offer high performance for smaller datasets. However, for large EMR datasets, we observe that the binary relevance approach with learning-to-rank based code reranking offers the best performance. Regardless of the training dataset size, for general EMRs, label calibration to select the optimal number of labels is an indispensable final step."
}
@article{COLGROVE2016215,
title = "History and genomic sequence analysis of the herpes simplex virus 1 KOS and KOS1.1 sub-strains",
journal = "Virology",
volume = "487",
pages = "215 - 221",
year = "2016",
issn = "0042-6822",
doi = "https://doi.org/10.1016/j.virol.2015.09.026",
url = "http://www.sciencedirect.com/science/article/pii/S0042682215004262",
author = "Robert C. Colgrove and Xueqiao Liu and Anthony Griffiths and Priya Raja and Neal A. Deluca and Ruchi M. Newman and Donald M. Coen and David M. Knipe",
keywords = "HSV history, HSV genome sequence, KOS sequence, KOS1.1sequence, Herpes simplex virus, HSV, KOS, KOS1.1",
abstract = "A collection of genomic DNA sequences of herpes simplex virus (HSV) strains has been defined and analyzed, and some information is available about genomic stability upon limited passage of viruses in culture. The nature of genomic change upon extensive laboratory passage remains to be determined. In this report we review the history of the HSV-1 KOS laboratory strain and the related KOS1.1 laboratory sub-strain, also called KOS (M), and determine the complete genomic sequence of an early passage stock of the KOS laboratory sub-strain and a laboratory stock of the KOS1.1 sub-strain. The genomes of the two sub-strains are highly similar with only five coding changes, 20 non-coding changes, and about twenty non-ORF sequence changes. The coding changes could potentially explain the KOS1.1 phenotypic properties of increased replication at high temperature and reduced neuroinvasiveness. The study also provides sequence markers to define the provenance of specific laboratory KOS virus stocks."
}
@article{DROBNY2018301,
title = "F-TRIDYN simulations of tungsten self-sputtering and applications to coupling plasma and material codes",
journal = "Computational Materials Science",
volume = "149",
pages = "301 - 306",
year = "2018",
issn = "0927-0256",
doi = "https://doi.org/10.1016/j.commatsci.2018.03.032",
url = "http://www.sciencedirect.com/science/article/pii/S092702561830185X",
author = "Jon Drobny and Davide Curreli",
keywords = "Binary Collision Approximation, Fractal, Surface roughness, Ion-solid interactions, Plasma Material Interactions, Code coupling",
abstract = "Fractal-TRIDYN (F-TRIDYN) is an upgraded version of the Monte Carlo, Binary Collision Approximation code TRIDYN for simulating ion-surface interactions. F-TRIDYN adds an explicit model of surface roughness and additional output modes for coupling to both plasma and material codes. Code-coupling represents a compelling path toward whole-device modeling, especially for future fusion reactors. Whole device models need to span length and time scales of many orders of magnitude. Atomic processes in materials that occur on the order of picoseconds, such as changes to surface morphology, will have an effect on fusion plasma performance over many hours of operational time. Conversely, interactions with the plasma will drive chemical, thermal, and morphological processes in the material. Simulating this complex interaction between the plasma and the plasma-facing material demands fast interfaces between material and plasma codes. F-TRIDYN is a flexible code for simulating atomic-scale ion-surface interactions, which are responsible for interactions between plasma and surface such as sputtering and implantation. F-TRIDYNs surface roughness model allows the effect of surface roughness on ion-surface interactions to be simulated. Surface roughness can significantly alter sputtering yields and other ion-surface interaction quantities. Understanding the role surface roughness plays in Plasma-Material Interactions will be crucial to modeling the performance of future fusion reactors such as ITER. F-TRIDYN is also suited for the simulation of a wider range of plasma-surface interactions where surface morphology may play a role, including those utilized for sputter-coating and plasma treating applications."
}
@article{SATO201226,
title = "Spatial imagery of novel places based on visual scene transformation",
journal = "Cognitive Systems Research",
volume = "14",
number = "1",
pages = "26 - 36",
year = "2012",
note = "Cognitive Systems Research: Special Issue on Modeling and Application of Cognitive Systems",
issn = "1389-0417",
doi = "https://doi.org/10.1016/j.cogsys.2010.12.010",
url = "http://www.sciencedirect.com/science/article/pii/S1389041711000052",
author = "Naoyuki Sato",
keywords = "Hippocampus, Object-place associative memory, Mental navigation, Spatial cognition",
abstract = "The hippocampus is known to maintain memories of object-place associations that can produce a scene expectation at a novel viewpoint. To implement such capabilities, the memorized distances and directions of an object from the viewer at a fixed location should be integrated with the imaginary displacement to the new viewpoint. However, neural dynamics of such scene expectation at the novel viewpoint have not been discussed. In this study, we propose a method of coding novel places based on visual scene transformation as a component of the object-place memory in the hippocampus. In this coding, a novel place is represented by a transformed version of a viewer’s scene with imaginary displacement. When the places of individual objects are stored with the coding in the hippocampus, the object’s displacement at the imaginary viewpoint can be evaluated through the comparison of a transformed viewer’s scene with the stored scene. Results of computer experiments demonstrated that the coding successfully produced scene expectation of a three object arrangement at a novel viewpoint. Such the scene expectation was retained even without similarities between the imaginary scene and the real scene at the location, where the imaginary scenes only functioned as indices to denote the topographical relationship between object locations. The results suggest that the hippocampus uses the place coding based on scene transformation and implements the spatial imagery of object-place associations from the novel viewpoint."
}
@article{FOTOUHI201517,
title = "mRPL: Boosting mobility in the Internet of Things",
journal = "Ad Hoc Networks",
volume = "26",
pages = "17 - 35",
year = "2015",
issn = "1570-8705",
doi = "https://doi.org/10.1016/j.adhoc.2014.10.009",
url = "http://www.sciencedirect.com/science/article/pii/S157087051400225X",
author = "Hossein Fotouhi and Daniel Moreira and Mário Alves",
keywords = "Wireless sensor networks, Internet of Things, Mobility, RPL, Hand-off, Test-bed",
abstract = "The 6loWPAN (the light version of IPv6) and RPL (routing protocol for low-power and lossy links) protocols have become de facto standards for the Internet of Things (IoT). In this paper, we show that the two native algorithms that handle changes in network topology – the Trickle and Neighbor Discovery algorithms – behave in a reactive fashion and thus are not prepared for the dynamics inherent to nodes mobility. Many emerging and upcoming IoT application scenarios are expected to impose real-time and reliable mobile data collection, which are not compatible with the long message latency, high packet loss and high overhead exhibited by the native RPL/6loWPAN protocols. To solve this problem, we integrate a proactive hand-off mechanism (dubbed smart-HOP) within RPL, which is very simple, effective and backward compatible with the standard protocol. We show that this add-on halves the packet loss and reduces the hand-off delay dramatically to one tenth of a second, upon nodes’ mobility, with a sub-percent overhead. The smart-HOP algorithm has been implemented and integrated in the Contiki 6LoWPAN/RPL stack (source-code available on-line mrpl: smart-hop within rpl, 2014) and validated through extensive simulation and experimentation."
}
@article{VURAL2016333,
title = "PSCAD modeling of a two-level space vector pulse width modulation algorithm for power electronics education",
journal = "Journal of Electrical Systems and Information Technology",
volume = "3",
number = "2",
pages = "333 - 347",
year = "2016",
issn = "2314-7172",
doi = "https://doi.org/10.1016/j.jesit.2016.01.005",
url = "http://www.sciencedirect.com/science/article/pii/S231471721630037X",
author = "Ahmet Mete Vural",
keywords = "Space vector pulse width modulation, Simulation model, Voltage source inverter, Power electronics, Graduate education",
abstract = "This paper presents the design details of a two-level space vector pulse width modulation algorithm in PSCAD that is able to generate pulses for three-phase two-level DC/AC converters with two different switching patterns. The presented FORTRAN code is generic and can be easily modified to meet many other kinds of space vector modulation strategies. The code is also editable for hardware programming. The new component is tested and verified by comparing its output as six gating signals with those of a similar component in MATLAB library. Moreover the component is used to generate digital signals for closed-loop control of STATCOM for reactive power compensation in PSCAD. This add-on can be an effective tool to give students better understanding of the space vector modulation algorithm for different control tasks in power electronics area, and can motivate them for learning."
}
@article{SONG201799,
title = "Semi-supervised manifold-embedded hashing with joint feature representation and classifier learning",
journal = "Pattern Recognition",
volume = "68",
pages = "99 - 110",
year = "2017",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2017.03.004",
url = "http://www.sciencedirect.com/science/article/pii/S0031320317301024",
author = "Tiecheng Song and Jianfei Cai and Tianqi Zhang and Chenqiang Gao and Fanman Meng and Qingbo Wu",
keywords = "Hashing, Manifold embedding, Locality sensitive hashing (LSH), Nearest neighbor search, Image retrieval",
abstract = "Recently, learning-based hashing methods which are designed to preserve the semantic information, have shown promising results for approximate nearest neighbor (ANN) search problems. However, most of these methods require a large number of labeled data which are difficult to access in many real applications. With very limited labeled data available, in this paper we propose a semi-supervised hashing method by integrating manifold embedding, feature representation and classifier learning into a joint framework. Specifically, a semi-supervised manifold embedding is explored to simultaneously optimize feature representation and classifier learning to make the learned binary codes optimal for classification. A two-stage hashing strategy is proposed to effectively address the corresponding optimization problem. At the first stage, an iterative algorithm is designed to obtain a relaxed solution. At the second stage, the hashing function is refined by introducing an orthogonal transformation to reduce the quantization error. Extensive experiments on three benchmark databases demonstrate the effectiveness of the proposed method in comparison with several state-of-the-art hashing methods."
}
@article{TROGDON20181304,
title = "Impact of introduction of the 9-valent human papillomavirus vaccine on vaccination coverage of youth in North Carolina",
journal = "Vaccine",
volume = "36",
number = "10",
pages = "1304 - 1309",
year = "2018",
issn = "0264-410X",
doi = "https://doi.org/10.1016/j.vaccine.2018.01.013",
url = "http://www.sciencedirect.com/science/article/pii/S0264410X18300379",
author = "Justin G. Trogdon and Paul Shafer and Brianna Lindsay and Tamera Coyne-Beasley",
keywords = "Human papillomavirus, Vaccination, Interrupted time series",
abstract = "Objectives
The objective of this study was to evaluate the impact of introduction of 9vHPV vaccine on HPV vaccination uptake (doses per capita) and initiation (≥1 doses), completion (≥3 doses) and compliance (≥3 doses within 12 months) by adolescents.
Methods
We used a retrospective cohort analysis using North Carolina Immunization Registry (NCIR) data from January 2008 through October 2016. The sample included Vaccines for Children eligible adolescents aged 9 to 17 years in 2016, for whom the NCIR contains complete vaccination history. We applied an interrupted time series design to measure associations between ZIP Code Tabulation Area (ZCTA)-level HPV vaccination outcomes over time with the introduction of 9vHPV in North Carolina (NC) in July 2015.
Results
Each outcome displayed a linear upward trend over time with large seasonal spikes near August of each year, corresponding to the time when adolescents often receive other vaccines required for school entry. After accounting for these underlying trends, introduction of 9vHPV was not associated with a change in publicly funded HPV vaccination rates in NC.
Conclusions
Our results indicate that 9vHPV substituted for 4vHPV in the first year after release in NC, but the release of 9vHPV was not associated with an overall change in HPV vaccination."
}
@article{PAN20192,
title = "Evaluation and utilization of CloudSat and CALIPSO data to analyze the impact of dust aerosol on the microphysical properties of cirrus over the Tibetan Plateau",
journal = "Advances in Space Research",
volume = "63",
number = "1",
pages = "2 - 15",
year = "2019",
issn = "0273-1177",
doi = "https://doi.org/10.1016/j.asr.2018.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S0273117718305490",
author = "Baiwan Pan and Zhendong Yao and Minzhong Wang and Honglin Pan and Lingbing Bu and K. Raghavendra Kumar and Haiyang Gao and Xingyou Huang",
keywords = "CALIPSO, CloudSat, Dust aerosol, Cirrus properties, Tibetan Plateau",
abstract = "The present study elucidates on the evaluation of two versions (V3 and V4.10) of vertical feature mask (VFM) and aerosol sub-types data derived from the Cloud-Aerosol LiDAR and Infrared Pathfinder Satellite Observations (CALIPSO), and its utilization to analyze the impact of dust aerosol on the microphysical properties of cirrus over the Tibetan Plateau (TP). In conjunction to the CALIPSO, we have also used the CloudSat data to study the same during the summer season for the years 2007–2010 over the study area 25–40°N and 75–100°E. Compared to V3 of CALIPSO, V4.10 was found to have undergone substantial changes in the code, algorithm, and data products. Intercomparison of both versions of data products in the selected grid between 30–31°N and 83–84°E within the study area during 2007–2017 revealed that the VFM and aerosol sub-types are in good agreement of ∼95.27% and ∼82.80%, respectively. Dusty cirrus is defined as the clouds mixed with dust aerosols or existing in dust aerosol conditions, while the pure cirrus is that in a dust-free environment. The obtained results illustrated that the various microphysical properties of cirrus, namely ice water content (IWC), ice water path (IWP), ice distribution width (IDW), ice effective radius (IER), and ice number concentration (INC) noticed a decrease of 17%, 18%, 4%, 19%, and 10%, respectively due to the existence of dust aerosol, consistent with the classical “Twomey effect” for liquid clouds. Moreover, the aerosol optical depth (AOD) showed moderate negative correlations between −0.4 and −0.6 with the microphysical characteristics of cirrus. As our future studies, in addition to the present work undertaken, we planned to gain knowledge and interested to explore the impact of a variety of aerosols apart from the dust aerosol on the microphysical properties of cirrus in different regions of China."
}
@article{RUPAKHETI2018366,
title = "On a pursuit for perfecting an undergraduate requirements engineering course",
journal = "Journal of Systems and Software",
volume = "144",
pages = "366 - 381",
year = "2018",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2018.07.008",
url = "http://www.sciencedirect.com/science/article/pii/S0164121218301390",
author = "Chandan R. Rupakheti and Mark Hays and Sriram Mohan and Stephen Chenoweth and Amanda Stouder",
keywords = "Requirements engineering, Project-Based learning, Course evolution",
abstract = "Requirements Engineering (RE) is an essential component of any software development cycle. Understanding and satisfying stakeholder needs and wants is the difference between the success and failure of a product. However, RE is often perceived as a “soft” skill by students and is often ignored by students who prioritize the learning of coding, testing, and algorithmic thinking. This view contrasts with the industry, where “soft” skills are instead valued equal to any other engineering ability. A key challenge in teaching RE is that students who are accustomed to technical work have a hard time relating to something that is non-technical. Furthermore, students are rarely afforded the opportunity to practice requirements elicitation and management skills in a meaningful way while learning the RE concepts as an adjunct to other content. At Rose-Hulman, several project-based approaches have been experimented with in teaching RE, and these have evolved over time. In this paper, the progress of teaching methodologies is documented to capture the pros and cons of these varied approaches, and to reflect on what worked and what did not in teaching RE to undergraduate engineering students."
}
@article{SETHY2015257,
title = "Real Time Strategy Games: A Reinforcement Learning Approach",
journal = "Procedia Computer Science",
volume = "54",
pages = "257 - 264",
year = "2015",
note = "Eleventh International Conference on Communication Networks, ICCN 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Data Mining and Warehousing, ICDMW 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Image and Signal Processing, ICISP 2015, August 21-23, 2015, Bangalore, India",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.06.030",
url = "http://www.sciencedirect.com/science/article/pii/S187705091501354X",
author = "Harshit Sethy and Amit Patel and Vineet Padmanabhan",
keywords = "Machine learning, Q learning, Reinforcement learning, Real time strategy, Reward functions.",
abstract = "In this paper we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method we use Q-learning1 and SARSA1 algorithms with generalised reward function to train the reinforcement learning agent. We evaluated the performance of our proposed algorithms on Real Time Strategy (RTS) game called BattleCity. There are two main advantages of having such an approach as compared to other works in RTS. (1) We can ignore the concept of a simulator which is often game specific and is usually hard coded in any type of RTS games (2) our system can learn from interaction with any opponents and quickly change the strategy according to the opponents and do not need any human traces as used in previous works."
}
@article{RAO201469,
title = "ASSERT-PV 3.2: Advanced subchannel thermalhydraulics code for CANDU fuel bundles",
journal = "Nuclear Engineering and Design",
volume = "275",
pages = "69 - 79",
year = "2014",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2014.04.016",
url = "http://www.sciencedirect.com/science/article/pii/S0029549314002283",
author = "Y.F. Rao and Z. Cheng and G.M. Waddington and A. Nava-Dominguez",
keywords = "ASSERT-PV, Subchannel analysis, CANDU bundles, Thermal hydraulics, Code development, Numerical modeling",
abstract = "Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The most recent release version, ASSERT-PV 3.2 has enhanced phenomenon models for improved predictions of flow distribution, dryout power and CHF location, and post-dryout (PDO) sheath temperature in horizontal CANDU fuel bundles. The focus of the improvements is mainly on modeling considerations for the unique features of CANDU bundles such as horizontal flows, small pitch to diameter ratios, high mass fluxes, and mixed and irregular subchannel geometries, compared to PWR/BWR fuel assemblies. This paper provides a general introduction to ASSERT-PV 3.2, and describes the model changes or additions in the new version to improve predictions of flow distribution, dryout power and CHF location, and PDO sheath temperatures in CANDU fuel bundles."
}
@article{NOWACK2014183,
title = "Continuous validation of ASTEC containment models and regression testing",
journal = "Nuclear Engineering and Design",
volume = "272",
pages = "183 - 194",
year = "2014",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2013.06.038",
url = "http://www.sciencedirect.com/science/article/pii/S0029549313005608",
author = "Holger Nowack and Nils Reinke and Martin Sonnenkalb",
abstract = "The focus of the ASTEC (Accident Source Term Evaluation Code) development at GRS is primarily on the containment module CPA (Containment Part of ASTEC), whose modelling is to a large extent based on the GRS containment code COCOSYS (COntainment COde SYStem). Validation is usually understood as the approval of the modelling capabilities by calculations of appropriate experiments done by external users different from the code developers. During the development process of ASTEC CPA, bugs and unintended side effects may occur, which leads to changes in the results of the initially conducted validation. Due to the involvement of a considerable number of developers in the coding of ASTEC modules, validation of the code alone, even if executed repeatedly, is not sufficient. Therefore, a regression testing procedure has been implemented in order to ensure that the initially obtained validation results are still valid with succeeding code versions. Within the regression testing procedure, calculations of experiments and plant sequences are performed with the same input deck but applying two different code versions. For every test-case the up-to-date code version is compared to the preceding one on the basis of physical parameters deemed to be characteristic for the test-case under consideration. In the case of post-calculations of experiments also a comparison to experimental data is carried out. Three validation cases from the regression testing procedure are presented within this paper. The very good post-calculation of the HDR E11.1 experiment shows the high quality modelling of thermal-hydraulics in ASTEC CPA. Aerosol behaviour is validated on the BMC VANAM M3 experiment, and the results show also a very good agreement with experimental data. Finally, iodine behaviour is checked in the validation test-case of the THAI IOD-11 experiment. Within this test-case, the comparison of the ASTEC versions V2.0r1 and V2.0r2 shows how an error was detected by the regression testing procedure and why the regression testing is an important part of the validation process. The corrected version V2.0r2 delivers a very good validation result for the iodine behaviour in the post-calculation of the THAI IOD-11 experiment."
}
@article{ABRAHAMSON2017380,
title = "Pulsed laser annealing of carbon black",
journal = "Carbon",
volume = "124",
pages = "380 - 390",
year = "2017",
issn = "0008-6223",
doi = "https://doi.org/10.1016/j.carbon.2017.08.080",
url = "http://www.sciencedirect.com/science/article/pii/S0008622317308758",
author = "Joseph P. Abrahamson and Madhu Singh and Jonathan P. Mathews and Randy L. Vander Wal",
abstract = "Laser heating was used to study the rates and trajectories of carbon black during the earliest stages of annealing. A commercial carbon black, Regal 250 (R250 Cabot Corporation) was heated with a Q-switched Nd:YAG laser and a continuous wave CO2 laser. Structural transformations were observed with transmission electron microscopy. Micrographs were processed with in-house codes for the purpose of extracting distributions of fringe length, tortuosity (curvature), and number of lamellae per stack. Time-temperature-histories with nanosecond temporal resolution and temperature reproducibility within tens of degrees Celsius were determined by spectrally resolving the laser induced incandescence signal and applying multi-wavelength pyrometry. The Nd:YAG laser fluences include: 25, 50, 100, 200, 300, and 550 mJ/cm2. The maximum observed temperature ranged from 2400 °C to the C2 sublimation temperature of 4180 °C. The CO2 laser was used to collect a series of isothermal (2600 °C) heat treatments versus time (100 ms–20 s). Laser heated samples are compared against R250 annealed in a furnace at 2600 °C. The material transformation trajectory of Nd:YAG laser heated R250 was different than the traditional furnace heating. The traditional furnace annealing pathway is followed for CO2 laser heating as based upon equivalent end structures."
}