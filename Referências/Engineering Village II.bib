@inproceedings{20161402189534,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Running ATLAS workloads within massively parallel distributed applications using Athena Multi-Process framework (AthenaMP)},
journal = {Journal of Physics: Conference Series},
author = {Calafiura, Paolo and Leggett, Charles and Seuster, Rolf and Tsulaia, Vakhtang and Van Gemmeren, Peter},
volume = {664},
number = {7},
year = {2015},
issn = {17426588},
address = {Okinawa, Japan},
abstract = {AthenaMP is a multi-process version of the ATLAS reconstruction, simulation and data analysis framework Athena. By leveraging Linux fork and copy-on-write mechanisms, it allows for sharing of memory pages between event processors running on the same compute node with little to no change in the application code. Originally targeted to optimize the memory footprint of reconstruction jobs, AthenaMP has demonstrated that it can reduce the memory usage of certain configurations of ATLAS production jobs by a factor of 2. AthenaMP has also evolved to become the parallel event-processing core of the recently developed ATLAS infrastructure for fine-grained event processing (Event Service) which allows the running of AthenaMP inside massively parallel distributed applications on hundreds of compute nodes simultaneously. We present the architecture of AthenaMP, various strategies implemented by AthenaMP for scheduling workload to worker processes (for example: Shared Event Queue and Shared Distributor of Event Tokens) and the usage of AthenaMP in the diversity of ATLAS event processing workloads on various computing resources: Grid, opportunistic resources and HPC.<br/> &copy; Published under licence by IOP Publishing Ltd.},
key = {High energy physics},
keywords = {Computer operating systems;},
note = {Analysis frameworks;Application codes;Computing resource;Event Processing;Event processors;Massively parallels;Memory footprint;Multi-Processes;},
URL = {http://dx.doi.org/10.1088/1742-6596/664/7/072050},
} 


@inproceedings{20172603848239,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Implementation of dynamic matrix control algorithm using field programmable gate array: Preliminary results},
journal = {Advances in Intelligent Systems and Computing},
author = {Wojtulewicz, Andrzej},
volume = {577},
year = {2017},
pages = {325 - 334},
issn = {21945357},
address = {Krakow, Poland},
abstract = {This paper describes implementation of the Dynamic Matrix Control (DMC) algorithm performed on an Altera Field Programmable Gate Array (FPGA) with the Cyclone IV chip. The DMC algorithm is implemented in its analytical (explicit) version which requires computationally simple matrix and vector operations in real time, no on-line optimisation is necessary. The test-bench application is prepared for fast comparison between C and HDL versions of code. A large number of independent logic cells can provide multi-parallel operations to achieve very fast operations. As a result, the algorithm may be used for controlling very fast dynamic processes characterised by sampling periods of millisecond order. Preliminary results of real experiments are demonstrated. The discussed control structure provides possibility to fast change of algorithm.<br/> &copy; Springer International Publishing AG 2017.},
key = {Field programmable gate arrays (FPGA)},
keywords = {C (programming language);Logic gates;Model predictive control;},
note = {Control structure;Dynamic matrix control;Dynamic matrix control algorithms;Fast operation;Multi-parallel operations;Optimisations;Sampling period;Vector operations;},
URL = {http://dx.doi.org/10.1007/978-3-319-60699-6_31},
} 


@inproceedings{20163502744709,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Connecting hardware and software in a middle school engineering outreach effort-RTP},
journal = {ASEE Annual Conference and Exposition, Conference Proceedings},
author = {Salzman, Noah and Loo, Sin Ming},
volume = {2016-June},
year = {2016},
issn = {21535965},
address = {New Orleans, LA, United states},
abstract = {Recent years have seen tremendous growth in outreach programs aimed at bringing computer programming to children and young adults via in-class and extracurricular coding activities. Programs such as the Hour of Code and Girls who Code have introduced millions of young people to programming around the world. For this study, we explored how combining programming with interactive electronics hardware can create a more engaging and dynamic learning environment for some students than what programming alone can achieve. In this paper, we describe an electrical engineering outreach effort in collaboration with the technology and engineering teacher at a local middle school. Beginning with an introduction to programming via the Hour of Code, we progressed to lessons utilizing the Sparkfun Electronics Digital Sandbox, an Arduino-compatible microcontroller board with numerous built-in sensors and outputs. Under the guidance of both a Professor of electrical and computer engineering and their own technology teacher, the students learned about the relationship between electronics hardware and software via a series of hands-on activities that culminated in a final design project. To understand the experiences of the students who participated in these activities and develop insights into the relationship between hardware and software and students' learning outcomes, we administered a survey and conducted a focus group with the students. The students described an overall positive experience, and also appreciated the ability to connect coding with the interactivity provided by the microcontroller board. The students described deriving significant satisfaction out of relatively simple tasks like programming an LED light to blink or change color. The students also overwhelmingly felt that learning about the interconnections between hardware and software gave them an understanding and better appreciation of the complexity of the electronics and computer software they interact with on a daily basis. The students generally found the programming to be the most challenging part of the activity but also rewarding, but tended to indicate activities utilizing hardware as the most engaging activity they encountered. Overall, the results of this study suggest that combined hardware and software educational activities can engage a wide number of students, help students understand the interconnectedness of these areas, and create a positive learning environment. &copy; American Society for Engineering Education, 2016.},
} 


@article{20154101348989,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A guide to A/B testing tools},
journal = {EContent},
author = {Kompella, Kashyap},
volume = {38},
number = {7},
year = {2015},
issn = {15252531},
abstract = {A/B testing can help resolve many of their quandaries related to online marketing. A/B testing is a process in which we choose the best performing version of a webpage, by randomly displaying different versions of the site to visitors and assessing the performance of each variant against a desired metric. Visual editor is used to create the variants without having to write HTML/CSS code. Multivariate testing is useful when we want to change several page elements. Multipage testing is used to test changes to a multistep shopping process in an e-commerce scenario. Segmentation is helpful if we want to run tests on certain visitors or understand if variants perform differently for different visitor segments. If the tool is easy to use, nontechnical users can run the tests on an ongoing basis without much support from the IT department. The tools use either client-side or server-side scripts to serve the appropriate variation and track performance. If A/B testing tools can integrate with such underlying systems, we cannot only conduct more fine-grained A/B tests but also can deliver more personalized web experiences to our site visitors.<br/>},
key = {Electronic commerce},
keywords = {Information analysis;Information systems;},
note = {Client sides;Fine grained;Non-technical users;Online marketing;Server sides;Underlying systems;Visual editors;Web experiences;},
} 


@article{20184606059038,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mock objects for testing java systems: Why and how developers use them, and how they evolve},
journal = {Empirical Software Engineering},
author = {Spadini, Davide and Aniche, Mauricio and Bruntink, Magiel and Bacchelli, Alberto},
year = {2018},
issn = {13823256},
abstract = {When testing software artifacts that have several dependencies, one has the possibility of either instantiating these dependencies or using mock objects to simulate the dependencies&rsquo; expected behavior. Even though recent quantitative studies showed that mock objects are widely used both in open source and proprietary projects, scientific knowledge is still lacking on how and why practitioners use mocks. An empirical understanding of the situations where developers have (and have not) been applying mocks, as well as the impact of such decisions in terms of coupling and software evolution can be used to help practitioners adapt and improve their future usage. To this aim, we study the usage of mock objects in three OSS projects and one industrial system. More specifically, we manually analyze more than 2,000 mock usages. We then discuss our findings with developers from these systems, and identify practices, rationales, and challenges. These results are supported by a structured survey with more than 100 professionals. Finally, we manually analyze how the usage of mock objects in test code evolve over time as well as the impact of their usage on the coupling between test and production code. Our study reveals that the usage of mocks is highly dependent on the responsibility and the architectural concern of the class. Developers report to frequently mock dependencies that make testing difficult (e.g., infrastructure-related dependencies) and to not mock classes that encapsulate domain concepts/rules of the system. Among the key challenges, developers report that maintaining the behavior of the mock compatible with the behavior of original class is hard and that mocking increases the coupling between the test and the production code. Their perceptions are confirmed by our data, as we observed that mocks mostly exist since the very first version of the test class, and that they tend to stay there for its whole lifetime, and that changes in production code often force the test code to also change.<br/> &copy; 2018, The Author(s).},
key = {Software testing},
keywords = {Codes (symbols);Computer aided software engineering;Java programming language;Open source software;},
note = {Empirical Software Engineering;Industrial systems;Mocking practices;Mockito;Quantitative study;Scientific knowledge;Software Evolution;Testing software;},
URL = {http://dx.doi.org/10.1007/s10664-018-9663-0},
} 


@inproceedings{20144800258275,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A component buildup method for canard controlled projectiles},
journal = {Proceedings - 27th International Symposium on Ballistics, BALLISTICS 2013},
author = {Sigal, Asher},
volume = {1},
year = {2013},
pages = {215 - 224},
address = {Freiburg, Germany},
abstract = {An engineering-level method has been applied for the estimation of the normalforce and the pitching-moment coefficients induced by canard controls (or wings) on body sections that feature change of diameter. These loads are missing in most component buildup cods. Adding them produces a more complete component buildup analysis. The validation of the procedure was done by calculating the longitudinal aerodynamic characteristics of three canard-projectile configurations, using the 1997 version of the USAF Missile Datcom code. The results show that the predicted normal-force coefficient of the canard unit is larger than test data. When the induced loads, calculated by the present method, are added as corrections to the results of the code, the agreement of the corrected predictions with test data greatly improves for all benchmarks.<br/>},
key = {Projectiles},
keywords = {Aerodynamic configurations;},
note = {Feature changes;Level method;Longitudinal aerodynamic characteristics;Normal force coefficients;On-body;Pitching moment coefficients;Test data;Usaf missile datcom;},
} 


@article{20183005610004,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Corrigendum to Implementation of tidal turbines in MIKE 3 and Delft3D models of Pentland Firth  Orkney waters [Ocean Coast. Manag. 147 (2017) 2136](S0964569117304192)(10.1016/j.ocecoaman.2017.04.015))},
journal = {Ocean and Coastal Management},
author = {Waldman, S. and Baston, S. and Nemalidinne, R. and Chatzirodou, A. and Venugopal, V. and Side, J.},
volume = {163},
year = {2018},
pages = {535 - 536},
issn = {09645691},
abstract = {The authors regret that a software error caused incorrect predictions for the effects of tidal turbines in Delft3D. The predictions without turbines are unaffected, as are those from the MIKE 3 model. The overall conclusions of the article remain valid. Figs. 12&ndash;15 as published are incorrect. Replacements for Figs. 12&ndash;14 are presented here. Following this correction the differences in the effects of energy extraction between the two models are much smaller. As a result the discussion of these differences in Section 6 should be disregarded, and Fig. 15 is no longer required. The authors would like to apologise for any inconvenience caused. The version of the code for adding turbines to Delft3D that is publicly available has been corrected, and anybody using this for their own work is urged to download the latest version. [Figure presented] Fig. 12: (a) 400 turbines in the Inner Sound, viewed through the MIKE Zero GUI; (b) The same 400 turbines represented as porous plates for Delft3D. Higher values of the c<inf>loss</inf> parameter, shown by bluer colours, indicate plates with higher drag. [Figure presented] Fig. 13: Changes in mean current speeds over 28 days as a result of adding turbines. [Figure presented] Fig. 14: Change in mean bed stress magnitude over 28 days as a result of adding turbines, expressed as a proportion of the value without turbines.<br/> &copy; 2017 Elsevier Ltd},
URL = {http://dx.doi.org/10.1016/j.ocecoaman.2018.07.014},
} 


@article{20182405308023,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Improved mixed oxide fuel calculations with the evaluated nuclear data library JEFF-3.2},
journal = {Nuclear Science and Engineering},
author = {Noguere, G. and Bernard, D. and Blaise, P. and Bouland, O. and Leal, L. and Leconte, P. and Litaize, O. and Peneliau, Y. and Roque, B. and Santamarina, A. and Vidal, J.-F.},
volume = {182},
number = {2},
year = {2016},
pages = {135 - 150},
issn = {00295639},
abstract = {An overestimation of the keff values for mixed oxide (MOX) fuels was identified with Monte Carlo (TRIPOLI-4) and deterministic (APOLLO2) calculations based on the Joint Evaluated Fission and Fusion (JEFF) evaluated nuclear data library. The overestimation becomes sizeable with Pu aging, reaching a reactivity change of &Delta;&rho;&sime;+700 pcm for integral measurements carried out with MOX fuel containing a large amount of americium. This bias was observed for various critical configurations performed in the zeropower reactor EOLE of the Commissariat &agrave; l&rsquo;&eacute;nergie atomique et aux &egrave;nergies alternatives (CEA), Cadarache, France. The present work focuses on the improvements achieved with the new 239Pu and 241Am evaluated nuclear data files available in the latest version of the JEFF library (JEFF-3.2). The resolved resonance range of the plutonium evaluation was reevaluated at Oak Ridge National Laboratory (ORNL), Oak Ridge, Tennessee, with the SAMMY code in collaboration with CEA Cadarache. The resonance parameters of the americium evaluation were obtained with the REFIT code in collaboration with the research institutes Institute for Reference Materials and Measurements (IRMM), Geel, Belgium, and Institut de recherche sur les lois fondamentales de l&rsquo;Univers (Irfu), Saclay, France.<br/> &copy; 2018, American Nuclear Society. All rights reserved.},
key = {Mixed oxide fuels},
keywords = {Americium;Fuels;Libraries;},
note = {Critical configurations;EOLE;Evaluated nuclear data file;Integral measurements;MINERVE;Oak ridge national laboratories;Resonance parameters;TRIPOLI-4;},
URL = {http://dx.doi.org/10.13182/NSE15-9},
} 


@inproceedings{20124115538487,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Information Security - 15th International Conference, ISC 2012, Proceedings},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {7483 LNCS},
year = {2012},
issn = {03029743},
address = {Passau, Germany},
abstract = {The proceedings contain 24 papers. The topics discussed include: differential attacks on reduced RIPEMD-160; on optimal bounds of small inverse problems and approximate GCD problems with higher degree; domain-specific pseudonymous signatures for the German identity card; solutions for the storage problem of McEliece public and private keys on memory-constrained platforms; 100% connectivity for location aware code based KPD in clustered WSN: merging blocks; learning fine-grained structured input for memory corruption detection; dynamic anomaly detection for more trustworthy outsourced computation; an empirical study of dangerous behaviors in Firefox extensions; collaboration-preserving authenticated encryption for operational transformation systems; additively homomorphic encryption with a double decryption mechanism, revisited; and compliance checking for usage-constrained credentials in trust negotiation systems.},
} 


@inproceedings{20170303258529,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
journal = {Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
volume = {2016-December},
year = {2016},
pages = {ASE Group; et al.; Huawei; IBM; Innergie - Power your Life; Oracle - },
issn = {10724451},
address = {Taipei, Taiwan},
abstract = {The proceedings contain 61 papers. The topics discussed include: dictionary sharing: an efficient cache compression scheme for compressed caches; pTask: a smart prefetching scheme for OS intensive applications; data-centric execution of speculative parallel programs; towards efficient server architecture for virtualized network function deployment: implications and implementations; bridging the I/O performance gap for big data workloads: a new NVDIMM-based approach; MIMD synchronization on SIMT architectures; KLAP: kernel launch aggregation and promotion for optimizing dynamic parallelism; cache-emulated register file: an integrated on-chip memory architecture for high performance GPGPUs; Zorua: a holistic approach to resource virtualization in GPUs; Cambricon-X: an accelerator for sparse neural networks; NEUTRAMS: neural network transformation and co-design under neuromorphic hardware constraints; continuous shape shifting: enabling loop co-optimization via near-free dynamic code rewriting; CrystalBall: statically analyzing runtime behavior via deep sequence learning; low-cost soft error resilience with unified data verification and fine-grained recovery for acoustic sensor based detection; improving energy efficiency of DRAM by exploiting half page row access; concise loads and stores: the case for an asymmetric compute-memory architecture for approximation; and the bunker cache for spatio-value approximation.},
} 


@article{20174304291119,
language = {Spanish},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ParaTrough: Modelica-based Simulation Library for Solar Thermal Plants},
title = {ParaTrough v1.0: Libreria en Modelica para Simulacion de Plantas Termosolares},
journal = {RIAI - Revista Iberoamericana de Automatica e Informatica Industrial},
author = {Cabrerizo, Juan A. Romera and Santos, Matilde},
volume = {14},
number = {4},
year = {2017},
pages = {412 - 423},
issn = {16977912},
abstract = {This paper describes a Modelica-based library developed to the modeling and simulation of solar thermal plants with parabolic trough collectors. The Dymola 6.1 environment has been used. Unlike other commercial tools, the ParaTrough library is offered as a free open source tool, under Modelica License 2. Its modular code makes it easily extensible and modifiable to the requirements of each plant and process in particular. In its current version 1.0, this library can be used for modeling and simulating the solar resource and the heat transfer fluid without phase change. The models have been validated with real data of an operating plant. ParaTrough can be freely used by process analysts for one or more of the following cases: performance assessment, fault detection, exploring new operation modes and plant optimization. While other elements can be added in future extensions, this contribution covers a new specific application area of Modelica and in its current state it facilitates the operation and maintenance of parabolic trough power plants.<br/> &copy; 2016 CEA.},
key = {Solar heating},
keywords = {Fault detection;},
note = {Modelica;Modeling and simulating;Operation and maintenance;Palabras clave Modelado;Parabolic trough collectors;Parabolic trough power plants;Performance assessment;planta termosolar;},
URL = {http://dx.doi.org/10.1016/j.riai.2017.06.005},
} 


@article{20142317783293,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Low-disruptive dynamic updating of Java applications},
journal = {Information and Software Technology},
author = {Gu, Tianxiao and Cao, Chun and Xu, Chang and Ma, Xiaoxing and Zhang, Linghao and Lu, Jian},
volume = {56},
number = {9},
year = {2014},
pages = {1086 - 1098},
issn = {09505849},
abstract = {Context In-use software systems are destined to change in order to fix bugs or add new features. Shutting down a running system before updating it is a normal practice, but the service unavailability can be annoying and sometimes unacceptable. Dynamic software updating (DSU) migrates a running software system to a new version without stopping it. State-of-the-art Java DSU systems are unsatisfactory as they may cause a non-negligible system pause during updating. Objective In this paper we present Javelus, a Java HotSpot VM-based Java DSU system with very short pausing time. Method Instead of updating everything at once when the running application is suspended, Javelus only updates the changed code during the suspension, and migrates stale objects on-demand after the application is resumed. With a careful design this lazy approach neither sacrifices the update flexibility nor introduces unnecessary object validity checks or access indirections. Results Evaluation experiments show that Javelus can reduce the updating pausing time by one to two orders of magnitude without introducing observable overheads before and after the dynamic updating. Conclusion Our experience with Javelus indicates that low-disruptive and type-safe dynamic updating of Java applications can be practically achieved with a lazy updating approach. &copy; 2014 Elsevier B.V. All rights reserved.<br/>},
key = {Program debugging},
keywords = {Computer software;Java programming language;},
note = {Dynamic software updating;Evaluation experiments;Java applications;Lazy updating;Low disruption;Orders of magnitude;Running applications;Updating approaches;},
URL = {http://dx.doi.org/10.1016/j.infsof.2014.04.003},
} 


@inproceedings{20184406004286,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CFD study of added resistance and motion of DTC in short and long waves},
journal = {Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE},
author = {Liu, Cong and Chen, Gang and Wan, Decheng},
volume = {7A},
year = {2018},
pages = {Ocean, Offshore and Arctic Engineering Division - },
address = {Madrid, Spain},
abstract = {In the present work, in-house CFD code naoe-FOAM-SJTU is used to investigate the added resistance and motion of DTC ship at Fr=0.052 and 0.138 in short and long head waves. The time history and Fourier series of resistance and motion is given. Validation against the experimental results shows that the computation agrees well with EFD data in high speed cases. Then, the time-averaged and the Fourier transform of hydrodynamic pressure on ship are analyzed. The time-averaged pressure shows the added resistance is mainly caused by the high pressure at the upper bow. The distribution of second harmonic pressure on the bow demonstrates the bow relative motion is closely related with the nonlinearity of resistance. It can be explained as the large bow relative motion induces the stronger non-linear change of instantaneous wetness at flare bow. The wave patterns are plotted and show that the angle which wave diverged from the ship increases as the ship speed decrease and wave length increase.<br/> &copy; 2018 ASME.},
key = {Arctic engineering},
keywords = {Computational fluid dynamics;Fourier series;Ocean engineering;Offshore oil well production;Ships;},
note = {Added resistances;High pressure;Hydrodynamic pressure;Relative motion;Second harmonics;Time averaged pressure;Time-averaged;Wave patterns;},
URL = {http://dx.doi.org/10.1115/OMAE201878380},
} 


@article{20143600047296,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The Ndynamics packageNumerical analysis of dynamical systems and the fractal dimension of boundaries},
journal = {Computer Physics Communications},
author = {Avellar, J. and Duarte, L.G.S. and da Mota, L.A.C.P. and de Melo, N. and Skea, J.E.F.},
volume = {183},
number = {9},
year = {2012},
pages = {2019 - 2020},
issn = {00104655},
abstract = {A set of Maple routines is presented, fully compatible with the new releases of Maple (14 and higher). The package deals with the numerical evolution of dynamical systems and provide flexible plotting of the results. The package also brings an initial conditions generator, a numerical solver manager, and a focusing set of routines that allow for better analysis of the graphical display of the results. The novelty that the package presents an optional C interface is maintained. This allows for fast numerical integration, even for the totally inexperienced Maple user, without any C expertise being required. Finally, the package provides the routines to calculate the fractal dimension of boundaries (via box counting). New version program summary Program Title: Ndynamics Catalogue identifier: %Leave blank, supplied by Elsevier. Licensing provisions: no. Programming language: Maple, C. Computer: Intel(R) Core(TM) i3 CPU M330 @ 2.13&nbsp;GHz. Operating system: Windows 7. RAM: 3.0&nbsp;GB Keywords: Dynamical systems, Box counting, Fractal dimension, Symbolic computation, Differential equations, Maple. Classification: 4.3. Catalogue identifier of previous version: ADKH_v1_0. Journal reference of previous version: Comput. Phys. Commun. 119 (1999) 256. Does the new version supersede the previous version?: Yes. Nature of problem Computation and plotting of numerical solutions of dynamical systems and the determination of the fractal dimension of the boundaries. Solution method The default method of integration is a fifth-order Runge&ndash;Kutta scheme, but any method of integration present on the Maple system is available via an argument when calling the routine. A box counting&nbsp;[1] method is used to calculate the fractal dimension&nbsp;[2] of the boundaries. Reasons for the new version The Ndynamics package met a demand of our research community for a flexible and friendly environment for analyzing dynamical systems. All the user has to do is create his/her own Maple session, with the system to be studied, and use the commands on the package to (for instance) calculate the fractal dimension of a certain boundary, without knowing or worrying about a single line of C programming. So the package combines the flexibility and friendly aspect of Maple with the fast and robust numerical integration of the compiled (for example C) basin. The package is old, but the problems it was designed to dealt with are still there. Since Maple evolved, the package stopped working, and we felt compelled to produce this version, fully compatible with the latest version of Maple, to make it again available to the Maple user. Summary of revisions Deprecated Maple Packages and Commands: Paraphrasing the Maple in-built help files, &ldquo;Some Maple commands and packages are deprecated. A command (or package) is deprecated when its functionality has been replaced by an improved implementation. The newer command is said to supersede the older one, and use of the newer command is strongly recommended&rdquo;. So, we have examined our code to see if some of these occurrences could be dangerous for it. For example, the &ldquo;readlib&rdquo; command is unnecessary, and we have removed its occurrences from our code. We have checked and changed all the necessary commands in order for us to be safe in respect to danger from this source. Another change we had to make was related to the tools we have implemented in order to use the interface for performing the numerical integration in C, externally, via the use of the Maple command &ldquo;ssystem&rdquo;. In the past, we had used, for the external C integration, the DJGPP system. But now we present the package with (free) Borland distribution. The compilation and compiling commands are now slightly changed. For example, to compile only, we had used &ldquo;gcc-c&rdquo;; now, we use &ldquo;bcc32-c&rdquo;, etc. All this installation (Borland) is explained on a &ldquo;README&rdquo; file we are submitting here to help the potential user. Restrictions Besides the inherent restrictions of numerical integration methods, this version of the package only deals with systems of first-order differential equations. Unusual features This package provides user-friendly software tools for analyzing the character of a dynamical system, whether it displays chaotic behaviour, and so on. Options within the package allow the user to specify characteristics that separate the trajectories into families of curves. In conjunction with the facilities for altering the user's viewpoint, this provides a graphical interface for the speedy and easy identification of regions with interesting dynamics. An unusual characteristic of the package is its interface for performing the numerical integrations in C using a fifth-order Runge&ndash;Kutta method (default). This potentially improves the speed of the numerical integration by some orders of magnitude and, in cases where it is necessary to calculate thousands of graphs in regions of difficult integration, this feature is very desirable. Besides that tool, somewhat more experienced users can produce their own C integrator and, by using the commands available in the package, use it as the C integrator provided with the package as long as the new integrator manages the input and output in the same format as the default one does. Running time This depends strongly on the dynamical system. With an Intel<sup>&reg;</sup>&nbsp;Core&trade;&nbsp;i3 CPU M330 @ 2.13&nbsp;GHz, the integration of 50 graphs, for a system of two first-order equations, typically takes less than a second to run (with the C integration interface). Without the C interface, it takes a few seconds. In order to calculate the fractal dimension, where we typically use 10,000 points to integrate, using the C interface it takes from 20 to 30&nbsp;s. Without the C interface, it becomes really impractical, taking, sometimes, for the same case, almost an hour. For some cases, it takes many hours.<br/> &copy; 2012 Elsevier B.V.},
key = {C (programming language)},
keywords = {Differential equations;Dynamical systems;Fractal dimension;Integral equations;Integration;Numerical methods;Problem oriented languages;Runge Kutta methods;Windows operating system;},
note = {Box-counting;Catalogue identifiers;First order differential equation;First order equations;Numerical integration methods;Numerical integrations;Symbolic computation;User-friendly software tools;},
URL = {http://dx.doi.org/10.1016/j.cpc.2012.03.024},
} 


@article{20154701596581,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A portable, extensible and fast stochastic volatility model calibration using multi and many-core processors},
journal = {Concurrency Computation},
author = {Dixon, Matthew and Lotze, Jorg and Zubair, Mohammad},
volume = {28},
number = {3},
year = {2016},
pages = {866 - 877},
issn = {15320626},
abstract = {Financial markets change precipitously, and on-demand pricing and risk models must be constantly recalibrated to reduce risk. However, certain classes of models are computationally intensive to robustly calibrate to intraday prices - stochastic volatility models being an archetypal example due to the non-convexity of the objective function. In order to accelerate this procedure through parallel implementation, financial application developers are faced with an ever growing plethora of low-level high-performance computing frameworks such as Open Multi-Processing, Open Computing Language, compute unified device architecture, or single instruction multiple data intrinsics, and forced to make a trade-off between performance versus the portability, flexibility, and modularity of the code required to facilitate rapid in-house model development and productionisation. This paper describes the acceleration of stochastic volatility model calibration on multi-core CPUs and graphics processing units (GPUs) using the Xcelerit platform. By adopting a simple programming model, the Xcelerit platform enables the application developer to write sequential, high-level C++ code, without concern for low-level high-performance computing frameworks. This platform provides the portability, flexibility, and modularity required by application developers. Speedups of up to 30x and 293x are respectively achieved on an Intel Xeon CPU and NVIDIA Tesla K40 GPU, compared with a sequential CPU implementation. The Xcelerit platform implementation is further shown to be equivalent in performance to a low-level compute unified device architecture version. Overall, we are able to reduce the entire calibration process time of the sequential implementation from 6189 to 183.8 and 17.8 s on the CPU and GPU, respectively, without requiring the developer to reimplement in low-level high-performance computing frameworks.<br/> Copyright &copy; 2015 John Wiley & Sons, Ltd.},
key = {Stochastic models},
keywords = {C++ (programming language);Calibration;Cesium;Computer graphics;Computer software portability;Economic analysis;Economic and social effects;Graphics processing unit;Multicore programming;Program processors;Risk assessment;Stochastic systems;},
note = {Compute unified device architectures;GPGPU;High performance computing;Platform implementations;Sequential implementation;Single instruction multiple data;Stochastic volatility;Stochastic Volatility Model;},
URL = {http://dx.doi.org/10.1002/cpe.3727},
} 


@article{20174104251830,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Erratum to: Controlled-source electromagnetic monitoring of reservoir oil saturation using a novel borehole-to-surface configuration: CSEM monitoring of oil saturation (Geophysical Prospecting, (2015), 63, 6, (1468-1490), 10.1111/1365-2478.12322)},
journal = {Geophysical Prospecting},
author = {Tietze, Kristina and Ritter, Oliver and Veeken, Paul},
volume = {65},
year = {2017},
pages = {317 - 321},
issn = {00168025},
abstract = {INTRODUCTION In the original manuscript, we presented figures of 3D controlled-source electromagnetic forward modelling results for sources, which extended vertically through a reservoir region. The original approach of modelling these sources and the 3D conductivity model led to inaccurate results with the secondary field approach of the code used. Below, we discuss the effects in more detail and show corrected results. Results obtained for &ldquo;conventional&rdquo; survey geometries, with (horizontal) sources and receivers at some distance from 3D conductivity anomalies, are correct. CORRECTIONS Figures and : In the original manuscript, data displayed in Figs (a) and (a) were extracted at the wrong position along a parallel profile with an offset of 1000&nbsp;m. The new plots displayed below show slightly higher absolute changes close to the reservoir for the electric fields, which are expected to be resolved under field conditions. 8 (Figure presented.) CSEM time-lapse responses for the same model and source configuration as in Fig. 7 but now for receivers located between surface and 1800&nbsp;m depth for a fixed frequency of 0.1&nbsp;Hz. (a) absolute differences; (b) relative differences (relative changes &lt; 1&nbsp;% are blanked). Isolines in (b) show log10 magnitudes of electric [V/(Am<sup>2</sup>)] and magnetic field [A/(Am<sup>2</sup>)] components for the initial state. Figures,,, and of our original paper were not correct. For the 3D simulations of the original manuscript, we used the 1D layered half-space without the reservoir as background model, and the reservoir was considered a 3D conductivity anomaly. For both source configurations under consideration, the vertical part was modelled as a set of 130 point-dipoles with a spacing of 10&nbsp;m in vertical direction. As the reservoir is located at 1200 m below surface with a thickness of 15&nbsp;m, 129 of the 130 point-dipoles were located above or below the reservoir, that is, within the 1D background structure. However, 1 of the point-dipoles was located within the reservoir, that is, within the 3D conductivity anomaly, which leads to inaccurate results with the 3D controlled-source electromagnetic modelling (CSEM) code for two reasons. (i) An anomalous cell at the source location leads to very strong secondary sources (the anomalous cell effectively behaves like a point source), and thus, fields near that source vary extremely rapidly, which cannot be modelled accurately with the finite difference (FD) discretisation used. (ii) Source coupling to the surrounding medium determines currents on the source. So therefore, by placing the source in the background medium in the original approach, the coupling between the source and the reservoir was not correctly modelled. As this only applied to 1 out of 130 (vertical dipoles) for the vertical source or even 180 dipoles for the horizontal&ndash;vertical (HV) source, it went unnoticed. 9 (Figure presented.) Vertical source: CSEM time-lapse (Ex) responses at surface using the 3D model shown in Fig. 6 for a resistivity change from 16&nbsp;&Omega;m (reduced oil saturation) to 0.6&nbsp;&Omega;m (100&nbsp;% brine-flushed) in the reservoir. The vertical dipole source is located at x = 5100&nbsp;m as indicated by the asterisk. Dipole lengths of 1150&nbsp;m ((2a) in Fig. 6) and 1300&nbsp;m ((2b) in Fig. 6). (a) Absolute differences, (b) relative differences (changes &lt; 1&nbsp;% suppressed). Isolines in (b) show log10 magnitudes of electric components [V/(Am<sup>2</sup>)] for the initial state. The black horizontal line above the top panel indicates the along-profile extent of reservoir. 10 (Figure presented.) CSEM time-lapse responses for the same model and source configurations as in Fig. but for receivers located at depth for a fixed frequency of 0.1&nbsp;Hz. (a) Absolute differences, (b) relative differences (changes &lt; 1&nbsp;% blanked). Isolines in (b) show log10 magnitudes of electric [V/(Am<sup>2</sup>)] and magnetic field [A/(Am<sup>2</sup>)] components for the initial state. 12 (Figure presented.) HV-source: CSEM time-lapse responses at surface using the 3D model shown in Fig. 6 for a resistivity change from 16 (reduced oil saturation) to 0.6&nbsp;&Omega;m (100&nbsp;% brine flushed) in the reservoir. The casing of the HV source ((3) in Fig. 6) is located at x = 5100&nbsp;m as indicated by the black line. (a) Absolute differences, (b) relative differences (changes &lt; 1&nbsp;% suppressed). The reservoir is located between 5000&nbsp;m and 6000&nbsp;m profile distance. 13 (Figure presented.) CSEM time-lapse responses for the same model and source configuration as in Fig. but for receivers located at depth for a fixed frequency of 0.1&nbsp;Hz. (a) absolute differences, (b) relative differences (changes &lt; 1&nbsp;% are suppressed). Isolines in (b) show log10 magnitudes of electric [V/(Am<sup>2</sup>)] and magnetic field [A/(Am<sup>2</sup>)] components for the initial state. Reconsidering the results in a different context, we found that even for the HV source, where source currents at reservoir depth are 1000 times smaller than currents at the top of the borehole and on the surface wires, errors in calculations of electric and magnetic fields can be severe over the entire modelling domain. Hence, our results shown in the original Figs &ndash; for the 1300 m-long vertical source and for the HV source using scaling factors s of 0.33 and 1.0 in the original Figs &ndash; were biased and wrong. The results for s = 2.0 were correct though, because the current strength was effectively 0 for all point-dipoles at depths &gt; 1165 m (including the anomalous 3D reservoir region). For the results shown in the new figures below, the reservoir was included in the 1D background model. The 3D region of anomalous conductivity comprises all cells at reservoir depth outside the reservoir. Consequently, all source dipoles including elements inside the reservoir reside in cells with conductivities identical to the 1D background model. In addition, the horizontal discretisation at and around the vertical part of the source was refined. For the new mesh, the horizontal edge length of cells was reduced to 20&nbsp;m at the source and the two neighbouring columns in all four directions. In the vertical direction, discretisation was kept the same. In general, cells measure 30&nbsp;m vertically. At the air&ndash;ground interface, as well as at reservoir depth &plusmn; 10&nbsp;m, vertical cell size is reduced to 5&nbsp;m with gradual change of cell heights to 30&nbsp;m following above and below. Modelling the HV source, we discretised the vertical part of the source as 10 m dipoles for depths between 0&nbsp;m and 1150&nbsp;m below surface. In the vicinity of the reservoir, we tested the original vertical discretisation of 10 m, as well as a refined source discretisation with 2 m dipole distance. In both cases, we ensured that the source position does not coincide with a vertical cell boundary. Horizontally, the vertical part of the source is centred in the model column. The results for the original and refined source discretisation were identical. Data for all scenarios were simulated with both staggering schemes implemented in the CSEM modelling code: (i) electric fields (E) defined as face normals and (ii) electric fields defined on cell edges (see, e.g., Streich). Comparison of both results showed excellent agreement for all electric and magnetic field components outside the zone of anomalous conductivity, which further supports reliability of the new modelling results. Deviations occurred at the boundaries of anomalous zones and resulted from differences of conductivity averaging schemes for the two staggering schemes during the calculation of secondary fields (see Streich). In the new modelling, the zone of anomalous conductivity has the same conductivity as cells above and below. Hence, at some lateral distance from the reservoir, electric and magnetic fields are expected to vary smoothly across the boundary of the anomalous region. Inspecting the modelling results for both staggering schemes, we could verify that horizontal electric fields at reservoir depth were varying smoothly and, thus, considered most accurate using staggering scheme with E defined on edges, whereas the vertical electric field required to use E defined on faces. Results shown in Figs,,, and were obtained with the optimal respective staggering scheme. Please note that the colour scale range for Figs (b) and (b) changed in comparison to the original version from +/&minus;&nbsp;100 % to +/&minus;&nbsp;10 %. DISCUSSION For the horizontal&ndash;vertical (HV) source (Figs. &ndash;), the corrected results indicate that measurements of the horizontal electric field Ex are sensitive to changes of conductivity at reservoir depth, but only if currents on the casing follow a source current distribution with a scaling factor of 0.33, corresponding to low frequencies and low resistivities. Absolute amplitude differences reach close to 10<sup>&minus;11</sup>&nbsp;V/(Am<sup>2</sup>) in Fig. (a), along with relative field changes of about 5% for frequencies &lt; 1&nbsp;Hz at 500 to 2000&nbsp;m distance to the source (Fig. b). Considering the results for scaling factors of 1.0 and 2.0, which are more representative for higher frequencies and lower resistivities of host rocks, sensitivities are below the resolution threshold under field conditions. As before, observable amplitude changes associated with a decrease of resistivity within the reservoir are even larger if sensors are deployed closer to the reservoir, in particular, the vertical electric field Ez (Fig.). Similar to the measurements of Ex, both absolute and relative changes are substantially smaller than those estimated in our original submission. The new results also indicate that measurements of magnetic fields both at surface and at depth do not provide measurable information on conductivity changes in the reservoir. Although sensitivities of the vertical dipole and the HV source and measurements of electric field components are not as large as previously stated for this particular scenario, such sources with vertical components are still essential to obtain sensitivity to deep targets such as hydrocarbon reservoirs. As before, we conclude that measurements of Ez are most promising. Better resolution may be obtained if background resistivities are higher or steel-cased boreholes can be energised at the bottom.<br/> &copy; 2017 European Association of Geoscientists & Engineers},
URL = {http://dx.doi.org/10.1111/1365-2478.12550},
} 


@inproceedings{20143600039613,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Cache-oblivious matrix algorithms in the age of multicores and many cores},
journal = {Concurrency Computation},
author = {Heinecke, Alexander and Trinitis, Carsten},
volume = {27},
number = {9},
year = {2015},
pages = {2215 - 2234},
issn = {15320626},
abstract = {This article highlights the issue of upcoming wider single-instruction, multiple-data units as well as steadily increasing core counts on contemporary and future processor architectures. We present the recent port to and latest results of cache-oblivious algorithms and implementations of our TifaMMy code on four architectures: SGI's UltraViolet distributed shared-memory machine, Intel's latest x86 architecture code-named Sandy Bridge, AMD's new Bulldozer architecture, and Intel's future Many Integrated Core architecture. TifaMMy's matrix multiplication and LU decomposition routines have been adapted and tuned with regard to these architectures. Results are discussed and compared with vendors' architecture-specific and optimized libraries, Math Kernel Library and AMD Core Math Library, for both a standard C++ version with vectorization compiler switches and TifaMMy's highly optimized vector intrinsics version. We provide insights into architectural properties and comment on the feasibility of heterogeneous cores and accelerators, namely graphics processing units. Besides bare-metal performance, the test platforms' ease of use is analyzed in detail, and the portability of our approach to new and upcoming silicon is discussed with regard to required effort on code change abstraction levels.As a result, we demonstrate that because of its generic structure in terms of memory organization, TifaMMy executes with equally efficient performance on all four architectures as it automatically adapts itself to architectural parameters without losing performance against the Math Kernel Library and AMD Core Math Library, underlining its generic and cache-oblivious properties, as the porting effort was relatively low compared with that in other implementations.<br/> Copyright &copy; 2012 John Wiley & Sons, Ltd.},
key = {Cache memory},
keywords = {C++ (programming language);Codes (symbols);Computer graphics;Graphics processing unit;Linear algebra;Matrix algebra;Memory architecture;Program processors;},
note = {block recursive;Cache-oblivious;Parallelizations;performance;Shared memory;},
URL = {http://dx.doi.org/10.1002/cpe.2974},
} 


@article{20163102658736,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Erratum: Cluster pattern analysis of energy deposition sites for the brachytherapy sources103Pd,125I,192Ir,137Cs and60Co (Physics in Medicine and Biology (2014) 59 (5531-5543))},
journal = {Physics in Medicine and Biology},
volume = {61},
number = {15},
year = {2016},
pages = {5883 - 5886},
issn = {00319155},
abstract = {Our aim in Villegas et al (2014) was to analyse the spatial distribution of energy deposition (ED) sites for several common radiation qualities used in brachytherapy. Track structure data were obtained through simulations with a modified version of the Monte Carlo (MC) code PENELOPE v.2008 (Fern&aacute;ndez-Varea et al 2012). The existence of a critical bug in this modified MC code has recently been pointed out (Villegas and Ahnesj&ouml; 2015) that overestimated the mean free path of the electrons. In this Corrigendum we present the corrected data for figures 14 in Villegas et al (2014) in the same order as in the published paper. In the original work a description of the procedure for scoring the distances from an energy deposition (ED) site to its ith nearest neighbours dNNi was given. From these, the frequency distribution of distances to the nearest neighbours f (dNNi) normalized per deposited energy - was derived for commonly used brachytherapy sources. In addition, a method for scoring clusters of EDs of different order (CO) based on a single parameter called the cluster distance dc was also described. These yielded the frequency of cluster order fdc (CO) normalized per deposited energy E In figure 1, the updated f (dNN) &lowast; distributions for the first nearest neighbour (NN) decrease with increasing distance for all radiation qualities; but the higher energy photon sources (<sup>192</sup>Ir,<sup>137</sup>Cs and<sup>60</sup>Co) have higher values for distances larger than 30 nm when compared to the lower energy sources (<sup>103</sup>Pd and<sup>125</sup>I). The fifth NN distributions present two peaks for the higher energy photon sources, one at 2 nm and the second at 200 nm. In contrast, the two lower photon energies decrease continuously. The ratios of the distributions show a clear separation between the lower energy photon sources (103Pd and 125I) and the higher energy photon sources (192Ir, 137Cs, and 60Co). The second peaks of the higher energy photon sources distributions coincide with the f (dNN)/&#1013; from a uniform random distribution of points as shown by the updated data in figure 2. The ratios in figure 2 show that the f (dNN)/&#1013; for 103Pd has no uniform random component. In figure 3, the updated f (dNN)/&#1013; for the first NN of the lowest photon energy (103Pd) does not change as dose decreases. Moreover, the frequency distributions are the same as the single track distribution which indicates that for this range of doses the analysed EDs belong to uncorrelated tracks, i.e. there are no inter-track interactions. For higher energy photon sources (e.g. 60Co), the random component of f (dNN)/&#1013; (observed in figure 1) deviates more from the single track f (dNN)/&#1013; as dose increases. For further discussion see Villegas et al (2014). In figure 4, the updated fdc (CO)/&#1013; decreases with increasing cluster order with a steeper slope for dc = 2 nm when compared to dc = 10 nm. For dc = 2 nm, the ratio between the lower photon energies with respect to 60Co is between 1.1 and 1.3, whereas the ratio for 192Ir is found between 0.97 and 1.15, for cluster orders between 2 and 14. For dc = 10, the ratios with respect to 60Co increase about 15 % overall. The ratio of fdc (CO)/&#1013; distributions from single tracks to the distributions from an ED density corresponding to a dose of 2 Gy varies from 0.87 to 1.11 for dc = 2 nm and from 0.89 to 1.11 for dc = 10 nm (corrigendum data for figure 5 in Villegas et al (2014)). Due to the fact that the corrected MC track data yields shorter tracks, the fdc (CO)/&#1013; distributions for dc = 40 nm were not calculated as this distance definition is unreasonably large. The results presented here indicate that the discussion and conclusions made in Villegas et al (2014) still hold true, despite the change in the magnitudes of the frequencies. Particularly, the corrected f (CO)/&#1013; indicate a larger variation between photon sources which further supports our clustering method yields adequate data for radiation quality characterization. &copy; 2016 Institute of Physics and Engineering in Medicine.},
URL = {http://dx.doi.org/10.1088/0031-9155/61/15/5883},
} 


@inproceedings{20174904514087,
language = {Russian},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Support for parallel computing in julia language},
journal = {CEUR Workshop Proceedings},
author = {Kulyabov, Dmitry S. and Gevorkyan, Migran N. and Korolkova, Anna V. and Sevastianov, Leonid A.},
volume = {1995},
year = {2017},
pages = {93 - 99},
issn = {16130073},
address = {Moscow, Russia},
abstract = {The purpose of this paper is a brief review of tools for parallel computing implemented in the current version of the Julia Language. Julia is a young promising language designed for scientific programming. Before describing directly the parallel programming capabilities of Julia we give a small overview of the main features of the language. We describe the main goals pursued by the authors of the language when it was created, and the ideology that they espoused. Separately discussed the scientific focus of Julia, features that make the language convenient for the needs of mathematical modelling, handling big data and intensive numerical computing. Separately discussed such a feature as Julia's multiple dispatch. This mechanism of language paying a lot of attention. Most built-in functions and operators using multiple dispatch. In the second part of the paper we turn to the description of parallel programming. Julia is under active development, so support for parallel computing will be expanded, and the existing mechanisms may change. However, now Julia provides enough capabilities for writing quite complex programs using parallel computing. The basis for concurrency are processes (parallelism based on threads is not yet available). We describe the basic functions and macros that allow you to parallelize the execution of the functions, cycles and separate blocks of code. As the basis for presentation used the official guide and also the experience obtained by the authors in the process of using language.<br/> &copy; Copyright 2017 for the individual papers by the papers' authors.},
key = {Modeling languages},
keywords = {Big data;Data handling;Parallel processing systems;Parallel programming;},
note = {Basic functions;Built-in functions;Complex programs;Julia language;Numerical computing;Scientific programming;},
} 


@inproceedings{20180704807290,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Efficient navigation using slow feature gradients},
journal = {IEEE International Conference on Intelligent Robots and Systems},
author = {Metka, Benjamin and Franzius, Mathias and Bauer-Wersing, Ute},
volume = {2017-September},
year = {2017},
pages = {1311 - 1316},
issn = {21530858},
address = {Vancouver, BC, Canada},
abstract = {A model of hierarchical Slow Feature Analysis (SFA) enables a mobile robot to learn a spatial representation of its environment directly from images captured during a random walk. After the unsupervised learning phase a subset of the resulting representations are orientation invariant and code for the position of the robot. Hence, they change monotonically over space even though the variation of the sensory signals received from the environment might change drastically e.g. during rotation on the spot. Furthermore, the property of spatial smoothness allows us to infer a navigation direction by taking the difference between the measurement at the current location and a measurement at a target location. In our work we investigated the use of slow feature representations, learned for a specific environment, for the purpose of navigation. We present a straightforward method for navigation using gradient descent on the difference between two points specified in slow feature space. Due to its slowness objective, the resulting slow feature representations implicitly encode information about static obstacles, allowing a mobile robot to efficiently circumnavigate them by simply following the steepest gradient in slow feature space.<br/> &copy; 2017 IEEE.},
key = {Intelligent robots},
keywords = {Mobile robots;Navigation;},
note = {Feature representation;Gradient descent;Sensory signals;Slow Feature Analysis(SFA);Spatial representations;Spatial smoothness;Static obstacles;Straight-forward method;},
URL = {http://dx.doi.org/10.1109/IROS.2017.8202307},
} 


@inproceedings{20150200416823,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2014},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {8802},
year = {2014},
pages = {1 - 546},
issn = {03029743},
address = {Imperial, Corfu, Greece},
abstract = {The proceedings contain 44 papers. The special focus in this conference is on Critical Systems, Rigorous Engineering of Autonomic Ensembles, Automata Learning, Formal Methods and Analysis, Model-Based Code Generators and Automata Learning in Practice. The topics include: Statistical abstraction boosts design and test efficiency of evolving critical systems; incremental syntactic-semantic reliability analysis of evolving structured workflows; domain-specific languages for enterprise systems; formalizing self-adaptive clouds with knowlang; towards performance-aware engineering of autonomic component ensembles; rigorous system design flow for autonomous systems; algorithms for inferring register automata; active learning of nondeterministic systems from an ioco perspective; fomal methods and analyses in software product line engineering; domain specific languages for managing feature models; deployment variability in delta-oriented models; coverage criteria for behavioural testing of software product lines; DSL implementation for model-based development of pumps; domain-specific code generator modeling; LNCS transactions on foundations for mastering change; formal methods for collective adaptive ensembles; current issues on model-based software quality assurance for mastering change and compositional model-based system design as a foundation for mastering change.},
} 


@inproceedings{20184406003743,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings - International Computer Software and Applications Conference},
journal = {Proceedings - International Computer Software and Applications Conference},
volume = {1},
year = {2018},
issn = {07303157},
address = {Tokyo, Japan},
abstract = {The proceedings contain 158 papers. The topics discussed include: an empirical analysis on web service anti-pattern detection using a machine learning framework; DevOps improvements for reduced cycle times with integrated test optimizations for continuous integration; a framework for updating functionalities based on the MAPE loop mechanism; combining constraint solving with different MOEAs for configuring large software product lines: a case study; visualizing a tangled change for supporting its decomposition and commit construction; an assertion framework for mobile robotic programming with spatial reasoning; DistGear: a lightweight event-driven framework for developing distributed applications; a lightweight program dependence based approach to concurrent mutation analysis; SPESC: a specification language for smart contracts; an insight into the impact of dockerfile evolutionary trajectories on quality and latency; automatic detection of outdated comments during code changes; search-based efficient automated program repair using mutation and fault localization; identifying supplementary bug-fix commits; a generalized approach to verification condition generation; runtime verification of robots collision avoidance case study; formalization and verification of mobile systems calculus using the rewriting engine Maude; predicting the breakability of blocking bug pairs; model checking of embedded systems using RTCTL while generating timed Kripke structure; PERDICE: towards discovering software inefficiencies leading to cache misses and branch mispredictions; and loop invariant generation for non-monotone loop structures.<br/>},
} 


@inproceedings{20161902370937,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Deep convolutional inverse graphics network},
journal = {Advances in Neural Information Processing Systems},
author = {Kulkarni, Tejas D. and Whitney, William F. and Kohli, Pushmeet and Tenenbaum, Joshua B.},
volume = {2015-January},
year = {2015},
pages = {2539 - 2547},
issn = {10495258},
address = {Montreal, QC, Canada},
abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [10]. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative tests of the model's efficacy at learning a 3D rendering engine for varied object classes including faces and chairs.<br/>},
key = {Convolution},
keywords = {Lighting;Stochastic models;Stochastic systems;Three dimensional computer graphics;},
note = {Interpretable representation;Lighting variations;Multiple layers;Quantitative tests;Stochastic gradient;Three-dimensional scenes;Training procedures;Variational bayes;},
} 


@inproceedings{20184406005983,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Textual Explanations for Self-Driving Vehicles},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep},
volume = {11206 LNCS},
year = {2018},
pages = {577 - 593},
issn = {03029743},
address = {Munich, Germany},
abstract = {Deep neural perception and control networks have become key components of self-driving vehicles. User acceptance is likely to benefit from easy-to-interpret textual explanations which allow end-users to understand what triggered a particular behavior. Explanations may be triggered by the neural controller, namely introspective explanations, or informed by the neural controller&rsquo;s output, namely rationalizations. We propose a new approach to introspective explanations which consists of two parts. First, we use a visual (spatial) attention model to train a convolutional network end-to-end from images to the vehicle control commands, i.e.,&nbsp;acceleration and change of course. The controller&rsquo;s attention identifies image regions that potentially influence the network&rsquo;s output. Second, we use an attention-based video-to-text model to produce textual explanations of model actions. The attention maps of controller and explanation model are aligned so that explanations are grounded in the parts of the scene that mattered to the controller. We explore two approaches to attention alignment, strong- and weak-alignment. Finally, we explore a version of our model that generates rationalizations, and compare with introspective explanations on the same video segments. We evaluate these models on a novel driving dataset with ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-X) dataset. Code is available at https://github.com/JinkyuKimUCB/explainable-deep-driving.<br/> &copy; 2018, Springer Nature Switzerland AG.},
key = {Controllers},
keywords = {Boolean functions;Computer vision;Control system synthesis;Vehicles;},
note = {Attention model;BDD-X dataset;Convolutional networks;Explainable deep driving;Neural controller;Self-driving vehicles;User acceptance;Vehicle Control;},
URL = {http://dx.doi.org/10.1007/978-3-030-01216-8_35},
} 


@inproceedings{20173404073021,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Search-based requirements traceability recovery: A multi-objective approach},
journal = {2017 IEEE Congress on Evolutionary Computation, CEC 2017 - Proceedings},
author = {Ghannem, Adnane and Hamdi, Mohamed Salah and Kessentini, Marouane and Ammar, Hany H.},
year = {2017},
pages = {1183 - 1190},
address = {Donostia-San Sebastian, Spain},
abstract = {Software systems nowadays are complex and difficult to maintain due to the necessity of continuous change and adaptation. One of the challenges in software maintenance is keeping requirements traceability up to date automatically. The process of generating requirements traceability is time-consuming and error-prone. Currently, most available tools do not support the automated recovery of traceability links. In some situations, companies accumulate the history of changes from past maintenance experiences. In this paper, we consider requirements traceability recovery as a multi objective search problem in which we seek to assign each requirement to one or many software elements (code elements, API documentation, and comments) by taking into account the recency of change, the frequency of change, and the semantic similarity between the description of the requirement and the software element. We use the Non-dominated Sorting Genetic Algorithm (NSGA-II) to find the best compromise between these three objectives. We report the results of our experiments on three open source projects.<br/> &copy; 2017 IEEE.},
key = {Requirements engineering},
keywords = {Application programming interfaces (API);Genetic algorithms;Open source software;Recovery;Semantics;},
note = {Non dominated sorting genetic algorithm (NSGA II);NSGA-II;Open source projects;Pareto front;Requirements traceability;Search-based software engineering;Semantic similarity;Traceability links;},
URL = {http://dx.doi.org/10.1109/CEC.2017.7969440},
} 


@inproceedings{20162102423067,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {10th International Scientific and Practical Conference, 2015},
journal = {Vide. Tehnologija. Resursi - Environment, Technology, Resources},
volume = {3},
year = {2015},
pages = {1 - 237},
issn = {16915402},
address = {Rezekne, Latvia},
abstract = {The proceedings contain 37 papers. The special focus in this conference is on Computer Technologies, Modelling Sociotechnical Systems, Intellectual Decision Support Systems, Environmental Education and Sustainable Development Processes. The topics include: Latvian language as a code in different communication channels; computer programming aptitude test as a tool for reducing student attrition; instrument of determination and prediction of public opinion using IPTV statistic data; conditions for successful development of electronic commerce in Latvia; the myths about and solutions for an android OS controlled and secure environment; the influence of hidden neurons factor on neural network training quality assurance; modern approaches to reduce webpage load times; mathematical modelling of aquatic ecosystem; modeling of time dependent thermal process in sliding electrical microcontact; wireless sensor networks lifetime assessment model development; models of data and their processing for introductory courses of computer science; importance of data acquisition in problem based learning; structuration of courses at studying disciplines of programming; fuzzy multiple criteria decision making approach in environmental risk assessment; using the concept of fuzzy random events in the assessment and analysis of ecological risks; application of the ontology concept for the needs of theoretical mechanics; automatic transformation of relational database schema into owl ontologies; calculation temperature and pressure of the rotary engine; artificial neural networks and human brain; survey of improvement possibilities of learning and Niskanen classical model implementation of the office&rsquo;s operations evaluation.},
} 


@inproceedings{20173003980708,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ACM International Conference Proceeding Series},
journal = {ACM International Conference Proceeding Series},
volume = {Part F128635},
year = {2017},
pages = {Axis Communications; Blekinge Institute of Technology; Ericsson; Sigma Technology; The Software Centre - },
address = {Karlskrona, Sweden},
abstract = {The proceedings contain 50 papers. The topics discussed include: on the use of ontologies in software process assessment: a systematic literature review; operationalizing the experience factory for effort estimation in agile processes; change prediction through coding rules violations; scoping and planning experiments in software engineering - a comparative analysis of specification models; attributes that predict which features to fix: lessons for app store mining; exploring the outsourcing relationships in software startups a multiple case study; on the benefits/limitations of agile software development: an interview study with Brazilian companies; how are conceptual models used in industrial software development? an online survey; a method for assessing class change proneness; an exploratory study of functionality and learning resources of web APIs on programmable web; using metrics to track code review performance; industry-academia collaborations in software engineering: an empirical analysis of challenges, patterns and anti-patterns in research projects; towards confidence with capture-recapture estimation: an exploratory study of dependence within inspections; how to reduce software development cost with personnel assignment optimization: exemplary improvement on the Hungarian algorithm; on using active learning and self-training when mining performance discussions on stack overflow; and preliminary study on applying semi-supervised learning to app store analysis.<br/>},
} 


@article{20160601894881,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Experimental evaluation of a glass curtain wall of a tall building},
journal = {Earthquake Engineering and Structural Dynamics},
author = {Lu, Wensheng and Huang, Baofeng and Mosalam, Khalid M. and Chen, Shiming},
volume = {45},
number = {7},
year = {2016},
pages = {1185 - 1205},
issn = {00988847},
abstract = {Summary: The seismic demand parameters including the floor acceleration amplification (FAA) factors and the interstory drift ratios (IDRs) were acquired from the floor response in time history analysis of a tall building subjected to selected ground motions. The FAA factors determined in this way are larger than those given in most current code provisions, but the obtained IDRs are close to the values given in some code provisions. Imposing a series of in-plane pre-deformations to two glass curtain wall (CW) specimens mounted on a shaking table, the IDRs were reproduced and the FAA factors were satisfied through applications of computed floor spectra compatible motion time histories, whose peak accelerations corresponded to the FAA factors. The CW specimens performed well during the whole experimental program with almost no change in the fundamental frequencies. No visible damage was observed in the glass panels. The maximum stresses detected in each component of the CW system were smaller than the design strengths. The obtained component acceleration amplification factor approached 3.35, which is larger than the value given in the current code provisions. In conclusion, the performance of the studied CW system is seismically safe.<br/> &copy; 2016 John Wiley & Sons, Ltd.},
key = {Walls (structural partitions)},
keywords = {Codes (symbols);Floors;Glass;Seismic design;Seismology;Tall buildings;},
note = {Floor accelerations;Glass curtain walls;Seismic Performance;Shaking table tests;Story drift;},
URL = {http://dx.doi.org/10.1002/eqe.2705},
} 


@article{20172603843682,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Comparison of photon organ and effective dose coefficients for PIMAL stylized phantom in bent positions in standard irradiation geometries},
journal = {Radiation and Environmental Biophysics},
author = {Dewji, Shaheen and Reed, K. Lisa and Hiller, Mauritius},
volume = {56},
number = {3},
year = {2017},
pages = {277 - 291},
issn = {0301634X},
abstract = {Computational phantoms with articulated arms and legs have been constructed to enable the estimation of radiation dose in different postures. Through a graphical user interface, the Phantom wIth Moving Arms and Legs (PIMAL) version 4.1.0 software can be employed to articulate the posture of a phantom and generate a corresponding input deck for the Monte Carlo N-Particle (MCNP) radiation transport code. In this work, photon fluence-to-dose coefficients were computed using PIMAL to compare organ and effective doses for a stylized phantom in the standard upright position with those for phantoms in realistic work postures. The articulated phantoms represent working positions including fully and half bent torsos with extended arms for both the male and female reference adults. Dose coefficients are compared for both the upright and bent positions across monoenergetic photon energies: 0.05, 0.1, 0.5, 1.0, and 5.0&nbsp;MeV. Additionally, the organ doses are compared across the International Commission on Radiological Protection&rsquo;s standard external radiation exposure geometries: antero-posterior, postero-anterior, left and right lateral, and isotropic (AP, PA, LLAT, RLAT, and ISO). For the AP and PA irradiation geometries, differences in organ doses compared to the upright phantom become more profound with increasing bending angles and have doses largely overestimated for all organs except the brain in AP and bladder in PA. In LLAT and RLAT irradiation geometries, energy deposition for organs is more likely to be underestimated compared to the upright phantom, with no overall change despite increased bending angle. The ISO source geometry did not cause a significant difference in absorbed organ dose between the different phantoms, regardless of position. Organ and effective fluence-to-dose coefficients are tabulated. In the AP geometry, the effective dose at the 45&deg; bent position is overestimated compared to the upright phantom below 1&nbsp;MeV by as much as 27% and 82% in the 90&deg; position. The effective dose in the 45&deg; bent position was comparable to that in the 90&deg; bent position for the LLAT and RLAT irradiation geometries. However, the upright phantom underestimates the effective dose to PIMAL in the LLAT and RLAT geometries by as much as 30% at 50&nbsp;keV.<br/> &copy; 2017, Springer-Verlag Berlin Heidelberg.},
key = {Phantoms},
keywords = {Geometry;Graphical user interfaces;Intelligent systems;Irradiation;Monte Carlo methods;Photons;Radiation;Radiation protection;},
note = {Anthropomorphic phantoms;Dose reconstruction;External radiation exposure;International commission on radiological protections;Irradiation geometries;MCNP;Monte carlo n particles;Radiation transport codes;},
URL = {http://dx.doi.org/10.1007/s00411-017-0698-1},
} 


@inproceedings{20183505753405,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security, QRS 2018},
journal = {Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security, QRS 2018},
year = {2018},
pages = {IEEE Reliability Society - },
address = {Lisbon, Portugal},
abstract = {The proceedings contain 50 papers. The topics discussed include: probabilistic sampling-based testing for accelerated reliability assessment; verifying stochastic behaviors of decentralized self-adaptive systems: a formal modeling and simulation based approach; machine learning to evaluate evolvability defects: code metrics thresholds for a given context; a method for predicting two-variable atomicity violations; cross-entropy: a new metric for software defect prediction; a security model for access control in graph-oriented databases; hypervisor-based sensitive data leakage detector; detecting errors in a humanoid robot; using crash frequency analysis to identify error-prone software technologies in multi-system monitoring; exploratory data analysis of fault injection campaigns; change-based test script maintenance for android apps; and how do defects hurt qualities? an empirical study on characterizing a software maintainability ontology in open source software.<br/>},
} 


@article{20160801965487,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modeling and simulation of air-assist atomizers with applications to food sprays},
journal = {Applied Mathematical Modelling},
author = {Tanner, Franz X. and Feigl, Kathleen and Kaario, Ossi and Windhab, Erich J.},
volume = {40},
number = {11-12},
year = {2016},
pages = {6121 - 6133},
issn = {0307904X},
abstract = {The Cascade Atomization and Drop Breakup (CAB) model has been developed originally for pressure atomizers. In this study, the CAB model is modified to accommodate air-assist atomization. The modifications include a change in the product drop distributions, namely, the uniform distribution used in the original CAB model is replaced with a &chi;-squared distribution with the same average drop size. The second modification addresses the air-assist atomization process. This process is modeled by estimating the Weber number due to the increased relative velocity caused by the air flow. Depending on the value of the Weber number this leads to a catastrophic, or a stripping (sheet-thinning), or a bag breakup. The model changes are validated with experimental data obtained from two different air-assist atomizers using an oil-in-water emulsion. The simulations were performed with a modified version of the KIVA-3 CFD code, and they showed good agreement with the experimental data.<br/> &copy; 2016.},
key = {Atomization},
keywords = {Air;Atomizers;Computational fluid dynamics;Drop breakup;Emulsification;Emulsions;Oils and fats;Probability distributions;},
note = {Air-assist;Air-assist atomizers;Atomization process;Breakup models;Chi-squared distribution;Model and simulation;Oil-in-water emulsions;Uniform distribution;},
URL = {http://dx.doi.org/10.1016/j.apm.2016.01.048},
} 


@article{20162002387183,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Real-time infrared thermography at ASDEX upgrade},
journal = {Fusion Science and Technology},
author = {Sieglin, B. and Faitsch, M. and Herrmann, A. and Martinov, S. and Eich, T.},
volume = {69},
number = {3},
year = {2016},
pages = {580 - 585},
issn = {15361055},
abstract = {Infrared (IR) thermography is a widely used tool in fusion research to study the thermal load onto plasma-facing components. In present-day fusion experiments with short-pulse duration, off-line data analysis is still feasible. For devices with long-pulse duration and actively cooled plasma-facing components, IR thermography is a common tool for machine protection. In future fusion devices with long-pulse duration, online data evaluation of the thermography measurement for additional physics studies is required. Real-time' capable IR thermography was developed at ASDEX Upgrade. The feasibility of real-time thermography is discussed in this work. The evaluation process from raw data to evaluated temperature and heat flux is shown. The real-time version of the THEODOR code allows online calculation of the heat flux. Exploiting the possibility of the IR system to change the integration time during acquisition opens up the possibility to have automated thermography. The current status of the thermography system at ASDEX Upgrade and future developments for its improvement are discussed.<br/>},
key = {Thermography (imaging)},
keywords = {Facings;Heat flux;Machine components;Thermal load;},
note = {Fusion experiments;Fusion research;Integration time;Long pulse durations;Online calculations;Plasma-facing components;Real time;Short pulse duration;},
URL = {http://dx.doi.org/10.13182/FST15-183},
} 


@inproceedings{20161302174130,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings},
journal = {2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings},
year = {2015},
pages = {IEEE Computer Society TCSE - },
address = {Bremen, Germany},
abstract = {The proceedings contain 79 papers. The topics discussed include: software history under the lens: a study on why and how developers examine it; to fix or to learn? how production bias affects developers' information foraging during debugging; developers' perception of co-change patterns: an empirical study; when and why developers adopt and change software licenses; investigating naming convention adherence in java references; developing a model of loop actions by mining loop characteristics from a large code corpus; delta extraction: an abstraction technique to comprehend why two objects could be related; modeling changeset topics for feature location; four eyes are better than two: on the impact of code reviews on software quality; and a comparative study on the bug-proneness of different types of code clones.},
} 


@article{20134316904495,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Effects of perforated baffle on reducing sloshing in rectangular tank: Experimental and numerical study},
journal = {China Ocean Engineering},
author = {Xue, Mi-an and Lin, Peng-zhi and Zheng, Jin-hai and Ma, Yu-xiang and Yuan, Xiao-li and Nguyen, Viet-Thanh},
volume = {27},
number = {5},
year = {2013},
pages = {615 - 628},
issn = {08905487},
abstract = {A liquid sloshing experimental rig driven by a wave-maker is designed and built to study liquid sloshing problems in a rectangular liquid tank with perforated baffle. A series of experiments are conducted in this experimental rig to estimate the free surface fluctuation and pressure distribution by changing external excitation frequency of the shaking table. An in-house CFD code is also used in this study to simulate the liquid sloshing in three-dimensional (3D) rectangular tank with perforated baffle. Good agreements of free surface elevation and pressure between the numerical results and the experimental data are obtained and presented. Spectral analysis of the time history of free surface elevation is conducted by using the fast Fourier transformation. &copy; 2013 Chinese Ocean Engineering Society and Springer-Verlag Berlin Heidelberg.<br/>},
key = {Liquid sloshing},
keywords = {Computational fluid dynamics;Experimental reactors;Spectrum analysis;Tanks (containers);},
note = {Experimental and numerical studies;experimental study;External excitation frequency;Fast Fourier transformations;Free surface elevations;Free surface fluctuation;Perforated baffles;Rectangular tank;},
URL = {http://dx.doi.org/10.1007/s13344-013-0052-6},
} 


@inproceedings{20183805815238,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ALM Tool Infrastructure with a Focus on DevOps Culture},
journal = {Communications in Computer and Information Science},
author = {Akman, Suha and Aksuyek, Elif Berru and Kaynak, Onur},
volume = {896},
year = {2018},
pages = {291 - 303},
issn = {18650929},
address = {Bilbao, Spain},
abstract = {At iNNOVA IT Solutions, with our team of more than 1300 IT professionals, we tailor platform-independent customized software and systems solutions. The ALM tool infrastructure has been continuously improved over the several years in order to maintain the work item traceability, define a single source of truth and increase communication and collaboration which is one of the key cultural aspects of DevOps. This paper describes how the tool infrastructure is set up and used in order to manage traceability between work items such as change requests, configuration items, technical documents, test cases, technical tasks and code; make use of the knowledge base and history during impact analysis; track the real time status of releases, change requests, sprints, test plans and all the related work items; generate formatted customer documentation in long term maintenance projects.<br/> &copy; 2018, Springer Nature Switzerland AG.},
key = {Life cycle},
keywords = {Application programs;Knowledge based systems;Process engineering;Software design;},
note = {Application lifecycle managements;DevOps;Requirement traceabilitys;Software development life cycle;Software Process Improvement;},
URL = {http://dx.doi.org/10.1007/978-3-319-97925-0_24},
} 


@inproceedings{20164903087499,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Vertical Axis Wind Turbine Design Load Cases Investigation and Comparison with Horizontal Axis Wind Turbine},
journal = {Energy Procedia},
author = {Galinos, Christos and Larsen, Torben J. and Madsen, Helge A. and Paulsen, Uwe S.},
volume = {94},
year = {2016},
pages = {319 - 328},
issn = {18766102},
address = {Trondheim, Norway},
abstract = {The paper studies the applicability of the IEC 61400-1 ed.3, 2005 International Standard of wind turbine minimum design requirements in the case of an onshore Darrieus VAWT and compares the results of basic Design Load Cases (DLCs) with those of a 3-bladed HAWT. The study is based on aeroelastic computations using the HAWC2 aero-servo-elastic code A 2-bladed 5 MW VAWT rotor is used based on a modified version of the DeepWind rotor For the HAWT simulations the NREL 3-bladed 5 MW reference wind turbine model is utilized Various DLCs are examined including normal power production, emergency shut down and parked situations, from cut-in to cut-out and extreme wind conditions. The ultimate and 1 Hz equivalent fatigue loads of the blade root and turbine base bottom are extracted and compared in order to give an insight of the load levels between the two concepts. According to the analysis the IEC 61400-1 ed.3 can be used to a large extent with proper interpretation of the DLCs and choice of parameters such as the hub-height. In addition, the design drivers for the VAWT appear to differ from the ones of the HAWT. Normal operation results in the highest tower bottom and blade root loads for the VAWT, where parked under storm situation (DLC 6.2) and extreme operating gust (DLC 2.3) are more severe for the HAWT. Turbine base bottom and blade root edgewise fatigue loads are much higher for the VAWT compared to the HAWT. The interpretation and simulation of DLC 6.2 for the VAWT lead to blade instabilities, while extreme wind shear and extreme wind direction change are not critical in terms of loading of the VAWT structure. Finally, the extreme operating gust wind condition simulations revealed that the emerging loads depend on the combination of the rotor orientation and the time stamp that the frontal passage of gust goes through the rotor plane.<br/> &copy; 2016 The Authors.},
key = {Turbine components},
keywords = {Aeroelasticity;Turbomachine blades;Wind turbines;},
note = {Aeroelastic computations;Darrieus;Extreme wind conditions;HAWT;Horizontal axis wind turbines;Onshore wind turbine;VAWT;Vertical axis wind turbines;},
URL = {http://dx.doi.org/10.1016/j.egypro.2016.09.190},
} 


@inproceedings{20141817683131,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Performance analysis of paralldroid generated programs},
journal = {Proceedings - 2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing, PDP 2014},
author = {Acosta, Alejandro and Almeida, Francisco},
year = {2014},
pages = {60 - 67},
address = {Turin, Italy},
abstract = {The advent of emergent System-on-Chip (SoCs) and multiprocessor System-on-Chip (MPSocs) opens a new era on the small mobile devices (Smartphones, Tablets,...) in terms of computing capabilities and applications to be addressed. The efficient use of such devices, including the parallel power, is still a challenge for general purpose programmers due to the very high learning curve demanding very specific knowledge of the devices. While some efforts are currently being made, mainly in the scientific scope, the scenario is still quite far from being the desirable for non-scientific applications where very few of them take advantage of the parallel capabilities of the devices. We develop a performance analysis in several SoCs using Paralldroid. Paralldroid (Framework for Parallelism in Android), is a parallel development framework oriented to general purpose programmers for standard mobile devices. Paralldroid presents a programming model that unifies the different programming models of Android. The user just implements a Java application and introduces a set of Paralldroid annotations in the sections of code to be optimized. The Paralldroid system automatically generates the native C, OpenCL or Renderscript code for the annotated section. The Paralldroid transformation model involves source-to-source transformations and skeletal programming. &copy; 2014 IEEE.<br/>},
key = {C (programming language)},
keywords = {Android (operating system);Constraint theory;Program compilers;Programmable logic controllers;System-on-chip;},
note = {Android;Computing capability;General-purpose programmers;Multiprocessor system on chips;Parallel development;Renderscript;Scientific applications;Source-to-source transformations;},
URL = {http://dx.doi.org/10.1109/PDP.2014.14},
} 


@inproceedings{20144300114455,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Flexible access control for JavaScript},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
author = {Hammer, Christian},
volume = {P227},
year = {2014},
pages = {79 - 80},
issn = {16175468},
address = {Kiel, Germany},
abstract = {Many popular Web applications mix content from different sources, such as articles coming from a newspaper, a search bar provided by a search engine, advertisements served by a commercial partner, and included third-party libraries to enrich the user experience. The behavior of such a web site depends on all of its parts working, especially so if it is financed by ads. Yet, not all parts are equally trusted. Typically, the main content provider is held to a higher standard than the embedded third-party elements. A number of well publicized attacks have shown that ads and third-party components can introduce vulnerabilities in the overall application. Taxonomies of these attacks are emerging [JJLS10]. Attacks such as cross site scripting, cookie stealing, location hijacking, clickjacking, history sniffing and behavior tracking are being catalogued, and the field is rich and varied.<sup>1</sup>This paper proposes a novel security infrastructure for dealing with this threat model. We extend JavaScript objects with dynamic ownership annotations and break up a web site's computation at ownership changes, that is to say when code belonging to a different owner is executed, into delimited histories. Subcomputations performed on behalf of untrusted code are executed under a special regime in which most operations are recorded into histories. Then, at the next ownership change, or at other well defined points, these histories are made available to user-configurable security policies which can, if the history violates some safety rule, issue a revocation request. Revocation undoes all the computational effects of the history, reverting the state of the heap to what it was before the computation. Delimiting histories is crucial for our technique to scale to real web sites. While JavaScript pages can generate millions of events, histories are typically short, and fit well within the computation model underlying Web 2.0 applications: once the history of actions of an untrusted code fragment is validated, the history can be discarded. Histories allow policies to reason about the impact of an operation within a scope by giving policies a view on the outcome of a sequence of computational steps. Consider storing a secret into an object's field. This could be safe if the modification was subsequently overwritten and replaced by the field's original value. Traditional access control policies would reject the first write, but policies in our framework can postpone the decision and observe if this is indeed a leak. While policies of interest could stretch all the way to dynamic information flow tracking, we focus on access control in this talk and present the following contributions [RHZN+13]: &bull; A novel security infrastructure: Access control decisions for untrusted code are based on delimited histories. Revocation can restore the program to a consistent state. The enforceable security policies are a superset of [Sch00] as revocation allows access decisions based on future events. &bull; Support of existing JavaScript browser security mechanisms: All JavaScript objects are owned by a principal. Ownership is integrated with the browser's same origin principle for backwards compatibility with Web 2.0 applications. Code owned by an untrusted principal is executed in a controlled environment, but the code has full access to the containing page. This ensures compatibility with existing code. &bull; Browser integration: Our system was implemented in the WebKit library. We instrument all means to create scripts in the browser at runtime, so if untrusted code creates another script we add its security principal to the new script as well. Additionally, we treat the eval function as untrusted and always monitor it. &bull; Flexible policies: Our security policies allow enforcement of semantic properties based on the notion of security principals attached to JavaScript objects, rather than mere syntactic properties like method or variable names that previous approaches generally rely on. Policies can be combined, allowing for both provider-specified security and user-defined security. &bull; Empirical Evaluation: We validated our approach on 50 real web sites and two representative policies. The results suggest that our model is a good fit for securing web ad content and third-party extensions, with less than 10% of sites' major functionality broken. Our policies have successfully prevented dangerous operations performed by third-party code. The observed performance overheads were between 11% and 106% in the interpreter.<br/> &copy; Gessellschaft f&uuml;r Informatik, Bonn 2014.},
key = {Access control},
keywords = {Codes (symbols);High level languages;Search engines;Security systems;Semantics;Syntactics;Websites;},
note = {Access control decisions;Access control policies;Computational effects;Controlled environment;Dynamic information flow tracking;Empirical evaluations;Security infrastructure;Web 2.0 applications;},
} 


@article{20153101090967,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Monte Carlo simulations of the relative biological effectiveness for DNA double strand breaks from 300 MeV u-1carbon-ion beams},
journal = {Physics in Medicine and Biology},
author = {Huang, Y.W. and Pan, C.Y. and Hsiao, Y.Y. and Chao, T.C. and Lee, C.C. and Tung, C.J.},
volume = {60},
number = {15},
year = {2015},
pages = {5995 - 6012},
issn = {00319155},
abstract = {Monte Carlo simulations are used to calculate the relative biological effectiveness (RBE) of 300 MeV u<sup>-1</sup>carbon-ion beams at different depths in a cylindrical water phantom of 10 cm radius and 30 cm long. RBE values for the induction of DNA double strand breaks (DSB), a biological endpoint closely related to cell inactivation, are estimated for monoenergetic and energy-modulated carbon ion beams. Individual contributions to the RBE from primary ions and secondary nuclear fragments are simulated separately. These simulations are based on a multi-scale modelling approach by first applying the FLUKA (version 2011.2.17) transport code to estimate the absorbed doses and fluence energy spectra, then using the MCDS (version 3.10A) damage code for DSB yields. The approach is efficient since it separates the non-stochastic dosimetry problem from the stochastic DNA damage problem. The MCDS code predicts the major trends of the DSB yields from detailed track structure simulations. It is found that, as depth is increasing, RBE values increase slowly from the entrance depth to the plateau region and change substantially in the Bragg peak region. RBE values reach their maxima at the distal edge of the Bragg peak. Beyond this edge, contributions to RBE are entirely from nuclear fragments. Maximum RBE values at the distal edges of the Bragg peak and the spread-out Bragg peak are, respectively, 3.0 and 2.8. The present approach has the flexibility to weight RBE contributions from different DSB classes, i.e. DSB0, DSB+ and DSB++.<br/> &copy; 2015 Institute of Physics and Engineering in Medicine.},
key = {Monte Carlo methods},
keywords = {Bioinformatics;Carbon;Codes (symbols);DNA;Free radicals;Intelligent systems;Ion beams;Ions;Stochastic systems;},
note = {Cell inactivation;DNA double strand breaks;Double strand breaks;Micro-dosimetry;Multi-scale modelling;Nuclear fragments;Relative biological effectiveness;Spread out Bragg peaks;},
URL = {http://dx.doi.org/10.1088/0031-9155/60/15/5995},
} 


@article{20163202682881,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Interpreting low-temperature thermochronology in magmatic terranes: Modeling and case studies from the Colorado Plateau},
journal = {ProQuest Dissertations and Theses Global},
author = {Murray, Kendra Elizabeth},
year = {2016},
abstract = {Exploring the complexities---and advantages---of interpreting low-temperature thermochronologic data in magmatic terranes is the principal theme of this work. Using simple analytical approximations as well as the finite-element code Pecube, we characterize the cooling age patterns inside and around plutons emplaced at upper and middle crustal levels and identify the advective and conductive scaling relationships that govern these patterns. We find that the resetting aureole width, the difference between reset and unreset cooling ages in country rocks, and the lag time between pluton crystallization age and pluton cooling age all scale with exhumation rate because this rate sets the advective timescale of cooling. Cooling age-elevation relationships in these steadily exhuming models have changes in slope that would masquerade as changes in exhumation or erosion rates in real datasets, if the thermal effects of the plutons were not accounted for. We also demonstrate the importance of considering the magmatic history of a region in field studies of the Colorado Plateau, where interpreting apatite (U-Th)/He data requires diagnosing significant inter- and intra-sample age variability. Prior to considering the thermal history of the region, we develop a new model for a common source of this age variability: excess He implantation from U and Th (i.e., eU) hosted in secondary grain boundary phases (GBPs), which can make very low eU apatites hundreds of percent 'too old'. Samples significantly affected by He implantation are not useful for thermal history interpretations, but this model does provide a diagnostic tool for discriminating these samples from those with useful age trends. Once the effects of GBPs have been accounted for, the remaining data from two different thermochronologic archives in the central Colorado Plateau provide a new perspective on the Cenozoic history of the region, which has a multiphase---and enigmatic---history of magmatism and erosion. We find that sandstones in the thermal aureoles around the Henry, La Sal, and Abajo mountains intrusive complexes were usefully primed by magmatic heating in the Oligocene to document the subsequent late Cenozoic history of the region more clearly than any other thermochronologic archive on the Plateau. These data document a stable Miocene landscape (erosion rates &lt;30 m/Ma) that rapidly exhumed ~1.5-2 km in the Plio-Pleistocene (~250-700 m/Ma no earlier than 5 Ma) in the Henry and Abajo mountains, and strongly suggest most of this erosion occurred in the last 3-2 Ma. The integration of the Colorado River ca. 6 Ma, which dropped regional base-level, is the principal driver of this erosion. It is likely, however, that a component of the rapid Pleistocene rock cooling is unique to the high mountains of the Colorado Plateau and reflects an increase in spring snow-melt discharge during glacial periods. Although apatite thermochronology results far from the Oligocene intrusive complexes cannot resolve this detailed Plio-Pleistocene history, they do constrain the onset of late Cenozoic erosion to no earlier than ~6 Ma. Moreover, apatite cooling ages from these rocks also document Oligocene cooling (ca. 25 Ma) that is contemporaneous with the emplacement of the laccoliths and the waning of the vigorous magmatic flare-up that swept through the southwestern USA ca. 40-25 Ma. Although the cooling ages are consistent with ~1 km of exhumation in the late Oligocene and early Miocene, as previous workers have suggested in the eastern Grand Canyon region, we demonstrate that a transient change in the geothermal gradient (peaking at ~50 ?C/Ma in the late Oligocene) driven by moderate mid-crustal magmatism can produce identical age patterns. Therefore, we re-interpret the mid-Cenozoic erosion event on the Colorado Plateau as primarily a change in the crustal thermal field, rather than an erosional event. This requires a more significant Laramide-age unroofing in parts of the central Plateau and perhaps a re-evaluation of the interpretations of Oligocene canyon cutting in the Grand Canyon region. (Abstract shortened by ProQuest.). ProQuest Subject Headings: Geology, Geochemistry.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.},
key = {Cooling},
keywords = {Apatite;Erosion;Geochemistry;Geochronology;Geology;Grain boundaries;Helium;Landforms;Salinity measurement;Tectonics;Temperature;},
note = {Analytical approximation;Crystallization age;Finite element codes;Geothermal gradients;Grain boundary phasis;Low temperatures;Scaling relationships;Transient changes;},
} 


@inproceedings{20182905578970,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ACM International Conference Proceeding Series},
journal = {ACM International Conference Proceeding Series},
year = {2018},
address = {Lakeland, FL, United states},
abstract = {The proceedings contain 30 papers. The topics discussed include: a new text representation method for clustering based on higher order Markov model; polysemy aware word embeddings;  semantic oriented document clustering using distribution semantics; RMDL: random multimodel deep learning for classification; snow leopard recognition using deep convolution neural network; full disk encryption: a comparison on data management attributes; complex adaptive systems: a data modelling and engineering perspective; master data management maturity model for the successful of MDM initiatives in the microfinance sector in Peru; application of near-infra-red spectroscopy for the analysis of the impact of black color on neural response; quality data extraction methodology based on the labeling of coffee leaves with nutritional deficiencies; towards investigation of iterative strategy for data mining of short-term traffic flow with recurrent neural networks; the issues affecting employees adopting online banking in Mahikeng; designing a decision tree for cross-device communication technology aimed at IoS and Andriod developers; an architecture for translating sequential code to parallel; in-vehicle software defined networking: an enabler for data interoperability; machine learning approach for malware detection using random forest classifier on process list data structure; an ontology framework of requirements change management process based on causality; and automation managed inventory system (AMI) creation for OTOP distribution center.<br/>},
} 


@inproceedings{20151000592957,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Abstract machine models and proxy architectures for exascale computing},
journal = {Proceedings of Co-HPC 2014: 1st International Workshop on Hardware-Software Co-Design for High Performance Computing - Held in Conjunction with SC 2014: The International Conference for High Performance Computing, Networking, Storage and Analysis},
author = {Ang, J.A. and Barrett, R.F. and Benner, R.E. and Burke, D. and Chan, C. and Cook, J. and Donofrio, D. and Hammond, S.D. and Hemmert, K.S. and Kelly, S.M. and Le, H. and Leung, V.J. and Resnick, D.R. and Rodrigues, A.F. and Shalf, J. and Stark, D. and Unat, D. and Wright, N.J.},
year = {2014},
pages = {25 - 32},
address = {New Orleans, LA, United states},
abstract = {To achieve exascale computing, fundamental hardware architectures must change. This will significantly impact scientific applications that run on current high performance computing (HPC) systems, many of which codify years of scientific domain knowledge and refinements for contemporary computer systems. To adapt to exascale architectures, developers must be able to reason about new hardware and determine what programming models and algorithms will provide the best blend of performance and energy efficiency in the future. An abstract machine model is designed to expose to the application developers and system software only the aspects of the machine that are important or relevant to performance and code structure. These models are intended as communication aids between application developers and hardware architects during the co-design process. A proxy architecture is a parameterized version of an abstract machine model, with parameters added to elucidate potential speeds and capacities of key hardware components. These more detailed architectural models enable discussion among the developers of analytic models and simulators and computer hardware architects and they allow for application performance analysis, system software development, and hardware optimization opportunities. In this paper, we present a set of abstract machine models and show how they might be used to help software developers prepare for exascale. We then apply parameters to one of these models to demonstrate how a proxy architecture can enable a more concrete exploration of how well application codes map onto future architectures.<br/> &copy; 2014 IEEE.},
key = {Computer hardware},
keywords = {Application programs;Computer architecture;Energy efficiency;Hardware;Hardware-software codesign;Machine components;Software design;},
note = {Application developers;Application performance;Architectural models;Future architectures;Hardware architecture;Hardware optimization;High performance computing systems;Scientific applications;},
URL = {http://dx.doi.org/10.1109/Co-HPC.2014.4},
} 


@inproceedings{20150400452530,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The risk management of the owner in the bidding process under the 2013 edition of the valuation mode of bill of engineering quantity},
journal = {Applied Mechanics and Materials},
author = {Guo, Zhang Lin and Jia, Rui Hong},
volume = {687-691},
year = {2014},
pages = {4670 - 4673},
issn = {16609336},
address = {Taiyuan, China},
abstract = {Taking the perspective of the owners,we analyze the major valuation risk which is based on the theory of the project risk management under the version of 2013 code of valuation with bill quantity of construction works when the owner want to bid to identify the possible risk factors. And in this article we propose some specific measures of the risk control about the imperfect of construction drawing design,the quatity problems of tender documents,unbalanced quote of the bidders,the price changing of the building materials and equipments and various rates of change in order to achieve active control risk and avoid exceeding the cost and provide a theoretical basis for the owner (or the tender agent) to bid. &copy; (2014) Trans Tech Publications, Switzerland.},
key = {Risk assessment},
keywords = {Construction equipment;Costs;Manufacture;Project management;Risk management;},
note = {Active control;Bidding process;Construction drawings;Construction works;Owner;Project risk management;Risk controls;Tender documents;},
URL = {http://dx.doi.org/10.4028/www.scientific.net/AMM.687-691.4670},
} 


@inproceedings{20131516187408,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Improved data minimization technique in reducing memory space complexity for DNA local alignment accelerator application},
journal = {ISCAIE 2012 - 2012 IEEE Symposium on Computer Applications and Industrial Electronics},
author = {Junid, Syed Abdul Mutalib Al and Tahir, Nooritwati Md and Majid, Zulkifli Abd and Halim, Abdul Karimi and Shariff, Khairul Khaizi Mohd},
year = {2012},
pages = {153 - 156},
address = {Kota Kinabalu, Malaysia},
abstract = {Improved data minimization technique to optimize the length of DNA sequence and alignment result characters representation is presented in this paper. The primary objective is to improve and optimize data representation for DNA sequences alignment and result character. The proposed design change in algorithm and architecture is presented in this paper. Algorithm design based on binary equivalent method is used to obtain the optimal size of characters representation. The code is written, compiled and simulated using Altera Quartus II Version 9.0 EDA tools. Verilog Hardware Description Language (HDL) and Altera Cyclone II EP2C35 FPGA are used as coding language and target device respectively. In addition, the structural modelling technique is used to reduce the design complexity. Simulation result showed that the improved data minimization technique takes 50% more memory compared to previous work, but it covers 6 DNA sequences and alignment result characters. &copy; 2012 IEEE.<br/>},
key = {Computer hardware description languages},
keywords = {Bioinformatics;DNA;DNA sequences;Field programmable gate arrays (FPGA);Industrial electronics;},
note = {Accelerator applications;Data minimizations;Data representations;Design complexity;Dna sequences alignments;Primary objective;Structural modelling;Verilog hardware description languages;},
URL = {http://dx.doi.org/10.1109/ISCAIE.2012.6482087},
} 


@article{20160101758710,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Compression of three-dimensional surfaces by means of chain coding},
journal = {Optical Engineering},
author = {Salazar, J. Miguel and Bribiesca, Ernesto},
volume = {54},
number = {12},
year = {2015},
issn = {00913286},
abstract = {Studies for representing three-dimensional (3-D) objects are an important subject in different fields, such as computer vision, data compression, creation of virtual scenes, and others. Any surface can be seen as a 3-D object and studied as such. 3-D voxel-based surfaces are described using a 3-D tree structure known as an enclosing tree. A modified version of this structure is used to describe the surface. In order to describe the surfaces, we used the enclosing trees that are represented via the orthogonal direction change chain code. The representation obtained is compared with the original to verify if proper data compression is achieved.<br/> &copy; 2015 Society of Photo-Optical Instrumentation Engineers (SPIE).},
key = {Trees (mathematics)},
keywords = {Chains;Codes (symbols);Data compression;Forestry;},
note = {Chain codes;Chain coding;enclosing trees;Orthogonal directions;Three-dimensional (3D) objects;Three-dimensional surface;Tree structures;Virtual scenes;},
URL = {http://dx.doi.org/10.1117/1.OE.54.12.124102},
} 


@inproceedings{20183605771587,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Integrated methodology for the analysis of fuel integrity in high burnup fuel assemblies using CTF-UPVIs, Frapcon and Fraptran codes},
journal = {17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics, NURETH 2017},
author = {Hidalga, P. and Abarca, A. and Miro, R. and Verdu, G.},
volume = {2017-September},
year = {2017},
pages = {Atomic Energy Society of Japan; Canadian Nuclear Society; JSME; Sociedad Nuclear Espanola (SNE); Sociedad Nuclear Mexicana (SNM) - },
address = {Xi'an, Shaanxi, China},
abstract = {Detailed fuel assembly description is increasingly important for Light Water Reactor (LWR) deterministic safety analysis in order to represent realistically the complexity of the high burn-up fuel designs. This need involves having the appropriate computer tools to simulate accurately the behavior of the fuel elements during the normal and accidental operational conditions. The subchannel analysis brings out a proper scale level for analyzing the phenomena occurring in the core during operation. Using this scale with thermo-mechanics and thermal-hydraulics analysis allows evaluating the safety parameters related to coolant and fuel rod properties. The sub-channel analysis defines the node mesh regarding the fuel pin and the coolant surrounding it. When using a thermal-hydraulic code at this level it can be observed the coolant and fuel behavior and how its properties change along the fuel assembly. Interesting parameters such as the Departure of Nucleate Boiling Ratio (DNBR), Critical Heat Flux (CHF), Critical Quality (CQ) or Peak Cladding Temperature (PCT) can be obtained for further safety analysis. On the other hand, thermo-mechanic codes at this scale are useful to calculate the fuel integrity and the hydrogen generation during the simulated transient. The fuel integrity can be evaluated using parameter like the cladding ballooning, elongation and stress, whereas the cladding corrosion and hydrogen generation are directly evaluated from the kinetics of the cladding metal electrochemical reaction. The objective of this work is to develop a procedure for evaluating the fuel integrity during normal operating conditions or during accidental transients using the sub-channel thermal-hydraulic code CTF-UPVIS (ISIRYM-UPV version of COBRA-TF (CTF) code) and the fuel performance codes FRAPCON/FRAPTRAN. The combination of detailed sub-channel thermal-hydraulics and fuel performance in a single methodology will increase the accuracy for the transient calculations as well as speeding up the simulation by means of automating the data exchange in a two-step process. The developed methodology will be tested in two scenarios: during the normal operating conditions and during a fast transient in a BWR reactor core. For both scenarios, the safety limits are calculated and presented in this work as an example of the capabilities of the developed tool.<br/> &copy; 2016 Association for Computing Machinery Inc. All Rights Reserved.},
key = {Light water reactors},
keywords = {Boiling water reactors;Cladding (coating);Codes (symbols);Coolants;Corrosion;Electronic data interchange;Fuels;Heat flux;Hydraulics;Hydrogen production;Nuclear fuel elements;Nuclear reactor licensing;Quality control;Reaction kinetics;Reactor cores;},
note = {Coupled;Multi-scale;Neutronics;Safety analysis;Thermal hydraulics;},
} 


@inproceedings{20164803066121,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Effect of block valve and crack arrestor spacing on thermal radiation hazards associated with ignited rupture incidents for natural gas pipelines},
journal = {Proceedings of the Biennial International Pipeline Conference, IPC},
author = {Rothwell, Brian and Dessein, Thomas and Collard, Andy},
volume = {2},
year = {2016},
pages = {Pipeline Division - },
address = {Calgary, AB, Canada},
abstract = {A study was undertaken to evaluate crack arrestor and mainline block valve (MLBV) spacing distances beyond the limits defined in the 49 CFR Part 192 for Class 1 locations for the design of a 42-inch (1,067-mm) OD arctic pipeline. The study assessed whether an MLBV spacing longer than that required by 49 CFR Part 192 for Class 1 locations can provide a level of safety equivalent to that afforded by the spacing recommended in the code. This was accomplished by comparing the hazards in terms of the volume of natural gas released over time, the potential for damage to surrounding structures, and the life safety risk to personnel and the public. The analysis was performed using the software tool PIPESAFE (version 2.20.0), which was developed for a group of pipeline operators by Advantica Technology (now DNV-GL). A full transient analysis of the flow inside the pipeline and through the rupture opening was carried out with automatic shut-off valve (ASV) closures simulated as boundary condition changes at the locations of the valves triggered by the local transient pressure. Gas outflow rates were fed to a structured flame model that calculates the temperature distribution within the flame and the radiant energy emitted and uses the latter to determine the incident thermal radiation field in the area surrounding the rupture, the associated hazard areas and the accumulated thermal radiation dosage over time. These results were compiled into contour plots of thermal radiation intensity for different times; plots of the total area within specific contours of thermal radiation intensity for different times; and plots of the total area within specific contours of accumulated dosage. The dosage-Area curves facilitate a direct comparison of the various MLBV and crack arrestor spacing options considered within this study by providing a simple means to establish if the change in spacing causes a substantial change to the affected areas for dosages up to the limits associated with specific levels of lethality to humans and for piloted ignition of wooden structures. It was found that valve spacing has a strong effect on the time at which closure begins to affect the outflow rate. The decline in flow rate after valve closure had significant influence on the thermal radiation field, but these effects only occurred at a relatively late stage. Increasing fracture length led to considerable changes in the shape of the thermal radiation field, but the total footprint within which casualties might be expected in the event of an ignited rupture release and the severity of injuries within the footprint are unaffected by valve closure under the assumed conditions. Similarly, the damage potential to surrounding buildings was unaffected by valve spacing, indicating that increased valve spacing could be implemented in remote, low population density areas without affecting safety.<br/> &copy; Copyright 2016 by ASME.},
key = {Radiation hazards},
keywords = {Cracks;Equivalence classes;Gas hazards;Heat radiation;Location;Natural gas;Natural gas pipelines;Population statistics;Project management;Radiation effects;Transient analysis;Wooden buildings;},
note = {Level of safeties;Pipeline operator;Population densities;Radiation intensity;Severity of injuries;Surrounding buildings;Transient pressures;Wooden structure;},
URL = {http://dx.doi.org/10.1115/IPC2016-64604},
} 


@inproceedings{20181304965440,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {4th International Conference on Internet Science, INSCI 2017 co-located with IFIN, DATA ECONOMY, DSI, and CONVERSATIONS 2017},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {10750 LNCS},
year = {2018},
issn = {03029743},
address = {Thessaloniki, Greece},
abstract = {The proceedings contain 18 papers. The special focus in this conference is on. The topics include: The Case for Collaborative Policy Experimentation Using Advanced Geospatial Data Analytics and Visualisation; an Engagement-Related Behaviour Change Approach for SavingFood in Greece; developing a Social Innovation Methodology in the Web 2.0 Era; code Hunting Games: A Mixed Reality Multiplayer Treasure Hunt Through a Conversational Interface; politician &ndash; An Imitation Game; Towards Open Domain Chatbots&mdash;A GRU Architecture for Data Driven Conversations; creating Dialogues Using Argumentation and Social Practices; an Overview of Open-Source Chatbots Social Skills; technology Adoption and Social Innovation: Assessing an Online Financial Awareness Platform; aalto Observatory on Digital Valuation Systems: A Position Paper; a Novel Lexicon-Based Approach in Determining Sentiment in Financial Data Using Learning Automata; a Hybrid Recommendation System Based on Density-Based Clustering; computing Platform for Virtual Economic Activities Index; data Based Stock Portfolio Construction Using Computational Intelligence; yourDataStories: Transparency and Corruption Fighting Through Data Interlinking and Visual Exploration; the Maker Movement and the Disruption of the Producer-Consumer Relation.<br/>},
} 


@article{20120614741845,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A multi-scale enriched model for the analysis of masonry panels},
journal = {International Journal of Solids and Structures},
author = {Addessi, Daniela and Sacco, Elio},
volume = {49},
number = {6},
year = {2012},
pages = {865 - 880},
issn = {00207683},
abstract = {A multi-scale model for the structural analysis of the in-plane response of masonry panels, characterized by periodic arrangement of bricks and mortar, is presented. The model is based on the use of two scales: at the macroscopic level the Cosserat micropolar continuum is adopted, while at the microscopic scale the classical Cauchy medium is employed. A nonlinear constitutive law is introduced at the microscopic level, which includes damage, friction, crushing and unilateral contact effects for the mortar joints. The nonlinear homogenization is performed employing the Transformation Field Analysis (TFA) technique, properly extended to the macroscopic Cosserat continuum. A numerical procedure is developed and implemented in a Finite Element (FE) code in order to analyze some interesting structural problems. In particular, four numerical applications are presented: the first one analyzes the response of the masonry Representative Volume Element (RVE) subjected to a cyclic loading history; in the other three applications, a comparison between the numerically evaluated response and the micromechanical or experimental one is performed for some masonry panels. &copy; 2011 Elsevier Ltd. All rights reserved.<br/>},
key = {Nonlinear analysis},
keywords = {Friction;Masonry construction;Mathematical transformations;Mortar;},
note = {Cosserat continuum;Damage-friction;Masonry;Multi-scale Modeling;Nonlinear homogenization;},
URL = {http://dx.doi.org/10.1016/j.ijsolstr.2011.12.004},
} 


@inproceedings{20170803373289,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An improved method to calculate the nonlinear rolling moment due to differential fin deflection of canard controlled missiles},
journal = {AIAA Atmospheric Flight Mechanics Conference, 2015},
author = {Moore, F.G. and Moore, L.Y. and McGowan, Gregory},
year = {2015},
address = {Chicago, IL, United states},
abstract = {An Improved Method to Calculate the Nonlinear Rolling Moment Due to Differential Fin Deflection of Canard Controlled Missiles has been developed. The method utilized a Computational Fluid Dynamics Data Base using the wing controlled Seasparrow as a baseline configuration. Improvements to the nonlinear rolling moment incorporated as a result of the new database include: a) accounting for fin interference as a function of angle of attack and Mach number, b) approximating the change in the leeward plane tail fin lateral center of pressure as a function of angle of attack and Mach number, and c) estimating the nonlinear change in rolling moment on the leeward plane tail fin as a function of angle of attack and Mach number. These improvements were incorporated into the 2013 version of the Aeroprediction Code to be released in 2014. Comparison of the improved method to existing approximate techniques and experimental data was made on several configurations. The improved method did a much better job in predicting the nonlinearities in roll moment due to differential fin deflection on all the configurations investigated than existing semiempirical codes, including the 2013 release of the Aeroprediction Code.<br/> &copy; 2015 by Aeroprediction, Inc.},
key = {Vertical stabilizers},
keywords = {Angle of attack;Codes (symbols);Computational fluid dynamics;Fins (heat exchange);Mach number;Missiles;Number theory;},
note = {Aeroprediction codes;Baseline configurations;Center of pressure;Leeward planes;Nonlinear changes;Nonlinear rolling;Rolling moments;Semi-empirical;},
URL = {http://dx.doi.org/10.2514/6.2015-1018},
} 


@inproceedings{20134216854503,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {2013 1st International Workshop on Data Analysis Patterns in Software Engineering, DAPSE 2013 - Proceedings},
journal = {2013 1st International Workshop on Data Analysis Patterns in Software Engineering, DAPSE 2013 - Proceedings},
year = {2013},
address = {San Francisco, CA, United states},
abstract = {The proceedings contain 13 papers. The topics discussed include: building statistical language models of code; commit graphs; concept to commit: a pattern designed to trace code changes from user requests to change implementation by analyzing mailing lists and code repositories; data analysis anti-patterns in empirical software engineering; effect size analysis; exploring software engineering data with formal concept analysis; extracting artifact lifecycle models from metadata history; measure what counts: an evaluation pattern for software data analysis; parametric classification over multiple samples; patterns for cleaning up bug data; patterns for extracting high level information from bug reports; structural and temporal patterns-based features; and the chunking pattern.},
} 


@inproceedings{20153201117411,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A Portable and Fast Stochastic Volatility Model Calibration Using Multi and Many-Core Processors},
journal = {Proceedings of WHPCF 2014: 7th Workshop on High Performance Computational Finance - Held in conjunction with SC 2014: The International Conference for High Performance Computing, Networking, Storage and Analysis},
author = {Dixon, Matthew and Lotze, Jorg and Zubair, Mohammad},
year = {2014},
pages = {23 - 28},
address = {New Orleans, LA, United states},
abstract = {Financial markets change precipitously and on-demand pricing and risk models must be constantly recalibrated to reduce risk. However, certain classes of models are computationally intensive to robustly calibrate to intraday pricesstochastic volatility models being an archetypal example due to the non-convexity of the objective function. In order to accelerate this procedure through parallel implementation,nancial application developers are faced with an ever growing plethora of low-level high-performance computing frameworks such as OpenMP, OpenCL, CUDA, or SIMD intrinsics, and forced to make a trade-off between performance versus the portability,exibility and modularity of the code required to facilitate rapid in-house model development and productionization.This paper describes the acceleration of stochastic volatility model calibration on multi-core CPUs and GPUs using the Xcelerit platform. By adopting a simple dataow programming model, the Xcelerit platform enables the application developer to write sequential, high-level C++ code, without concern for low-level high-performance computing frameworks. This platform provides the portability,exibility and modularity required by application developers. Speedups of up to 30x and 293x are respectively achieved on an Intel Xeon CPU and NVIDIA Tesla K40 GPU, compared to a sequential CPU implementation. The Xcelerit platform implementation is further shown to be equivalent in performance to a low-level CUDA version. Overall, we are able to reduce the entire calibration process time of the sequential implementation from 6; 189 seconds to 183:8 and 17:8 seconds on the CPU and GPU respectively without requiring the developer to reimplement in low-level high performance computing frameworks.<br/> &copy; 2014 IEEE.},
key = {Stochastic models},
keywords = {Application programming interfaces (API);C++ (programming language);Calibration;Cesium;Computer software portability;Economic analysis;Economic and social effects;Finance;Graphics processing unit;Program processors;Risk assessment;Stochastic systems;},
note = {Application developers;GPGPU;High performance computing;Parallel implementations;Platform implementations;Sequential implementation;Stochastic volatility;Stochastic Volatility Model;},
URL = {http://dx.doi.org/10.1109/WHPCF.2014.12},
} 


@inproceedings{20174404358471,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ACM International Conference Proceeding Series},
journal = {ACM International Conference Proceeding Series},
volume = {Part F130951},
year = {2017},
pages = {ACM Special Interest Group on Software Engineering (SIGSOFT); China Computer Federation (CCF); Fudan University - },
address = {Shanghai, China},
abstract = {The proceedings contain 20 papers. The topics discussed include: towards release strategy optimization for apps in Google Play; LogPruner: a tool for pruning logging call in android apps; un-preprocessing: extended CPP that works with your tools; refining traceability links between code and software document; application-centric SSD cache allocation for hadoop applications; AgileRabbit: a feedback-driven offloading middleware for smartwatch apps; learning from internet: handling uncertainty in robotic environment modeling; NavyDroid: detecting energy inefficiency problems for smartphone applications; scalable relevant project recommendation on GitHub; and API usage change rules mining based on fine-grained call dependency analysis.<br/>},
} 


@article{20153801298074,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Discrete element simulations of direct shear tests with particle angularity effect},
journal = {Granular Matter},
author = {Zhao, Shiwei and Zhou, Xiaowen and Liu, Wenhui},
volume = {17},
number = {6},
year = {2015},
pages = {793 - 806},
issn = {14345021},
abstract = {This paper investigated the effect of the particle angularity in light of its importance in angular particle assemblies, using the discrete element method (DEM). A discrete element model with a general contact force law for arbitrarily shaped particles was developed, in which angular particles were modeled using convex polyhedra. Quasi-spherical polyhedral shapes with different vertexes were adopted to reflect the change of angularity. Four categories of assemblies with different angularities were generated. A series of direct shear tests performed on these assemblies were simulated at different vertical stresses. All numerical implementations were achieved using a modified version of the open source DEM code YADE. It was found that the macroscopic shear strength and dilatancy characteristics are in agreement with experimental and numerical results in the literature, indicating that the present numerical model is reasonable. Besides, the evolutions of coordination number, normal contact force distribution, and anisotropies of particle orientation and contact normal were investigated. The results show that the angularity plays a vital role in strengthening the interlocking of angular particles.<br/> &copy; 2015, Springer-Verlag Berlin Heidelberg.},
key = {Shear flow},
keywords = {Anisotropy;Finite difference method;Micromechanics;Open systems;},
note = {Angularity;Direct shear test;Discrete element modeling;Discrete element simulation;Discrete elements;Numerical implementation;Particle orientation;Polyhedral particles;},
URL = {http://dx.doi.org/10.1007/s10035-015-0593-x},
} 


@inproceedings{20164903100939,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},
journal = {Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},
volume = {2016-November},
year = {2016},
issn = {19436092},
address = {Cambridge, United kingdom},
abstract = {The proceedings contain 54 papers. The topics discussed include: label management: keeping complex diagrams usable; who changed my annotation? an investigation into refitting freeform ink annotation; an evolutionary approach to determining hidden lines from a natural sketch; measuring perceived clutter in concept diagrams; evaluation of a modelling language for customer journeys; an empirical study of user perceived usefulness and preference of open learner model visualisations; learning programming from tutorials and code puzzles: children&#65533;s perceptions of value; coding, reading, and writing: integrated instruction in written language; visual discovery and model-driven explanation of time series patterns; diagnostic visualization for non-expert machine learning practitioners: a design study; supporting end-users in defining complex queries on evolving and domain-specific data models; Yestercode: improving code-change support in visual dataflow programming environments; declarative setup-free web application prototyping combining local and cloud datastores; a domain-specific visual modeling language for testing environment emulation; and trials and tribulations of developers of intelligent systems: a field study.},
} 


@article{20132416420318,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The limited impact of individual developer data on software defect prediction},
journal = {Empirical Software Engineering},
author = {Bell, Robert M. and Ostrand, Thomas J. and Weyuker, Elaine J.},
volume = {18},
number = {3},
year = {2013},
pages = {478 - 505},
issn = {13823256},
abstract = {Previous research has provided evidence that a combination of static code metrics and software history metrics can be used to predict with surprising success which files in the next release of a large system will have the largest numbers of defects. In contrast, very little research exists to indicate whether information about individual developers can profitably be used to improve predictions. We investigate whether files in a large system that are modified by an individual developer consistently contain either more or fewer faults than the average of all files in the system. The goal of the investigation is to determine whether information about which particular developer modified a file is able to improve defect predictions. We also extend earlier research evaluating use of counts of the number of developers who modified a file as predictors of the file's future faultiness. We analyze change reports filed for three large systems, each containing 18 releases, with a combined total of nearly 4 million LOC and over 11,000 files. A buggy file ratio is defined for programmers, measuring the proportion of faulty files in Release R out of all files modified by the programmer in Release R-1. We assess the consistency of the buggy file ratio across releases for individual programmers both visually and within the context of a fault prediction model. Buggy file ratios for individual programmers often varied widely across all the releases that they participated in. A prediction model that takes account of the history of faulty files that were changed by individual developers shows improvement over the standard negative binomial model of less than 0.13% according to one measure, and no improvement at all according to another measure. In contrast, augmenting a standard model with counts of cumulative developers changing files in prior releases produced up to a 2% improvement in the percentage of faults detected in the top 20% of predicted faulty files. The cumulative number of developers interacting with a file can be a useful variable for defect prediction. However, the study indicates that adding information to a model about which particular developer modified a file is not likely to improve defect predictions. &copy; 2011 Springer Science+Business Media, LLC.<br/>},
key = {Forecasting},
keywords = {Defects;Regression analysis;},
note = {Buggy file ratio;Empirical studies;Fault percentile averages;Fault-prone;Regression model;Software fault;},
URL = {http://dx.doi.org/10.1007/s10664-011-9178-4},
} 


@article{20124515646081,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Polarization of light scattered by large aggregates},
journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
author = {Kolokolova, Ludmilla and Mackowski, Daniel},
volume = {113},
number = {18},
year = {2012},
pages = {2567 - 2572},
issn = {00224073},
abstract = {Study of cosmic dust and planetary aerosols indicate that some of them contain a large number of aggregates of the size that significantly exceeds the wavelengths of the visible light. In some cases such large aggregates may dominate in formation of the light scattering characteristics of the dust. In this paper we present the results of computer modeling of light scattering by aggregates that contain more than 1000 monomers of submicron size and study how their light scattering characteristics, specifically polarization, change with phase angle and wavelength. Such a modeling became possible due to development of a new version of Multi Sphere T-Matrix (MSTM) code for parallel computing. The results of the modeling are applied to the results of comet polarimetric observations to check if large aggregates dominate in formation of light scattering by comet dust. We compare aggregates of different structure and porosity. We show that large aggregates of more than 98% porosity (e.g. ballistic cluster-cluster aggregates) have angular dependence of polarization almost identical to the Rayleigh one. Large compact aggregates (less than 80% porosity) demonstrate the curves typical for solid particles. This rules out too porous and too compact aggregates as typical comet dust particles. We show that large aggregates not only can explain phase angle dependence of comet polarization in the near infrared but also may be responsible for the wavelength dependence of polarization, which can be related to their porosity. &copy; 2012 Elsevier Ltd.},
key = {Aggregates},
keywords = {Agglomeration;Dust;Light scattering;Models;Parallel architectures;Parallel processing systems;Polarization;Porosity;},
note = {Angular dependence;Comet dust particles;Cosmic dusts;Different structure;Large aggregates;Near Infrared;Phase angle dependence;Phase angles;Rayleigh;Results of computer modeling;Scattering char-acteristics;Solid particles;Submicron size;T-matrix;Visible light;Wavelength dependence;},
URL = {http://dx.doi.org/10.1016/j.jqsrt.2012.02.002},
} 


@inproceedings{20141517551375,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {RHINO cluster control and management system},
journal = {IEEE AFRICON Conference},
author = {Chiriseri, Valerie and Winberg, Simon and Rajan, Shanly},
year = {2013},
issn = {21530025},
address = {Pointe-Aux-Piments, Mauritius},
abstract = {The objective of this paper is to present the RHINO ARM API Cluster Control System (RAACMS) that will enable a user to access and control networked Reconfigurable Hardware Interface for computing and radio (RHINO) platforms. The framework is designed to run on a reconfigurable platforms consisting of FPGA and an ARM processor connected in a cluster. This system is built around a client-server design, and includes an API on the ARM and control PC, that enables users to execute their code on the control PC and control and change variables on a cluster of RHINO platforms. This paper will present the design and implementation of the RAACMS on the cluster of RHINOs at the University of Cape Town. Tests are performed on a prototype version of the framework. The conclusions discuss uses of the systems, together with further plans for improving the framework. &copy; 2013 IEEE.<br/>},
key = {Networked control systems},
keywords = {Access control;Application programming interfaces (API);ARM processors;Cluster computing;Field programmable gate arrays (FPGA);Integrated circuit design;Radio astronomy;Reconfigurable hardware;Signal processing;},
note = {Client server;Cluster control;Cluster management system;Control PC;Design and implementations;Prototype versions;Reconfigurable plat-forms;University of Cape Town;},
URL = {http://dx.doi.org/10.1109/AFRCON.2013.6757640},
} 


@inproceedings{20173003986547,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Technology, education and access: A 'fair go' for peoplewith disabilities},
journal = {Proceedings of the 14th Web for All Conference, W4A 2017},
author = {Hollier, Scott},
year = {2017},
pages = {et al.; Google; IBM; Intuit; Microsoft; The Paciello Group - },
address = {1st Floor, 300 Murray Street, Perth, WA, Australia},
abstract = {Australians are renowned for their cultural quirks, some of which include abbreviating everyday words such as 'breakfast' becoming 'brekky' and McDonalds becoming 'Maccas'. Australians also love sport and feature their own unique version of football, which to the untrained eye could be mistaken for a game whereby 36 players fight over a rugby-shaped ball for three hours while trying to kick the ball between two large posts. However, there's one aspect of Australian culture which effectively represents the core values of the nation - the importance of a 'fair go'. The concept of a 'fair go' is not about becoming a leader, achieving fame or fortune. Rather it's about the ability to achieve the everyday things in an environment that provides neither advantage or disadvantage. This could include having a regular income, a home, a good job and a family. For Dr. Scott Hollier, his early years suggested that he was destined for a fair go in life, growing up in a well-educated middle-income family in the hills area of Perth, Australia However when he was diagnosed with the Retinitis Pigmentosa eye condition at the age of five, it was a shock to his parents; they were told that Scott would go blind almost immediately, he would need to go to a 'special' school and his future career prospects were bleak. While each of these statements were cause for concern in itself, for Scott's parents the biggest issue was that it all represented the concern that Scott would not receive a fair go in life as a result of his disability. Scott's parents went and sought a second opinion about the diagnosis at which point it was discovered that there was no immediate need to change his schooling, and as the future was unwritten, the hope for a fair go was restored. As Scott progressed through primary and high school, he discovered a love for computers and video games, leading to his interest in Computer Science. While his initial exposure to the possibilities of technology supporting is disability came whit software Automated Mouth (SAM) software on the Commodore 64, it was a discussion in the comp.sys.cbm newsgroup that sparked Scott's interest in the potential of accessibility. Scott describes the experience in his memoir 'Outrunning the Night' as follows: "One of the posts that I found particularly interesting was by someone asking how they could get their Commodore 64 on the Internet. From memory, this post led to a few responses, most of which were either critical of the author for making the ridiculous suggestion...or from others viewing it as a deliberate attempt to 'flame' the group. After a few posts, though, someone posted code in BASIC that would create a TCP/IP stack, and various primitive connectivity options to get it online. It was at this point that it dawned on me [if] the world could come together to get a computer from 1982 on the Internet, imagine the possibilities if similar efforts were put into using technologies to support people with disabilities." It became clear to Scott at this point that the power of education and technology had the potential to ensure that people with disabilities could get a fair go if the world community came together to support it. After Scott completed his Computer Science degree he worked in the information technology industry for six years at which point he considered the implications for emerging consumer technologies and how beneficial it could be for employment. Scott undertook additional studies which ultimately led to studying a PhD in the field. Once completed, Scott moved to Sydney with his wife and two children to start a new position with a not-for-profit organisation in 2008. With rapid evolution of policy and consumer technologies including the release of the Web Content Accessibility Guidelines (WCAG) 2.0 in 2008, the first mainstream accessible touchscreen device with the iPhone 3GS in 2009 and the inclusion of accessibity features in popular operating systems including Windows MacOS, iOS and Android, Scott was able to focus his work on highlighting to people with disabilities that not only were previously expensive assistive technologies now built-in to popular everyday products, but people with disabilities had the same consumer choices as the general public. While Scott enjoyed supporting consumers with disabilities in embracing the benefits technology can provide, Scott also focused on supporting the ICT community in creating accessible content online. However, one challenge in seeking to actively participate in this space was the time zones in Australia for meeting with W3C working groups. With Scott returning to Perth in 2010 to establish a new office for his employer and more recently becoming an independent consultant and researcher, Scott has had the opportunity to become an active participant with the Research Questions Task Force (RQTF) despite the late-night teleconference calls. Scott encourages W3C and other organisations to considerhow to improve engagement with the region as the +8UTC time zone of Perth is the world's most populated time zone including Indonesia, Malaysia and China. Ultimately people in parts of the world outside of Europe and North America need to be given a fair go in terms of participation and Scott has volunteered to work with W3C and others to make improvements in this area. However, while issues of community participation remain, the challenges of people with disabilities continue to be addressed with the arrival of new technologies and platforms such as social media. Scott recalls a project called Sociability whereby he interviewed 49 people about their access to social media, and it became clear that for people with disabilities, such platforms genuinely provided a fair go. In some cases, it was about providing support to a hearing-impaired woman in socialising at parties with the help of Facebook, for a blind man it was about keeping up with industry knowledge using LinkedIn. Yet for another it was about not being able to drive and using social media to find cheap pizza coupons so dinner could be delivered. Moving forward, Scott considers that the arrival of digital assistants in the home and the Internet of Things (IoT) provides the next frontier of engagement whereby people with disabilities have one more interface option to communicate with devices. For example, if a person in a wheelchair can't reach the buttons on a microwave, or a blind person can't read the display on a washing machine, the ability to talk to devices or use digital assistants such as Google Home provides even more opportunities for people to get a fair go. The final point highlighted by Scott is that the opportunity of a fair go is only possible if people come together to make accessibility happen, With this in mind, Scott has highlighted the importance of people attending conferences such as W4A as they have effectively dedicated their lives and careers to support people with disabilities, while often people working in this research space do not always get the chance to see the practical outcomes of their work at the 'coal-face', Scott has offered his thanks both professionally and personally, to the hard work and dedication of people that have given him and people with disabilities all over the world the opportunity for a fair go.Copyright is held by the owner/author(s).<br/>},
key = {Engineering education},
keywords = {Audition;Computer games;Digital devices;Display devices;Internet of things;Medical problems;Social networking (online);Sports;},
note = {Assistive technology;Community participation;Independent consultants;Information technology industry;Internet of thing (IOT);People with disabilities;Retinitis pigmentosa;Web content accessibility guidelines;},
URL = {http://dx.doi.org/10.1145/3058555.3058557},
} 


@inproceedings{20120814797552,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evaluation of associated and non-associated flow metal plasticity; application for DC06 deep drawing steel},
journal = {Key Engineering Materials},
author = {Safaei, Mohsen and Waele, Wim De and Zang, Shun-Lai},
volume = {504-506},
year = {2012},
pages = {661 - 666},
issn = {10139826},
address = {Erlangen, Germany},
abstract = {In this paper the capabilities of Associated Flow Rule (AFR) and non-AFR based finite element models for sheet metal forming simulations is investigated. In case of non-AFR, Hill's quadratic function used as plastic potential function, makes use of plastic strain ratios to determine the direction of effective plastic strain rate. In addition, the yield function uses direction dependent yield stress data. Therefore more accurate predictions are expected in terms of both yield stress and strain ratios at different orientations. We implemented a modified version of the non-associative flow rule originally developed by Stoughton [1] into the commercial finite element code ABAQUS by means of a user material subroutine UMAT. The main algorithm developed includes combined effects of isotropic and kinematic hardening [2]. This paper assumes proportional loading cases and therefore only isotropic hardening effect is considered. In our model the incremental change of plastic strain rate tensor is not equal to the incremental change of the compliance factor. The validity of the model is demonstrated by comparing stresses and strain ratios obtained from finite element simulations with experimentally determined values for deep drawing steel DC06. A critical comparison is made between numerical results obtained from AFR and non-AFR based models.&copy; (2012) Trans Tech Publications.<br/>},
key = {Strain rate},
keywords = {ABAQUS;Deep drawing;Drawing (forming);Finite element method;Hardening;Plastic deformation;Sheet metal;Yield stress;},
note = {Associated flow rule;Commercial finite element codes;Finite element simulations;Material constitutive models;Non-associated;Plastic potential function;User material subroutine;Yield function;},
URL = {http://dx.doi.org/10.4028/www.scientific.net/KEM.504-506.661},
} 


@inproceedings{20142717895033,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {2013 International Conference on Information Engineering, ICIE 2013},
journal = {WIT Transactions on Information and Communication Technologies},
volume = {49},
year = {2014},
pages = {WIT Transactions on Information and Communication Technologies - },
issn = {17433517},
address = {Hong Kong, China},
abstract = {The proceedings contain 192 papers. The special focus in this conference is on Information and Communication Engineering, Electronics Science, Technology, and Application, Computer science and Technology, ICT for Business and Management, and Information Engineering. The topics include: Bidirectional channel assignment for multiradio wireless mesh backhaul; application of neural networks in Taiwan train quali system performance evaluation; an evaluation of tourist attraction ranking methods; joint just noticeable distortion based stereo image watermarking method with self-recovery; decision fusion rule for dynamic large-scale wireless sensor networks; the research of black hole detection in AODV based on NS2; strumming pattern recognition from ukulele songs; shared optical infrastructure for precise time transfer; multicast management in OpenFlow network environment; a survey of code-based and social-based routing protocols in delay tolerant networks; learning strategies of domain ontology concepts from the web; domain-specific evolving network model for complex systems; a community partition algorithm for the network public opinion; spectral analysis of the moving system with multi-object based on V-system; the characteristics of APT attacks and strategies of countermeasure; research on serial assembling scheme for network coding in the internet; joint processing with local channel state information for heterogeneous networks; research on SVM-based securities time series; simulation and verification of high dynamic IF GPS signal; satellites selection method for high-dynamic vector GPS receiver; design and implementation of instant message system based on P2P in LAN; exploring the translation mode for scientific discourses under the framework of information dualism; extraction method for image region of interest based on visual attention model; analysis on the effect of channel transmission rate on communication efficiency; design of AMBA-compliant image scaler circuit for low bus bandwidth; design of improved 8 mm band multimode matching feed; overview of digital watermarking; design of the common framework of LIN low-level driver; image processing in research of the digital human; comparison of SVM and ANN classifier for mammogram classification using ICA features; note on scheduling with group technology and the effects of deterioration and learning; reliability analysis and development of microcontroller unit system; finding all approximate palindromes of the string; an improved algorithm of Cohen-Sutherland line clipping; an improved algorithm of the ID3 based on impact factors; computing semantic relatedness for domain entities from encyclopaedias of digital publishing resource; a new algorithm of generating a minimum spanning tree of a graph based on recursion; performance of adaptive proportional average delay index for scheduler in LTE-A systems; a method for compositing web services based on model-checking multiagent systems; survey on support vector machine algorithms; adaptive fuzzy de-interlacing algorithm with motion detection; a new SVM algorithm and selected application in sequence analysis; image resolution enhancement method using multiple interpolations; design and implementation of handheld wireless LAN analyzer system; a single-field deinterlacing method utilizing second-order derivative; analysis of computer image processing technology and pattern recognition technology; conjoint analysis of statistics and computer technology; application of computer information technology in library information management system; safety protection of computer network and multiport technology; research on partner selection in cloud services supply chain; research of CRB based on grey-relation analysis and its application in aviation fault diagnosis; detecting emotional diction in texts; research on entrepreneurial opportunity identification with technology acceptance model; on building an English website for Tujia traditional culture; enhancing bank reputation by centralizing bank debtor information system; research on promotion of grinding fineness on beneficiation indicators; about the transformation methods on the spectral moments of trees; design of secured U-health application middleware based on OTP; semiautomatic construction of lexicon for the recognition of restaurant attributes; building innovative design service model; developing the knowledge management platform in the APQP procedure; analysis on new enterprise marketing model in the internet age; research on the significance of enterprises human resource management in business process; analysis of tourism enterprises standardization and its long-term development; role of business English on business management; analysis of the relationship between corporate culture and performance management; music appreciation on the construction of enterprise culture; enterprise financial management innovation based on market economy; role of the motivation method in the physical education management of school; analysis on status and suggestion of competitive sports management system; application of humanistic management in sports club; influence of intangible cultural heritages on tourism; personalized management measures in educational reform; three-dimensional visualization technology in landscape design; management system of community sports in the new era; restriction of accounting computerization on financial management; applications of computer digital technology in sports competition; benign development of industry supply chain under financial system; enforcement of scientific management strategy in education; study of strategic enterprise management based on circular economy; role of human-based management in student; the research and implementation of bus monitoring card of fiber channel; legal sentencing as knowledge processing and design of college students personal physical health management system.},
} 


@book{20184906190855,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Software engineering: The current practice},
journal = {Software Engineering: The Current Practice},
author = {Rajlich, Vaclav},
year = {2016},
pages = {1 - 281},
abstract = {Software Engineering: The Current Practice teaches students basic software engineering skills and helps practitioners refresh their knowledge and explore recent developments in the field, including software changes and iterative processes of software development. After a historical overview and an introduction to software technology and models, the book discusses the software change and its phases, including concept location, impact analysis, refactoring, actualization, and verification. It then covers the most common iterative processes: agile, directed, and centralized processes. The text also journeys through the software life span from the initial development of software from scratch to the final stages that lead toward software closedown. For Professionals. The book gives programmers and software managers a unified view of the contemporary practice of software engineering. It shows how various developments fit together and fit into the contemporary software engineering mosaic. The knowledge gained from the book allows practitioners to evaluate and improve the software engineering processes in their projects. For Instructors. Instructors have several options for using this classroom-tested material. Designed to be run in conjunction with the lectures, ideas for student projects include open source programs that use Java or C++ and range in size from 50 to 500 thousand lines of code. These projects emphasize the role of developers in a classroom-tailored version of the directed iterative process (DIP). For Students. Students gain a real understanding of software engineering processes through the lectures and projects. They acquire hands-on experience with software of the size and quality comparable to that of industrial software. As is the case in the industry, students work in teams but have individual assignments and accountability.<br/> &copy; 2012 by Taylor & Francis Group, LLC. All rights reserved.},
key = {C++ (programming language)},
keywords = {Iterative methods;Open source software;Open systems;Software design;Students;Verification;},
note = {Contemporary practices;Individual assignments;Industrial software;Initial development;Iterative process;Open source projects;Software engineering process;Software technology;},
} 


@inproceedings{20164502976877,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {23rd International Conference on Neural Information Processing, ICONIP 2016},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {9950 LNCS},
year = {2016},
pages = {1 - 662},
issn = {03029743},
address = {Kyoto, Japan},
abstract = {The proceedings contain 296 papers. The special focus in this conference is on Applications, Computational, Cognitive Neurosciences, Theory and Algorithms. The topics include: Classifying human activities with temporal extension of random forest; unregistered bosniak classification with multi-phase convolutional neural networks; data analysis of correlation between project popularity and code change frequency; prediction of bank telemarketing with co-training of mixture-of-experts and MLP; android malware detection method based on function call graphs; fast color quantization via fuzzy clustering; topological order discovery via deep knowledge tracing; the effect of reward information on perceptual decision-making; an internal model of the human hand affects recognition of graspable tools; a framework for ontology based management of neural network as a service; bihemispheric cerebellar spiking network model to simulate acute VOR motor learning; speaker detection in audio stream via probabilistic prediction using generalized GEBI; attention estimation for input switch in scalable multi-display environments; fissionable deep neural network; on the singularity in deep neural networks; compressing word embeddings; self-organization on a sphere with application to topological ordering of Chinese characters; the ability of learning algorithms for fuzzy inference systems using vector quantization; data-based optimal tracking control of nonaffine nonlinear discrete-time systems and rule-based grass biomass classification for roadside fire risk assessment.},
} 


@inproceedings{20163502740425,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The application of ICASPA to critical and Safe mass calculations},
journal = {ICNC 2015 - International Conference on Nuclear Criticality Safety},
author = {Putley, D. and Martin, J.S. and Henderson, M.},
year = {2015},
pages = {1178 - 1187},
address = {Charlotte, NC, United states},
abstract = {ICASPA, an Improved Critical and Safe Parameters Algorithm, calculates the size of a fissile body for a given value of k-effective. ICASPA uses an iterative predictor/corrector algorithm in which the MONK9A code is used to derive a precise k-effective value for a given candidate body size; auxiliary equations are then used to adjust the body size. The process is repeated until MONK9A outputs the chosen target value of k-effective. EDF Energy operates eight nuclear power stations comprising one Pressurized Water Reactor (PWR) and seven Advanced Gas-cooled Reactor (AGR) stations. At these sites, the term "fuel route" is used to encompass all of the ex-reactor arrangements for the handling and storage of fuel. The criticality safety cases for these fuel routes are based on the use of geometrically safe arrangements for the required quantities of fuel. The allowed quantities and arrangements of fuel are safely sub-critical, even under the contingency of accidental moderator ingress. The fuel route safety cases consider all credible potential fault conditions, including events such as dropped fuel. Any such events might change the geometry of the fuel elements, but it is difficult to calculate exact k-effective values for realistic models of dropped fuel. This difficulty is overcome by the use of an alternative approach in which limiting safe masses are calculated for bounding (worst-case) rearrangements of fuel rods and moderators, using geometries similar to those covered by criticality handbook data. Where required, the resulting safe masses are used to set safety case limits and conditions. For example, some fire extinguishants, such as water and ABC dry powder, are also effective moderators. Hence, in some fuel route areas, administrative criticality controls are used to either prohibit or limit the permitted quantities of these materials. This paper describes the work that has been carried out to adopt ICASPA as the approved calculation method for these safe mass calculations. The work has been carried out within the company quality assurance requirements for nuclear safety related software. It has involved the development of a configuration controlled version of ICASPA and the local verification and validation of the resulting code. The benefits from the adoption of ICASPA are also presented. These include accuracy improvements and, because the method uses the MONK9A code for k-effective calculations, the ability to use more sophisticated models for limiting case geometries.<br/>},
key = {Pressurized water reactors},
keywords = {Anthropometry;Codes (symbols);Computer software selection and evaluation;Criticality (nuclear fission);Digital storage;Fuel storage;Gas cooled reactors;Geometry;Iterative methods;Moderators;Nuclear fuels;Quality assurance;Safety engineering;},
note = {Accuracy Improvement;Assurance requirements;Auxiliary equations;Criticality safety;Potential faults;Realistic model;Software Quality;Verification-and-validation;},
} 


@article{20162002381480,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Tuning for software analytics: Is it really necessary?},
journal = {Information and Software Technology},
author = {Fu, Wei and Menzies, Tim and Shen, Xipeng},
volume = {76},
year = {2016},
pages = {135 - 146},
issn = {09505849},
abstract = {Context: Data miners have been widely used in software engineering to, say, generate defect predictors from static code measures. Such static code defect predictors perform well compared to manual methods, and they are easy to use and useful to use. But one of the "black arts" of data mining is setting the tunings that control the miner. Objective: We seek simple, automatic, and very effective method for finding those tunings. Method: For each experiment with different data sets (from open source Java systems), we ran differential evolution as an optimizer to explore the tuning space (as a first step) then tested the tunings using hold-out data. Results: Contrary to our prior expectations, we found these tunings were remarkably simple: it only required tens, not thousands, of attempts to obtain very good results. For example, when learning software defect predictors, this method can quickly find tunings that alter detection precision from 0% to 60%. Conclusion: Since (1) the improvements are so large, and (2) the tuning is so simple, we need to change standard methods in software analytics. At least for defect prediction, it is no longer enough to just run a data miner and present the result without conducting a tuning optimization study. The implication for other kinds of analytics is now an open and pressing issue.<br/> &copy; 2016 Elsevier B.V. All rights reserved.},
key = {Open systems},
keywords = {Data mining;Decision trees;Defects;Evolutionary algorithms;Miners;Open source software;Optimization;},
note = {CART;Defect prediction;Differential Evolution;Random forests;Search-based software engineering;},
URL = {http://dx.doi.org/10.1016/j.infsof.2016.04.017},
} 


@inproceedings{20160301831109,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Analysis of turbofan performance under total pressure distortion at various operating points},
journal = {Proceedings of the ASME Turbo Expo},
author = {Weston, David B. and Gorrell, Steven E. and Marshall, Matthew L. and Wallis, Carol V.},
volume = {2A},
year = {2015},
pages = {International Gas Turbine Institute - },
address = {Montreal, QC, Canada},
abstract = {Inlet distortion is an important consideration in fan performance. The focus of this paper is a series of high-fidelity time accurate Computational Fluid Dynamics (CFD) simulations of a multistage fan at choke, design, and near stall operating conditions. These investigate distortion transfer and generation as well as the underlying flow physics of these phenomena under different operating conditions. The simulations are performed on the full annulus of a 3 stage fan and are analyzed. The code used to carry out these simulations is a modified version of OVERFLOW 2.2. The inlet is specified as a 1/rev total pressure distortion. Analysis includes the phase and amplitude of total temperature and pressure distortion through each stage of the fan and blade loading. The total pressure distortion does not change in severity through the fan, but the peak pressure distortion rotates by as much as 45&deg; at the near stall point. This is due to a variation in the work input around the blades of the rotor. This variation is also responsible for the generation of total temperature distortion in the fan. The rotation of the total temperature distortion becomes more pronounced as the fan approaches stall, and the total temperature distortion levels increase. The amount of work performed by a single blade can vary by as much as 25% in the first stage at near stall. The variation in work becomes more pronounced as the fan approaches stall. The passage shock in the rotor blades moves nearly 20% of the blade chord in both the peak efficiency and near stall cases.<br/> Copyright &copy; 2015 by ASME.},
key = {Computational fluid dynamics},
keywords = {Gas turbines;},
note = {Computational fluid dynamics simulations;Different operating conditions;Fan performance;Inlet distortion;Operating condition;Operating points;Total pressure distortion;Total temperatures;},
URL = {http://dx.doi.org/10.1115/GT2015-42879},
} 


@inproceedings{20163502743571,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {EarSketch: An authentic, STEAM-based approach to computing education},
journal = {ASEE Annual Conference and Exposition, Conference Proceedings},
author = {Moore, Roxanne and Edwards, Douglas and Freeman, Jason and Magerko, Brian and McKlin, Tom and Xambo, Anna},
volume = {2016-June},
year = {2016},
address = {New Orleans, LA, United states},
abstract = {Demand for computer scientists is robust, but the pipeline for producing them is not. US universities are only meeting about a third of demand for computer scientists, and recruiting a diverse student body is a struggle; the number of women in computer science has actually declined in the past decade. To help change the perception of the computing field, researchers at Georgia Institute of Technology developed EarSketch. EarSketch is an authentic STEAM (STEM + Arts) environment for teaching and learning programming (i.e. where learners are engaged in authentic practices both in computing and in the aesthetics of music remixing) aimed at increasing and broadening participation in computing. In the EarSketch environment, students write code to manipulate, or remix, musical samples. It is an integrated programming environment, digital audio workstation, curriculum, and audio loop library. EarSketch has already been piloted in multiple classroom environments, including Computer Science Principles (CSP) classes in Atlanta-area high schools, in summer workshops, as part of a MOOC music technology course, in undergraduate computing courses for non-majors, and in a graduate digital media course at Georgia Tech. EarSketch is unique from other STEAM projects in computing education in that it is authentic both from an artistic perspective and from a computing perspective. That is, students create music in popular, personally relevant styles and genres, like dubstep and techno, and also learn to code in an industry-relevant language, like Python or JavaScript, in a free, browser-based environment. In addition, the barriers to entry are kept low; no previous knowledge of music performance or composition is required to engage successfully with EarSketch. In this paper, we present a description of the EarSketch environment and curriculum. We also present an overview of the classroom environments in which EarSketch has been implemented to date, including professional development feedback, student artifacts, student engagement data, and student achievement. The authors believe that EarSketch has the potential to serve as an introductory programming unit for a variety of courses in both pre-college and college settings. Based on initial data, EarSketch is an effective method for teaching programming of musical content and is effective in improving motivation to succeed on computing problems. &copy; American Society for Engineering Education, 2016.},
} 


@article{20130215892617,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Interactive topology optimization on hand-held devices},
journal = {Structural and Multidisciplinary Optimization},
author = {Aage, Niels and Nobel-Jorgensen, Morten and Andreasen, Casper Schousboe and Sigmund, Ole},
volume = {47},
number = {1},
year = {2013},
pages = {1 - 6},
issn = {1615147X},
abstract = {This paper presents an interactive topology optimization application designed for hand-held devices running iOS or Android. The TopOpt app solves the 2D minimum compliance problem with interactive control of load and support positions as well as volume fraction. Thus, it is possible to change the problem settings on the fly and watch the design evolve to a new optimum in real time. The use of an interactive app makes it extremely simple to learn and understand the influence of load-directions, support conditions and volume fraction. The topology optimization kernel is written in C# and the graphical user interface is developed using the game engine Unity3D. The underlying code is inspired by the publicly available 88 and 99 line Matlab codes for topology optimization but does not utilize any low-level linear algebra routines such as BLAS or LAPACK. The TopOpt App can be downloaded on iOS devices from the Apple App Store, at Google Play for the Android platform, and a web-version can be run from www.topopt.dtu.dk. &copy; 2012 Springer-Verlag.<br/>},
key = {Android (operating system)},
keywords = {Compliance control;Constrained optimization;Graphical user interfaces;Hand held computers;iOS (operating system);Linear algebra;MATLAB;Shape optimization;Smartphones;Topology;Volume fraction;},
note = {Android platforms;Hand held device;Interactive control;Interactiveness;Minimum compliance;PDE-constrained optimization;Support conditions;Tablets;},
URL = {http://dx.doi.org/10.1007/s00158-012-0827-z},
} 


@inproceedings{20171703594292,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
year = {2017},
issn = {15301362},
address = {Hamilton, New zealand},
abstract = {The proceedings contain 58 papers. The topics discussed include: task recommendation with developer social network in software crowdsourcing; EXPSOL: recommending online threads for exception-related bug reports; retrieving design pattern usage examples using domain matching; LibSift: automated detection of third-party libraries in Android applications; does the role matter? an investigation of the code quality of casual contributors in GitHub; a model checking based approach for containment checking of UML sequence diagrams; model driven software security architecture of systems-of-systems; analytical study of cognitive layered approach for understanding security requirements using problem domain ontology; a map of threats to validity of systematic literature reviews in software engineering; heterogeneous cross-company effort estimation through transfer learning; an algorithmic-based change effort estimation model for software development; achieving high code coverage in Android UI testing via automated widget exercising; testing android apps via guided gesture event generation; model-based API-call constraint checking for automotive control software; and minimalist qualitative models for model checking cyber-physical feature coordination.},
} 


@inproceedings{20143918182925,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Effect of superficial velocity of air and riser cross sectional area on heat transfer characteristics of circulating fluidized bed},
journal = {Lecture Notes in Engineering and Computer Science},
author = {Patil, R.S. and Mahanta, P. and Pandey, M.},
volume = {2},
year = {2014},
pages = {1425 - 1430},
issn = {20780958},
address = {London, United kingdom},
abstract = {The present paper describes a numerical study on wall-to-bed heat transfer characteristics of circulating fluidized bed (CFB) risers of cross section 0.15 (m) &times;0.15 (m), 0.30 (m) &times; 0.30 (m), each of height 2.85 (m). 3-D CFD simulations for heat transfer characteristics were carried out under same operating conditions for heated portion (heater) of risers. For modeling and simulation, CFD code Ansys - Fluent version 13 was used. Modeling and meshing were done using ProE and Ansys ICEM CFD software, respectively. The wall of heater was maintained at the constant heat flux q" = 1000 (W/m2). RNG k-&Epsilon; model was used for turbulence modeling. Gidaspow model for phase interaction was used for the simulation of two phase flow (air + sand mixture flow). Effect of increase in superficial velocity of air from 2.5 (m/s) to 4 (m/s) on heat transfer characteristics was studied experimentally and numerically for the CFB riser of riser of cross section 0.15 (m) x 0.15 (m). Effect of change in riser cross sectional area on heat transfer characteristics was also studied numerically when both the CFB risers were operated under same operating conditions. Results on heat transfer characteristics were obtained In terms of distribution of bed (air + sand mixture) temperature across the heater and local heat transfer coefficient along the height of the heater of the CFB risers. Results obtained through CFD simulations were compared with available experimental data which was obtained using available CFB setup of IIT Guwahati.<br/>},
key = {Fluidized beds},
keywords = {Air;Computational fluid dynamics;Fluidized bed process;Heat flux;Heat transfer coefficients;Mixtures;Two phase flow;},
note = {Bed temperature;CFB riser;CFD simulations;Circulating fluidized bed;Circulating fluidized bed riser;Cross sectional area;Heat transfer characteristics;Local heat transfer coefficient;},
} 


@inproceedings{20173104002712,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ACM International Conference Proceeding Series},
journal = {ACM International Conference Proceeding Series},
volume = {Part F128822},
year = {2013},
address = {Baltimore, MD, United states},
abstract = {The proceedings contain 11 papers. The topics discussed include: sharing information; using evidential reasoning to make qualified predictions of software quality; are comprehensive quality models necessary for evaluating software quality?; a comparative evaluation of static analysis actionable alert identification techniques; using code change types in an analogy-based classifier for short-term defect prediction; training data selection for cross-project defect prediction; incremental development productivity decline; a n analysis of multi-objective evolutionary algorithms for training ensemble models based on different performance measures in software effort estimation; the impact of parameter tuning on software effort estimation using learning machines; an algorithmic approach to missing data problem in modeling human aspects in software development; beyond data mining; towards 'idea engineering'; and building a second opinion: learning cross-company data<br/>},
} 


@article{IP52182201,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An evolutionary framework for culture and creativity: Selectionism versus communal exchange},
journal = {Physics of Life Reviews},
author = {Gabora, Liane},
year = {2012},
issn = {15710645},
abstract = {Dawkins' replicator-based conception of evolution has led to widespread mis-application of selectionism across the social sciences because it does not address the paradox that inspired the theory of natural selection in the first place: how do organisms accumulate change when traits acquired over their lifetime are obliterated? This is addressed by von Neumann's concept of a self-replicating automaton (SRA). An SRA consists of a self-assembly code that is used in two distinct ways: (1) actively deciphered during development to construct a self-similar replicant, and (2) passively copied to the replicant to ensure that it can reproduce. Information that is acquired over a lifetime is not transmitted to offspring, whereas information that is inherited during copying is transmitted. In cultural evolution there is no mechanism for discarding acquired change. Acquired change can accumulate orders of magnitude faster than, and quickly overwhelm, inherited change due to differential replication of variants in response to selection. This prohibits a selectionist but not an evolutionary framework for culture and the creative processes that fuel it. Recent work on the origin of life suggests that early life evolved through a non-Darwinian process referred to as communal exchange that does not involve a self-assembly code, and that natural selection emerged from this more haphazard, ancestral evolutionary process. It is proposed that communal exchange provides an evolutionary framework for culture that enables specification of cognitive features necessary for a (real or artificial) society to evolve culture. This is supported by a computational model of cultural evolution and a conceptual network based program for documenting material cultural history, and it is consistent with high levels of human cooperation. &copy; 2012 Elsevier B.V. All rights reserved.},
key = {History},
keywords = {Biology;Self assembly;},
note = {Computational model;Creative process;Cultural evolution;Early life;Evolutionary framework;Evolutionary process;Natural selection;Network-based;Orders of magnitude;Origin of life;Response to selection;Self-similar;},
URL = {http://dx.doi.org/10.1016/j.plrev.2012.06.005},
} 


@inproceedings{20135117101992,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Investigation on the aerodynamic performance of an annular exhaust system for a small turboshaft engine},
journal = {Proceedings of the ASME Turbo Expo},
author = {De La Calzada, Pedro and Parra, Jorge and Minguez, Belen},
volume = {6 B},
year = {2013},
address = {San Antonio, Tx, United states},
abstract = {An annular exhaust system design for being used in the bench testing of MTR390-E turboshaft engine has been performed at ITP. The exhaust system is aimed at improving the aerodynamic performance at high power compared with an existing exhaust system used in the previous version of the engine. The exhaust cone emulates to some extend the exhaust system in the helicopter and it is comprised of outer and inner cones supported by three struts. The CFD commercial code FLUENT is used to investigate the aerodynamic performance of the baseline design and to optimise the inner and outer cone angles in the new design based on 2D axisymmetric models. Representative radial exit turbine conditions and far field conditions are imposed in the model comprising the exhaust cones plus a large external domain. Two outer and inner cone angles and two inner cone lengths are analysed at low and high power conditions. The aerodynamic performance of the exhaust shows high sensitivity to the inlet flow angle which varies up to 30/40 between the high and low power conditions In all the simulated cases a large separation region is generated after the inner cone. Due to the high swirling flow the separation bubble behind the plug growths downstream hence reducing the effective flow exit area compared with the geometry area and reducing the pressure recovery downstream once the flow has been separated from the inner cone. Although all cases show similar qualitative behaviour, the best case based on the computed figures of merit (i.e., lowest total pressure loss) is chosen for the new design In order to further optimise the behaviour of the exhaust at high power, in the new design the three struts are aligned with the flow angle at high power conditions (struts were axially oriented in the baseline design) and the resulting geometry is analysed by 3D CFD simulations. As expected, the orientation of the struts has a dramatic impact in the aerodynamic behaviour of the exhaust. The new design shows an improvement of 29% in pressure recovery at high power compared with the baseline configuration, although it shows a degradation of 12% at low power. Both the baseline and the new exhaust systems are tested with the real engine in the test bench. The general aerodynamic performance of the new design is compared with the CFD simulation. As a consequence of the design change an important modification in the aerodynamic behaviour of the exhaust is obtained impacting the whole engine performance. Therefore a new performance model of the exhaust system is proposed to be implemented in the whole engine performance model in order to accurately simulate the behaviour of the engine coupled with the new exhaust. &copy; 2013 ASME.<br/>},
key = {Exhaust systems (engine)},
keywords = {Aerodynamics;Computational fluid dynamics;Gas turbines;Jet engines;Struts;},
note = {2d axisymmetric models;Aero-dynamic performance;Baseline configurations;Engine performance;Engine performance models;Far-field conditions;Total-pressure loss;Turbo-shaft engines;},
URL = {http://dx.doi.org/10.1115/GT2013-94022},
} 


@inproceedings{20162402490515,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {AODV Routing Protocol Modification with Dqueue(dqAODV) and Optimization with Neural Network for VANET in City Scenario},
journal = {MATEC Web of Conferences},
author = {Saha, Soumen and Roy, Utpal and Sinha, D.D.},
volume = {57},
year = {2016},
issn = {2261236X},
address = {Sangrur, Punjab, India},
abstract = {Vehicular ad hoc network (VANET) is considered as a sub-set of mobile ad hoc network (MANET). VANET can provide road safety by generating collision warning messages before a collision takes place, lane change assistance; can provide efficient traffic system by introducing cooperation among vehicles; and can also improves in infotainment applications like cooperative file accessing, accessing internet, viewing movies etc. It provides smart Transportation System i.e., wireless ad-hoc communication among vehicles and vehicle to roadside equipments. VANET communication broadly distinguished in two types; 1) vehicle to vehicle interaction, 2) vehicle to infrastructure interaction. The main objective of VANET is to provide safe, secure and automated traffic system. For this automated traffic techniques, there are several types of routing protocols has been developed. MANET routing protocols are not equally applicable in VANET. In the recent past Roy and his group has proposed several study in VANET transmission in [1-3]. In this study, we propose a modified AODV routing protocol in the context of VANET with the help of dqueue introduction into the RREQ header. Recently Saha et al [4] has reported the results showing the nature of modified AODV obtained from the rudimentary version of their simulation code. It is mainly based on packet delivery throughput. It shows greater in-throughput information of packet transmission compare to original AODV. Hence our proposal has less overhead and greater performance routing algorithm compared to conventional AODV. In this study, we propose and implement in the NCTUns-6.0 simulator, the neural network based modified dqueue AODV (dqAODV) routing protocol considering Power, TTL, Node distance and Payload parameter to find the optimal route from the source station (vehicle) to the destination station in VANET communications. The detail simulation techniques with result and output will be presented in the conference.<br/> &copy; Owned by the authors, published by EDP Sciences, 2016.},
key = {Vehicular ad hoc networks},
keywords = {Cooperative communication;Internet protocols;Motor transportation;Network routing;Routing protocols;Vehicle to roadside communications;Vehicle to vehicle communications;Vehicles;},
note = {AODV routing protocol;MANET routing protocols;Packet transmissions;Simulation technique;Smart transportations;Vehicle to vehicles;Vehicle-to-infrastructure;Wireless ad hoc communication;},
URL = {http://dx.doi.org/10.1051/matecconf/20165702001},
} 


@inproceedings{20174104247853,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Work in progress: Teaching design theory and mastercam in a hybrid flipped classroom environment},
journal = {ASEE Annual Conference and Exposition, Conference Proceedings},
author = {Talley, Austin and Talley, Kimberly Grau},
volume = {2017-June},
year = {2017},
issn = {21535965},
address = {Columbus, OH, United states},
abstract = {The revision of the junior-level Computer Aided Design and Manufacturing course MFGE3316 was driven by three forces: ABET, keeping current on an ever-changing software program, and fostering classroom discussion. At an ABET outcomes annual review, the consensus opinion was that the students could effectively design but they needed more practice to better recognize the concepts of engineering design theory. This weakness led to a push for students to practice more with designing and design theory before their senior design courses. Traditionally, MFGE3316 lectured on design theory, fundamentals of CAD/CAM systems, and CNC code generation by CAD/CAM software using a combined class/lab time. The Mastercam software is important for preparing students for industry, but was taking significant classroom time and resources. The revised course pedagogy is a hybrid flipped classroom environment to shift instruction of software use out of the classroom, but the instructor did not have the time or resources to create and continually update video content on how to use Mastercam. Instead, the instructor assigned an "e-text" (SolidProfesor account) for the course. The videos from the e-text are assigned to be watched before coming to class for that topic. During class the instructor does a short overview, leads discussion, and then the students work on the lab. Previously, a significant portion of the lab and class time was devoted to lecturing on software use. This change in pedagogy has allowed more time for in-class discussion and in-class design exercises. As well, the change in presentation style has resulted in more rapid understanding of Mastercam, as evidenced by the semester week in which the class completes the labs. The use of the e-text has also assisted the instructor with keeping class content up-to-date for each new version of the software without having to personally create new videos. The effectiveness of the additional time spent on design theory was assessed with the beginning of semester and end of semester engineering design self-efficacy survey instrument. This instrument was administered to determine if the course and time spent on design had an effect on the students' engineering design self-efficacy.<br/> &copy; American Society for Engineering Education, 2017.},
key = {Computer aided design},
keywords = {Associative storage;Cams;Computer software;Curricula;Design;Engineering education;Students;Teaching;},
note = {Engineering design;Flipped;Flipped classrooms;Hybrid;In-class discussions;Manufacturing course;MasterCAM;Rapid understanding;},
} 


@article{20150100398656,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Numerical modeling of flow boiling instabilities using TRACE},
journal = {Annals of Nuclear Energy},
author = {Kommer, Eric M.},
volume = {76},
year = {2015},
pages = {263 - 270},
issn = {03064549},
abstract = {Dynamic flow instabilities in two-phase systems are a vitally important area of study due to their effects on a great number of industrial applications, including heat exchangers in nuclear power plants. Several next generation nuclear reactor designs incorporate once through steam generators which will exhibit boiling flow instabilities if not properly designed or when operated outside design limits. A number of numerical thermal hydraulic codes attempt to model instabilities for initial design and for use in accident analysis. TRACE, the Nuclear Regulatory Commission's newest thermal hydraulic code is used in this study to investigate flow instabilities in both single and dual parallel channel configurations. The model parameters are selected as to replicate other investigators' experimental and numerical work in order to provide easy comparison. Particular attention is paid to the similarities between analysis using TRACE Version 5.0 and RELAP5/MOD3.3. Comparison of results is accomplished via flow stability maps non-dimensionalized via the phase change and subcooling numbers. Results of this study show that TRACE does indeed model two phase flow instabilities, with the transient response closely mimicking that seen in experimental studies. When compared to flow stability maps generated using RELAP, TRACE shows similar results with differences likely due to the somewhat qualitative criteria used by various authors to determine when the flow is truly unstable.<br/> &copy; 2014 Elsevier Ltd. All rights reserved.},
key = {Two phase flow},
keywords = {Industrial plants;Nuclear fuels;Nuclear power plants;Nuclear reactor accidents;Nuclear reactors;Numerical models;Stability criteria;Steam generators;Transient analysis;},
note = {Flow boiling instabilities;Flow instabilities;Next generation reactor;Nuclear regulatory commission;Once through steam generator;RELAP;TRACE;Two-phase flow instabilities;},
URL = {http://dx.doi.org/10.1016/j.anucene.2014.09.052},
} 


@inproceedings{20140417228660,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Trace based phase prediction for tightly-coupled heterogeneous cores},
journal = {MICRO 2013 - Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture},
author = {Padmanabha, Shruti and Lukefahr, Andrew and Das, Reetuparna and Mahlke, Scott},
year = {2013},
pages = {445 - 456},
address = {Davis, CA, United states},
abstract = {Heterogeneous multicore systems are composed of multiple cores with varying energy and performance characteristics. A controller dynamically detects phase changes in applications and migrates execution onto the most efficient core that meets the performance requirements. In this paper, we show that existing techniques that react to performance changes break down at fine-grain intervals, as performance variations between consecutive intervals are high. We propose a predictive trace-based switching controller that predicts an upcoming phase change in a program and preemptively migrates execution onto a more suitable core. This prediction is based on a phase's individual history and the current program context. Our implementation detects repeatable code sequences to build history, uses these histories to predict an phase change, and preemptively migrates execution to the most appropriate core. We compare our method to phase prediction schemes that track the frequency of code blocks touched during execution as well as traditional reactive controllers, and demonstrate significant increases in prediction accuracy at fine-granularities. For a big-little heterogeneous system that is comprised of a high performing out-of-order core (Big) and an energy-efficient, in-order core (Little), at granularities of 300 instructions, the trace based predictor can spend 28% of execution time on the Little, while targeting a maximum performance degradation of 5%. This translates to an increased energy savings of 15% on average over running only on Big, representing a 60% increase over existing techniques. &copy; 2013 ACM.<br/>},
key = {Energy efficiency},
keywords = {Computer architecture;Controllers;Energy conservation;Forecasting;Reboilers;},
note = {Fine-grained phase;Heterogeneous multi-core systems;Heterogeneous processors;Heterogeneous systems;Performance characteristics;Performance degradation;Performance requirements;Performance variations;},
URL = {http://dx.doi.org/10.1145/2540708.2540746},
} 


@article{20154401488584,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Improved absolute calibration of LOPES measurements and its impact on the comparison with REAS 3.11 and CoREAS simulations},
journal = {Astroparticle Physics},
author = {Apel, W.D. and Arteaga-Velazquez, J.C. and Bahren, L. and Bekk, K. and Bertaina, M. and Biermann, P.L. and Blumer, J. and Bozdog, H. and Brancus, I.M. and Cantoni, E. and Chiavassa, A. and Daumiller, K. and De Souza, V. and Di Pierro, F. and Doll, P. and Engel, R. and Falcke, H. and Fuchs, B. and Gemmeke, H. and Grupen, C. and Haungs, A. and Heck, D. and Hiller, R. and Horandel, J.R. and Horneffer, A. and Huber, D. and Huege, T. and Isar, P.G. and Kampert, K.-H. and Kang, D. and Kromer, O. and Kuijpers, J. and Link, K. and Luczak, P. and Ludwig, M. and Mathes, H.J. and Melissas, M. and Morello, C. and Nehls, S. and Oehlschlager, J. and Palmieri, N. and Pierog, T. and Rautenberg, J. and Rebel, H. and Roth, M. and Ruhle, C. and Saftoiu, A. and Schieler, H. and Schmidt, A. and Schoo, S. and Schroder, F.G. and Sima, O. and Toma, G. and Trinchero, G.C. and Weindl, A. and Wochele, J. and Zabierowski, J. and Zensus, J.A.},
volume = {75},
year = {2016},
pages = {72 - 74},
issn = {09276505},
abstract = {LOPES was a digital antenna array detecting the radio emission of cosmic-ray air showers. The calibration of the absolute amplitude scale of the measurements was done using an external, commercial reference source, which emits a frequency comb with defined amplitudes. Recently, we obtained improved reference values by the manufacturer of the reference source, which significantly changed the absolute calibration of LOPES. We reanalyzed previously published LOPES measurements, studying the impact of the changed calibration. The main effect is an overall decrease of the LOPES amplitude scale by a factor of 2.6 &plusmn; 0.2, affecting all previously published values for measurements of the electric-field strength. This results in a major change in the conclusion of the paper 'Comparing LOPES measurements of air-shower radio emission with REAS 3.11 and CoREAS simulations' published by Apel et al. (2013): With the revised calibration, LOPES measurements now are compatible with CoREAS simulations, but in tension with REAS 3.11 simulations. Since CoREAS is the latest version of the simulation code incorporating the current state of knowledge on the radio emission of air showers, this new result indicates that the absolute amplitude prediction of current simulations now is in agreement with experimental data.<br/> &copy; 2015 Elsevier B.V.},
key = {Cosmic rays},
keywords = {Antenna arrays;Calibration;Cosmic ray detectors;Cosmology;Digital radio;Electric field effects;Gain control;},
note = {Absolute amplitude;Absolute calibration;Cosmic ray air showers;Current simulation;Electric field strength;Extensive air showers;LOPES;Radio emission;},
URL = {http://dx.doi.org/10.1016/j.astropartphys.2015.09.002},
} 


@inproceedings{20121514934878,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Scientific Research - 4th International Conference, S-BPM ONE 2012, Proceedings},
journal = {Lecture Notes in Business Information Processing},
volume = {104 LNBIP},
year = {2012},
pages = {Metasonic AG; VALIAL Solution GmbH; JKU, Competence Center on Knowledge Management; Interact. Acquis., Negot. Enactment Subj.-Oriented; Bus. Process Knowl.(IANES) - EU FP 7 Marie Curie IAPP - },
issn = {18651348},
address = {Vienna, Austria},
abstract = {The proceedings contain 14 papers. The topics discussed include: the subject-oriented approach to software design and the abstract state machines method; ad-hoc adaptation of subject-oriented business processes at runtime to support organizational learning; an approach towards subject-oriented access control; building a conceptual roadmap for systemic change - a novel approach to change management in expert organizations in health care; e-learning support for business process modeling: linking modeling language concepts to general modeling concepts and vice versa; from subject-phase model based process specifications to an executable workflow; modeling business objectives for business process management; stakeholder-driven collaborative modeling of subject-oriented business processes; and using S-BPM for PLC code generation and extension of subject-oriented methodology to all layers of modern control systems.},
} 


@inproceedings{20142317797056,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Fast reactor design using the advanced reactor modeling interface},
journal = {International Conference on Nuclear Engineering, Proceedings, ICONE},
author = {Cheatham, Jesse and Truong, Bao and Touran, Nicholas and Latta, Ryan and Reed, Mark and Petroski, Robert},
volume = {2},
year = {2013},
pages = {Nuclear Engineering Division - },
address = {Chengdu, China},
abstract = {The Advanced Reactor Modeling Interface (ARMI) code system has been developed at TerraPower to enable rapid and robust core design. ARMI is a modular modeling framework that loosely couples nuclear reactor simulations to provide high-fidelity system analysis in a highly automated fashion. Using a unified description of the reactor as input, a wide variety of independent modules run sequentially within ARMI. Some directly calculate results, while others write inputs for external simulation tools, execute them, and then process the results and update the state of the ARMI model. By using a standardized framework, a single design change, such as the modification of the fuel pin diameter, is seamlessly translated to every module involved in the full analysis; bypassing errorprone multi-analyst, multi-code approaches. Incorporating global flux and depletion solvers, subchannel thermalhydraulics codes, pin-level power and flux reconstruction methods, detailed fuel cycle and history tracking systems, finite element-based fuel performance coupling, reactivity coefficient generation, SASSYS-1/SAS4A transient modeling, control rod worth routines, and multi-objective optimization engines, ARMI allows "one click" steady-state and transient assessments throughout the reactor lifetime by a single user. This capability allows a user to work on the full-system design iterations required for reactor performance optimizations that has traditionally required the close attention of a multidisciplinary team. Through the ARMI framework, a single user can quickly explore a design concept and then consult the multi-disciplinary team for model validation and design improvements. This system is in full production use for reactor design at TerraPower, and some of its capabilities are demonstrated in this paper by looking at how design perturbations in fast reactor core assemblies affect steady-state performance at equilibrium as well as transient performance. Additionally, the pin-power profile is examined in the high flux gradient portion of the core to show the impact of the perturbations on pin peaking factors. Copyright &copy; 2013 by ASME.<br/>},
key = {Codes (symbols)},
keywords = {Fast reactors;Fuels;Multiobjective optimization;Nuclear engineering;Systems analysis;},
note = {High fidelity systems;Multi-disciplinary teams;Nuclear reactor simulation;Reactivity coefficients;Steady state and transients;Steady state performance;Thermal hydraulics codes;Transient performance;},
URL = {http://dx.doi.org/10.1115/ICONE21-16815},
} 


@article{20153301165641,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {DEM simulation of direct shear tests considering particle angularity effect},
journal = {Yantu Lixue/Rock and Soil Mechanics},
author = {Zhao, Shi-Wei and Zhou, Xiao-Wen and Liu, Wen-Hui and Liu, Pan},
volume = {36},
year = {2015},
pages = {602 - 608},
issn = {10007598},
abstract = {Particle shape has a significant effect on the microscopic and macroscopic mechanical behavior of granular assemblies. This paper focuses on effects of particle angularity on the mechanical behavior of granular materials and evolution of the contact force anisotropy during direct shear tests. To this end, a discrete polyhedral element model with a general contact force law for arbitrarily shaped bodies is carried out. Particle angularity is defined in terms of the sphericity and the number of vertexes of a polyhedron. Four groups of assemblies with different particle angularities consist of non-cohesive mono-sized reasonably symmetric polyhedral particles. Direct shear tests are simulated using a modified version of the open source DEM code YADE. The results show that: the granular assembly has increasing shear strength and dilatation as particle angularity increases; the effect of angularity on the shear strength and dilative behavior of assemblies is more significant as the vertical loading is larger; the anisotropy of normal contact force increases at the start, then decreasing to remain a relatively stable value during shearing; and a larger change of the anisotropy of normal contact force after shearing is corresponding to a larger angularity.<br/> &copy;, 2015, Academia Sinica. All right reserved.},
key = {Particles (particulate matter)},
keywords = {Anisotropy;Finite difference method;Geometry;Granular materials;Open systems;Shear flow;Shear strength;Shearing;Shearing machines;},
note = {Angularity;Arbitrarily shaped bodies;Direct shear test;Mechanical behavior;Normal contact force;Polyhedral elements;Polyhedral particles;Polyhedron;},
URL = {http://dx.doi.org/10.16285/j.rsm.2015.S1.105},
} 


@inproceedings{20120514723217,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CWSNET: An object-oriented toolkit for water distribution system simulations},
journal = {Water Distribution Systems Analysis 2010 - Proceedings of the 12th International Conference, WDSA 2010},
author = {Guidolin, M. and Burovskiy, P. and Kapelan, Z. and Savic, D.A.},
year = {2012},
pages = {1 - 13},
address = {Tucson, AZ, United states},
abstract = {In the field of water distribution systems the EPANET 2 toolkit is considered nowadays the industry standard for hydraulic modelling. Unfortunately, the design and programming model of EPANET 2 have some limitations that make any attempt to extend its hydraulic solver, add new functionalities or improve performance difficult to achieve and time consuming. A new software toolkit for water distribution system modelling, CWSNet, is presented. CWSNet is developed in C++ using the object-oriented programming model. The aim is to deliver an open-source substitute for EPANET 2 that obtains numerically comparable results while providing similar or better performance, a higher degree of extensibility, as well as backward compatibility where possible. The idea behind this project is to simplify development and testing of new hydraulic elements (specific types of valves, pumps, etc) and computational algorithms (pressure-driven approaches, etc.) by keeping logically independent parts of the code separate. This also allows the performance and accuracy of new computational methods as well as the use of advanced programming techniques (multi-threading, OpenMP, GPGPU, etc) to be studied without the need for extensive code refactoring. The basic version of CWSNet gives numerically the same results as EPANET 2 for various networks while allowing the following: (a) to change the topology of the network at runtime; (b) to run different simulations of the same network or different networks in parallel (thread-safe); (c) to easily change the mathematical model and other particulars behind the hydraulic simulation engine; (d) to allow a high degree of customisation of the output of an extended period simulation. The CWSNet software capabilities are demonstrated using several examples. The results obtained demonstrate the effectiveness and efficiency of the proposed approach. &copy; 2011 ASCE.<br/>},
key = {Object oriented programming},
keywords = {Application programming interfaces (API);C++ (programming language);Computer aided software engineering;Open source software;Program processors;Systems analysis;Topology;Water distribution systems;},
note = {Backward compatibility;Computational algorithm;CWSNet;Development and testing;Effectiveness and efficiencies;EPANET;Extended period simulations;Open sources;},
URL = {http://dx.doi.org/10.1061/41203(425)2},
} 


@inproceedings{20135217133638,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Ptask: An educational C library for programming real-time systems on Linux},
journal = {IEEE International Conference on Emerging Technologies and Factory Automation, ETFA},
author = {Buttazzo, Giorgio and Lipari, Giuseppe},
year = {2013},
pages = {IEEE Industrial Electronics Society; University of Cagliari - },
issn = {19460740},
address = {Cagliari, Italy},
abstract = {When learning real-time programming, the novice is faced with many technical difficulties due to low-level C libraries that require considerable programming effort even for implementing a simple periodic task. For example, the POSIX Real-Time standard only provides a low level notion of thread, hence programmers usually build higher level code on top of the POSIX API, every time re-inventing the wheel. In this paper we present a simple C library that simplifies realtime programming in Linux by hiding low-level details of task creation, allocation and synchronization, and provides utilities for more high-level functionalities, like support for mode-change and adaptive systems. The library is released as open-source and it is currently being employed to teach real-time programming in university courses in embedded systems. &copy; 2013 IEEE.<br/>},
key = {Real time systems},
keywords = {C (programming language);Embedded systems;Factory automation;Frequency standards;Interactive computer systems;Linux;Open source software;Open systems;Teaching;},
note = {Low-level notion;Mode changes;Open sources;Periodic tasks;Real time;Real time programming;Technical difficulties;University course;},
URL = {http://dx.doi.org/10.1109/ETFA.2013.6648001},
} 


@inproceedings{20173504097103,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {1st International Conference on Advanced Informatics for Computing Research, ICAICR 2017},
journal = {Communications in Computer and Information Science},
volume = {712},
year = {2017},
pages = {1 - 367},
issn = {18650929},
address = {Jalandhar, India},
abstract = {The proceedings contain 32 papers. The special focus in this conference is on Advanced Informatics for Computing Research. The topics include: Fuzzy based efficient mechanism for URL assignment in dynamic web crawler; towards filtering of SMS spam messages using machine learning based technique; intelligent computing methods in language processing by brain; classification algorithms for prediction of lumbar spine pathologies; keyword based identification of thrust area using mapreduce for knowledge discovery; an efficient genetic algorithm for fuzzy community detection in social network; priority based service broker policy for fog computing environment; software remodularization by estimating structural and conceptual relations among classes and using hierarchical clustering; requirements traceability through information retrieval using dynamic integration of structural and co-change coupling; bilingual code-mixing in Indian social media texts for Hindi and English; performance evaluation and comparative study of color image segmentation algorithm; electroencephalography based analysis of emotions among Indian film viewers; fuel assembly height measurements at the nuclear power plant unit active zone; a novel approach to segment nucleus of uterine cervix pap smear cells using watershed segmentation; parametric study of various direction of arrival estimation techniques; deep CNN-based method for segmenting lung fields in digital chest radiographs; quality assessment of a job portal system designed using bout design pattern; analyzing factors affecting the performance of data mining tools; stable feature selection with privacy preserving data mining algorithm and an efficient routing protocol for DTN.},
} 


@inproceedings{20141317514314,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Development of a graphical user interface using Windows 7-C# to download Intel-Hex formatted file into the flash of 89S52 microcontroller},
journal = {Proceedings of 2013 2nd International Conference on Advances in Electrical Engineering, ICAEE 2013},
author = {Mostafa, Golam and Abedin, Yeasin},
year = {2013},
pages = {136 - 141},
address = {Dhaka, Bangladesh},
abstract = {ABSTRACT The 89S52 microcontroller (MCU) is very popular among the students, teachers, engineers, scientists and amateurs to build small and cost effective projects for learning purposes and or to incorporate it in a bigger project. In these projects, the chip remains in the system and occasionally need arises to change the program bytes of the code memory (flash) without removing the MCU from the holding instrument. To facilitate entering new codes into the flash without employing expensive commercial ROM Programmer, an interface circuit exists within the 89S52 known as 'In-System Programming Interface (ISP)'. ISP Programming requires two components viz., (i) an interactive GUI (Graphical User) Interface (Fig. 7) at the IBMPC side, and (ii) an auxiliary communication controller (ACC) at the target MCU side (Fig. 1). The GUI interface transfers 'control information (Chip Erase, chip Blank, Chip Write, Chip Read, Lock Security Bits and etc.)' and 'Intel-Hex' formatted program bytes over COM port to the ACC. The ACC decodes the control information, extracts the program bytes and then configures the programming mode of the target MCU. It then activates the ISP interface as per 'Serial Programming Instructions [1]' for writing the received program bytes into the flash of the target MCU. This paper has presented the development procedures of the GUI interface written using C# programming language, which is compatible with Windows 7 operating system. The GUI has been tested in an existing 89S52 based 'CMCKIT: CISC Microcontroller Learning Kit (Fig. 1)' and found to be working as expected. The contents of this paper will encourage the interested readers to learn C# programming language, physical COM port hardware and programming, virtual COM port concept and finally creating new versions of GUI Interfaces for their own ISP Programmers. &copy; 2013 IEEE.<br/>},
key = {Controllers},
keywords = {Cesium;Computer programming languages;Cost effectiveness;Flash memory;Graphical user interfaces;Indium compounds;Microcontrollers;Teaching;Windows operating system;},
note = {89S52 MCU;Communication controllers;Control information;Cost-effective projects;In-system programming;Intel-Hex Frame;Programming instruction;Virtual serial COM port;},
URL = {http://dx.doi.org/10.1109/ICAEE.2013.6750321},
} 


@inproceedings{20170403267616,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Characterization with microturbulence simulations of the zero particle flux condition in case of a TCV discharge showing toroidal rotation reversal},
journal = {Journal of Physics: Conference Series},
author = {Mariani, A. and Merlo, G. and Brunner, S. and Merle, A. and Sauter, O. and Gorler, T. and Jenko, F. and Told, D.},
volume = {775},
number = {1},
year = {2016},
issn = {17426588},
address = {Varenna, Italy},
abstract = {In view of the stabilization effect of sheared plasma rotation on microturbulence, it is important to study the intrinsic rotation that develops in tokamaks that present negligible external toroidal torque, like ITER. Remarkable observations have been made on TCV, analysing discharges without NBI injection, as reported in [A. Bortolon et al. 2006 Phys. Rev. Lett. 97] and exhibiting a rotation inversion occurring in conjunction with a relatively small change in the plasma density. We focus in particular on a limited L-mode TCV shot published in [B. P. Duval et al. 2008 Phys. Plasmas 15], that shows a rotation reversal during a density ramp up. In view of performing a momentum transport analysis on this TCV shot, some constraints have to be considered to reduce the uncertainty on the experimental parameters. One useful constraint is the zero particle flux condition, resulting from the absence of direct particle fuelling to the plasma core. In this work, a preliminary study of the reconstruction of the zero particle flux hyper-surface in the physical parameters space is presented, taking into account the effect of the main impurity (carbon) and beginning to explore the effect of collisions, in order to find a subset of this hyper-surface within the experimental error bars. The analysis is done performing gyrokinetic simulations with the local (flux-tube) version of the Eulerian code GENE [Jenko et al 2000 Phys. Plasmas 7 1904], computing the fluxes with a Quasi-Linear model, according to [E. Fable et al. 2010 PPCF 52], and validating the QL results with Non-Linear simulations in a subset of cases.<br/> &copy; Published under licence by IOP Publishing Ltd.},
key = {Electric discharges},
keywords = {Computation theory;Fusion reactions;Magnetoplasma;Plasma density;Plasma simulation;Rotation;Tokamak devices;Uncertainty analysis;},
note = {Experimental errors;Experimental parameters;Gyrokinetic simulations;Microturbulence simulations;Momentum transports;Nonlinear simulations;Physical parameters;Stabilization effects;},
URL = {http://dx.doi.org/10.1088/1742-6596/775/1/012007},
} 


@inproceedings{20163902839362,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Production-Run Software Failure Diagnosis via Adaptive Communication Tracking},
journal = {Proceedings - 2016 43rd International Symposium on Computer Architecture, ISCA 2016},
author = {Alam, Mohammad Mejbah Ul and Muzahid, Abdullah},
year = {2016},
pages = {354 - 366},
address = {Seoul, Korea, Republic of},
abstract = {Software failure diagnosis techniques work either by sampling some events at production-run time or by using some bug detection algorithms. Some of the techniques require the failure to be reproduced multiple times. The ones that do not require such, are not adaptive enough when the execution platform, environment or code changes. We propose ACT, a diagnosis technique for production-run failures, that uses the machine intelligence of neural hardware. ACT learns some invariants (e.g., data communication invariants) on-the-fly using the neural hardware and records any potential violation of them. Since ACT can learn invariants on-the-fly, it can adapt to any change in execution setting or code. Since it records only the potentially violated invariants, the postprocessing phase can pinpoint the root cause fairly accurately without requiring to observe the failure again. ACT works seamlessly for many sequential and concurrency bugs. The paper provides a detailed design and implementation of ACT in a typical multiprocessor system. It uses a three stage pipeline for partially configurable one hidden layer neural networks. We have evaluated ACT on a variety of programs from popular benchmarks as well as open source programs. ACT diagnoses failures caused by 16 bugs from these programs with accurate ranking. Compared to existing learning and sampling based approaches, ACT has better diagnostic ability. For the default configuration, ACT has an average execution overhead of 8.2%.<br/> &copy; 2016 IEEE.},
key = {Program diagnostics},
keywords = {Computer architecture;Computer hardware;Embedded systems;Failure (mechanical);Failure analysis;Hardware;Network architecture;Network layers;Open source software;Production control;Program debugging;},
note = {Adaptive communications;Concurrency bugs;Default configurations;Dependence;Machine intelligence;Multi processor systems;Neural hardware;Sequential bugs;},
URL = {http://dx.doi.org/10.1109/ISCA.2016.39},
} 


@article{20124815729665,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Low complexity time-concatenated turbo equalization for block transmission without guard interval: Part 1-The concept},
journal = {Wireless Personal Communications},
author = {Anwar, Khoirul and Matsumoto, Tad},
volume = {67},
number = {4},
year = {2012},
pages = {761 - 781},
issn = {09296212},
abstract = {This paper proposes a novel time-concatenated turbo equalization technique, chained turbo equalization (CHATUE), that allows block transmission systems to eliminate the guard interval (GI), while achieving excellent performance. The proposedCHATUEalgorithm connects turbo equalizers neighboring in time, so that they exchange information about their inter-block-interference components in the form of a posteriori log-likelihood ratio. The latest version of the low complexity sub-optimal turbo equalization technique for block-wise single carrier transmission, frequency domain soft cancellation and minimum mean squared error, is fully exploited in developing the CHATUE algorithm. Results of extrinsic information transfer chart analysis as well as a series of bit-error rate (BER) simulations show that excellent performances can be achieved without imposing heavy computational burden in multipath-rich (quasi-static) block Rayleigh fading channels. It is shown that, if the information bit-rate is kept identical (because it may be unpreferable for the industry to change the frame structure), the CHATUE algorithm achieves lower BER than that with block transmission with GI, because lower rate (strong) code for error protection can be used by utilizing the time-duration made available by eliminating the GI. In addition, by combining the proposed structure with a simple rate-1 doped accumulator, further BER improvement exhibiting clear turbo cliff can be achieved. A sister paper (a Part-2 paper) applies the proposed CHATUE algorithm to single carrier frequency division multiple access systems Hui et al. (Wirel Pers Commun, 2011). &copy; The Author(s) 2012.},
key = {Data communication systems},
keywords = {Algorithms;Carrier communication;Frequency division multiple access;Frequency domain analysis;Turbo codes;},
note = {EXIT-Charts;Frequency domains;Guard intervals;Inter-symbol interference (ISI);Interblock interference;Log-likelihood ratios;MMSE;Single carrier;Turbo equalizations;},
URL = {http://dx.doi.org/10.1007/s11277-012-0563-0},
} 


@inproceedings{20134416917805,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {2013 International Forum on Materials Science and Industrial Technology, IFMSIT 2013},
journal = {Advanced Materials Research},
volume = {798},
year = {2013},
pages = {Education Department of Shandong Province; Qingdao University; Qingdao Science Culture Communication Co., Ltd; Test and analysis of association of Qingdao; Faculty of Chemical Engineering Universiti Teknologi Mara; et al - },
issn = {10226680},
address = {Qingdao, China},
abstract = {The proceedings contain 264 papers. The special focus in this conference is on Materials Science and Industrial Technology. The topics include: Enzymatic actions of acid cellulase on cotton fabrics; lumping kinetics of hydrodesulfurization for crude longkou shale oil; effect of double elements Co-doping on nano-TiO<inf>2</inf> photocatalysis of rhodamine B solution; evaluation of empirical kinetics models of athermal martensite transformation in plain carbon and low alloy steels; preparation and characterization of super pure coal; preparation and characterization of stable TiO<inf>2</inf> colloid composing of nearly monodispersive TiO<inf>2</inf> nanocrystallite; molecular orbitals and the wave equation; a review for granular media; review of thyristor junction temperature calculation methods; effect of MPBM on crosslinking reaction of tetrafluoroethylene-propylene copolymer/DCP; studies on an ecofriendly oxidative degradation system for phenol; the degradation of decabromodiphenyl ether in the supercritical fluid; a soil water characteristic curve model considering urea concentration; solid-phase extraction of salidroside by electrospun nanofibers; study on water stability of warm mix drainage asphalt with sasobit; the study of surface longitudinal crack on SPA-H steel slab by CSP; mesitylene-assisted formation of large pore SBA-15 microspheres; intercalation of Na-montmorillonite with ionic liquids; preparation of gold nanoparticles; a comparative study of the selective oxidation of NH3 to N2 over transition metal catalysts; applications of FRP material in bridge structures; the design of self-service intelligent stereo garage; forming stability analysis of surface flexible rolling; a video-based theodolite simulation system; injection moulding simulation analysis of handle shell; the key stamping process analysis and mold design; the method of optimal phasor measurement unit placement; output displacement analysis of symmetric four-bar mechanism with right angle flexible hinge; the force state analysis of the service door on an airliner; cavity tool path optimization in high-speed milling based on NX CAM; the mobile oilfield map based on SVG and inforamtion integration; the brittleness source identification of electric grid system; the function of the materials science in industrial design; applications of 3D stratum visualization CAD in geological exploration; simulation research on the performance of vehicle suspension system; the effectively method of detecting network traffic anomaly; iterative construct of reversible network based on cascade operation; rapidly construct the network teaching environment for training; microblog social network analysis based on network group behavior; hierarchical mobile IPv6 based handoff optimization scheme; analysis and optimization of mobile IPv6 handoff technology; variable multi-channel high frequency signal acquisition system; research on pattern classification of SVM-based gait signal; recent advances in preserving privacy data mining; research on a positioning application by RTK technique and with goGPS; design of temperature controlling system for CCD based on DSP; numerical simulation of chaotic laser secure communication; hardware design of code transmitter and monitor station; research on automatic line of wheeled robot based on MCU; study on dynamics of small tracked steering on ramp; research on propagation characteristics of SAW based on ANSYS; a new way for handwritten numeral recognition; the empirical analysis of function-call graph; chaos synchronization of the modified sprott E system; a 3D model watermarking algorithm; application of fuzzy mathematical method in insulator state inspection; image fusion with sparse representation; discussions on securities software expert system MA and RSI; sequences and series of functions on fractal space; on the perron root of nonnegative matrices; an data replication and deletion algorithm for web objects; the research on Chinese automatic segmentation; the importance of exhibition design in modern society; anonymity query method of outsourced database; an empirical study on factors influencing Taiwan's future basis of securities market; influence of modeling methods for housing price forecasting; VISC application in securities technology analysis teaching; models research of enterprise logistics cost control; research on the current situation of the efficiency evaluation of logistics; E-commerce business process modeling and verification based on Pi calculus; the development strategy of tourism informatization in small and medium-sized cities; the construction enterprises culture building based on knowledge management; history, current situation and development prospect of the TRIZ; content-based medical image retrieval system for color endoscopic images; effect of salidroside on cardiac functional recovery; characterization for staling of Chinese rolls; study on the changes of urea content during brewing of Chinese Huangjiu; analysis and evaluation of the health care systems; progress of microbial removal of arsenic from wastewater; assessment on industrial land-use fitness in a resource based town; fitness evaluation for residential land in a mountainous town; the application and development of EM technology in municipal sewage treatment and introduction of new methods for insulator contaminant detection.},
} 


@inproceedings{20134716993194,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {New elements in old tools: Nucleosynthesis in N-body codes},
journal = {Proceedings of Science},
author = {Loyola, Guido R.I. Moyano and Hurley, Jarrod R.},
year = {2012},
issn = {18248039},
address = {Cairns, QLD, Australia},
abstract = {The main scientific goal of this project is to unravel for the first time the stellar assembly history of our Galaxy using an accurate dynamical and chemical approach. Of particular importance is to identify the early dissolution events of primordial stellar clusters and the overall impact on the chemical evolution of the Galaxy. We will do this by adding detailed nucleosynthesis models into the current state-of-the-art N-body code NBODY6. In this way we will obtain accurate predictions for the chemical evolution of the Galaxy and the evolution of stellar clusters, particularly the change of surface abundances from non-standard evolution, aspects which will be a novel implementation in the field. &copy; Copyright owned by the author(s) under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike Licence.<br/>},
key = {Nucleosynthesis},
keywords = {Chemical elements;Galaxies;Negative impedance converters;Stars;},
note = {Accurate prediction;Chemical evolution;Standard evolutions;State of the art;Stellar cluster;},
} 


@inproceedings{20174404358872,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dynamic seismic response analysis of nuclear storage tank based on fluid-structure coupling method},
journal = {International Conference on Nuclear Engineering, Proceedings, ICONE},
author = {Weng, Yu and Liu, Lang and Jiang, Yang and Gu, Hongfang and Wang, Haijun},
volume = {2},
year = {2017},
pages = {Nuclear Engineering Division - },
address = {Shanghai, China},
abstract = {The storage tanks in nuclear facilities has a significant impact on the safety of the reactor and the radiation shielding, so its mechanical property analysis has been widely concerned in the field of engineering and scientific research. Meanwhile, the storage tank is usually filled with gas and liquid medium. In the presence of external disturbances (such as external force, displacement, earthquake etc.), the position and structure of the vessel changes, that lead to changing of the gas-liquid interface. This characteristic can make the storage tank system as a tightly fluid-structure coupling system. In this paper, a storage tank which stored radioactive gas liquid medium is choosing to study such fluid-structure coupling system phenomenon, and a typical dynamic seismic condition is assumed. A two-way fluid-structure coupling method is used with CFD (Computational Fluid Dynamics) and FEM (Finite Element Method) numerical method. The study considered interaction between structure and two phase turbulent fluid. In FEM calculation, the time history seismic acceleration load is applied to the support of tank, and the flow loading coming from fluid medium is applied to the wall of tank which is send from CFD code. Then, the structure displacement which is calculate by FEM is transferred to CFD code. In CFD calculation, multiphase fluid numerical model is applied to simulate the flow characteristics of gas-water two phase fluid, and the turbulent properties are also considered in the calculation. Mesh deformation method is used to simulate the displacement of flow passage boundary which is send by FEM code. After CFD calculation, flow loading is transferred to the tank wall of FEM code again. Such loop of FEM and CFD calculation continues to go on with the seismic time history, the response characteristics of the tank will be solved. In order to evaluate the difference between the above method and the traditional analysis method. An independent calculation used added mass approach is carrying out, in which the effect of steady state water is applied to the wall of the vessel, and this load will not change with the earthquake. All others load and constraint mode are same with the above method. According to the two-way fluid-structure coupling analysis, the detailed characteristics of liquid free surface distribution and structural response of the vessel are obtained. The results show that the response vibration amplitude of the tank structure increases with the earthquake, and the response is mainly affected by the liquid sloshing. According to comparative analysis, the advantages of coupling method are proved. The method from this study can be used for the same type of analysis.<br/> &copy; Copyright 2017 ASME.},
key = {Computational fluid dynamics},
keywords = {Codes (symbols);Earthquakes;Finite element method;Fluid structure interaction;Liquid sloshing;Loads (forces);Nuclear engineering;Numerical methods;Phase interfaces;Radiation shielding;Reactor shielding;Storage (materials);Tanks (containers);Two phase flow;},
note = {CFD (computational fluid dynamics);FEM (finite element method);Fluid structure coupling analysis;Fluid structure couplings;Response characteristic;Seismic accelerations;Seismic response analysis;Structure displacement;},
URL = {http://dx.doi.org/10.1115/ICONE2566835},
} 


@article{20172103689254,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Verifying increasingly expressive temporal logics for infinite-state systems},
journal = {Journal of the ACM},
author = {Cook, Byron and Khlaaf, Heidy and Piterman, Nir},
volume = {64},
number = {2},
year = {2017},
issn = {00045411},
abstract = {Temporal logic is a formal system for specifying and reasoning about propositions qualified in terms of time. It offers a unified approach to program verification as it applies to both sequential and parallel programs and provides a uniform framework for describing a system at any level of abstraction. Thus, a number of automated systems have been proposed to exclusively reason about either Computation-Tree Logic (CTL) or Linear Temporal Logic (LTL) in the infinite-state setting. Unfortunately, these logics have significantly reduced expressiveness as they restrict the interplay between temporal operators and path quantifiers, thus disallowing the expression of many practical properties, for example, "along some future an event occurs infinitely often." Contrarily, CTL&lowast;, a superset of both CTL and LTL, can facilitate the interplay between path-based and state-based reasoning. CTL&lowast; thus exclusively allows for the expressiveness of properties involving existential system stabilization and "possibility" properties. Until now, there have not existed automated systems that allow for the verification of such expressive CTL&lowast; properties over infinite-state systems. This article proposes a method capable of such a task, thus introducing the first known fully automated tool for symbolically proving CTL&lowast; properties of (infinite-state) integer programs. The method uses an internal encoding that admits reasoning about the subtle interplay between the nesting of temporal operators and path quantifiers that occurs within CTL&lowast; proofs. A program transformation is first employed that trades nondeterminism in the transition relation for nondeterminism explicit in variables predicting future outcomes when necessary. We then synthesize and quantify preconditions over the transformed program that represent program states that satisfy a CTL&lowast; formula. This article demonstrates the viability of our approach in practice, thus leading to a new class of fullyautomated tools capable of proving crucial properties that no tool could previously prove. Additionally, we consider the linear-past extension to CTL&lowast; for infinite-state systems in which the past is linear and each moment in time has a unique past. We discuss the practice of this extension and how it is further supported through the use of history variables.We have implemented our approach and report our benchmarks carried out on case studies ranging from smaller programs to demonstrate the expressiveness of CTL&lowast; specifications, to larger code bases drawn from device drivers and various industrial examples.<br/> &copy; 2017 ACM.},
key = {Temporal logic},
keywords = {Automation;Computer circuits;Integer programming;Model checking;},
note = {Computation tree logic;Infinite state systems;Level of abstraction;Linear temporal logic;Program transformations;Program Verification;System stabilization;Transition relations;},
URL = {http://dx.doi.org/10.1145/3060257},
} 


@inproceedings{20174404361153,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Uncertainty analysis of the tru-burning thorium-fueled rbwr using generalized perturbation theory},
journal = {International Conference on Nuclear Engineering, Proceedings, ICONE},
author = {Bogetic, Sandra and Gorman, Phillip and Aufiero, Manuele and Fratoni, Massimiliano and Greenspan, Ehud and Vujic, Jasmina},
volume = {9},
year = {2017},
pages = {Nuclear Engineering Division - },
address = {Shanghai, China},
abstract = {The RBWR-TR is a thorium-based reduced moderation BWR (RBWR) with a high transuranic (TRU) consumption rate. It is charged with LWR TRU and thorium, and it recycles all actinides an unlimited number of times while discharging only fission products and trace amounts of actinides through reprocessing losses. This design is a variant of the Hitachi RBWR-TB2, which arranges its fuel in a hexagonal lattice, axially segregates seed and blanket regions, and fits within an existing ABWR pressure vessel. The RBWR-TR eliminates the internal axial blanket, eliminates absorbers from the upper reflector, and uses thorium rather than depleted uranium as the fertile makeup fuel. This design has been previously shown to perform comparably to the RBWR-TB2 in terms of TRU consumption rate and burnup, while providing significantly larger margin against critical heat flux. This study examines the uncertainty in key neutronics parameters due to nuclear data uncertainty. As most of the fissions are induced by epithermal neutrons and since the reactor uses higher actinides as well as thorium and 233U, the cross sections have significantly more uncertainty than in typical LWRs. The sensitivity of the multiplication factor (keff) to the cross sections of many actinides is quantified using a modified version of Serpent 2.1.19 [1]. Serpent [2] is a Monte Carlo code which uses delta tracking to speed up the simulation of reactors; in this modified version, cross sections are artificially inflated to sample more collision, and collisions are rejected to preserve a "fair game." The impact of these rejected collisions is then propagated to the multiplication factor using generalized perturbation theory [3]. Covariance matrices are retrieved for the ENDF/B-VII.1 library [4], and used to collapse the sensitivity vectors to an uncertainty on the multiplication factor. The simulation is repeated for several reactor configurations (for example, with a reduced flow rate, and with control rods inserted), and the difference in keff sensitivity is used to assess the uncertainty associated with the change (the uncertainty in the void feedback and the control rod worth). The uncertainty in the RBWR-TR is found to be dominated by the epithermal fission cross section for 233U in reference conditions, although when the spectrum hardens, the uncertainty in fast capture cross sections of 232Th becomes dominant.<br/> Copyright &copy; 2017 ASME.},
key = {Uncertainty analysis},
keywords = {Boiling water reactors;Control rods;Covariance matrix;Fission products;Heat flux;Light water reactors;Monte Carlo methods;Nuclear engineering;Nuclear fuel reprocessing;Perturbation techniques;Thorium;},
note = {Capture cross sections;Fission cross section;Generalized perturbation theories;Multiplication factor;Neutronics parameters;Reactor configuration;Reference condition;Sensitivity vectors;},
URL = {http://dx.doi.org/10.115/ICONE25-68001},
} 


@article{20182605375874,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mean composite fire severity metrics computed with google earth engine offer improved accuracy and expanded mapping potential},
journal = {Remote Sensing},
author = {Parks, Sean A. and Holsinger, Lisa M. and Voss, Morgan A. and Loehman, Rachel A. and Robinson, Nathaniel P.},
volume = {10},
number = {6},
year = {2018},
issn = {20724292},
abstract = {Landsat-based fire severity datasets are an invaluable resource for monitoring and research purposes. These gridded fire severity datasets are generally produced with pre- and post-fire imagery to estimate the degree of fire-induced ecological change. Here, we introduce methods to produce three Landsat-based fire severity metrics using the Google Earth Engine (GEE) platform: The delta normalized burn ratio (dNBR), the relativized delta normalized burn ratio (RdNBR), and the relativized burn ratio (RBR). Our methods do not rely on time-consuming a priori scene selection but instead use a mean compositing approach in which all valid pixels (e.g., cloud-free) over a pre-specified date range (pre- and post-fire) are stacked and the mean value for each pixel over each stack is used to produce the resulting fire severity datasets. This approach demonstrates that fire severity datasets can be produced with relative ease and speed compared to the standard approach in which one pre-fire and one post-fire scene are judiciously identified and used to produce fire severity datasets. We also validate the GEE-derived fire severity metrics using field-based fire severity plots for 18 fires in the western United States. These validations are compared to Landsat-based fire severity datasets produced using only one pre- and post-fire scene, which has been the standard approach in producing such datasets since their inception. Results indicate that the GEE-derived fire severity datasets generally show improved validation statistics compared to parallel versions in which only one pre-fire and one post-fire scene are used, though some of the improvements in some validations are more or less negligible. We provide code and a sample geospatial fire history layer to produce dNBR, RdNBR, and RBR for the 18 fires we evaluated. Although our approach requires that a geospatial fire history layer (i.e., fire perimeters) be produced independently and prior to applying our methods, we suggest that our GEE methodology can reasonably be implemented on hundreds to thousands of fires, thereby increasing opportunities for fire severity monitoring and research across the globe.<br/> &copy; 2018 by the authors.},
URL = {http://dx.doi.org/10.3390/rs10060879},
} 


@inproceedings{20123115287371,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {2012 6th International Workshop on Software Clones, IWSC 2012 - Proceedings},
journal = {2012 6th International Workshop on Software Clones, IWSC 2012 - Proceedings},
year = {2012},
address = {Zurich, Switzerland},
abstract = {The proceedings contain 24 papers. The topics discussed include: an accurate estimation of the Levenshtein distance using metric trees and Manhattan distance; a novel approach based on formal methods for clone detection; claims and beliefs about code clones: do we agree as a community? a survey; clone detection using rolling hashing, suffix trees and dagification: a case study; dispersion of changes in cloned and non-cloned code; java bytecode clone detection via relaxation on code fingerprint and semantic web reasoning; mining object-oriented design models for detecting identical design structures; safe clone-based refactoring through stereotype identification and iso-generation; a case study on applying clone technology to an industrial application framework; industrial application of clone change management system; a common conceptual model for clone detection results; and filtering clones for individual user based on machine learning analysis.},
} 


@article{20134016803880,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Adoption and use of Java generics},
journal = {Empirical Software Engineering},
author = {Parnin, Chris and Bird, Christian and Murphy-Hill, Emerson},
volume = {18},
number = {6},
year = {2013},
pages = {1047 - 1089},
issn = {13823256},
abstract = {Support for generic programming was added to the Java language in 2004, representing perhaps the most significant change to one of the most widely used programming languages today. Researchers and language designers anticipated this addition would relieve many long-standing problems plaguing developers, but surprisingly, no one has yet measured how generics have been adopted and used in practice. In this paper, we report on the first empirical investigation into how Java generics have been integrated into open source software by automatically mining the history of 40 popular open source Java programs, traversing more than 650 million lines of code in the process. We evaluate five hypotheses and research questions about how Java developers use generics. For example, our results suggest that generics sometimes reduce the number of type casts and that generics are usually adopted by a single champion in a project, rather than all committers. We also offer insights into why some features may be adopted sooner and others features may be held back. &copy; 2012 Springer Science+Business Media New York.<br/>},
key = {Java programming language},
keywords = {Information dissemination;Open source software;Open systems;Query languages;},
note = {Annotations;Empirical investigation;Generic programming;Generics;Java;Post mortem analysis;Research questions;Standing problems;},
URL = {http://dx.doi.org/10.1007/s10664-012-9236-6},
} 


@inproceedings{20144600195193,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Analyzing the significance of process metrics for TTC software defect prediction},
journal = {Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
author = {Xia, Ye and Yan, Guoying and Zhang, Huiying},
year = {2014},
pages = {77 - 81},
issn = {23270586},
address = {Beijing, China},
abstract = {In the existing studies on software prediction, the most proposed methods are usually assessed over the public datasets like NASA metrics data repository, which include a combination of code metrics merely. Obviously, the process metric is also one of the key factors that affect the defect-proneness of software modules. In this paper, life-cycle based management process metrics set and history change process metrics set have been proposed based on the characteristics of development process. In order to analyze the importance of these different metrics for predicting defects in aerospace tracking telemetry and control (TT&amp;C) software, an improved PSO optimized support vector machine algorithm (PSO-SVM) has been presented and took into application. The experiment results over the actual TT&amp;C projects suggest that the prediction performance can be significance improved if the 2 kinds of process metrics are included in the model.<br/> &copy; 2014 IEEE.},
key = {C (programming language)},
keywords = {Application programs;Defects;Forecasting;Life cycle;NASA;Support vector machines;},
note = {Defect prediction;Development process;Management process;Prediction performance;Process metrics;Software defect prediction;Software metrics;Support vector machine algorithm;},
URL = {http://dx.doi.org/10.1109/ICSESS.2014.6933517},
} 


@inproceedings{20174404358502,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Simulation of cross-flow vortex-induced vibration of single tube based on two-way fluid-solid interaction method},
journal = {International Conference on Nuclear Engineering, Proceedings, ICONE},
author = {Zengzeng, Wang and Tao, Lu and Bo, Liu},
volume = {8},
year = {2017},
pages = {Nuclear Engineering Division - },
address = {Shanghai, China},
abstract = {The fatigue damage and lift force caused by vortex induced vibration occur very often in the core of the Pressurized Water Reactor (PWR) [1] It is extremely complex to illustrate the mechanism of vibration which induced by Cross-flow. With the spacer grids and wings, the flow direction which in axial direction at the inlet will change and create swirls, so there are many flow directions in the nuclear fuel component. Assumed the tube endure cross-flow only in this article to simplify the fluid model. Most researchers in this field often ignore the displacement of structure induced by the cross flow because the value is so small that not enough to change the fluid region. In truth conditions, the motion of the cylinder caused the wake oscillation and strengthen the vortex shedding, in turn, the vortex shedding will aggravate the vibration amplitude. According that, one way FSI (Fluid Solid Interaction) can't capture the influence from the cylinder vibration. In this article, Two-way FSI method was executed to get the vibration in time history in order to get the random vibration induced by the cross flow more close to the actual project. Using Finite Volume Method to discrete the fluid control equation and finite element method to discrete structure control equation combined with moving mesh technology. An interface between the fluid region and the structure region was created to transfer the fluid force and the structure displacement. Coupling CFD code and CSD(Computational Solid Dynamics) code to solve the differential equation and obtain the displacement of the cylinder in time history. A Fast Fourier Transfer (FFT) has been done to get the vibration frequency. An Analysis of the vortex shedding frequency and vibration frequency to find the correlation between the vortex shedding and the vibration frequency has been done. A modal analysis for the cylinder without water has been done to get the natural frequency. Results shows the cylinder has different response to the vortex shedding at different position of the cylinder in the same condition. There are more works need to be done aim to get the vibration mechanism in tandem tube and parallel tube to get clearly mechanism of vortex induced vibration in nuclear fuel assembly. The research of the vortex induced vibration in this article is a key to get on the follow research in more tubes array in different methods.<br/> &copy; Copyright 2017 ASME.},
key = {Vibration analysis},
keywords = {Axial flow;Codes (symbols);Computational fluid dynamics;Cylinders (shapes);Differential equations;Fatigue damage;Finite element method;Finite volume method;Modal analysis;Nuclear engineering;Oscillating cylinders;Pressurized water reactors;Tubes (components);Vortex flow;Vortex shedding;},
note = {Cross flows;Fluid solid interaction;Nuclear fuel assemblies;Structure displacement;Vibration amplitude;Vibration mechanisms;Vortex induced vibration;Vortex shedding frequency;},
URL = {http://dx.doi.org/10.1115/ICONE25-66955},
} 


@inproceedings{20164903102592,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Defragmentation and mashup: Ludic mashup as a design approach},
journal = {DiGRA 2013 - Proceedings of the 2013 DiGRA International Conference: DeFragging GameStudies},
author = {Lenhart, Isaac},
year = {2013},
address = {Atlanta, GA, United states},
abstract = {The history of technological progress has involved a repeated application of abstraction, of encapsulation, specialization and composition. Film, for example, has moved from a specialized field of equipment and concepts only available to trained professionals, into a field which has been commoditized and composited, and made available to almost everyone with basic equipment. New media has become more modular and thus passes into the hands of users who rely less on crafting from scratch and rely more on pre-built, readymade components that can be assembled. This "pulling together", i.e. this "mashup" or "remix" approach is already trivially true in the field of games in the modding community, which may introduce new 3D models, images, music or even new code blocks which change behaviors. These are very important, but signal a future move toward more sophisticated, pre-packaged modular blocks which players might assemble on their own in a more controlled manner. This might include swappable A.I. algorithms, interchangeable in-game weapons, interoperable "rulesets" and other key game entities that are normally thought of as being integral to a specific, single game. While mashup, assemblage and perhaps actor-network-theory has highlighted the ways in which a game played in context is more than the sum of its parts, this paper looks to the future of game design, in which players can assemble (on-the-fly) a set of game components. Such a situation is a defragmenting of ready-made ludic chunks, resulting in unpredictable and chaotic games created by players, and forces designers to consider their role less as a creator of a game in toto, but also as designers of interoperable ludic components.<br/> &copy; 2013 Lenhart & Digital Games Research Association DiGRA.},
key = {Three dimensional computer graphics},
keywords = {Computer games;Human computer interaction;Interactive computer graphics;Social sciences computing;},
note = {Assemblage;Defragment;Digital games;Games;Mash-up;New media;Personalized games;Pervasive game;Play;Remix;Ubiquitous games;},
} 


@inproceedings{20162202438767,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {IEEE International Symposium on Information Theory - Proceedings},
journal = {IEEE International Symposium on Information Theory - Proceedings},
volume = {2015-June},
year = {2015},
pages = {Information Theory Society of the Institute of Electrical and Electronics Engineers - },
issn = {21578095},
address = {Hong Kong, Hong kong},
abstract = {The proceedings contain 603 papers. The topics discussed include: some gabidulin codes cannot be list decoded efficiently at any radius; state-dependent multiple-access channels with partially cribbing encoders; cooperative multiple access channels with oblivious encoders; on the cost and benefit of cooperation; rate-compatible spatially-coupled LDPC code ensembles with nearly-regular degree distributions; the multi-step peg and ace constrained PEG algorithms can design the LDPC codes with better cycle-connectivity; spatially-coupled split-component codes with bounded-distance component decoding; learning immune-defectives graph through group tests; blind identification of an unknown interleaved convolutional code; the likelihood decoder: error exponents and mismatch; exact asymptotics for the random coding error probability; achievable rates and exponents for asynchronous communication with ml decoding; and quickest change detection and Kullback-Leibler divergence for two-state hidden Markov models.},
} 


@article{20132916503351,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Self-adaptive global best harmony search algorithm applied to reactor core fuel management optimization},
journal = {Annals of Nuclear Energy},
author = {Poursalehi, N. and Zolfaghari, A. and Minuchehr, A. and Valavi, K.},
volume = {62},
year = {2013},
pages = {86 - 102},
issn = {03064549},
abstract = {The aim of this work is to apply the new developed optimization algorithm, Self-adaptive Global best Harmony Search (SGHS), for PWRs fuel management optimization. SGHS algorithm has some modifications in comparison with basic Harmony Search (HS) and Global-best Harmony Search (GHS) algorithms such as dynamically change of parameters. For the demonstration of SGHS ability to find an optimal configuration of fuel assemblies, basic Harmony Search (HS) and Global-best Harmony Search (GHS) algorithms also have been developed and investigated. For this purpose, Self-adaptive Global best Harmony Search Nodal Expansion package (SGHSNE) has been developed implementing HS, GHS and SGHS optimization algorithms for the fuel management operation of nuclear reactor cores. This package uses developed average current nodal expansion code which solves the multi group diffusion equation by employment of first and second orders of Nodal Expansion Method (NEM) for two dimensional, hexagonal and rectangular geometries, respectively, by one node per a FA. Loading pattern optimization was performed using SGHSNE package for some test cases to present the SGHS algorithm capability in converging to near optimal loading pattern. Results indicate that the convergence rate and reliability of the SGHS method are quite promising and practically, SGHS improves the quality of loading pattern optimization results relative to HS and GHS algorithms. As a result, it has the potential to be used in the other nuclear engineering optimization problems. &copy; 2013 Elsevier Ltd. All rights reserved.<br/>},
key = {Pressurized water reactors},
keywords = {Fuels;Learning algorithms;Optimization;Reactor cores;},
note = {Engineering optimization problems;Global bests;Harmony search algorithms;Hexagonal geometry;Loading patterns;Nodal expansion method;Optimization algorithms;Rectangular geometry;},
URL = {http://dx.doi.org/10.1016/j.anucene.2013.06.003},
} 


@inproceedings{20180204642480,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {UAPD: Predicting Urban Anomalies from Spatial-Temporal Data},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Wu, Xian and Dong, Yuxiao and Huang, Chao and Xu, Jian and Wang, Dong and Chawla, Nitesh V.},
volume = {10535 LNAI},
year = {2017},
pages = {622 - 638},
issn = {03029743},
address = {Skopje, Macedonia},
abstract = {Urban city environments face the challenge of disturbances, which can create inconveniences for its citizens. These require timely detection and resolution, and more importantly timely preparedness on the part of city officials. We term these disturbances as anomalies, and pose the problem statement: if it is possible to also predict these anomalous events (proactive), and not just detect (reactive). While significant effort has been made in detecting anomalies in existing urban data, the prediction of future urban anomalies is much less well studied and understood. In this work, we formalize the future anomaly prediction problem in urban environments, such that those can be addressed in a more efficient and effective manner. We develop the Urban Anomaly PreDiction (UAPD) framework, which addresses a number of challenges, including the dynamic, spatial varieties of different categories of anomalies. Given the urban anomaly data to date, UAPD first detects the change point of each type of anomalies in the temporal dimension and then uses a tensor decomposition model to decouple the interrelations between the spatial and categorical dimensions. Finally, UAPD applies an autoregression method to predict which categories of anomalies will happen at each region in the future. We conduct extensive experiments in two urban environments, namely New York City and Pittsburgh. Experimental results demonstrate that UAPD outperforms alternative baselines across various settings, including different region and time-frame scales, as well as diverse categories of anomalies. Code related to this chapter is available at: https://bitbucket.org/xianwu9/uapd.<br/> &copy; 2017, Springer International Publishing AG.},
key = {Forecasting},
keywords = {Artificial intelligence;Learning systems;Regression analysis;Urban planning;},
note = {Anomalous events;Anomaly predictions;Autoregression;Problem statement;Spatial-temporal data;Temporal dimensions;Tensor decomposition;Urban environments;},
URL = {http://dx.doi.org/10.1007/978-3-319-71246-8_38},
} 


@article{20140617274567,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Pattern formation of multi mobile robots using arm gesture action},
journal = {ICIC Express Letters, Part B: Applications},
author = {Hsia, Kuo-Hsien and Li, Bo-Yi and Su, Kuo-Lan},
volume = {5},
number = {1},
year = {2014},
pages = {169 - 174},
issn = {21852766},
abstract = {In this paper, we get the arm gesture from Compass, Gyroscope and Ac- celerometer, and use fuzzy identification algorithm to analyze the arm gesture actions to control the formation of multiple mobile robots. The mobile robots move on a grid plane. The supervised computer determines the planning motion path of each mobile robot using the A*search algorithm with collision avoidance in consideration. In case of possible collision, robot with smaller ID code has higher priority to move. Five pattern formations are developed in our applications. The mobile robot contains a controller module, three IR sensor modules, a wireless RF module, and two DC servomotors. The mobile robot can detect signals from reective IR sensor module, decide the cross points of the aisle, receive the command from the supervised computer, and transmit the status of environment to the supervised computer via wireless RF interface. In the experimental results, we show that the mobile robots can receive the pattern formation command from the supervised computer, and then change the pattern formation on the motion platform without any collision. &copy; 2014 ICIC International.},
key = {Mobile robots},
keywords = {Infrared detectors;Learning algorithms;},
note = {Controller modules;Fuzzy identification;Multi mobile robot;Multiple mobile robot;Pattern formation;Search Algorithms;Wireless RF interfaces;Wireless RF modules;},
} 


@article{20183705800868,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Drought influence on forest plantations in Zululand, South Africa, using MODIS time series and climate data},
journal = {Forests},
author = {Xulu, Sifiso and Peerbhay, Kabir and Gebreslasie, Michael and Ismail, Riyad},
volume = {9},
number = {9},
year = {2018},
issn = {19994907},
abstract = {South Africa has a long history of recurrent droughts that have adversely affected its economic performance. The recent 2015 drought has been declared the most serious in 26 years and impaired key agricultural sectors including the forestry sector. Research on the forests' responses to drought is therefore essential for management planning and monitoring. The effects of the latest drought on the forests in South Africa have not been studied and are uncertain. The study reported here addresses this gap by using Moderate Resolution Imaging Spectroradiometer (MODIS)-derived normalized difference vegetation index (NDVI) and precipitation data retrieved and processed using the JavaScript code editor in the Google Earth Engine (GEE) and the corresponding normalized difference infrared index (NDII), Palmer drought severity index (PDSI), and El Ni&ntilde;o time series data for KwaMbonambi, northern Zululand, between 2002 and 2016. The NDVI and NDII time series were decomposed using the Breaks for Additive Seasonal and Trend (BFAST) method to establish the trend and seasonal variation. Multiple linear regression and Mann-Kendall tests were applied to determine the association of the NDVI and NDII with the climate variables. Plantation trees displayed high NDVI values (0.74-0.78) from 2002 to 2013; then, they decreased sharply to 0.64 in 2015. The Mann-Kendall trend test confirmed a negative significant (p = 0.000353) trend between 2014 and 2015. This pattern was associated with a precipitation deficit and low NDII values during a strong El Ni&ntilde;o phase. The PDSI (-2.6) values indicated severe drought conditions. The greening decreased in 2015, with some forest remnants showing resistance, implying that the tree species had varying sensitivity to drought. We found that the plantation trees suffered drought stress during 2015, although it seems that the trees began to recover, as the NDVI signals rose in 2016. Overall, these results demonstrated the effective use of the NDVI- and NDII-derived MODIS data coupled with climatic variables to provide insights into the influence of drought on plantation trees in the study area.<br/> &copy; 2018 by the authors.},
key = {Forestry},
keywords = {Climate change;Drought;Linear regression;Precipitation (chemical);Radiometers;Remote sensing;Satellite imagery;Time series analysis;},
note = {Forest disturbances;Moderate resolution imaging spectroradiometer;Multiple linear regressions;NDII;NDVI;Normalized difference vegetation index;Palmer drought severity indices;PDSI;},
URL = {http://dx.doi.org/10.3390/f9090528},
} 


@article{20122815227345,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {SIGNUM: A Matlab, TIN-based landscape evolution model},
journal = {Computers and Geosciences},
author = {Refice, A. and Giachetta, E. and Capolongo, D.},
volume = {45},
year = {2012},
pages = {293 - 303},
issn = {00983004},
abstract = {Several numerical landscape evolution models (LEMs) have been developed to date, and many are available as open source codes. Most are written in efficient programming languages such as Fortran or C, but often require additional code efforts to plug in to more user-friendly data analysis and/or visualization tools to ease interpretation and scientific insight. In this paper, we present an effort to port a common core of accepted physical principles governing landscape evolution directly into a high-level language and data analysis environment such as Matlab. SIGNUM (acronym for Simple Integrated Geomorphological Numerical Model) is an independent and self-contained Matlab, TIN-based landscape evolution model, built to simulate topography development at various space and time scales. SIGNUM is presently capable of simulating hillslope processes such as linear and nonlinear diffusion, fluvial incision into bedrock, spatially varying surface uplift which can be used to simulate changes in base level, thrust and faulting, as well as effects of climate changes. Although based on accepted and well-known processes and algorithms in its present version, it is built with a modular structure, which allows to easily modify and upgrade the simulated physical processes to suite virtually any user needs. The code is conceived as an open-source project, and is thus an ideal tool for both research and didactic purposes, thanks to the high-level nature of the Matlab environment and its popularity among the scientific community. In this paper the simulation code is presented together with some simple examples of surface evolution, and guidelines for development of new modules and algorithms are proposed. &copy; 2011 Elsevier Ltd.<br/>},
key = {C (programming language)},
keywords = {Climate change;Codes (symbols);Data handling;Data visualization;FORTRAN (programming language);Information analysis;MATLAB;Open source software;Open systems;Tin;},
note = {Landscape evolutions;MATLAB environment;Morphodynamics;Nonlinear diffusion;Open source projects;Physical principles;Scientific community;Visualization tools;},
URL = {http://dx.doi.org/10.1016/j.cageo.2011.11.013},
} 


@inproceedings{20135017083237,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards transparent hardening of distributed systems},
journal = {Proceedings of the 9th Workshop on Hot Topics in Dependable Systems, HotDep 2013},
author = {Behrens, Diogo and Fetzer, Christof and Junqueira, Flavio P. and Serafini, Marco},
year = {2013},
pages = {ACM Special Interest Group on Operating Systems (SIGOPS) - },
address = {Farmington, PA, United states},
abstract = {In distributed systems, errors such as data corruption or arbitrary changes to the flow of programs might cause processes to propagate incorrect state across the system. To prevent error propagation in such systems, an efficient and effective technique is to harden processes against Arbitrary State Corruption (ASC) faults through local detection, without replication. For distributed systems designed from scratch, dealing with state corruption can be made fully transparent, but requires that developers follow a few concrete design patterns. In this paper, we discuss the problem of hardening existing code bases of distributed systems transparently. Existing systems have not been designed with ASC hardening in mind, so they do not necessarily follow required design patterns. For such systems, we focus here on both performance and number of changes to the existing code base. Using memcached as an example, we identify and discuss three areas of improvement: reducing the memory overhead, improving access to state variables, and supporting multi-threading. Our initial evaluation of memcached shows that our ASC-hardened version obtains a throughput that is roughly 76% of the throughput of stock memcached with 128-byte and 1k-byte messages. &copy; 2013 ACM.<br/>},
key = {Fault tolerant computer systems},
keywords = {Crime;Fault tolerance;Hardening;},
note = {Arbitrary change;Data corruption;Distributed systems;Existing systems;Memory overheads;Prevent error propagation;State corruptions;State variables;},
URL = {http://dx.doi.org/10.1145/2524224.2524230},
} 


@inproceedings{20173804174627,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dynamics of content quality in collaborative knowledge production},
journal = {Proceedings of the 11th International Conference on Web and Social Media, ICWSM 2017},
author = {Ferrara, Emilio and Alipoufard, Nazanin and Burghardt, Keith and Gopal, Chiranth and Lerman, Kristina},
year = {2017},
pages = {520 - 523},
address = {Montreal, QC, Canada},
abstract = {We explore the dynamics of user performance in collaborative knowledge production by studying the quality of answers to questions posted on Stack Exchange. We propose four indicators of answer quality: answer length, the number of code lines and hyperlinks to external web content it contains, and whether it is accepted by the asker as the most helpful answer to the question. Analyzing millions of answers posted over the period from 2008 to 2014, we uncover regular short-term and long-term changes in quality. In the short-term, quality deteriorates over the course of a single session, with each successive answer becoming shorter, with fewer code lines and links, and less likely to be accepted. In contrast, performance improves over the long-term, with more experienced users producing higher quality answers. These trends are not a consequence of data heterogeneity, but rather have a behavioral origin. Our findings highlight the complex interplay between short-term deterioration in performance, potentially due to mental fatigue or attention depletion, and long-term performance improvement due to learning and skill acquisition, and its impact on the quality of user-generated content.<br/> &copy; Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
key = {Knowledge management},
keywords = {Hypertext systems;Social networking (online);},
note = {Collaborative knowledge;Content qualities;Data heterogeneity;Long term change;Long term performance;Skill acquisition;User performance;User-generated content;},
} 


@inproceedings{20144600188682,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Application of energy reduction techniques using niched pareto GA of energy analzyer for HPC applications},
journal = {2014 7th International Conference on Contemporary Computing, IC3 2014},
author = {Benedict, Shajulin},
year = {2014},
pages = {559 - 564},
address = {Noida, India},
abstract = {Energy consumption of High Performance Computing (HPC) architectures, on the path to exa-scale systems, is still a challenging problem among the HPC community owingto the technological issues, such as, power limitations of processor technologies, increased degree of parallelism (both in a node level and in a systemlevel), and a hefty cost of communication which arises while executing applications on such architectures. In addition, the increased electrical billing andthe other ensuing ecological hazards, including climate changes, have urgedseveral researchers to focus much on framing solutions that address the energy consumption issues of future HPC systems. Reducing the energyconsumption of HPC systems, however, is not an easy task due to its assortednature of muddled up complicated issues that are tightly dependent on the performance of applications, the energy efficiency of hardware components, and theenergy consumption of the compute center infrastructure. This paper presents Niched Pareto Genetic Algorithm (NPGA) based application of energy reduction techniques, namely, code version selection mechanism and compiler optimization switch selection mechanism, for HPC applications using EnergyAnalyzer tool. The proposed mechanism was tested with HPC applications, such as, MPI-C based HPCC benchmarks, Jacobi, PI, and matrix multiplication applications, on the HPCCLoud Research Laboratory of our premise. This paper could be of an interest to various researchers, namely, HPC application developers, performance analysis tool developers, environmentalist, and energy-awarehardware designers.<br/> &copy; 2014 IEEE.},
key = {Benchmarking},
keywords = {Climate change;Energy efficiency;Energy utilization;Genetic algorithms;Green computing;Research laboratories;},
note = {Application developers;Compiler optimizations;Energy tuning;High performance computing (HPC);Niched Pareto;Pareto genetic algorithms;Performance analysis;Scientific applications;},
URL = {http://dx.doi.org/10.1109/IC3.2014.6897234},
} 


@article{20184005905024,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {TFmix: A high-precision implementation of the finite-temperature Thomas–Fermi model for a mixture of atoms},
journal = {Computer Physics Communications},
author = {Shemyakin, O.P. and Levashov, P.R. and Krasnova, P.A.},
volume = {235},
year = {2019},
pages = {378 - 387},
issn = {00104655},
abstract = {In this work we present a TFmix code intended for numerical calculation of the thermal part of electronic thermodynamic properties of a mixture of elements by the finite-temperature Thomas&ndash;Fermi model. The code is based on analytical models for both first and second derivatives of Helmholtz thermodynamic potential. All numerical calculations are made within a controlled high accuracy: tests for thermodynamic consistency give at least 11 coinciding decimal digits. The code calculates thermodynamic functions on a regular grid of isotherms and isochores; at each grid point some extensive parameters and the number of free electrons are output both for the whole mixture and for each component. Other extensive or intensive thermodynamic properties, including pressure, entropy, isochoric and isobaric heat capacities, isothermal and adiabatic sound velocities can be easily calculated from the information available at each grid point. Several unit systems are provided for convenience. A cross-platform graphical user interface is developed to simplify the use of the code. Program summary: Program Title: TFmix, version 1.0 Program Files doi: http://dx.doi.org/10.17632/mc3vj77jfn.1 Licensing provisions: GPLv3 Programming language: C, Python Nature of problem: Any substance consists of elements so its equation of state contains a contribution of electronic gas. Thermodynamics of the electronic gas in a mixture of ions and electrons has been studied in many approaches. Thermodynamic properties of a uniform ideal electron gas can be calculated using the well-known analytical model of Fermi-gas. On the other hand, models of electron gas which take into account interaction effects are quite complicated and require sophisticated computational techniques. Even a simplified semiclassical Thomas&ndash;Fermi model is based upon the numerical solution of a non-linear boundary problem. Two main issues of the Thomas&ndash;Fermi model restrict its usage: uncontrolled accuracy of calculated thermodynamic functions (especially second derivatives of a thermodynamic potential), and unphysical behavior of the model at relatively low temperatures. Solution method: Each atom in the mixture is surrounded by a spherical cell. The radii of the cells are fitted to equalize the chemical potentials of all atoms. A guaranteed accuracy of first derivatives of the thermodynamic potential is provided by a transformation of integrals over the Thomas&ndash;Fermi potential to a system of differential equations. One of equations in the system is the Thomas&ndash;Fermi equation. Second derivatives of the thermodynamic potential are calculated similarly with the only difference that a corresponding derivative of the Thomas&ndash;Fermi equation is used in the system of differential equation. To avoid the unphysical behavior of the Thomas&ndash;Fermi model at low temperatures we extract a thermal contribution to thermodynamic properties which vanishes at zero temperature. To eliminate the error which appears from the subtraction of the cold part at low temperatures we use asymptotic expressions for thermodynamic functions and the Thomas&ndash;Fermi equation. The code calculates regular tables of thermodynamic functions on a grid of isotherms and isochores including second derivatives of a thermodynamic potential. This information is necessary for astrophysical applications, for continuum mechanics simulation of processes in plasma and for the creation of wide-range equations of state. A graphical user interface is provided with the code and allows to specify input parameters, to perform calculations and to plot the results. Additional comments including restrictions and unusual features: GSL library version 1.16 or 2.x is required for compilation; matplotlib Python library is required to run the graphical user interface.<br/> &copy; 2018 Elsevier B.V.},
key = {Temperature},
keywords = {Analytical models;Atoms;C (programming language);Codes (symbols);Computer software;Continuum mechanics;Differential equations;Electron gas;Electrons;Equations of state of gases;Fermions;Gases;Graphical user interfaces;Isotherms;Mixtures;Problem oriented languages;},
note = {Astrophysical applications;Equation of state;Finite temperatures;Isothermal and adiabatic;System of differential equations;Thermodynamic consistency;Thermodynamic functions;Thermodynamic potentials;},
URL = {http://dx.doi.org/10.1016/j.cpc.2018.09.008},
} 


@article{20184706106925,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Thermal hydraulic model of the molten salt reactor experiment with the NEAMS system analysis module},
journal = {Annals of Nuclear Energy},
author = {Leandro, Adrian M. and Heidet, Florent and Hu, Rui and Brown, Nicholas R.},
volume = {126},
year = {2019},
pages = {59 - 67},
issn = {03064549},
abstract = {System analysis codes have a long history of providing best-estimate and conservative safety analysis for both light water and advanced reactor technologies, including molten salt reactors. As interest continues to expand with advanced reactor concepts, system analysis codes will need revisions to accommodate the behavior of these technologies. Legacy system analysis codes will need to be updated to the latest numerical techniques to shorten execution time and increase the accuracy of results. One example of a modern system analysis code that already encompasses these characteristics is the System Analysis Module (SAM). One key objective of this paper was to review available information for system code modeling of the Molten Salt Reactor Experiment (MSRE) from sources in the open literature and collect the information from these open sources in one place for the first time. This supports the potential objective of developing an open specification for system code analysis for MSRE steady state and transients with and without reactor kinetics. Data from actual MSRE tests will serve as the basis for code-to-code comparison exercises, including the MSRE zero power physics tests, the fuel pump start-up and coast down tests, and the natural circulation transient. The objective is to produce a code-to-code benchmark with a standardized set of comparison problems, recognizing the limitations of the original data. To demonstrate an initial application of this objective and the usefulness of compiling this open data, two Molten Salt Reactor Experiment (MSRE)-related models were developed to evaluate SAM for liquid fueled molten salt reactors. One model was the SAM MSRE hydraulic mockup, which provided experimental data for pressure drop measurements. The second model was the complete MSRE primary loop. The MSRE primary loop model incorporated a fluoride salt fuel/coolant with heat transfer in both the core and heat exchanger. For both the hydraulic mockup and MSRE primary loop models, a holistic 1-D system description was built using open documentation, an open description that can be readily modified and applied for any system analysis code. SAM results for the pressure drop of the hydraulic mockup model were within 6% with measurements. Coolant temperatures for the primary loop model matched the expected axial change in temperature from historical calculations. Using alternative coolant properties obtained from the literature, corresponding to salts with different actinide contents, returned similar trends in core temperature profiles. A thermal hydraulic demonstration of a loss-of-flow transient showed the importance of coupling SAM thermal hydraulic analysis to neutronics. This coupling is essential for simulating MSR transients with system analysis codes.<br/> &copy; 2018 Elsevier Ltd},
key = {Molten salt reactor},
keywords = {Codes (symbols);Coolants;Drops;Fluorine compounds;Fused salts;Heat exchangers;Heat transfer;Hydraulic models;Legacy systems;Light water reactors;Mockups;Pressure drop;Systems analysis;},
note = {Coolant temperature;FLiBe;Loss of flow transients;Numerical techniques;Pressure drop measurements;Steady state and transients;Thermal hydraulic modeling;Thermal-hydraulic analysis;},
URL = {http://dx.doi.org/10.1016/j.anucene.2018.10.060},
} 


@article{20142517831952,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dynamic and speculative polyhedral parallelization using compiler-generated skeletons},
journal = {International Journal of Parallel Programming},
author = {Jimborean, Alexandra and Clauss, Philippe and Dollinger, Jean-Francois and Loechner, Vincent and Martinez Caamano, Juan Manuel},
volume = {42},
number = {4},
year = {2014},
pages = {529 - 545},
issn = {08857458},
abstract = {We propose a framework based on an original generation and use of algorithmic skeletons, and dedicated to speculative parallelization of scientific nested loop kernels, able to apply at run-time polyhedral transformations to the target code in order to exhibit parallelism and data locality. Parallel code generation is achieved almost at no cost by using binary algorithmic skeletons that are generated at compile-time, and that embed the original code and operations devoted to instantiate a polyhedral parallelizing transformation and to verify the speculations on dependences. The skeletons are patched at run-time to generate the executable code. The run-time process includes a transformation selection guided by online profiling phases on short samples, using an instrumented version of the code. During this phase, the accessed memory addresses are used to compute on-the-fly dependence distance vectors, and are also interpolated to build a predictor of the forthcoming accesses. Interpolating functions and distance vectors are then employed for dependence analysis to select a parallelizing transformation that, if the prediction is correct, does not induce any rollback during execution. In order to ensure that the rollback time overhead stays low, the code is executed in successive slices of the outermost original loop of the nest. Each slice can be either a parallel version which instantiates a skeleton, a sequential original version, or an instrumented version. Moreover, such slicing of the execution provides the opportunity of transforming differently the code to adapt to the observed execution phases, by patching differently one of the pre-built skeletons. The framework has been implemented with extensions of the LLVM compiler and an x86-64 runtime system. Significant speed-ups are shown on a set of benchmarks that could not have been handled efficiently by a compiler. &copy; 2013 Springer Science+Business Media New York.<br/>},
key = {Program compilers},
keywords = {Codes (symbols);Embedded systems;Musculoskeletal system;Parallel programming;},
note = {Algorithmic skeleton;Automatic Parallelization;Compilation;Loop nests;Parallelizations;Polytope models;Speculative parallelization;},
URL = {http://dx.doi.org/10.1007/s10766-013-0259-4},
} 


@inproceedings{20182105236924,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {2nd International Conference on Real-Time Intelligent Systems, RTIS 2017},
journal = {Advances in Intelligent Systems and Computing},
volume = {756},
year = {2019},
pages = {University of Hassan II in Morocco - },
issn = {21945357},
address = {Casablanca, Morocco},
abstract = {The proceedings contain 46 papers. The special focus in this conference is on Real-Time Intelligent Systems. The topics include: A New Vision for Multilingual Architecture; using Continuous Hopfield Neural Network for Choice Architecture of Probabilistic Self-Organizing Map; the Hybrid Framework for Multi-objective Evolutionary Optimization Based on Harmony Search Algorithm; new Prior Model for Bayesian Neural Networks Learning and Application to Classification of Tissues in Mammographic Images; Multilayer Perceptron: NSGA II for a New Multi-objective Learning Method for Training and Model Complexity; a New Quasi-Cyclic Majority Logic Codes Constructed from Disjoint Difference Sets by Genetic Algorithm; prediction of Coordinate Measuring Machines Geometric Errors by Measuring a Ball Step-Gauge; hierarchical Load Balancing Strategy in Cloud Environment; Impact of Hybrid Virtualization Using VM and Container on Live Migration and Cloud Performance; Applying Data Analytics and Cumulative Accuracy Profile (CAP) Approach in Real-Time Maintenance of Instructional Design Models; ioT Interoperability Architectures: Comparative Study; collaborative and Communicative Logistics Flows Management Using the Internet of Things; performance Analysis of Internet of Things Application Layer Protocol; simulation Automation of Wireless Network on Opnet Modeler; Smart SDN Policy Management Based VPN Multipoint; towards Smart Software Defined Wireless Network for Quality of Service Management; transformation of High Level Specification Towards nesC Code; Flexible Mobile Network Service Chaining in an NFV Environment: IMS Use Case; Performance Analysis of the Vertical Handover Across Wifi/3G Networks Based on IEEE 802.21; a New Encryption Scheme to Perform Smart Computations on Encrypted Cloud Big Data; a Meta-model for Real-Time Embedded Systems.<br/>},
} 


@article{20181605016950,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Airfoil optimization for wind turbine application},
journal = {Wind Energy},
author = {Hansen, T.H.},
volume = {21},
number = {7},
year = {2018},
pages = {502 - 514},
issn = {10954244},
abstract = {An airfoil optimization method for wind turbine applications that controls the loss in performance due to leading edge contamination is developed and tested. The method uses the class-shape-transformation technique to parametrize the airfoil geometry and uses an adjusted version of the panel code XFOIL to calculate the aerodynamic performance. To find optimal airfoil shapes, the derivative-free Covariance Matrix Adaptation Evolution Strategy is used in combination with an adaptive penalty function. The method is tested for the design of airfoils for the outer part of a megawatt-class wind turbine rotor blade, and the results are compared with airfoils from Delft University. It is found that the method is able to automatically create airfoils with equal or improved performance compared with the Delft designs. For the tested application, the adjustments performed to the XFOIL code improve the maximum lift, post stall, and the overall drag predictions.<br/> Copyright &copy; 2018 John Wiley & Sons, Ltd.},
key = {Turbine components},
keywords = {Airfoils;Covariance matrix;Evolutionary algorithms;Optimization;Turbomachine blades;Wind turbines;},
note = {Adaptive penalty functions;Aero-dynamic performance;Covariance matrix adaptation evolution strategies;Leading-edge contaminations;Parametrizations;Penalty function;RFOIL;XFOIL;},
URL = {http://dx.doi.org/10.1002/we.2174},
} 


@inproceedings{20185106271173,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Software regression and migration assistance using dynamic instrumentation},
journal = {Advances in Intelligent Systems and Computing},
author = {Chatterjee, Nachiketa and Chakrabarti, Amlan and Das, Partha Pratim},
volume = {897},
year = {2019},
pages = {159 - 170},
issn = {21945357},
address = {Kolkata, India},
abstract = {Companies and organizations use the legacy software for decades to serve various purposes. During this journey, the software system travels through several change requests and amendments of functionalities due to the changing nature of business and other requirements. As a result, different methodologies and implementations employed over the time are often not at all documented. So, modifying or migrating those software systems become difficult due to lack of technical knowledge about their behavior. This difficulty is even more when there is no Subject-Matter Expert (SME). Here, we propose a technique to verify the unchanged functionalities of untouched modules of the modified application by comparing with the older version of the application. Sometimes, the number of functional behaviors become irrelevant as they are no longer required by the business. However, significantly large portions of legacy applications continue executing, untouched by any modification or customization, to serve tiny yet critical purposes. Stakeholders also remain reluctant to cleanup or migrate because only for finding out the active part or functionals scope of the application is very tedious and consumes lot of effort due to lack of knowledge or documentation. Here, we have devised a mechanism to assist the migration specialists to identify the active part of an application, associated files, and data used by the active code that help in building the new one with similar functionalities. We can also assist the performance engineer by detecting the resource leakage in the application.<br/> &copy; 2019, Springer Nature Singapore Pte Ltd.},
key = {Computer software},
keywords = {Computer programming;Computer science;},
note = {Dynamic instrumentation;Functional behaviors;Legacy applications;Legacy software;Regression;Software migration;Software systems;Subject matter experts;},
URL = {http://dx.doi.org/10.1007/978-981-13-3250-0_12},
} 


@article{20185206297359,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Corrigendum to “Relationships between human activity and biodiversity in Europe at the national scale: Spatial density of human activity as a core driver of biodiversity erosion” (Ecological Indicators (2018) 90 (356–365), (S1470160X18301559) (10.1016/j.ecolind.2018.03.010))},
journal = {Ecological Indicators},
author = {Gosselin, Frederic and Callois, Jean-Marc},
volume = {99},
year = {2019},
pages = {394 - 395},
issn = {1470160X},
abstract = {This corrigendum aims to correct the first three columns of Table 3 in Gosselin and Callois (2018), which were based on an erroneous program to calculate marginal Leave-one-out Information Criteria (LOOIC). Table 1 in the present paper replaces this previous Table 3. We discuss why the new results are more in agreement with the other results of Gosselin and Callois (2018) and why the marginal version of the LOOIC appears better than the conditional version. Statistical methods: Here, the statistical models are the same as the ones in Gosselin and Callois (2018). They are Bayesian models which incorporate country random effects when observations were repeated in each country (for the two State Indicators and the Response Indicator), and which have a beta-binomial probability distribution for the two State Indicators and a zero-inflated beta distribution for the Response Inidcator and the two Pressure indicators. The zero-inflated beta distribution is a direct sub-product of the zero-inflated cumulative beta distribution that Herpigny and Gosselin (2015; MTUnlimited2 version) proposed to analyze plant cover class data. Indeed, we used statistical methods with a priori relevant probabilistic properties relative to our data. Based on the lessons learned on partly similar data from Gosselin (2015), we paid careful attention to the potential over-dispersion of data as well as to the inclusion of random country effects. We compared the models with the Leave-one-out Information Criterion (LOOIC) developed by Vehtari et al. (2016). For State and Response indicators, we used the marginal version of the LOOIC &ndash; i.e. the LOOIC integrated over the random effects &ndash; for reasons discussed by Millar (2009, 2017), and because the country random effect introduced a number of parameters which was of the same order of magnitude as the number of observations. We calculated the marginal versions of the LOOIC for 10,000 parameter values randomly drawn from the MCMC output by taking the mean of probabilities of the observed data grouped by country over 1,000 random draws of the country random effect. The original code in Gosselin and Callois (2018) was erroneous in that it did not take into account the dependence between observations in the same country (i.e. the mean probability of each observation was taken instead of the mean probability of groups of obervations of each country). The new calculations used here correct this previous error. Results: Results of the new calculations are presented in Table 1. Discussion: The new corrected values of the marginal LOOIC in Table 3 do not change any of the practical conclusions in Gosselin and Callois (2018). However, they are more coherent with the levels of significance of the estimators (Tables 4 to 7 in Gosselin and Callois, 2018) than were the previous values obtained (Table 3 in Gosselin and Callois, 2018). Indeed, in the previous table, the levels of significance of the most significant effects of explanatory variables were overall less significant for the Proportion of Extinct and Threatened species (Tables 4 and 5 in Gosselin and Callois, 2018) than for the proportion of Sealed area or the dynamics of this proportion (Tables 6 and 7 in Gosselin and Callois, 2018). Yet, previous differences in LOOIC values of the null model had indicated stronger differences for Extinct and Threatened species with respect to the dynamics in the proportion of Sealed areas (Table 3 in Gosselin and Callois, 2018). The new model comparison results presented herein in Table 1 establish an ordering that is more in line with the significance of the effects. This behavior is in agreement with McQuarrie and Tsai's (1999) results; for orthogonal regressions, they showed a direct link between the statistical significance of the effects of models and their Akaike Information Criterion (AIC), a criterion close to LOOIC for such simple settings. Our settings are not very different from orthogonal regressions since we mostly chose models with not too much correlated explanatory variables. The fact that the new marginal LOOIC values are more coherent with the levels of significance of the estimators than were the previous values obtained is therefore welcome. In the case of the SC model for the proportion of Threatened species (Table 1), the results for marginal LOOIC shown in Table 1 are also more satisfactory than the conditional LOOIC results presented in Table SM1 (Supplementary material): given the two significant estimators involved in the model (Table 5 in Gosselin and Callois, 2018), marginal LOOIC indicates that the SC model is better than the Null model, qualitatively in agreement with McQuarrie and Tsai (1999)&rsquo;s results, while conditional LOOIC indicates that it is not as good as the Null model.<br/> &copy; 2018 Elsevier Ltd},
URL = {http://dx.doi.org/10.1016/j.ecolind.2018.12.026},
} 


@inproceedings{20171603585114,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Veriabs: Verification by abstraction},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Chimdyalwar, Bharti and Darke, Priyanka and Chauhan, Avriti and Shah, Punit and Kumar, Shrawan and Venkatesh, R.},
volume = {10206 LNCS},
year = {2017},
pages = {404 - 408},
issn = {03029743},
address = {Uppsala, Sweden},
abstract = {VeriAbs verifies C programs by transforming them to abstract programs. The transformation replaces loops in the original code by abstract loops of small known bounds. Bounded model checkers can then be used to prove properties over such programs. To perform such a transformation, VeriAbs implements (i) a static value analysis to compute loop invariants, (ii) abstract acceleration and output abstraction for numerical loops, (iii) a novel array witness selection for loops that iterate over arrays, and (iv) an iterative refinement using an enhanced k-induction technique. To find errors, VeriAbs computes bounds of the original loops and then checks for errors within those bounds. VeriAbs can thus prove properties and find errors using bounded model checking. It uses the C Bounded Model Checker (CBMC) version 5.4 with MiniSat version 2.2.<br/> &copy; Springer-Verlag GmbH Germany 2017.},
key = {Model checking},
keywords = {Abstracting;C (programming language);Errors;Iterative methods;},
note = {Abstract programs;Bounded model checkers;Bounded model checking;C programs;Iterative refinement;Loop invariants;Numerical loops;},
URL = {http://dx.doi.org/10.1007/978-3-662-54580-5_32},
} 


@inproceedings{20150100402577,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dependency-based automatic parallelization of java applications},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Rafael, Joao and Correia, Ivo and Fonseca, Alcides and Cabral, Bruno},
volume = {8806},
year = {2014},
pages = {182 - 193},
issn = {03029743},
address = {Porto, Portugal},
abstract = {There are billions of lines of sequential code inside nowadays software which do not benefit from the parallelism available in modern multicore architectures. Transforming legacy sequential code into a parallel version of the same programs is a complex and cumbersome task. Trying to perform such transformation automatically and without the intervention of a developer has been a striking research objective for a long time. This work proposes an elegant way of achieving such a goal. By targeting a task-based runtime which manages execution using a task dependency graph, we developed a translator for sequential JAVA code which generates a highly parallel version of the same program. The translation process interprets the AST nodes for signatures such as read-write access, execution-flow modifications, among others and generates a set of dependencies between executable tasks. This process has been applied to well known problems, such as the recursive Fibonacci and FFT algorithms, resulting in versions capable of maximizing resource usage. For the case of two CPU bounded applications we were able to obtain 10.97x and 9.0x speedup on a 12 core machine.<br/> &copy; Springer International Publishing Switzerland 2014.},
key = {Java programming language},
keywords = {Automatic programming;Codes (symbols);Program translators;Software architecture;},
note = {Automatic Parallelization;Multicore architectures;Recursive procedure;Research objectives;Runtimes;Symbolic analysis;Task dependencies;Translation process;},
} 


@article{20164002872415,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {J3Model: A novel framework for improved Modified Condition/Decision Coverage analysis},
journal = {Computer Standards and Interfaces},
author = {Godboley, Sangharatna and Dutta, Arpita and Mohapatra, Durga Prasad and Mall, Rajib},
volume = {50},
year = {2017},
pages = {1 - 17},
issn = {09205489},
abstract = {In the real-time systems and safety critical domains, software quality assurance adheres to protocols such as DO-178C standard. Regarding these issues, concolic testing generates test cases that can attain high coverage using an augmented approach based on Modified Condition/Decision Coverage (MC/DC). In this paper, we propose a framework to compute MC/DC percentage for test case generation. To achieve an increase in MC/DC, we transform the input Java program, J, into its transformed version, J&prime;, using Java Program Code Transformer (JPCT). Then, we use JCUTE tool to generate test cases. At last, we use Java Coverage Analyzer (JCA) to compute MC/DC percentage. The Java program code transformer adds additional empty nested if-else conditional statements for each decision that causes variation in MC/DC percentage. In later step, these extra conditional statements get stripped-off. This approach resolves some of the bottleneck issues associated with traditional concolic testers. In our experimental study, we have experimented with forty Java programs. We have computed the difference of MC/DC%, for both the scenarios (i.e. with code transformation and without code transformation). Our approach (i.e. with code transformation achieves) 24.09% average increase in MC/DC% over the traditional approach (i.e. without code transformation).<br/> &copy; 2016 Elsevier B.V.},
key = {C (programming language)},
keywords = {Codes (symbols);Computer software selection and evaluation;Cosine transforms;Interactive computer systems;Java programming language;Quality assurance;Real time systems;Safety engineering;},
note = {Concolic testing;JCUTE;JPCT;MC/DC;Modified condition/decision coverages;Safety-critical domain;Software quality assurance;Traditional approaches;},
URL = {http://dx.doi.org/10.1016/j.csi.2016.09.006},
} 


@article{20180604760303,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automatic parallelization of recursive functions with rewriting rules},
journal = {Science of Computer Programming},
author = {Rocha, Rodrigo C.O. and Goes, Luis F.W. and Pereira, Fernando M.Q.},
year = {2018},
issn = {01676423},
abstract = {Functional programming languages, since their early days, have been regarded as the holy grail of parallelism. And, in fact, the absence of race conditions, coupled with algorithmic skeletons such as map and reduce, have given developers the opportunity to write many different techniques aimed at the automatic parallelization of programs. However, there are many functional programs that are still difficult to parallelize. This difficulty stems from many factors, including the complex syntax of recursive functions. This paper provides new equipment to deal with this problem. Such instrument consists of an insight, plus a code transformation that is enabled by this insight. Concerning the first contribution, we demonstrate that many recursive functions can be rewritten as a combination of associative operations. We group such functions into two categories, which involve monoid and semiring operations. Each of these categories admits a parallel implementation. To demonstrate the effectiveness of this idea, we have implemented an automatic code rewriting tool for Haskell, and have used it to convert six well-known recursive functions to algorithms that run in parallel. Our tool is totally automatic, and it is able to deliver non-trivial speedups upon the sequential version of the programs that it receives. In particular, the automatically generated parallel code delivers good scalability when varying the number of threads or the input size.<br/> &copy; 2018 Elsevier B.V.},
key = {Recursive functions},
keywords = {Algebra;Codes (symbols);Cosine transforms;Functional programming;Parallel processing systems;},
note = {Abstract algebra;Algebraic framework;Algorithmic skeleton;Automatic Parallelization;Automatically generated;Functional programs;Parallel implementations;Semiring operations;},
URL = {http://dx.doi.org/10.1016/j.scico.2018.01.004},
} 


@inproceedings{20184406015100,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ACM International Conference Proceeding Series},
journal = {ACM International Conference Proceeding Series},
year = {2018},
pages = {Faber-Castell - },
address = {Sao Carlos, SP, Brazil},
abstract = {The proceedings contain 10 papers. The topics discussed include: testing mobile apps: challenges, state of the art and future trends; Canvas MVP: an agile tool for digital transformation; testing environmental models supported by machine learning; test framework for Jenkins shared library; testing strategies for smart cities applications: a systematic mapping study; assessing agile testing practices for enterprise systems: a survey approach; gamification in software testing: a characterization study; a study on the impact of model evolution in MBT suites; evaluating the impact of different testers on model-based testing; using a search and model based framework to improve robustness tests in cloud platforms; mutation operators for code annotations; and mutation operators for code annotations.<br/>},
} 


@inproceedings{20184305983801,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {IWoR 2016 - Proceedings of the 1st International Workshop on Software Refactoring, co-located with ASE 2016},
journal = {IWoR 2016 - Proceedings of the 1st International Workshop on Software Refactoring, co-located with ASE 2016},
year = {2016},
pages = {ACM SIGAI; ACM SIGSOFT; IEEE; Singapore Management University - },
address = {Singapore, Singapore},
abstract = {The proceedings contain 7 papers. The topics discussed include: refactoring for software architecture smells; empirical evaluation of code smells in open source projects: preliminary results; measuring refactoring benefits: a survey of the evidence; graph-based approach for detecting impure refactoring from version commits; refactoring verification using model transformation; automated translation among EPSILON languages for performance-driven UML Software model refactoring; and full application of the extract interface refactoring: conceptual structures in the hands of master students.<br/>},
} 


@article{20154901646045,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Stochastic hyperfine interactions modeling libraryVersion 2},
journal = {Computer Physics Communications},
author = {Zacate, Matthew O. and Evenson, William E.},
volume = {199},
year = {2016},
pages = {180 - 181},
issn = {00104655},
abstract = {The stochastic hyperfine interactions modeling library (SHIML) provides a set of routines to assist in the development and application of stochastic models of hyperfine interactions. The library provides routines written in the C programming language that (1) read a text description of a model for fluctuating hyperfine fields, (2) set up the Blume matrix, upon which the evolution operator of the system depends, and (3)&nbsp;find the eigenvalues and eigenvectors of the Blume matrix so that theoretical spectra of experimental techniques that measure hyperfine interactions can be calculated. The optimized vector and matrix operations of the BLAS and LAPACK libraries are utilized. The original version of SHIML constructed and solved Blume matrices for methods that measure hyperfine interactions of nuclear probes in a single spin state. Version 2 provides additional support for methods that measure interactions on two different spin states such as M&ouml;ssbauer spectroscopy and nuclear resonant scattering of synchrotron radiation. Example codes are provided to illustrate the use of SHIML to (1) generate perturbed angular correlation spectra for the special case of polycrystalline samples when anisotropy terms of higher order than A<inf>22</inf>can be neglected and (2) generate M&ouml;ssbauer spectra for polycrystalline samples for pure dipole or pure quadrupole transitions. New version program summary Program title: SHIML Catalogue identifier: AEIF_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIF_v2_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU General Public License version 3 with supplemental citation provision No. of lines in distributed program, including test data, etc.: 88510 No. of bytes in distributed program, including test data, etc.: 3311047 Distribution format: tar.gz Programming language: C. Computer: Any. Operating system: LINUX, OS X. RAM: Variable Catalogue identifier of previous version: AEIF_v1_0 Journal reference of previous version: Comput. Phys. Comm. 182(2011)1061 Classification: 7.4. External routines: TAPP [1], BLAS [2], a C-interface to BLAS [3], and LAPACK [4]. Additionally, GSL [3] is needed to compile the example code that simulates M&ouml;ssbauer spectra. Does the new version supersede the previous version?: No Nature of problem: In condensed matter systems, hyperfine methods such as nuclear magnetic resonance (NMR), M&ouml;ssbauer effect (ME), muon spin rotation (&mu;SR), and perturbed angular correlation spectroscopy (PAC) measure electromagnetic fields due to electronic and magnetic structure within Angstroms of nuclear probes through the hyperfine interaction. When interactions fluctuate at rates comparable to the time scale of a hyperfine method, there is a loss in signal coherence, and spectra in the time domain are damped while spectra in the frequency domain are broadened. The degree of damping or broadening can be used to determine fluctuation rates, provided that theoretical expressions for spectra can be derived for relevant physical models of the fluctuations. SHIML provides routines to help researchers quickly develop code to incorporate stochastic models of fluctuating hyperfine interactions in calculations of hyperfine spectra. Solution method: Calculations are based on the method for modeling stochastic hyperfine interactions for PAC by Winkler and Gerdau [5]. The method is extended to include other hyperfine methods following the work of Dattagupta [6]. The code provides routines for reading model information from text files, allowing researchers to develop new models quickly without the need to modify computer code for each new model to be considered. Reasons for new version: The original version of the library provided support only for those methods that measure hyperfine interactions on one spin state of the nuclear probe. As such, it excluded important hyperfine methods that measure the interactions on two spin states such as M&ouml;ssbauer spectroscopy and nuclear resonant scattering of synchrotron radiation. The present version of SHIML provides the necessary support for such double spin-state methods while maintaining backward compatibility for code already developed using the original version. Summary of revisions: Routines now check that values representing nuclear spins are positive integers or positive half-integers. Additional utility functions are provided to make it easier for code developers to calculate Hamiltonians of electric quadrupole interactions. A correction was made to the portion of code responsible for calculating the Blume matrix of single spin-state methods; however, this change will not alter results obtained from single spin-state simulations using version 1 of the library. The remaining revisions support calculations for double spin-state methods. (1) Model-file syntax is expanded in order to allow users to specify different hyperfine interactions for ground and excited spin states and to input isomer shifts. (2) New routines for initialization and for Blume-matrix calculations are included for the double spin-state case. (3) New example code is provided to illustrate how SHIML can be used to simulate M&ouml;ssbauer spectra of polycrystalline samples for pure dipole or pure quadrupole transitions; background information about the M&ouml;ssbauer examples can be found in Ref. [7]. Finally, updated software documentation is included in a User's Guide as a PDF file in the code distribution. Running time: Variable References: [1] M. O. Zacate, The Adjustable Parameter Package, Technical Report 2, CINSAM Grant 2006-R7 (unpublished); available for download at http://tapp.nku.edu/.[2] L. S. Blackford et al., ACM Trans. Math. Soft. 28 (2002) 135; J. Dongarra, International Journal of High Performance Applications and Supercomputing 16 (2002) 1; http://www.netlib.org/blas/.[3] M. Galassi et al., GNU Scientific Library Reference Manual, third edition (2009); available for download at http://www.gnu.org/software/gsl/.[4] E. Anderson et al., LAPACK Users&rsquo; Guide, third ed. (Society for Industrial and Applied Mathematics, Philadelphia, PA, 1999); http://www.netlib.org/lapack/.[5] H. Winkler, E. Gerdau, Z. Phys. 262 (1973) 363.[6] S. Dattagupta, Hyperfine Interact. 11 (1981) 77.[7] M. O. Zacate, W. E. Evenson, Hyperfine Interact. 231 (2015) 143.<br/> &copy; 2015 Elsevier B.V.},
key = {Time domain analysis},
keywords = {Bimetals;C (programming language);Codes (symbols);Condensed matter physics;Eigenvalues and eigenfunctions;Electromagnetic fields;Electron spin resonance spectroscopy;Frequency domain analysis;Hamiltonians;Linux;Matrix algebra;Nuclear magnetic resonance;Nuclear magnetic resonance spectroscopy;Nuclear reactor licensing;Open source software;Probes;Software testing;Spin dynamics;Stochastic models;Stochastic systems;Synchrotron radiation;Synchrotrons;},
note = {Electric quadrupole interactions;High performance applications;Hyperfines;Mossbauer;Nuclear Magnetic Resonance (NMR);Perturbed angular correlation;Perturbed angular correlation spectroscopy;TDPAC;},
URL = {http://dx.doi.org/10.1016/j.cpc.2015.10.013},
} 


@inproceedings{20163502745421,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automatic Parallel Pattern Detection in the Algorithm Structure Design Space},
journal = {Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium, IPDPS 2016},
author = {Huda, Zia Ul and Atre, Rohit and Jannesari, Ali and Wolf, Felix},
year = {2016},
pages = {43 - 52},
address = {Chicago, IL, United states},
abstract = {Parallel design patterns have been developed to help programmers efficiently design and implement parallel applications. However, identifying a suitable parallel pattern for a specific code region in a sequential application is a difficult task. Transforming an application according to support structures applicable to these parallel patterns is also very challenging. In this paper, we present a novel approach to automatically find parallel patterns in the algorithm structure design space of sequential applications. In our approach, we classify code blocks in a region according to the appropriate supportstructure of the detected pattern. This classification eases the transformation of a sequential application into its parallel version. Weevaluated our approach on 17 applications from four different benchmark suites. Our method identified suitable algorithm structure patterns in the sequential applications. We confirmed our results by comparing them with the existing parallel versions of these applications. We also implemented the patterns we detected in cases in which parallel implementations were not available and achieved speedups of up to 14x.<br/> &copy; 2016 IEEE.},
key = {Pattern recognition},
keywords = {Benchmarking;},
note = {Design and implements;Parallel application;Parallel design patterns;Parallel implementations;Parallel patterns;Parallelism;Sequential applications;Task paralleism;},
URL = {http://dx.doi.org/10.1109/IPDPS.2016.60},
} 


@inproceedings{20162002373598,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Increasing Automation in the Backporting of Linux Drivers Using Coccinelle},
journal = {Proceedings - 2015 11th European Dependable Computing Conference, EDCC 2015},
author = {Rodriguez, Luis R. and Lawall, Julia},
year = {2015},
pages = {132 - 143},
address = {Paris, France},
abstract = {Software is continually evolving, to fix bugs and add new features. Industry users, however, often value stability, and thus may not be able to update their code base to the latest versions. This raises the need to selectively backport new features to older software versions. Traditionally, backporting has been done by cluttering the backported code with preprocessor directives, to replace behaviors that are unsupported in an earlier version by appropriate workarounds. This approach however, involves writing a lot of error-prone backporting code, and results in implementations that are hard to read and maintain. We consider this issue in the context of the Linux kernel, for whicholder versions are in wide use. We present a new backporting strategy that relies on the use of a backporting compatability library and on code that is automatically generated using the program transformation tool Coccinelle. This approach reduces the amount of code that must be manually written, and thus can help the Linux kernel backporting effort scale while maintainingthe dependability of the backporting process.<br/> &copy; 2015 IEEE.},
key = {Linux},
keywords = {Codes (symbols);Program debugging;},
note = {Automatically generated;Backports;Compatability;Error prones;Linux drivers;Linux kernel;Program transformations;Software versions;},
URL = {http://dx.doi.org/10.1109/EDCC.2015.23},
} 


@article{20180904837333,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {PAREMD: A parallel program for the evaluation of momentum space properties of atoms and molecules},
journal = {Computer Physics Communications},
author = {Meena, Deep Raj and Gadre, Shridhar R. and Balanarayan, P.},
volume = {224},
year = {2018},
pages = {299 - 310},
issn = {00104655},
abstract = {The present work describes a code for evaluating the electron momentum density (EMD), its moments and the associated Shannon information entropy for a multi-electron molecular system. The code works specifically for electronic wave functions obtained from traditional electronic structure packages such as GAMESS and GAUSSIAN. For the momentum space orbitals, the general expression for Gaussian basis sets in position space is analytically Fourier transformed to momentum space Gaussian basis functions. The molecular orbital coefficients of the wave function are taken as an input from the output file of the electronic structure calculation. The analytic expressions of EMD are evaluated over a fine grid and the accuracy of the code is verified by a normalization check and a numerical kinetic energy evaluation which is compared with the analytic kinetic energy given by the electronic structure package. Apart from electron momentum density, electron density in position space has also been integrated into this package. The program is written in C++ and is executed through a Shell script. It is also tuned for multicore machines with shared memory through OpenMP. The program has been tested for a variety of molecules and correlated methods such as CISD, M&oslash;ller&ndash;Plesset second order (MP2) theory and density functional methods. For correlated methods, the PAREMD program uses natural spin orbitals as an input. The program has been benchmarked for a variety of Gaussian basis sets for different molecules showing a linear speedup on a parallel architecture. Program summary: Program Title: PAREMD Program Files doi: http://dx.doi.org/10.17632/gcr9gmh6zv.1 Licensing provisions: GPLv3 Programming language: C, Csh External routines/libraries: GSL[1], BLAS[2,3], OpenMP[4], GAMESS[5,6] and GAUSSIAN[7]. Nature of problem: Momentum space properties for a multi-electron system. Solution method: Analytic Fourier transformation of Gaussian basis in position space to get momentum space basis followed by EMD evaluation on spherical or Cartesian grids. A numerical integration procedure is implemented for evaluating moments of EMD and Shannon information entropy on a polar grid. [1] Galassi et al, GNU Scientific Library Reference Manual (3rd Ed.), ISBN 0954612078. [2] L. S. Blackford, J. Demmel, J. Dongarra, I. Duff, S. Hammarling, G. Henry, M. Heroux, L. Kaufman, A. Lumsdaine, A. Petitet, R. Pozo, K. Remington, R. C. Whaley, An Updated Set of Basic Linear Algebra Subprograms (BLAS), ACM Trans. Math. Soft., 28-2 (2002), pp. 135&ndash;151. [3] J. Dongarra, Basic Linear Algebra Subprograms Technical Forum Standard, International Journal of High Performance Applications and Supercomputing, 16(1) (2002), pp. 1&ndash;111, and International Journal of High Performance Applications and Supercomputing, 16(2) (2002), pp. 115&ndash;199. [4] OpenMP Architecture Review Board, &rdquo;OpenMP Application Program Interface, Version 4.5&rdquo;, November 2015. [5] &rdquo;General Atomic and Molecular Electronic Structure System&rdquo; M.W. Schmidt, K.K. Baldridge, J.A. Boatz, S.T. Elbert, M.S. Gordon, J.H. Jensen, S. Koseki, N. Matsunaga, K.A. Nguyen, S.J. Su, T.L. Windus, M. Dupuis, J.A. MontgomeryJ. Comput. Chem. 14 1347&ndash;1363, 1993. [6] &ldquo;Advances in electronic structure theory: GAMESS a decade later M.S. Gordon, M.W. Schmidt pp. 1167&ndash;1189, in Theory and Applications of Computational Chemistry: the first forty years C.E. Dykstra, G. Frenking, K.S. Kim, G.E. Scuseria (editors), Elsevier, Amsterdam, 2005. [7] Gaussian 09, Revision C.01, M. J. Frisch, G. W. Trucks, H. B. Schlegel, G. E. Scuseria, M. A. Robb, J. R. Cheeseman, G. Scalmani, V. Barone, G. A. Petersson, H. Nakatsuji, X. Li, M. Caricato, A. Marenich, J. Bloino, B. G. Janesko, R. Gomperts, B. Mennucci, H. P. Hratchian, J. V. Ortiz, A. F. Izmaylov, J. L. Sonnenberg, D. Williams-Young, F. Ding, F. Lipparini, F. Egidi, J. Goings, B. Peng, A. Petrone, T. Henderson, D. Ranasinghe, V. G. Zakrzewski, J. Gao, N. Rega, G. Zheng, W. Liang, M. Hada, M. Ehara, K. Toyota, R. Fukuda, J. Hasegawa, M. Ishida, T. Nakajima, Y. Honda, O. Kitao, H. Nakai, T. Vreven, K. Throssell, J. A. Montgomery, Jr., J. E. Peralta, F. Ogliaro, M. Bearpark, J. J. Heyd, E. Brothers, K. N. Kudin, V. N. Staroverov, T. Keith, R. Kobayashi, J. Normand, K. Raghavachari, A. Rendell, J. C. Burant, S. S. Iyengar, J. Tomasi, M. Cossi, J. M. Millam, M. Klene, C. Adamo, R. Cammi, J. W. Ochterski, R. L. Martin, K. Morokuma, O. Farkas, J. B. Foresman, and D. J. Fox, Gaussian, Inc., Wallingford CT, 2016.<br/> &copy; 2017 Elsevier B.V.},
key = {C++ (programming language)},
keywords = {Application programming interfaces (API);Application programs;Carrier concentration;Codes (symbols);Computation theory;Computational chemistry;Density functional theory;Electron density measurement;Electronic structure;Electrons;Fourier transforms;Function evaluation;Gaussian distribution;Kinetic energy;Kinetics;Linear algebra;Memory architecture;Molecular orbitals;Molecules;Momentum;Open source software;Parallel architectures;Software testing;Wave functions;},
note = {Basic linear algebra subprograms;Density-functional methods;Electron momentum densities;Electronic structure calculations;Electronic structure theory;Electronic wave functions;High performance applications;Shannon information entropy;},
URL = {http://dx.doi.org/10.1016/j.cpc.2017.12.002},
} 


@article{20171203482953,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A method to localize faults in concurrent C programs},
journal = {Journal of Systems and Software},
author = {S. Alves, Erickson H. da and Cordeiro, Lucas C. and L. Filho, Eddie B. de},
volume = {132},
year = {2017},
pages = {336 - 352},
issn = {01641212},
abstract = {We describe a new approach to localize faults in concurrent programs, which is based on bounded model checking and sequentialization techniques. The main novelty is the idea of reproducing a faulty behavior, in a sequential version of a concurrent program. In order to pinpoint faulty lines, we analyze counterexamples generated by a model checker, to the new instrumented sequential program, and search for a diagnostic value, which corresponds to actual lines in a program. This approach is useful to improve debugging processes for concurrent programs, since it tells which line should be corrected and what values lead to a successful execution. We implemented this approach as a code-to-code transformation from concurrent into non-deterministic sequential programs, which are used as inputs to existing verification tools. Experimental results show that our approach is effective and capable of identifying faults in our benchmark set, which was extracted from the SV-COMP 2016 suite.<br/> &copy; 2017 Elsevier Inc.},
key = {C (programming language)},
keywords = {Cosine transforms;Model checking;Program debugging;Program diagnostics;},
note = {Bounded model checking;Concurrent software;Fault localization;Non Determinism;Sequentialization;},
URL = {http://dx.doi.org/10.1016/j.jss.2017.03.010},
} 


@inproceedings{20183105617359,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {32nd european conference on objectoriented programming (ECOOP 2018)},
journal = {Leibniz International Proceedings in Informatics, LIPIcs},
volume = {109},
year = {2018},
pages = {768 - },
issn = {18688969},
address = {Amsterdam, Netherlands},
abstract = {The proceedings contain 25 papers. The topics discussed include: fault-tolerant distributed reactive programming; theory and practice of coroutines with snapshots; a characteristic study of parameterized unit tests in .NET open source projects; learning to accelerate symbolic execution via code transformation; accelerating dynamically-typed languages on heterogeneous platforms using guards optimization; automating object transformations for dynamic software updating via online execution synthesis; LEGATO: an at-most-once analysis with applications to dynamic configuration updates; and efficient reflection string analysis via graph coloring.<br/>},
} 


@inproceedings{20174104250277,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Performance Analysis and Optimization of the FFTXlib on the Intel Knights Landing Architecture},
journal = {Proceedings of the International Conference on Parallel Processing Workshops},
author = {Wagner, Michael and Lopez, Victor and Morillo, Julian and Cavazzoni, Carlo and Affinito, Fabio and Gimenez, Judit and Labarta, Jesus},
volume = {0},
year = {2017},
pages = {243 - 250},
issn = {15302016},
address = {Bristol, United kingdom},
abstract = {In this paper, we address the decreasing performance of the FFTXlib, the Fast Fourier Transformation (FFT) kernel of Quantum ESPRESSO, when scaling to a full KNL node. An increased performance in the FFTXlib will likewise increase the performance of the entire Quantum ESPRESSO code one of the most used plane-wave DFT codes in the community of material science. Our approach focuses on, first, overlapping computation and communication and, second, decreasing resource contention for higher compute efficiency. In order to achieve this we use the OmpSs programming model based on task dependencies. We allow overlapping of computation and communication by converting all steps of the FFT into tasks following a flow dependency. In the same way, we decrease resource contention by converting each FFT into an individual task that can be scheduled asynchronously. In both cases, multiple FFTs can be computed in parallel. The task-based optimizations are implemented in the FFTXlib and show up to 10% runtime reduction on the already highly optimized version. Since the task scheduling is done dynamically during execution by the parallel runtime, not statically by the user, it also frees the user from finding the ideal parallel configuration himself.<br/> &copy; 2017 IEEE.},
key = {Fast Fourier transforms},
keywords = {Technical presentations;Tools;},
note = {Extrae;FFTXlib;Paraver;Performance analysis;Quantum Espresso;Tracing;Xeon Phi;},
URL = {http://dx.doi.org/10.1109/ICPPW.2017.44},
} 


@inproceedings{20171703609422,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
journal = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
volume = {237},
year = {2017},
pages = {Generalitat Valenciana PROMETEOII/2015/013; MINECO TIN 2015-69175-C4-1-R; Universitat Politecnica de Valencia - },
issn = {20752180},
address = {Salamanca, Spain},
abstract = {The proceedings contain 5 papers. The topics discussed include: a tutorial on using Dafny to construct verified software; comparing MapReduce and pipeline implementations for counting triangles; towards a semantics-aware code transformation toolchain for heterogeneous systems; towards automatic learning of heuristics for mechanical transformations of procedural code; and an introduction to liquid Haskell.},
} 


@inproceedings{20130515950403,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Static compilation analysis for host-accelerator communication optimization},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Amini, Mehdi and Coelho, Fabien and Irigoin, Francois and Keryell, Ronan},
volume = {7146 LNCS},
year = {2013},
pages = {237 - 251},
issn = {03029743},
address = {Fort Collins, CO, United states},
abstract = {We present an automatic, static program transformation that schedules and generates efficient memory transfers between a computer host and its hardware accelerator, addressing a well-known performance bottleneck. Our automatic approach uses two simple heuristics: to perform transfers to the accelerator as early as possible and to delay transfers back from the accelerator as late as possible. We implemented this transformation as a middle-end compilation pass in the pips /Par4All compiler. In the generated code, redundant communications due to data reuse between kernel executions are avoided. Instructions that initiate transfers are scheduled effectively at compile-time. We present experimental results obtained with the Polybench 2.0, some Rodinia benchmarks, and with a real numerical simulation. We obtain an average speedup of 4 to 5 when compared to a nai&die;ve parallelization using a modern gpu with Par4All, hmpp, and pgi, and 3.5 when compared to an OpenMP version using a 12-core multiprocessor. &copy; 2013 Springer-Verlag.<br/>},
key = {Parallel architectures},
keywords = {Application programming interfaces (API);Computer hardware;Graphics processing unit;Optimization;Program compilers;},
note = {Automatic approaches;Automatic Parallelization;Communication optimization;Hardware accelerators;Performance bottlenecks;Simple heuristics;Source-to-source compilation;Static compilation;},
URL = {http://dx.doi.org/10.1007/978-3-642-36036-7_16},
} 


@article{20155201711676,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An automatic method for assessing the versions affected by a vulnerability},
journal = {Empirical Software Engineering},
author = {Nguyen, Viet Hung and Dashevskyi, Stanislav and Massacci, Fabio},
volume = {21},
number = {6},
year = {2016},
pages = {2268 - 2297},
issn = {13823256},
abstract = {Vulnerability data sources are used by academics to build models, and by industry and government to assess compliance. Errors in such data sources therefore not only are threats to validity in scientific studies, but also might cause organizations, which rely on retro versions of software, to lose compliance. In this work, we propose an automated method to determine the code evidence for the presence of vulnerabilities in retro software versions. The method scans the code base of each retro version of software for the code evidence to determine whether a retro version is vulnerable or not. It identifies the lines of code that were changed to fix vulnerabilities. If an earlier version contains these deleted lines, it is highly likely that this version is vulnerable. To show the scalability of the method we performed a large scale experiments on Chrome and Firefox (spanning 7,236 vulnerable files and approximately 9,800 vulnerabilities) on the National Vulnerability Database (NVD). The elimination of spurious vulnerability claims (e.g. entries to a vulnerability database such as NVD) found by our method may change the conclusions of studies on the prevalence of foundational vulnerabilities.<br/> &copy; 2015, Springer Science+Business Media New York.},
key = {Codes (symbols)},
keywords = {Database systems;},
note = {Browsers;Empirical validation;National vulnerability database;Software security;Vulnerability analysis;},
URL = {http://dx.doi.org/10.1007/s10664-015-9408-2},
} 


@inproceedings{20183905878851,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {30th IFIP International Conference on Testing Software and Systems, ICTSS 2018},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {11146 LNCS},
year = {2018},
issn = {03029743},
address = {Cadiz, Spain},
abstract = {The proceedings contain 14 papers. The special focus in this conference is on Testing Software and Systems. The topics include: Neural networks as artificial specifications; combining model learning and data analysis to generate models of component-based systems; deriving tests with guaranteed fault coverage for finite state machines with timeouts; from ontologies to input models for combinatorial testing; validation of transformation from abstract state machine models to C++ code; security testing for chatbots; JMCTest: Automatically testing inter-method contracts in Java; testing ambient assisted living solutions with simulations; Generating OCL constraints from test case schemas for testing model behavior; Test derivation for SDN-enabled switches: A logic circuit based approach; an energy aware testing framework for smart-spaces; c++11/14 mutation operators based on common fault patterns.<br/>},
} 


@inproceedings{20162002403726,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Enriching model execution with feedback to support testing of semantic conformance between models and requirements: Design and evaluation of feedback automation architecture},
journal = {AMARETTO 2016 - Proceedings of the International Workshop on DomAin Specific Model-Based AppRoaches to vErificaTion and validaTiOn},
author = {Sedrakyan, Gayane and Snoeck, Monique},
year = {2016},
pages = {14 - 22},
address = {Rome, Italy},
abstract = {Model Driven Development (MDD) has traditionally been used to support model transformations and code generation. While plenty of techniques and tools are available to support modeling and transformations, tool support for checking the model quality in terms of semantic conformance with respect to the domain requirements is largely absent. In this work we present a model verification and validation approach based on model-driven feedback generation in a model-to-code transformation. The transformation is achieved using a single click. The generated output of the transformation is a compiled code which is achieved by a single click. This also serves as a rapid prototyping instrument that allows simulating a model (the terms prototyping and simulation are thus used interchangeably in the paper). The proposed feedback incorporation method in the generated prototype allows linking event execution in the generated code to its causes in the model used as input for the generation. The goal of the feedback is twofold: (1) to assist a modeler in validating semantic conformance of a model with respect to a domain to be engineered; (2) to support the learning perspective of less experienced modelers (such as students or junior analysts in their early career) by allowing them to detect modeling errors that result from the misinterpreted use of modeling language constructs. Within this work we focus on conceptual and platform independent models (PIM) that make use of two prominent UML diagrams - a class diagram (for modeling the structure of a system) and multiple interacting statecharts (for modeling a system's dynamic behavior). The tool has been used in the context of teaching a requirements analysis and modeling course at KU Leuven. The proposed feedback generation technique has been constantly validated by means of "usability" evaluations, and demonstrates a high level of self-reported utility of the feedback. Additionally, the findings of our experimental studies also show a significant positive impact of feedback-enabled rapid prototyping method on semantic validation capabilities of novices. Despite our focus on specific diagramming techniques, the principles of the approach presented in this work can be used to support educational feedback automation for a broader spectrum of diagram types in the context of MDD and simulation.<br/> Copyright &copy; 2016 by SCITEPRESS - Science and Technology Publications.},
key = {Modeling languages},
keywords = {Automation;Codes (symbols);Cosine transforms;Markup languages;Rapid prototyping;Semantics;Systems analysis;Teaching;},
note = {Conceptual model;Design and evaluations;Generation techniques;Model driven development;Model testing;Platform independent model;Requirements analysis;Validation;},
} 


@article{20163402727998,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Influence apart from Adoption: How Interaction between Programming and Scientific Practices Shapes Modes of Inquiry in Four Oceanography Teams},
journal = {ProQuest Dissertations and Theses Global},
author = {Kuksenok, Kateryna},
year = {2016},
abstract = {Scientists have been producing and sharing code for decades. Code work done by scientists spans simulation, data processing, analysis, visualization, communication, and data stewardship. Robust instrumentation generates data beyond the scale and comprehension of individuals in their current tools, requiring new approaches to automation and collaboration. Sophisticated web frameworks enable more interactive web portals for displaying data or simulation results to various stakeholders. Educational initiatives that target scientists learning to program are increasingly available, and increasingly reach enrollment limits. Given this context, how is programming in science changing? I argue that changes enacted in scientific programming practices are intended to lead to code that is not only exists and runs for a long time, but continues to be understandable, usable, and extensible. My dissertation examines the interaction between coding practices and scientific inquiry. Although deliberate change is oriented toward this goal, a particular tool or protocol does not require sustained use by a group to have impact on the work practices of that group. In this dissertation, I develop an alternative conceptual framework for reflecting on the goals and outcomes of change. My findings are based on over 300 hours of observation of a total of 46 scientists from four different oceanography groups. Of the 46 scientists, 21 comprise the core study participants, doing code work at graduate, post-graduate, and faculty levels. Of the four oceanography groups I studied, two focus on simulation and two on observational data analysis. All engaged in deliberate, reflective change of their programming skills and practices. In collecting and analysing qualitative data, I focused on "code work" in a broader sense, rather than referring to "scripting," "high-performance computing," "scientific software engineering," "data science," or other more specific terms that imply particular working environments and aesthetics. Maintaining an inclusive scope allowed me to not only draw parallels between these practices, but also to consider ways in which they intersect and influence one another. Particular practices or philosophies can be pervasive through all layers of code work, from maintaining a co-authored LaTeX-typeset manuscript in GitHub, to "adding biology" to a well-established model, to implementing an automated test suite for an analytic pipeline that generates daily results and images for a web endpoint. I propose a conceptual framework of change and use stories from my qualitative study to illustrate its components and dynamics. This framework defines relationships between (1) the working environment, which is subject to deliberate change; (2) the perfect world, which directs that change; and the (3) moment of flux, which constitutes taking action to bring about a change and its immediate outcome. The working environment combines resources that are technical (e.g., iPython Notebook, Google search), cognitive (e.g., looking at many small charts encoding information in a familiar and consistent way to aid quick understanding), and social (e.g., a shared office with frequent "hey, how do you [do a particular tricky thing]?"). The working environment is subject to change, including changes in not only the technical components (e.g., tools) but also cognitive (e.g., skills) and social (e.g., communication practices and language). This change is informed and directed by a collective imagination of a perfect world: the moving target to which possible modifications to the current way of working can be compared. The moment of flux when a scientist elects to pursue deliberate change requires both momentum and opportunity, which can arise in the wake of a breakdown of the prior approach, in the space created by embarking on a new project, or through an energizing workshop or group event. These situations allow the exploration of options that are already in the awareness or intention. Actually making the ``leap'' of deliberate change to integrate an unfamiliar component or learn a new skill is associated with uncertainty and the possibility of disappointment or failure. The conceptual framework I propose creates optimistic vocabulary for reflecting upon these changes. As projects involve more people, longer time spans, and more ambitious collaboration between disciplines, understanding how coding practices influence scientific inquiry is increasingly important. The discussion of "best practices" in open science encourages the sharing of negative results and disappointing data as a top priority. This call for reflection on failure must be extended to include code work. With data as well as with code sharing, repeated "best practices" are not sufficient to inspire change, even for those scientists who openly feel they "should" do it. I present qualitative findings that demonstrate concrete ways to deliver interpersonal rewards in the wake of particular efforts not panning out as well as hoped or intended. ProQuest Subject Headings: Computer science, Physical oceanography.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.},
key = {Search engines},
keywords = {Codes (symbols);Computer science;Data handling;Data visualization;Oceanography;Philosophical aspects;Portals;Wakes;},
note = {Communication practices;Conceptual frameworks;Encoding informations;High performance computing;Observational data;Physical oceanography;Scientific programming;Working environment;},
} 


@article{20160501877733,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Photoactivation of Mutant Isocitrate Dehydrogenase 2 Reveals Rapid Cancer-Associated Metabolic and Epigenetic Changes},
journal = {Journal of the American Chemical Society},
author = {Walker, Olivia S. and Elsasser, Simon J. and Mahesh, Mohan and Bachman, Martin and Balasubramanian, Shankar and Chin, Jason W.},
volume = {138},
number = {3},
year = {2016},
pages = {718 - 721},
issn = {00027863},
abstract = {Isocitrate dehydrogenase is mutated at a key active site arginine residue (Arg172 in IDH2) in many cancers, leading to the synthesis of the oncometabolite (R)-2-hydroxyglutarate (2HG). To investigate the early events following acquisition of this mutation in mammalian cells we created a photoactivatable version of IDH2(R172K), in which K172 is replaced with a photocaged lysine (PCK), via genetic code expansion. Illumination of cells expressing this mutant protein led to a rapid increase in the levels of 2HG, with 2HG levels reaching those measured in patient tumor samples, within 8 h. 2HG accumulation is closely followed by a global decrease in 5-hydroxymethylcytosine (5-hmC) in DNA, demonstrating that perturbations in epigenetic DNA base modifications are an early consequence of mutant IDH2 in cells. Our results provide a paradigm for rapidly and synchronously uncloaking diverse oncogenic mutations in live cells to reveal the sequence of events through which they may ultimately cause transformation.<br/> &copy; 2016 American Chemical Society.},
key = {Cells},
keywords = {Amino acids;Cytology;Diseases;Mammals;},
note = {5-hydroxymethylcytosine;Arginine residue;Isocitrate dehydrogenase;Mammalian cells;Mutant proteins;Photo activations;Photoactivatable;Sequence of events;},
URL = {http://dx.doi.org/10.1021/jacs.5b07627},
} 


@inproceedings{20184906170855,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Seamless GPU evaluation of smart expression templates},
journal = {Proceedings - 2018 International Conference on High Performance Computing and Simulation, HPCS 2018},
author = {Wicht, Baptiste and Fischer, Andreas and Hennebert, Jean},
year = {2018},
pages = {196 - 203},
address = {Orleans, France},
abstract = {Expression Templates is a technique allowing to write linear algebra code in C++ the same way it would be written on paper. It is also used extensively as a performance optimization technique, especially as the Smart Expression Templates form which allows for even higher performance. It has proved to be very efficient for computation on a Central Processing Unit (CPU). However, due to its design, it is not easily implemented on a Graphics Processing Unit (GPU). In this paper, we devise a set of techniques to allow the seamless evaluation of Smart Expression Templates on the GPU. The execution is transparent for the user of the library which still uses the matrices and vector as if it was on the CPU and profits from the performance and higher multi-processing capabilities of the GPU. We also show that the GPU version is significantly faster than the CPU version, without any change to the code of the user.<br/> &copy; 2018 IEEE.},
key = {Graphics processing unit},
keywords = {C++ (programming language);Computer graphics;Computer graphics equipment;Computer programming languages;Image coding;Linear algebra;Object oriented programming;Program processors;},
note = {Expression templates;General Purpose Computation on Graphics Processing Unit (GPGPU);Graphics Processing Unit (GPU);High performance computing;Multi-processing;Performance optimizations;},
URL = {http://dx.doi.org/10.1109/HPCS.2018.00045},
} 


@inproceedings{20174004236136,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards a Principled Integration of Multi-camera Re-identification and Tracking Through Optimal Bayes Filters},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
author = {Beyer, Lucas and Breuers, Stefan and Kurin, Vitaly and Leibe, Bastian},
volume = {2017-July},
year = {2017},
pages = {1444 - 1453},
issn = {21607508},
address = {Honolulu, HI, United states},
abstract = {With the rise of end-to-end learning through deep learning, person detectors and re-identification (ReID) models have recently become very strong. Multi-target multicamera (MTMC) tracking has not fully gone through this transformation yet. We intend to take another step in this direction by presenting a theoretically principled way of integrating ReID with tracking formulated as an optimal Bayes filter. This conveniently side-steps the need for dataassociation and opens up a direct path from full images to the core of the tracker. While the results are still sub-par, we believe that this new, tight integration opens many interesting research opportunities and leads the way towards full end-to-end tracking from raw pixels. Code and models for all experiments are publicly available.<br/> &copy; 2017 IEEE.},
key = {Deep learning},
keywords = {Bandpass filters;Computer vision;},
note = {Bayes filter;Direct paths;Multi-cameras;Multi-targets;Person detector;Re identifications;Research opportunities;Tight integrations;},
URL = {http://dx.doi.org/10.1109/CVPRW.2017.187},
} 


@inproceedings{20151500729903,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CEUR Workshop Proceedings},
journal = {CEUR Workshop Proceedings},
volume = {1214},
year = {2014},
pages = {126 - },
issn = {16130073},
address = {Demanovska Dolina - Jasna, Slovakia},
abstract = {The proceedings contain 18 papers. The topics discussed include: machine translation within one language as a paraphrasing technique; employing evolutionary algorithms for classification of astrophysical spectra; transformation of pipeline stage algorithms to event-driven code; acting and Bayesian reinforcement structure learning of partially observable environment; smart and easy object tracking; control of depth-sensing camera via plane of interaction; extracting product data from e-shops; an improved algorithm for ancestral gene order reconstruction; deviations prediction in timetables based on AVL data; neuropeptide recognition by machine learning methods; fairytale child chatbot; towards automation of ontology analysis reporting; and contemplating efficiency of the root file format for data intensive simulations in particle physics.},
} 


@article{20181705110214,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Variability abstractions for lifted analyses},
journal = {Science of Computer Programming},
author = {Dimovski, Aleksandar S. and Brabrand, Claus and Wasowski, Andrzej},
volume = {159},
year = {2018},
pages = {1 - 27},
issn = {01676423},
abstract = {Family-based (lifted) static analysis for &ldquo;highly configurable programs&rdquo; (program families) is capable of analyzing all variants at once without generating any of them explicitly. It takes as input only the common code base, which encodes all variants of a program family, and produces precise analysis results corresponding to all variants. However, the computational cost of the lifted analysis still depends inherently on the number of variants, which is in the worst case exponential in the number of statically configurable options (features). For a large number of features, the lifted analysis may be too costly or even infeasible. In this work, we introduce variability abstractions defined as Galois connections, which simplify variability away from program families based on #ifdef-s. Then, we use abstract interpretation as a formal method for the calculational-based derivation of abstracted lifted analyses, which are sound by construction. Our approach for abstracting lifted analysis is orthogonal to the particular program analysis chosen as a client. While a single program analysis operates on program states and depends on language-specific constructs, the lifted analysis assumes that a single program analysis already exists and lifts its results to all variants of the analyzed program family. Variability abstractions aim to reduce this variability-specific component of the lifted analysis, which handles variability and #ifdef-s. Furthermore, given the &ldquo;orthogonality&rdquo; of variability abstractions to the rest of the analysis (its language-specific component), we can implement abstractions as a preprocessor. In particular, given an abstraction we define a syntactic transformation, which translates any program family into an abstracted version of it, such that the analysis of the abstracted program family coincides with the corresponding abstracted analysis of the original program family. We have implemented the proposed approach, and we evaluate its practicality on three Java benchmarks. The evaluation shows that abstractions yield significant performance gains, especially for families with higher variability.<br/> &copy; 2018},
key = {Static analysis},
keywords = {Abstracting;Formal methods;Model checking;},
note = {Abstract interpretations;Computational costs;Galois connection;Performance Gain;Program analysis;Program family;Specific component;Syntactic transformations;},
URL = {http://dx.doi.org/10.1016/j.scico.2017.12.012},
} 


@article{20173104005246,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Energy use implications of different design strategies for multi-storey residential buildings under future climates},
journal = {Energy},
author = {Tettey, Uniben Yao Ayikoe and Dodoo, Ambrose and Gustavsson, Leif},
volume = {138},
year = {2017},
pages = {846 - 860},
issn = {03605442},
abstract = {The effects of climate change on the final and primary energy use of versions of a multi-storey residential building have been analysed. The building versions are designed to the Swedish building code (BBR 2015) and passive house criteria (Passive 2012) with different design and overheating control strategies under different climate scenarios. Future climate datasets are based on Representative Concentration Pathway scenarios for 2050&ndash;2059 and 2090&ndash;2099. The analysis showed that strategies giving the lowest space heating and cooling demands for the Passive 2012 building version remained the same under all climate scenarios. In contrast, strategies giving the lowest space heating and cooling demands for the BBR 2015 version varied, as cooling demand became more significant under future climate scenarios. Cooling demand was more dominant than heating for the Passive 2012 building version under future climate scenarios. Household equipment and technical installations based on best available technology gave the biggest reduction in total primary energy use among considered strategies. Overall, annual total operation primary energy decreased by 37&ndash;54% for the building versions when all strategies are implemented under the considered climate scenarios. This study shows that appropriate design strategies could result in significant primary energy savings for low-energy buildings under changing climates.<br/> &copy; 2017 Elsevier Ltd},
key = {Climate change},
keywords = {Architectural design;Energy conservation;Energy utilization;Heating;Housing;Space heating;},
note = {Control measures;Heating and cooling;Primary energies;Representative concentration pathways (RCPs);Residential building;},
URL = {http://dx.doi.org/10.1016/j.energy.2017.07.123},
} 


@article{20184406022219,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Applying Transactional Memory for Concurrency-Bug Failure Recovery in Production Runs},
journal = {IEEE Transactions on Parallel and Distributed Systems},
author = {Chen, Yuxi and Wang, Shu and Lu, Shan and Sankaralingam, Karthikeyan},
year = {2018},
issn = {10459219},
abstract = {Concurrency bugs widely exist and severely threaten system availability. Techniques that help recover from concurrency-bug failures during production runs are highly desired. This paper proposes BugTM, an approach that applies transactional memory techniques for concurrency-bug recovery in production runs. Requiring no knowledge about where are concurrency bugs, BugTM uses static analysis and code transformation to enable BugTM-transformed software to recover from a concurrency-bug failure by rolling back and re-executing the recent history of a failure thread. BugTM is instantiated as three schemes that have different trade-offs in performance and recovery capability: BugTMH uses existing hardware transactional memory (HTM) support, BugTMS leverages software transactional memory techniques, and BugTMHS is a software-hardware hybrid design. BugTM greatly improves the recovery capability of state-of-the-art techniques with low run-time overhead and no changes to OS or hardware, while guarantees not to introduce new bugs.<br/> IEEE},
key = {Program debugging},
keywords = {Availability;Computer hardware;Computer software;Concurrency control;Cosine transforms;Economic and social effects;Hardware;Production;Recovery;Static analysis;Storage allocation (computer);},
note = {Check pointing;Computer bugs;Concurrency bugs;Concurrent computing;Failure recovery;Message systems;Software availability;Transactional memory;},
URL = {http://dx.doi.org/10.1109/TPDS.2018.2877656},
} 


@inproceedings{20165103146972,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Natural supervised hashing},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
author = {Liu, Qi and Lu, Hongtao},
volume = {2016-January},
year = {2016},
pages = {1788 - 1794},
issn = {10450823},
address = {New York, NY, United states},
abstract = {Among learning-based hashing methods, supervised hashing tries to find hash codes which preserve semantic similarities of original data. Recent years have witnessed much efforts devoted to design objective functions and optimization methods for supervised hashing learning, in order to improve search accuracy and reduce training cost. In this paper, we propose a very straightforward supervised hashing algorithm and demonstrate its superiority over several state-of-the-art methods. The key idea of our approach is to treat label vectors as binary codes and to learn target codes which have similar structure to label vectors. To circumvent direct optimization on large n &times; n Gram matrices, we identify an inner-product-preserving transformation and use it to bring close label vectors and hash codes without changing the structure. The optimization process is very efficient and scales well. In our experiment, training 16-bit and 96-bit code on NUS-WIDE cost respectively only 3 and 6 minutes.<br/>},
key = {Codes (symbols)},
keywords = {Artificial intelligence;Hash functions;Linear transformations;Semantics;Structural optimization;},
note = {Design objectives;Direct optimization;Hashing algorithms;Hashing method;Optimization method;Search accuracy;Semantic similarity;State-of-the-art methods;},
} 


@article{20183805839087,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Multiple feature fusion for unconstrained palm print authentication},
journal = {Computers and Electrical Engineering},
author = {Jaswal, Gaurav and Kaul, Amit and Nath, Ravinder},
volume = {72},
year = {2018},
pages = {53 - 78},
issn = {00457906},
abstract = {Over the last decade, palm print recognition has emerged as the strongest technology for human authentication in many aspects. To carry out an effective recognition, this paper presents a feature level fusion of block-wise scale invariant feature transform and texture code co-occurrence matrix based features. Initially, an attempt to access the quality of extracted region of interest image is made. This is followed by application of fractional differential mask resulting in improvement of textural detail. In order to select the most discriminate palm features, a feature transformation algorithm inspired by subspace learning is employed. It led to reduction in computation time and feature dimensions, along with higher level of performance. A trained support vector machine utilizes the selected features to determine whether image belongs to genuine or imposter class. Comparative experimental analysis described in this paper indicates customarily outperforming results than competing methods and validate efficacy of proposed approach.<br/> &copy; 2018 Elsevier Ltd},
key = {Vectors},
keywords = {Authentication;Biometrics;Image segmentation;Information fusion;Palmprint recognition;Support vector machines;Vector quantization;},
note = {Fractional differential;Fractional masks;Multiple feature fusion;Palmprints;Principal subspace;Region-of-interest images;Scale invariant feature transforms;Sift;},
URL = {http://dx.doi.org/10.1016/j.compeleceng.2018.09.006},
} 


@inproceedings{20181004855635,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {International conference on Software Technologies: Applications and Foundations, STAF 2017},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {10748 LNCS},
year = {2018},
issn = {03029743},
address = {Marburg, Germany},
abstract = {The proceedings contain 37 papers. The special focus in this conference is on Software Technologies: Applications and Foundations. The topics include: On the Need for Temporal Model Repositories; on the Need for Artifact Models in Model-Driven Systems Engineering Projects; cognifying Model-Driven Software Engineering; non-human Modelers: Challenges and Roadmap for Reusable Self-explanation; Some Narrow and Broad Challenges in MDD; modelling by the People, for the People; from Building Systems Right to Building Right Systems: A Generic Architecture and Its Model Based Realization; the Tool Generation Challenge for Executable Domain-Specific Modeling Languages; toward Product Lines of Mathematical Models for Software Model Management; Introduction of an OpenCL-Based Model Transformation Engine; model-Driven Interaction Design for Social Robots; towards Integration of Context-Based and Scenario-Based Development; (An Example for) Formally Modeling Robot Behavior with UML and OCL; Synthesizing Executable PLC Code for Robots from Scenario-Based GR(1) Specifications; evaluating a Graph Query Language for Human-Robot Interaction Data in Smart Environments; a Simulation Framework to Analyze Knowledge Exchange Strategies in Distributed Self-adaptive Systems; Workshop in OCL and Textual Modelling: Report on Recent Trends and Panel Discussions; improving Incremental and Bidirectional Evaluation with an Explicit Propagation Graph; Translating UML-RSDS OCL to ANSI C; Mapping USE Specifications into Spec#; collaborative Modelling with Version Control; Deterministic Lazy Mutable OCL Collections; Step 0: An Idea for Automatic OCL Benchmark Generation; SICOMORo-CM: Development of Trustworthy Systems via Models and Advanced Tools; Developer-Centric Knowledge Mining from Large Open-Source Software Repositories (CROSSMINER); technical Obsolescence Management Strategies for Safety-Related Software for Airborne Systems; loose Graph Simulations.<br/>},
} 


@inproceedings{20161902351733,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ICSOFT-PT 2015 - 10th International Conference on Software Paradigm Trends, Proceedings; Part of 10th International Joint Conference on Software Technologies, ICSOFT 2015},
journal = {ICSOFT-PT 2015 - 10th International Conference on Software Paradigm Trends, Proceedings; Part of 10th International Joint Conference on Software Technologies, ICSOFT 2015},
year = {2015},
pages = {Institute for Systems and Technologies of Information, Control and Communication (INSTICC) - },
address = {Colmar, Alsace, France},
abstract = {The proceedings contain 17 papers. The topics discussed include: SuperMod - a model-driven tool that combines version control and software product line engineering; semantic version management based on formal certification; Java-meets Eclipse - an IDE for teaching Java following the object-later approach; systematic identification of information flows from requirements to support privacy impact assessments; model checking to improve precision of design pattern instances identification in OO systems; transformation from R-UML to R-TNCES: new formal solution for verification of flexible control systems; a tool for management of knowledge dispersed throughout multiple references; a pi-calculus-based approach for the verification of UML2 sequence diagrams; on a-posteriori integration of Ecore models and hand-written Java code; OCL for rich domain models implementation - an incremental aspect based solution; and novel approach for computing skyline services with fuzzy consistent model for QoS- based service composition.},
} 


@article{20162402491961,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The flame graph},
journal = {Communications of the ACM},
author = {Gregg, Brendan},
volume = {59},
number = {6},
year = {2016},
pages = {48 - 57},
issn = {00010782},
abstract = {AN EVERYDAY PROBLEM in our industry is understanding how software is consuming resources, particularly CPUs. What exactly is consuming how much, and how did this change since the last software version? These questions can be answered using software profilers - tools that help direct developers to optimize their code and operators to tune their environment. The output of profilers can be verbose, however, making it laborious to study and comprehend. The flame graph provides a new visualization for profiler output and can make for much faster comprehension, reducing the time for root cause analysis. Copyright held by author.<br/>},
key = {Program processors},
keywords = {Computer applications;Computer science;},
note = {Root cause analysis;Software versions;},
URL = {http://dx.doi.org/10.1145/2909476},
} 


@inproceedings{20133716735743,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Implicit coupling approach for simulation of charring carbon ablators},
journal = {44th AIAA Thermophysics Conference},
author = {Chen, Yih-Kanq and Gokcen, Tahir},
year = {2013},
pages = {American Institute of Aeronautics and Astronautics (AIAA) - },
address = {San Diego, CA, United states},
abstract = {This study demonstrates that coupling of a material thermal response code and a flow solver with nonequilibrium gas/surface interaction for simulation of charring carbon ablators can be performed using an implicit approach. The material thermal response code used in this study is the three-dimensional version of Fully Implicit Ablation and Thermal response program, which predicts charring material thermal response and shape change on hypersonic space vehicles. The flow code solves the reacting Navier-Stokes equations using Data Parallel Line Relaxation method. Coupling between the material response and flow codes is performed by solving the surface mass balance in flow solver and the surface energy balance in material response code. Thus, the material surface recession is predicted in flow code, and the surface temperature and pyrolysis gas injection rate are computed in material response code. It is demonstrated that the time-lagged explicit approach is sufficient for simulations at low surface heating conditions, in which the surface ablation rate is not a strong function of the surface temperature. At elevated surface heating conditions, the implicit approach has to be taken, because the carbon ablation rate becomes a stiff function of the surface temperature, and thus the explicit approach appears to be inappropriate resulting in severe numerical oscillations of predicted surface temperature. Implicit coupling for simulation of arc-jet models is performed, and the predictions are compared with measured data. Implicit coupling for trajectory based simulation of Stardust fore-body heat shield is also conducted. The predicted stagnation point total recession is compared with that predicted using the chemical equilibrium surface assumption.<br/>},
key = {Fighter aircraft},
keywords = {Ablation;Atmospheric temperature;Carbon;Codes (symbols);Flow simulation;Heat shielding;Hypersonic vehicles;Navier Stokes equations;Surface properties;},
note = {Charring materials;Chemical equilibriums;Gas injection rate;Hypersonic space vehicles;Material response;Numerical oscillation;Surface mass balance;Surface temperatures;},
} 


@inproceedings{20142817914668,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Implicit coupling approach for simulation of charring carbon ablators},
journal = {Journal of Spacecraft and Rockets},
author = {Chen, Yih-Kanq and Gokcen, Tahir},
volume = {51},
number = {3},
year = {2014},
pages = {779 - 788},
issn = {00224650},
abstract = {This study demonstrates that coupling of a material thermal response code and a flow solver with nonequilibrium gas-surface interaction for simulation of charring carbon ablators can be performed using an implicit approach. The material thermal response code used in this study is the three-dimensional version of fully implicit ablation and thermal response program, which predicts charring material thermal response and shape change on hypersonic space vehicles. The flow code solves the reacting Navier-Stokes equations using data-parallel line relaxation method. Coupling between the material response and flow codes is performed by solving the surface mass balance in the flow solver and the surface energy balance in the material response code. Thus, the material surface recession is predicted in the flow code, and the surface temperature and pyrolysis gas injection rate are computed in the material response code. It is demonstrated that the time-lagged explicit approach is sufficient for simulations at low surface heating conditions, in which the surface ablation rate is not a strong function of the surface temperature. At elevated surface heating conditions, the implicit approach has to be taken because the carbon ablation rate becomes a stiff function of the surface temperature, and thus the explicit approach appears to be inappropriate, resulting in severe numerical oscillations of predicted surface temperature. Implicit coupling for simulation of arc-jet models is performed, and the predictions are compared with measured data. Implicit coupling for trajectory-based simulation of Stardust forebody heat shield is also conducted. The predicted stagnation point total recession is compared with that predicted using the chemical equilibrium surface assumption. Copyright &copy; 2013 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved.<br/>},
key = {Fighter aircraft},
keywords = {Ablation;Atmospheric temperature;Carbon;Codes (symbols);Flow simulation;Heat shielding;Hypersonic vehicles;Navier Stokes equations;Surface properties;},
note = {Charring materials;Chemical equilibriums;Gas injection rate;Gas-surface interaction;Hypersonic space vehicles;Numerical oscillation;Surface mass balance;Surface temperatures;},
URL = {http://dx.doi.org/10.2514/1.A32753},
} 


@inproceedings{20184406023920,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {18th Asia Simulation Conference, AsiaSim 2018},
journal = {Communications in Computer and Information Science},
volume = {946},
year = {2018},
issn = {18650929},
address = {Kyoto, Japan},
abstract = {The proceedings contain 45 papers. The special focus in this conference is on Asia Simulation. The topics include: Deep Dissimilarity Measure for Trajectory Analysis; Performance Comparison of Eulerian Kinetic Vlasov Code Between Xeon Phi KNL and Xeon Broadwell; heterogeneous Scalable Multi-languages Optimization via Simulation; smart Simulation Cloud (Simulation Cloud 2.0)&mdash;The Newly Development of Simulation Cloud; a Semantic Composition Framework for Simulation Model Service; dynamic Optimization of Two-Coil Power-Transfer System Using L-Section Matching Network for Magnetically Coupled Intrabody Communication; demand and Supply Model for the Natural Gas Supply Chain in Colombia; Deep-Learning-Based Storage-Allocation Approach to Improve the AMHS Throughput Capacity in a Semiconductor Fabrication Facility; research on the Cooperative Behavior in Cloud Manufacturing; simulation Credibility Evaluation Based on Multi-source Data Fusion; Particle in Cell Simulation to Study the Charging and Evolution of Wake Structure of LEO Spacecraft; Wise-Use of Sediment for River Restoration: Numerical Approach via HJBQVI; calculation of Extreme Precipitation Threshold by Percentile Method Based on Box-Cox Transformation; OpenPTDS Dataset: Pedestrian Trajectories in Crowded Scenarios; description and Analysis of Cognitive Processes in Ground Control Using a Mutual Belief-Based Team Cognitive Model; a Credibility Assessment Method for Training Simulations from the View of Training Effectiveness; digital Twin-Based Energy Modeling of Industrial Robots; Dyna-Q Algorithm for Path Planning of Quadrotor UAVs; Boarding Stations Inferring Based on Bus GPS and IC Data; acoustic Properties of Resonators Using Deployable Cylinders; a Method of Parameter Calibration with Hybrid Uncertainty; iterative Unbiased Conversion Measurement Kalman Filter with Interactive Multi-model Algorithm for Target Tracking.<br/>},
} 


@article{20175104565701,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modeling Techniques of Storage Modules with PCM Microcapsules: Case Study},
journal = {Journal of Energy Engineering},
author = {Mazzucco, G. and Xotta, G. and Salomoni, V.A. and Majorana, C.E. and Giannuzzi, G.M. and Miliozzi, A.},
volume = {144},
number = {1},
year = {2018},
issn = {07339402},
abstract = {A preliminary study on the increased thermal capacity in thermal energy storage concrete solids with inclusions of phase change materials is presented here. Particularly, the change in the thermal behavior of such composites, by varying the inclusions percentage, is evaluated. An ad hoc hygrothermal finite-element code has been developed, able to evaluate the nonlinear concrete behavior during the phase changes of phase change materials (PCM), subjected to a load history of the test plant designed by ENEA (Italian National Agency for New Technologies, Energy and Sustainable Economic Development). Transient thermal analyses have been conducted, assuming a homogeneous distribution of phase change materials within the cementitious matrix. Hygro-thermo-mechanical models have been developed to evaluate the heat storage capacity of the composite material, and its change in mechanical strength has been analytically and numerically investigated, both at environmental temperature and during heating. The adopted code takes into account a homogenized composite, having a uniform distribution of PCM within the cementitious matrix and characterized by homogenized hygrothermal properties.<br/> &copy; 2017 American Society of Civil Engineers.},
key = {Phase change materials},
keywords = {Concretes;Heat storage;Homogenization method;Reboilers;Storage (materials);Sustainable development;Thermoanalysis;},
note = {Environmental temperature;Homogeneous distribution;Hygrothermal properties;Mesoscale;Sustainable economic development;Thermal storage;Thermomechanical model;Transient thermal analysis;},
URL = {http://dx.doi.org/10.1061/(ASCE)EY.1943-7897.0000501},
} 


@inproceedings{20175004520919,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ACM International Conference Proceeding Series},
journal = {ACM International Conference Proceeding Series},
volume = {Part F130277},
year = {2017},
pages = {ACM Special Interest Group on Knowledge Discovery in Data (SIGKDD); Amazon.com - },
address = {Chennai, India},
abstract = {The proceedings contain 17 papers. The topics discussed include: multi-criteria recommendations through preference learning; supervised analysis dictionary learning application in consumer electronics appliance classification; embedding learning of figurative phrases for emotion classification in micro-blog texts; hybrid trust-aware model for personalized top-n recommendation; modeling and detecting change in user behavior through his social media posting using cluster analysis; protein structure optimization in 3D AB off-lattice model using biogeography based optimization with chaotic mutation; a user activity-based measurement study characterizing and classifying stack exchange communities across multiple domains; spatio-temporal autocorrelation analysis for regional land-cover change detection from remote sensing data; role of temporal diversity in inferring social ties based on spatio-temporal data; differentiating code-borrowing from code-mixing; code-borrowedness of English words in Hindi language; borrowing likeliness ranking based on relevance factor; modelling end of online session from streaming data; recommending resolutions of ITIL services tickets using deep neural network; on discovery of permanent land cover changes using time series segmentation approach; noisy deep dictionary learning; and detecting factual and non-factual content in news articles.<br/>},
} 


@article{20133216580511,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A novel grid synchronization PLL method based on adaptive low-pass notch filter for grid-connected PCS},
journal = {IEEE Transactions on Industrial Electronics},
author = {Lee, Kyoung-Jun and Lee, Jong-Pil and Shin, Dongsul and Yoo, Dong-Wook and Kim, Hee-Je},
volume = {61},
number = {1},
year = {2014},
pages = {292 - 301},
issn = {02780046},
abstract = {The amount of distributed energy resources (DERs) has increased constantly worldwide. The power ratings of DERs have become considerably high, as required by the new grid code requirement. To follow the grid code and optimize the function of grid-connected inverters based on DERs, a phase-locked loop (PLL) is essential for detecting the grid phase angle accurately when the grid voltage is polluted by harmonics and imbalance. This paper proposes a novel low-pass notch filter PLL (LPN-PLL) control strategy to synchronize with the true phase angle of the grid instead of using a conventional synchronous reference frame PLL (SRF-PLL), which requires a d-q-axis transformation of three-phase voltage and a proportional-integral controller. The proposed LPN-PLL is an upgraded version of the PLL method using the fast Fourier transform concept (FFT-PLL) which is robust to the harmonics and imbalance of the grid voltage. The proposed PLL algorithm was compared with conventional SRF-PLL and FFT-PLL and was implemented digitally using a digital signal processor TMS320F28335. A 10-kW three-phase grid-connected inverter was set, and a verification experiment was performed, showing the high performance and robustness of the proposal under low-voltage ride-through operation. &copy; 1982-2012 IEEE.<br/>},
key = {Phase locked loops},
keywords = {Adaptive filtering;Bandpass filters;Codes (symbols);Controllers;Digital signal processing;Digital signal processors;Electric inverters;Energy resources;Fast Fourier transforms;Locks (fasteners);Low pass filters;Notch filters;Two term control systems;},
note = {Digital Signal Processor (DSP);Distributed Energy Resources;Grid codes;Low-pass;Phase Locked Loop (PLL);},
URL = {http://dx.doi.org/10.1109/TIE.2013.2245622},
} 


@article{20174904491446,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Gradient-enhanced model and its micromorphic regularization for simulation of Luders-like bands in shape memory alloys},
journal = {International Journal of Solids and Structures},
author = {Rezaee Hajidehi, Mohsen and Stupkiewicz, Stanislaw},
volume = {135},
year = {2018},
pages = {208 - 218},
issn = {00207683},
abstract = {Shape memory alloys, notably NiTi, often exhibit softening pseudoelastic response that results in formation and propagation of L&uuml;ders-like bands upon loading, for instance, in uniaxial tension. A common approach to modelling softening and strain localization is to resort to gradient-enhanced formulations that are capable of restoring well-posedness of the boundary-value problem. This approach is also followed in the present paper by introducing a gradient-enhancement into a simple one-dimensional model of pseudoelasticity. In order to facilitate computational treatment, a micromorphic-type regularization of the gradient-enhanced model is subsequently performed. The formulation employs the incremental energy minimization framework that is combined with the augmented Lagrangian treatment of the resulting non-smooth minimization problem. A thermomechanically coupled model is also formulated and implemented in a finite-element code. The effect of the loading rate on the localization pattern in a NiTi wire under tension is studied, and the features predicted by the model show a good agreement with the experimental observations. Additionally, an analytical solution is provided for a propagating interface (macroscopic transformation front) both for the gradient-enhanced model and for its micromorphic version.<br/> &copy; 2017},
key = {Finite element method},
keywords = {Binary alloys;Boundary value problems;Constrained optimization;Martensite;One dimensional;Phase transitions;Shape memory effect;Titanium alloys;},
note = {Augmented Lagrangians;Localization patterns;Minimization problems;One-dimensional model;Propagating interface;Pseudoelastic response;Strain localizations;Thermo-mechanical coupling;},
URL = {http://dx.doi.org/10.1016/j.ijsolstr.2017.11.021},
} 


@article{20150700509365,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Comparison of different numerical approaches to the 1D sea-ice thermodynamics problem},
journal = {Ocean Modelling},
author = {Dupont, Frederic and Vancoppenolle, Martin and Tremblay, Louis-Bruno and Huwald, Hendrik},
volume = {87},
year = {2015},
pages = {20 - 29},
issn = {14635003},
abstract = {The vertical one-dimensional sea-ice thermodynamic problem using the principle of conservation of enthalpy is revisited here using (1) the Bitz and Lipscomb (1999) finite-difference approach (FD), (2) a reformulation of the sigma-level transformation of Huwald et al. (2005b) (FV) and (3) a Finite Element approach also in sigma coordinates (FE). These three formulations are compared in terms of physics, numerics, and performance, in order to identify the best choice for large-scale climate models. The BL99 formulation sequentially treats the diffusion of heat and the changes in the vertical position of the ice-snow layers. In contrast, the FV sigma-level transformation elegantly treats both simultaneously. The original FV formulation suffers however from slow convergence. The convergence can nonetheless be improved significantly with a few simple modifications to the original code. The three formulations are compared following the experimental protocol of the Sea Ice Model Intercomparison Project for ice thermodynamics (SIMIP2). It is found that all formulations converge to the same solution. The FD approach, however, suffers from the added cost of the remapping step at large number of ice layers we include in the appendix an optimized version of the FD code-written by one of the reviewer-that resolves this issue. Finally the FE formulation results in a sub-surface temperature over-estimation at low resolution, a problem which disappears at high resolution. Hence, only FD and FV are found suitable for climate models.<br/> &copy; 2014.},
key = {Finite element method},
keywords = {Climate models;Enthalpy;Finite difference method;Sea ice;},
note = {Experimental protocols;Finite difference approach;Finite-element approach;Mode comparison;Numerical scheme;Simple modifications;Subsurface temperature;Vertical coordinates;},
URL = {http://dx.doi.org/10.1016/j.ocemod.2014.12.006},
} 


@inproceedings{20160902002649,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Variability abstractions: Trading precision for speed in family-based analyses},
journal = {Leibniz International Proceedings in Informatics, LIPIcs},
author = {Dimovski, Aleksandar S. and Brabrand, Claus and Wasowski, Andrzej},
volume = {37},
year = {2015},
pages = {247 - 270},
issn = {18688969},
address = {Prague, Czech republic},
abstract = {Family-based (lifted) data-flow analysis for Software Product Lines (SPLs) is capable of analyzing all valid products (variants) without generating any of them explicitly. It takes as input only the common code base, which encodes all variants of a SPL, and produces analysis results corresponding to all variants. However, the computational cost of the lifted analysis still depends inherently on the number of variants (which is exponential in the number of features, in the worst case). For a large number of features, the lifted analysis may be too costly or even infeasible. In this paper, we introduce variability abstractions defined as Galois connections and use abstract interpretation as a formal method for the calculational-based derivation of approximate (abstracted) lifted analyses of SPL programs, which are sound by construction. Moreover, given an abstraction we define a syntactic transformation that translates any SPL program into an abstracted version of it, such that the analysis of the abstracted SPL coincides with the corresponding abstracted analysis of the original SPL. We implement the transformation in a tool, that works on Object-Oriented Java program families, and evaluate the practicality of this approach on three Java SPL benchmarks.<br/> &copy; Aleksandar S. Dimovski, Claus Brabrand, and Andrzej Wasowski;.},
key = {Object oriented programming},
keywords = {Abstracting;Computer software;Data flow analysis;Formal methods;Java programming language;Model checking;},
note = {Abstract interpretations;Computational costs;Galois connection;Object oriented;Program analysis;Software Product Line;Software product line (SPLs);Syntactic transformations;},
URL = {http://dx.doi.org/10.4230/LIPIcs.ECOOP.2015.247},
} 


@inproceedings{20134817032602,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {On testing the source compatibility in java},
journal = {SPLASH 2013 - Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, and Applications: Software for Humanity},
author = {Hybl, Jan and Tronicek, Zdenek},
year = {2013},
pages = {87 - 88},
address = {Indianapolis, IN, United states},
abstract = {When software components evolve, they change interfaces, which may break backward compatibility. We present a tool that facilitates checking whether a new version of component is source compatible with a previous version. This tool figures out the component interface and generates the client code that uses the component interface to maxi-mum extent. If the generated client compiles against the new component interface, those two versions are more or less compatible. The tool can be useful for API authors. Copyright &copy; 2013 by the Association for Computing Machinery, Inc. (ACM).<br/>},
key = {Java programming language},
keywords = {Application programs;Computer systems programming;},
note = {API evolution;Backward compatibility;Component interfaces;Java;New components;Software component;Software Evolution;Source compatibilities;},
URL = {http://dx.doi.org/10.1145/2508075.2508094},
} 


@inproceedings{20183205664959,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Connecting software metrics across versions to predict defects},
journal = {25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
author = {Liu, Yibin and Li, Yanhui and Guo, Jianbo and Zhou, Yuming and Xu, Baowen},
volume = {2018-March},
year = {2018},
pages = {232 - 243},
address = {Campobasso, Italy},
abstract = {Accurate software defect prediction could help software practitioners allocate test resources to defect-prone modules effectively and efficiently. In the last decades, much effort has been devoted to build accurate defect prediction models, including developing quality defect predictors and modeling techniques. However, current widely used defect predictors such as code metrics and process metrics could not well describe how software modules change over the project evolution, which we believe is important for defect prediction. In order to deal with this problem, in this paper, we propose to use the Historical Version Sequence of Metrics (HVSM) in continuous software versions as defect predictors. Furthermore, we leverage Recurrent Neural Network (RNN), a popular modeling technique, to take HVSM as the input to build software prediction models. The experimental results show that, in most cases, the proposed HVSM-based RNN model has significantly better effort-aware ranking effectiveness than the commonly used baseline models.<br/> &copy; 2018 IEEE.},
key = {Software testing},
keywords = {Defects;Forecasting;Recurrent neural networks;Reengineering;},
note = {Defect prediction;Defect prediction models;Modeling technique;Recurrent neural network (RNN);Software defect prediction;Software practitioners;Software prediction models;Software versions;},
URL = {http://dx.doi.org/10.1109/SANER.2018.8330212},
} 


@inproceedings{20171103443043,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of 2017 11th International Conference on Intelligent Systems and Control, ISCO 2017},
journal = {Proceedings of 2017 11th International Conference on Intelligent Systems and Control, ISCO 2017},
year = {2017},
address = {Coimbatore, India},
abstract = {The proceedings contain 94 papers. The topics discussed include: two degree of freedom controller optimization using GA for shell and tube heat exchanger; code review analysis of software system using machine learning techniques; a low cost wireless sensor system for monitoring the air handling unit of the university building; revamped market-basket analysis using in-memory computation framework; an improved face recognition method using local binary pattern method; fast-converging MPPT technique for photovoltaic system using dsPIC controller; an approach for classification using simple CART algorithm in Weka; a full band adaptive harmonic model based speaker identity transformation using radial basis function; a new chaotic attractor from Rucklidge system and its application in secured communication using OFDM; EPPN: extended prime product number based wormhole detection scheme for MANETs; load balancing and position based adaptive clustering scheme for effective data communication in WBAN healthcare monitoring systems; PAPR reduction in OFDM systems using higher order prediction filter; and impact of length and thickness of active region on radiated output power of InP/InGaAsP laser.},
} 


@inproceedings{20181905138607,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Current-limiting droop controller with fault-ride-through capability for grid-tied inverters},
journal = {2017 IEEE PES Innovative Smart Grid Technologies Conference Europe, ISGT-Europe 2017 - Proceedings},
author = {Paspatis, A.G. and Konstantopoulos, G.C. and Mayfield, M. and Nikolaidis, V.C.},
volume = {2018-January},
year = {2018},
pages = {1 - 6},
address = {Torino, Italy},
abstract = {In this paper, the recently proposed current-limiting droop (CLD) controller for grid-connected inverters is enhanced in order to comply with the Fault-Ride-Through (FRT) requirements set by the Grid Code under grid voltage sags. The proposed version of the CLD extends the operation of the original CLD by fully utilizing the power capacity of the inverter under grid faults. It is analytically proven that during a grid fault, the inverter current increases but never violates a given maximum value. Based on this property, an FRT algorithm is proposed and embedded into the proposed control design to support the voltage of the grid. In contrast to the existing FRT algorithms that change the desired values of both the real and reactive power, the proposed method maximizes only the reactive power to support the grid voltage and the real power automatically drops due to the inherent current-limiting property. Extensive simulations are presented to compare the proposed control approach with the original CLD under a faulty grid.<br/> &copy; 2017 IEEE.},
key = {Electric power transmission networks},
keywords = {Controllers;Electric inverters;Reactive power;Smart power grids;},
note = {Current limiting;Droop control;Fault ride-through;Inverter;Voltage sags;},
URL = {http://dx.doi.org/10.1109/ISGTEurope.2017.8260159},
} 


@article{20173904203829,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {On the current AISC approach to stability analysis and design of steel structures},
journal = {Jordan Journal of Civil Engineering},
author = {Mohamed, Osama},
volume = {11},
number = {4},
year = {2017},
pages = {557 - 568},
issn = {19930461},
abstract = {In 2005, the American Institute of Steel Construction (AISC) stability analysis and design requirements changed significantly compared to procedures required prior to 2005. The most significant change related to stability analysis and design is the requirement to include geometric imperfections in the calculation of the required strength. Direct Analysis Method (DAM) is currently the recommended code method, while a modified version of the traditional Effectiveness Length Method (ELM) is now referred to as an alternative method of design. The critical changes appeared in the 13<sup>th</sup>edition (AISC, 2005) and continued to the present 14<sup>th</sup>edition (AISC, 2011). The objectives of this paper are to: 1) provide an overview of the rationale behind the code change that took place in 2005 and remained in the current specifications and 2) present the current features of ELM and DAM methods. A case study is presented to assess the differences in structural response when DAM and ELM methods are used. It was shown that DAM predicts higher demand on beams and columns of the structural system at the lower levels, but the difference in demand between DAM and ELM methods decreases at the upper levels of the structural system. This is due to the variation of effective length factor required in ELM for compression members, from top levels to bottom levels of the system. DAM, however, permits the use of an effective length factor of 1.0.<br/> &copy; 2017 JUST. All Rights Reserved.},
key = {Structural analysis},
keywords = {Dams;Stability;Steel structures;},
note = {AISC approach;American institute of steel constructions;Compression member;Effective length factor;Geometric imperfection;Stability analysis;Structural response;Structural systems;},
} 


@article{20183005611240,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {NCAR Release of CAM-SE in CESM2.0: A Reformulation of the Spectral Element Dynamical Core in Dry-Mass Vertical Coordinates With Comprehensive Treatment of Condensates and Energy},
journal = {Journal of Advances in Modeling Earth Systems},
author = {Lauritzen, P.H. and Nair, R.D. and Herrington, A.R. and Callaghan, P. and Goldhaber, S. and Dennis, J.M. and Bacmeister, J.T. and Eaton, B.E. and Zarzycki, C.M. and Taylor, Mark A. and Ullrich, P.A. and Dubos, T. and Gettelman, A. and Neale, R.B. and Dobbins, B. and Reed, K.A. and Hannay, C. and Medeiros, B. and Benedict, J.J. and Tribbia, J.J.},
volume = {10},
number = {7},
year = {2018},
pages = {1537 - 1570},
issn = {19422466},
abstract = {It is the purpose of this paper to provide a comprehensive documentation of the new NCAR (National Center for Atmospheric Research) version of the spectral element (SE) dynamical core as part of the Community Earth System Model (CESM2.0) release. This version differs from previous releases of the SE dynamical core in several ways. Most notably the hybrid sigma vertical coordinate is based on dry air mass, the condensates are dynamically active in the thermodynamic and momentum equations (also referred to as condensate loading), and the continuous equations of motion conserve a more comprehensive total energy that includes condensates. Not related to the vertical coordinate change, the hyperviscosity operators and the vertical remapping algorithms have been modified. The code base has been significantly reduced, sped up, and cleaned up as part of integrating SE as a dynamical core in the CAM (Community Atmosphere Model) repository rather than importing the SE dynamical core from High-Order Methods Modeling environment as an external code.<br/> &copy;2018. The Authors.},
key = {Dynamical systems},
keywords = {Beams and girders;Cams;Computational efficiency;Equations of motion;},
note = {Aqua planet;CESM;Community atmosphere model;Comprehensive documentation;Modeling environments;National center for atmospheric researches;Spectral element;Total energy;},
URL = {http://dx.doi.org/10.1029/2017MS001257},
} 


@inproceedings{20180704793771,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {1st International Symposium on Mobile Internet Security, MobiSec 2016},
journal = {Communications in Computer and Information Science},
volume = {797},
year = {2018},
issn = {18650929},
address = {Taichung, Taiwan},
abstract = {The proceedings contain 15 papers. The special focus in this conference is on Mobile Internet Security. The topics include: The cryptanalysis of WPA &amp; WPA2 using the parallel-computing with GPUs; a policy management system based on multi-dimensional attribution label; access control for cross-border transfer of sensor data; security analysis oriented physical components modeling in quantum key distribution; a novel hybrid architecture for high speed regular expression matching; mobile security assurance for automotive software through archimate; a generalized data inconsistency detection model for wireless sensor networks; building a frame-based cyber security learning game; Personal identification using time and frequency domain features of ECG lead-I; a secure color-code key exchange protocol for mobile chat application; Design and implementation of SPEEX speech technology on ARM processor; a discrete wavelet transformation based fast and secure transmission of images for group communication; an automated graph based approach to risk assessment for computer networks with mobile components.<br/>},
} 


@inproceedings{20142817925452,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automating and optimizing data transfers for many-core coprocessors},
journal = {Proceedings of the International Conference on Supercomputing},
author = {Ren, Bin and Ravi, Nishkam and Yang, Yi and Feng, Min and Agrawal, Gagan and Chakradhar, Srimat},
year = {2014},
pages = {177 - },
address = {Munich, Germany},
abstract = {Orchestrating data transfers between CPUs and a coprocessor manually is cumbersome, particularly for multi-dimensional arrays and other data structures with multi-level pointers, which are common in scientific computations. This work describes a system that includes both compile-time and runtime solutions for this problem, with the overarching goal of improving programmer productivity while maintaining performance. We implemented our best compile-time solution, partial linearization with pointer reset, as a source-to-source transformation, and evaluated our work by multiple C benchmarks. Our experiment results demonstrate that our best compile-time solution can perform 2.5x-5x faster than original runtime solution, and the CPU-Coprocessor code with it can achieve 1.5x-2.5x speedup over the 16-thread CPU version. &copy; 2014 Authors.<br/>},
key = {C (programming language)},
keywords = {Coprocessor;Data transfer;Intelligent control;Linearization;Program processors;Static analysis;},
note = {Multidimensional arrays;offloading;Partial linearization;Programmer productivity;Run-time analysis;Runtime solution;Scientific computation;Source-to-source transformations;},
URL = {http://dx.doi.org/10.1145/2597652.2600114},
} 


@inproceedings{20183105617349,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Legato: An at-most-once analysis with applications to dynamic configuration updates},
journal = {Leibniz International Proceedings in Informatics, LIPIcs},
author = {Toman, John and Grossman, Dan},
volume = {109},
year = {2018},
pages = {AITO - },
issn = {18688969},
address = {Amsterdam, Netherlands},
abstract = {Modern software increasingly relies on external resources whose location or content can change during program execution. Examples of such resources include remote network hosts, database entries, dynamically updated configuration options, etc. Long running, adaptable programs must handle these changes gracefully and correctly. Dealing with all possible resource update scenarios is difficult to get right, especially if, as is common, external resources can be modified without prior warning by code and/or users outside of the application&rsquo;s direct control. If a resource unexpectedly changes during a computation, an application may observe multiple, inconsistent states of the resource, leading to incorrect program behavior. This paper presents a sound and precise static analysis, Legato, that verifies programs correctly handle changes in external resources. Our analysis ensures that every value computed by an application reflects a single, consistent version of every external resource&rsquo;s state. Although consistent computation in the presence of concurrent resource updates is fundamentally a concurrency issue, our analysis relies on the novel at-most-once condition to avoid explicitly reasoning about concurrency. The at-most-once condition requires that all values depend on at most one access of each resource. Our analysis is flow-, field-, and context-sensitive. It scales to real-world Java programs while producing a moderate number of false positives. We applied Legato to 10 applications with dynamically updated configurations, and found several non-trivial consistency bugs.<br/> &copy; John Toman and Dan Grossman.},
key = {Object oriented programming},
keywords = {Application programs;Program debugging;Static analysis;},
note = {Configuration options;Context sensitive;Dynamic configuration;External resources;False positive;Program behavior;Program execution;Remote networks;},
URL = {http://dx.doi.org/10.4230/LIPIcs.ECOOP.2018.24},
} 


@inproceedings{20185106262937,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Combining and Visualizing Time-Oriented Data from the Software Engineering Toolset},
journal = {Proceedings - 6th IEEE Working Conference on Software Visualization, VISSOFT 2018},
author = {Grabner, Johann and Decker, Roman and Artner, Thomas and Bernhart, Mario and Grechenig, Thomas},
year = {2018},
pages = {76 - 86},
address = {Madrid, Spain},
abstract = {The simultaneous use of more than two different data sources from the software engineering toolset is still uncommon in the research areas of software evolution and visualization. In our work, we address this research gap by the design and evaluation of three interactive visualizations which combine the data from the version control (VCS), the issue tracking (ITS), and the continuous integration (CI) system. After analyzing the information needs of a project team and describing the available data, we selected change impact, code ownership, and activity peaks as our visualization topics. Then, we adapted suitable visualization techniques from the literature to meet our design requirements. After implementation, we evaluated our visualizations by conducting a usability test with ten senior software engineers. On the system usability scale (SUS), our visualizations achieved the rating 'good' from the participants. A scenario success rate of 88% and the qualitative user feedback has provided evidence for the benefits of visualizing combined data from the VCS, ITS, and CI system.<br/> &copy; 2018 IEEE.},
key = {Data visualization},
keywords = {Data mining;Engineering research;Software engineering;Software testing;Visualization;},
note = {Continuous integrations;Design and evaluations;Interactive visualizations;Mining software repositories;Software Evolution;System Usability Scale (SUS);Time-oriented datum;Visualization technique;},
URL = {http://dx.doi.org/10.1109/VISSOFT.2018.00016},
} 


@inproceedings{20185006244520,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A thermal parametric study of non-evaporative spray cooling process},
journal = {MATEC Web of Conferences},
author = {Otmani, Abdessalam and Mzad, Hocine and Bey, Kamel},
volume = {240},
year = {2018},
issn = {2261236X},
address = {Gesia Street, Cracow, Poland},
abstract = {Ordinary water spray cooling is connected with very high temperatures where heat transfer during evaporation plays a key role. However, during cooling without phase change, the behaviour of the spray cooling parameters is rarely considered. The purpose of this paper is to study the influence of spray hydrodynamic parameters on heat transfer without liquid phase change during the cooling of an aluminium 3003-H18 plate at a temperature of 92 &deg;C. First of all, the flow rate was varied from 0.497 up to 1 l/min. Then, the inlet pressure varied from 0.7 to 2.1 bars. The influence of nozzle-to-target distance is also tested since the simulations were carried out in a wide height range, from 100 mm to 505 mm. The present simulation was achieved using the version 5.2 of COMSOL Multiphysics code.<br/> &copy; The Authors, published by EDP Sciences, 2018.},
key = {Cooling},
keywords = {Momentum transfer;Spray steelmaking;},
note = {Comsol multiphysics;Evaporative spray cooling;High temperature;Hydrodynamic parameters;Inlet pressures;Liquid phase changes;Parametric study;Spray cooling;},
URL = {http://dx.doi.org/10.1051/matecconf/201824001030},
} 


@inproceedings{20163502740223,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Monk10: Burnup credit capability},
journal = {ICNC 2015 - International Conference on Nuclear Criticality Safety},
author = {Shepherd, Max and Davies, Nigel and Richards, Simon and Smith, Paul N. and Philpott, Will and Baker, Chris and Hiles, Richard and Hanlon, Dave},
year = {2015},
pages = {1144 - 1153},
address = {Charlotte, NC, United states},
abstract = {MONK&reg; is a Monte Carlo code for nuclear criticality and reactor physics analyses. It has a proven track record of application to the whole of the nuclear fuel cycle and is well established in the UK criticality community. Furthermore it is increasingly being used for reactor physics analysis (as described at ICNC 2011), which makes it an ideal tool for burn-up credit (BUC) calculations. Throughout the paper, example calculations based on a PWR are presented to illustrate the capabilities of the MONK10 code. In order to account for the spatial dependence of material burn-up it has in the past been necessary to design models with multiple regions and materials specifically to allow material burn-up to vary spatially. This is very labour intensive and difficult to change at a later stage. A new code version, MONK10, was released last year which includes the facility to allow a burn-up (BU) mesh to be superimposed on an existing model in order to account for the spatial dependence of the burn-up. This facility is used to consider the effect of radial position of a fuel element in a PWR core on BUC. Additionally, a thermal hydraulics (TH) mesh can be used to specify region dependent temperature. This, coupled with the fact that MONK10 also incorporates an on-the-fly Doppler broadening methodology facilitates the modelling of spatially dependent temperatures for the different components. A TH mesh is used to superimpose a temperature profile on a PWR based model and the effect of this on BUC is considered. The burn-up modelling in MONK has been benchmarked against the ANSWERS WIMS deterministic reactor physics code. Once the burn-up calculation has been completed and the depleted fuel compositions determined the spent fuel compositions can be transferred into a model of a storage facility or transport flask in order to perform burn-up credit analysis. The initial model is usually described as the donor model and the latter model as the receiver model. This transfer is carried out using the COWL option which allows the specification of a material in the receiver model based on the material's composition in a given BU mesh cell from the donor model. This allows compositions and densities to be transferred and also allows user specified adjustments to be made. For example, this could include omitting the fission products in order to estimate their contribution to burn-up credit and provide an actinide-only analysis. The effect of excluding appropriate nuclides is presented. An example of how the ANSWERS SPRUCE code can be used to quantify uncertainty in a BUC calculation is also presented.<br/>},
key = {Pressurized water reactors},
keywords = {Codes (symbols);Criticality (nuclear fission);Fission products;Fuel storage;Fuels;Mesh generation;Nuclear fuel reprocessing;},
note = {Burn up;Burn up calculations;MONK10;Nuclear criticality;Nuclear fuel cycles;Reactor physics;Spatial dependence;Temperature profiles;},
} 


@inproceedings{20160902003080,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evaluation strategies for monadic computations},
journal = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
author = {Petricek, Tomas},
volume = {76},
year = {2012},
pages = {68 - 89},
issn = {20752180},
address = {Tallinn, Estonia},
abstract = {Monads have become a powerful tool for structuring effectful computations in functional programming, because they make the order of effects explicit. When translating pure code to a monadic version, we need to specify evaluation order explicitly. Two standard translations give call-by-value and call-by-name semantics. The resulting programs have different structure and types, which makes revisiting the choice difficult. In this paper, we translate pure code to monadic using an additional operation malias that abstracts out the evaluation strategy. The malias operation is based on computational comonads; we use a categorical framework to specify the laws that are required to hold about the operation. For any monad, we show implementations of malias that give call-by-value and call-by-name semantics. Although we do not give call-by-need semantics for all monads, we show how to turn certain monads into an extended monad with call-by-need semantics, which partly answers an open question. Moreover, using our unified translation, it is possible to change the evaluation strategy of functional code translated to the monadic form without changing its structure or types.<br/>},
key = {Functional programming},
keywords = {Codes (symbols);Semantics;Structured programming;},
note = {Call-by-name;Call-by-need;Comonads;Different structure;Evaluation order;Evaluation strategies;Functional codes;Monadic computations;},
URL = {http://dx.doi.org/10.4204/EPTCS.76.7},
} 


@inproceedings{20154001337022,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Case-based instruction of 'how computer works' courses for high school students},
journal = {Proceedings - 2015 International Conference on Learning and Teaching in Computing and Engineering, LaTiCE 2015},
author = {Wang, Ting-Chung},
year = {2015},
pages = {207 - 208},
address = {Taipei, Taiwan},
abstract = {The concept of binary, which is the basis of how computer works, is often taught in Taiwan's high school computer courses as the introductory unit of the class 'Introduction to Computers'. In the past, teachers used to lecture through binary concepts such as the decimal binary conversion, ASCII code transformation, and several traditional courses. However, most students did not know why they needed to learn binary concepts, and seemed less interested in learning. To enhance the effectiveness of learning, we incorporated some examples corresponding to each binary concept to link students' life experiences to the concepts in order to make learning meaningful. In this paper, we will share our teaching materials for the binary concept from the course 'Introduction to Computers'. The lesson starts from how computer stores values, text or image data to introduce basic concepts of binary data interpretation and hex conversion. Afterwards, a combination of simple logic gates adder instance is introduced to guide students to learn how to perform a binary compute operation. Finally, software CPU simulator is covered to allow students to learn and visualize how computer uses binary data and basic operations to perform some specific compute functions.<br/> &copy; 2015 IEEE.},
key = {Teaching},
keywords = {Character sets;Computation theory;Computer software;Cosine transforms;Logic gates;Students;},
note = {Basic operation;Binary conversion;Case-based instructions;Computer work;High school students;Life experiences;Teaching materials;Traditional course;},
URL = {http://dx.doi.org/10.1109/LaTiCE.2015.56},
} 


@inproceedings{20160101747609,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {1st International Conference on Advanced Intelligent System and Informatics, AISI 2015},
journal = {Advances in Intelligent Systems and Computing},
volume = {407},
year = {2016},
pages = {1 - 534},
issn = {21945357},
address = {Beni Suef, Egypt},
abstract = {The proceedings contain 46 papers. The special focus in this conference is on Intelligent Systems and Informatics. The topics include: Automatic rules generation approach for data cleaning in medical applications; action classification using weighted directional wavelet LBP histograms; markerless tracking for augmented reality using different classifiers; an experimental comparison between seven classification algorithms for activity recognition; machine learning based classification approach for predicting students performance in blended learning; detection of dead stained microscopic cells based on color intensity and contrast; building numbers with rods; telepresence robot using microsoft kinect sensor and video glasses; video foreground object extraction using an intermittent flash; enhanced region growing segmentation for CT liver images; a multi-objective genetic algorithm for community detection in multidimensional social network; creativity in the era of social networking; OCR system for poor quality images using chain-code representation; human thermal face extraction based on superpixel technique; unsupervised brain MRI tumor segmentation with deformation-based feature; face sketch synthesis and recognition based on linear regression transformation and multi-classifier technique; a new learning strategy for complex-valued neural networks using particle swarm optimization; abdominal CT liver parenchyma segmentation based on particle swarm optimization; grey wolf optimizer and case-based reasoning model for water quality assessment; new rough set attribute reduction algorithm based on grey wolf optimization; solving manufacturing cell design problems using a shuffled frog leaping algorithm and a fish detection approach based on BAT algorithm.},
} 


@inproceedings{20172503800497,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {14th International Work-Conference on Artificial Neural Networks, IWANN 2017},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {10305 LNCS},
year = {2017},
pages = {1 - 760},
issn = {03029743},
address = {Cadiz, Spain},
abstract = {The proceedings contain 127 papers. The special focus in this conference is on Artificial Neural Networks. The topics include: A parallel swarm library based on functional programming; a parallel island approach to multiobjective feature selection for brain computer interfaces; deep belief networks and multiobjective feature selection for BCI with multiresolution analysis; an intelligent multi-objective genetic algorithm using self organizing map; solving scheduling problems with genetic algorithms using a priority encoding scheme; tuning of clustering search based metaheuristic by cross-validated racing approach; a transformation approach towards big data multilabel decision trees; analysis of electroreception with temporal code-driven stimulation; a novel technique to estimate biological parameters in an epidemiology problem; deep learning using EEG data in time and frequency domains for sleep stage classification; application of an eye tracker over facility layout problem to minimize user fatigue; active sensing in human activity recognition; searching the sky for neural networks; non-linear least mean squares prediction based on non-Gaussian mixtures; synchronized multi-chain mixture of independent component analyzers; pooling spike neural network for acceleration of global illumination rendering; automatic tool for optic disc and cup detection on retinal fundus images; uncertainty analysis of ANN based spectral analysis using monte Carlo method; using deep learning for image similarity in product matching; enhanced similarity measure for sparse subspace clustering method; neural network-based simultaneous estimation of actuator and sensor faults.},
} 


@article{20125115826721,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A correlated-polaron electronic propagator: Open electronic dynamics beyond the Born-Oppenheimer approximation},
journal = {Journal of Chemical Physics},
author = {Parkhill, John A. and Markovich, Thomas and Tempel, David G. and Aspuru-Guzik, Alan},
volume = {137},
number = {22},
year = {2012},
issn = {00219606},
abstract = {In this work, we develop an approach to treat correlated many-electron dynamics, dressed by the presence of a finite-temperature harmonic bath. Our theory combines a small polaron transformation with the second-order time-convolutionless master equation and includes both electronic and system-bath correlations on equal footing. Our theory is based on the ab initio Hamiltonian, and is thus well-defined apart from any phenomenological choice of basis states or electronic system-bath coupling model. The equation-of-motion for the density matrix we derive includes non-Markovian and non-perturbative bath effects and can be used to simulate environmentally broadened electronic spectra and dissipative dynamics, which are subjects of recent interest. The theory also goes beyond the adiabatic Born-Oppenheimer approximation, but with computational cost scaling such as the Born-Oppenheimer approach. Example propagations with a developmental code are performed, demonstrating the treatment of electron-correlation in absorption spectra, vibronic structure, and decay in an open system. An untransformed version of the theory is also presented to treat more general baths and larger systems. &copy; 2012 American Institute of Physics.<br/>},
key = {Computation theory},
keywords = {Approximation theory;Dynamics;Equations of motion;Hamiltonians;Polarons;Quantum chemistry;},
note = {Born-Oppenheimer approximation;Computational costs;Dissipative dynamics;Electronic dynamics;Electronic spectrum;Electronic systems;Finite temperatures;Vibronic structure;},
URL = {http://dx.doi.org/10.1063/1.4762441},
} 


@inbook{20172603826506,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Metallo-Thermo-Mechanical Coupling in Quenching},
journal = {Comprehensive Materials Processing},
author = {Inoue, T.},
volume = {12},
year = {2014},
pages = {177 - 251},
abstract = {This chapter summarizes the computer simulation on the macroscopic feature of the quenching process carried out by the present author. The history of this area up to the year 2000 is introduced in the first part since there have been so many works in the twenty-first century. A governing theory termed metallo-thermo-mechanics is presented. This theory is relevant to describing coupled temperature, metallic structure, and mechanical fields, which are predominant parameters in the course of some processes involving phase transformation and related theories of the kinetics of phase transformation and inelastic constitutive relations. Based on the theory, the computer simulation code COSMAP recently developed by the author is summarized in addition to the material database MATEQ, which must be utilized for the simulation. The next four sections treat the results of simulation on practical quenching processes such as Jominy end quenching, carburized quenching, induction hardening, and laser hardening, as well as quenching of Japanese swords. The final section briefly introduces the results of a benchmark project conducted by our group in Japan, where some results obtained by quenching simulation solvers are presented and discussed. &copy; 2014 Elsevier Ltd All rights reserved.<br/>},
key = {Quenching},
keywords = {Alloy steel;Hardening;Induction heating;Metals;Phase transitions;Reactor cores;Thermal Engineering;},
note = {Carburized quenching;Continuum thermodynamics;Cooperative activity;Induction hardening;Japanese swords;Jominy end quenching;Laser hardening;Materials database;Metallic structures;Metallo-thermo-mechanics;Thermoplasticity;},
URL = {http://dx.doi.org/10.1016/B978-0-08-096532-1.01206-1},
} 


@inproceedings{20144400145958,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Implementation of two-group interfacial are a transport in a one-dimensional computational environment},
journal = {Embedded Topical Meeting on Advances in Thermal Hydraulics, ATH 2014, Held at the American Nuclear Society 2014 Annual Meeting},
author = {Schlegel, J.P. and Hibiki, T. and Ishii, M. and Shen, X. and Appathurai, S.},
year = {2014},
pages = {555 - 567},
address = {Reno, NV, United states},
abstract = {A simplified one-dimensional computer code has been written which uses the two-fluid model to predict two-phase flows in large diameter vertical channels. Simplifying assumptions have been used to eliminate the energy equation and simplify the momentum equations. This code, written using MATLAB, uses a two-group momentum approach with two-group interfacial area transport to dynamically predict the development of the flow. This provides the ability to evaluate the interfacial area transport equation within a similar computational environment to that used in best estimate thermal-hydraulics analysis codes used in the nuclear industry and allows the evaluation of the performance of the transport equation, constitutive relations, bubble coalescence and breakup models, and other models using the same framework in which they will be used. The current version is limited to systems without phase change, i.e. where boiling and condensation are not significant. While the force-balance approach to interfacial drag has been used in the analyses presented, the code also includes a two-group drift-flux correlation and the implementation of the drag-coefficient approach recommended by Ishii and Hibiki [5] as options within the code, allowing the comparison of various interfacial drag approaches. The code has been used to simulate the experiments performed by Shen et al [18] in pipes with diameter of 0.2 m and length of 26.0 m. The code was able to duplicate the characteristic axial void fraction profile caused by the transition between the bubbly and cap-bubbly flow regimes in the experimental facility, showing that the two-group approach can be applied within the framework of existing best estimate codes. Further improvements to the accuracy of the code are expected if the bubble coalescence and breakup models are benchmarked based on a wider database and using the new approach to compute the flow development.<br/>},
key = {Two phase flow},
keywords = {Codes (symbols);Drag;Hydraulics;MATLAB;Nuclear industry;One dimensional;Void fraction;},
note = {Best estimates;Boiling and condensations;Computational environments;Experimental facilities;Interfacial area transport equation;Interfacial area transports;Large diameter;Simplifying assumptions;},
} 


@inproceedings{20151900835578,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {17th European Conference on Genetic Programming, EuroGP 2014},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {8599},
year = {2014},
pages = {1 - 245},
issn = {03029743},
address = {Granada, Spain},
abstract = {The proceedings contain 22 papers. The special focus in this conference is on Genetic programming. The topics include: Higher order functions for kernel regression; a GP-GPU ensemble learning system for handling large datasets; learning dynamical systems using standard symbolic regression; semantic crossover based on the partial derivative error; a multi-dimensional genetic programming approach for multi-class classification problems; generalisation enhancement via input space transformation; on diversity, teaming, and hierarchical policies; genetically improved CUDA C++ software; measuring mutation operators&rsquo; exploration-exploitation behaviour and long-term biases; exploring the search space of hardware / software embedded systems by means of GP; enhancing branch-and-bound algorithms for order acceptance and scheduling with genetic programming; using genetic improvement and code transplants to specialise a C++ program to a problem class; ESAGP - a semantic GP framework based on alignment in the error space; building a stage 1 computer aided detector for breast cancer using genetic programming; NEAT, there&rsquo;s no bloat; the best things don&rsquo;t always come in small packages; asynchronous evolution by reference-based evaluation; behavioral search drivers for genetic programing; cartesian genetic programming: why no bloat? and on evolution of multi-category pattern classifiers suitable for embedded systems.},
} 


@article{20144100088507,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Assessment of ASSERT-PV for prediction of post-dryout heat transfer in CANDU bundles},
journal = {Nuclear Engineering and Design},
author = {Cheng, Z. and Rao, Y.F. and Waddington, G.M.},
volume = {278},
year = {2014},
pages = {239 - 248},
issn = {00295493},
abstract = {Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The recently released ASSERT-PV 3.2 provides enhanced models for improved predictions of subchannel flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against PDO tests performed during five full-size CANDU bundle experiments conducted between 1992 and 2009 by Stern Laboratories (SL), using 28-, 37-and 43-element bundles. A total of 10 PDO test series with varying pressure-tube creep and/or bearing-pad height were analyzed. The SL experiments encompassed the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for existing CANDU reactors. Code predictions of maximum PDO fuel-sheath temperature were compared against measurements from the SL PDO tests to quantify the code's prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. Furthermore, separate-effects sensitivity studies quantified the contribution of each PDO model change or enhancement to the improvement in PDO heat transfer prediction. Overall, the assessment demonstrated significant improvement in prediction of PDO sheath temperature in horizontal fuel channels containing CANDU bundles. Crown<br/> Copyright &copy; 2014 Published by Elsevier B.V. All rights reserved.},
key = {Forecasting},
keywords = {Codes (symbols);Fuels;Heat flux;Heat transfer;Nuclear industry;Nuclear reactors;Two phase flow;},
note = {Atomic energy of canada limiteds;Canadian nuclear industry;Critical heat flux(CHF);Flow distribution;Heat transfer predictions;Prediction accuracy;Sensitivity studies;Thermal hydraulics codes;},
URL = {http://dx.doi.org/10.1016/j.nucengdes.2014.07.031},
} 


@article{20143618128703,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Assessment of ASSERT-PV for prediction of post-dryout heat transfer in CANDU bundles},
journal = {Nuclear Engineering and Design},
author = {Cheng, Z. and Rao, Y.F. and Waddington, G.M.},
volume = {278},
year = {2014},
pages = {239 - 248},
issn = {00295493},
abstract = {Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The recently released ASSERT-PV 3.2 provides enhanced models for improved predictions of subchannel flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against PDO tests performed during five full-size CANDU bundle experiments conducted between 1992 and 2009 by Stern Laboratories (SL), using 28-, 37- and 43-element bundles. A total of 10 PDO test series with varying pressure-tube creep and/or bearing-pad height were analyzed. The SL experiments encompassed the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for existing CANDU reactors. Code predictions of maximum PDO fuel-sheath temperature were compared against measurements from the SL PDO tests to quantify the code's prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. Furthermore, separate-effects sensitivity studies quantified the contribution of each PDO model change or enhancement to the improvement in PDO heat transfer prediction. Overall, the assessment demonstrated significant improvement in prediction of PDO sheath temperature in horizontal fuel channels containing CANDU bundles. &copy; 2014 Published by Elsevier B.V. All rights reserved.<br/>},
key = {Forecasting},
keywords = {Codes (symbols);Fuels;Heat flux;Heat transfer;Nuclear industry;Nuclear reactors;Two phase flow;},
note = {Atomic energy of canada limiteds;Canadian nuclear industry;Critical heat flux(CHF);Flow distribution;Heat transfer predictions;Prediction accuracy;Sensitivity studies;Thermal hydraulics codes;},
URL = {http://dx.doi.org/10.1016/j.nucengdes.2014.07.031},
} 


@inproceedings{20181304941937,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Adapting proof automation to adapt proofs},
journal = {CPP 2018 - Proceedings of the 7th ACM SIGPLAN International Conference on Certified Programs and Proofs, Co-located with POPL 2018},
author = {Ringer, Talia and Leo, John and Yazdani, Nathaniel and Grossman, Dan},
volume = {2018-January},
year = {2018},
pages = {115 - 129},
address = {Los Angeles, CA, United states},
abstract = {We extend proof automation in an interactive theorem prover to analyze changes in specifications and proofs. Our approach leverages the history of changes to specifications and proofs to search for a patch that can be applied to other specifications and proofs that need to change in analogous ways. We identify and implement five core components that are key to searching for a patch. We build a patch finding procedure from these components, which we configure for various classes of changes. We implement this procedure in a Coq plugin as a proof-of-concept and use it on real Coq code to change specifications, port definitions of a type, and update the Coq standard library. We show how our findings help drive a future that moves the burden of dealing with the brittleness of small changes in an interactive theorem prover away from the programmer and into automated tooling.<br/> &copy; 2018 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery.},
key = {Theorem proving},
keywords = {Automation;Fracture mechanics;Specifications;},
note = {Core components;Interactive theorem prover;Plug-ins;Proof evolution;Proof of concept;Standard libraries;},
URL = {http://dx.doi.org/10.1145/3167094},
} 


@inproceedings{20165203173107,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {SLE 2016 - Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering, co-located with SPLASH 2016},
journal = {SLE 2016 - Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering, co-located with SPLASH 2016},
year = {2016},
pages = {ACM SIGPLAN - },
address = {Amsterdam, Netherlands},
abstract = {The proceedings contain 23 papers. The topics discussed include: parsing and reflective printing, bidirectionally; taming context-sensitive languages with principled stateful parsing; efficient development of consistent projectional editors using grammar cells; experiences of Models@run-time with EMF and CDO; runtime support for rule-based access-control evaluation through model-transformation; object-oriented design pattern for DSL program monitoring; execution framework of the GEMOC studio; language design and implementation for the domain of coding conventions; BSML-mbeddr: integrating semantically configurable state-machine models in a C programming environment; adding uncertainty and units to quantity types in software models; FRaMED: full-fledge role modeling editor; towards a universal code formatter through machine learning; the IDE portability problem and its solution in Monto; principled syntactic code completion using placeholders; automated testing support for reactive domain-specific modelling languages; symbolic execution of high-level transformations; and raincode assembler compiler.},
} 


@inproceedings{20134416925192,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Safety transformations: Sound and complete?},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Schiffel, Ute},
volume = {8153 LNCS},
year = {2013},
pages = {190 - 201},
issn = {03029743},
address = {Toulouse, France},
abstract = {Safety transformations transform unsafe original software into safe software that, in contrast to the unsafe version, detects if its execution was incorrect due to execution errors. Especially transformations based on arithmetic codes such as an AN- or ANB-code apply complex and error-prone transformations, while at the same time aiming for safety- or mission-critical applications. Testing and error injection are used so far to ensure correctness and error detection capabilities. But both are incomplete and might miss errors that change functionality or reduce error detection rates. Our research provides tools for a complete analysis of AN-encoding safety transformations. This paper presents our analysis tools and results for the AN-encoded operations. While we were able to demonstrate functional correctness, we discovered bugs that prevent propagation of errors almost completely for AN-encoded divisions and reduce propagation significantly for logical bitwise operations. &copy; 2013 Springer-Verlag.<br/>},
key = {Error detection},
keywords = {Artificial intelligence;Computer science;Computers;},
note = {Bitwise operations;Detection capability;Error injection;Execution errors;Functional correctness;Mission critical applications;Propagation of error;Sound and complete;},
URL = {http://dx.doi.org/10.1007/978-3-642-40793-2_18},
} 


@inproceedings{20150200416765,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {49th Annual Convention of Computer Society of India: Emerging ICT for Bridging the Future, CSI 2014},
journal = {Advances in Intelligent Systems and Computing},
volume = {338},
year = {2015},
pages = {1 - 658},
issn = {21945357},
address = {Hyderabad, India},
abstract = {The proceedings contain 70 papers. The special focus in this conference is on Machine Learning, Network and Information security, Data Mining, Data Engineering and Soft Computing. The topics include: A non-local means filtering algorithm for restoration of rician distributed MRI; a probabilistic based multi-label classification method using partial information; design and utilization of bounding box in human detection and activity identification; recognition and tracking of occluded objects in dynamic scenes; design of fuzzy logic controller to drive autopilot altitude in landing phase; a discrete particle swarm optimization based clustering algorithm for wireless sensor networks; a short survey on teaching learning based optimization; linear array optimization using teaching learning based optimization; multilevel thresholding in image segmentation using swarm algorithms; efficient data aggregation approaches over cloud in wireless sensor networks; sensor controlled sanitizer door knob with scan technique; detection of black hole attack using code division security method; network quality estimation - error protection and fault localization in router based network; the probabilistic encryption algorithm using linear transformation; network management framework for network forensic analysis; a new approach for data hiding with LSB steganography; designing energy-aware adaptive routing for wireless sensor networks; dynamic recurrent FLANN based adaptive model for forecasting of stock indices; effect of mahalanobis distance on time series classification using shapelets; diabetic retinal exudates detection using extreme learning machine and a memory efficient algorithm with enhance preprocessing technique for web usage mining.},
} 


@article{20174504371848,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {FEVER: An approach to analyze feature-oriented changes and artefact co-evolution in highly configurable systems},
journal = {Empirical Software Engineering},
author = {Dintzner, Nicolas and van Deursen, Arie and Pinzger, Martin},
volume = {23},
number = {2},
year = {2018},
pages = {905 - 952},
issn = {13823256},
abstract = {The evolution of highly configurable systems is known to be a challenging task. Thorough understanding of configuration options their relationships, and their implementation in various types of artefacts (variability model, mapping, and implementation) is required to avoid compilation errors, invalid products, or dead code. Recent studies focusing on co-evolution of artefacts detailed feature-oriented change scenarios, describing how related artefacts might change over time. However, relying on manual analysis of commits, such work do not provide the means to obtain quantitative information on the frequency of described scenarios nor information on the exhaustiveness of the presented scenarios for the evolution of a large scale system. In this work, we propose FEVER and its instantiation for the Linux kernel. FEVER extracts detailed information on changes in variability models (KConfig files), assets (preprocessor based C code), and mappings (Makefiles). We apply this methodology to the Linux kernel and build a dataset comprised of 15 releases of the kernel history. We performed an evaluation of the FEVER approach by manually inspecting the data and compared it with commits in the system&rsquo;s history. The evaluation shows that FEVER accurately captures feature related changes for more than 85% of the 810 manually inspected commits. We use the collected data to reflect on occurrences of co-evolution in practice. Our analysis shows that complex co-evolution scenarios occur in every studied release but are not among the most frequent change scenarios, as they only occur for 8 to 13% of the evolving features. Moreover, only a minority of developers working on a given release will make changes to all artefacts related to a feature (between 10% and 13% of authors). While our conclusions are derived from observations on the evolution of the Linux kernel, we believe that they may have implications for tool developers as well as guide further research in the field of co-evolution of artefacts.<br/> &copy; 2017, The Author(s).},
key = {Linux},
keywords = {C (programming language);Large scale systems;Mapping;},
note = {Co-evolution;Configurable systems;Configuration options;Feature;Quantitative information;Variability;Variability model;Variable systems;},
URL = {http://dx.doi.org/10.1007/s10664-017-9557-6},
} 


@inproceedings{20165203194009,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A neural words encoding model},
journal = {Proceedings of the International Joint Conference on Neural Networks},
author = {Liu, Dayiheng and Lv, Jiancheng and Qi, Xiaofeng and Wei, Jiangshu},
volume = {2016-October},
year = {2016},
pages = {532 - 536},
address = {Vancouver, BC, Canada},
abstract = {This paper proposes a neural network model and learning algorithm that can be applied to encode words. The model realizes the function of words encoding and decoding which can be applied to text encryption/decryption and word-based compression. The model is based on Deep Belief Networks (DBNs) and it differs from traditional DBNs in that it is asymmetric structured and the output of it is a binary vector. With pre-training of multi-layer Restricted Boltzmann Machines (RBMs) and fine-tuning to reconstruct word set, the output of code layer can be used as a kind of representation code of words. We can change the number of neurons of code layer to control the length of representation code for different applications. This paper reports on experiments using English words of American National Corpus to train a neural words encoding model which can be used to encode/decode English words, realizing text encryption and data compression.<br/> &copy; 2016 IEEE.},
key = {Cryptography},
keywords = {Encoding (symbols);Signal encoding;},
note = {Binary vectors;Deep belief networks;Encoding and decoding;Encoding models;Neural network model;Pre-training;Restricted boltzmann machine;Text encryptions;},
URL = {http://dx.doi.org/10.1109/IJCNN.2016.7727245},
} 


@inproceedings{20173003975041,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {IEEE International Conference on Program Comprehension},
journal = {IEEE International Conference on Program Comprehension},
year = {2017},
address = {Buenos Aires, Argentina},
abstract = {The proceedings contain 41 papers. The topics discussed include: software engineers' information seeking behavior in change impact analysis-an interview study; variability through the eyes of the programmer; meaningful identifier names: the case of single-letter variables; effects of variable names on comprehension: an empirical study; exploiting type hints in method argument names to improve lightweight type inference; binary code clone detection across architectures and compiling configurations; identifying code clones having high possibilities of containing bugs; smells are sensitive to developers! on the efficiency of (un)guided customized detection; on the uniqueness of code redundancies; RepDroid: an automated tool for android application repackaging detection; NetDroid: summarizing network behavior of Android apps for network code maintenance; an exploratory study on the relationship between changes and refactoring; developer-related factors in change prediction: an empirical assessment; analyzing user comments on YouTube coding tutorial videos; bug localization with combination of deep learning and information retrieval; and bug report enrichment with application of automated fixer recommendation.},
} 


@inproceedings{20170703342981,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Real-time implementation of DVFS enhanced LEON3 MPSoC on FPGA},
journal = {International Conference on Intelligent and Advanced Systems, ICIAS 2016},
author = {Najam, Zohaib and Qadri, Muhammad Yasir and Najam, Shaheryar and Khan, Umar S.},
year = {2016},
pages = {TECHSOURCE - },
address = {Kuala Lumpur, Malaysia},
abstract = {In the field of embedded system there exists a trade off between power/performance optimization, hence many heuristics and techniques were presented at various development levels such as hardware software co-design, schedulers and optimal code compilation. This paper presents an enhanced version of LEON3 architecture which includes support for run-time management of supply voltage and processor operating frequency. This enhancement can be useful to implement various DVFS driving algorithms in LEON3 architecture aiming to leverage power consumption and throughput. Frequency scaling on the fly is based on dynamically reconfigurable clock synthesis feature available in Xilinx FPGA Virtex-4 or higher. The implementation of DVFS in LEON3 architecture is driven and controlled by general-purpose I/O port attached to advanced peripheral bus (APB) to change processor frequency on the fly during the execution of application programs. The work is implemented as a prototype on a Xilinx FPGA platform and incurs very small hardware overheads.<br/> &copy; 2016 IEEE.},
key = {Field programmable gate arrays (FPGA)},
keywords = {Application programs;Computer architecture;Computer hardware;Dynamic frequency scaling;Economic and social effects;Embedded systems;Hardware;Hardware-software codesign;Optimal systems;Optimization;Real time control;System-on-chip;},
note = {Advanced peripheral bus;DVFS;GPIO;Hardware overheads;LEON3;Operating frequency;Real-time implementations;Xilinx FPGA;},
URL = {http://dx.doi.org/10.1109/ICIAS.2016.7824035},
} 


@article{20133716730356,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A model-driven approach to teaching concurrency},
journal = {ACM Transactions on Computing Education},
author = {Carro, Manuel and Herranz, Angel and Marino, Julio},
volume = {13},
number = {1},
year = {2013},
issn = {19466226},
abstract = {We present an undergraduate course on concurrent programming where formal models are used in different stages of the learning process. The main practical difference with other approaches lies in the fact that the ability to develop correct concurrent software relies on a systematic transformation of formal models of interprocess interaction (so called shared resources), rather than on the specific constructs of some programming language. Using a resource-centric rather than a language-centric approach has some benefits for both teachers and students. Besides the obvious advantage of being independent of the programming language, the models help in the early validation of concurrent software design, provide students and teachers with a lingua franca that greatly simplifies communication at the classroom and during supervision, and help in the automatic generation of tests for the practical assignments. This method has been in use, with slight variations, for some 15 years, surviving changes in the programming language and course length. In this article, we describe the components and structure of the current incarnation of the course - which uses Java as target language - and some tools used to support our method. We provide a detailed description of the different outcomes that the model-driven approach delivers (validation of the initial design, automatic generation of tests, and mechanical generation of code) from a teaching perspective. A critical discussion on the perceived advantages and risks of our approach follows, including some proposals on how these risks can be minimized. We include a statistical analysis to show that our method has a positive impact in the student ability to understand concurrency and to generate correct code. &copy; 2013 ACM.<br/>},
key = {Teaching},
keywords = {Ada (programming language);Automatic programming;Codes (symbols);Formal specification;Java programming language;Software design;Software testing;Students;},
note = {Ada95;Coding idiom;Concurrency;Correct-by-construction;Java;Model-driven Engineering;},
URL = {http://dx.doi.org/10.1145/2414446.2414451},
} 


@inproceedings{20151200665634,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A study on SiC cladding option for light water reactor fuel},
journal = {International Congress on Advances in Nuclear Power Plants, ICAPP 2013: Nuclear Power - A Safe and Sustainable Choice for Green Future, Held with the 28th KAIF/KNS Annual Conference},
author = {Choi, Hangbok and Gutierrez, Oscar},
year = {2013},
pages = {741 - 748},
address = {Jeju Island, Korea, Republic of},
abstract = {In this study, the public version of FEMAXI code was modified for silicon carbide (SiC)composite cladding Light Water Reactor (LWR) fuel analysis by implementing SiC material property data. A sample calculation was conducted using a short LWR fuel rod and the results were compared to those of zircaloy cladding fuel. The apparent differences in material properties of SiC and zircaloy are the thermal conductivity and stiffness. Though the thermal conductivity of SiC is excellent in a non-irradi at edcondition, it degrades appreciably when irradiated, which can cause higher temperature in the fuel pellet, more thermal expansion, and more release of fission gas when compared with the zircaloy cladding fuel. While the fuel gap is closed earlier (at 5 GWd/t for the sample case) for the zircaloy cladding fuel, the same initial fuel gap of the SiC cladding fuel is not closed until 18GWd/t due to high stiffness of SiC. The radial dimensional change of the SiC cladding is very small as SiC doesn't shrink into the pellet. Because no experi-mental data are available so far for SiC composite cladding fuel, sensitivity calculations are used to explore the implementation of the SiC cladding model.<br/>},
key = {Light water reactors},
keywords = {Cladding (coating);Fission products;Nuclear energy;Nuclear fuels;Nuclear power plants;Pelletizing;Silicon carbide;Stiffness;Thermal conductivity of gases;Thermal expansion;Zirconium alloys;},
note = {High stiffness;Light water reactor (LWR);Light water reactor fuels;Public versions;Radial dimensional changes;SiC materials;Silicon carbides (SiC);Zircaloy cladding;},
} 


@inproceedings{20161402187482,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Continuous deployment and schema evolution in SQL databases},
journal = {Proceedings - 3rd International Workshop on Release Engineering, RELENG 2015},
author = {De Jong, Michael and Van Deursen, Arie},
year = {2015},
pages = {16 - 19},
address = {Florence, Italy},
abstract = {Continuous Deployment is an important enabler of rapid delivery of business value and early end user feedback. While frequent code deployment is well understood, the impact of frequent change on persistent data is less understood and supported. SQL schema evolutions in particular can make it expensive to deploy a new version, and may even lead to downtime if schema changes can only be applied by blocking operations. In this paper we study the problem of continuous deployment in the presence of database schema evolution in more detail. We identify a number of shortcomings to existing solutions and tools, mostly related to avoidable downtime and support for foreign keys. We propose a novel approach to address these problems, and provide an open source implementation. Initial evaluation suggests the approach is effective and sufficiently efficient. &copy; 2015 IEEE.},
URL = {http://dx.doi.org/10.1109/RELENG.2015.14},
} 


@inproceedings{20164703042567,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of the International Conference on Compilers, Architectures and Synthesis for Embedded Systems, CASES 2016},
journal = {Proceedings of the International Conference on Compilers, Architectures and Synthesis for Embedded Systems, CASES 2016},
year = {2016},
address = {Pittsburgh, PA, United states},
abstract = {The proceedings contain 21 papers. The topics discussed include: ILP-based modulo scheduling for high-level synthesis; handling large data sets for high-performance embedded applications in heterogeneous systems-on-chip; theoretical foundations for workload modeling with implications on power optimization; thermal-driven resource allocation and application mapping for complex many core systems; runtime management of adaptive MPSoCs for graceful degradation; towards the design of fault-tolerant mixed-criticality systems on multicores; COMET: communication-optimised multi-threaded error-detection technique; neural network transformation and co-design under neuromorphic hardware constraints; cambricon: an instruction set architecture for neural networks; RRAM based learning acceleration; a real-time digital-microfluidic platform for epigenetics; LOCUS: low-power customizable many-core architecture for wearables; D-PUF: an intrinsically reconfigurable DRAM PUF for device authentication in embedded systems; hybrid network-on-chip architectures for accelerating deep learning kernels on heterogeneous Manycore platforms; matrix multiplication beyond auto-tuning: rewrite-based GPU code generation; and a jump-target identification method for multi-architecture static binary translation.},
} 


@inproceedings{20170403267623,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Fluid and gyrokinetic modelling of particle transport in plasmas with hollow density profiles},
journal = {Journal of Physics: Conference Series},
author = {Tegnered, D. and Oberparleiter, M. and Nordman, H. and Strand, P.},
volume = {775},
number = {1},
year = {2016},
issn = {17426588},
address = {Varenna, Italy},
abstract = {Hollow density profiles occur in connection with pellet fuelling and L to H transitions. A positive density gradient could potentially stabilize the turbulence or change the relation between convective and diffusive fluxes, thereby reducing the turbulent transport of particles towards the center, making the fuelling scheme inefficient. In the present work, the particle transport driven by ITG/TE mode turbulence in regions of hollow density profiles is studied by fluid as well as gyrokinetic simulations. The fluid model used, an extended version of the Weiland transport model, Extended Drift Wave Model (EDWM), incorporates an arbitrary number of ion species in a multi-fluid description, and an extended wavelength spectrum. The fluid model, which is fast and hence suitable for use in predictive simulations, is compared to gyrokinetic simulations using the code GENE. Typical tokamak parameters are used based on the Cyclone Base Case. Parameter scans in key plasma parameters like plasma &beta;, R/L<inf>T</inf>, and magnetic shear are investigated. It is found that &beta; in particular has a stabilizing effect in the negative R/L<inf>n</inf>region, both nonlinear GENE and EDWM show a decrease in inward flux for negative R/L<inf>n</inf>and a change of direction from inward to outward for positive R/L<inf>n</inf>. This might have serious consequences for pellet fuelling of high &beta; plasmas.<br/> &copy; Published under licence by IOP Publishing Ltd.},
key = {Transport properties},
keywords = {Fueling;Fusion reactions;Genes;Magnetoplasma;Particle separators;Pelletizing;Plasma simulation;Storms;Turbulence;},
note = {Density gradients;Gyrokinetic simulations;Particle transport;Predictive simulations;Stabilizing effects;Transport modeling;Turbulent transports;Wavelength spectra;},
URL = {http://dx.doi.org/10.1088/1742-6596/775/1/012014},
} 


@inproceedings{20174404342388,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An adapted yield criterion for the evolution of subsequent yield surfaces},
journal = {Journal of Physics: Conference Series},
author = {Kusters, N. and Brosius, A.},
volume = {896},
number = {1},
year = {2017},
issn = {17426588},
address = {Munich, Germany},
abstract = {In numerical analysis of sheet metal forming processes, the anisotropic material behaviour is often modelled with isotropic work hardening and an average Lankford coefficient. In contrast, experimental observations show an evolution of the Lankford coefficients, which can be associated with a yield surface change due to kinematic and distortional hardening. Commonly, extensive efforts are carried out to describe these phenomena. In this paper an isotropic material model based on the Yld2000-2d criterion is adapted with an evolving yield exponent in order to change the yield surface shape. The yield exponent is linked to the accumulative plastic strain. This change has the effect of a rotating yield surface normal. As the normal is directly related to the Lankford coefficient, the change can be used to model the evolution of the Lankford coefficient during yielding. The paper will focus on the numerical implementation of the adapted material model for the FE-code LS-Dyna, mpi-version R7.1.2-d. A recently introduced identification scheme [1] is used to obtain the parameters for the evolving yield surface and will be briefly described for the proposed model. The suitability for numerical analysis will be discussed for deep drawing processes in general. Efforts for material characterization and modelling will be compared to other common yield surface descriptions. Besides experimental efforts and achieved accuracy, the potential of flexibility in material models and the risk of ambiguity during identification are of major interest in this paper.<br/> &copy; Published under licence by IOP Publishing Ltd.},
key = {Sheet metal},
keywords = {Characterization;Drawing (forming);Metal forming;Metal testing;Numerical analysis;Strain hardening;},
note = {Anisotropic material;Deep-drawing process;Identification scheme;Isotropic work-hardening;Lankford coefficients;Material characterizations;Numerical implementation;Subsequent yield surfaces;},
URL = {http://dx.doi.org/10.1088/1742-6596/896/1/012020},
} 


@inproceedings{20164302945069,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {2016 21st International Conference on Methods and Models in Automation and Robotics, MMAR 2016},
journal = {2016 21st International Conference on Methods and Models in Automation and Robotics, MMAR 2016},
year = {2016},
address = {Miedzyzdroje, Poland},
abstract = {The proceedings contain 224 papers. The topics discussed include: solvability of reactions and inverse dynamics problem for complex kinematic chains; ArchGenTool: a system-independent collaborative tool for robotic architecture design; signal fusion of changes in the inductive loop impedance components for vehicle axle detection; implementation of non-zero initial conditions for multi-notch FIR filter using Raspberry Pi; real time localization system with extended Kalman filter for indoor applications; linearization by generalized input-output injections on homogeneous time scales; computation of final dimension initial conditions and inputs for given outputs of differential-algebraic systems with delay; optimal sensor selection for model identification in iterative learning control of spatio-temporal systems; design of iterative learning control schemes for systems with sector-bounded nonlinearities; transformation of a fuzzy interpreted Petri net diagram into structured text code; estimation of the blood volume in pneumatically controlled ventricular assist device by vision sensor and image processing technique; and comparison of real-time industrial process control solutions: glass melting case study.},
} 


@article{20171003415848,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Hyperspectral Anomaly Detection via a Sparsity Score Estimation Framework},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
author = {Zhao, Rui and Du, Bo and Zhang, Liangpei},
volume = {55},
number = {6},
year = {2017},
pages = {3208 - 3222},
issn = {01962892},
abstract = {Anomaly detection has become an important topic in hyperspectral imagery (HSI) analysis over the last 20 years. HSIs usually possess complexly cluttered spectral signals due to the complicated conditions of the land-cover distribution. This in turn makes it difficult to obtain an accurate background estimation to distinguish the anomaly targets. The sparse learning technique provides a way to obtain an implicit background representation with the learned dictionary and corresponding sparse codes. In this paper, we explore the background/anomaly information content for each atom of the learned dictionary, from an analysis based on the frequency of the dictionary atoms for HSI reconstruction. From this perspective, we propose a novel sparsity score estimation framework for hyperspectral anomaly detection. First, an overcomplete dictionary and the corresponding sparse code matrix are obtained from the HSI. The frequency of each dictionary atom for reconstruction, which is also called the atom usage probability, is then estimated from the sparse code matrix. Finally, the estimated frequencies are transformed to the sparsity score for each pixel, which can be seen as the degree of "anomalousness." In the proposed detection framework, two strategies are proposed to enhance the diversity between the background and anomaly information in the learned dictionary: 1) dictionary-based background feature transformation and 2) dictionary iterative reweighting. A series of real-world HSI data sets is utilized to evaluate the performance of the proposed framework. The experimental results show that the proposed framework achieves a superior performance compared to some of the state-of-the-art anomaly detection methods.<br/> &copy; 2017 IEEE.},
key = {Atoms},
keywords = {Codes (symbols);Iterative methods;Spectroscopy;},
note = {Anomaly detection;Anomaly detection methods;HyperSpectral;Hyperspectral anomaly detection;Iterative reweighting;K-svd algorithms;Over-complete dictionaries;Sparse coding;},
URL = {http://dx.doi.org/10.1109/TGRS.2017.2664658},
} 


@inproceedings{20185006249509,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European So ftware Engineering Conference and Symposium on the Foundations of So ftware Engineering},
journal = {ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2018},
pages = {ACM SIGSOFT - },
address = {Lake Buena Vista, FL, United states},
abstract = {The proceedings contain 122 papers. The topics discussed include: Testing multithreaded programs via thread speed control; data race detection on compressed traces; using finite-state models for log differencing; identifying impactful service system problems via log analysis; learning to sample: exploiting similarities across environments to learn performance models for configurable systems; scalability-first pointer analysis with self-tuning context-sensitivity; code vectors: understanding programs through embedded abstracted symbolic traces; what makes a code change easier to review: an empirical investigation on code change reviewability; and the impact of regular expression denial of service (ReDoS) in practice: an empirical study at the ecosystem scale.<br/>},
} 


@inproceedings{20174904495961,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Development of an in-core fuel management tool for boiling water reactors},
journal = {2017 International Congress on Advances in Nuclear Power Plants, ICAPP 2017 - A New Paradigm in Nuclear Power Safety, Proceedings},
author = {Gilli, Luca and Wakker, Pieter H. and Elder, Brian R.},
year = {2017},
pages = {et al.; Hitachi-GE; Kobelco; Mitsubishi Heavy Industries, Ltd.; Nuclear Engineering, Ltd. (NEL); TOA Valve Engineering Inc. (TVE) - },
address = {Fukui and Kyoto, Japan},
abstract = {The in-core fuel management of a nuclear reactor is a challenging task due to the virtually infinite number of loading patterns one could theoretically adopt. The ROSA (Reloading Optimization by Simulated Annealing) code is an optimization tool that has been successfully used in the last two decades to facilitate the core design of several Pressurized Water Reactors (PWRs). It is designed to perform a stochastic search for an optimal Loading Pattern (LP) using a simulated annealing algorithm. This corresponds to performing a depletion calculation for each one of the hundreds of thousands of unique LPs generated during the stochastic search. Therefore, speed is one of the most important requirements that the solvers used by the depletion tool must fulfill. ROSA's depletion analysis tool makes use of a particularly fast nodal method (known as the kernel method) for the evaluation of the power distribution associated with a particular LP. One of the strongest assumptions behind the kernel method is that the neutron migration length does not change considerably between the point where a neutron is generated and the point where the same neutron is absorbed. Although strong, this assumption is quite compatible with the neutronic characteristics of PWRs cores. In this paper we give an overview of the work done in order to develop a version of ROSA capable of performing the core design of Boiling Water Reactors (BWRs). We focus the discussion on the development of the depletion analysis tool by outlining the modifications of the kernel methods implemented in order to make the solver accurate for BWR cores. An improvement of the definition of the transport kernel is necessary to take the strong anisotropies characterizing the neutronic problem into account. These anisotropies arise due to the presence of strong changes in the moderator density and due to the presence of control blades. Furthermore, we are going to discuss how the boundary conditions are adopted by the neutronic solver in the form of albedos, which are used as a calibration parameter to tune the solution using the information obtained from an external license code. We then analyze the accuracy of ROSA-BWR when predicting power distributions and safety parameters by benchmarking it against the core follow calculations obtained by using the license code MICROBURN-B2 for three cycles of the Browns Ferry 3 nuclear power plant. This benchmark is also discussed in terms of computational performances and potential gains achievable through parallelization. The final section of the paper contains a description of the optimization capabilities that are currently implemented in ROSA-BWR and an overview of the preliminary applications of the code to real life core design studies.<br/>},
key = {Pressurized water reactors},
keywords = {Anisotropy;Boiling water reactors;Codes (symbols);Neutrons;Nuclear energy;Nuclear fuels;Nuclear power plants;Nuclear reactor licensing;Simulated annealing;Stochastic systems;},
note = {Calibration parameters;Computational performance;Depletion calculation;In-core fuel management;Optimization capabilities;Optimization tools;Power distributions;Simulated annealing algorithms;},
} 


@article{20172803923446,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Impact of a hollow density profile on turbulent particle fluxes: Gyrokinetic and fluid simulations},
journal = {Physics of Plasmas},
author = {Tegnered, D. and Oberparleiter, M. and Strand, P. and Nordman, H.},
volume = {24},
number = {7},
year = {2017},
issn = {1070664X},
abstract = {Hollow density profiles may occur in connection with pellet fuelling and L to H transitions. A positive density gradient could potentially stabilize the turbulence or change the relation between convective and diffusive fluxes, thereby reducing the turbulent transport of particles towards the center, making the pellet fuelling scheme inefficient. In the present work, the particle transport driven by Ion Temperature Gradient/Trapped Electron (ITG/TE) mode turbulence in hollow density profiles is studied by fluid as well as gyrokinetic simulations. The fluid model used, an extended version of the Weiland transport model, Extended Drift Wave Model (EDWM), incorporates an arbitrary number of ion species in a multi-fluid description and an extended wavelength spectrum. The fluid model, which is fast and hence suitable for use in predictive simulations, is compared to gyrokinetic simulations using the code GENE. Typical tokamak parameters are used based on the Cyclone Base Case. Parameter scans in key plasma parameters like plasma &beta;, R/L<inf>T</inf>, and magnetic shear are investigated. In addition, the effects of a fast species are studied and global ITG simulations in a simplified physics description are performed in order to investigate nonlocal effects. It is found that &beta; in particular, has a stabilizing effect in the negative R/L<inf>n</inf>region. Both nonlinear GENE and EDWM simulations show a decrease in inward flux for negative R/L<inf>n</inf>and a change in the direction from inward to outward for positive R/L<inf>n</inf>. Moreover, the addition of fast particles was shown to decrease the inward main ion particle flux in the positive gradient region further. This might have serious consequences for pellet fuelling of high &beta; plasmas. Additionally, the heat flux in global ITG turbulence simulations indicates that nonlocal effects can play a different role from usual in connection with pellet fuelling.<br/> &copy; 2017 Author(s).},
key = {Transport properties},
keywords = {Fueling;Fusion reactions;Genes;Heat flux;Ions;Magnetoplasma;Pelletizing;Plasma simulation;Storms;Turbulence;},
note = {Gyrokinetic simulations;Ion temperature gradient;Particle transport;Predictive simulations;Stabilizing effects;Transport modeling;Turbulence simulation;Turbulent transports;},
URL = {http://dx.doi.org/10.1063/1.4990078},
} 


@inproceedings{20144700227508,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {How do developers react to RESTful API evolution?},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Wang, Shaohua and Keivanloo, Iman and Zou, Ying},
volume = {8831},
year = {2014},
pages = {245 - 259},
issn = {03029743},
address = {Paris, France},
abstract = {With the rapid adoption of REpresentational State Transfer (REST), more software organizations expose their applications as RESTful web APIs and client code developers integrate RESTful APIs into their applications. When web APIs evolve, the client code developers have to update their applications to incorporate the API changes accordingly. However client code developers often encounter challenges during the migration and API providers have little knowledge of how client code developers react to the API changes. In this paper, we investigate the changes among subsequent versions of APIs and classify the identified changes to understand how the RESTful web APIs evolve. We study the on-line discussion from developers to the API changes by analyzing the StackOverflow questions. Through an empirical study, we identify 21 change types and 7 of them are new compared with existing studies. We find that a larger portion of RESTful web API elements are changed between versions compared with Java APIs and WSDL services. Moreover, our results show that adding new methods in the new version causes more questions and views from developers. However the deleted methods draw more relevant discussions. In general, our results provide valuable insights of RESTful web API evolution and help service providers understand how their consumers react to the API changes in order to improve the practice of evolving the service APIs.<br/> &copy; Springer-Verlag Berlin Heidelberg 2014.},
key = {Application programming interfaces (API)},
keywords = {Application programs;Codes (symbols);Distributed computer systems;},
note = {API evolution;Empirical studies;Representational state transfer;Rest api;Service provider;Social media;Software organization;StackOverflow;},
} 


@article{20175004514670,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An empirical study on the impact of AspectJ on software evolvability},
journal = {Empirical Software Engineering},
author = {Przybylek, Adam},
volume = {23},
number = {4},
year = {2018},
pages = {2018 - 2050},
issn = {13823256},
abstract = {Since its inception in 1996, aspect-oriented programming (AOP) has been believed to reduce the effort required to maintain software systems by replacing cross-cutting code with aspects. However, little convincing empirical evidence exists to support this claim, while several studies suggest that AOP brings new obstacles to maintainability. This paper discusses two controlled experiments conducted to evaluate the impact of AspectJ (the most mature and popular aspect-oriented programming language) versus Java on software evolvability. We consider evolvability as the ease with which a software system can be updated to fulfill new requirements. Since a minor language was compared to the mainstream, the experiments were designed so as to anticipate that the participants were much more experienced in one of the treatments. The first experiment was performed on 35 student subjects who were asked to comprehend either Java or AspectJ implementation of the same system, and perform the corresponding comprehension tasks. Participants of both groups achieved a high rate of correct answers without a statistically significant difference between the groups. Nevertheless, the Java group significantly outperformed the AspectJ group with respect to the average completion time. In the second experiment, 24 student subjects were asked to implement (in a non-invasive way) two extension scenarios to the system that they had already known. Each subject evolved either the Java version using Java or the AspectJ version using AspectJ. We found out that a typical AspectJ programmer needs significantly fewer atomic changes to implement the change scenarios than a typical Java programmer, but we did not observe a significant difference in completion time. The overall result indicates that AspectJ has a different effect on two sub-characteristics of the evolvability: understandability and changeability. While AspectJ decreases the former, it improves one aspect of the latter.<br/> &copy; 2017, The Author(s).},
key = {Java programming language},
keywords = {Aspect oriented programming;Computer software;Maintainability;},
note = {Aspect-Oriented Programming (AOP);Comprehension tasks;Controlled experiment;Different effects;Empirical studies;Separation of concerns;Statistically significant difference;Understandability;},
URL = {http://dx.doi.org/10.1007/s10664-017-9580-7},
} 


@article{20164202914064,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The hydrogen issue in the initial operation of a filtered containment venting system},
journal = {Nuclear Technology},
author = {Na, Young Su and Cho, Song-Won and Ha, Kwang Soon},
volume = {195},
number = {3},
year = {2016},
pages = {329 - 334},
issn = {00295450},
abstract = {This study evaluated the hydrogen issue in the initial operation of a filtered containment venting system (FCVS). We calculated the volumetric concentration of hydrogen, steam, and air in the postulated FCVS connected with the OPR 1000, as a target nuclear power plant, under a station blackout using the MELCOR computer code (version 1.8.6). A large amount of steam and a flammable mixture generated during a severe accident are immediately released from the containment building to the FCVS when the pressure in the containment building approaches a set value. The constituent ratio of the flammable mixture of hydrogen, steam, and air can change due to the different thermal-hydraulic conditions between those due to a severe accident in the containment building and the initial condition in the FCVS. The volumetric concentration of hydrogen was 6% in the containment building just before the operation of the FCVS. It increased up to 9% in the FCVS vessel during the early operation, and steam condensation occurred simultaneously. The atmospheric condition including steam, hydrogen, and air in the FCVS can enter the combustion zone in the Shapiro diagram.<br/>},
key = {Hydrogen},
keywords = {Accidents;Mixtures;Nuclear fuels;Nuclear power plants;Steam;Steam condensers;Steam power plants;},
note = {Atmospheric conditions;Containment buildings;Containment venting;Flammable mixtures;Severe accident;Steam condensation;Thermal-hydraulic conditions;Volumetric concentrations;},
URL = {http://dx.doi.org/10.13182/NT15-160},
} 


@inproceedings{20131016079687,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Compiler Construction - 22nd International Conference, CC 2013, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2013, Proceedings},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {7791 LNCS},
year = {2013},
issn = {03029743},
address = {Rome, Italy},
abstract = {The proceedings contain 13 papers. The topics discussed include: optimal register allocation in polynomial time; optimal and heuristic global code motion for minimal spilling; efficient and effective handling of exceptions in java points-to analysis; an incremental points-to analysis with CFL-reachability; FESA: fold- and expand-based shape analysis; simple and efficient construction of static single assignment form; PolyGLoT: a polyhedral loop transformation framework for a graphical dataflow language; architecture-independent dynamic information flow tracking; automatic generation of program affinity policies using machine learning; compiler-guided identification of critical sections in parallel code; refactoring MATLAB; and on LR parsing with selective delays.},
} 


@inproceedings{20172503798696,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Improving of a circuit checkability and trustworthiness of data processing results in LUT-based FPGA components of safety-related systems},
journal = {CEUR Workshop Proceedings},
author = {Drozd, Oleksandr and Drozd, Miroslav and Martynyuk, Oleksandr and Kuznietsov, Mykola},
volume = {1844},
year = {2017},
pages = {654 - 661},
issn = {16130073},
address = {Kyiv, Ukraine},
abstract = {The possibility of improving on attributes of a solution which are traditionally opposed each other is proved. For digital components of safety-related systems, the method of improving on attributes of a checkability of the circuit and trustworthiness of the results calculated on FPGA with the LUT-oriented architecture is offered. The method is directed to improving of the ready project by a choice of the version of a program code without change of the hardware decision. Versions of LUT memory programming and a set of faults on which these versions exert impact are generated. Faults of shorts between adjacent address inputs of LUT are considered. Operation of the circuit is simulated on all versions and on all set of faults. The method selects the versions providing increase in a checkability of the circuit in a normal mode and trustworthiness of results in emergency mode of the safety-related systems.<br/>},
key = {Field programmable gate arrays (FPGA)},
keywords = {Data handling;Industrial research;Knowledge management;Timing circuits;},
note = {Digital components;Normal modes;Program code;Safety-related systems;Trustworthiness of the results;},
} 


@inproceedings{20153001071028,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Verification of new floating capabilities in FAST v8},
journal = {33rd Wind Energy Symposium},
author = {Wendt, Fabian and Robertson, Amy and Jonkman, Jason and Hayman, Greg},
year = {2015},
address = {Kissimmee, FL, United states},
abstract = {FAST v8 is the latest release of the National Renewable Energy Laboratory&rsquo;s wind turbine aero-hydro-servo-elastic simulation software, with several new capabilities and major changes from the previous version. FAST has been significantly altered to improve the simulator&rsquo;s modularity and to include new functionalities in the form of modules in the FAST v8 framework. This paper focuses on the improvements made for the modeling of floating offshore wind systems. The most significant change was to the hydrodynamic load calculation algorithms, which are embedded in the HydroDyn module. HydroDyn is now capable of applying strip-theory (via an extension of Morison&rsquo;s equation) at the member level for user-defined geometries. Users may now use a strip-theory-only approach for applying the hydrodynamic loads, as well as the previous potential-flow (radiation/diffraction) approach and a hybrid combination of both methods (radiation/diffraction and the drag component of Morison&rsquo;s equation). Second-order hydrodynamic implementations in both the wave kinematics used by the strip-theory solution and the wave-excitation loads in the potential-flow solution were also added to HydroDyn. The new floating capabilities were verified through a direct code-to-code comparison. We conducted a series of simulations of the International Energy Agency Wind Task 30 Offshore Code Comparison Collaboration Continuation (OC4) floating semisubmersible model and compared the wind turbine response predicted by FAST v8, the corresponding FAST v7 results, and results from other participants in the OC4 project. We found good agreement between FAST v7 and FAST v8 when using the linear radiation/diffraction modeling approach. The strip-theory-based approach inherently differs from the radiation/diffraction approach used in FAST v7 and we identified and characterized the differences. Enabling the second-order effects significantly improved the agreement between FAST v8 and the other OC4 participants.<br/> &copy; 2015, American Institute of Aeronautics and Astronautics Inc. All rights reserved.},
key = {Wind turbines},
keywords = {Codes (symbols);Computer software;Hydrodynamics;Potential flow;Wind power;},
note = {Code comparisons;Hydrodynamic loads;International energy agency;National Renewable Energy Laboratory;Second order effect;Simulation software;Wave excitation;Wave kinematics;},
} 


@inproceedings{20184005899043,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Framework for Detecting Metamorphic Malware Based on Opcode Feature Extraction},
journal = {2nd International Conference on Computational Systems and Information Technology for Sustainable Solutions, CSITSS 2017},
author = {Prapulla, S.B. and Bhat, Sharath J and Shobha, G.},
year = {2017},
address = {Bengaluru, India},
abstract = {Malware is a PC program that is intended to enter and hinder PCs without proprietor's consent. There are different malware types such as key loggers, logic bomb, viruses, rootkits, Trojans, spywares, ransom wares, worms, backdoors, bots, etc. Metamorphic malwares are the malware type which can undergo change at runtime such that they change their code and signature's to bypass the security. Malware's are written in such a way that they get mutated frequently but the original algorithm remains the same. To distinguish a malware, systems like conduct based location, signature based identification and machine learning based procedures are utilized. The mark based discovery framework falls flat in the event that it experiences another obscure malware. In the event of conduct based identification framework, if the antivirus program distinguishes any odd conduct over web then it will produce alert flag, yet there is a high false positive rate as it is inclined to arrange kind records as malware. Machine learning methodology is proposed to detect the malware where feature extraction is based on opcodes in the executable file of the malware. Opcodes of the malware can be extracted into a text file using disassemblers like IDA pro. As success rate of a machine learning algorithm depends on correctness of the features extracted, a robust approach to extract the features is proposed. This model is expected to give success rate of more than 96% in detection of malware.<br/> &copy; 2017 IEEE.},
key = {Malware},
keywords = {Artificial intelligence;Computation theory;Computer viruses;Computer worms;Extraction;Feature extraction;Learning algorithms;Learning systems;Microcomputers;},
note = {Anti-virus programs;Discovery frameworks;False positive rates;Metamorphic;Metamorphic malware;Opcodes;Original algorithms;Robust approaches;},
URL = {http://dx.doi.org/10.1109/CSITSS.2017.8447810},
} 


@article{20161002046487,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Generalized independent component analysis over finite alphabets},
journal = {IEEE Transactions on Information Theory},
author = {Painsky, Amichai and Rosset, Saharon and Feder, Meir},
volume = {62},
number = {2},
year = {2016},
pages = {1038 - 1053},
issn = {00189448},
abstract = {Independent component analysis (ICA) is a statistical method for transforming an observable multi-dimensional random vector into components that are as statistically independent as possible from each other. Usually, the ICA framework assumes a model according to which the observations are generated (such as a linear transformation with additive noise). ICA over finite fields is a special case of ICA in which both the observations and the independent components are over a finite alphabet. In this paper, we consider a generalization of this framework in which an observation vector is decomposed to its independent components (as much as possible) with no prior assumption on the way it was generated. This generalization is also known as Barlow's minimal redundancy representation problem and is considered an open problem. We propose several theorems and show that this hard problem can be accurately solved with a branch and bound search tree algorithm, or tightly approximated with a series of linear problems. Our contribution provides the first efficient set of solutions to Barlow's problem. The minimal redundancy representation (also known as factorial code) has many applications, mainly in the fields of neural networks and deep learning. The binary ICA is also shown to have applications in several domains, including medical diagnosis, multi-cluster assignment, network tomography, and internet resource management. In this paper, we show that this formulation further applies to multiple disciplines in source coding, such as predictive coding, distributed source coding, and coding of large alphabet sources.<br/> &copy; 2015 IEEE.},
key = {Independent component analysis},
keywords = {Additive noise;Blind source separation;Codes (symbols);Deep learning;Diagnosis;Linear transformations;Mathematical transformations;Minimum entropy methods;Neural networks;Redundancy;Trees (mathematics);},
note = {BICA;Distributed source coding;Factorial codes;Galois fields;Minimal redundancy;Minimum entropies;Predictive coding;},
URL = {http://dx.doi.org/10.1109/TIT.2015.2510657},
} 


@inproceedings{20141717613372,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Facilitating the specification of wsmo ontology using model-driven development},
journal = {International Journal of Metadata, Semantics and Ontologies},
author = {Bensaber, Djamel Amar},
volume = {9},
number = {2},
year = {2014},
pages = {103 - 113},
issn = {17442621},
abstract = {A semantic web service extends the capabilities of a web service by associating a semantic description of the web service in order to enable better search, discovery, selection, composition and integration. Semantically rich language such as Web Service Modeling Ontology (WSMO) has been created in order to provide a mechanism for describing the semantics of semantic web services. Unfortunately, for the common developer the learning curve for such languages can be steep, providing a barrier for adoption and widespread use. In this paper, we present a Model- Driven Architecture (MDA) approach for facilitating the specification of WSMO ontology through the use of meta-model and UML profile for modelling semantic web services; we concentrate our efforts to develop a transformation approach based on MDA to automate the generation of code by translating XMI specifications (e.g. XML encodings of UML) into equivalent WSMO specifications using Atlas Transformation Language (ATL) transformations. &copy;2014 Inderscience Enterprises Ltd.<br/>},
key = {Web services},
keywords = {Metalorganic frameworks;Modeling languages;Ontology;Semantic Web;Semantics;Software architecture;Software design;Specifications;Websites;XML;},
note = {Atlas transformation languages;Ecore;Meta model;Meta object facility;Model driven architectures;Web service modeling ontologies;WSMO;XML metadata;},
URL = {http://dx.doi.org/10.1504/IJMSO.2014.060322},
} 


@article{20170903385408,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Selecting and scaling methods of bi-directional horizontal ground motion records using BdMA spectrum},
journal = {Jianzhu Jiegou Xuebao/Journal of Building Structures},
author = {Wang, Min and Li, Wenjie and Zhu, Aiping and Fu, Jianping},
volume = {38},
number = {1},
year = {2017},
pages = {76 - 84},
issn = {10006869},
abstract = {Bi-directional ground motions with two orthogonal horizontal values are required to predict seismic response of high-rise and complicated buildings using time history analysis. By reviewing current methods of selecting and scaling bi-directional ground motion records, it points out shortcomings of the Chinese codes in this aspect. In addition, based on the bi-directional maximum acceleration (BdMA) spectrum, the BdMA method of selecting and scaling ground motion records was developed. Nonlinear time history analyses were performed for a representative building subjected to bi-directional ground motions selected using the BdMA method and the one used in the current code in China. The differences of predicting the structural seismic response using these two methods were compared. Further the reliability of seismic response prediction based on the current Chinese code was evaluated. The results indicate that, the BdMA method produces a larger structural seismic response than the Chinese code method. The former one has 7% and 15% larger results than the latter for force and displacement prediction respectively. With the change of types of ground motions controlling seismic hazard, the method in China code has different levels of hazard compared to the BdMA method. Using uniform performance index to control the structural seismic performance, the ultimate structural seismic safety for these two methods is different. The BdMA method of selecting and scaling bi-directional ground motions is recommended.<br/> &copy; 2017, Editorial Office of Journal of Building Structures. All right reserved.},
key = {Seismic response},
keywords = {Codes (standards);Codes (symbols);Concrete bridges;Forecasting;Hazards;Seismic design;Spectrum analysis;Structural analysis;},
note = {Acceleration spectrum;Bi-directional;Ground motions;History analysis;Scaling method;},
URL = {http://dx.doi.org/10.14006/j.jzjgxb.2017.01.008},
} 


@inproceedings{20144800244880,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Using random error correcting codes in near-collision attacks on generic hash-functions},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Polak, Inna and Shamir, Adi},
volume = {8885},
year = {2014},
pages = {219 - 236},
issn = {03029743},
address = {New Delhi, India},
abstract = {In this paper we consider the problem of finding nearcollisions with Hamming distance bounded by r in generic n-bit hash functions. In 2011, Lamberger and Rijmen proposed a modified version of Pollard&rsquo;s rho method, and in 2012 Leurent improved this memoryless algorithm by using any available memory to store chain endpoints. Both algorithms use a perfect error correcting code to change near-collisions into full-collisions, but such codes are rare and have very small distance. In this paper we propose using randomly chosen linear codes, whose decoding can be made efficient by using some of the available memory to store error-correction tables. Compared to Leurent&rsquo;s algorithm, we experimentally verified an improvement ratio of about 3 in a small example with n = 160 and r = 33 which we implemented on a single PC, and mathematically predicted a significant improvement ratio of about 730 in a larger example with n = 1024 and r = 100, using 2<sup>40</sup>memory.<br/> &copy; Springer International Publishing Switzerland 2014.},
key = {Codes (symbols)},
keywords = {Economic and social effects;Error correction;Hamming distance;Hash functions;},
note = {Error correcting code;Generic attack;Improvement ratio;Linear codes;Near-collisions;Random codes;S-algorithms;Time-memory trade-offs;},
URL = {http://dx.doi.org/10.1007/978-3-319-13039-2_13},
} 


@inproceedings{20135017065949,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {APPification of hospital healthcare and data management using QRcodes},
journal = {IISA 2013 - 4th International Conference on Information, Intelligence, Systems and Applications},
author = {Mersini, Paschou and Sakkopoulos, Evangelos and Tsakalidis, Athanasios},
year = {2013},
pages = {216 - 218},
address = {Piraeus-Athens, Greece},
abstract = {In this work, we describe an integrated system, developed for use by the healthcare personnel within healthcare facilities, adapted to smartphones, tablets and handheld devices. Our key goal is to facilitate doctors, nurses and the involved personnel throughout the facility, regardless of the existence of network connection in the area using a typical smartphone. The proposed application and its backend system support access to patient's history, i.e. previous diagnoses, medication, and specification of allergies. More features include updates on the progress of the patient, sending referrals directly to hematology, microbiology or biochemistry laboratory and instant notification for the retrieval of laboratory results within a hospital or any healthcare institution. Additionally, we integrate Quick Response (QR code) for coding and accessing medical related data of the patient using a smartphone or a tablet, to be used by the facility itself or anyone else certified. The QR code significantly improves interoperability cases for legacy and non interrelated systems based on HL7 and XML transformation of the HL7 patient referrals. &copy; 2013 IEEE.<br/>},
key = {Medical computing},
keywords = {Codes (symbols);Diagnosis;Hospitals;Information management;Information systems;Interoperability;Legacy systems;mHealth;Smartphones;},
note = {Health care information system;Health systems;Healthcare facility;Healthcare institutions;Mobile Technology;Network connection;Quick response;XML transformation;},
URL = {http://dx.doi.org/10.1109/IISA.2013.6623716},
} 


@inproceedings{20162002384111,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Understanding the propagation of transient errors in HPC applications},
journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
author = {Ashraf, Rizwan A. and Gioiosa, Roberto and Kestor, Gokcen and Demara, Ronald F. and Cher, Chen-Yong and Bose, Pradip},
volume = {15-20-November-2015},
year = {2015},
pages = {Association for Computing Machinery (ACM); IEEE Computer Society - },
issn = {21674329},
address = {Austin, TX, United states},
abstract = {Resiliency of exascale systems has quickly become an important concern for the scientific community. Despite its importance, still much remains to be determined regarding how faults disseminate or at what rate do they impact HPC applications. The understanding of where and how fast faults propagate could lead to more efficient implementation of application-driven error detection and recovery. In this work, we propose a fault propagation framework to analyze how faults propagate in MPI applications and to understand their vulnerability to faults. We employ a combination of compiler-level code transformation and instrumentation, along with a runtime checker. Using the information provided by our framework, we employ machine learning technique to derive application fault propagation models that can be used to estimate the number of corrupted memory locations at runtime.<br/> &copy; 2015 ACM.},
key = {Errors},
keywords = {Chemical detection;Cosine transforms;Learning systems;Radiation hardening;},
note = {Application vulnerabilities;Distributed applications;Fault injection;Fault propagation;resiliency;Soft error;},
URL = {http://dx.doi.org/10.1145/2807591.2807670},
} 


@inproceedings{20161502229532,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A hybrid generative/discriminative model based object tracking primary exploration},
journal = {IntelliSys 2015 - Proceedings of 2015 SAI Intelligent Systems Conference},
author = {Chen, Yehong and Park, Pil Seong},
year = {2015},
pages = {765 - 772},
address = {Canary Wharf, Level 32, 40 Bank Street, London, United kingdom},
abstract = {Based on analysis and discussion of object representation, a hybrid model based tracking by detection algorithm is presented as yet a primary exploration. The whole system is made of a learning-detecting two phase loop. Object model is built on a general Haar-like feature space which is automatically generated and extracted by a special random projection. Our proposed algorithm involves two type of methods for object modeling, one is to learn a transformation matrix by Principal Component Analysis (PCA) as the multi-view appearance model of the target object, and the other is to learn a classifier by Fisher Linear Discriminant Analysis (FLD) as the classification between the foreground and the background. We extend the Fisher criterion to a multi-mode background situation, which is used to formulate features' discriminating power as feature weighting from the online captured positive/negative training data. In additionally, a two-stage detection is involved, in which all input samples firstly are tested by the learned FLD classifier to pick up candidates, then amongst candidates the maximum likelihood to the target template as the final detection result is searched for by PCA code matching. All generative model, discriminative model and target templates should online update due to appearance variation. A number of experiments illustrate that the proposed hybrid model based tracking algorithm does has advantages.<br/> &copy; 2015 IEEE.},
key = {Principal component analysis},
keywords = {Discriminant analysis;Intelligent systems;Linear transformations;Mathematical transformations;Maximum likelihood;Tracking (position);},
note = {Automatically generated;Feature weighting;Fisher linear discriminant analysis;Generative/Discriminative Models;Hybrid model;Multimodes;Online modeling;Transformation matrices;},
URL = {http://dx.doi.org/10.1109/IntelliSys.2015.7361227},
} 


@article{20153301160626,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mixing numerical and categorical data in a Self-Organizing Map by means of frequency neurons},
journal = {Applied Soft Computing Journal},
author = {Del Coso, Carmelo and Fustes, Diego and Dafonte, Carlos and Novoa, Francisco J. and Rodriguez-Pedreira, Jose M. and Arcay, Bernardino},
volume = {36},
year = {2015},
pages = {246 - 254},
issn = {15684946},
abstract = {Even though Self-Organizing Maps (SOMs) constitute a powerful and essential tool for pattern recognition and data mining, the common SOM algorithm is not apt for processing categorical data, which is present in many real datasets. It is for this reason that the categorical values are commonly converted into a binary code, a solution that unfortunately distorts the network training and the posterior analysis. The present work proposes a SOM architecture that directly processes the categorical values, without the need of any previous transformation. This architecture is also capable of properly mixing numerical and categorical data, in such a manner that all the features adopt the same weight. The proposed implementation is scalable and the corresponding learning algorithm is described in detail. Finally, we demonstrate the effectiveness of the presented algorithm by applying it to several well-known datasets.<br/> &copy; 2015 Elsevier B.V. All rights reserved.},
key = {Self organizing maps},
keywords = {Big data;Clustering algorithms;Conformal mapping;Data handling;Data mining;Learning algorithms;Mixing;Network architecture;Pattern recognition;},
note = {Categorical data;Mixed data;Network training;Posterior analysis;Real data sets;Self organizing maps(soms);SOM algorithms;},
URL = {http://dx.doi.org/10.1016/j.asoc.2015.06.058},
} 


@inproceedings{20142217775486,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards a unified heterogeneous development model in android},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Acosta, Alejandro and Almeida, Francisc},
volume = {8374 LNCS},
year = {2014},
pages = {238 - 248},
issn = {03029743},
address = {Aachen, Germany},
abstract = {The advent of emergent SoCs and MPSocs opens a new era on The small mobile devices (Smartphones, Tablets,.) in Terms of computing capabilities and applications To be addressed. The efficient use of such devices, including The parallel power, is still a challenge for general purpose programmers due To The very high learning curve demanding very specific knowledge of The devices. While some efforts are currently being made, mainly in The scientific scope, The scenario is still quite far from being The desirable for non-scientific applications where very few of Them Take advantage of The parallel capabilities of The devices. We propose Paralldroid (Framework for Parallelism in Android), a parallel development framework oriented To general purpose programmers for standard mobile devices. Paralldroid presents a programming model That unifies The different programming models of Android. The user just implements a Java application and introduces a set of Paralldroid annotations in The sections of code To be optimized. The Paralldroid system automatically generates The native C, OpenCL or Renderscript code for The annotated section. The Paralldroid Transformation model involves source-To-source Transformations and skeletal programming. &copy; 2014 Springer-Verlag Berlin Heidelberg.<br/>},
key = {Android (operating system)},
keywords = {C (programming language);Constraint theory;},
note = {Android;Computing capability;General-purpose programmers;Parallel development;Renderscript;Scientific applications;Source-to-source transformations;Transformation model;},
URL = {http://dx.doi.org/10.1007/978-3-642-54420-0_24},
} 


@article{20170103212450,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A perturbation-based susbtep method for coupled depletion Monte-Carlo codes},
journal = {Annals of Nuclear Energy},
author = {Kotlyar, Dan and Aufiero, Manuele and Shwageraus, Eugene and Fratoni, Massimiliano},
volume = {102},
year = {2017},
pages = {236 - 244},
issn = {03064549},
abstract = {Coupled Monte Carlo (MC) methods are becoming widely used in reactor physics analysis and design. Many research groups therefore, developed their own coupled MC depletion codes. Typically, in such coupled code systems, neutron fluxes and cross sections are provided to the depletion module by solving a static neutron transport problem. These fluxes and cross sections are representative only of a specific time-point. In reality however, both quantities would change through the depletion time interval. Recently, Generalized Perturbation Theory (GPT) equivalent method that relies on collision history approach was implemented in Serpent MC code. This method was used here to calculate the sensitivity of each nuclide and reaction cross section due to the change in concentration of every isotope in the system. The coupling method proposed in this study also uses the substep approach, which incorporates these sensitivity coefficients to account for temporal changes in cross sections. As a result, a notable improvement in time dependent cross section behavior was obtained. The method was implemented in a wrapper script that couples Serpent with an external depletion solver. The performance of this method was compared with other existing methods. The results indicate that the proposed method requires substantially less MC transport solutions to achieve the same accuracy.<br/> &copy; 2016},
key = {Monte Carlo methods},
keywords = {Codes (symbols);Isotopes;Neutron flux;Perturbation techniques;},
note = {Coupled code;Depletion;General perturbations;Generalized perturbation theories;Neutron transport problems;Reaction cross-section;Sensitivity coefficient;Serpent;},
URL = {http://dx.doi.org/10.1016/j.anucene.2016.12.022},
} 


@article{20160401845370,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Bridging a gap: in search of an analytical tool capturing teachers perceptions of their own teaching},
journal = {International Journal of Technology and Design Education},
author = {Rolandsson, Lennart and Skogh, Inga-Britt and Mannikko Barbutiu, Sirkku},
volume = {27},
number = {3},
year = {2017},
pages = {445 - 458},
issn = {09577572},
abstract = {Computing and computers are introduced in school as important examples of technology, sometimes as a subject matter of their own, and sometimes they are used as tools for other subjects. All in all, one might even say that learning about computing and computers is part of learning about technology. Lately, many countries have implemented programming in their curricula as a means to address society&rsquo;s dependence on, and need for programming knowledge and code. Programming is a fairly new school subject without educational traditions and, due to the rapid technological development, in constant change. This means that most programming teachers must decide for themselves what and how to teach. In this study, programming teachers&rsquo; teaching is studied. With the aim of exploring the connection/possible gap between teacher&rsquo;s intentions and the teacher&rsquo;s instructional practice, an expansion of the conceptual apparatus of phenomenography and variation theory is tested. In the article, phenomenography and variation theory and the suggested supplementary theoretical tool (Georg Henrik von Wright&rsquo;s model of logic of events) are briefly presented and then deployed upon one selected case. Findings reveal that teachers&rsquo; intentions (reflected in their actions) include an emphasis (of teachers&rsquo; side) on the importance of balancing theory and practice, using different learning strategies, encouraging learning by trial-and-error and fostering collaboration between students for a deeper understanding of concepts. In conclusion, logic of events interpretations proves to be useful as a complementary tool to the conceptual apparatus of phenomenography.<br/> &copy; 2016, Springer Science+Business Media Dordrecht.},
key = {Teaching},
keywords = {Computation theory;Computer circuits;Curricula;Education computing;},
note = {Action;Intention;Intentionality;Logic of events;Object of learning;Phenomenography;Programming education;Teacher;},
URL = {http://dx.doi.org/10.1007/s10798-016-9353-4},
} 


@inproceedings{20161302174137,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {To fix or to learn? How production bias affects developers' information foraging during debugging},
journal = {2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings},
author = {Piorkowski, David and Fleming, Scott D. and Scaffidi, Christopher and Burnett, Margaret and Kwan, Irwin and Henley, Austin Z. and Macbeth, Jamie and Hill, Charles and Horvath, Amber},
year = {2015},
pages = {11 - 20},
address = {Bremen, Germany},
abstract = {Developers performing maintenance activities must balance their efforts to learn the code vs. their efforts to actually change it. This balancing act is consistent with the 'production bias' that, according to Carroll's minimalist learning theory, generally affects software users during everyday tasks. This suggests that developers' focus on efficiency should have marked effects on how they forage for the information they think they need to fix bugs. To investigate how developers balance fixing versus learning during debugging, we conducted the first empirical investigation of the interplay between production bias and information foraging. Our theory-based study involved 11 participants: half tasked with fixing a bug, and half tasked with learning enough to help someone else fix it. Despite the subtlety of difference between their tasks, participants foraged remarkably differently-making foraging decisions from different types of 'patches,' with different types of information, and succeeding with different foraging tactics.<br/> &copy; 2015 IEEE.},
key = {Program debugging},
keywords = {Computer debugging;Computer software maintenance;Costs;},
note = {Empirical investigation;Information foraging;Learning Theory;Performing maintenance;},
URL = {http://dx.doi.org/10.1109/ICSM.2015.7332447},
} 


@inproceedings{20181805122838,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Teaching cyber security using competitive software obfuscation and reverse engineering activities},
journal = {SIGCSE 2018 - Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
author = {Asghar, Muhammad Rizwan and Luxton-Reilly, Andrew},
volume = {2018-January},
year = {2018},
pages = {179 - 184},
address = {Baltimore, MD, United states},
abstract = {Teaching cyber security techniques can be challenging due to the complexity associated with building secure systems. The major issue is these systems could easily be broken if proper protection techniques are not employed. This requires students to understand the offensive approaches that can be used to breach security in order to better understand how to properly defend against cyber attacks. We present a novel approach to teaching cyber security in a graduate course using an innovative assessment task that engages students in both software obfuscation and reverse engineering of obfuscated code. Students involved in the activities gain an appreciation of the challenges in defending against attacks. Our results demonstrate a positive change in the students' perception during the learning process.<br/> &copy; 2018 Association for Computing Machinery.},
key = {Students},
keywords = {Competition;Education computing;Network security;Reverse engineering;Teaching;},
note = {Active Learning;Contributing Student Pedagogy;Cyber security;Obfuscation;Peer learning;},
URL = {http://dx.doi.org/10.1145/3159450.3159489},
} 


@inproceedings{20151800814348,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mutations: How close are they to real faults?},
journal = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
author = {Gopinath, Rahul and Jensen, Carlos and Groce, Alex},
year = {2014},
pages = {189 - 200},
issn = {10719458},
address = {Naples, Italy},
abstract = {Mutation analysis is often used to compare the effectiveness of different test suites or testing techniques. One of the main assumptions underlying this technique is the Competent Programmer Hypothesis, which proposes that programs are very close to a correct version, or that the difference between current and correct code for each fault is very small. Researchers have assumed on the basis of the Competent Programmer Hypothesis that the faults produced by mutation analysis are similar to real faults. While there exists some evidence that supports this assumption, these studies are based on analysis of a limited and potentially non-representative set of programs and are hence not conclusive. In this paper, we separately investigate the characteristics of bug-fixes and other changes in a very large set of randomly selected projects using four different programming languages. Our analysis suggests that a typical fault involves about three to four tokens, and is seldom equivalent to any traditional mutation operator. We also find the most frequently occurring syntactical patterns, and identify the factors that affect the real bug-fix change distribution. Our analysis suggests that different languages have different distributions, which in turn suggests that operators optimal in one language may not be optimal for others. Moreover, our results suggest that mutation analysis stands in need of better empirical support of the connection between mutant detection and detection of actual program faults in a larger body of real programs.<br/> &copy; 2014 IEEE.},
key = {Software testing},
keywords = {Computer systems programming;Software reliability;Testing;},
note = {Bug fixes;Correct code;Different distributions;Mutation analysis;Mutation operators;Test suites;Testing technique;Typical faults;},
URL = {http://dx.doi.org/10.1109/ISSRE.2014.40},
} 


@article{20182305273608,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Confidentiality breach through acoustic side-channel in cyber-physical additive manufacturing systems},
journal = {ACM Transactions on Cyber-Physical Systems},
author = {Chhetri, Sujit Rokka and Canedo, Arquimedes and Faruque, Mohammad Abdullah Al},
volume = {2},
number = {1},
year = {2018},
issn = {2378962X},
abstract = {In cyber-physical systems, due to the tight integration of the computational, communication, and physical components, most of the information in the cyber-domain manifests in terms of physical actions (such as motion, temperature change, etc.). This leads to the system being prone to physical-to-cyber domain attacks that affect the confdentiality. Physical actions are governed by energy flows, which may be observed. Some of these observable energy flows unintentionally leak information about the cyber-domain and hence are known as the side-channels. Side-channels such as acoustic, thermal, and power allow attackers to acquire the information without actually leveraging the vulnerability of the algorithms implemented in the system. As a case study, we have taken cyber-physical additive manufacturing systems (fused deposition modelingbased three-dimensional (3D) printer) to demonstrate how the acoustic side-channel can be used to breach the confdentiality of the system. In 3D printers, geometry, process, and machine information are the intellectual properties, which are stored in the cyber domain (G-code). We have designed an attack model that consists of digital signal processing, machine-learning algorithms, and context-based post processing to steal the intellectual property in the form of geometry details by reconstructing the G-code and thus the test objects. We have successfully reconstructed various test objects with an average axis prediction accuracy of 86% and an average length prediction error of 11.11%.<br/> &copy; 2018 ACM.},
key = {Side channel attack},
keywords = {3D printers;Computer crime;Cyber Physical System;Digital signal processing;Embedded systems;Learning algorithms;Learning systems;Printing presses;},
note = {Physical action;Physical components;Prediction accuracy;Prediction errors;Side-channel;Temperature changes;Threedimensional (3-d);Tight integrations;},
URL = {http://dx.doi.org/10.1145/3078622},
} 


@inproceedings{20151200653040,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automatic identification of important clones for refactoring and tracking},
journal = {Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014},
author = {Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.},
year = {2014},
pages = {11 - 20},
address = {Victoria, BC, Canada},
abstract = {Code cloning is a controversial software engineering practice due to contradictory claims regarding its impacts on software evolution and maintenance. While a number of studies identify some positive aspects of code clones, there is strong empirical evidence of some negative impacts of clones too. Focusing on the issues related to clones researchers suggest to manage code clones through detection, refactoring, and tracking. However, all clones in a software system are not suitable for refactoring or tracking. Thus, it is important to identify which clones we should consider for refactoring and which clones should be considered for tracking. In this research work we apply the concept of evolutionary coupling to identify clones that are important for refactoring or tracking. By mining software evolution history, we determine and analyze constrained association rules of clone fragments that evolved following a particular change pattern called Similarity Preserving Change Pattern and are important from the perspective of refactoring and tracking. According to our investigation with rigorous manual analysis on thousands of revisions of six diverse subject systems covering two programming languages, overall 13.20% of all clones in a software system are important candidates for refactoring, and overall 10.27% of all clones are important candidates for tracking. Our implemented system can automatically identify these important candidates and thus, can help us in better maintenance of code clones in terms of refactoring and tracking.<br/> &copy; 2014 IEEE.},
key = {Cloning},
keywords = {Association rules;Automation;Codes (symbols);Computer software;Engineering research;},
note = {Automatic identification;Code clone;Mining software;Refactorings;Similarity preserving;Software engineering practices;Software evolution and maintenances;Software systems;},
URL = {http://dx.doi.org/10.1109/SCAM.2014.11},
} 


@inproceedings{20125115821847,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ModChecker: Kernel module integrity checking in the cloud environment},
journal = {Proceedings of the International Conference on Parallel Processing Workshops},
author = {Ahmed, Irfan and Zoranic, Aleksandar and Javaid, Salman and Richard III, Golden G.},
year = {2012},
pages = {306 - 313},
issn = {15302016},
address = {Pittsburgh, PA, United states},
abstract = {Kernel modules are an integral part of most operating systems (OS) as they provide flexible ways of adding new functionalities (such as file system or hardware support) to the kernel without the need to recompile or reload the entire kernel. Aside from providing an interface between the user and the hardware, these modules maintain system security and reliability. Malicious kernel level exploits (e.g. code injections) provide a gateway to a system's privileged level where the attacker has access to an entire system. Such attacks may be detected by performing code integrity checks. Several commodity operating systems (such as Linux variants and MS Windows) maintain signatures of different pieces of kernel code in a database for code integrity checking purposes. However, it quickly becomes cumbersome and time consuming to maintain a database of legitimate dynamic changes in the code, such as regular module updates. In this paper we present Mod Checker, which checks in-memory kernel modules' code integrity in real time without maintaining a database of hashes. Our solution applies to virtual environments that have multiple virtual machines (VMs) running the same version of the operating system, an environment commonly found in large cloud servers. Mod Checker compares kernel module among a pool of VMs within a cloud. We thoroughly evaluate the effectiveness and runtime performance of Mod Checker and conclude that Mod Checker is able to detect any change in a kernel module's headers and executable content with minimal or no impact on the guest operating systems' performance. &copy; 2012 IEEE.<br/>},
key = {Linux},
keywords = {Cloud computing;Codes (symbols);Computer hardware;Database systems;Hardware;Malware;Network security;Virtual machine;Virtual reality;},
note = {Cloud environments;Code integrity;Commodity operating systems;Guest operating systems;Hardware supports;Integrity checking;Kernel modules;Run-time performance;},
URL = {http://dx.doi.org/10.1109/ICPPW.2012.46},
} 


@inproceedings{20164803065526,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Design of in-service repair welding procedures for operating pipelines: Critical assessment of variables affecting restraint level and heat-affected zone microstructures of vintage pipelines},
journal = {Proceedings of the Biennial International Pipeline Conference, IPC},
author = {Guest, Stuart and Dyck, Jason and Egbewande, Afolabi and MacKenzie, Robert and Sadowski, Mark},
volume = {3},
year = {2016},
pages = {Pipeline Division - },
address = {Calgary, AB, Canada},
abstract = {In order to maintain pipeline operation during repair and maintenance work, operators typically install branch (i.e. hot-tap) and repair fittings (i.e. sleeves) onto flowing pipelines. In-service welding procedures must be designed for these installations per code requirement. Welding induced cracking during the installation of pressure containing repair fittings is a major concern when welding onto flowing pipelines. Repair fitting dimensions influence cooling rates and restraint conditions. A combination of high stress and brittle microstructures formed during the rapid cooling of high carbon equivalent vintage pipeline steel can create conditions that promote the formation of cracks. CSA Z662 and API 1104 specify essential variables (requirements) that aim to mitigate risk of cracking by qualifying the weld procedure to equal or more severe conditions than expected in the field. These essential variables can include material carbon equivalent, cooling rate, and level of restraint limitations to be applied during qualification of the weld procedure. This paper will focus on the creation of a safe welding procedure by prewelding assessment of the phase transformations that occur during welding on liquid product vintage pipelines and modelling the influence of readily quantifiable variables on the level of restraint induced by repair fittings. Finite element analysis (FEA) was utilized to study the thermal history of simulated in-service weld heat affected zones to approximate the stress and strain magnitudes (level of restraint) at the fillet weld toe of simulated sleeve repairs. Thermal analysis was conducted on various weld bead geometries to simulate the effects of cooling rates and tempering. To aid in the design of a safe weld procedure, two continuous cooling transformation (CCT) diagrams were constructed from a vintage 1960s API 5L X52 pipe with a carbon equivalent of 0.51% (CEN and IIW). This enabled the selection of optimal welding parameters that produced desirable HAZ microstructures. The modeling of restraint level accounted for the thermal expansion and contraction of a multipass fillet weld sequence on various pipe and sleeve thicknesses. The sleeve-on-pipe configuration was compared to the plate-on-plate configuration. Sleeve wall thickness was varied from 1 to 7 times the pipe wall thickness to account for any possible instances where a very thick fitting, such as emergency fittings (e.g. STOPPLE), may be installed on a thin pipeline. Test welds were completed on the 1960s vintage pipeline steel with a high volume water flow loop to simulate operating conditions. The heat affected zone hardness values correlated well with those predicted by the FEA and CCT results.<br/> &copy; Copyright 2016 by ASME.},
key = {Heat affected zone},
keywords = {Cooling;Corrosion;Cracks;Finite element method;Flow of water;Microstructure;Pipe fittings;Pipelines;Piping systems;Repair;Steel pipe;Thermal expansion;Thermoanalysis;Welding;},
note = {Continuous cooling transformation;Critical assessment;Expansion and contraction;Pipe wall thickness;Plate configuration;Repair and maintenance works;Restraint conditions;Weld heat-affected zone;},
URL = {http://dx.doi.org/10.1115/IPC201664206},
} 


@article{20152300908072,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Online structural health monitoring and parameter estimation for vibrating active cantilever beams using low-priced microcontrollers},
journal = {Shock and Vibration},
author = {Takacs, Gergely and Vachalek, Jan and Rohal'-Ilkiv, Boris},
volume = {2015},
year = {2015},
issn = {10709622},
abstract = {This paper presents a structural health monitoring and parameter estimation system for vibrating active cantilever beams using low-cost embedded computing hardware. The actuator input and the measured position are used in an augmented nonlinear model to observe the dynamic states and parameters of the beam by the continuous-discrete extended Kalman filter (EKF). The presence of undesirable structural change is detected by variations of the first resonance estimate computed from the observed equivalent mass, stiffness, damping, and voltage-force conversion coefficients. A fault signal is generated upon its departure from a predetermined nominal tolerance band. The algorithm is implemented using automatically generated and deployed machine code on an electronics prototyping platform, featuring an economically feasible 8-bit microcontroller unit (MCU). The validation experiments demonstrate the viability of the proposed system to detect sudden or gradual mechanical changes in real-time, while the functionality on low-cost miniaturized hardware suggests a strong potential for mass-production and structural integration. The modest computing power of the microcontroller and automated code generation designates the proposed system only for very flexible structures, with a first dominant resonant frequency under 4 Hz; however, a code-optimized version certainly allows much stiffer structures or more complicated models on the same hardware.<br/> &copy; 2015 Gergely Tak&aacute;cs et al.},
key = {Parameter estimation},
keywords = {Cantilever beams;Codes (symbols);Controllers;Extended Kalman filters;Flexible structures;Hardware;Microcontrollers;Nanocantilevers;Natural frequencies;Structural health monitoring;},
note = {8-bit microcontrollers;Automated code generation;Automatically generated;Continuous-discrete Extended Kalman filters;Embedded computing;Non-linear model;Prototyping platform;Structural integration;},
URL = {http://dx.doi.org/10.1155/2015/506430},
} 


@article{20180704791321,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Experimental and crash-worthiness optimization of end-capped conical tubes under quasi-static and dynamic loading},
journal = {Mechanics of Advanced Materials and Structures},
author = {Rahi, Abbas},
year = {2018},
pages = {1 - 10},
issn = {15376494},
abstract = {In the present study, the energy absorption capacity of thin-walled end-capped conical geometries is taken into consideration and the collapse of the absorbers under different loading types and strengths is investigated. First, the manual spinning method is utilized in order to manufacture the specimen. The manufacturing geometry quality of the parts is then evaluated. Next, quasi-static load-deflection tests are employed to investigate the collapse process as well as the calculation of energy absorption for conical tubes. Drop experiments are carried out using a free flight drop tower on the conical tubes to obtain the acceleration time-history of the hammer. The time history of hammer velocity change during its collision with the energy absorber, the mean collapse load of the absorber and absorbed energy are calculated. The explicit FE code Abaqus/explicit is employed and validated using dynamic experimental data. Finally, a multi-objective optimization method is used to find the tube geometry which has the maximum energy absorption and specific energy absorption. The results show that the impactor's velocity, as well as the geometrical characteristics of the end-capped conical tubes, have significant effects on the energy absorption and specific energy absorption capacity.<br/> &copy; 2018 Taylor & Francis Group, LLC},
key = {Energy absorption},
keywords = {ABAQUS;Dynamic loads;Free flight;Geometry;Hammers;Manufacture;Multiobjective optimization;Optimization;Thin walled structures;Tubes (components);},
note = {Acceleration-time history;collapse;Energy absorbers;Energy absorption capacity;Geometrical characteristics;impact;Quasi-static loads;Specific energy absorption;},
URL = {http://dx.doi.org/10.1080/15376494.2018.1432815},
} 


@article{20160301826173,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mathematical modeling of steel heat treating using CCT diagrams},
journal = {Materials Performance and Characterization},
author = {Hernandez-Morales, B. and Tellez-Martinez, J.S. and Duenas-Perez, A.M. and Diaz-Cruz, M.},
volume = {1},
number = {1},
year = {2012},
pages = {1 - 17},
issn = {21653992},
abstract = {Mathematical modeling is a powerful tool to design, control and optimize heat-treating processes. However, the complex interactions occurring between the thermal, microstructural, and stress fields inside the part during those processes (which must be taken into account in detailed modeling work) precludes its use in many instances-especially in a production environment. Thus, it is desirable to find methodologies that can speed up the simulations while maintaining the mathematical model close to reality. In this work, the evolution of the microstructural field was estimated from fraction-transformed-temperature correlations derived directly from a published continuous-cooling-transformation (CCT) diagram, which uses the cooling rate at 750&deg;C as the x-axis. This approach "softens" the coupling between the thermal and fraction-transformed fields resulting in an efficient algorithm. The thermal field evolution was computed using standard procedures embedded in the commercially available code Abaqus, whereas empirical equations describing the fraction transformed-temperature relationships were programmed through user subroutines. The mathematical model was validated by comparing measured and model-predicted thermal response and final microstructure. In the experiments, austenitized AISI 4140 steel cylindrical probes (0.5-in. diameter &times; 2-in. length) were cooled in: (1) still air, and (2) a fluidized bed reactor, both at room temperature. The thermal response was measured during the cooling process by inserting two thermocouples: one at the geometrical center of the probe and the other near the probe surface, at mid-length. The latter was input to a code developed in-house to estimate the surface heat flux history, which constitutes the active boundary condition for the direct heat conduction problem and was, in turn, fed to the computational model. Once heat treated, the probes were prepared for metallographic observation using standard techniques. The results indicate that the proposed methodology can be used for predicting the thermo-microstructural evolution during a heat-treating process.<br/> &copy; 2012 by ASTM International.},
key = {Heating},
keywords = {Chemical reactors;Cooling;Estimation;Finite element method;Fluid catalytic cracking;Fluidized beds;Heat conduction;Heat flux;Probes;Quenching;Steel heat treatment;Thermocouples;},
note = {Continuous cooling transformation;Coupled;Heat conduction problems;Metallographic observations;Multi-physics;Numerical;Production environments;Temperature correlation;},
URL = {http://dx.doi.org/10.1520/MPC104392},
} 


@inproceedings{20140717303553,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Filtering noise in mixed-purpose fixing commits to improve defect prediction and localization},
journal = {2013 IEEE 24th International Symposium on Software Reliability Engineering, ISSRE 2013},
author = {Nguyen, Hoan Anh and Nguyen, Anh Tuan and Nguyen, Tien N.},
year = {2013},
pages = {138 - 147},
address = {Pasadena, CA, United states},
abstract = {In open-source software projects, during fixing software faults, developers sometimes also perform other types of non-fixing code changes such as functionality enhancement, code restructuring/improving, or documentation. They commit non-fixing changes together with the fixing ones in the same transaction. We call them mixed-purpose fixing commits (MFCs). We have conducted an empirical study on MFCs in several popular open-source projects. Our results showed that MFCs are about 11%-39% of total fixing commits. In 3%-41% of MFCs, developers performed other change types without indicating them in the commit logs. Our study also showed that mining software repositories (MSR) approaches that rely on the recovery of the history of fixed/buggy files are affected by the noisy data where non-fixing changes in MFCs are considered as fixing ones. The results of our study motivated us to develop Cardo, a tool to identify MFCs and filter non-fixing changed files in the change sets of the fixing commits. It uses natural language processing to analyze the sentences in commit logs and program analysis to cluster the changes in the change sets to determine if a changed file is for non-fixing. Our empirical evaluation on several open-source projects showed that Cardo achieves on average 93% precision, and existing MSR approaches can be relatively improved up to 32% with data filtered by Cardo. &copy; 2013 IEEE.<br/>},
key = {Open source software},
keywords = {Filtration;Natural language processing systems;Open systems;Software reliability;},
note = {Code restructuring;Defect prediction;Empirical evaluations;Empirical studies;Mining software repository (MSR);Open source projects;Open source software projects;Program analysis;},
URL = {http://dx.doi.org/10.1109/ISSRE.2013.6698913},
} 


@inproceedings{20150900581454,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Bridging the gap between ABET outcomes and industry expectations - A case study on software engineering course},
journal = {Proceedings of the 2014 IEEE International Conference on MOOCs, Innovation and Technology in Education, IEEE MITE 2014},
author = {Seshagiri, Saradhi and Goteti, Lns Prakash},
year = {2014},
pages = {210 - 214},
address = {Patiala, India},
abstract = {The role of ABET accreditation system is quite significant in providing guidance towards program and course design. The program outcomes encompassing through knowledge, skill and attitude play an important role towards competency development of the students. To improve the employability quotient of these fresh graduates, the alignment of respective outcomes along with the base lined expectations of IT industry is needed. In this context Software Engineering (SE) plays an important role in the transformation journey of the graduates to become entry level developers. It also helps to bridge the gap between academia and IT industry so that these new hires are productive as soon as possible. In this paper, the expectations of an IT industry from the new hires is described in conjunction with ABET educational outcomes highlighting pedagogical aspects that enable students develop these abilities. SE course is used as vehicle and an approach is presented integrating software code of ethics (recommended by ACM - IEEE), SE principles, pedagogical aspects and assessment instruments. Subsequently experiences from our corporate learning environment are highlighted.<br/> &copy; 2014 IEEE.},
key = {Students},
keywords = {Computer aided instruction;Curricula;Engineering education;Software engineering;Teaching;},
note = {Assessment instruments;Bloom's taxonomy;Competency development;Learning environments;Outcome-based education;Pedagogical aspects;Pedagogy;Software engineering course;},
URL = {http://dx.doi.org/10.1109/MITE.2014.7020273},
} 


@inproceedings{20130615994486,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A coding style-based plagiarism detection},
journal = {2012 International Conference on Interactive Mobile and Computer Aided Learning, IMCL 2012},
author = {Arabyarmohamady, S. and Moradi, H. and Asadpour, M.},
year = {2012},
pages = {180 - 186},
address = {Amman, Jordan},
abstract = {In this paper a plagiarism detection framework is proposed based on coding style. Furthermore, the typical style-based approach is improved to better detect plagiarism in programming codes. The plagiarism detection is performed in two phases: in the first phase the main features representing a coding style are extracted. In the second phase the extracted features are used in three different modules to detect the plagiarized codes and to determine the giver and takers of the codes. The extracted features for each code developer are kept in a history log, i.e. a user profile as his/her style of coding, and would be used to determine the change in coding style. The user profile allows the system to detect if a code is truly developed by the claimed developer or it is written by another person, having another style. Furthermore, the user profile allows determining the code giver and code taker when two codes are similar by comparing the codes' styles with the style of the programmers. Also if a code is copied from the internet or developed by a third party, then the style of who claims the ownership of the code is normally less proficient in coding than the third party and can be detected. The difference between the style levels is done through the style level checker module in the proposed framework. The proposed framework has been implemented and tested and the results are compared to Moss which shows comparable performance in detecting plagiarized codes. &copy; 2012 IEEE.<br/>},
key = {Codes (symbols)},
keywords = {Computer aided instruction;Computer programming;Feature extraction;Intellectual property;},
note = {Author identification;Code developers;Plagiarism detection;Programming codes;Second phase;Software forensics;Source codes;Third parties;},
URL = {http://dx.doi.org/10.1109/IMCL.2012.6396471},
} 


@article{20173504083885,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Time-dependent damage characteristics of steel-concrete composite structures under rarely occurred earthquake},
journal = {Dongnan Daxue Xuebao (Ziran Kexue Ban)/Journal of Southeast University (Natural Science Edition)},
author = {Wang, Ying and Wang, Ruozhu and Li, Zhaoxia},
volume = {47},
number = {4},
year = {2017},
pages = {766 - 775},
issn = {10010505},
abstract = {For steel-concrete composite structure, the dynamic elastic-plastic time history analysis method is utilized to obtain the structural dynamic response under the rarely occurred earthquake. The seismic damage time-dependent model, which integrates global method and weighting combination method, respectively, is utilized to study the structural damage time-dependent characteristics under the rarely occurred earthquake. Considering that the parameters such as the stiffness and the damping are constants in the calculation but have the variable characteristics in the whole service period, they are treated as pseudo variables. A typical steel-concrete composite sample is studied. The results show that the damage has significantly time-dependent characteristics under rare earthquake and maybe exceeds the performance limits specified in the code. The stiffness change can cause the changes of lateral resistance force and earthquake conduction ability, thus leading to the significant fluctuations of the time-dependent damage. The influence of lateral resistance force on damage plays a dominant role. The more severe the earthquake, the greater the effect of earthquake conduction ability on damage. This can lead to more significant fluctuation of damage and vibration shapes. Comparing the time-dependent damage values and the design values without considering material time-dependent characteristics, it can be found that the material time-dependent characteristics have an important effect on the accuracy of the seismic design and evaluation.<br/> &copy; 2017, Editorial Department of Journal of Southeast University. All right reserved.},
key = {Earthquakes},
keywords = {Concretes;Dynamic response;Elastoplasticity;Rare earths;Seismic design;Stiffness;Structural dynamics;},
note = {Design and evaluations;Dynamic elastic-plastic time history analysis;Lateral resistance;Rarely occurred earthquakes;Steel concrete composite structures;Steel-concrete composite;Time-dependent characteristics;Time-dependent damages;},
URL = {http://dx.doi.org/10.3969/j.issn.1001-0505.2017.04.022},
} 


@article{20142917941227,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Optimal power allocation in DS-CDMA with adaptive SIC technique},
journal = {Telecommunication Systems},
author = {Maity, Santi P. and Hati, Sumanta and Maji, Chinmay},
volume = {56},
number = {3},
year = {2014},
pages = {335 - 346},
issn = {10184864},
abstract = {This paper proposes an optimal power allocation in direct sequence-code division multiple access (DS-CDMA) system. The objective is to minimize total transmit power, while simultaneously meeting the certain sum channel capacity (data transmission rate) and outage probability constraints on Rayleigh fading channel. Then a weighted correlator with an adaptive successive interference cancelation (SIC) scheme is developed using neural network (NN) for an improvement in receiver performance. A closed mathematical form of joint probability of error (JPOE) is derived. This determines the number of active users' interfering effect that needs to be canceled in order to achieve a desired bit error rate (BER) value. Mathematical analysis shows that better receiver performance can be achieved through large change in weight up-gradation (w) for the strong users with a particular change in learning rate (&eta;). Simulation results in terms of sum capacity as well as weak user's (users with poor channel gain) capacity, outage probability and BER performance duly support the effectiveness of the proposed scheme over the existing works. &copy; 2013 Springer Science+Business Media New York.<br/>},
key = {Code division multiple access},
keywords = {Bit error rate;Cellular radio systems;Error statistics;Fading channels;Neural networks;Optimal systems;Outages;Probability;Rayleigh fading;Turbo codes;},
note = {Data transmission rates;Direct sequence code division multiple access systems;DS-CDMA;JPOE;Optimal power allocation;Outage probability constraints;Power allocations;Successive interference cancelations;},
URL = {http://dx.doi.org/10.1007/s11235-013-9847-2},
} 


@article{20124315599724,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Multi-point seismic motions based on focal mechanism and considering local site multi-layer soil effect: Theory and program implementation},
journal = {Jisuan Lixue Xuebao/Chinese Journal of Computational Mechanics},
author = {Liu, Guo-Huan and Lu, Xin-Zheng and Guo, Wei},
volume = {29},
number = {4},
year = {2012},
pages = {582 - 588},
issn = {10074708},
abstract = {In this paper, a framework for generating multi-point earthquake motion of target filed in China, basing on focal mechanism and taking account of the spatial variability of soil properties, is given and proposed. The main contents include: (1) the transfer function of site including multiple soil layers is es-tablished based on random vibration theory, (2) based on the improved bedrock spectrum applicable to the target site in China, the variability among different site condition is reflected by establishing the transfer function of local site, (3) non-flat factor of site surface is considered on the basis of previous literature, (4) explicit expression phase-angle change by the filtration of multiple soil layers is also given for the convenience of the subsequent program code. Then, according to the theory framework, the visual program MEMS V2011.6 (Multi-support Earthquake Motions Simulation Version 2011.6) is developed and run successfully. The specific functions of the program involves the focal parameters assignment, inputting soil parameters, calculating and displaying transfer function of site soil with multi-layer, adjusting non-stationary parameters and frequency, generating multi-point earthquake motion histories, verifying the coherence of spatial seismic motions, fitting code-specified spectrum for bridge, building and electronic facility. In addition, based on a bridge example, the multi-point seismic motions of the target field are generated using the program MEMS V2011.6, and the sensibility analysis of the generated ground motions to local site effect and epicentral distance is analyzed as well. The content in this paper involve theory and practicability, and can provide directly reference for engineering.<br/>},
key = {Earthquakes},
keywords = {Codes (symbols);Plasma diagnostics;Soils;Transfer functions;},
note = {Beck-rock spectrums;Code-specified response spectrums;Local soil sites;Multi-point earthquake motions;Nonstationary;},
} 


@inproceedings{20121414917032,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An adaptive SIC technique in DS-CDMA using neural network},
journal = {Procedia Engineering},
author = {Maity, Santi P. and Hati, Sumanta},
volume = {30},
year = {2012},
pages = {1056 - 1063},
issn = {18777058},
address = {Coimbatore, India},
abstract = {In this paper, we propose an efficient successive interference cancellation (SIC) method in direct sequence code division multiple access (DS-CDMA) system using neural network (NN). Neural network is used here to estimate the amplitudes of different users signals under frequency selective Rayleigh fading channel. The correlation values between the received signal and the signature /spreading waveforms for different users are given as the inputs to a NN and the output acts as an estimation of corresponding user's signal amplitude. A closed mathematical form of joint probability of error (JPOE) is developed to determine the number of active users needs to be canceled to achieve a desired bit error rate (BER) value. Simulation results strongly support the mathematical results. Mathematical analysis shows that better performance results can be achieved through large change in weight up-gradation (w) for the strong users with a particular change in learning rate (&eta;). Performance of the SIC system has been studied for initial wrong ordering of a couple of pairs of interfering users based on the correlation values. Simulation results show that BER performance is better when users are ordered based on signal to interference ratio (SIR) values rather than correlation values.<br/>},
key = {Code division multiple access},
keywords = {Bit error rate;Cellular radio systems;Error statistics;Estimation;Fading channels;Frequency selective fading;Multiple access interference;Neural networks;Rayleigh fading;Systems analysis;Turbo codes;},
note = {Direct sequence code division multiple access systems;DS-CDMA;JPOE;Mathematical analysis;Neural network (nn);Number of active users;Signal to interference ratios (SIR);Successive interference cancellation(SIC);},
URL = {http://dx.doi.org/10.1016/j.proeng.2012.01.963},
} 


@inproceedings{20180704797373,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Video quality assessment based on the improved LSTM model},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Bao, Qiuxia and Huang, Ruochen and Wei, Xin},
volume = {10667 LNCS},
year = {2017},
pages = {313 - 324},
issn = {03029743},
address = {Shanghai, China},
abstract = {With the development of computer and network technologies, video service and content will continue to dominate while comparing to all other applications. It is particularly important to build a real-time and effective video quality assessment system. Video content, network status, viewing environment and so on will affect the end user quality of experience (QoE). In this work, we evaluate the prediction accuracy and code running time of Support Vector Machine (SVM), Decision Tree (DT) and Long Short Term Memory (LSTM). Moreover, we try to further improve the prediction accuracy from two aspects. One is to introduce some new input features to change the characteristic parameters. The other is to combine the LSTM with traditional machine learning algorithms. Experimental results show that the QoE prediction accuracy can be improved with the increased characteristic parameters. It is worth mentioning that the prediction accuracy can be increased by 8% with the improved LSTM algorithm.<br/> &copy; Springer International Publishing AG 2017.},
key = {Long short-term memory},
keywords = {Brain;Decision trees;Forecasting;Learning algorithms;Quality of service;Support vector machines;},
note = {Computer and networks;Feature preprocessing;Input features;Network status;Prediction accuracy;Quality of experience (QoE);Video quality assessment;Video services;},
URL = {http://dx.doi.org/10.1007/978-3-319-71589-6_28},
} 


@inproceedings{20124915745738,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Multimodal detection of salient behaviors of Approach-Avoidance in dyadic interactions},
journal = {ICMI'12 - Proceedings of the ACM International Conference on Multimodal Interaction},
author = {Xiao, Bo and Georgiou, Panayiotis G. and Baucom, Brian and Narayanan, Shrikanth S.},
year = {2012},
pages = {141 - 144},
address = {Santa Monica, CA, United states},
abstract = {Approach-Avoidance (AA) coding is a measure of involvement and immediacy in human dyadic interactions. We focus on analyzing the salient events in interactions that trigger change points in AA code in time, as perceived by domain experts. We employ coarse level visual cues associated with body parts, as well as vocal energy features. Motion vector extraction and body pose estimation techniques are used for extracting visual cues. Functionals of these cues are used as features for SVM based machine learning experiments. We found that the coder's judgments on salient events are related to the short time interval preceding the labeling. We also show that visual cues are the main information source for decision making on salient AA events, and that considering the information from a subset of body parts provides the same information as considering the full set. The mean of absolute value and standard deviation of motion streams are the most effective functionals as feature. We achieve an F-score of 0.55 in detecting salient events using cross-validation with a one-subject-out approach. Copyright 2012 ACM.<br/>},
key = {Modal analysis},
keywords = {Decision making;Interactive computer systems;Learning systems;},
note = {Approach-avoidance;Motion Vectors;Multimodal features;Pose estimation;Salience;},
URL = {http://dx.doi.org/10.1145/2388676.2388707},
} 


@inproceedings{20130716008258,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Selection and ranking of optimal routes through genetic algorithm in a cognitive routing system for mobile ad-hoc network},
journal = {Proceedings - 2012 5th International Symposium on Computational Intelligence and Design, ISCID 2012},
author = {Afridi, Muhammad Ishaq},
volume = {1},
year = {2012},
pages = {507 - 510},
address = {Hangzhou, China},
abstract = {Genetic algorithm can be used for proper selection and ranking of all possible variable route addresses in mobile ad-hoc network (MANET). Ranking is based upon priority code of the links. A priority code is calculated by respective routing protocol, which depends on different parameters and metrics. A node can change its position and new nodes may join the MANET, so genetic algorithm can better estimate such kind of variations through its crossover and mutation genetic operators. Genetic algorithm is especially useful in cases of novel cognitive routing for MANET. Cognition in MANET is either based upon learning automata method as in some wireless sensor networks or specialized cognitive neural networks. Ranking of optimal links in MANET after a regular interval through genetic algorithm enhance the performance of cognitive routing. It help in proper selection of desired routing protocol for a given set of network conditions. &copy; 2012 IEEE.<br/>},
key = {Mobile ad hoc networks},
keywords = {Artificial intelligence;Automata theory;Genetic algorithms;Internet protocols;Mobile telecommunication systems;Neural networks;Routing algorithms;Routing protocols;Sensor nodes;},
note = {Cognitive routing;Crossover and mutation;Genetic operators;Hybrid routing;Learning Automata;Network condition;Optimal routes;Proactive protocol;},
URL = {http://dx.doi.org/10.1109/ISCID.2012.132},
} 


@inproceedings{20140117155380,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Better backtracking support for programmers},
journal = {Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},
author = {Yoon, YoungSeok},
year = {2013},
pages = {187 - 188},
issn = {19436092},
address = {San Jose, CA, United states},
abstract = {Programmers often need to backtrack while coding, yet there is only limited support for backtracking in modern programming tools. Our study results confirmed the prevalence of backtracking and identified several problems programmers face while backtracking. To mitigate these problems, we are building an IDE plug-in aimed at providing better support for backtracking by combining a selective undo mechanism, novel visualizations, and code change history search features. We envision that this approach will help programmers perform backtracking tasks more easily. &copy; 2013 IEEE.<br/>},
key = {Visual languages},
keywords = {Visualization;},
note = {backtracking;Code changes;Integrated development environment;Novel visualizations;Programming tools;Search features;selective undo;Software visualization;},
URL = {http://dx.doi.org/10.1109/VLHCC.2013.6645278},
} 


@article{20143600043322,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Variation factors in the design and analysis of replicated controlled experiments: Three (dis)similar studies on inspections versus unit testing},
journal = {Empirical Software Engineering},
author = {Runeson, Per and Stefik, Andreas and Andrews, Anneliese},
volume = {19},
number = {6},
year = {2014},
pages = {1781 - 1808},
issn = {13823256},
abstract = {In formal experiments on software engineering, the number of factors that may impact an outcome is very high. Some factors are controlled and change by design, while others are are either unforeseen or due to chance. This paper aims to explore how context factors change in a series of formal experiments and to identify implications for experimentation and replication practices to enable learning from experimentation. We analyze three experiments on code inspections and structural unit testing. The first two experiments use the same experimental design and instrumentation (replication), while the third, conducted by different researchers, replaces the programs and adapts defect detection methods accordingly (reproduction). Experimental procedures and location also differ between the experiments. Contrary to expectations, there are significant differences between the original experiment and the replication, as well as compared to the reproduction. Some of the differences are due to factors other than the ones designed to vary between experiments, indicating the sensitivity to context factors in software engineering experimentation. In aggregate, the analysis indicates that reducing the complexity of software engineering experiments should be considered by researchers who want to obtain reliable and repeatable empirical measures.<br/> &copy; 2013, Springer Science+Business Media New York.},
key = {Design of experiments},
keywords = {Inspection;Software engineering;},
note = {Code inspections;Experiment design;Formal Experiments;Replication;Reproduction;Unit testing;},
URL = {http://dx.doi.org/10.1007/s10664-013-9262-z},
} 


@inproceedings{20165103159247,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Effect of loading modes on the limit behaviors for pipe bends under combined loads},
journal = {American Society of Mechanical Engineers, Pressure Vessels and Piping Division (Publication) PVP},
author = {Li, Jian and Zhou, Changyu and He, Xiaohua},
volume = {5},
year = {2016},
pages = {Pressure Vessels and Piping Division - },
issn = {0277027X},
address = {Vancouver, BC, Canada},
abstract = {Pipe bends are widely used in industrial piping and pipelines. Because of their flexibility properties, they can accommodate thermal expansions and absorb other externally induced loads. Under severe loading conditions, bends exhibit significant cross-section ovalization, associated with strains well beyond the elastic limit and fail because of excessive cross-section ovalization. The past references provide several studies on the ovalization induced loading capacity change. The main aim of the study is to establish the effects of load history on the calculated failure load, when a bend is subjected to a combination loading of in-plane bending, torsion and internal pressure. The results of the investigation show that geometric nonlinearity is a significant consideration when calculating plastic failure loads of pipe bends subject to combined loads. The limit load is path independent of the loading sequence in the cases of bending and torsion combinations. In the cases of pressure and moment combinations the proportional and pressure-moment load cases exhibit significant geometric strengthening, whereas the moment- pressure load case exhibits a little. The results according to the ASME Boiler and Pressure Vessel Code may show overall conservative in all loading conditions.<br/> Copyright &copy; 2016 by ASME.},
key = {Failure (mechanical)},
keywords = {Nondestructive examination;Outages;Pipe joints;Pipeline bends;Plastic pipe;Pressure vessel codes;Pressure vessels;Torsional stress;},
note = {Boiler and Pressure Vessel Code;Geometric non-linearity;Industrial piping;Loading capacities;Loading condition;Loading sequence;Plastic failure loads;Pressure moments;},
URL = {http://dx.doi.org/10.1115/PVP2016-63183},
} 


@inproceedings{20160601908407,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The uniqueness of changes: Characteristics and applications},
journal = {IEEE International Working Conference on Mining Software Repositories},
author = {Ray, Baishakhi and Nagappan, Meiyappan and Bird, Christian and Nagappan, Nachiappan and Zimmermann, Thomas},
volume = {2015-August},
year = {2015},
pages = {34 - 43},
issn = {21601852},
address = {Florence, Italy},
abstract = {Changes in software development come in many forms. Some changes are frequent, idiomatic, or repetitive (e.g. Adding checks for nulls or logging important values) while others are unique. We hypothesize that unique changes are different from the more common similar (or non-unique) changes in important ways, they may require more expertise or represent code that is more complex or prone to mistakes. As such, these unique changes are worthy of study. In this paper, we present a definition of unique changes and provide a method for identifying them in software project history. Based on the results of applying our technique on the Linux kernel and two large projects at Microsoft, we present an empirical study of unique changes. We explore how prevalent unique changes are and investigate where they occur along the architecture of the project. We further investigate developers' contribution towards uniqueness of changes. We also describe potential applications of leveraging the uniqueness of change and implement two of those applications, evaluating the risk of changes based on uniqueness and providing change recommendations for non-unique changes.<br/> &copy; 2015 IEEE.},
key = {Software design},
keywords = {Buildings;Cloning;Computer software;History;Linux;Syntactics;},
note = {Context;Empirical studies;Important value;Large project;Linux kernel;MicroSoft;Software project;},
URL = {http://dx.doi.org/10.1109/MSR.2015.11},
} 


@inproceedings{20162002380581,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Cross coupled digital NAND gate comparator based flash ADC},
journal = {2015 International Conference on Communication and Signal Processing, ICCSP 2015},
author = {Kute, Parag P. and Dakhole, Pravin and Palsodkar, Prachi},
year = {2015},
pages = {1718 - 1721},
address = {Melmaruvathur, India},
abstract = {This paper demonstrates the Flash ADC which is constructed using digital 3-input cross coupled NAND gates. These cross coupled configuration of NAND gates form a comparator of flash ADC. It is latch comparator which operates on single phase clock &Phi;. The output of comparator is thermometer code. An encoder is constructed that encodes the thermometer code to binary as output of comparator. The design is simulated in 180nm technology and implement 4 bit flash ADC version. The Flash ADC is designed in TANNER S-EDIT 13.0<br/> &copy; 2015 IEEE.},
key = {Comparator circuits},
keywords = {Analog to digital conversion;Clocks;Comparators (optical);NAND circuits;Signal processing;Thermometers;},
note = {Analog to digital convertor (ADC);Clocked comparators;Cross-coupled;FLASH-ADC;NAND gate;Single phase;},
URL = {http://dx.doi.org/10.1109/ICCSP.2015.7322814},
} 


@inproceedings{20182905568617,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A novel image encoding and communication technique of B/W images for IOT, robotics and drones using (15, 11) reed solomon scheme},
journal = {2018 Advances in Science and Engineering Technology International Conferences, ASET 2018},
author = {Ejaz, Muhammad Zaid and Khurshid, Khurram and Abbas, Zeeshan and Aizaz, Muhammad Ali and Nawaz, Asif},
year = {2018},
pages = {1 - 6},
address = {Dubai, Sharjah, Abu Dhabi, United arab emirates},
abstract = {In the modern age of IOT and Robotics, different intelligent entities like robots, drones, IOT nodes or smart vehicles need fast and error-free communication of data, which is predominantly in the form of images. The medium used for communication of this data is mainly wireless and it can be short distance, medium distance, long haul or even satellite links crossing the ionosphere layers. Different wireless mediums incorporate different types of noises in the images being transmitted by Drones, Robots or IOT Nodes. For better analysis and then performing subsequent action on the basis of these received images using artificial intelligence, machine learning or machine vision, it is imperative that the images transmitted are encoded and recovered as fast and as error-free as possible. Normal conventional methods use different image correction algorithms for detection and correction of errors in images. Reed Solomon codes, which are normally used for error detection and correction at data link layer in TCP/IP protocol stack, have a high probability of signal correction and are highly efficient due to their burst error detection and correction capabilities. The RS codes can be implemented where there is a large number of input symbols and noise duration is relatively small as compared to the code word. Sometimes at the receiver end, we get images which are partially corrupted and only half or some part of them is visible. Most of the filters used for image reconstruction insert the approximated bits in place of the corrupted bits by using some algorithms but if only partial part of the image is corrupted, no filter will be able to recover the images properly as it will also change the bits in the non-corrupted part of the image. We have proposed a novel approach of using RS codes for the detection and correction of errors in the images. This novel technique can be used over a variety of applications including robotics, drones, IOT nodes, smart vehicles using wireless and satellite communication, which include the transfer of images and decision making on the basis of the content of the images.<br/> &copy; 2018 IEEE.},
key = {Image reconstruction},
keywords = {Artificial intelligence;Computer vision;Decision making;Decoding;Drones;Encoding (symbols);Image coding;Intelligent robots;Internet of things;Ionosphere;Learning systems;Reed-Solomon codes;Robotics;Satellite communication systems;Satellite links;Satellites;Signal encoding;Wireless telecommunication systems;},
note = {Communication techniques;Conventional methods;Error detection and correction;Error free communications;Reed solomon;Satellite communications;Smart vehicles;Wireless communications;},
URL = {http://dx.doi.org/10.1109/ICASET.2018.8376846},
} 


@article{20150700529690,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Two dimensional hashing for visual tracking},
journal = {Computer Vision and Image Understanding},
author = {Ma, Chao and Liu, Chuancai},
volume = {135},
year = {2015},
pages = {83 - 94},
issn = {10773142},
abstract = {Appearance model is a key part of tracking algorithms. To attain robustness, many complex appearance models are proposed to capture discriminative information of object. However, such models are difficult to maintain accurately and efficiently. In this paper, we observe that hashing techniques can be used to represent object by compact binary code which is efficient for processing. However, during tracking, online updating hash functions is still inefficient with large number of samples. To deal with this bottleneck, a novel hashing method called two dimensional hashing is proposed. In our tracker, samples and templates are hashed to binary matrices, and the hamming distance is used to measure confidence of candidate samples. In addition, the designed incremental learning model is applied to update hash functions for both adapting situation change and saving training time. Experiments on our tracker and other eight state-of-the-art trackers demonstrate that the proposed algorithm is more robust in dealing with various types of scenarios.<br/> &copy; 2015 Elsevier Inc. All rights reserved.},
key = {Hamming distance},
keywords = {Hash functions;Surface discharges;},
note = {Appearance modeling;Appearance models;Hashing;Hashing techniques;Incremental learning;Number of samples;State of the art;Tracking algorithm;},
URL = {http://dx.doi.org/10.1016/j.cviu.2015.01.003},
} 


@article{20143900075376,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Influence of different seismic shear force coefficient control methods on seismic performance of a super-tall building},
journal = {Jianzhu Jiegou Xuebao/Journal of Building Structures},
author = {Liu, Bin and Qi, Wuhui and Ye, Lieping and Lu, Xinzheng and Chen, Binlei},
volume = {35},
number = {8},
year = {2014},
pages = {1 - 7},
issn = {10006869},
abstract = {The seismic shear force coefficient is the key parameter of structural seismic design. There are clear specifications for the minimum seismic shear force coefficient in Chinese seismic design code. When this requirement cannot be satisfied, it is required to adjust the design base shear force and story shear force to meet the limitation. If the gap between the target design base shear force and the actual base shear force is too large, it is required to change the structural arrangement or the structural system. Engineering practice shows that if using the method of changing structural arrangement, the amount of construction materials will be too large for a super-tall building whose vibration period is very long and the minimum seismic shear force coefficient is hard to meet the requirement. A super-tall building using three control methods of seismic shear force coefficient was studied. The seismic performance and amount of construction materials were compared and analyzed through elastic analysis and elasto-plastic analysis. The model A, whose structural stiffness was adjusted to meet the seismic shear force coefficient limit, has the largest structural stiffness and highest cost. The model B, whose designed base shear force was adjusted to meet the minimum seismic shear force and the maximum story drift, has lower cost and smaller structural stiffness than the model A. The model C, whose designed base shear was adjusted to meet the minimum seismic shear force has lower cost and smaller structural stiffness than the model B. After large numbers of nonlinear time history analysis, it is found that all the models can meet the displacement limitations in the code. The displacement response of the model B and model C are close. The displacement response of the model A is a bit smaller than the other models.<br/>},
key = {Seismic design},
keywords = {Building materials;C (programming language);Construction;Cost benefit analysis;Costs;Elastoplasticity;Plastic building materials;Seismic waves;Seismology;Stiffness;Structural analysis;Tall buildings;},
note = {Displacement response;Elasto-plastic analysis;Engineering practices;Nonlinear time history analysis;Seismic Performance;Shear force;Structural arrangement;Super tall buildings;},
} 


@article{20150900570309,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Two-dimensional ablation and thermal response analyses for mars science laboratory heat shield},
journal = {Journal of Spacecraft and Rockets},
author = {Chen, Yih-Kanq and Gokcen, Tahir and Edquist, Karl T.},
volume = {52},
number = {1},
year = {2015},
pages = {134 - 143},
issn = {00224650},
abstract = {This paper examines transient simulations performed to predict in-depth thermal response and surface recession of the proposed heat shield material for the Mars Science Laboratory entry capsule, that is, phenolic impregnated carbon ablator. The finite volume material response code used in this paper solves the time-dependent governing equations, including energy conservation and a three-component decomposition model, with a surface energy-balance condition and a moving grid system to predict shape change due to surface recession. The predicted in-depth thermal response of heat shield material generally agrees well with the thermocouple data under various arcjet conditions. Also, two-dimensional computations using aerothermal environment for Mars entry (derived from a proposed three-sigma trajectory) are performed around the heat shield shoulder region, where high heating occurs as the result of angle of attack. Parametric studies are conducted to examine the effects of carbon-fiber orientation, material properties, and surface recession on heat shield bondline temperature history. It is proved that the fiber orientation configuration of the baseline heat shield has the lowest maximum bondline temperature.<br/> Copyright Clearance Center, Inc.},
key = {Heat shielding},
keywords = {Angle of attack;Carbon fibers;Martian surface analysis;Radiation shielding;Thermocouples;},
note = {Aerothermal environments;Governing equations;Impregnated carbons;Mars science laboratory;Material response;Surface recession;Temperature history;Transient simulation;},
URL = {http://dx.doi.org/10.2514/1.A32868},
} 


@article{20174904486122,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Engineering bond model for corroded reinforcement},
journal = {Engineering Structures},
author = {Blomfors, Mattias and Zandi, Kamyab and Lundgren, Karin and Coronelli, Dario},
volume = {156},
year = {2018},
pages = {394 - 410},
issn = {01410296},
abstract = {Corrosion of the reinforcement in concrete structures affects their structural capacity. This problem affects many existing concrete bridges and climate change is expected to worsen the situation in future. At the same time, assessment engineers lack simple and reliable calculation methods for assessing the structural capacity of structures damaged by corrosion. This paper further develops an existing model for assessing the anchorage capacity of corroded reinforcement. The new version is based on the local bond stress-slip relationships from fib Model Code 2010 and has been modified to account for corrosion. The model is verified against a database containing the results from nearly 500 bond tests and by comparison with an empirical model from the literature. The results show that the inherent scatter among bond tests is large, even within groups of similar confinement and corrosion level. Nevertheless, the assessment model that has been developed can represent the degradation of anchorage capacity due to corrosion reasonably well. This new development of the model is shown to represent the experimental data better than the previous version; it yields similar results to an empirical model in the literature. In contrast to many empirical models, the model developed here represents physical behaviour and shows the full local bond stress-slip relationship. Using this assessment model will increase the ability of professional engineers to estimate the anchorage capacity of corroded concrete structures.<br/> &copy; 2017 Elsevier Ltd},
key = {Concrete reinforcements},
keywords = {Anchorages (foundations);Bonding;Climate change;Concrete beams and girders;Concrete buildings;Concrete construction;Corrosion;Dielectric properties;Reinforced concrete;},
note = {Assessment;Assessment models;Corroded reinforcement;Corrosion levels;Empirical model;Existing concrete bridge;Professional engineer;Structural capacities;},
URL = {http://dx.doi.org/10.1016/j.engstruct.2017.11.030},
} 


@article{20172503795973,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An evolution model for elliptic-cylindrical void in viscous materials considering the evolvements of void shape and orientation},
journal = {Mechanics of Materials},
author = {Feng, Chao and Cui, Zhenshan and Shang, Xiaoqing and Liu, Mingxiang},
volume = {112},
year = {2017},
pages = {101 - 113},
issn = {01676636},
abstract = {The evolution behavior of the internal voids influences the mechanical property of the materials significantly. Considering the void deformation and rotation, the evolution behavior of the elliptic-cylindrical void in power-law viscous materials was investigated by using the representative volume element (RVE) model. The rigid visco-plastic finite element (FE) method was applied to calculate the velocity field in the RVE under different loading conditions, and the instantaneous changing rate of the void radius and orientation were determined by evaluating the evolving of the void profiles at the instant. The calculated results show the deviatoric stress takes an important role in the void radius evolution, and the shear stress influences the change of the void orientation significantly. Based on the investigation, a void evolution model was established to relate the changing rates of the void radius and orientation to the void aspect ratio and the macroscopic stress/strain conditions. This model was incorporated into the FE code to predict the evolvements of void radius and orientation in each step of the deformation history. The predictions agree well with the results of the numerical simulations containing embedded void geometries in the mesh, which demonstrates that this model is capable to evaluate the void evolution behavior under large deformation. As an application, this model was used to predict the closure behavior of the void defects in the large ingot during the hot forging process.<br/> &copy; 2017 Elsevier Ltd},
key = {Finite element method},
keywords = {Aspect ratio;Deformation;Forecasting;Forging;Ingots;Shear stress;Velocity;Volume measurement;},
note = {Deformation history;Evolution behavior;Evolution modeling;Hot forging process;Large ingot;Representative volume element (RVE);Viscous materials;Void evolution;},
URL = {http://dx.doi.org/10.1016/j.mechmat.2017.06.002},
} 


@article{20182605384300,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A volatile-by-Default JVM for server applications},
journal = {Proceedings of the ACM on Programming Languages},
author = {Liu, Lun and Millstein, Todd and Musuvathi, Madanlal},
volume = {1},
number = {OOPSLA},
year = {2017},
issn = {24751421},
abstract = {A memory consistency model (or simply memory model) defines the possible values that a shared-memory read may return in a multithreaded programming language. Choosing a memory model involves an inherent performance-programmability tradeoff. The Java language has adopted a relaxed (or weak) memory model that is designed to admit most traditional compiler optimizations and obviate the need for hardware fences on most shared-memory accesses. The downside, however, is that programmers are exposed to a complex and unintuitive semantics and must carefully declare certain variables as volatile in order to enforce program orderings that are necessary for proper behavior. This paper proposes a simpler and stronger memory model for Java through a conceptually small change: every variable has volatile semantics by default, but the language allows a programmer to tag certain variables, methods, or classes as relaxed and provides the current Java semantics for these portions of code. This volatile-by-default semantics provides sequential consistency (SC) for all programs by default. At the same time, expert programmers retain the freedom to build performance-critical libraries that violate the SC semantics. At the outset, it is unclear if the volatile-by-default semantics is practical for Java, given the cost of memory fences on today&rsquo;s hardware platforms. The core contribution of this paper is to demonstrate, through comprehensive empirical evaluation, that the volatile-by-default semantics is arguably acceptable for a predominant use case for Java today &ETH; server-side applications running on Intel x86 architectures. We present VBD-HotSpot, a modification to Oracle&rsquo;s widely used HotSpot JVM that implements the volatile-by-default semantics for x86. To our knowledge VBD-HotSpot is the first implementation of SC for Java in the context of a modern JVM. VBD-HotSpot incurs an average overhead versus the baseline HotSpot JVM of 28% for the Da Capo benchmarks, which is significant though perhaps less than commonly assumed. Further, VBD-HotSpot incurs average overheads of 12% and 19% respectively on standard benchmark suites for big-data analytics and machine learning in the widely used Spark framework.<br/> &copy; 2017 Copyright held by the owner/author(s).},
key = {Java programming language},
keywords = {Big data;Computer hardware;Fences;Hardware;Learning systems;Memory architecture;Program compilers;Semantics;},
note = {Compiler optimizations;Empirical evaluations;Java virtual machines;Memory consistency models;Multithreaded programming;Sequential consistency;Server applications;Volatile by default;},
URL = {http://dx.doi.org/10.1145/3133873},
} 


@inproceedings{20150900581449,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Impact of classification of lab assignments and problem solving approach in object oriented programming lab course: A case study},
journal = {Proceedings of the 2014 IEEE International Conference on MOOCs, Innovation and Technology in Education, IEEE MITE 2014},
author = {Rajashekharaiah, K.M.M. and Patil, Mahesh S. and Joshi, G.H.},
year = {2014},
pages = {205 - 209},
address = {Patiala, India},
abstract = {Information systems are growing in both size and complexity. Object orientation has emerged as a dominant paradigm in designing and constructing such large and complex information systems, which is being taught to undergraduate students of Computer Science and Engineering. The use of java technology to develop applications is found preferred one [1]. There exists a knowledge gap between industry expectations and students knowledge. Hence deep learning of java programming is essential attribute to reduce the gap [1]. This paper presents classification of lab assignments as demonstration, Exercises, structured Enquiry type and open ended types with an approach of using conceptual model as intermediate step before writing java code for a given problem statement. This classification of assignments/problem statements move the responsibility from a professor to student where the latter involvement is observed high in achieving the objectives set for each type in classification of assignments, hence helps to meet industry expectations. The conceptual model, drawing a class diagram using standard notations (diagrammatic representations) improves the students understanding of object oriented concepts [7], for a given lab assignment which in-turn eases the student effort in translating the class diagram into program code by following the syntax. The students will be graded during assessment as Grade-S, Grade-A, Grade-B, Grade-C, Grade-D, Grade-E, Grade-F and where Grade-S is highest and Grade-F is lowest. The results of assessment are analyzed for year 2013 and 2014 batches and it is observed that, there is increase in grade A in year 2014 compared to year 2013. The observed change is due to the above said approach. The result analysis of year 2013 and 2012 shows that, the grade F is increased little in results of year 2013, it is due to the current approach where the lab assignments are classified and students are expected to apply the concepts studied effectively and few students unable to follow the approach.<br/> &copy; 2014 IEEE.},
key = {Object oriented programming},
keywords = {Classification (of information);Deep learning;Education computing;Encoding (symbols);Engineering education;Information systems;Information use;Java programming language;Laboratories;Problem solving;Program translators;Students;},
note = {assignment;Class diagrams;Conceptual model;dominant;paradigm;shrink;},
URL = {http://dx.doi.org/10.1109/MITE.2014.7020272},
} 


@inproceedings{20181304944621,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Steamfront detection using time-lapse micro-gravity inversion},
journal = {Society of Petroleum Engineers - SPE Abu Dhabi International Petroleum Exhibition and Conference 2017},
author = {Al-Lazki, Ali and Al-Ismaili, Ali and Stammeijer, Johannes and Hamm, Mark},
volume = {2017-January},
year = {2017},
address = {Abu Dhabi, United arab emirates},
abstract = {Objectives/Scope: PDO 4D gravity inversion is aimed at detecting lateral steam chest movement within a highly fractured carbonate oil reservoir. The PDO field uses Thermally Assisted Gas-Oil Gravity Drainage (TA-GOGD) method to produce the low API oil. The shallow reservoir rests at about 300 meters below the surface. Methods/Procedures/Process: We use regularized least-squares inversion code to invert for reservoir 4D density changes between base and repeat. Base gravity survey is carried out in 2013 and two repeat surveys in 2015 and 2016. Two methods of gravity surveying were deployed in 2013, relative and absolute gravity surveys, to acquire a total of 302 points per each method. Only relative surveys were acquired for both 2015 and 2016 repeat surveys. Relative surveys used a CG5 gravimeters, while the absolute gravity survey used the A10 Micro-g LaCoste specialized instrument to measure absolute gravity at each point. In this study the calculated 4D gravity signal is based on the difference between the 2013 Absolute base survey and the Relative repeats in 2015 and 2016. Gravity survey data is known to suffer from different types of noise, the highest of which is the gravity overprint caused by shallow water aquifer water level changes on the reservoir gravity signal, and secondly, any changes in surface elevation. Using water level monitoring wells we estimated shallow aquifer signal and removed from the observed gravity data values prior to inversion. Surface elevation changes are tracked using InSAR satellite data. Also, observed gravity data is pre-conditioned by removing outliers and de-bias the data to improve inversion stability. Results/Observations/Conclusions: After the removal of the shallow aquifer gravity signal, the observed 4D gravity anomaly is found to vary between 21&mu;Gal to -35 &mu;Gal between 2016-2013, which is relatively comparable with an estimated forward 4D gravity signal calculated from history matched reservoir models for the same periods of time (2013-2015 &amp; 2013-2016). The Least Squares inversion is carried out assuming a single reservoir layer of a certain (heated) thickness, inverting the 4D gravity signal to estimate the density changes at the reservoir for the periods 2013-2015 and 2013-2016. The inversion results, with the shallow aquifer compensated, do not show dramatic difference with the non compensated inversion. Assuming a thickness of the steamed/heated zone 100m, the estimated change in density varies between -29 kg/m3 to 12 kg/m3. We find the largest negative anomaly to correspond with the steam injection area. We concludefrom this work that 4D gravity signal is detectable, which is the result from reservoir density changes caused by steam injection. A steam chest outline can be estimated by applying a density cut-off. Also, we conclude from this study that surface elevation changes error contribution in this field is negligible, and a single elevation survey combined with time-lapse, InSAR satellite elevation changes is sufficient for both base and repeat survey.<br/> Copyright 2017, Society of Petroleum Engineers.},
key = {Petroleum reservoir evaluation},
keywords = {Aquifers;Density (specific gravity);Geological surveys;Least squares approximations;Microgravity;Petroleum reservoirs;Steam;Water levels;},
note = {Gas oil gravity drainages;History-matched reservoir models;Inversion;Least-squares inversion;Regularized least squares;Surface elevation changes;Time lapse;Water level monitoring;},
} 


@inproceedings{20182305271671,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Toward enhanced forming simulation of woven fabrics using a coupled non-orthogonal hypoelastic constitutive model, integrated with a new wrinkling onset criterion},
journal = {32nd Technical Conference of the American Society for Composites 2017},
author = {Kashani, M. Haghi and Hejazi, M. and Hosseini, A. and Sassani, F. and Ko, F. and Milani, A.S.},
volume = {4},
year = {2017},
pages = {2940 - 2954},
address = {West Lafayette, IN, United states},
abstract = {One of the advantages of woven fabrics over unidirectional prepregs is their superior formability thanks to the large shear deformation capability. There exists, however, a limit on the shear deformation of woven fabrics, namely the wrinkling. Applying tension to delay wrinkling during forming processes, a consequence of the inherent coupling in woven fabrics, is widely known to the industry. Yet, inherent coupling - change in the effective material properties of a given direction of the fabric due to the applied deformation in other directions - has not been fully understood and implemented in the forming simulations of fabric reinforcements. Coupling should be incorporated in numerical optimization routines to accurately predict the deformation of the material under complex forming set-ups, and more importantly to predict a realistic yarn tension level that can suppress wrinkles. Towards this goal, the present study proposes and implements a new coupled non-orthogonal model which predicts not only the stress-strain path, but also the critical point (shear wrinkling) of the woven fabrics, under combined loading conditions similar to the draping processes. Furthermore, the study reveals that the concept of inherent coupling raises a new issue in fabric forming simulations; the load history dependency of the fabric. Accordingly, the constitutive model has been enhanced to a hypoelastic form to capture the load path dependency of the forming material. Finally, the constitutive model has been integrated with a newly developed analytical fabric instability criterion by authors to account for the occurrence of wrinkling, based on the fabric properties and the level of tension applied during forming. To show its application, the model has been implemented in ABAQUS via a UMAT code to predict the stress and strain fields as well as the onset of wrinkling under large shear deformations.<br/> &copy; 2017 by DEStech Publications, Inc.},
key = {Weaving},
keywords = {Constitutive models;Fabrics;Forecasting;Optimization;Shear deformation;Stress analysis;Textile industry;},
note = {Effective material property;Fabric reinforcement;Forming simulations;Instability criterion;Non-orthogonal model;Numerical optimizations;Textile fabric;UMAT code;},
} 


@inproceedings{20140817354483,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The performance assessment of a multi-span, box girder reinforced concrete bridge with and without seismic isolation},
journal = {Civil-Comp Proceedings},
author = {Borekci, M. and Altun, M. and Arslan, G.},
volume = {99},
year = {2012},
issn = {17593433},
address = {Dubrovnik, Croatia},
abstract = {This paper investigates earthquake performance of a multi-span, box girder R/C bridge with and without isolation. Bridges and viaducts are important structures since they must be functional immediately and remain stable after an earthquake. For this reason, seismic isolations can be used to decrease the effects of earthquake loads on bridges. Seismic isolation on a bridge elongates the period of structure, thus the inertia and the earthquake forces are decreased without a significant change in the stiffness of the structure. Among various types of seismic isolators used in engineering practice, lead rubber bearing (LRB) was used in this study. Performance assessment was applied to the un-isolated and isolated bridge to make some comparisons, and the benefits of isolation were discussed. Linear elastic method, nonlinear static method (pushover analysis) and nonlinear time history analysis, which are defined in Turkish Earthquake Code (TEC) 2007, were used for performance assessment. &copy; Civil-Comp Press, 2012.<br/>},
key = {Earthquakes},
keywords = {Bearings (structural);Box girder bridges;Bridges;Concrete bridges;Damage detection;Offshore pipelines;Reinforced concrete;},
note = {Damage;Earthquake performance;Engineering practices;Nonlinear static methods;Nonlinear time history analysis;Performance assessment;Period of structures;Seismic isolation;},
} 


@article{20134817040884,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Direct Kernel Perceptron (DKP): Ultra-fast kernel ELM-based classification with non-iterative closed-form weight calculation},
journal = {Neural Networks},
author = {Fernandez-Delgado, Manuel and Cernadas, Eva and Barro, Senen and Ribeiro, Jorge and Neves, Jose},
volume = {50},
year = {2014},
pages = {60 - 71},
issn = {08936080},
abstract = {The Direct Kernel Perceptron (DKP) (Ferna&acute;ndez-Delgado etal., 2010) is a very simple and fast kernel-based classifier, related to the Support Vector Machine (SVM) and to the Extreme Learning Machine (ELM)(Huang, Wang, &amp; Lan, 2011), whose &alpha;-coefficients are calculated directly, without any iterative training, using an analytical closed-form expression which involves only the training patterns. The DKP, which is inspired by the Direct Parallel Perceptron, (Auer etal., 2008), uses a Gaussian kernel and a linear classifier (perceptron). The weight vector of this classifier in the feature space minimizes an error measure which combines the training error and the hyperplane margin, without any tunable regularization parameter. This weight vector can be translated, using a variable change, to the &alpha;-coefficients, and both are determined without iterative calculations. We calculate solutions using several error functions, achieving the best trade-off between accuracy and efficiency with the linear function. These solutions for the &alpha; coefficients can be considered alternatives to the ELM with a new physical meaning in terms of error and margin: in fact, the linear and quadratic DKP are special cases of the two-class ELM when the regularization parameter C takes the values C = 0 and C = &infin;. The linear DKP is extremely efficient and much faster (over a vast collection of 42 benchmark and real-life data sets) than 12 very popular and accurate classifiers including SVM, Multi-Layer Perceptron, Adaboost, Random Forest and Bagging of RPART decision trees, Linear Discriminant Analysis, K-Nearest Neighbors, ELM, Probabilistic Neural Networks, Radial Basis Function neural networks and Generalized ART. Besides, despite its simplicity and extreme efficiency, DKP achieves higher accuracies than 7 out of 12 classifiers, exhibiting small differences with respect to the best ones (SVM, ELM, Adaboost and Random Forest), which are much slower. Thus, the DKP provides an easy and fast way to achieve classification accuracies which are not too far from the best one for a given problem. The C and Matlab code of DKP are freely available.<sup>1</sup>1http://www.gsi.dec.usc.es/~delgado/papers/dkp. &copy; 2013 Elsevier Ltd.<br/>},
key = {C (programming language)},
keywords = {Adaptive boosting;Decision trees;Discriminant analysis;Economic and social effects;Efficiency;Errors;Image retrieval;Iterative methods;Knowledge acquisition;MATLAB;Nearest neighbor search;Parameterization;Radial basis function networks;Support vector machines;Vector spaces;Vectors;},
note = {Closed form solutions;Extreme learning machine;Kernel based classification;Parallel Delta rule;Weight calculation;},
URL = {http://dx.doi.org/10.1016/j.neunet.2013.11.002},
} 


@article{20134917058609,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Injecting mechanical faults to localize developer faults for evolving software},
journal = {ACM SIGPLAN Notices},
author = {Zhang, Lingming and Zhang, Lu and Khurshid, Sarfraz},
volume = {48},
number = {10},
year = {2013},
pages = {765 - 784},
issn = {15232867},
abstract = {This paper presents a novel methodology for localizing faults in code as it evolves. Our insight is that the essence of failure-inducing edits made by the developer can be captured using mechanical program transformations (e.g., mutation changes). Based on the insight, we present the FIFL framework, which uses both the spectrum information of edits (obtained using the existing FaultTracer approach) as well as the potential impacts of edits (simulated by mutation changes) to achieve more accurate fault localization. We evaluate FIFL on real-world repositories of nine Java projects ranging from 5.7KLoC to 88.8KLoC. The experimental results show that FIFL is able to outperform the stateof-the-art FaultTracer technique for localizing failureinducing program edits significantly. For example, all 19 FIFL strategies that use both the spectrum information and simulated impact information for each edit outperform the existing FaultTracer approach statistically at the significance level of 0.01. In addition, FIFL with its default settings outperforms FaultTracer by 2.33% to 86.26% on 16 of the 26 studied version pairs, and is only inferior than FaultTracer on one version pair. Copyright &copy; 2013. Copyright &copy; 2013 ACM.<br/>},
key = {Software testing},
keywords = {Information use;},
note = {Fault localization;Mechanical faults;Mechanical programs;Mutation testing;Regression testing;Significance levels;Software Evolution;Spectrum information;},
} 


@inproceedings{20162102414233,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Long term, interrelated interventions to increase women's participation in STEM in the Netherlands},
journal = {Proceedings of the 43rd SEFI Annual Conference 2015 - Diversity in Engineering Education: An Opportunity to Face the New Trends of Engineering, SEFI 2015},
author = {Booij, C. and Jansen, E.J.M. and Van Schaik, E.J.},
year = {2015},
address = {8 Rue Leonard de Vinci, Orleans, France},
abstract = {Traditionally, the Netherlands lag behind other countries in terms of the percentage of girls opting for Science, Technology, Engineering, Mathematics (STEM)-study programs (Eurostat, 2009; OECD, 2003). In international research a number of determinants for the under-representation of girls/women in STEM have been appointed, including girls? lower self-concepts, non-stimulating learning environments, lack of female role models, stereotyped associations in society about girls/women and STEM, fertility/lifestyle factors, and career preferences of girls and women (e.g., [1-3]). Since the early 1980s, VHTO (www.vhto.nl, WiTEC partner), has been building up knowledge and experience about the participation of girls and women in STEM. VHTO has been deploying this expertise in the whole chain of education ? from primary to higher education ? through the labour market in the Netherlands. In the course of time it has proved to be essential to commit interrelated interventions for a long period of time, instead of ad hoc or short term single actions.Implementation strategies are directed towards change in educational policy (i.e., awareness and gender inclusiveness), professional development for staff and teachers, career exploration opportunities and offering counter stereotypes (e.g., support of role models, Girls day and girls code events) at critical junctions/moments of choice in the girls? (school) careers, parents? involvement, and dissemination. Statistics show that in the course more girls (and boys) opted for STEM subjects and higher STEM education, especially in pre-university secondary schools. We will show how policies regarding the under-representation of girls and women in STEM (specifically in education) have developed over the years in the Netherlands and other countries. We will introduce a policy compass that can support institutions to choose appropriate policies and to set up an adequate action plan.<br/>},
key = {STEM (science, technology, engineering and mathematics)},
keywords = {Computer aided instruction;Curricula;Employment;Engineering education;Teaching;},
note = {Career development;Gender mainstreaming;International researches;Knowledge and experience;Learning environments;Professional development;Science, technology, engineering, mathematics;STEM education;},
} 


@inproceedings{20134817031979,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Injecting mechanical faults to localize developer faults for evolving software},
journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
author = {Zhang, Lingming and Zhang, Lu and Khurshid, Sarfraz},
year = {2013},
pages = {765 - 784},
address = {Indianapolis, IN, United states},
abstract = {This paper presents a novel methodology for localizing faults in code as it evolves. Our insight is that the essence of failure-inducing edits made by the developer can be captured using mechanical program transformations (e.g., mutation changes). Based on the insight, we present the FIFL framework, which uses both the spectrum information of edits (obtained using the existing FAULTTRACER approach) as well as the potential impacts of edits (simulated by mutation changes) to achieve more accurate fault localization. We evaluate FIFL on real-world repositories of nine Java projects ranging from 5.7KLoC to 88.8KLoC. The experimental results show that FIFL is able to outperformthe stateof- the-art FAULTTRACER technique for localizing failureinducing program edits significantly. For example, all 19 FIFL strategies that use both the spectrum information and simulated impact information for each edit outperform the existing FAULTTRACER approach statistically at the significance level of 0.01. In addition, FIFL with its default settings outperforms FAULTTRACER by 2.33% to 86.26% on 16 of the 26 studied version pairs, and is only inferior than FAULTTRACER on one version pair. Copyright &copy; 2013 ACM.<br/>},
key = {Object oriented programming},
keywords = {Computer systems programming;Information use;Software testing;},
note = {Fault localization;Mechanical faults;Mechanical programs;Mutation testing;Regression testing;Significance levels;Software Evolution;Spectrum information;},
URL = {http://dx.doi.org/10.1145/2509136.2509551},
} 


@inbook{20172803908563,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Software engineering ethics education: Incorporating critical pedagogy into student outreach projects},
journal = {Contemporary Ethical Issues in Engineering},
author = {Kadoda, Gada},
year = {2015},
pages = {228 - 250},
abstract = {The difficulties inherent in the nature of software as an intangible object pose problems for specifying its needs, predicting overall behavior or impact on users, and therefore on defining the ethical questions that are involved in software development. Whereas software engineering drew from older engineering disciplines for process and practice development, culminating in the IEEE/ACM Professional Code in 1999, the topic of Software Engineering Ethics is entwined with Computer Science, and developments in Computer and Information Ethics. Contemporary issues in engineering ethics such as globalization have raised questions for software engineers about computer crime, civil liberties, open access, digital divide, etc. Similarly, computer-related ethics is becoming increasingly important for engineering ethics because of the dominance of computers in modern engineering practice. This is not to say that software engineers should consider everything, but the diversity of ethical issues presents a challenge to the approach of accumulating resources that many ethicists maintain can be overcome by developing critical thinking skills as part of technical training courses. This chapter explores critical pedagogies in the context of student outreach activities such as service learning projects and considers their potential in broadening software engineering ethics education. The practical emphasis in critical pedagogy can allow students to link specific software design decisions and ethical positions, which can perhaps transform both student and teacher into persons more curious about their individual contribution to the public good and more conscious of their agency to change the conditions around them. After all, they share with everyone else a basic human desire to survive and flourish.<br/> &copy; 2015, IGI Global.},
key = {Software design},
keywords = {Computer software;Engineering education;Personnel training;Philosophical aspects;Students;Teaching;},
note = {Contemporary issues;Critical pedagogies;Critical thinking skills;Engineering disciplines;Information ethics;Service learning project;Software design decisions;Technical training;},
URL = {http://dx.doi.org/10.4018/978-1-4666-8130-9.ch016},
} 


@article{20154101364700,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Ergodic Quotients in Analysis of Dynamical Systems},
journal = {ProQuest Dissertations and Theses Global},
author = {Budisic, Marko},
year = {2012},
abstract = {The analysis of dynamical systems typically taught in introductory graduate courses focuses on analysis of trajectories: curves in the state space, parametrized by time. While the trajectory-based approach is successful in analyzing exceptional solutions, e.g., stable and unstable manifolds attached to fixed points, it is a poor choice for recognizing coarse patterns in state spaces, e.g., vortices in fluid-like flows. An alternative is the operator-theoretic description of dynamical systems. This doctoral project studied the Koopman operator, which describes how functions, or observables, change as the dynamical system evolves in time. The main interest was in identification of invariant sets in the state space, where behavior of the system is uniform ''on average''. The arrangement of invariant sets is connected to the ergodic quotient: a subset of a sequence space where trajectories are described using values of invariant functions along them. By endowing the ergodic quotient with a Sobolev space metric structure, we are able to identify coherent sets: invariant subsets in the state space that possess a complete set of continuous invariant functions. These coherent sets correspond to continuous segments in the ergodic quotient. The ergodic quotient analysis is model-free: it requires only data obtained by simulation or through experiments. As a proof-of-concept, we implemented the algorithm as a numerical code that approximates the ergodic quotient using trajectory averages of a function basis on the state space. The geometry of the ergodic quotient is analyzed with help of a machine-learning algorithm, Diffusion Maps, which enables computational identification of continuous segments and, consequently, partitioning of the state space into coherent sets. To demonstrate effectiveness, several examples of dynamical systems were analyzed, ranging from iterated maps with mixed state spaces to forced fluid-like flows. The results obtained by the ergodic quotient analysis match the results of previous analyses of the systems. To conclude, further research directions are proposed, based on the results of the doctoral project. ProQuest Subject Headings: Mathematics, Mechanical engineering, Applied mathematics.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.},
key = {Dynamical systems},
keywords = {Computational geometry;Diffusion in liquids;Flow of fluids;Learning algorithms;Learning systems;Mathematical techniques;Sobolev spaces;Trajectories;},
note = {Applied mathematics;Computational identification;Diffusion maps;Graduate course;Invariant functions;Proof of concept;Trajectory-based;Unstable manifold;},
} 


@inproceedings{20152400931654,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An analysis of the feasibility and benefits of GPU/multicore acceleration of the Weather Research and Forecasting model},
journal = {Concurrency Computation},
author = {Vanderbauwhede, Wim and Takemi, Tetsuya},
volume = {28},
number = {7},
year = {2016},
pages = {2052 - 2072},
issn = {15320626},
abstract = {There is a growing need for ever more accurate climate and weather simulations to be delivered in shorter timescales, in particular, to guard against severe weather events such as hurricanes and heavy rainfall. Due to climate change, the severity and frequency of such events - and thus the economic impact - are set to rise dramatically. Hardware acceleration using graphics processing units (GPUs) or Field-Programmable Gate Arrays (FPGAs) could potentially result in much reduced run times or higher accuracy simulations. In this paper, we present the results of a study of the Weather Research and Forecasting (WRF) model undertaken in order to assess if GPU and multicore acceleration of this type of numerical weather prediction (NWP) code is both feasible and worthwhile. The focus of this paper is on acceleration of code running on a single compute node through offloading of parts of the code to an accelerator such as a GPU. The governing equations set of the WRF model is based on the compressible, non-hydrostatic atmospheric motion with multi-physics processes. We put this work into context by discussing its more general applicability to multi-physics fluid dynamics codes: in many fluid dynamics codes, the numerical schemes of the advection terms are based on finite differences between neighboring cells, similar to the WRF code. For fluid systems including multi-physics processes, there are many calls to these advection routines. This class of numerical codes will benefit from hardware acceleration. We studied the performance of the original code of the WRF model and proposed a simple model for comparing multicore CPU and GPU performance. Based on the results of extensive profiling of representative WRF runs, we focused on the acceleration of the scalar advection module. We discuss the implementation of this module as a data-parallel kernel in both OpenCL and OpenMP. We show that our data-parallel kernel version of the scalar advection module runs up to seven times faster on the GPU compared with the original code on the CPU. However, as the data transfer cost between GPU and CPU is very high (as shown by our analysis), there is only a small speed-up (two times) for the fully integrated code. We show that it would be possible to offset the data transfer cost through GPU acceleration of a larger portion of the dynamics code. In order to carry out this research, we also developed an extensible software system for integrating OpenCL code into large Fortran code bases such as WRF. This is one of the main contributions of our work. We discuss the system to show how it allows the replacement of the sections of the original codebase with their OpenCL counterparts with minimal changes - literally only a few lines - to the original code. Our final assessment is that, even with the current system architectures, accelerating WRF - and hence also other, similar types of multi-physics fluid dynamics codes - with a factor of up to five times is definitely an achievable goal. Accelerating multi-physics fluid dynamics codes including NWP codes is vital for its application to weather forecasting, environmental pollution warning, and emergency response to the dispersion of hazardous materials. Implementing hardware acceleration capability for fluid dynamics and NWP codes is a prerequisite for up-to-date and future computer architectures.<br/> &copy; Copyright 2015 John Wiley & Sons, Ltd.},
key = {Weather forecasting},
keywords = {Acceleration;Advection;Application programming interfaces (API);Climate change;Codes (symbols);Computer architecture;Computer graphics;Computer graphics equipment;Cost benefit analysis;Data transfer;Field programmable gate arrays (FPGA);Fluid dynamics;Graphics processing unit;Hardware;Hazardous materials;Multiprocessing systems;Pollution control;Program processors;Rain;},
note = {Environmental pollutions;Field programmable gate array (FPGAs);General Purpose Computation on Graphics Processing Unit (GPGPU);Hardware acceleration;Numerical weather prediction;Parallelizations;Severe weather events;Weather research and forecasting models;},
URL = {http://dx.doi.org/10.1002/cpe.3522},
} 


@inproceedings{20183805830545,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Image segmentation through fuzzy clustering: A survey},
journal = {Advances in Intelligent Systems and Computing},
author = {Jain, Rashi and Sharma, Rama Shankar},
volume = {741},
year = {2019},
pages = {497 - 508},
issn = {21945357},
address = {Gurgaon, India},
abstract = {In modern years, image processing is a vast area for research. Image segmentation is the most popular part of image processing which divides the image into number of segments to analyze the better quality of image. It is used to detect objects and boundaries in images. Main goal of image segmentation is to change the representation of image into the more meaningful regions. Image segmentation results in a set of segments that covers the whole image or curves that are extracted from the image. In this paper, different image segmentation techniques and algorithms are presented, and clustering is one of the techniques that is used for segmentation. Fuzzy c-means clustering (FCM) algorithm is presented in this paper for image segmentation. On the basis of literature reviewed, several problems are analyzed in previously FCM, and the problems have been overcome by modifying the objective function of the previously FCM, and spatial information is incorporated in objective function of FCM. Fuzzy c-mean clustering is also known as soft clustering. The techniques that are explained in this survey are segmentation of the noisy medicinal images along spatial probability, histogram-based FCM, improved version of fuzzy c-means (IFCM), fuzzy possibilistic c-means (FPCM), possibilistic c-means (PCM), and possibilistic fuzzy c-means (PFCM) algorithms are to be explained in further sections on the basis of literature review. Moreover, several recent works on fuzzy c-means using clustering till 2017 are presented in this survey.<br/> &copy; Springer Nature Singapore Pte Ltd. 2019.},
key = {Image segmentation},
keywords = {Clustering algorithms;Computation theory;Fuzzy clustering;Fuzzy systems;Image enhancement;Membership functions;Object detection;Pulse code modulation;Soft computing;Surveys;},
note = {FPCM;Fuzzy C mean clustering;Fuzzy C means clustering;Fuzzy possibilistic c-means;PFCM;Possibilistic C-means;Segmentation results;Segmentation techniques;},
URL = {http://dx.doi.org/10.1007/978-981-13-0761-4_48},
} 


@inproceedings{20131716234971,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {PARFUME modeling status update},
journal = {Transactions of the American Nuclear Society},
author = {Skerjanc, William F. and Maki, John T. and Petti, David A.},
volume = {106},
year = {2012},
pages = {618 - 621},
issn = {0003018X},
address = {Chicago, IL, United states},
abstract = {The recent developments in PARFUME have increased the flexibility of the code as a fuel performance simulator. PARFUME now has the option to replace the SiC layer with ZrC to explore the effects of a possible material change. Also, PARFUME can capture the multidimensional behavior of different particle types and sizes over a broad range of both normal and accident conditions. Finally, it has provided a way to explore fuel particle behavior via response surfaces to identify operating conditions that may be adverse to fuel behavior to feed into larger system codes. Currently, PARFUME development is focused on the addition of a cylindrical geometry option similar to the prismatic fuel geometry option. In addition, an initially defective SiC model has been added to the code and is currently being validated. These models will be included in a future version of the code to assist in fuel performance modeling applications.<br/>},
key = {Nuclear reactor accidents},
keywords = {Codes (symbols);Cylinders (shapes);Nuclear fuels;Nuclear reactors;Silicon carbide;Silicon compounds;Zirconium compounds;},
note = {Accident conditions;Cylindrical geometry;Fuel particles;Fuel performance;Material change;Multi-dimensional behavior;Operating condition;Response surface;},
} 


@inproceedings{20172003677944,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Interactive generalized keyboard driver for Bengali braille embosser},
journal = {ECCE 2017 - International Conference on Electrical, Computer and Communication Engineering},
author = {Dhar, Kishalay and Rahman, M. Ashifur and Ullah, M. Ahsan},
year = {2017},
pages = {850 - 854},
address = {Cox's Bazar, Bangladesh},
abstract = {Bengali Braille Embosser is a very essential device for learning of visually impaired person in Bangladesh. This paper presents an interactive generalized keyboard driver for Bengali Braille Embosser. This driver is operated by single AVR microcontroller. It can interface any PC keyboard through PS2 interface protocol with the Braille Embosser. The driver is interactive and can communicate with visually impaired person by generating sound of Bengali alphabet after pressing related key of the keyboard. In this paper the Bijoy keyboard layout is used. This driver can stores the written text in memory (micro SD card) and playback the sound of the text also. The designed device supports 16 GB of micro SD storage. This keyboard driver serves the user to edit the written text without help of visual person. So, this device makes a visually impaired person independent of writing and reading. At the same time the device makes Braille code from the written text for printing the Braille text on the Braille paper.<br/>},
key = {Engineering},
keywords = {Computer science;Computers;Industrial engineering;},
note = {AVR microcontrollers;Braille;Embosser;Interface protocol;Keyboard layout;Micro SD;SPI bus;Visually impaired persons;},
URL = {http://dx.doi.org/10.1109/ECACE.2017.7913021},
} 


@article{20134817031514,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Assessing possible shifts in wildfire regimes under a changing climate in mountainous landscapes},
journal = {Forest Ecology and Management},
author = {Mori, Akira S. and Johnson, Edward A.},
volume = {310},
year = {2013},
pages = {875 - 886},
issn = {03781127},
abstract = {Climate change may affect the probability of extreme events such as wildfires. Although wildfires are some of the most important ecological processes in forest ecosystems, large-scale wildfires are often perceived as an environmental disaster. Since failure to include the dynamic nature of ecosystems in planning will inevitably lead to unexpected outcomes, we need to enhance our ability to cope with future extreme events coupled with climate change. This study presents several future scenarios in three different time periods for Canada's Columbia Montane Cordillera Ecoprovince, which is prone to wildfires. These scenarios predict the probability of occurrence of widespread wildfires based on the hierarchical Bayesian model. The model was based on the relationships between wildfires and the Monthly Drought Code (MDC). The MDC is a generalized monthly version of the Daily Drought Code widely used across Canada by forest fire management agencies for monitoring of wildfire risk. To calculate future MDC values, we relied on different possible future conditions of climate, given by the Global Circulation Models. We found a regime shift in drought intensity with abrupt decreases in lightning-caused wildfire activity around 1940, suggesting that future wildfire risks can be inferred primarily from the summer drought code. For future periods, we found increasing trends in the probabilities of large-scale fires with time in most areas. It should be notable that, by the 2080s, there is a probability of some areas having more than 50% of large-scale wildfires under the "average" climatic conditions in the future, indicating that, even without "extreme" weather conditions, some ecosystems will have a fundamental probability of experiencing catastrophic fires under the condition of average summer. However, the rate of progression toward a fire-prone condition is quite different among the three climate change scenarios and among the region analyzed. Given such scenario-sensitive, spatially-heterogeneous patterns of wildfire probability in response to climate variability, management strategy should be flexible and more localized. By drawing on this knowledge, it may be possible to mitigate climate change impacts both before they arise and once they have occurred. These considerations are critical for maintaining the integrity of systems shaped by large-scale natural disturbances to increase their resilience to the changing climate while protecting human society and infrastructures. Working with alternative scenarios will facilitate our adaptation to climate change in managing fire-prone forest ecosystems. &copy; 2013 Elsevier B.V.<br/>},
key = {Climate change},
keywords = {Bayesian networks;Climate models;Codes (symbols);Deforestation;Drought;Ecosystems;Fires;Probability;},
note = {Adaptation to climate changes;Climate change scenarios;Ecosystem-based management;Fire scenarios;Hierarchical Bayesian modeling;Natural disturbance;Probability of occurrence;Wildfire;},
URL = {http://dx.doi.org/10.1016/j.foreco.2013.09.036},
} 


@inproceedings{20141417534016,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An experience of using numerical methods in experimental investigations of TsAGI},
journal = {28th Congress of the International Council of the Aeronautical Sciences 2012, ICAS 2012},
author = {Bosnyakov, S.M. and Gorbushin, A.R. and Mikhaylov, S.V. and Neyland, V. Ya},
volume = {2},
year = {2012},
pages = {1657 - 1668},
address = {Brisbane, Australia},
abstract = {Description of EWT (Electronic Wind Tunnel) code is done. This code is used in TsAGI from 1996. It permits to solve stationary (RANS) and non-stationary (URANS) Navier-Stokes equations with Reynolds-averaging. A possibility to simulate large-scale vortices (LES) is realized. Special boundary conditions, such as "wind tunnel start", "permeable walls" (perforated and slotted), "threadmill" and "plenum chamber walls" are discussed in details. It is mentioned that code effectively uses chimera-type grids based on original "connect" technology. Practical aspects are described. It is shown that grid templates with special blocks for model and wind tunnel parts are prepared in advance. It permits to change model in EWT operatively. Important algorithm for grid rebuilding, in the case of changing the model incidence and slip angles, is also developed and works reliable. Initially (1996), the code was adapted for conditions of T-128 wind tunnel (TsAGI). Later on (2006), the version for ETW (European Transonic Windtunnel) appeared. Special code for T-104 wind tunnel (TsAGI) with simulation of "moving runway" and open test section has been developed after that (2008). Examples of results obtained in the frame of code integration to experimental cycle of different wind tunnels are presented. Important tasks are solved: 1) wall interference prediction; 2) systematic mistakes such as support influence etc. estimation; 3) creation of optimal wind tunnel parts. Picture bellow shows result of penishe height problem solution during mounting the half model in T-128 TsAGI. Streamlines around "free" model and halfmodel in wind tunnel are compared.<br/>},
key = {Wind tunnels},
keywords = {Boundary conditions;Codes (symbols);Computer simulation;Navier Stokes equations;Numerical methods;Well perforation;},
note = {Change modeling;Experimental cycles;Experimental investigations;Large-scale vortices;Problem solutions;Reynold save raging;Slotted walls;Wall interference;},
} 


@inproceedings{20141117451605,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Lazy stateless incremental evaluation machinery for attribute grammars},
journal = {PEPM 2014 - Proceedings of the ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation, Co-located with POPL 2014},
author = {Bransen, Jeroen and Dijkstra, Atze and Doaitse Swierstra, S.},
year = {2014},
pages = {145 - 156},
address = {San Diego, CA, United states},
abstract = {Many computer programs work with data that changes over time. Computations done over such data usually are repeated completely after a change in the data. For complex computations such repetitive recomputation can become too inefficient. When these recomputations take place on data which has only changed slightly, it often is possible to reformulate the computation to an incremental version which reuses the result of the computation on previous data. Such a situation typically occurs in compilers and editors for structured data (like a program) where program analyses and transformations (for example error checking) are done while editing. Although rewriting to incremental versions thus offers a solution to this problem, a manual rewrite of an already complex computation to its incremental counterpart is tedious, error prone, and inhibits further development of the original computation. We therefore intend to generate such incremental counterparts (semi)automatically by focusing on computations expressed using Attribute Grammars (AGs). In this paper we do groundwork for this goal and develop machinery for incremental attribute grammar evaluation based on change propagation and pure functions.We use pretty printing with free variable annotation to explain our techniques. Furthermore, our techniques also expose rules of conduct for a programmer desiring incrementality: The automatic translation of code to an incremental version does not always directly result in efficiency improvements because code often is written in a style unsuitable for automatic incrementalization.We show some common cases in which (small) code changes facilitating incrementality are required. We evaluate the effectiveness of the overall approach using a simple benchmark for the example, and a more extensive benchmark based on constraint-based type inference implemented with AGs. Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors-Incremental compilers General Terms Algorithms, Languages, Theory.<br/>},
key = {Program compilers},
keywords = {Codes (symbols);Computer programming;Context sensitive grammars;Machinery;},
note = {Attribute grammars;Change propagation;Incremental evaluation;Program transformations;Type inferences;},
URL = {http://dx.doi.org/10.1145/2543728.2543735},
} 


@article{20183005594506,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Improving regression test efficiency with an awareness of refactoring changes},
journal = {Information and Software Technology},
author = {Chen, Zhiyuan and Guo, Hai-Feng and Song, Myoungkyu},
volume = {103},
year = {2018},
pages = {174 - 187},
issn = {09505849},
abstract = {Context. Developers often improve software quality through refactorings&mdash;the practice of behavior-preserving changes to existing code. Recent studies showed that, despite their awareness of tool support for automated refactorings, developers prefer manual refactorings. This practice can be often error-prone and increase testing cost. Objective. To address the problem, we present the Refactorings Investigation and Testing technique, called RIT. RIT improves the testing efficiency for validating refactoring changes and providing confidence that changed parts behave as intended. As testing is expensive for developers of high-assurance software, RIT reduces a considerable amount of its costs by only identifying dependent statements on a failure in each test and by detecting specific refactoring edits responsible for testing failures. Method. Our approach identifies refactorings by analyzing original and edited versions of a program. It then uses the semantic impact of a set of identified refactoring changes to detect tests whose behavior may have been affected and modified by refactoring edits. Given each failed asserts after running regression test suites, RIT helps developers focus their attention on logically related program statements by applying program slicing for minimizing each test. For debugging purposes, RIT determines specific failure-inducing refactoring edits, separating from other changes that only affect other asserts or tests. Results. We evaluated RIT on three open source projects, and found that RIT detected tests affected by refactorings with 80.9% accuracy on average. Furthermore, it identified and formed partitions relating program statements only dependent on failed asserts with 97.2% accuracy on average. Conclusion. RIT, which combines a refactoring reconstruction technique with change impact analysis to localize failure-inducing program edits, helps developers localize fault causes by focusing on refactoring changes as opposed to all the code fragments in the new version.<br/> &copy; 2018 Elsevier B.V.},
key = {Software testing},
keywords = {Computer software selection and evaluation;Efficiency;Open source software;Program debugging;Semantics;},
note = {Change impact analysis;Fault localization;Refactorings;Software Evolution;Test selection;},
URL = {http://dx.doi.org/10.1016/j.infsof.2018.07.003},
} 


@inproceedings{20180704794557,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Avoiding Useless Mutants},
journal = {GPCE 2017 - Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences, co-located with SPLASH 2017},
author = {Fernandes, Leonardo and Gheyi, Rohit and Cavalcanti, Ana and Ribeiro, Marcio and Mongiovi, Melina and Ferrari, Fabiano and Carvalho, Luiz and Santos, Andre and Maldonado, Jose Carlos},
year = {2018},
pages = {187 - 198},
address = {Vancouver, BC, Canada},
abstract = {Mutation testing is a program-transformation technique that injects articial bugs to check whether the existing test suite can detect them. However, the costs of using mutation testing are usually high, hindering its use in industry. Useless mutants (equivalent and duplicated) contribute to increase costs. Previous research has focused mainly on detecting useless mutants only after they are generated and compiled. In this paper, we introduce a strategy to help developers with deriving rules to avoid the generation of useless mutants. To use our strategy, we pass as input a set of programs. For each program, we also need a passing test suite and a set of mutants. As output, our strategy yields a set of useless mutants candidates. After manually conrming that the mutants classied by our strategy as &ldquo;useless&rdquo; are indeed useless, we derive rules that can avoid their generation and thus decrease costs. To the best of our knowledge, we introduce 37 new rules that can avoid useless mutants right before their generation. We then implement a subset of these rules in the M mutation testing tool. Since our rules have been derived based on articial and small Java programs, we take our M version embedded with our rules and execute it in industrial-scale projects. Our rules reduced the number of mutants by almost 13% on average. Our results are promising because (i) we avoid useless mutants generation; (ii) our strategy can help with identifying more rules in case we set it to use more complex Java programs; and (iii) our M version has only a subset of the rules we derived.<br/> &copy; 2017 Association for Computing Machinery.},
key = {Software testing},
keywords = {Automatic programming;Costs;Java programming language;Program debugging;},
note = {Code Generation;Duplicated Mutants;Equivalent Mutants;Mutation testing;Program transformations;},
URL = {http://dx.doi.org/10.1145/3136040.3136053},
} 


@article{20171203479427,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A fast and efficient python library for interfacing with the Biological Magnetic Resonance Data Bank},
journal = {BMC Bioinformatics},
author = {Smelter, Andrey and Astra, Morgan and Moseley, Hunter N.B.},
volume = {18},
number = {1},
year = {2017},
issn = {14712105},
abstract = {Background: The Biological Magnetic Resonance Data Bank (BMRB) is a public repository of Nuclear Magnetic Resonance (NMR) spectroscopic data of biological macromolecules. It is an important resource for many researchers using NMR to study structural, biophysical, and biochemical properties of biological macromolecules. It is primarily maintained and accessed in a flat file ASCII format known as NMR-STAR. While the format is human readable, the size of most BMRB entries makes computer readability and explicit representation a practical requirement for almost any rigorous systematic analysis. Results: To aid in the use of this public resource, we have developed a package called nmrstarlib in the popular open-source programming language Python. The nmrstarlib's implementation is very efficient, both in design and execution. The library has facilities for reading and writing both NMR-STAR version 2.1 and 3.1 formatted files, parsing them into usable Python dictionary- and list-based data structures, making access and manipulation of the experimental data very natural within Python programs (i.e. "saveframe" and "loop" records represented as individual Python dictionary data structures). Another major advantage of this design is that data stored in original NMR-STAR can be easily converted into its equivalent JavaScript Object Notation (JSON) format, a lightweight data interchange format, facilitating data access and manipulation using Python and any other programming language that implements a JSON parser/generator (i.e., all popular programming languages). We have also developed tools to visualize assigned chemical shift values and to convert between NMR-STAR and JSONized NMR-STAR formatted files. Full API Reference Documentation, User Guide and Tutorial with code examples are also available. We have tested this new library on all current BMRB entries: 100% of all entries are parsed without any errors for both NMR-STAR version 2.1 and version 3.1 formatted files. We also compared our software to three currently available Python libraries for parsing NMR-STAR formatted files: PyStarLib, NMRPyStar, and PyNMRSTAR. Conclusions: The nmrstarlib package is a simple, fast, and efficient library for accessing data from the BMRB. The library provides an intuitive dictionary-based interface with which Python programs can read, edit, and write NMR-STAR formatted files and their equivalent JSONized NMR-STAR files. The nmrstarlib package can be used as a library for accessing and manipulating data stored in NMR-STAR files and as a command-line tool to convert from NMR-STAR file format into its equivalent JSON file format and vice versa, and to visualize chemical shift values. Furthermore, the nmrstarlib implementation provides a guide for effectively JSONizing other older scientific formats, improving the FAIRness of data in these formats.<br/> &copy; 2017 The Author(s).},
key = {Stars},
keywords = {Chemical shift;Computer systems programming;Data structures;High level languages;Macromolecules;Magnetism;Nuclear magnetic resonance;Open source software;},
note = {Biological macromolecule;Biological Magnetic Resonance Bank;Explicit representation;JavaScript object notations (JSON);JSON;Nmrstarlib;Nuclear Magnetic Resonance (NMR);Python;},
URL = {http://dx.doi.org/10.1186/s12859-017-1580-5},
} 


@inproceedings{20183505750009,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Iterator-based optimization of imperfectly-nested loops},
journal = {Proceedings - 2018 IEEE 32nd International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2018},
author = {Feshbach, Daniel and Glaser, Mary and Strout, Michelle and Wonnacott, David G.},
year = {2018},
pages = {906 - 914},
address = {Vancouver, BC, Canada},
abstract = {Effective optimization of dense array codes often depends upon the selection of the appropriate execution order for the iterations of nested loops. Tools based on the Polyhedral Model have demonstrated dramatic success in performing such optimizations on many such codes, but others remain an area of active research, leaving programmers to optimize code in other ways. Bertolacci et. al demonstrated that programmer-defined iterators can be used to explore iteration-space reorderings, and that Cray's compiler for the Chapel language can optimize such codes to be competitive with polyhedral tools. This 'iterator-based' approach allows programmers to explore iteration orderings not identified by automatic optimizers, but was only demonstrated for perfectly-nested loops, and lacked any system for warning about an iterator that would produce an incorrect result. We have now addressed these shortcomings of iterator-based loop optimization, and explored the use of our improved techniques to optimize the imperfectly-nested loops that form the core of Nussinov's algorithm for RNA secondary-structure prediction. Our C++ iterator provides performance that equals the fastest C code, several times faster than was achieved by using the same C compiler on the code with the original iteration ordering, or the code produced by the Pluto loop optimizer. Our Chapel iterators produce run-time that is competitive with the equivalent iterator-free Chapel code, though the Chapel performance does not equal that of the C/C++ code. We have also implemented an iterator that produces an incorrect-but-fast version of Nussinov's algorithm, and used this iterator to illustrate our approaches to error-detection. Manual application of our compile-time error-detection algorithm (which has yet to be integrated into a compiler) identifies this error, as does the run-time approach that we use for codes on which the static test proves inconclusive.<br/> &copy; 2018 IEEE.},
key = {C++ (programming language)},
keywords = {Bioinformatics;Codes (symbols);Error detection;Iterative methods;Optimization;Program compilers;},
note = {Error detection algorithms;Improved techniques;Iterator;Loop optimizations;Loop transformation;Perfectly nested loops;Polyhedral modeling;RNA secondary structure prediction;},
URL = {http://dx.doi.org/10.1109/IPDPSW.2018.00144},
} 


@article{20134216846648,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Transient, spatially varied groundwater recharge modeling},
journal = {Water Resources Research},
author = {Assefa, Kibreab Amare and Woodbury, Allan D.},
volume = {49},
number = {8},
year = {2013},
pages = {4593 - 4606},
issn = {00431397},
abstract = {The objective of this work is to integrate field data and modeling tools in producing temporally and spatially varying groundwater recharge in a pilot watershed in North Okanagan, Canada. The recharge modeling is undertaken by using the Richards equation based finite element code (HYDRUS-1D), ArcGIS&trade;, ROSETTA, in situ observations of soil temperature and soil moisture, and a long-term gridded climate data. The public version of HYDUS-1D and another version with detailed freezing and thawing module are first used to simulate soil temperature, snow pack, and soil moisture over a one year experimental period. Statistical analysis of the results show both versions of HYDRUS-1D reproduce observed variables to the same degree. After evaluating model performance using field data and ROSETTA derived soil hydraulic parameters, the HYDRUS-1D code is coupled with ArcGIS&trade; to produce spatially and temporally varying recharge maps throughout the Deep Creek watershed. Temporal and spatial analysis of 25 years daily recharge results at various representative points across the study watershed reveal significant temporal and spatial variations; average recharge estimated at 77.8 &plusmn; 50.8 mm/year. Previous studies in the Okanagan Basin used Hydrologic Evaluation of Landfill Performance without any attempt of model performance evaluation, notwithstanding its inherent limitations. Thus, climate change impact results from this previous study and similar others, such as Jyrkama and Sykes (2007), need to be interpreted with caution. &copy;2013. American Geophysical Union. All Rights Reserved.<br/>},
key = {Recharging (underground waters)},
keywords = {Aquifers;Climate change;Geographic information systems;Groundwater resources;Hydrogeology;Infiltration;Land fill;Rain;Soil moisture;Soil surveys;Temperature;Watersheds;},
note = {climate;Evaluating model performance;Ground water recharge;Hydrologic evaluations;Model performance evaluations;recharge;Soil hydraulic parameters;Temporal and spatial variation;},
URL = {http://dx.doi.org/10.1002/wrcr.20332},
} 


@inproceedings{20173404073649,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Integrating design freeze in to large-scale construction projects in Sri Lanka},
journal = {3rd International Moratuwa Engineering Research Conference, MERCon 2017},
author = {Hemal, R.M.N. and Waidyasekara, K.G.A.S. and Ekanayake, E.M.A.C.},
year = {2017},
pages = {259 - 264},
address = {Katubedda, 10400, Moratuwa, Sri lanka},
abstract = {Change is inevitable in most construction projects due to uniqueness of projects and limited resources. Previous studies revealed that design changes are accountable for 78% of the total cost deviation in construction projects. Design Freeze (DF) is one method used during design development stage to mitigate the risks associated with change. This organizes and complies the design process, control changes, and force the completion of design stages on time. However, this is a novel concept to the construction industry. Hence, this study focused on investigating the DF concept in terms of its adoptability in large-scale construction projects to enhance project performance by minimizing changes. The present study adopted a mixed approach where the questionnaire survey and expert interviews helped to derive findings. Questionnaire survey was administered to hundred (100) industry professionals with a response rate of 87%. Further, an expert opinion survey was performed with nine subject matter experts. The quantitative data were analysed using SPSS software, whereas code based content analysis using Nvivo version 11 supported to analyse qualitative data. The findings revealed that the client and the consultant are the major parties causing design changes. Selecting an experienced and qualified design team, permitting adequate time frame for design development, and etc. are the significant factors to be considered when implementing a DF. Furthermore, the study identified the impracticability of adopting a 100% design freeze in large-scale construction projects, and the interviewees accepted the concept of partial design freeze by identifying strategies targeted to maximize the design percentage.<br/> &copy; 2017 IEEE.},
key = {Design},
keywords = {Construction industry;Engineering research;Surveys;},
note = {Construction projects;Design change;Design freezes;Industry professionals;Integrating design;Project performance;Questionnaire surveys;Subject matter experts;},
URL = {http://dx.doi.org/10.1109/MERCon.2017.7980492},
} 


@article{20171003426272,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Numerical simulation of metal jet breakup, cooling and solidification in water},
journal = {International Journal of Heat and Mass Transfer},
author = {Zhou, Yuan and Chen, Jingtan and Zhong, Mingjun and Wang, Junfeng and Lv, Meng},
volume = {109},
year = {2017},
pages = {1100 - 1109},
issn = {00179310},
abstract = {During transient intrusion of molten metal into water, metal go through cooling, breakup before fully solidified. This paper describes a numerical code which combines cooling, solidification and breakup in a single computation. In the code free surface of jet is tracked by Volume of Fluid Method (VOF), both the heat transfer and viscosity variation during liquid-solid phase change are taken into account. The simulation results of melt jet pattern, front position history, jet breakup length and breakup time are in good agreement with the experimental results. The effects of interfacial temperature and jet velocity are also determined. The molten jet thermal history and solidification, droplet generation rate at different penetration times, which are difficult to observe in experiment, are presented to gain an insight into this complicated process. Solidified metal proportion increases with jet penetration depth. Melt jet breakup with surface solidification can be divided into three zones in space: (1) liquid core, (2) solidifying zone, (3) solid droplets. These simulation data are helpful to substantiate the understanding of the phenomena during molten melt jet interactions with water.<br/> &copy; 2017 Elsevier Ltd},
key = {Fighter aircraft},
keywords = {Computational fluid dynamics;Computer simulation;Cooling;Drop breakup;Liquid metals;Metals;Numerical models;Solidification;},
note = {Droplet generation;Interfacial temperature;Jet interactions;Jet penetration depth;Liquid-solid phase change;Single computation;Viscosity variations;Volume of fluid method;},
URL = {http://dx.doi.org/10.1016/j.ijheatmasstransfer.2017.02.083},
} 


@inproceedings{20121214868848,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Social coding in GitHub: Transparency and collaboration in an open software repository},
journal = {Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW},
author = {Dabbish, Laura and Stuart, Colleen and Tsay, Jason and Herbsleb, Jim},
year = {2012},
pages = {1277 - 1286},
address = {Seattle, WA, United states},
abstract = {Social applications on the web let users track and follow the activities of a large number of others regardless of location or affiliation. There is a potential for this transparency to radically improve collaboration and learning in complex knowledge-based activities. Based on a series of in-depth interviews with central and peripheral GitHub users, we examined the value of transparency for large-scale distributed collaborations and communities of practice. We find that people make a surprisingly rich set of social inferences from the networked activity information in GitHub, such as inferring someone else's technical goals and vision when they edit code, or guessing which of several similar projects has the best chance of thriving in the long term. Users combine these inferences into effective strategies for coordinating work, advancing technical skills and managing their reputation. &copy; 2012 ACM.<br/>},
key = {Open source software},
keywords = {Computer supported cooperative work;Groupware;Interactive computer systems;Knowledge based systems;Open systems;Software design;Transparency;},
note = {Activity informations;awareness;collaboration;Communities of Practice;coordination;Distributed collaboration;Social applications;Social computing;},
URL = {http://dx.doi.org/10.1145/2145204.2145396},
} 


@inproceedings{20140917375288,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An extensible, multi-model software library for simulating crop growth and development},
journal = {iEMSs 2012 - Managing Resources of a Limited Planet: Proceedings of the 6th Biennial Meeting of the International Environmental Modelling and Software Society},
author = {Confalonieri, R. and Bregaglio, S. and Stella, T. and Negrini, G. and Acutis, M. and Donatelli, M.},
year = {2012},
pages = {884 - 892},
address = {Leipzig, Germany},
abstract = {The availability of different crop models and of a variety of techniques to evaluate their behaviour led to a change of paradigm in crop models use. Modellers are now looking beyond the idea of groups of users and developers grounded on a specific model, and international initiatives focusing on model improvement basing on intercomparison and knowledge sharing are recently catalysing the attention of the international community. Also, the analysis under environmental conditions of no adaptation by crops, such as the ones of climate change scenarios, demand for extension of crop models to account for extreme events, diseases and pests impact, difficult to implement into legacy code. The Crop Models Library (CropML) is a framework-independent MS .NET software component where different pure (e.g., WOFOST, CropSyst, WARM), hybrid and new modelling solutions for crop growth and development are implemented following a fine level of granularity, according to a high level software architecture. CropML can be extended by third parties and is distributed at no cost with a software development kit, including documentation of code and algorithms and sample applications. CropML provides modellers with an environment favouring the hybridization of models with parts from others, the evolution of existing approaches, and the possibility of analysing and easily comparing diverse modelling solutions. As an example, a new generation of SUCROS-type models has been developed and included in the component. Comparison of the standard and of the new version of the WOFOST model carried out using data from rice field experiments revealed an increase in accuracy and robustness with less than half of the parameters used by the standard version of the model. These results support the idea that high-level technology for models formalization can favour the development of the models themselves.<br/>},
key = {Climate models},
keywords = {Application programs;Climate change;Crops;Software design;},
note = {Crop model;CropSyst;Software component;WARM;WOFOST;},
} 


@article{20161402176857,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Cross-Layer Software Dependability on Unreliable Hardware},
journal = {IEEE Transactions on Computers},
author = {Rehman, Semeen and Chen, Kuan-Hsun and Kriebel, Florian and Toma, Anas and Shafique, Muhammad and Chen, Jian-Jia and Henkel, Jorg},
volume = {65},
number = {1},
year = {2016},
pages = {80 - 94},
issn = {00189340},
abstract = {To enable reliable embedded systems, it is imperative to leverage the compiler and system software for joint optimization of functional correctness (i.e., vulnerability indexes) and timing correctness (i.e., deadline misses). This paper considers the optimization of the reliability-timing (RT) penalty, defined as a linear combination of the vulnerability and deadline misses. We propose a cross-layer approach to achieve reliable code generation and execution at compilation and system software layers for embedded systems. This is enabled by the concept of generating multiple versions for given application functions, with diverse performance and reliability tradeoffs, by exploiting different reliability-guided compilation options. As the execution time of a function is not fixed, the selection of the versions depends upon the execution behavior of the previous functions. Based on the reliability and execution time profiling of these versions, our reliability-driven system software decides the prioritization of the functions for determining their execution order and employs dynamic version selection to dynamically select a suitable version of a function. Specifically, our scheme builds a schedule table offline to optimize the RT penalty, and uses this table at run time to select suitable versions for the subsequent functions. A complex real-world application of 'secure video and audio processing' composed of various functions is evaluated for reliable code generation and execution.<br/> &copy; 1968-2012 IEEE.},
key = {Embedded systems},
keywords = {Fault tolerance;Program compilers;Radiation hardening;Real time systems;Reliability;Scheduling;Software reliability;},
note = {Compiler;Cross layer;Real time;Soft error;Software transformation;},
URL = {http://dx.doi.org/10.1109/TC.2015.2417554},
} 


@inproceedings{20180204627622,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Exploiting Earth observation data pools for urban analysis: The TEP URBAN project},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
author = {Heldens, W. and Esch, T. and Asamer, H. and Boettcher, M. and Brito, F. and Hirner, A. and Marconcini, M. and Mathot, E. and Metz, A. and Permana, H. and Zeidler, J. and Balhar, J. and Kuchar, S. and Soukop, T. and Stankek, F.},
volume = {10431},
year = {2017},
pages = {The Society of Photo-Optical Instrumentation Engineers (SPIE) - },
issn = {0277786X},
address = {Warsaw, Poland},
abstract = {Large amounts of Earth observation (EO) data have been collected to date, to increase even more rapidly with the upcoming Sentinel data. All this data contains unprecedented information, yet it is hard to retrieve, especially for nonremote sensing specialists. As we live in an urban era, with more than 50% of the world population living in cities, urban studies can especially benefit from the EO data. Information is needed for sustainable development of cities, for the understanding of urban growth patterns or for studying the threats of natural hazards or climate change. Bridging this gap between the technology-driven EO sector and the information needs of environmental science, planning, and policy is the driver behind the TEP-Urban project. Modern information technology functionalities and services are tested and implemented in the Urban Thematic Exploitation Platform (U-TEP). The platform enables interested users to easily exploit and generate thematic information on the status and development of the environment based on EO data and technologies. The beta version of the web platform contains value added basic earth observation data, global thematic data sets, and tools to derive user specific indicators and metrics. The code is open source and the architecture of the platform allows adding of new data sets and tools. These functionalities and concepts support the four basic use scenarios of the U-TEP platform: explore existing thematic content; task individual on-demand analyses; develop, deploy and offer your own content or application; and, learn more about innovative data sets and methods.<br/> &copy; 2017 SPIE.},
key = {Urban growth},
keywords = {Climate change;Electrooptical devices;Environmental technology;Monitoring;Observatories;Open systems;Population statistics;Remote sensing;Sustainable development;},
note = {Earth observation data;Earth observations;Environmental science;Indicators and metrics;Modern information technologies;Service platforms;Urban;Urban growth patterns;},
URL = {http://dx.doi.org/10.1117/12.2299947},
} 


@inproceedings{20150700532607,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mapping afforestation and forest biomass using time-series Landsat stacks},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
author = {Liu, Liangyun and Peng, Dailiang and Wang, Zhihui and Hu, Yong},
volume = {9260},
year = {2014},
pages = {Ministry of Earth Sciences; National Aeronautics and Space Administration (NASA); State Key Laboratory of Remote Sensing Science; The Society of Photo-Optical Instrumentation Engineers (SPIE) - },
issn = {0277786X},
address = {Beijing, China},
abstract = {Satellite data can adequately capture forest dynamics over larger areas. Firstly, the Landsat ground surface reflectance (GSR) images from 1974 to 2013 were collected and processed based on 6S atmospheric transfer code and a relative reflectance normalization algorithm. Subsequently, we developed a vegetation change tracking method to reconstruct the forest change history (afforestation and deforestation) from the dense time-series Landsat GSR images, and the afforestation age was successfully retrieved from the Landsat time-series stacks in the last forty years and shown to be consistent with the surveyed tree ages. Then, the above ground biomass (AGB) regression models were greatly improved by integrating the simple ratio vegetation index (SR) and tree age. Finally, the forest AGB images were mapped at eight epochs from 1985 to 2013 using SR and afforestation age. The total forest AGB in six counties of Yulin District increased by 20.8 G kg, from 5.8 G kg in 1986 to 26.6 G kg in 2013, a total increase of 360%. For the forest area, the forest AGB density increased from 15.72 t/ha in 1986 to 44.53 t/ha in 2013, with an annual rate of about 1 t/ha. The results present a noticeable carbon increment for the planted artificial forest in Yulin District over the last four decades.<br/> &copy; 2014 SPIE.},
key = {Remote sensing},
keywords = {Biomass;Deforestation;Image processing;Reflection;Reforestation;Regression analysis;Surface measurement;Time series;Vegetation;},
note = {Above ground biomass;Afforestation age;Artificial forest;Landsat time series;Normalization algorithms;Ratio vegetation indices;Vegetation change;Vegetation index;},
URL = {http://dx.doi.org/10.1117/12.2069479},
} 


@article{20171703594140,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automating Live Update for Generic Server Programs},
journal = {IEEE Transactions on Software Engineering},
author = {Giuffrida, Cristiano and Iorgulescu, Clin and Tamburrelli, Giordano and Tanenbaum, Andrew S.},
volume = {43},
number = {3},
year = {2017},
pages = {207 - 225},
issn = {00985589},
abstract = {The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage the adoption of live update. This paper presents Mutable Checkpoint-Restart (MCR), a new live update solution for generic (multiprocess and multithreaded) server programs written in C. Compared to prior solutions, MCR can support arbitrary software updates and automate most of the common live update operations. The key idea is to allow the running version to safely reach a quiescent state and then allow the new version to restart as similarly to a fresh program initialization as possible, relying on existing code paths to automatically restore the old program threads and reinitialize a relevant portion of the program data structures. To transfer the remaining data structures, MCR relies on a combination of precise and conservative garbage collection techniques to trace all the global pointers and apply the required state transformations on the fly. Experimental results on popular server programs (Apache httpd, nginx, OpenSSH and vsftpd) confirm that our techniques can effectively automate problems previously deemed difficult at the cost of negligible performance overhead (2 percent on average) and moderate memory overhead (3.9\times on average, without optimizations).<br/> &copy; 1976-2012 IEEE.},
key = {Refuse collection},
keywords = {Data structures;},
note = {checkpoint-restart;Garbage collection;Live updates;Memory overheads;Quiescent state;Record-replay;Software updates;State transformation;},
URL = {http://dx.doi.org/10.1109/TSE.2016.2584066},
} 


@inbook{20172703869334,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Scaling agile mechatronics: An industrial case study},
journal = {Continuous software engineering},
author = {Lantz, Jonn and Eliasson, Ulf},
volume = {9783319112831},
year = {2014},
pages = {211 - 222},
abstract = {The automotive industry is currently in a state of rapid change. The traditional mechanical industry has, forced by electronic revolution and global threats of climate change, transformed into a computerized electromechanical industry. A hybrid or electric car of 2013 can have, in the order of 100 electronic control units, running gigabytes of code, working together in a complex network within the car as well as being connected to networks in the world outside. This exponential increase of software has posed new challenges for the R&amp;D organizations. In many cases the commonly used method of requirement engineering towards external suppliers in a waterfall process has shown to be unmanageable. Part of the solution has been to introduce more in-house software development and the new standardized platform for embedded software, AUTOSAR. During the past few years, Volvo Cars has focused on techniques and processes for continuous integration of embedded software for active safety, body functions, and motor and hybrid technology. The feedback times for ECU system test have decreased from months to, in the best cases, hours. Domain-specific languages (DSL), for both software and physical models, have been used to great extent when developing in-house embedded software at Volvo Cars. The main reasons are the close connection with mechatronic systems (motors, powertrain, servos, etc.), the advantage of having domain experts (not necessarily software experts) developing control software, and the facilitated reuse of algorithms. Model-driven engineering also provides a method for agile development and early learning in projects where hardware and mechanics usually are available only late. Model-based testing of the software is performed, both as pure simulation (MIL) and in hardware-in-the-loop (HIL) rigs, before it is deployed in real cars. This testing is currently being automated for several rigs, as part of the continuous integration strategy. The progress is, however, not without challenges. Details of the work split with Tier 1 suppliers, using the young AUTOSAR standard, and the efficiency of AUTOSAR code are still open problems. Another challenge is to manage the complex model framework required for virtual verification when applied on system level and numerous DSLs have to be executed together.<br/> &copy; 2014 Springer International Publishing Switzerland. All rights reserved.},
key = {Software design},
keywords = {Agile manufacturing systems;Automotive industry;Climate change;Complex networks;Computer software reusability;Digital subscriber lines;Electric machine control;Embedded software;Embedded systems;Hardware;Integration testing;Model checking;Problem oriented languages;Traction motors;},
note = {Continuous integrations;Domain specific language (DSL);Electromechanical industry;Electronic control units;In-house software development;Industrial case study;Model-driven Engineering;Requirement engineering;},
URL = {http://dx.doi.org/10.1007/978-3-319-11283-1-17},
} 


@inproceedings{20171103435478,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Predictive modeling methodology for compiler phase-ordering},
journal = {ACM International Conference Proceeding Series},
author = {Ashouri, Amir Hossein and Bignoli, Andrea and Palermo, Gianluca and Silvano, Cristina},
year = {2016},
pages = {7 - 12},
address = {Prague, Czech republic},
abstract = {Today's compilers offer a huge number of transformation options to choose among and this choice can significantly impact on the performance of the code being optimized. Not only the selection of compiler options represents a hard problem to be solved, but also the ordering of the phases is adding further complexity, making it a long standing problem in compilation research. This paper presents an innovative approach for tackling the compiler phase-ordering problem by using predictive modeling. The proposed methodology enables i) to efficiently explore compiler exploration space including optimization permutations and repetitions and ii) to extract the application dynamic features to predict the next-best optimization to be applied to maximize the performance given the current status. Experimental results are done by assessing the proposed methodology with utilizing two different search heuristics on the compiler optimization space and it demonstrates the effectiveness of the methodology on the selected set of applications. Using the proposed methodology on average we observed up to 4% execution speedup with respect to LLVM standard baseline.<br/> &copy; 2016 ACM.},
key = {Program compilers},
keywords = {Embedded systems;Heuristic algorithms;Heuristic methods;Industrial management;Learning systems;Multicore programming;Optimization;Parallel architectures;Parallel programming;},
note = {Autotuning;Compiler optimizations;Dynamic features;Innovative approaches;Phase Ordering;Predictive modeling;Search heuristics;Standing problems;},
URL = {http://dx.doi.org/10.1145/2872421.2872424},
} 


@inproceedings{20180604778019,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A common backend for hardware acceleration on FPGA},
journal = {Proceedings - 35th IEEE International Conference on Computer Design, ICCD 2017},
author = {Del Sozzo, Emanuele and Baghdadi, Riyadh and Amarasinghe, Saman and Santambrogio, Marco D.},
year = {2017},
pages = {427 - 430},
address = {Boston, MA, United states},
abstract = {Field Programmable Gate Arrays (FPGAs) are configurable integrated circuits able to provide a good trade-off in terms of performance, power consumption, and flexibility with respect to other architectures, like CPUs, GPUs and ASICs. The main drawback in using FPGAs, however, is their steep learning curve. An emerging solution to this problem is to write algorithms in a Domain Specific Language (DSL) and to let the DSL compiler generate efficient code targeting FPGAs. This work proposes FROST, a unified backend that enables different DSL compilers to target FPGA architectures. Differently from other code generation frameworks targeting FPGA, FROST exploits a scheduling co-language that enables users to have full control over which optimizations to apply in order to generate efficient code (e.g. loop pipelining, array partitioning, vectorization). At first, FROST analyzes and manipulates the input Abstract Syntax Tree (AST) in order to apply FPGA-oriented transformations and optimizations, then generates a C/C++ implementation suitable for High-Level Synthesis (HLS) tools. Finally, the output of HLS phase is synthesized and implemented on the target FPGA using Xilinx SDAccel toolchain. The experimental results show a speedup up of 15 with respect to O3-optimized implementations of the same algorithms on CPU.<br/> &copy; 2017 IEEE.},
key = {Field programmable gate arrays (FPGA)},
keywords = {C++ (programming language);Codes (symbols);Digital subscriber lines;DSL;Economic and social effects;High level synthesis;Logic Synthesis;Problem oriented languages;Program compilers;Scheduling;Trees (mathematics);},
note = {Abstract Syntax Trees;Back ends;Domain specific language (DSL);Field programmable gate array (FPGAs);FROST;Hardware acceleration;Optimized implementation;Oriented transformation;},
URL = {http://dx.doi.org/10.1109/ICCD.2017.75},
} 


@inproceedings{20141317516621,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Reliable implementation of linear filters with fixed-point arithmetic},
journal = {IEEE Workshop on Signal Processing Systems, SiPS: Design and Implementation},
author = {Hilaire, Thibault and Lopez, Benoit},
year = {2013},
pages = {401 - 406},
issn = {15206130},
address = {Taipei, Taiwan},
abstract = {This article deals with the implementation of linear filters or controllers with fixed-point arithmetic. The finite precision of the computations and the roundoff errors induced may have an important impact on the numerical behavior of the implemented system. Moreover, the fixed-point transformation is a time consuming and errorprone task, specially with the objective of minimizing the quantization impact. Based on a formalism able to describe every structure of linear filters/controllers, this paper proposes an automatic method to generate fixed-point version of the inputs-to-outputs algorithm and an analysis of the global error added on the output. An example illustrates the approach. &copy; 2013 IEEE.<br/>},
key = {Fixed point arithmetic},
keywords = {Bandpass filters;Error analysis;Errors;Signal filtering and prediction;Silicon compounds;},
note = {Automatic method;Code Generation;Error prones;Filter implementation;Finite precision;Global errors;Linear filters;Round-off errors;},
} 


@inproceedings{20180304659013,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Reusable Specification Templates for Defining Dynamic Semantics of DSLs},
journal = {Proceedings - ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems, MODELS 2017},
author = {Tikhonova, Ulyana},
year = {2017},
pages = {74 - },
address = {Austin, TX, United states},
abstract = {In the context of Model Driven Engineering (MDE), the dynamic (execution) semantics of domain specific languages (DSLs) is usually not specified explicitly and stays (hard)coded in model transformations and code generation. This poses challenges such as learning, debugging, understanding, maintaining, and updating a DSL. Facing the lack of supporting tools for specifying the dynamic semantics of DSLs (or programming languages in general), we propose to specify the architecture and the detailed design of the software that implements the DSL, rather than requirements for the behavior expected from DSL programs. To compose such a specification we use specification templates that capture software design solutions typical for the (application) domain of the DSL. As a result, on the one hand, our approach allows for an explicit and clear definition of the dynamic semantics of a DSL, supports separation of concerns and reuse of typical design solutions. On the other hand, we do not introduce (yet another) specification formalism, but we base our approach on an existing formalism and apply its extensive tool support for verification and validation to the dynamic semantics of a DSL.<br/> &copy; 2017 IEEE.},
key = {Digital subscriber lines},
keywords = {Application programs;Computer software reusability;Graphical user interfaces;Problem oriented languages;Semantics;Software design;Specifications;},
note = {Design solutions;Domain specific languages;Dynamic semantic;Model transformation;Model-driven Engineering;Reusable specifications;Separation of concerns;Verification-and-validation;},
URL = {http://dx.doi.org/10.1109/MODELS.2017.37},
} 


@article{20171203483957,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Reusable specification templates for defining dynamic semantics of DSLs},
journal = {Software and Systems Modeling},
author = {Tikhonova, Ulyana},
year = {2017},
pages = {1 - 30},
issn = {16191366},
abstract = {In the context of model-driven engineering, the dynamic (execution) semantics of domain-specific languages (DSLs) is usually not specified explicitly and stays (hard)coded in model transformations and code generation. This poses challenges such as learning, debugging, understanding, maintaining, and updating a DSL. Facing the lack of supporting tools for specifying the dynamic semantics of DSLs (or programming languages in general), we propose to specify the architecture and the detailed design of the software that implements the DSL, rather than requirements for the behavior expected from DSL programs. To compose such a specification, we use specification templates that capture software design solutions typical for the (application) domain of the DSL. As a result, on the one hand, our approach allows for an explicit and clear definition of the dynamic semantics of a DSL, supports separation of concerns and reuse of typical design solutions. On the other hand, we do not introduce (yet another) specification formalism, but we base our approach on an existing formalism and apply its extensive tool support for verification and validation to the dynamic semantics of a DSL.<br/> &copy; 2017 The Author(s)},
key = {Digital subscriber lines},
keywords = {Application programs;Aspect oriented programming;Computer software reusability;Graphical user interfaces;Problem oriented languages;Semantics;Software design;Specifications;},
note = {Domain specific languages;Dynamic semantic;Generic programming;Model transformation;Model-driven Engineering;Reusable specifications;Separation of concerns;Verification-and-validation;},
URL = {http://dx.doi.org/10.1007/s10270-017-0590-0},
} 


@inproceedings{20184506040169,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards automatic complex feature engineering},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Zhang, Jianyu and Fogelman-Soulie, Francoise and Largeron, Christine},
volume = {11234 LNCS},
year = {2018},
pages = {312 - 322},
issn = {03029743},
address = {Dubai, United arab emirates},
abstract = {Feature engineering is one of the most difficult and time-consuming tasks in data mining projects, and requires strong expert knowledge. Existing feature engineering techniques tend to use limited numbers of simple feature transformation methods and validate on simple datasets (small volume, simple structure), obviously limiting the benefits of feature engineering. In this paper, we propose a general Automatic Feature Engineering Machine framework (AFEM for short), which defines families of complex features and introduces them one family at a time (block bottom-up). We show that this framework covers most of the existing features used in the literature and allows us to efficiently generate complex feature families: in particular, local time, social network and representation-based families for relational and graph datasets, as well as composition of features. We validate our approach on two large realistic competitions datasets and a recommendation system task with social network. In the first two tasks, AFEM automatically reached ranks 15 and 12 compared to human teams; in the last task, it achieved 1.5% regression error reduction, compared to best results in the literature. Furthermore, in the context of big data and web applications, by balancing computation time and number of features/performance, in one case, we could reduce 2/3 computation time with only 0.2% AUC performance loss. Our code is publicly available on GitHub (https://github.com/TjuJianyu/AFEM).<br/> &copy; Springer Nature Switzerland AG 2018.},
key = {Balancing},
keywords = {Big data;Complex networks;Data mining;Engineering education;HTTP;Information systems;Information use;Learning systems;Social networking (online);Social sciences computing;Systems engineering;},
note = {Expert knowledge;Feature engineerings;Feature transformations;Network computing;Regression errors;Simple structures;Time-consuming tasks;WEB application;},
URL = {http://dx.doi.org/10.1007/978-3-030-02925-8_22},
} 


@inproceedings{20150200410070,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mutable checkpoint-restart: Automating live update for generic server programs},
journal = {Proceedings of the 15th International Middleware Conference, Middleware 2014},
author = {Giuffrida, Cristiano and Iorgulescu, Clin and Tanenbaum, Andrew S.},
year = {2014},
pages = {133 - 144},
address = {Bordeaux, France},
abstract = {The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage live update adoption. This paper presents Mutable Checkpoint-Restart (MCR), a new live update solution for generic (multiprocess and multithreaded) server programs written in C. Compared to prior solutions, MCR can support arbitrary software updates and automate most of the common live update operations. The key idea is to allow the new version to restart as similarly to a fresh program initialization as possible, relying on existing code paths to automatically restore the old program threads and reinitialize a relevant portion of the program data structures. To transfer the remaining data structures, MCR relies on a combination of precise and conservative garbage collection techniques to trace all the global pointers and apply the required state transformations on the fly. Experimental results on popular server programs (Apache httpd, nginx, OpenSSH and vsftpd) confirm that our techniques can effectively automate problems previously deemed difficult at the cost of negligible run-time performance overhead (2% on average) and moderate memory overhead (3.9x on average).<br/> Copyright &copy; 2014 ACM.},
key = {Middleware},
keywords = {Data structures;Embedded systems;Refuse collection;},
note = {Garbage collection;Live updates;Memory overheads;Record-replay;Run-time performance;Server programs;Software updates;State transformation;},
URL = {http://dx.doi.org/10.1145/2663165.2663328},
} 


@article{20131216138920,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Beam model refinement and reduction},
journal = {Engineering Structures},
author = {Medic, Senad and Dolarevic, Samir and Ibrahimbegovic, Adnan},
volume = {50},
year = {2013},
pages = {158 - 169},
issn = {01410296},
abstract = {In this paper we present a method for systematic construction of the stiffness matrix of an arbitrary spatial frame element by performing a series of elementary transformations. The procedure of this kind is capable of including a number of element refinements (addition of shear deformation, variable cross-section, etc.) that are not easily accessible to standard displacement-based method. We also discuss the necessary modifications of the element stiffness matrix in order to accommodate different constraints, such as point constraints in terms of joint releases (or hinges) for moments or shear forces. This is obtained by means of model reduction providing a more effective approach than the alternative one in which the global number of degrees of freedom has to be increased by one for each new release. Finally, we elaborate upon the global constraints imposing the length-invariant deformation of frame elements with an arbitrary position in space. Several numerical examples are used to illustrate the performance of the proposed procedures. The computations are carried out by a modified version of computer code CAL. &copy; 2012 Elsevier Ltd.<br/>},
key = {Stiffness matrix},
keywords = {Degrees of freedom (mechanics);Hinges;Linear transformations;Shear deformation;Stiffness;},
note = {Beam model;Displacement-based method;Effective approaches;Element stiffness matrix;Elementary transformation;Length invariances;Number of degrees of freedom;Variable cross section;},
URL = {http://dx.doi.org/10.1016/j.engstruct.2012.10.004},
} 


@inproceedings{20124315606637,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A meta-model for DEVS: Designed following Model Driven Engineering specifications},
journal = {SIMULTECH 2012 - Proceedings of the 2nd International Conference on Simulation and Modeling Methodologies, Technologies and Applications},
author = {Garredu, Stephane and Vittori, Evelyne and Santucci, Jean-Francois and Bisgambiglia, Paul-Antoine},
year = {2012},
pages = {152 - 157},
address = {Rome, Italy},
abstract = {In this paper we give a state-of-art of DEVS components interoperability, and we propose a meta-model for classic DEVS formalism, designed following a Model-Driven Engineering philosophy. After glancing at the existing related works, we explain in a step-by-step way how our meta-model is built, starting from the formal definition of DEVS formalism. As the hardest steps when defining a DEVS Platform-Independent Model (PIM) are the definition of the states and the definition of the DEVS functions, we particularly focus on those concepts and we propose a way to describe them in a simple and platform-independent way. UML class diagrams were chosen to represent this meta-model. Not only can this meta-model be useful to generate DEVS PIMs but it can also be seen as a powerful tool to improve interoperability between DEVS models (and in a larger way discrete-event models, via model-to-model transformations) and to provide automatic code generation towards DEVS simulators (model-to-text transformations). As this meta-model is not a final version but rather a starting point, we tried to make it as modular and upgradable as possible.<br/>},
key = {Discrete event simulation},
keywords = {Automatic programming;Computer aided software engineering;Interoperability;Metalorganic frameworks;Software architecture;Systems analysis;},
note = {DEVS;Devs interoperability;Meta model;Model to model transformation;Model to text transformations;OMG MTL;},
} 


@inproceedings{20130916072843,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Management of the quality of teaching at Universities - A course for teachers at the Technical University of Kosice developed within the operational programme 'Education'},
journal = {ICETA 2012 - 10th IEEE International Conference on Emerging eLearning Technologies and Applications, Proceedings},
author = {Blasko, Michal and Raschman, Pavel},
year = {2012},
pages = {45 - 48},
address = {Stara Lesna, Slovakia},
abstract = {The aim of this contribution is to provide the basic information concerning the course 'Management of the Quality of Teaching at Universities'. This course was prepared within the framework of the operational programme Education 'Package of Innovative Elements for the Transformation of Education at TUKE' (code ITMS 26110230018). In recent weeks we finished the implementation of the pilot course that was focused to the development of professional skills of teachers and to the enhancement of quality education at TUKE (Technical University of Kosice). &copy; 2012 IEEE.<br/>},
key = {Teaching},
keywords = {E-learning;},
note = {Educational process;Information concerning;Management of qualities;Operational programmes;Pilot course;Professional skills;Quality education;Quality of teaching;Technical universities;},
URL = {http://dx.doi.org/10.1109/ICETA.2012.6418284},
} 


@inproceedings{20132116347205,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Complex inference in neural circuits with probabilistic population codes and topic models},
journal = {Advances in Neural Information Processing Systems},
author = {Beck, Jeff and Heller, Katherine and Pouget, Alexandre},
volume = {4},
year = {2012},
pages = {3059 - 3067},
issn = {10495258},
address = {Lake Tahoe, NV, United states},
abstract = {Recent experiments have demonstrated that humans and animals typically reason probabilistically about their environment. This ability requires a neural code that represents probability distributions and neural circuits that are capable of implementing the operations of probabilistic inference. The proposed probabilistic population coding (PPC) framework provides a statistically efficient neural representation of probability distributions that is both broadly consistent with physiological measurements and capable of implementing some of the basic operations of probabilistic inference in a biologically plausible way. However, these experiments and the corresponding neural models have largely focused on simple (tractable) probabilistic computations such as cue combination, coordinate transformations, and decision making. As a result it remains unclear how to generalize this framework to more complex probabilistic computations. Here we address this short coming by showing that a very general approximate inference algorithm known as Variational Bayesian Expectation Maximization can be naturally implemented within the linear PPC framework. We apply this approach to a generic problem faced by any given layer of cortex, namely the identification of latent causes of complex mixtures of spikes. We identify a formal equivalent between this spike pattern demixing problem and topic models used for document classification, in particular Latent Dirichlet Allocation (LDA). We then construct a neural network implementation of variational inference and learning for LDA that utilizes a linear PPC. This network relies critically on two non-linear operations: divisive normalization and super-linear facilitation, both of which are ubiquitously observed in neural circuits. We also demonstrate how online learning can be achieved using a variation of Hebb's rule and describe an extension of this work which allows us to deal with time varying and correlated latent causes.<br/>},
key = {Probability distributions},
keywords = {Codes (symbols);Complex networks;Decision making;Inference engines;Information retrieval systems;Maximum principle;Statistics;Timing circuits;},
note = {Approximate inference;Co-ordinate transformation;Document Classification;Latent dirichlet allocations;Neural representations;Physiological measurement;Probabilistic computation;Probabilistic inference;},
} 


@inproceedings{20140717317074,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Personalized defect prediction},
journal = {2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings},
author = {Jiang, Tian and Tan, Lin and Kim, Sunghun},
year = {2013},
pages = {279 - 289},
address = {Palo Alto, CA, United states},
abstract = {Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction - building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java - the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification. &copy; 2013 IEEE.<br/>},
key = {C (programming language)},
keywords = {Codes (symbols);Computer operating systems;Defects;Forecasting;Learning systems;Software reliability;},
note = {Classification technique;Defect patterns;Defect prediction;Prediction model;Prediction performance;Proof of concept;Software defects;Software project;},
URL = {http://dx.doi.org/10.1109/ASE.2013.6693087},
} 


@inproceedings{20185006221563,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Are social network sites the future of engineering design education?},
journal = {Proceedings of the 20th International Conference on Engineering and Product Design Education, E and PDE 2018},
author = {Brisco, Ross and Whitfield, Robert and Grierson, Hilary},
year = {2018},
address = {London, United kingdom},
abstract = {This paper presents how online social network sites (SNSs) are being used by students in distributed engineering design teams to support design activities; and its implications for the future of design education. Ethnographic studies of a Global Design Project (GDP) were conducted from 2015-2017 to collect information on the growing use of SNSs by students. Team diaries were kept, systematically recording observations, and students reported their personal experiences in reports. Nvivo 11 was utilised to code data and make conclusions on team&rsquo;s collaborative behaviour, and their successes and failures with the technologies used. This study has revealed that students of the GDP have made a change in the way they collaborate by means of SNSs. Evidence shows that students are able to utilise the functionality of SNSs to support the design process, design activities and design thinking. The growth of SNSs within academia and industry suggest that students will need to utilise the technology or at least the functionalities of SNSs in the future. It is important to question how future engineering design education might be delivered and how social network site functionality can be best used.<br/> &copy; 2018 Institution of Engineering Designers The Design Society. All Rights Reserved.},
key = {Social networking (online)},
keywords = {Product design;Students;},
note = {Collaborative design;Distributed engineering;Engineering design;Engineering design education;On-line social networks;Personal experience;Project based learning;Social Network Sites;},
} 


@article{20160902037133,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Survey of static software defect prediction},
journal = {Ruan Jian Xue Bao/Journal of Software},
author = {Chen, Xiang and Gu, Qing and Liu, Wang-Shu and Liu, Shu-Long and Ni, Chao},
volume = {27},
number = {1},
year = {2016},
pages = {1 - 25},
issn = {10009825},
abstract = {Static software defect prediction is an active research topic in the domain of software engineering data mining. The phases of the study include designing novel code or process metrics to characterize the faults in the program modules, constructing software defect prediction model based on the training data gathered after mining software historical repositories, using the trained model to predict potential defect-proneness of program modules. The research on software defect prediction can optimize the allocation of testing resources and improve the quality of software. This paper offers a systematic survey of existing research achievements of the domestic and foreign researchers in recent years. First, a research framework is proposed and three key factors (i.e., metrics, model construction approaches, and issues in datasets) influencing the performance of defect prediction are identified. Next, existing research achievements in these three key factors are discussed in sequence. Then, the existing achievements on a special defect prediction issues (i.e., code change based defect prediction) are summarized. Finally a perspective of the future work in this research area is discussed.<br/> &copy; Copyright 2016, Institute of Software, the Chinese Academy of Sciences. All rights reserved.},
key = {Software testing},
keywords = {Computer software selection and evaluation;Data mining;Defects;Engineering research;Forecasting;Learning systems;Quality assurance;Surveys;},
note = {Data preprocessing;Quality of softwares;Research achievements;Research frameworks;Software defect prediction;Software engineering data;Software metrics;Software quality assurance;},
URL = {http://dx.doi.org/10.13328/j.cnki.jos.004923},
} 


@article{20130916061649,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Multi-feature extraction and selection in writer-independent off-line signature verification},
journal = {International Journal on Document Analysis and Recognition},
author = {Rivard, Dominique and Granger, Eric and Sabourin, Robert},
volume = {16},
number = {1},
year = {2013},
pages = {83 - 103},
issn = {14332833},
abstract = {Some of the fundamental problems faced in the design of signature verification (SV) systems include the potentially large number of input features and users, the limited number of reference signatures for training, the high intra-personal variability among signatures, and the lack of forgeries as counterexamples. In this paper, a new approach for feature selection is proposed for writer-independent (WI) off-line SV. First, one or more preexisting techniques are employed to extract features at different scales. Multiple feature extraction increases the diversity of information produced from signature images, allowing to produce signature representations that mitigate intra-personal variability. Dichotomy transformation is then applied in the resulting feature space to allow for WI classification. This alleviates the challenges of designing off-line SV systems with a limited number of reference signatures from a large number of users. Finally, boosting feature selection is used to design low-cost classifiers that automatically select relevant features while training. Using this global WI feature selection approach allows to explore and select from large feature sets based on knowledge of a population of users. Experiments performed with real-world SV data comprised of random, simple, and skilled forgeries indicate that the proposed approach provides a high level of performance when extended shadow code and directional probability density function features are extracted at multiple scales. Comparing simulation results to those of off-line SV systems found in literature confirms the viability of the new approach, even when few reference signatures are available. Moreover, it provides an efficient framework for designing a wide range of biometric systems from limited samples with few or no counterexamples, but where new training samples emerge during operations. &copy; 2011 Springer-Verlag.<br/>},
key = {Feature extraction},
keywords = {Biometrics;Character recognition;Classification (of information);Data mining;Decision trees;Extraction;Personnel training;Probability density function;},
note = {Boosting;Decision tree classification;Handwriting recognition;Incremental learning;Signature verification;},
URL = {http://dx.doi.org/10.1007/s10032-011-0180-6},
} 


@inproceedings{20132216366748,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evaluating the conventional wisdom in clone removal: A genealogy-based empirical study},
journal = {Proceedings of the ACM Symposium on Applied Computing},
author = {Zibran, Minhaz F. and Saha, Ripon K. and Roy, Chanchal K. and Schneider, Kevin A.},
year = {2013},
pages = {1123 - 1130},
address = {Coimbra, Portugal},
abstract = {Clone management has drawn immense interest from the research community in recent years. It is recognized that a deep understanding of how code clones change and are refactored is necessary for devising effective clone management tools and techniques. This paper presents an empirical study based on the clone genealogies from a significant number of releases of six software systems, to characterize the patterns of clone change and removal in evolving software systems. With a blend of qualitative analysis, quantitative analysis and statistical tests of significance, we address a number of research questions. Our findings reveal insights into the removal of individual clone fragments and provide empirical evidence in support of conventional clone evolution wisdom. The results can be used to devise informed clone management tools and techniques. Copyright 2013 ACM.<br/>},
key = {Cloning},
keywords = {Codes (symbols);Computer software;History;Reengineering;},
note = {Clone management;Clone removals;Empirical studies;Qualitative analysis;Refactorings;Research communities;Research questions;Software systems;},
URL = {http://dx.doi.org/10.1145/2480362.2480573},
} 


@article{20120714767808,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Quantifying rates of landscape evolution and tectonic processes by thermochronology and numerical modeling of crustal heat transport using PECUBE},
journal = {Tectonophysics},
author = {Braun, Jean and van der Beek, Peter and Valla, Pierre and Robert, Xavier and Herman, Frederic and Glotzbach, Christoph and Pedersen, Vivi and Perry, Claire and Simon-Labric, Thibaud and Prigent, Cecile},
volume = {524-525},
year = {2012},
pages = {1 - 28},
issn = {00401951},
abstract = {PECUBE is a three-dimensional thermal-kinematic code capable of solving the heat production-diffusion-advection equation under a temporally varying surface boundary condition. It was initially developed to assess the effects of time-varying surface topography (relief) on low-temperature thermochronological datasets. Thermochronometric ages are predicted by tracking the time-temperature histories of rock-particles ending up at the surface and by combining these with various age-prediction models. In the decade since its inception, the PECUBE code has been under continuous development as its use became wider and addressed different tectonic-geomorphic problems. This paper describes several major recent improvements in the code, including its integration with an inverse-modeling package based on the Neighborhood Algorithm, the incorporation of fault-controlled kinematics, several different ways to address topographic and drainage change through time, the ability to predict subsurface (tunnel or borehole) data, prediction of detrital thermochronology data and a method to compare these with observations, and the coupling with landscape-evolution (or surface-process) models. Each new development is described together with one or several applications, so that the reader and potential user can clearly assess and make use of the capabilities of PECUBE. We end with describing some developments that are currently underway or should take place in the foreseeable future. &copy; 2012 Elsevier B.V.<br/>},
key = {Inverse problems},
keywords = {Codes (symbols);Forecasting;Heat transfer;Kinematics;Numerical models;Surface topography;Tectonics;Temperature;},
note = {Continuous development;Detrital thermochronology;Diffusion advections;Neighborhood algorithm;PECUBE;Surface boundary conditions;Thermochronology;Time-temperature history;},
URL = {http://dx.doi.org/10.1016/j.tecto.2011.12.035},
} 


@inproceedings{20170903403837,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Runtime Verification of Scientific Computing: Towards an Extreme Scale},
journal = {Proceedings of ESPT 2016: 5th Workshop on Extreme-Scale Programming Tools - Held in conjunction with SC 2016: The International Conference for High Performance Computing, Networking, Storage and Analysis},
author = {Dinh, Minh Ngoc and Jin, Chao and Abramson, David and Jeffery, Clinton L.},
year = {2016},
pages = {26 - 33},
address = {Salt Lake City, UT, United states},
abstract = {Relative debugging helps trace software errors by comparing two concurrent executions of a program-one code being a reference version and the other faulty. By locating data divergence between the runs, relative debugging is effective at finding coding errors when a program is scaled up to solve larger problem sizes or migrated from one platform to another. In this work, we envision potential changes to our current relative debugging scheme in order to address exascale factors such as the increase of faults and the nondeterministic outputs. First, we propose a statistical-based comparison scheme to support verifying results that are stochastic. Second, we leverage a scalable data reduction network to adapt to the complex network hierarchy of an exascale system, and extend our debugger to support the statistical-based comparison in an environment subject to failures.<br/> &copy; 2016 IEEE.},
key = {Program debugging},
keywords = {Coding errors;Complex networks;Digital storage;Stochastic systems;},
note = {Concurrent execution;Exascale computing;Extreme scale;Network hierarchies;On-line verifications;Potential change;Run-time verification;Software errors;},
URL = {http://dx.doi.org/10.1109/ESPT.2016.008},
} 


@inproceedings{20151700788949,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Lunar micro rover design for exploration through virtual reality tele-operation},
journal = {Springer Tracts in Advanced Robotics},
author = {Britton, Nathan and Yoshida, Kazuya and Walker, John and Nagatani, Keiji and Taylor, Graeme and Dauphin, Loic},
volume = {105},
year = {2015},
pages = {259 - 272},
issn = {16107438},
address = {Brisbane, QLD, Australia},
abstract = {A micro rover, code-named Moonraker, was developed to demonstrate the feasibility of 10kg-class lunar rover missions. Requirements were established based on the Google Lunar X-Prize mission guidelines in order to effectively evaluate the prototype. A 4-wheel skid steer configuration was determined to be effective to reduce mass, maximize regolith traversability, and fit within realistic restrictions on the rover&rsquo;s envelope by utilizing the top corners of the volume. A static, hyperbolic mirror-based omnidirectional camera was selected in order to provide full 360<sup>&deg;</sup>views around the rover, eliminating the need for a pan/tilt mechanism and motors. A front mounted, motorless MEMS laser scanner was selected for similar mass reduction qualities. A virtual reality interface is used to allow one operator to intuitively change focus between various narrow targets of interest within the wide set of fused data available from these sensors. Lab tests were conducted on the mobility system, as well as field tests at three locations in Japan and Mauna Kea. Moonraker was successfully teleoperated to travel over 900m up and down a peak with slopes of up to 15&deg;. These tests demonstrate the rover&rsquo;s capability to traverse across lunar regolith and gather sufficient data for effective situational awareness and near real-time tele-operation.<br/> &copy; Springer International Publishing Switzerland 2015.},
key = {Lunar missions},
keywords = {Lunar surface analysis;Robot learning;Robotics;Virtual reality;},
note = {Hyperbolic mirrors;Mass reduction;Mobility systems;Omnidirectional cameras;Situational awareness;Targets of interest;Traversability;Virtual reality interfaces;},
URL = {http://dx.doi.org/10.1007/978-3-319-07488-7_18},
} 


@inproceedings{20133916775972,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Developing a summer bridge course for improving retention in engineering},
journal = {ASEE Annual Conference and Exposition, Conference Proceedings},
author = {Volcy, Jerry and Sidbury, Carmen},
year = {2013},
issn = {21535965},
address = {Atlanta, GA, United states},
abstract = {This paper outlines the details of a summer bridge, project-based, cooperative, introduction to engineering pilot course developed and successfully implemented at Spelman College in an effort to increase the retention rate of students to be enrolled in its dual-degree engineering program. The course aims to expose incoming students of any STEM discipline to a broad array of practical and theoretical engineering principles for the purpose of helping students make informed decisions about pursuing engineering as a study major prior to the start of their freshman year. To satisfy this objective, the cross-disciplinary course that was developed is based on completing a software-driven, electro-mechanical engineering project that, at various times and to various extents, calls upon students to function in the capacity of an electrical engineer, a mechanical engineer, a technician, a mathematician, a computer scientist, a researcher and a communicator of technical material. In so doing, the students gain insight about how engineers combine knowledge from these diverse disciplines to solve a real problem-in this case, constructing and characterizing a 2-DOF, servoed laser system used to trace arbitrary patterns against a wall. Using an "inverted curriculum" approach that by-passes the first two years of the classic engineering curriculum, the course immerses the students directly into an engineering design project in an attempt to capture, as closely as possible, the end-goal of engineering training while providing a window to the challenges and gratifications of engineering both in practice and in an R&amp;D setting. It was observed that, far from having the feared effect of driving the students to disinterest, challenging the students with a difficult curriculum of technical concepts to be used to solve a non-trivial but well-defined and tangible problem elicited high interest and thoughtful evaluations and re-evaluations of engineering as a study major. Details of the course, which involves building a circuit from a schematic, developing code for a multi-core microcontroller, learning and applying the concept of pulse-width-modulation to control servo motors and developing the required mathematical coordinate transformations to successfully control the orientation of the laser are discussed. &copy; American Society for Engineering Education, 2013.<br/>},
key = {Curricula},
keywords = {Engineering education;Engineers;Mathematical transformations;Students;Voltage control;},
note = {Co-ordinate transformation;Computer scientists;Engineering curriculum;Engineering design projects;Engineering training;Introduction to engineering;Multi-core microcontrollers;Theoretical engineering;},
} 


@inproceedings{20155201718858,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Metalearning to choose the level of analysis in nested data: A case study on error detection in foreign trade statistics},
journal = {Proceedings of the International Joint Conference on Neural Networks},
author = {Zarmehri, Mohammad Nozari and Soares, Carlos},
volume = {2015-September},
year = {2015},
address = {Killarney, Ireland},
abstract = {Traditionally, a single model is developed for a data mining task. As more data is being collected at a more detailed level, organizations are becoming more interested in having specific models for distinct parts of data (e.g. customer segments). From the business perspective, data can be divided naturally into different dimensions. Each of these dimensions is usually hierarchically organized (e.g. country, city, zip code), which means that, when developing a model for a given part of the problem (e.g. a zip code) the training data may be collected at different levels of this nested hierarchy (e.g. the same zip code, the city and the country it is located in). Selecting different levels of granularity may change the performance of the whole process, so the question is which level to use for a given part. We propose a metalearning model which recommends a level of granularity for the training data to learn the model that is expected to obtain the best performance. We apply decision tree and random forest algorithms for metalearning. At the base level, our experiment uses results obtained by outlier detection methods on the problem of detecting errors in foreign trade transactions. The results show that using metalearning help finding the best level of granularity.<br/> &copy; 2015 IEEE.},
key = {Data mining},
keywords = {Codes (symbols);Database systems;Decision trees;Error detection;Error statistics;Feature extraction;International trade;Metals;},
note = {Business perspective;Customer segments;Data mining tasks;Meta-learning models;Nested hierarchy;Outlier Detection;Random forest algorithm;Trade statistics;},
URL = {http://dx.doi.org/10.1109/IJCNN.2015.7280656},
} 


@inproceedings{20163502757987,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Regression test selection for android applications},
journal = {Proceedings - International Conference on Mobile Software Engineering and Systems, MOBILESoft 2016},
author = {Do, Quan and Yang, Guowei and Che, Meiru and Hui, Darren and Ridgeway, Jefferson},
year = {2016},
pages = {27 - 28},
address = {Austin, TX, United states},
abstract = {Mobile platform pervades human life, and much research in recent years has focused on improving the reliability of mobile applications on this platform, for example by applying automatic testing. However, researchers have primarily considered testing of single version of mobile applications. Although regression testing has been extensively studied for desktop applications, the approaches for desktop applications cannot be directly applied to mobile applications. Our approach leverages the combination of static impact analysis and dynamic code coverage information, and identifies a subset of test cases for re-execution on the modified app version. We implement our approach for Android apps, and illustrate its usefulness based on an Android application.<br/> &copy; 2016 Copyright held by the owner/author(s).},
key = {Android (operating system)},
keywords = {Automatic testing;Data flow analysis;Flow graphs;Mobile computing;Software testing;},
note = {Android applications;Change impact analysis;Control flow graphs;Coverage;Desktop applications;Mobile applications;Regression test selection;Regression testing;},
URL = {http://dx.doi.org/10.1145/2897073.2897127},
} 


@inproceedings{20124015488388,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automatic improvement of graph based image segmentation},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Vu, Huyen and Olsson, Roland},
volume = {7432 LNCS},
number = {PART 2},
year = {2012},
pages = {578 - 587},
issn = {03029743},
address = {Rethymnon, Crete, Greece},
abstract = {Automatic Design of Algorithms through Evolution (ADATE) is a system for fully automatic programming that has the ability to either generate algorithms from scratch or improve existing ones. In this paper, we employ ADATE to improve a standard image processing algorithm, namely graph based segmentation (GBS), which has emerged as one of the very most popular methods for image segmentation, that is partitioning an image into regions. The key contribution of the paper is to show that a proven and well-known computer vision code is easy to improve through automatic programming. This may presage a change to the entire field of computer vision where automatic programming becomes a routine way of improving standard as well as state-of-the art image processing and pattern analysis algorithms. GBS was mostly chosen as case study to investigate how useful the ADATE automatic programming system may be in computer vision. Numerous other algorithms in the field could have been chosen instead. &copy; 2012 Springer-Verlag.<br/>},
key = {Image enhancement},
keywords = {Automatic programming;Computer systems programming;Computer vision;Evolutionary algorithms;Graphic methods;Image processing;Image segmentation;Learning systems;},
note = {ADATE;Automatic design;Automatic programming systems;Graph-based image segmentations;Graph-based segmentation;Pattern analysis;Standard images;State of the art;},
URL = {http://dx.doi.org/10.1007/978-3-642-33191-6_57},
} 


@article{20165003119093,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {SUSPEND: Determining software suspiciousness by non-stationary time series modeling of entropy signals},
journal = {Expert Systems with Applications},
author = {Wojnowicz, Michael and Chisholm, Glenn and Wallace, Brian and Wolff, Matt and Zhao, Xuan and Luan, Jay},
volume = {71},
year = {2017},
pages = {301 - 318},
issn = {09574174},
abstract = {Commercial anti-virus software traditionally memorizes specific byte sequences (known as &ldquo;signatures&rdquo;) in the code of previously encountered malware. However, malware authors can evade signature-based detection in many ways; for instance, by using obfuscation techniques such as &ldquo;packing&rdquo; (encryption or compression) to hide snippets of malicious code; by writing metamorphic malware; or by tampering with existing malware. We hypothesize that certain evasion techniques can leave traces in the file's entropy signal, revealing either similarities to known malware or the presence of tampering per se. To this end, we present SUSPEND (SUSPicious ENtropy signal Detector), an expert system which evaluates the suspiciousness of an executable file's entropy signal in order to subserve malware classification. Whereas traditionally, entropy analysis has been used for the goal of packer detection (and therefore entropy-based features often merely comprise mean entropy or the entropy of a few file subcomponents), SUSPEND applies non-stationary time series modeling to aid in malware detection. In particular, SUSPEND (a) quantifies the &ldquo;amount of structure&rdquo; in the entropy signal (through detrended fluctuation analysis), (b) finds the location and size of sudden jumps in entropy (through mean change point modeling), and (c) computes the distribution of entropic variation across multiple spatial scales (through wavelet decomposition). In addition, SUSPEND (d) summarizes the entropy signal's empirical probability distribution. Because SUSPEND's run time can be made to scale linearly in file size, it is well-suited for large-scale malware analysis. We apply SUSPEND to a large-scale malware detection task with 500,000 heterogeneous real-world samples and over 1 million features. We find that SUSPEND boosts the predictive performance of traditional entropy analysis (as found in packer detectors) from 77.02% to 96.62%. Moreover, SUSPEND's focus on entropy signals make it a natural candidate for combining with other types of features; for instance, combining SUSPEND with a strings-based feature set boosts predictive accuracy from 97.18% to 98.62%. Thus, whereas traditionally, entropy analysis has focused on detecting that a file is packed, SUSPEND's more comprehensive representation of the entropy signal appears to be useful to help determine that a file is malicious. We illustrate the application of SUSPEND by studying 18 pieces of VirRansom, a family of viral ransomware which could cost millions to large organizations. SUSPEND is able to detect 100% of the studied files with over 99% confidence, whereas a more traditional strings-based model was very closed to undecided and represents the entire family with a single string.<br/> &copy; 2016},
key = {Malware},
keywords = {Computer crime;Cryptography;Entropy;Expert systems;Feature extraction;Learning systems;Packers;Probability distributions;Time series;Time series analysis;Wavelet decomposition;},
note = {Change-points;Detrended fluctuation analysis;Malware classifications;Non-stationary time series;Predictive accuracy;Predictive performance;Signature based detections;Wavelet;},
URL = {http://dx.doi.org/10.1016/j.eswa.2016.11.027},
} 


@inproceedings{20163502742333,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automatically Documenting Unit Test Cases},
journal = {Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation, ICST 2016},
author = {Li, Boyang and Vendome, Christopher and Linares-Vasquez, Mario and Poshyvanyk, Denys and Kraft, Nicholas A.},
year = {2016},
pages = {341 - 352},
address = {Chicago, IL, United states},
abstract = {Maintaining unit test cases is important during the maintenance and evolution of a software system. In particular, automatically documenting these unit test cases can ameliorate the burden on developers maintaining them. For instance, by relyingon up-to-date documentation, developers can more easily identify test cases that relate to some new or modified functionality of the system. We surveyed 212 developers (both industrial and open-source) to understand their perspective towards writing, maintaining, and documenting unit test cases. In addition, we mined change histories of C# software systems and empirically found that unit test methods seldom had preceding comments andinfrequently had inner comments, and both were rarely modified as those methods were modified. In order to support developers in maintaining unit test cases, we propose a novel approach-UnitTestScribe-that combines static analysis, natural language processing, backwardslicing, and code summarization techniques to automatically generate natural language documentation of unit test cases. We evaluated UnitTestScribe on four subject systems by means of an online survey with industrial developers and graduate students. In general, participants indicated that UnitTestScribe descriptions are complete, concise, and easy to read.<br/> &copy; 2016 IEEE.},
key = {Software testing},
keywords = {Natural language processing systems;Online systems;Open source software;Static analysis;Students;Surveys;Testing;Verification;},
note = {Change history;Graduate students;Natural languages;Online surveys;Open sources;Software systems;summarization;Unit tests;},
URL = {http://dx.doi.org/10.1109/ICST.2016.30},
} 


@inproceedings{20121114843709,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Algebraic foundations for effect-dependent optimisations},
journal = {Conference Record of the Annual ACM Symposium on Principles of Programming Languages},
author = {Kammar, Ohad and Plotkin, Gordon D.},
year = {2012},
pages = {349 - 360},
issn = {07308566},
address = {Philadelphia, PA, United states},
abstract = {We present a general theory of Gifford-style type and effect annotations, where effect annotations are sets of effects. Generality is achieved by recourse to the theory of algebraic effects, a development of Moggi's monadic theory of computational effects that emphasises the operations causing the effects at hand and their equational theory. The key observation is that annotation effects can be identified with operation symbols. We develop an annotated version of Levy's Call-by-Push-Value language with a kind of computations for every effect set; it can be thought of as a sequential, annotated intermediate language. We develop a range of validated optimisations (i.e., equivalences), generalising many existing ones and adding new ones. We classify these optimisations as structural, algebraic, or abstract: structural optimisations always hold; algebraic ones depend on the effect theory at hand; and abstract ones depend on the global nature of that theory (we give modularly-checkable sufficient conditions for their validity). Copyright &copy; 2012 ACM.<br/>},
key = {Computation theory},
keywords = {Algebra;Cosine transforms;Semantics;Structural optimization;},
note = {Algebraic theories;Call by push values;Code transformation;Computational effects;Denotational semantics;Domain theory;Inequational logic;Optimisations;Relevant and affine monads;Type and effect systems;},
URL = {http://dx.doi.org/10.1145/2103656.2103698},
} 


@inproceedings{20120814792309,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Algebraic foundations for effect-dependent optimisations},
journal = {ACM SIGPLAN Notices},
author = {Kammar, Ohad and Plotkin, Gordon D.},
volume = {47},
number = {1},
year = {2012},
pages = {349 - 360},
issn = {15232867},
abstract = {We present a general theory of Gifford-style type and effect annotations, where effect annotations are sets of effects. Generality is achieved by recourse to the theory of algebraic effects, a development of Moggi's monadic theory of computational effects that emphasises the operations causing the effects at hand and their equational theory. The key observation is that annotation effects can be identified with operation symbols. We develop an annotated version of Levy's Call-by-Push-Value language with a kind of computations for every effect set; it can be thought of as a sequential, annotated intermediate language. We develop a range of validated optimisations (i.e., equivalences), generalising many existing ones and adding new ones. We classify these optimisations as structural, algebraic, or abstract: structural optimisations always hold; algebraic ones depend on the effect theory at hand; and abstract ones depend on the global nature of that theory (we give modularly-checkable sufficient conditions for their validity). &copy; 2012 ACM.<br/>},
key = {Computation theory},
keywords = {Algebra;Cosine transforms;Semantics;Structural optimization;},
note = {Algebraic theories;Call by push values;Code transformation;Computational effects;Denotational semantics;Domain theory;Inequational logic;Optimisations;Relevant and affine monads;Type and effect systems;Universal algebra;},
URL = {http://dx.doi.org/10.1145/2103621.2103698},
} 


@article{20154901642338,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The effects of climate model similarity on probabilistic climate projections and the implications for local, risk-based adaptation planning},
journal = {Geophysical Research Letters},
author = {Steinschneider, Scott and McCrary, Rachel and Mearns, Linda O. and Brown, Casey},
volume = {42},
number = {12},
year = {2015},
pages = {5014 - 5022},
issn = {00948276},
abstract = {Approaches for probability density function (pdf) development of future climate often assume that different climate models provide independent information, despite model similarities that stem from a common genealogy (models with shared code or developed at the same institution). Here we use an ensemble of projections from the Coupled Model Intercomparison Project Phase 5 to develop probabilistic climate information, with and without an accounting of intermodel correlations, for seven regions across the United States. We then use the pdfs to estimate midcentury climate-related risks to a water utility in one of the regions. We show that the variance of climate changes is underestimated across all regions if model correlations are ignored, and in some cases, the mean change shifts as well. When coupled with impact models of the hydrology and infrastructure of a water utility, the underestimated likelihood of large climate changes significantly alters the quantification of risk for water shortages by midcentury.<br/> &copy; 2015. American Geophysical Union. All Rights Reserved.},
key = {Climate models},
keywords = {Climate change;History;Probability density function;Risk perception;Water supply;},
note = {adaptation;Climate information;Climate related risks;Coupled Model Intercomparison Project;Model correlation;Probabilistic climate projections;Probability density function (pdf);Water shortages;},
URL = {http://dx.doi.org/10.1002/2015GL064529},
} 


@article{20181004877980,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Empowering Professional Teaching in Engineering: Sustaining the Scholarship of Teaching},
journal = {Synthesis Lectures on Engineering},
author = {Heywood, John},
volume = {12},
number = {1},
year = {2018},
pages = {1 - 245},
issn = {19395221},
abstract = {Each one of us has views about education, how discipline should function, how individuals learn, how they should be motivated, what intelligence is, and the structures (content and subjects) of the curriculum. Perhaps the most important beliefs that (beginning) teachers bring with them are their notions about what constitutes "good teaching". The scholarship of teaching requires that (beginning) teachers should examine (evaluate) these views in the light of knowledge currently available about the curriculum and instruction, and decide their future actions on the basis of that analysis. Such evaluations are best undertaken when classrooms are treated as laboratories of inquiry (research) where teachers establish what works best for them. Two instructor centred and two learner centred philosophies of knowledge, curriculum and instruction are used to discern the fundamental (basic) questions that engineering educators should answer in respect of their own beliefs and practice. They point to a series of classroom activities that will enable them to challenge their own beliefs, and at the same time affirm, develop, or change their philosophies of knowledge, curriculum and instruction.<br/> Copyright &copy; 2018 by Morgan & Claypool.},
key = {Technical presentations},
keywords = {Animation;Communication;Curricula;Data storage equipment;Decision making;Design;Diagnosis;Engineering education;Engineering research;Grading;Maps;Motivation;Philosophical aspects;Reflection;Surfaces;Taxonomies;Teaching;Testing;},
note = {academic);accountability;Action research;Active Learning;advanced organiser;affective;answerability;assessment;attitudes;clusters;Code of Ethics;Cognitive dissonance;cognitive organisation;community;competence;complexity;concept (cartoons;content (syllabus);convergent;creativity;Critical thinking;debates;deep;discipline (s) (of knowledge);discovery;divergent;educational connoisseurship;emotional;Engineering educators;evaluation;Examinations (tests);examples;experts;expository instruction;expressive activities;heuristic(s);independent;Inquiry-based learning;Instructional designs;instructor centred;Intellectual development;intelligence ( applied;interdisciplinary;inventories;kinesthetic activities;knowing);knowledge (fields of;Laboratory work;learner;Learner-centred;learning (active;learning);lectures;Lesson planning;listening;mediating response;Mind maps;misperception;mock trials;modes of;negotiate(ion);novice(s);objectives (behavioral/focussing);originality;outcomes;paradigms;Peer teaching;perceptual;Personality types;Polya;practical;principles;prior procedural;professionalism (restricted/extended);qualitative thinking;questioning;questions;Reflective judgments;scholar academic ideology;Social efficiency;social reconstruction ideology;styles of);tacit;Teaching-as-research;},
URL = {http://dx.doi.org/10.2200/S00830ED1V01Y201802ENG029},
} 


@inproceedings{20121414921404,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Tracing your maintenance work - A cross-project validation of an automated classification dictionary for commit messages},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Mauczka, Andreas and Huber, Markus and Schanes, Christian and Schramm, Wolfgang and Bernhart, Mario and Grechenig, Thomas},
volume = {7212 LNCS},
year = {2012},
pages = {301 - 315},
issn = {03029743},
address = {Tallinn, Estonia},
abstract = {A commit message is a description of a change in a Version Control System (VCS). Besides the actual description of the change, it can also serve as an indicator for the purpose of the change, e.g. a change to refactor code might be accompanied by a commit message in the form of "Refactored class XY to improve readability". We would label the change in our example a perfective change, according to maintenance literature. This simplified example shows how it is possible to classify a change by its commit message. However, commit messages are unstructured, textual data and efforts to automatically label changes into categories like perfective have only been applied to a small set of projects within the same company or the same community. In this work, we present a cross-project evaluated and valid mapping of changes to the code base and their purpose that is usable without any customization on any open-source project. We provide further the Eclipse Plug-In Subcat which allows for a comfortable analysis of projects from within Eclipse. By using Subcat, we are able to automatically assess if a commit to the code was e.g. a bug fix or a refactoring. This information is very useful for e.g. developer profiling or locating bad smells in modules. &copy; 2012 Springer-Verlag Berlin Heidelberg.<br/>},
key = {Open systems},
keywords = {Codes (symbols);Open source software;},
note = {Automated classification;Bad smells;Bug fixes;Maintenance work;Open source projects;Refactorings;Textual data;Version control system;},
URL = {http://dx.doi.org/10.1007/978-3-642-28872-2_21},
} 


@inproceedings{20161102111832,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Tracing your maintenance work  a cross-project validation of an automated classification dictionary for commit messages},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Mauczka, Andreas and Huber, Markus and Schanes, Christian and Schramm, Wolfgang and Bernhart, Mario and Grechenig, Thomas},
volume = {7212},
year = {2012},
pages = {301 - 315},
issn = {03029743},
address = {Tallinn, Estonia},
abstract = {A commit message is a description of a change in a Version Control System (VCS). Besides the actual description of the change, it can also serve as an indicator for the purpose of the change, e.g. a change to refactor code might be accompanied by a commit message in the form of &ldquo;Refactored class XY to improve readability&rdquo;. We would label the change in our example a perfective change, according to maintenance literature. This simplified example shows how it is possible to classify a change by its commit message. However, commit messages are unstructured, textual data and efforts to automatically label changes into categories like perfective have only been applied to a small set of projects within the same company or the same community. In this work, we present a cross-project evaluated and valid mapping of changes to the code base and their purpose that is usable without any customization on any open-source project. We provide further the Eclipse Plug-In Subcat which allows for a comfortable analysis of projects from within Eclipse. By using Subcat, we are able to automatically assess if a commit to the code was e.g. a bug fix or a refactoring. This information is very useful for e.g. developer profiling or locating bad smells in modules.<br/> &copy; Springer-Verlag Berlin Heidelberg 2012.},
key = {Open systems},
keywords = {Codes (symbols);Open source software;},
note = {Automated classification;Bad smells;Bug fixes;Maintenance work;Open source projects;Refactorings;Textual data;Version control system;},
} 


@article{20173804193055,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automatic Contract Insertion with CCBot},
journal = {IEEE Transactions on Software Engineering},
author = {Carr, Scott A. and Logozzo, Francesco and Payer, Mathias},
volume = {43},
number = {8},
year = {2017},
pages = {701 - 714},
issn = {00985589},
abstract = {Existing static analysis tools require significant programmer effort. On large code bases, static analysis tools produce thousands of warnings. It is unrealistic to expect users to review such a massive list and to manually make changes for each warning. To address this issue we propose CCBot (short for CodeContracts Bot), a new tool that applies the results of static analysis to existing code through automatic code transformation. Specifically, CCBot instruments the code with method preconditions, postconditions, and object invariants which detect faults at runtime or statically using a static contract checker. The only configuration the programmer needs to perform is to give CCBot the file paths to code she wants instrumented. This allows the programmer to adopt contract-based static analysis with little effort. CCBot's instrumented version of the code is guaranteed to compile if the original code did. This guarantee means the programmer can deploy or test the instrumented code immediately without additional manual effort. The inserted contracts can detect common errors such as null pointer dereferences and out-of-bounds array accesses. CCBot is a robust large-scale tool with an open-source C# implementation. We have tested it on real world projects with tens of thousands of lines of code. We discuss several projects as case studies, highlighting undiscovered bugs found by CCBot, including 22 new contracts that were accepted by the project authors.<br/> &copy; 1976-2012 IEEE.},
key = {Static analysis},
keywords = {Codes (symbols);Cosine transforms;Object detection;},
note = {assertions;automated patching;Automatic codes;Class invariants;Instrumented code;Large code basis;Object invariants;Real world projects;},
URL = {http://dx.doi.org/10.1109/TSE.2016.2625248},
} 


@article{20184205944071,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Customized performance evaluation approach for Indian green buildings},
journal = {Building Research and Information},
author = {Gupta, Rajat and Gregg, Matt and Manu, Sanyogita and Vaidya, Prasad and Dixit, Maaz},
volume = {47},
number = {1},
year = {2019},
pages = {56 - 74},
issn = {09613218},
abstract = {The green building movement in India is lacking an important link: ensuring that the design intent of such buildings is actually realized. This paper undertakes an exploratory investigation to develop and test a customized building performance evaluation (BPE) approach (I-BPE framework) for the Indian context. As academia is considered to be an initial primary outlet of BPE, a survey of experts is conducted to investigate the drivers and barriers for implementing BPE-based methods in educational curricula. The I-BPE approach is tested in a case study building to gain insights for refining the underlying methods and processes for conducting further BPE studies in the context of India. The expert survey reveals the lack of trained people for teaching BPE as a key challenge to its adoption, implying that trained people are needed as much as frameworks. To enable widespread adoption of I-BPE in India, what will be necessary is a new cadre of building performance evaluators who can be trained (or up-skilled) through formal or continuing education. This will need to be driven by both policy (energy code) and market transformation (&lsquo;green&rsquo; rating systems). A series of delivery routes are suggested to enable rapid and deeper learning.<br/> &copy; 2018, &copy; 2018 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
key = {Buildings},
keywords = {Education;Energy efficiency;Surveys;},
note = {Building performance;Building performance evaluation;Continuing education;Green buildings;India;Market transformation;Performance evaluations;Postoccupancy evaluation (POE);},
URL = {http://dx.doi.org/10.1080/09613218.2019.1525962},
} 


@inproceedings{20131816286082,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Neural acceleration for general-purpose approximate programs},
journal = {Proceedings - 2012 IEEE/ACM 45th International Symposium on Microarchitecture, MICRO 2012},
author = {Esmaeilzadeh, Hadi and Sampson, Adrian and Ceze, Luis and Burger, Doug},
year = {2012},
pages = {449 - 460},
address = {Vancouver, BC, Canada},
abstract = {This paper describes a learning-based approach to the acceleration of approximate programs. We describe the \emph{Parrot transformation}, a program transformation that selects and trains a neural network to mimic a region of imperative code. After the learning phase, the compiler replaces the original code with an invocation of a low-power accelerator called a \emph{neural processing unit} (NPU). The NPU is tightly coupled to the processor pipeline to accelerate small code regions. Since neural networks produce inherently approximate results, we define a programming model that allows programmers to identify approximable code regions-code that can produce imprecise but acceptable results. Offloading approximable code regions to NPUs is faster and more energy efficient than executing the original code. For a set of diverse applications, NPU acceleration provides whole-application speedup of 2.3x and energy savings of 3.0x on average with quality loss of at most 9.6%. &copy; 2012 IEEE.<br/>},
key = {Pipeline processing systems},
keywords = {Acceleration;Codes (symbols);Computer architecture;Energy conservation;Energy efficiency;Neural networks;Particle accelerators;},
note = {Approximate computing;Approximate results;Diverse applications;Learning-based approach;Neural-processing;Processor pipelines;Program transformations;Programming models;},
URL = {http://dx.doi.org/10.1109/MICRO.2012.48},
} 


@inproceedings{20180904845470,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Impact of compiler phase ordering when targeting GPUs},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Nobre, Ricardo and Reis, Luis and Cardoso, Joao M. P.},
volume = {10659 LNCS},
year = {2018},
pages = {427 - 438},
issn = {03029743},
address = {Santiago de Compostela, Spain},
abstract = {Research in compiler pass phase ordering (i.e., selection of compiler analysis/transformation passes and their order of execution) has been mostly performed in the context of CPUs and, in a small number of cases, FPGAs. In this paper we present experiments regarding compiler pass phase ordering specialization of OpenCL kernels targeting NVIDIA GPUs using Clang/LLVM 3.9 and the libclc OpenCL library. More specifically, we analyze the impact of using specialized compiler phase orders on the performance of 15 PolyBench/GPU OpenCL benchmarks. In addition, we analyze the final NVIDIA PTX assembly code generated by the different compilation flows in order to identify the main reasons for the cases with significant performance improvements. Using specialized compiler phase orders, we were able to achieve performance improvements over the CUDA version and OpenCL compiled with the NVIDIA driver. Compared to CUDA, we were able to achieve geometric mean improvements of 1.54&times; (up to 5.48&times;). Compared to the OpenCL driver version, we were able to achieve geometric mean improvements of 1.65&times; (up to 5.70&times;).<br/> &copy; Springer International Publishing AG, part of Springer Nature 2018.},
key = {Program compilers},
keywords = {Benchmarking;Graphics processing unit;Optimization;},
note = {Assembly code;Compiler analysis;Geometric mean;Performance improvements;Phase Ordering;},
URL = {http://dx.doi.org/10.1007/978-3-319-75178-8_35},
} 


@inproceedings{20174704427406,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {SPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Merono-Penuela, Albert and Hoekstra, Rinke},
volume = {10577 LNCS},
year = {2017},
pages = {143 - 148},
issn = {03029743},
address = {Portoroz, Slovenia},
abstract = {In this demo, we show how an effective and application agnostic way of curating SPARQL queries can be achieved by leveraging Git-based architectures. Often, SPARQL queries are hard-coded into Linked Data consuming applications. This tight coupling poses issues in code maintainability, since these queries are prone to change to adapt to new situations; and query reuse, since queries that might be useful in other applications remain inaccessible. In order to enable decoupling, version control, availability and accessibility of SPARQL queries, we propose SPARQL2Git, an interface for editing, curating and storing SPARQL queries that uses cloud based Git repositories (such as GitHub) as a backend. We describe the query management capabilities of SPARQL2Git, its convenience for SPARQL users that lack Git knowledge, and its combination with grlc to easily generate Linked Data APIs.<br/> &copy; Springer International Publishing AG 2017.},
key = {Linked data},
keywords = {Data handling;Semantic Web;},
note = {Back ends;Cloud-based;Curation;Query management;SPARQL;Sparql queries;Tight coupling;Version control;},
URL = {http://dx.doi.org/10.1007/978-3-319-70407-4_27},
} 


@inproceedings{20160201788316,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Logic controller design system supporting UML activity diagrams},
journal = {Proceedings of the 22nd International Conference Mixed Design of Integrated Circuits and Systems, MIXDES 2015},
author = {Grobelny, Michal and Grobelna, Iwona},
year = {2015},
pages = {624 - 627},
address = {Torun, Poland},
abstract = {The paper introduces a logic controller design system, called PNAD, supporting UML activity diagrams in version 2.x as a semi-formal specification technique. The system enables transformation of activity diagrams into control Petri nets, their formal verification using model checking technique and the nuXmv tool, generation of synthesizable code in hardware description language VHDL and generation of C code for microcontrollers. The benefits include the support for discrete event system development since the specification till prototype implementation. Additionally, reverse transformation from control Petri nets into UML activity diagrams is also pobible. The internal representation of diagrams is based on XML files. The usage of proposed system is illustrated on an example of concrete production proceb.<br/> Copyright &copy; 2015 Department of Microelectronics and Computer Science, Lodz Univeristy of Technology.},
key = {Controllers},
keywords = {C (programming language);Computer circuits;Computer hardware description languages;Design;Discrete event simulation;Formal specification;Graphic methods;Integrated circuits;Model checking;Petri nets;Specifications;},
note = {Activity diagram;Internal representation;Logic controller;Model-checking techniques;Prototype implementations;Reverse Transformation;Semi-formal specification;system;},
URL = {http://dx.doi.org/10.1109/MIXDES.2015.7208599},
} 


@inproceedings{20173804166923,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Using runtime state analysis to decide applicability of dynamic software updates},
journal = {ICSOFT 2017 - Proceedings of the 12th International Conference on Software Technologies},
author = {elajev, Oleg and Gregersen, Allan},
year = {2017},
pages = {38 - 49},
address = {Madrid, Spain},
abstract = {Updating application code while it is running is a popular approach to the dynamic software update problem. But in many cases the behavior of the updated application bears side effects of the update in the form of a runtime phenomena that breaks application state assumptions leading to unwanted complications. We present a runtime state analysis system, Genrih, that enhances a dynamic system update solution and automatically decides if the state transformation functions of a DSU solution are sufficient for the given update. Genrih analyzes the atomic changes in the updated code compared to the already running version and based on these changes automatically determines whether updating the system's runtime state will lead to the observable runtime phenomena. The designed system does not break the update procedure, but observes the state and produces notifications for enhanced analysis and crash management. The practical evaluation shows that the designed system imposes acceptable overhead and can help the developer be aware of several kinds of runtime phenomena.<br/> Copyright &copy; 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
key = {Application programs},
keywords = {Availability;Reliability;Reliability analysis;Software reliability;},
note = {Application codes;Atomic changes;Dynamic software update;Run-time state;Runtimes;Side effect;State analysis;State transformation;},
} 


@article{20154701579959,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Nontarget Vision Sensor for Remote Measurement of Bridge Dynamic Response},
journal = {Journal of Bridge Engineering},
author = {Feng, Maria Q. and Fukuda, Yoshio and Feng, Dongming and Mizuta, Masato},
volume = {20},
number = {12},
year = {2015},
issn = {10840702},
abstract = {Displacements of railroad bridges under trainloads need to be closely monitored, but conventional displacement sensors have limitations for use in the field. This paper presents a new vision-based sensor system developed for remote measurement of structural dynamic displacements without requiring a specially installed target-marker panel. By implementing a robust object-search algorithm, the displacement can be accurately measured by tracking existing bridge surface features from a remote distance. The accuracy of measured dynamic displacements was first evaluated using a shaking table test. Then field tests were carried out on two railroad bridges subjected to freight trainloads traveling at various speeds. Measurements were taken remotely during the daytime and also at night from different distances with and without a target panel. Through comparison with a conventional contact-type displacement sensor, the high accuracy of the proposed nontarget remote-sensor system was demonstrated in the realistic field environments. From the measured displacement time histories, frequency-domain characteristics associated with the train-bridge systems were further analyzed, confirming the capability of the vision system in measuring high-frequency components. By targeting existing features on a structure without requiring a target panel installed on a fixed location of the structure, the vision sensor system developed in this study provides the flexibility to easily change displacement measurement locations, in addition to other advantages, such as easy setup and no need to access the structure.<br/> &copy; 2015 American Society of Civil Engineers.},
key = {Displacement measurement},
keywords = {Dynamic response;Electric measuring bridges;Frequency domain analysis;Image processing;Railroad bridges;Railroads;Remote sensing;Structural dynamics;},
note = {Displacement-time history;Dynamic displacements;High frequency components;Orientation code matching;Remote sensor systems;Remote sensors;Vision sensor systems;Vision-based sensors;},
URL = {http://dx.doi.org/10.1061/(ASCE)BE.1943-5592.0000747},
} 


@article{20131616213821,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Compressive sensing multiple description image coding with hybrid sampling},
journal = {Guangxue Jingmi Gongcheng/Optics and Precision Engineering},
author = {Wang, Liang-Jun and Shi, Guang-Ming and Li, Fu and Shi, Si-Qi},
volume = {21},
number = {3},
year = {2013},
pages = {724 - 733},
issn = {1004924X},
abstract = {A Compressive Sensing(CS) multiple description coding scheme with hybrid sampling was proposed to improve the coding efficiency of the traditional CS coding system and to maintain the ability of resisting packet loss. In the scheme, both 2-D Discrete Cosine Transformation(DCT) matrix and sub-Gaussian matrix were used to measure the image signal simultaneously. Then, a Golomb code and its improved version were used to encode for the resulted measurements, respectively. As a result, the 2-D DCT measurement bit streams with complete code words and the Gaussian measurement bit streams with incomplete code words were obtained respectively. In the decoder, these incomplete code words could be decoded successfully with a Maximum A posteriori Probability (MAP) estimator, and the deficient code words could be estimated by the relevance between 2-D DCT and Gaussian measurements. Finally, these decoded measurements were grouped together again to reconstruct the image signal by solving a 1-norm optimization problem. Experimental results on both natural and remote sensing images show that the Peak Signal to Noise Ratio(PSNRs) of the images reconstructed by proposed method can be superior to that of traditional CS coding scheme by 2~4 dB at different packet loss rates, meanwhile, it has a robust resisting packet loss ability.<br/>},
key = {Image coding},
keywords = {Binary sequences;Compressed sensing;Decoding;Discrete cosine transforms;Gaussian distribution;Image reconstruction;Linear transformations;Packet loss;Remote sensing;Signal to noise ratio;},
note = {Compressive sensing;Discrete cosine transformation;Golomb coding;Maximum A posteriori probabilities;Multiple description coding;Optimization problems;Peak signal to noise ratio;Remote sensing images;},
URL = {http://dx.doi.org/10.3788/OPE.20132103.0724},
} 


@inproceedings{20183905848093,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A fast GPU point-cloud registration algorithm},
journal = {Proceedings - 2018 IEEE/ACIS 19th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2018},
author = {Rahman, Md Mushfiqur and Galanakou, Panagiota and Kalantzis, Georgios},
year = {2018},
pages = {111 - 116},
address = {Busan, Korea, Republic of},
abstract = {The purpose of point cloud registration is to find a 3D rigid body transformation so that the 3D coordinates of the point cloud at different angles can be correctly matched. With the current advances of high definition (HD) 3D cameras, several applications have emerged utilizing stereoscopic cameras. For real time objects tracking, fast point cloud registration calculations are required. In the current study we have considered two methods: A standard Singular Value Decomposition (SVD) and a truncated SVD (TSVD) point cloud registration algorithm. The registration process can be summarized in the following steps; the centroids of the chosen point datasets are first found, then they both are aligned to the origin, and then the optimal rotation and translation are determined based on the SVD or on the TSVD technique. Our strategy was firstly to identify the major computational bottlenecks of our code, and secondly parallelize them on the GPU accordingly. Performance tests were conducted on three GPU cards in comparison to a serial version of the algorithm executed on a CPU. Performance comparisons are also conducted between the parallel SVD and the parallel TSVD in order to test the computational efficiency of them on GPU cards. The studies indicated that there is no computational benefit from the parallization of the simple SVD on GPU. On the contrary, there is computational advantage of the parallel TSVD, but it varies with the GPU architecture. Speedup factors were recorded for every registration steps for all GPU cards. The step 2 of registration process was the most computational expensive task for the algorithm, and when it was parallelized on K40m card gave a maximum speed up of 100 for the maximum number of pixels, while for other resolution sizes the performance of K40m decreased dramatically. The GTX1080Ti card achieved the highest speed up of 150 for block 2 calculations, for 8K resolution. In overall, for the full registration process GTX1080Ti indicated a linear increase of speedup factors versus the number of pixels, fact that renders it is the most suitable GPU card with respect to the other GPU cards used for the specific application.<br/> &copy; 2018 IEEE.},
key = {Singular value decomposition},
keywords = {Artificial intelligence;Cameras;Computational efficiency;Digital television;Graphics processing unit;Pixels;Software engineering;Stereo image processing;Surface measurement;},
note = {Computational advantages;Computational bottlenecks;Performance comparison;Point cloud registration;Registration process;Rigid body transformation;Speed-up factors;Truncated SVD;},
URL = {http://dx.doi.org/10.1109/SNPD.2018.8441108},
} 


@article{20180704789552,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Comparison of ultra-rapid orbit prediction strategies for GPS, GLONASS, galileo and BeiDou},
journal = {Sensors (Switzerland)},
author = {Geng, Tao and Zhang, Peng and Wang, Wei and Xie, Xin},
volume = {18},
number = {2},
year = {2018},
issn = {14248220},
abstract = {Currently, ultra-rapid orbits play an important role in the high-speed development of global navigation satellite system (GNSS) real-time applications. This contribution focuses on the impact of the fitting arc length of observed orbits and solar radiation pressure (SRP) on the orbit prediction performance for GPS, GLONASS, Galileo and BeiDou. One full year&rsquo;s precise ephemerides during 2015 were used as fitted observed orbits and then as references to be compared with predicted orbits, together with known earth rotation parameters. The full nine-parameter Empirical Center for Orbit Determination in Europe (CODE) Orbit Model (ECOM) and its reduced version were chosen in our study. The arc lengths of observed fitted orbits that showed the smallest weighted root mean squares (WRMSs) and medians of the orbit differences after a Helmert transformation fell between 40 and 45 h for GPS and GLONASS and between 42 and 48 h for Galileo, while the WRMS values and medians become flat after a 42 h arc length for BeiDou. The stability of the Helmert transformation and SRP parameters also confirmed the similar optimal arc lengths. The range around 42-45 h is suggested to be the optimal arc length interval of the fitted observed orbits for the multi-GNSS joint solution of ultra-rapid orbits.<br/> &copy; 2018 by the authors. Licensee MDPI, Basel, Switzerland.},
key = {Orbits},
keywords = {Carbon dioxide arc welding;Global positioning system;Pressure;Radio navigation;Solar radiation;},
note = {Earth rotation parameters;Global Navigation Satellite Systems;Gnss;Helmert transformation;MGEX;Real-time application;Solar radiation pressure;Ultra-rapid;},
URL = {http://dx.doi.org/10.3390/s18020477},
} 


@inproceedings{20182705402320,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dynamic service versioning: Network-based approach (DSV-N)},
journal = {Advances in Intelligent Systems and Computing},
author = {Jauhari, Prashank and Kumar, Sachin and Mal, Chiranjeev and Marwaha, Preeti and Preveen, Anu},
volume = {669},
year = {2019},
pages = {251 - 264},
issn = {21945357},
address = {Moga, India},
abstract = {The version of a service describes the service functionality, guiding client on the details for accessing the service. Service versioning requires optimal strategies to appropriately manage versions of the service which result from changes during service life cycle. However, there is no standard for handling service versions which leads to the difficulties in tracing changes, measuring their impact as well as managing multiple services concurrently without any backward compatibility issues. Sometime these services need to be modified as per the client&rsquo;s prerequisites which result in a new version of the existing service, and changes done in the services may or may not be backward compatible. If changes done in the services are not backward compatible, then it can create compatibility issues in the client side code which is using these service features. The problem aggregates even more when a product requires customization for its clients with minor differences in each version. This results in deploying multiple versions of the service for each one of them. This work describes DSV-N (Dynamic Service Versioning-Network-Based Approach) to handle issues related to change management of the service. DSV-N is also capable of handling both backward and incompatible changes. This paper also extends the functionality of dynamic service dispatching by using it with service versioning so that the versions do not need to reside in the memory permanently except the system bus which will execute the appropriate version at run time. Another advantage of using DSV-N is that multiple service versions are required to be bound with the system bus merely (which is the address of all the service versions for the client). DSV-N automates the process of replicating identical modules in different versions of the service with the help of component file in which version id(s) is(are) prefixed to each module name. DSV-N also provides backward compatibility because the appropriate version that needs to be executed will be resolved at run time. Another advantage DSV-N provides is that the component file needs to be parsed only if the service modules of the component file are modified or only for the first request.<br/> &copy; Springer Nature Singapore Pte Ltd. 2019.},
key = {Web services},
keywords = {Networks (circuits);Service life;System buses;},
note = {Backward compatibility;Backward compatible;Change management;Multiple services;Network-based approach;Optimal strategies;Service functionalities;Versioning;},
URL = {http://dx.doi.org/10.1007/978-981-10-8968-8_22},
} 


@article{20145100337707,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Coding interactions in Motivational Interviewing with computer-software: What are the advantages for process researchers?},
journal = {Computers in Human Behavior},
author = {Klonek, Florian E. and Quera, Vicenc and Kauffeld, Simone},
volume = {44},
year = {2015},
pages = {284 - 292},
issn = {07475632},
abstract = {Motivational Interviewing (MI) is an evidence-based behavior change intervention. The interactional change processes that make MI effective have been increasingly studied using observational coding schemes. We introduce an implementation of a software-supported MI coding scheme - the Motivational Interviewing Treatment Integrity code (MITI) - and discuss advantages for process researchers. Furthermore, we compared reliability of the software version with prior results of the paper version. A sample of 14 double-coded dyadic interactions showed good to excellent interrater reliabilities. We selected a second sample of 22 sessions to obtain convergent validity results of the software version: substantial correlations were obtained between the software instrument and the Rating Scales for the Assessment of Empathic Communication. Finally, we demonstrate how the software version can be used to test whether single code frequencies obtained by using intervals shorter than 20 min (i.e.; 5 or 10 min) are accurate estimates of the respective code frequencies for the entire session (i.e.; behavior slicing). Our results revealed that coding only a 10-min interval provides accurate estimates of the entire session. Our study demonstrates that the software implementation of the MITI is a reliable and valid instrument. We discuss advantages of the software version for process research in MI.<br/> &copy; 2014 Elsevier Ltd. All rights reserved.},
key = {Software reliability},
keywords = {Codes (symbols);Software testing;},
note = {Behavioral slicing;Integrity code;Motivational Interviewing;Motivational Interviewing Treatment;Observational method;Software coding;Timed-event data;},
URL = {http://dx.doi.org/10.1016/j.chb.2014.10.034},
} 


@article{20184706112000,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Seismic design of low-rise buildings based on frequent earthquake response spectrum},
journal = {Journal of Building Engineering},
author = {Barros, Jose and Santa-Maria, Hernan},
volume = {21},
year = {2019},
pages = {366 - 372},
issn = {23527102},
abstract = {Special provisions for the design of concrete frame structures to fulfill the immediate occupancy performance level for frequent (or low magnitude, or service) earthquakes are unavailable, or it is believed that drift control assures this compliance. This paper evaluates a different procedure for the structural design that guarantees an appropriate behavior for frequent (or service) earthquakes and for rare (or design) earthquakes, limited to short period concrete frame structures up to two stories. A two-story building with the typical architecture of a school is proposed as the study case. In order to compare the design procedure behavior, special moment frame (SMF) and intermediate moment frame (IMF) designs have been developed, as ASCE/SEI 7&ndash;16 and ACI 318-14 suggests in the regulations that the Ecuadorian code (NEC-2015) presents. An additional building is designed with the proposed procedure, for an earthquake with a return period of 43 years. To evaluate the behavior of the proposed structures, FEMA P-695 recommendations were followed, and non-linear models were developed using pushover and time history analyses. Results show for the buildings that were designed with the current regulations excessive demands in the non-structural elements expected for the frequent earthquake and that the proposed design methodology satisfies the behavior levels required by the regulations, without producing an abrupt change in the existing design procedure.<br/> &copy; 2018 Elsevier Ltd},
key = {Seismic design},
keywords = {Architectural design;Compliance control;Composite structures;Concretes;Earthquakes;Structural design;Structural frames;},
note = {Concrete frame structures;Earthquake response;Immediate occupancy;Non-structural elements;Performance based seismic design;Service levels;Special moment frames;Time history analysis;},
URL = {http://dx.doi.org/10.1016/j.jobe.2018.11.005},
} 


@inproceedings{20180704804243,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Deformable Convolutional Networks},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
volume = {2017-October},
year = {2017},
pages = {764 - 773},
issn = {15505499},
address = {Venice, Italy},
abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets.<br/> &copy; 2017 IEEE.},
key = {Computer vision},
keywords = {Backpropagation;Convolution;Deformation;Mathematical transformations;Neural networks;Object detection;Semantics;},
note = {Convolutional networks;Convolutional neural network;Geometric structure;Geometric transformations;Semantic segmentation;Spatial sampling;Spatial transformation;Transformation model;},
URL = {http://dx.doi.org/10.1109/ICCV.2017.89},
} 


@inproceedings{20165103139731,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A metaphone based chaotic searchable encryption algorithm for border management},
journal = {ICETE 2016 - Proceedings of the 13th International Joint Conference on e-Business and Telecommunications},
author = {Awad, Abir and Lee, Brian},
volume = {4},
year = {2016},
pages = {397 - 402},
address = {Lisbon, Portugal},
abstract = {In this paper, we consider a use case for national border control and management involving the assurance of privacy and protection of personally identifiable information (PII) in a shared multi-tenant environment, i.e. the cloud. A fuzzy searchable encryption scheme is applied on a watch list of names which are used as indexes for the identification files that are in their turn encrypted and stored on the cloud. Two propositions are described and tested in this paper. The first entails the application of a chaotic fuzzy searchable encryption scheme directly on the use case and its subsequent verification on a number of phonetics synonyms for each name. In the second version, a metaphone based chaotic fuzzy transformation method is used to perform a secure search and query. In this latter case, the fuzzy transformation is performed in two stages: the first stage is the application of the metaphone algorithm which maps all the words pronounced in the same way to a single code and the second stage is the application of the chaotic Local Sensitive Hashing (LSH) to the code words. In both the first and second propositions, amplification of the LSH is also performed which permits controlled fuzziness and ranking of the results. Extensive tests are performed and experimental results show that the proposed scheme can be used for secure searchable identification files and a privacy preserving scheme on the cloud.<br/> Copyright &copy; 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
key = {Cryptography},
keywords = {Data privacy;Electronic commerce;},
note = {Fuzzy searches;Locality sensitive hashing;Metaphone;Personally identifiable information;Privacy preserving;Searchable encryptions;Sensitive hashing;Transformation methods;},
URL = {http://dx.doi.org/10.5220/0005953503970402},
} 


@inproceedings{20132416406766,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {OpenMP-style parallelism in data-centered multicore computing with R},
journal = {ACM SIGPLAN Notices},
author = {Jiang, Lei and Patel, Pragneshkumar B. and Ostrouchov, George and Jamitzky, Ferdinand},
volume = {47},
number = {8},
year = {2012},
pages = {335 - 336},
issn = {15232867},
abstract = {R<sup>1</sup>is a domain specific language widely used for data analysis by the statistics community as well as by researchers in finance, biology, social sciences, and many other disciplines. As R programs are linked to input data, the exponential growth of available data makes high-performance computing with R imperative. To ease the process of writing parallel programs in R, code transformation from a sequential program to a parallel version would bring much convenience to R users. In this paper, we present our work in semiautomatic parallelization of R codes with user-added OpenMPstyle pragmas. While such pragmas are used at the frontend, we take advantage of multiple parallel backends with different R packages. We provide flexibility for importing parallelism with plug-in components, impose built-in MapReduce for data processing, and also maintain code reusability. We illustrate the advantage of the on-the-fly mechanisms which can lead to significant applications in data-centered parallel computing. Copyright &copy; 2012 ACM.<br/>},
key = {Data handling},
keywords = {Application programming interfaces (API);Automatic programming;Codes (symbols);Cosine transforms;Multiprocessing systems;Parallel programming;Problem oriented languages;Reusability;Systems analysis;},
note = {Automatic code generations;Data-centered applications;Domain specific languages;Map-reduce;Parallelizations;},
URL = {http://dx.doi.org/10.1145/2370036.2145882},
} 


@inproceedings{20121214879979,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {OpenMP-style parallelism in data-centered multicore computing with R},
journal = {Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP},
author = {Jiang, Lei and Patel, Pragneshkumar B. and Ostrouchov, George and Jamitzky, Ferdinand},
year = {2012},
pages = {335 - 336},
address = {New Orleans, LA, United states},
abstract = {R1 is a domain specific language widely used for data analysis by the statistics community as well as by researchers in finance, biology, social sciences, and many other disciplines. As R programs are linked to input data, the exponential growth of available data makes high-performance computing with R imperative. To ease the process of writing parallel programs in R, code transformation from a sequential program to a parallel version would bring much convenience to R users. In this paper, we present our work in semiautomatic parallelization of R codes with user-added OpenMPstyle pragmas. While such pragmas are used at the frontend, we take advantage of multiple parallel backends with different R packages. We provide flexibility for importing parallelism with plug-in components, impose built-in MapReduce for data processing, and also maintain code reusability. We illustrate the advantage of the on-the-fly mechanisms which can lead to significant applications in data-centered parallel computing.<br/>},
key = {Data handling},
keywords = {Application programming interfaces (API);Automatic programming;Codes (symbols);Cosine transforms;Multiprocessing systems;Parallel programming;Problem oriented languages;Reusability;Systems analysis;},
note = {Automatic code generations;Data-centered applications;Domain specific languages;Map-reduce;Parallelizations;},
URL = {http://dx.doi.org/10.1145/2145816.2145882},
} 


@inproceedings{20133716733178,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Power control in the uplink of a wireless multi-carrier CDMA system},
journal = {Proceedings of the American Control Conference},
author = {Campos-Delgado, Daniel U. and Luna-Rivera, J.M.},
year = {2013},
pages = {2733 - 2738},
issn = {07431619},
address = {Washington, DC, United states},
abstract = {This paper addresses the power allocation problem in the uplink of wireless multi-carrier code-division multiple-access (MC-CDMA) in order to guarantee QoS requirements. QoS is evaluated with respect to the signal to interference-noise ratio (SINR), estimated after the detection process at the base station (BS). First, departing of the received signal model in a MC-CDMA system, the SINR is computed by considering the application of linear multiuser detectors and a common transmission power through all subcarriers. By assuming that the measured SINR must follow an objective SINR value, an open-loop solution is introduced to the power allocation problem. Next, an iterative version is derived that relies on an integral correction of the required transmission power. By applying a loop transformation and the small-gain theorem, general distributed controllers can be proposed, where specific stability and performance conditions are introduced. A simulation evaluation is presented at different load levels in the MC-CDMA, and time-varying channel gains. In all the studied cases, the resulting power allocation system was able to allocate transmission power to achieve the objective SINR. &copy; 2013 AACC American Automatic Control Council.<br/>},
key = {Code division multiple access},
keywords = {Communication channels (information theory);Fading channels;Multicarrier modulation;Multiuser detection;Power control;Quality control;Quality of service;Signal interference;Signal to noise ratio;Turbo codes;},
note = {Closed-loop control;Distributed controller;Linear multiuser detectors;MC-CDMA;Multi-carrier CDMA systems;Multicarrier code-division multiple access;Power allocations;Signal to interference-noise ratios;},
} 


@inproceedings{20164903101822,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dynamically adaptive parsons problems},
journal = {ICER 2016 - Proceedings of the 2016 ACM Conference on International Computing Education Research},
author = {Ericson, Barbara J.},
year = {2016},
pages = {269 - 270},
address = {Melbourne, VIC, Australia},
abstract = {Parsons problems are code segments that must be placed in the correct order with the correct indention. Research on Parsons problems suggests that they might be a more effective and efficient learning approach than writing equivalent code, especially for time-strapped secondary teachers. I am exploring this hypothesis with empirical experiments, observations, and log file analyses. Our research team also plans to modify the opensource js-parsons software to allow Parsons problems to be dynamically adaptive, which means that the difficulty of the problem will change based on the user's past performance. I plan to compare dynamically adaptive Parsons problems to the current non-adaptive Parsons problems to determine which learners prefer and to see if solving dynamically adaptive Parsons problems leads to more efficient and effective learning than solving non-adaptive Parsons problems.<br/>},
key = {E-learning},
keywords = {Teaching;},
note = {Adaptive learning;Ebook;Learning programming;Online learning;Parsons problems;},
URL = {http://dx.doi.org/10.1145/2960310.2960342},
} 


@inproceedings{20184806132373,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A vision for interactive suggested examples for novice programmers},
journal = {Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},
author = {Ichinco, Michelle},
volume = {2018-October},
year = {2018},
pages = {303 - 304},
issn = {19436092},
address = {Lisbon, Portugal},
abstract = {Many systems aim to support programmers within a programming context, whether they recommend API methods, example code, or hints to help novices solve a task. The recommendations may change based on the user's code context, history, or the source of the recommendation content. They are designed to primarily support users in improving their code or working toward a task solution. The recommendations themselves rarely provide support for a user to interact with them directly, especially in ways that benefit the knowledge or understanding of the user. This poster presents a vision and preliminary designs for three ways a user might learn from interactions with suggested examples: describing examples, providing detailed relevance feedback, and selective visualization and tinkering.<br/> &copy; 2018 IEEE.},
key = {Visual languages},
keywords = {Application programming interfaces (API);Codes (symbols);},
note = {Example code;Interactive suggestions;Novice programmer;Preliminary design;Relevance feedback;},
URL = {http://dx.doi.org/10.1109/VLHCC.2018.8506535},
} 


@article{20142417806990,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Simplifying translation validation via model extrapolation},
journal = {Journal of Integrated Design and Process Science},
author = {Howar, Falk and Margaria, Tiziana and Wagner, Christian},
volume = {17},
number = {3},
year = {2013},
pages = {71 - 91},
issn = {10920617},
abstract = {We revisit our case study on the NASA's Voyager space mission to automatically discover its behaviour by means of model transformation and automata learning. We investigate the conformance of three structurally different types of specification of the case study: (1) a formal specification given in ASSL, (2) a derived implementation in Java, and (3) two behavioral models, one derived from the ASSL specification and one learned from the Java implementation. This way we show that Behavioural Mining, that extracts directly analyzable behavioural models from other artifacts (specifications or code) is a practicable and very simple way to obtain a process-oriented description of third-party systems. As the learning technique can be tailored to different abstraction levels according what behavioural primitives we decide to observe, we show and discuss different alternative learned models. This process oriented description is directly amenable to formal verification, as we show here by means of model checking. &copy; 2013 - Society for Design and Process Science.<br/>},
key = {Model checking},
keywords = {Automata theory;Behavioral research;Formal specification;Java programming language;NASA;Space flight;},
note = {automata learning;Legacy software;Model extraction;simplicity;Translation validation;},
URL = {http://dx.doi.org/10.3233/jid-2013-0022},
} 


@article{20144900276326,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A linear approach for sparse coding by a two-layer neural network},
journal = {Neurocomputing},
author = {Montalto, Alessandro and Tessitore, Giovanni and Prevete, Roberto},
volume = {149},
number = {PC},
year = {2015},
pages = {1315 - 1323},
issn = {09252312},
abstract = {Many approaches to transform classification problems from non-linear to linear by feature transformation have been recently presented in the literature. These notably include sparse coding methods and deep neural networks. However, many of these approaches require the repeated application of a learning process upon the presentation of unseen data input vectors, or else involve the use of large numbers of parameters and hyper-parameters, which must be chosen through cross-validation, thus increasing running time dramatically. In this paper, we propose and experimentally investigate a new approach for the purpose of overcoming limitations of both kinds. The proposed approach makes use of a linear auto-associative network (called SCNN) with just one hidden layer. The combination of this architecture with a specific error function to be minimized enables one to learn a linear encoder computing a sparse code which turns out to be as similar as possible to the sparse coding that one obtains by re-training the neural network. Importantly, the linearity of SCNN and the choice of the error function allow one to achieve reduced running time in the learning phase. The proposed architecture is evaluated on the basis of two standard machine learning tasks. Its performances are compared with those of recently proposed non-linear auto-associative neural networks. The overall results suggest that linear encoders can be profitably used to obtain sparse data representations in the context of machine learning problems, provided that an appropriate error function is used during the learning phase.<br/> &copy; 2014 Elsevier B.V.},
key = {Codes (symbols)},
keywords = {Artificial intelligence;Deep neural networks;Errors;Linear transformations;Mathematical transformations;Network architecture;Network coding;Network layers;Neural networks;},
note = {Autoassociative networks;Autoassociative neural networks;Encoder-decoder;Feature transformations;Linear approach;Machine learning problem;Proposed architectures;Sparse coding;},
URL = {http://dx.doi.org/10.1016/j.neucom.2014.08.066},
} 


@article{20154701596126,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {FE modeling of the cooling and tempering steps of bimetallic rolling mill rolls},
journal = {International Journal of Material Forming},
author = {Neira Torres, Ingrid and Gilles, Gaetan and Tchoufang Tchuindjang, Jerome and Flores, Paulo and Lecomte-Beckers, Jacqueline and Habraken, Anne Marie},
volume = {10},
number = {3},
year = {2017},
pages = {287 - 305},
issn = {19606206},
abstract = {Numerical simulations enable the analysis of the stress and strain histories of bimetallic rolling mill rolls. The history of rolling mill rolls is simulated by thermo-mechanical metallurgical finite element code while considering two steps: post-casting cooling and subsequent tempering heat treatment. The model requires a notably large set of material parameters. For different phases and temperatures, Young modulus, yield limit and tangent plastic modulus are determined through compression tests. Rupture stresses and strains are obtained by tensile tests. Thermo-physical parameters are measured by such experimental methods as dilatometry, DSC (Differential Scanning Calorimetry) and Laser Flash methods. Such parameters as the transformation plasticity coefficients for the ferrite, pearlite and martensite phases are identified through an inverse method. From the simulation results, the profile of the stresses evolution at different critical times is presented. An analysis of the potential damage is proposed by comparing the predicted axial stress with rupture stresses. The perspective of the Ghosh and McClintock damage criteria is also investigated.<br/> &copy; 2015, Springer-Verlag France.},
key = {Inverse problems},
keywords = {Compression testing;Cooling;Differential scanning calorimetry;Elasticity;Finite element method;Heat treatment;Pearlitic transformations;Phase transitions;Residual stresses;Rolling;Rolling mills;Strain;Structural design;Tempering;Tensile testing;},
note = {Bimetallic rolls;Dsc(differential scanning calorimetry);Experimental methods;FE-simulation;Finite element codes;Laser flash methods;Thermo-physical parameters;Transformation plasticity coefficient;},
URL = {http://dx.doi.org/10.1007/s12289-015-1277-0},
} 


@article{20144600201165,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Kitsune: Efficient, general-purpose dynamic software updating for C},
journal = {ACM Transactions on Programming Languages and Systems},
author = {Hayden, Christopher M. and Saur, Karla and Smith, Edward K. and Hicks, Michael and Foster, Jeffrey S.},
volume = {36},
number = {4},
year = {2014},
issn = {01640925},
abstract = {Dynamic software updating (DSU) systems facilitate software updates to running programs, thereby permitting developers to add features and fix bugs without downtime. This article introduces Kitsune, a DSU system for C. Kitsune's design has three notable features. First, Kitsune updates the whole program, rather than individual functions, using a mechanism that places no restrictions on data representations or allowed compiler optimizations. Second, Kitsune makes the important aspects of updating explicit in the program text, making the program's semantics easy to understand while minimizing programmer effort. Finally, the programmer can write simple specifications to direct Kitsune to generate code that traverses and transforms old-version state for use by new code; such state transformation is often necessary and is significantly more difficult in prior DSU systems. We have used Kitsune to update six popular, open-source, single- and multithreaded programs and find that few program changes are required to use Kitsune, that it incurs essentially no performance overhead, and that update times are fast.<br/> &copy; 2014 ACM.},
key = {Program compilers},
keywords = {Multitasking;Open source software;Program debugging;Semantics;},
note = {Compiler optimizations;Data representations;Dynamic software updating;Multi-threaded programs;Open sources;Software updates;State transformation;},
URL = {http://dx.doi.org/10.1145/2629460},
} 


@inproceedings{20124815720737,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Kitsune: Efficient, general-purpose dynamic software updating for C},
journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
author = {Hayden, Christopher M. and Smith, Edward K. and Denchev, Michail and Hicks, Michael and Foster, Jeffrey S.},
year = {2012},
pages = {249 - 264},
address = {Tucson, AZ, United states},
abstract = {Dynamic software updating (DSU) systems allow programs to be updated while running, thereby permitting developers to add features and fix bugs without downtime. This paper introduces Kitsune, a new DSU system for C whose design has three notable features. First, Kitsune's updating mechanism updates the whole program, not individual functions. This mechanism is more flexible than most prior approaches and places no restrictions on data representations or allowed compiler optimizations. Second, Kitsune makes the important aspects of updating explicit in the program text, making the program's semantics easy to understand while minimizing programmer effort. Finally, the programmer can write simple specifications to direct Kitsune to generate code that traverses and transforms old-version state for use by new code; such state transformation is often necessary, and is significantly more difficult in prior DSU systems. We have used Kitsune to update five popular, open-source, single- and multi-threaded programs, and find that few program changes are required to use Kitsune, and that it incurs essentially no performance overhead.<br/>},
key = {Object oriented programming},
keywords = {Computer systems programming;Dynamics;Open source software;Program compilers;Program debugging;Semantics;},
note = {Compiler optimizations;Data representations;Dynamic software updating;Multi-threaded programs;Open sources;State transformation;},
URL = {http://dx.doi.org/10.1145/2384616.2384635},
} 


@inproceedings{20124915767457,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Kitsune: Efficient, general-purpose dynamic software updating for C},
journal = {ACM SIGPLAN Notices},
author = {Hayden, Christopher M. and Smith, Edward K. and Denchev, Michail and Hicks, Michael and Foster, Jeffrey S.},
volume = {47},
number = {10},
year = {2012},
pages = {249 - 264},
issn = {15232867},
abstract = {Dynamic software updating (DSU) systems allow programs to be updated while running, thereby permitting developers to add features and fix bugs without downtime. This paper introduces Kitsune, a new DSU system for C whose design has three notable features. First, Kitsune's updating mechanism updates the whole program, not individual functions. This mechanism is more flexible than most prior approaches and places no restrictions on data representations or allowed compiler optimizations. Second, Kitsune makes the important aspects of updating explicit in the program text, making the program's semantics easy to understand while minimizing programmer effort. Finally, the programmer can write simple specifications to direct Kitsune to generate code that traverses and transforms old-version state for use by new code; such state transformation is often necessary, and is significantly more difficult in prior DSU systems. We have used Kitsune to update five popular, open-source, single- and multi-threaded programs, and find that few program changes are required to use Kitsune, and that it incurs essentially no performance overhead. Copyright &copy; 2012 ACM.<br/>},
key = {Program debugging},
keywords = {Dynamics;Open source software;Program compilers;Semantics;},
note = {Compiler optimizations;Data representations;Dynamic software updating;Multi-threaded programs;Open sources;State transformation;},
URL = {http://dx.doi.org/10.1145/2398857.2384635},
} 


@inproceedings{20165303203610,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A satellite relative motion model including j2and j3via Vinti's intermediary},
journal = {Advances in the Astronautical Sciences},
author = {Biria, Ashley D. and Russell, Ryan P.},
volume = {158},
year = {2016},
pages = {3475 - 3494},
issn = {00653438},
address = {Napa, CA, United states},
abstract = {Vinti's potential is revisited for analytical propagation of the main satellite problem, this time in the context of relative motion. A particular version of Vinti's spheroidal method is chosen that is valid for arbitrary elliptical orbits, encapsulating J<inf>2</inf>, J<inf>3</inf>, and approximately two thirds of J<inf>4</inf>in an orbit propagation theory without resorting to perturbation methods. As a child of Vinti's solution, the proposed relative motion model inherits these properties. Furthermore, the problem is solved in oblate spheroidal elements, leading to large regions of validity for the linearization approximation. After offering several enhancements to Vinti's solution, including boosts in accuracy and removal of some singularities, the proposed model is derived and subsequently reformulated so that Vinti's solution is piecewise differentiable. While the model is valid for the critical inclination and nonsingular in the element space, singularities remain in the linear transformation from ECI coordinates to spheroidal elements when the eccentricity nears zero or for nearly circular equatorial orbits. The new state transition matrix is evaluated against numerical solutions including the J<inf>2</inf>through J<inf>5</inf>terms for a wide range of chief orbits and separation distances. The solution is also compared with side-by-side simulations of the original Gim-Alfriend state transition matrix, which considers the J<inf>2</inf>perturbation. Code for computing the resulting state transition matrix and associated reference frame and coordinate transformations is provided online as supplementary material.<br/>},
key = {Linear transformations},
keywords = {Computation theory;Mathematical transformations;Matrix algebra;Orbits;Perturbation techniques;Space flight;},
note = {Co-ordinate transformation;Critical inclination;Linearization approximation;Perturbation method;Relative motion models;Satellite relative motions;Separation distances;State transition Matrix;},
} 


@article{20144500174245,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {State of the art of dynamic software updating in Java},
journal = {Communications in Computer and Information Science},
author = {Gregersen, Allan Raundahl and Rasmussen, Michael and Jorgensen, Bo Norregaard},
volume = {457},
year = {2014},
pages = {99 - 113},
issn = {18650929},
abstract = {The dynamic software updating system JRebel from Zeroturnaround has proven to be an efficient mean to improve developer productivity, as it allows developers to change the code of their applications while developing and testing them. Hence, developers no longer have to go through the tedious cycle of serializing application state, halting execution, redeploy the binary, restarting, and de-serializing state before they can test the effect of a code change. However, the current version of JRebel has its limits, as it does not support all kinds of code changes. In this paper, we compare the three most comprehensive dynamic updating systems developed for Java to date. Together, these systems provide comprehensive support for changing class definitions of live objects, including adding, removing and moving fields, methods, classes and interfaces anywhere in the inheritance hierarchy. We then investigate the effects of dynamic updating by performing a dynamic updating experiment on five consecutive revisions of the classical arcade game Breakout using the dynamic updating system Gosh! (Prior to the acquisition by zeroturnaround.com known as Javeleon.). Based on the result of this experiment we show that dynamic updating of class definitions for live objects may under some circumstances result in different run-time behavior than would be observed after a cold restart of the upgraded application. Finally, we conclude by discussing the implication of integrating the dynamic updating model of Gosh! with JRebel. The successful integration of these two systems will set a new standard for dynamic software updating in Java.<br/> &copy; Springer-Verlag Berlin Heidelberg 2014.},
key = {Java programming language},
keywords = {Application programs;Codes (symbols);Software testing;},
note = {Code changes;Dynamic software updating;Dynamic updating;Inheritance hierarchies;Moving field;Runtimes;State of the art;},
URL = {http://dx.doi.org/10.1007/978-3-662-44920-2_7},
} 


@article{20171503549780,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Reply to comment by Anel on Most computational hydrology is not reproducible, so is it really science?},
journal = {Water Resources Research},
author = {Hutton, Christopher and Wagener, Thorsten and Freer, Jim and Han, Dawei and Duffy, Chris and Arheimer, Berit},
volume = {53},
number = {3},
year = {2017},
pages = {2575 - 2576},
issn = {00431397},
abstract = {In this article, we reply to a comment made on our previous commentary regarding reproducibility in computational hydrology. Software licensing and version control of code are important technical aspects of making code and workflows of scientific experiments open and reproducible. However, in our view, it is the cultural change that is the greatest challenge to overcome to achieve reproducible scientific research in computational hydrology. We believe that from changing the culture and attitude among hydrological scientists, details will evolve to cover more (technical) aspects over time.<br/> &copy; 2017. American Geophysical Union. All Rights Reserved.},
key = {Hydrology},
keywords = {Codes (symbols);},
note = {code;Cultural changes;Reproducibilities;Scientific experiments;Scientific researches;Software licensing;Technical aspects;workflow;},
URL = {http://dx.doi.org/10.1002/2017WR020480},
} 


@inproceedings{20165203167513,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards robust color recovery for high-capacity color QR codes},
journal = {Proceedings - International Conference on Image Processing, ICIP},
author = {Yang, Zhibo and Cheng, Zhiyi and Loy, Chen Change and Lau, Wing Cheong and Li, Chak Man and Li, Guanchen},
volume = {2016-August},
year = {2016},
pages = {2866 - 2870},
issn = {15224880},
address = {Phoenix, AZ, United states},
abstract = {Color brings extra data capacity for QR codes, but it also brings tremendous challenges to the decoding because of color interference and illumination variation, especially for high-density QR codes. In this paper, we put forth a framework for high-capacity QR codes, HiQ, which optimizes the decoding algorithm for high-density QR codes to achieve robust and fast decoding on mobile devices, and adopts a learning-based approach for color recovery. Moreover, we propose a robust geometric transformation algorithm to correct the geometric distortion. We also provide a challenging color QR code dataset, CUHK-CQRC, which consists of 5390 high-density color QR code samples captured by different smartphones under different lighting conditions. Experimental results show that HiQ outperforms the baseline [1] by 286% in decoding success rate and 60% in bit error rate.<br/> &copy; 2016 IEEE.},
key = {Image processing},
keywords = {Bit error rate;Color;Decoding;Mathematical transformations;Recovery;},
note = {Color recovery;Geometric distortion;Geometric transformations;High capacity;Illumination variation;Learning-based approach;Lighting conditions;QR codes;},
URL = {http://dx.doi.org/10.1109/ICIP.2016.7532883},
} 


@inproceedings{20154201406284,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The knowledge accumulation and transfer in open-source software (OSS) development},
journal = {Proceedings of the Annual Hawaii International Conference on System Sciences},
author = {Kim, Youngsoo and Jiang, Lingxiao},
volume = {2015-March},
year = {2015},
pages = {3811 - 3820},
issn = {15301605},
address = {Kauai, HI, United states},
abstract = {We examine the learning curves of individual software developers in Open-Source Software (OSS) Development. We collected the dataset of multi-year code change histories from the repositories for 20 open source software projects involving more than 200 developers. We build and estimate regression models to assess individual developers' learning progress (in reducing the likelihood they make a bug). Our estimation results show that developer's coding and indirect bug-fixing experiences do not decrease bug ratios while bug-fixing experience can lead to the decrease of bug ratio of learning progress. We also find that developer's coding and bug-fixing experiences in other projects do not decrease the developer's bug ratio in a focal project. We empirically confirm the moderating effects of bug types on learning progress. Developers exhibit learning effects for some simple bug types (e.g., Wrong literals) or bug types with many instances (e.g., Wrong if conditionals). The results may have managerial implications and provoke future research on project management about allocating resources on tasks that add new code versus tasks that debug and fix existing code.<br/> &copy; 2015 IEEE.},
key = {Open source software},
keywords = {Codes (symbols);Knowledge management;Open systems;Project management;Regression analysis;},
note = {Estimation results;Knowledge accumulation;Knowledge transfer;Learning effects;Managerial implications;Moderating effect;Open source software projects;Software developer;},
URL = {http://dx.doi.org/10.1109/HICSS.2015.458},
} 


@inproceedings{20163202690707,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Input selection for fast feature engineering},
journal = {2016 IEEE 32nd International Conference on Data Engineering, ICDE 2016},
author = {Anderson, Michael R. and Cafarella, Michael},
year = {2016},
pages = {577 - 588},
address = {Helsinki, Finland},
abstract = {The application of machine learning to large datasets has become a vital component of many important and sophisticated software systems built today. Such trained systems are often based on supervised learning tasks that require features, signals extracted from the data that distill complicated raw data objects into a small number of salient values. A trained system's success depends substantially on the quality of its features. Unfortunately, feature engineering-the process of writing code that takes raw data objects as input and outputs feature vectors suitable for a machine learning algorithm-is a tedious, time-consuming experience. Because big data inputs are so diverse, feature engineering is often a trial-and-error process requiring many small, iterative code changes. Because the inputs are so large, each code change can involve a time-consuming data processing task (over each page in a Web crawl, for example). We introduce Zombie, a data-centric system that accelerates feature engineering through intelligent input selection, optimizing the inner loop of the feature engineering process. Our system yields feature evaluation speedups of up to 8&times; in some cases and reduces engineer wait times from 8 to 5 hours in others.<br/> &copy; 2016 IEEE.},
key = {Big data},
keywords = {Application programs;Artificial intelligence;Codes (symbols);Data handling;Engineering education;Feature extraction;Iterative methods;Learning algorithms;Learning systems;Web crawler;},
note = {Feature engineerings;Feature evaluation;Feature vectors;Input and outputs;Input selection;Raw data objects;Software systems;Trial-and-error process;},
URL = {http://dx.doi.org/10.1109/ICDE.2016.7498272},
} 


@inproceedings{20163102667600,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mining network traffic for application category recognition on Android platform},
journal = {Proceedings of 2015 IEEE International Conference on Progress in Informatics and Computing, PIC 2015},
author = {Wei, Songjie and Wu, Gaoxiang and Zhou, Ziyang and Yang, Ling},
year = {2015},
pages = {409 - 413},
address = {Nanjing, China},
abstract = {Signature-based static mobile malware detection is fragile when facing code obfuscation and transformation attacks. Behavior based malware detection mechanisms have been widely studied and experimented. So far only the application's running behaviors, such as API calls and resource consumption are used, which can also be easily concealed and obfuscated with various coding tricks. Most mobile malware need either cellular or network connection to conduct their malicious activities. We propose to monitor an application's network behavior and interaction to characterize application behaviors. An integrated testbed system has been designed and prototyped for such network behavior collection. Statistical features are derived from application network traffic, which are further fed to a machine-learning based classifier to build one general model for each typical category of mobile applications. Experiments show that applications in each category with identical functionality exhibit similar network behaviors, which makes it possible to use the derived category model of network behaviors to evaluate future unknown application for its trustworthiness.<br/> &copy; 2015 IEEE.},
key = {Android (operating system)},
keywords = {Application programming interfaces (API);Computer crime;Computer networks;Learning systems;Malware;Photonic integration technology;},
note = {Android;Application behaviors;Category recognition;Identical functionalities;Malicious activities;Network behaviors;Resource consumption;Statistical features;},
URL = {http://dx.doi.org/10.1109/PIC.2015.7489879},
} 


@article{20163902836529,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Joint palaeoclimate reconstruction from pollen data via forward models and climate histories},
journal = {Quaternary Science Reviews},
author = {Parnell, Andrew C. and Haslett, John and Sweeney, James and Doan, Thinh K. and Allen, Judy R.M. and Huntley, Brian},
volume = {151},
year = {2016},
pages = {111 - 126},
issn = {02773791},
abstract = {We present a method and software for reconstructing palaeoclimate from pollen data with a focus on accounting for and reducing uncertainty. The tools we use include: forward models, which enable us to account for the data generating process and hence the complex relationship between pollen and climate; joint inference, which reduces uncertainty by borrowing strength between aspects of climate and slices of the core; and dynamic climate histories, which allow for a far richer gamut of inferential possibilities. Through a Monte Carlo approach we generate numerous equally probable joint climate histories, each of which is represented by a sequence of values of three climate dimensions in discrete time, i.e. a multivariate time series. All histories are consistent with the uncertainties in the forward model and the natural temporal variability in climate. Once generated, these histories can provide most probable climate estimates with uncertainty intervals. This is particularly important as attention moves to the dynamics of past climate changes. For example, such methods allow us to identify, with realistic uncertainty, the past century that exhibited the greatest warming. We illustrate our method with two data sets: Laguna de la Roya, with a radiocarbon dated chronology and hence timing uncertainty; and Lago Grande di Monticchio, which contains laminated sediment and extends back to the penultimate glacial stage. The procedure is made available via an open source R package, Bclim, for which we provide code and instructions.<br/> &copy; 2016 Elsevier Ltd},
key = {Climate models},
keywords = {Climate change;Open source software;Open systems;Uncertainty analysis;},
note = {Chronological uncertainty;Climate history;Forward models;Palaeoclimate reconstruction;Palynology;Statistical modelling;},
URL = {http://dx.doi.org/10.1016/j.quascirev.2016.09.007},
} 


@inproceedings{20182605381969,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Bug patterns detection from android apps},
journal = {Communications in Computer and Information Science},
author = {Ramay, Waheed Yousuf and Akbar, Arslan and Sajjad, Muhammad},
volume = {849},
year = {2018},
pages = {273 - 283},
issn = {18650929},
address = {Chiang Mai, Thailand},
abstract = {Android has become the most popular OS because of its user-friendly environment, free-ware licensing and thousands of available applications. It is an open source for contributors and developers. The challenging problem in Android apps is to handle the bugs those are generated because of code segment (code constructs) written by developers to fix the reported bug. so code change management is also as critical task, as bug tracking. We have investigated all available previous history of Android bug reports and code changes to identify bug introducing changes. Apply the chi square test to observe the buggy construct. This study will help the reviewers, contributors, developers and quality assurance testers to concentrate and take special care while making or accepting changes to those constructs where it is most likely to induce a bug, which will lead to improve the quality of services provided by Android platform, and ultimately will get more satisfied user.<br/> &copy; Springer Nature Singapore Pte Ltd. 2018.},
key = {Android (operating system)},
keywords = {Codes (symbols);Open source software;Open systems;Pattern recognition;Quality assurance;Statistical tests;},
note = {Android platforms;Bug detection;Chi-square tests;Code constructs;Code segments;Critical tasks;Open sources;Software coding;},
URL = {http://dx.doi.org/10.1007/978-981-13-0896-3_27},
} 


@inproceedings{20182705397262,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {POSTER: A framework for phylogenetic analysis in mobile environment},
journal = {ASIACCS 2018 - Proceedings of the 2018 ACM Asia Conference on Computer and Communications Security},
author = {Martinelli, Fabio and Mercaldo, Francesco and Saracino, Andrea},
year = {2018},
pages = {825 - 827},
address = {Incheon, Korea, Republic of},
abstract = {To maximize the probability of successful attacks and reduce the odds of being detected, malware developers implement different versions of the same malicious payloads. As a matter of fact, malware writers often generate new malicious code starting from existing ones, adding small programmed variations, or applying obfuscation mechanisms, that change the code structure, without altering the malicious functionalities. For these reasons phylogenetic analysis is becoming of interest as instrument for malware analysts in order to understand the derivation of a malicious payload, being thus able to reconduct a derived piece of code to its original, known originator. In this poster we describe a framework designed to infer and shape the phylogenetic tree of mobile malicious applications. The framework considers multi-level features with rule-based machine learning algorithm to retrieve antecedents and descendants of malicious samples.<br/> &copy; 2018 Association for Computing Machinery.},
key = {Malware},
keywords = {Codes (symbols);Computer crime;Learning algorithms;Learning systems;},
note = {Android;Lineage;Phylogenesis;Security;Triage;},
URL = {http://dx.doi.org/10.1145/3196494.3201588},
} 


@article{20175104561752,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {2017 NESC(R) Handbook, Eighth Edition},
journal = {National Electrical Safety Code (NESC) Handbook, Eighth Edition},
author = {Bingel, Nelson G. and Bleakley, Charlie C. and Clapp, Allen L. and Dagenhart, John H. and Konz, Mark A. and Gaunt, Lauren E. and Gunter, Mickey B. and Jurgemeyer, Mark F.},
year = {2016},
pages = {1 - 428},
abstract = {The 2017 NESC Handbook, Eight Edition, a Discussion of the National Electrical Safety Code(R), is an essential companion to the Code. This handbook includes text directly from Code which provides users an easy reference back to the code, ruling-by-ruling. It gives users insight into what lies behind the NESC(R)s rules and how to apply them. The Handbook was developed for use at many levels in the electric and communication industries, including those involved in system design, construction, maintenance, inspection, standards development and worker training. The Handbook also discusses how the NESC Committee has developed the rules in the Code and responded to change proposals during the past 100 years. This allows users to understand how questions they may have were dealt with in the past. These are key points from the 2017 Handbook Edition:-Revising the purpose rule to include only the safeguarding of persons and utility facilities and clarifying the application-Deleting unused definitions and adding definitions for communication and supply space.-Revising the substation impenetrable fence requirements.-Adding an exception to exempt underground cable grounding requirements from the 4 grounds in each mile rule under certain conditions.-Revising and reorganizing the guy insulator placement rules along with eliminating the voltage transfer requirements associated with them.-Requiring a 40 vertical clearance from communication cables in the communication space if a luminaire is not effectively grounded.-Deleting the conductance requirement for underground insulating jacketed grounded neutral supply cables and revising the grounding and bonding rules for supply and communication cables in random separation installations.-Revising and reorganizing the Grades of Construction Table 242-1 that will now include service drops.-Revising the strength rules to require that all conductors be considered for damage due to Aeolian vibration.-Revising the rules in Part 4 to align with changes made to 29 CFR by the Occupational Safety and Health Administration (OSHA).<br/>Scope: This rule details the coverage of the NESC. The Code covers supply and communication lines, equipment, and associated work practices employed by a public or private electric supply, communications, railway, or similar utility in the exercise of its function as a utility. The NESC no longer covers electric fences, radio installations, or utilization equipment (see the NEC) except as covered in Part 1 or Part 3. It does not cover mines, ships, aircraft, automotive equipment, or railway rolling stock. (Refer to the edition of the NEC called out in the applicable edition of the NESC.) The difference between the facilities involved in the utility function (covered by the NESC) and those involved in the utilization function (covered by the NEC) was amplified in the 1990 Code. This language was again revised in the 1993 Code to clearly state that these requirements apply to public and private utility systems. In the 1980s and early 1990s, electricians started a controversy over whether area lights installed by an electric utility and fed off the distribution system could only meet the NESC or had to meet the NEC. Such installations have always been covered by the NESC and exempted from the NEC. The 1996 NEC revised its Article 90-2(b)(5) to exclude lighting associated with an electric distribution system that is under the exclusive control of an electric utility and is located on or along public highways, streets, roads, etc., or outdoors on private property by established rights such as easements. As a practical matter, customers generally grant either specific or "blanket" easements to utilities when applying for area lighting. If the electrical system feeding the lighting comes directly off the utility distribution system, it is clear that the NESC applies to such installations. However, if the lights are fed off the customer service entrance equipment, or if the customer has access to a switch to control the lightning, the NEC will govern. This was clarified in Rule 011C of the 2002 NESC. Both the NESC and the NEC cover some equivalent facilities, such as service drops, because they could be maintained by the customer or the utility. Depending upon local ordinances, if the installation is under qualified control (such as in some large industrial and large commercial complexes), the utility delivery system portion of such installations would be entirely under the NESC until such point as they connected to the utilization wiring system (such as at a building weatherhead on an aerial service), at which point the NEC would take over. In 2002, the NESC added an explanatory note under Rule 011B referencing the service point as the point where the NEC picks up from the NESC. The service point (point of delivery) between the NESC-and the NEC-covered facilities is easy to determine for overhead service. The connectors form the service point between the NESC-covered utility service drop conductors and the NEC-covered premises-wiring service entrance conductors located at the weatherhead. The NEC allows the NESCcovered utility meter to be located in the NEC-covered service entrance conductor run and, in a fine print note (FPN), exempts the metering from NEC application. In an underground service, the underground service cable can be under either code, depending upon ownership and control. In a typical installation where the utility installs the service drop cable underground from the transformer (or underground secondary bus cable) to the building and brings it up to the meter base, the service drop is covered by the NESC. If the customer ran the cable from the building out to the utility transformer and maintained ownership and control over the service drop, the NEC would govern. In some situations, the customer's electrical workers will initially install the underground service cable out to a utility transformer pad and the customer will transfer ownership to the utility which will own, control, and maintain the service drop from then on. In such cases, the NESC applies. The 2012 Code significantly revised the language of Rule 011 to more fully reflect the practical scope of the NESC that has existed since its inception. These revisions were the result of considerations by a joint NESC/NEC Task Force that addressed the Purpose and Scope rules of both the NESC and the NEC to limit confusion as to which code applies in different circumstances. By fleshing out the language of each of the codes with more detail about the respective scopes, each of the codes will aid its users in better understanding the differences in intended application. The NESC addresses utility system installations under the exclusive control of utilities. The NEC addresses utilization wiring and premise wiring systems, as well as other systems not under the exclusive control of a utility. Part of the confusion results from the fact that both codes must address certain types of facilities and installations, because those facilities might be under the exclusive control of a public or private utility in one instance (in which case, the NESC applies) and might not be under exclusive control of a utility in another instance (in which case, the NEC applies). For example, consider the question of whether the NESC or the NEC applies to service drops in a situation in which a utility secondary cable runs from a line pole out at the road to a center pole in a farm yard and individual service drops run from the center pole to each barn or other building. If the utility installs and maintains the service drops under its exclusive control, the NESC applies to the service drops. However, if the center pole is a meter pole and the farmer runs service drops to the farm buildings, those service drops are not under exclusive utility control and the NEC applies to the service drops. Similarly, if a utility installs and maintains area lighting under its control, the NESC applies. However, if the customer requests that 120 V outlets be placed at the base of each luminaire structure to allow the use of electric weed trimmers, etc., the system is not under exclusive control of a utility and the NEC applies. The 2012 Code language revisions also recognized that provision of lumens from luminaires is different from provision of electricity; the systems for each are subject to NESC requirements if under the exclusive control of a public or private utility and on the line side of the service point. Extensive revisions, including lists of applicable utilities and facilities and a new Figure 011-1 were added to Rule 011 to clearly indicate what is covered by the NESC and what is not covered by the NESC to help limit confusion between the codes. These revisions are not changes in scope; rather, they are clarifications to answer questions that have arisen in the past few years. The 2017 Code further clarified that the NESC applies to underground and overhead facilities located on the line side of the service point under exclusive control of utilities. The 2017 Code modified Rule 011A3 in response to IR 572 issued 28 May 2013 to clearly show that solar and wind farm generation under exclusive control of a public or private utility are included along with more traditional forms of generation in the scope of the NESC.<br/>Purpose: In the 1977 Code and later editions, it is made clear by choice of wording that the purpose of these rules is the practical safeguarding of persons during the installation, operation, or maintenance of overhead and underground supply and communication lines and their associated equipment. The NESC Subcommittees made every effort to emphasize that it is not merely enough that an installation be possible-it must be practical as well-to qualify as a requirement of the Code. It is unfortunate that earlier editions sometimes used the word "practicable" and that some individuals instigating legal actions have tried to infer that the word was intended to convey the meaning "possible." It is clear from the official Discussion of the very earliest codified edition, the 1916 Code, that general practicality of installation was intended. This emphasis on "practicality," as opposed to the extreme requirement of "possibility," is especially noted in Rule 202 (Design and Construction) of the 1916 Code and its Discussion. The language of that rule is as follows: "202-Design and Construction. All electrical supply lines and equipment shall be of suitable design and construction for the service and conditions under which they are to be operated, and all lines shall be so installed and maintained as to reduce the life hazard as far as practicable." The language of the 1916 Code Discussion is as follows: "This rule-strikes the keynote of the code. There is no intention of requiring or even recommending more expensive construction than good practice requires and good business justifies. But it must be remembered that the public in the end pays whatever extra cost is caused by requiring safer and better construction, and hence the public may rightly require a good degree of safety in the construction-" Rules 101, 201, and 301 of the 1920 Code and later editions included either exactly or substantially the following language: "The rules shall apply to all installations except as modified or waived by the proper administrative authority. They are intended to be so modified or waived whenever they involve expense not justified by the protection secured, or for any other reasons are impracticable; or whenever equivalent or safer construction can be more readily provided in other ways." It is clear that the original codifiers intended to achieve a reasoned balance between the public's needs for both safe and economical utility service, reflecting both the expected degree of a problem and the degree of difficulty in solving the problem. That balance has been continued in the intervening years, as operating conditions have changed and new equipment and installation types have become available. Although these words no longer appear in the NESC, their effect does. The practical experience of the intervening years has led to the inclusion of more stringent requirements in some areas and more relaxed requirements in others. As a result, the NESC is itself the compilation of design, installation, operation, and maintenance requirements that have been shown over the entire history of utility construction to be appropriate to "reduce the life hazard as far as practicable." The NESC comprises specific actions required in recognition of specific conditions. These actions are based upon the potentially conflicting activity that is normally encountered or reasonably anticipated. For example, in all areas except those limited to pedestrians or restricted-height vehicles, the clearances above grade plan for a 4 m (14 ft) high truck (see NESC Appendix A). Vertical clearances are based upon the reference distance based on potentially conflicting activity plus the clearance building block that includes appropriate mechanical and electrical components based upon the part, conductor, or cable above the area. Where the conditions encountered in a given local situation are those specified within the NESC, the required actions constitute good practice for the specific conditions. Where the local conditions differ in some particular way from those specified in the NESC, it is the responsibility of the appropriate party to recognize the differences in conditions with actions that constitute good practice under such differing conditions. Such practice may be reflected in the design of the installation, the construction practices, the maintenance practices, the operating practices, or some combination of the above, as applicable for the given local conditions. An example of such an area is a lumber yard, where fork lifts are normally encountered or reasonably anticipated with vertical extensions exceeding a 4.0 m (14 ft) truck. In such a case, the expected height of the forklift can be added to the appropriate mechanical and electrical component from Table A-1 of NESC Appendix A to produce the appropriate clearance. However, the better way to perform the same task would be to add the difference between the expected conflicting activity and the applicable reference dimension from NESC Appendix Table A-1 (i.e., a 4.0 m [14 ft] truck in this case) to the clearance in the applicable table, thus recognizing the difference in conditions. The result is the same, but it avoids any problem with pulling the wrong mechanical and electrical component from NESC Appendix Table A-2, which is a more complicated table than Table A-1. It is important to note that the NESC recognizes the limitation on expected activities around electrical facilities required under federal and state regulations, Occupational Health and Safety Administration (OSHA) regulations, and high-voltage line safety acts. Those performing acts around power lines have a personal responsibility to plan and control their actions so as to avoid contact with power lines. The rules for lines differ from those for stations. In stations, the apparatus, equipment, and wires are confined to limited areas where access is restricted to trained personnel. In these latter cases, the safeguarding of persons by (1) actual enclosure of the current-carrying parts, (2) use of barriers, or (3) elevation of such parts beyond reach is not only desirable but generally feasible. With overhead lines, on the other hand, the wires and equipment are not confined to limited areas and, with few exceptions, are not under constant observation by trained personnel. Safeguarding by enclosure is feasible with underground lines and, in fact, is in most cases essential to operation. For overhead lines, however, isolation by elevation generally must be depended upon for the safety of persons in the vicinity. The elevations required for effective isolation of overhead lines must be greater than ordinarily would be required inside buildings; the voltages are usually higher, and the height of expected traffic is usually greater. Practice and experience have determined reasonable limits for elevation of lines and equipment and for the necessary strength of their construction. These rules are intended to include the more important requirements from the standpoint of safety, both to the public and to utility workers. Clearance requirements are determined relative to the degree of hazard involved, and strength requirements necessary to meet the required clearances are determined by (1) the degree of safety problem presented by the installation and (2) the mechanical loads to which it is assumed the lines may be subjected. The NESC is a performance code, not a set of design specifications. The NESC construction rules specify what is to be performed, not how it is to be accomplished. For example, to meet the vertical clearance required above a corn field, either (1) taller structures spaced farther apart or (2) shorter structures spaced closer together may be used. The NESC is indifferent to what type of structures or materials are used, as long as applicable clearances and strength requirements are met. In essence, the rules of the NESC give the basic requirements of construction that are necessary for safety. If the responsible party wishes to exceed these requirements for any reason, he may do so for his own purpose, but need not do so for safety purposes. For example, if the combination of required pole placement and overhead clearance requirements indicated that a 11.4 m (37.5 ft) pole would be needed, a 12.2 m (40 ft) pole could be used. Because poles are inventoried in 1.50 m (5 ft) increments for economy purposes, the additional 0.8 m (2.5 ft) of conductor attachment height would be for economy purposes; it is not required for safety. Thus, even though older editions of the Code sometimes used the word "minimum" for clearance or other requirements, the wording generally used in later editions is "not less than" to indicate the basic amount that is required for safety purposes. The 1990 Code was specifically editorially revised to delete the use of the word "minimum" because of intentional or inadvertent misuse of the term by some to imply that the NESC values were some kind of minimum number that should be exceeded in practice; such is not the case. The NESC is the best information that we have available about what needs to be done for safety and what must not be done in various circumstances; it is based on the experiences of hundreds of thousands of installations located in and serving areas with a variety of conditions in a variety of ways. The NESC is the national standard for safety in the installation, maintenance, and operation of electric supply and communication system facilities. Rule 010 is a general statement of the purpose of the Code; the bulk of the rules are concerned with applying this principle in detail to the various construction situations. Where a specific rule provides detailed requirements for particular conditions, the general "purpose" rule is considered to be superseded by the specific requirements. The 2012 Code significantly revised the language of Rule 010 by adding "affected property" and "utility facilities" to the purpose rule as a result of a joint NESC/NEC Task Force that addressed the Purpose and Scope rules of both the NESC and the NEC to limit confusion as to which code applies in different circumstances. The 2017 Code further clarified the language by removing the verbiage "affected property" from the purpose rule due to its being vague and not fully understood.<br/> &copy; 2016 IEEE.},
key = {Electric power system control},
keywords = {Accident prevention;Antennas;Building wiring;Cables;Codes (symbols);Construction equipment;Construction industry;Drops;Electric control equipment;Electric machine control;Electric power transmission;Electric utilities;Enclosures;Farm buildings;Fighter aircraft;Hazards;Industrial hygiene;Laws and legislation;Lighting;Maintenance;Mine trucks;Occupational risks;Overhead lines;Poles;Problem solving;Railroads;Sales;Transformer protection;Underground equipment;Voltage control;Weed control;Wind power;},
note = {ANSI standards;Communication lines;Communications industry;Communications systems;Electric supply;Electric supply systems;Electrical safety;High voltage safety;National Electric Code;Power station;Safety work;},
URL = {http://dx.doi.org/10.1109/IEEESTD.2016.7526285},
versions = {2},
status = {Active - Approved},
standardID = {NESC-2017},
} 


@article{20173504093013,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Erasure correction-based CSMA/CA},
journal = {Annales des Telecommunications/Annals of Telecommunications},
author = {Tortelier, Patrick and Le Ruyet, Didier},
volume = {72},
number = {11-12},
year = {2017},
pages = {653 - 660},
issn = {00034347},
abstract = {It is well known that the performance of carrier sense multiple access with collision avoidance (CSMA/CA) is poor when the number of users increases, because of collisions. In this paper, we consider a modified version of CSMA/CA based on erasure codes at the packet level, which significantly reduces the complexity of the decoding and does not require any change in the underlying physical layer. In order to improve the performance, we use non-binary maximum distance separable (MDS) codes. We give analytical derivation of the global goodput and show that there is a trade-off between the code parameters and the length of the contention window in order to maximize the global goodput for a given number of users.<br/> &copy; 2017, Institut Mines-T&eacute;l&eacute;com and Springer-Verlag France SAS.},
key = {Carrier sense multiple access},
keywords = {Carrier communication;Codes (symbols);Economic and social effects;Forward error correction;IEEE Standards;Network layers;Packet loss;},
note = {802.11;Carrier sense multiple access with collision avoidances;Contention window;CSMA/CA;Erasure codes;Erasure correction;Maximum distance separable code;Performance evaluations;},
URL = {http://dx.doi.org/10.1007/s12243-017-0606-3},
} 


@book{20182705488489,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Introduction to devOps with chocolate, LEGO and scrum game},
journal = {Introduction to DevOps with Chocolate, LEGO and Scrum Game},
author = {Pylayeva, Dana},
year = {2017},
pages = {1 - 140},
abstract = {Discover a role-based simulation game designed to introduce DevOps in a very unusual way. Working with LEGO and chocolate, using avatars, personas, and role cards, you will gain an understanding of the Dev and Ops roles as well as their interdependencies. Throughout the game, players go through a range of emotions and learn to expand the boundaries of individual roles, acquire T-shaped skills, and grow the Scrum-team circle to include Operations. The game combines ideas from &ldquo;The Phoenix Project&rdquo; with the experience gained from real-life challenges, encountered by development and operations teams in many organizations. Security vulnerabilities, environments patching, deployment code freeze, development and operations silos - the game helps simulate an end-to-end product delivery process and visualize the bottlenecks in the value delivery flow. Introduction to DevOps with Chocolate, LEGO and Scrum Game engages all five senses to maximize learning effectiveness and in three sprints takes players through a gamified DevOps transformation journey. What You Will Learn Play the Chocolate, LEGO and Scrum role-simulation game Gain knowledge of DevOps and how to apply the game to it See how this game illustrates the DevOps cycle as a case study Who This Book Is For Programmers or system admins/project managers who are new to DevOps. DevOps trainers and Agile Coaches who are interested in offering a collaborative and engaging learning experience to their teams.<br/> &copy; 2017 by Dana Pylayeva.},
note = {Development and operations;Learning effectiveness;Learning experiences;Role-based;Security vulnerabilities;Simulation games;T-shaped skills;Value delivery;},
URL = {http://dx.doi.org/10.1007/978-1-4842-2565-3},
} 


@inproceedings{20144700229878,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Semantics-aware Android malware classification using weighted contextual API dependency graphs},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
author = {Zhang, Mu and Duan, Yue and Yin, Heng and Zhao, Zhiruo},
year = {2014},
pages = {1105 - 1116},
issn = {15437221},
address = {Scottsdale, AZ, United states},
abstract = {The drastic increase of Android malware has led to a strong interest in developing methods to automate the malware analysis process. Existing automated Android malware detection and classification methods fall into two general categories: 1) signature-based and 2) machine learning-based. Signature-based approaches can be easily evaded by bytecode-level transformation attacks. Prior learning-based works extract features from application syntax, rather than program semantics, and are also subject to evasion. In this paper, we propose a novel semantic-based approach that classifies Android malware via dependency graphs. To battle transformation attacks, we extract a weighted contextual API dependency graph as program semantics to construct feature sets. To fight against malware variants and zero-day malware, we introduce graph similarity metrics to uncover homogeneous application behaviors while tolerating minor implementation differences. We implement a prototype system, DroidSIFT, in 23 thousand lines of Java code. We evaluate our system using 2200 malware samples and 13500 benign samples. Experiments show that our signature detection can correctly label 93% of malware instances; our anomaly detector is capable of detecting zero-day malware with a low false negative rate (2%) and an acceptable false positive rate (5.15%) for a vetting purpose. Copyright 2014 ACM.<br/>},
key = {Malware},
keywords = {Android (operating system);Application programs;Computer crime;Image retrieval;Learning systems;Program processors;Semantics;},
note = {Android;Anomaly detection;Graph similarity;Malware classifications;Signature detection;},
URL = {http://dx.doi.org/10.1145/2660267.2660359},
} 


@inproceedings{20153101079891,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Semantics-aware android malware classification using weighted contextual API dependency graphs},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
author = {Zhang, Mu and Duan, Yue and Yin, Heng and Zhao, Zhiruo},
volume = {2014-November},
number = {November},
year = {2014},
pages = {1105 - 1116},
issn = {15437221},
address = {Scottsdale, AZ, United states},
abstract = {The drastic increase of Android malware has led to a strong interest in developing methods to automate the malware analysis process. Existing automated Android malware detection and classification methods fall into two general categories: 1) signature-based and 2) machine learning-based. Signature-based approaches can be easily evaded by bytecode-level transformation attacks. Prior learning-based works extract features from application syntax, rather than program semantics, and are also subject to evasion. In this paper, we propose a novel semantic-based approach that classifies Android malware via dependency graphs. To battle transformation attacks, we extract a weighted contextual API dependency graph as program semantics to construct feature sets. To fight against malware variants and zero-day malware, we introduce graph similarity metrics to uncover homogeneous application behaviors while tolerating minor implementation differences. We implement a prototype system, DroidSIFT, in 23 thousand lines of Java code. We evaluate our system using 2200 malware samples and 13500 benign samples. Experiments show that our signature detection can correctly label 93% of malware instances; our anomaly detector is capable of detecting zero-day malware with a low false negative rate (2%) and an acceptable false positive rate (5.15%) for a vetting purpose.<br/> Copyright &copy; 2014 ACM.},
key = {Malware},
keywords = {Android (operating system);Application programs;Computer crime;Image retrieval;Learning systems;Program processors;Semantics;},
note = {Android;Anomaly detection;Graph similarity;Malware classifications;Signature detection;},
URL = {http://dx.doi.org/10.1145/2660267.2660359},
} 


@inproceedings{20141817640355,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {On-stack replacement to improve JIT-based obfuscation a preliminary study},
journal = {Proceedings of the 2013 2nd International Japan-Egypt Conference on Electronics, Communications and Computers, JEC-ECC 2013},
author = {Yusuf, Marwa and El-Mahdy, Ahmed and Rohou, Erven},
year = {2013},
pages = {94 - 99},
address = {Cairo, Egypt},
abstract = {As more devices are connecting together, more effective security techniques are needed to protect running software from hackers. One possible security technique is to continuously change the binary code running of given software by recompiling it on the fly. This switching need to be done frequently, quickly, and randomly, not constrained by specific locations in code, to make it difficult for the hacker to track the behavior of the running code or predict its functionality. In our research we are working on a technique that recompiles speculatively and concurrently with current execution, and switches to the new compiled version dynamically, at arbitrary points. This paper presents an early analytical study augmented by experimental analysis on manually applying this technique on simple kernels, to study the concept in comparison with other similar techniques. &copy; 2013 IEEE.<br/>},
key = {Codes (symbols)},
keywords = {Personal computing;},
note = {Code-switching;LLVM;obfuscation;on stack replacement;Recompilation;security;},
URL = {http://dx.doi.org/10.1109/JEC-ECC.2013.6766392},
} 


@inproceedings{20131416167570,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Lightweight online software updates for C applications},
journal = {IASTED Multiconferences - Proceedings of the IASTED International Conference on Software Engineering, SE 2013},
author = {Kim, Dong Kwan},
year = {2013},
pages = {768 - 774},
address = {Innsbruck, Austria},
abstract = {It is obvious that software after its initial deployment changes over time statically or dynamically. Some software systems such as servers and safety-critical systems cannot allow for static code changes-fixing and rebooting a running program. Dynamic software updates (DSU) make it possible to change executing program code without rebooting or stopping. DSU systems can be used to modify applications on the fly, thus saving the programmer's time and using computing resources more productively. This paper presents a lightweight dynamic updating system which replaces currently running software using dynamic linking. This update system implements software updates in the same process memory space. Thus, the new version of software is required to be loaded into the shard memory of the existing process's virtual memory space. The state of the old version (i.e., local and global variables) is transferred to the new one. The experimental results indicate that the proposed updating system have the potential to become an effective tool for C applications.<br/>},
key = {C (programming language)},
keywords = {Application programs;Codes (symbols);Computer software maintenance;Safety engineering;},
note = {Code changes;Computing resource;Dynamic software update;Dynamic software updating;Initial deployments;On-line softwares;Safe updating points;Safety critical systems;},
URL = {http://dx.doi.org/10.2316/P.2013.796-029},
} 


@inproceedings{20161502209342,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Comparison of absolute and relative strategies to encode sensorimotor transformations in tool-use},
journal = {5th Joint International Conference on Development and Learning and Epigenetic Robotics, ICDL-EpiRob 2015},
author = {Braud, Raphael and Pitti, Alexandre and Gaussier, Philippe},
year = {2015},
pages = {267 - 268},
address = {Providence, RI, United states},
abstract = {We explore different strategies to overcome the problem of sensorimotor transformation that babies face during development, especially in the case of tool-use. From a developmental perspective, we investigate a model based on absolute coordinate frames of reference, and another one based on relative coordinate frames of reference. In a situation of sensorimotor learning and of adaptation to tool-use, we perform a computer simulation of a 4 degrees of freedom robot. We show that the relative coordinate strategy is the most rapid and robust to re-adapt the neural code.<br/> &copy; 2015 IEEE.},
key = {Degrees of freedom (mechanics)},
keywords = {Robotics;},
note = {4 degrees of freedom;Absolute coordinate;Frames of reference;Model-based OPC;Neural code;Relative coordinates;Sensorimotor transformations;Tool use;},
URL = {http://dx.doi.org/10.1109/DEVLRN.2015.7346154},
} 


@article{20121114845241,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Compiler techniques to improve dynamic branch prediction for indirect jump and call instructions},
journal = {Transactions on Architecture and Code Optimization},
author = {McCandless, Jason and Gregg, David},
volume = {8},
number = {4},
year = {2012},
issn = {15443566},
abstract = {Indirect jump instructions are used to implement multiway branch statements and virtual function calls in object-oriented languages. Branch behavior can have significant impact on program performance, but fortunately hardware predictors can alleviate much of the risk. Modern processors include indirect branch predictors which use part of the target address to update a global history. We present a code generation technique to maximize the branch history information available to the predictor. We implement our optimization as an assembly language transformation, and evaluate it for SPEC benchmarks and interpreters using simulated and real hardware, showing indirect branch misprediction decreases. &copy; 2012 ACM.<br/>},
key = {Program compilers},
keywords = {Hardware;Program interpreters;},
note = {Assembly language;Branch prediction;Compiler techniques;History informations;Modern processors;Object oriented;Program performance;Virtual functions;},
URL = {http://dx.doi.org/10.1145/2086696.2086703},
} 


@inproceedings{20161102106856,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Fraglight: Shedding light on broken pointcuts in evolving Aspect-Oriented software},
journal = {SPLASH Companion 2015 - Companion Proceedings of the 2015 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
author = {Khatchadourian, Raffi and Rashid, Awais and Masuhara, Hidehiko and Watanabe, Takuya},
year = {2015},
pages = {17 - 18},
address = {Pittsburgh, PA, United states},
abstract = {Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base-code can lead to join points incorrectly falling in or out of the scope of pointcuts. Deciding which pointcuts have broken due to base-code changes is a daunting venture, especially in large and complex systems. We demonstrate an automated tool called FRAGLIGHT that recommends a set of pointcuts that are likely to require modification due to a particular base-code change. The underlying approach is rooted in harnessing unique and arbitrarily deep structural commonality between program elements corresponding to join points selected by a pointcut in a particular software version. Patterns describing such commonality are used to recommend pointcuts that have potentially broken with a degree of confidence as the developer is typing. Our tool is implemented as an extension to the Mylyn Eclipse IDE plug-in, which maintains focused contexts of entities relevant to a task.<br/> &copy; 2015 ACM.},
key = {Aspect oriented programming},
keywords = {Application programs;Codes (symbols);Computer systems programming;},
note = {Aspect-oriented software;Automated tools;Code changes;Degree of confidence;Program elements;Shedding light;Software Evolution;Software versions;},
URL = {http://dx.doi.org/10.1145/2814189.2814195},
} 


@inproceedings{20171003423717,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Performance of Direct Displacement Based Design on Regular Concrete Building Against Indonesian Response Spectrum},
journal = {Procedia Engineering},
author = {Muljati, Ima and Lumantarna, Benjamin and Intan, Reynaldo P. and Valentino, Arygianny},
volume = {171},
year = {2017},
pages = {1019 - 1024},
issn = {18777058},
address = {Bali, Indonesia},
abstract = {The renewal of Indonesian seismic code from SNI 1726-2002 into SNI 1726-2012 brings significant change in the design spectrum. Focused on several regular plan concrete building which have been design using displacement based design method, the aim of this study is to verify their performance using nonlinear time history analysis based on parameters: drift, damage indices, and plastic mechanism determined by FEMA 356. The excitation is spectrum consistent accelerogram based on El-Centro 1940 N-S, to match with the new Indonesian response spectrum for soft soil in low- and high intensity area. It is found that the code-designed buildings are not suitable for the targeted design of level-2 with maximum drift of 2.5% due to major. This is caused by improper selection of SNI spectrum as the design major earthquake. In fact, it is only equivalent to small earthquake. Although buildings survive up to a very rare earthquake without collapse but they suffer excessive damage and rotation due to small- to major-earthquake. The capacity design procedure is able to maintain ductile mechanism, but some columns experience yielding at prohibited locations.<br/> &copy; 2017 The Authors.},
key = {Architectural design},
keywords = {Concrete buildings;Concretes;Earthquakes;Rare earths;Seismic design;Spectrum analysis;},
note = {Direct displacement-based design;Displacement-based design methods;Nonlinear time history analysis;Performance based design;Plastic mechanism;Response spectra;Small earthquakes;Time history analysis;},
URL = {http://dx.doi.org/10.1016/j.proeng.2017.01.438},
} 


@article{20142917945336,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Assessment of ASSERT-PV for prediction of critical heat flux in CANDU bundles},
journal = {Nuclear Engineering and Design},
author = {Rao, Y.F. and Cheng, Z. and Waddington, G.M.},
volume = {276},
year = {2014},
pages = {216 - 227},
issn = {00295493},
abstract = {Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The recently released ASSERT-PV 3.2 provides enhanced models for improved predictions of flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against five full-scale CANDU bundle experiments conducted in 1990s and in 2009 by Stern Laboratories (SL), using 28-, 37- and 43-element (CANFLEX) bundles. A total of 15 CHF test series with varying pressure-tube creep and/or bearing-pad height were analyzed. The SL experiments encompassed the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for CANDU reactors. Code predictions of channel dryout power and axial and radial CHF locations were compared against measurements from the SL CHF tests to quantify the code prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. Furthermore, the sensitivity studies evaluated the contribution of each CHF model change or enhancement to the improvement in CHF prediction. Overall, the assessment demonstrated significant improvement in prediction of channel dryout power and axial and radial CHF locations in horizontal fuel channels containing CANDU bundles. &copy; 2014 Elsevier Ltd. All rights reserved.<br/>},
key = {Heat flux},
keywords = {Codes (symbols);Forecasting;Heat transfer;Nuclear industry;Nuclear reactors;Two phase flow;},
note = {Atomic energy of canada limiteds;Canadian nuclear industry;Code predictions;Critical heat flux(CHF);Flow distribution;Pv applications;Sensitivity studies;Thermal hydraulics codes;},
URL = {http://dx.doi.org/10.1016/j.nucengdes.2014.06.004},
} 


@article{20161002072282,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Development of a three-zone transport model for activated corrosion products analysis of Tokamak Cooling Water System},
journal = {Fusion Engineering and Design},
author = {Zhang, Jingyu and Li, Lu and He, Shuxiang and Song, Wen and Fu, Yu and Zhang, Bin and Chen, Yixue},
volume = {109-111},
year = {2016},
pages = {407 - 410},
issn = {09203796},
abstract = {In the Tokamak Cooling Water System (TCWS), the activated corrosion products (ACPs) play as an important radioactive source, which have impact on reactor inspection and maintenance. A three-zone transport model of ACPs was elaborated in this paper, which is based on the theory that the main driving force for ACPs transport is the temperature change of the coolant throughout the loop and the resulting change in metal ion solubility in the coolant. The three-zone transport model was used to replace the loop-homogenization model in the ACPs evaluation code CATE 1.0, developing CATE to give the capability to calculate spatial distribution of ACPs. As a result, CATE was upgraded to version 2.0. For code testing, a FW/BLK cooling loop of ITER was simulated using CATE 2.0, and the composition and radioactivity of ACPs were calculated. The results showed that the major contributors came from the short-life nuclides, especially Mn-56, which can influence material choice in reactor design and shutdown time before reactor maintenance.<br/> &copy; 2016 Elsevier B.V.},
key = {Cooling water},
keywords = {Codes (symbols);Coolants;Cooling;Corrosion;Metal ions;Metals;Radiation;Radioactivity;Tokamak devices;Water cooling systems;Waterworks;},
note = {CATE 2.0 code;Cooling water systems;Corrosion products;Corrosion products analysis;Inspection and maintenance;Radioactive sources;Temperature changes;Transport modeling;},
URL = {http://dx.doi.org/10.1016/j.fusengdes.2016.02.091},
} 


@article{20181605025399,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Local Transformed Features for Epileptic Seizure Detection in EEG Signal},
journal = {Journal of Medical and Biological Engineering},
author = {Jaiswal, Abeg Kumar and Banka, Haider},
volume = {38},
number = {2},
year = {2018},
pages = {222 - 235},
issn = {16090985},
abstract = {Epilepsy is a well known neurological disorder characterized by the presence of recurrent seizures. Electroencephalograms (EEGs) record electrical activity in the brain and are used to detect epilepsy. Traditional EEG analysis methods for epileptic seizure detection are time-consuming, which has led to the recent proposal of several automated seizure detection frameworks. Feature extraction and classification are two important steps in this procedure. Feature extraction focuses on finding the informative features that could be used in the classification step for correct decision making; therefore, proposing some effective feature extraction techniques for seizure detection is of great significance. This paper introduces two novel feature extraction techniques: local centroid pattern (LCP) and one-dimensional local ternary pattern (1D-LTP) for seizure detection in EEG signal. Both the techniques are computationally simple and easy to implement. In both the techniques, the histograms are formed in the first step using the transformation code and then these histogram-based feature vectors are fed into a classifier in the second step. The performance of the proposed techniques was evaluated through 10-fold cross-validation tested on the benchmark dataset. Different machine learning classifiers were used for the classification. The experimental results show that LCP and 1D-LTP achieved the highest accuracy of 100% for the classification between normal and seizure EEG signals with the artificial neural network classifier. Nine different experimental cases have been tested. The results achieved for different experimental cases were higher than the results of some existing techniques in the literature. The experimental results indicate that LCP and 1D-LTP could be effective feature extraction techniques for seizure detection.<br/> &copy; 2017, Taiwanese Society of Biomedical Engineering.},
key = {Biomedical signal processing},
keywords = {Benchmarking;Bioelectric phenomena;Brain;Classification (of information);Decision making;Electroencephalography;Extraction;Feature extraction;Graphic methods;Learning systems;Neural networks;Neurodegenerative diseases;Neurophysiology;Signal detection;},
note = {10-fold cross-validation;Artificial neural network classifiers;Electroencephalogram signals;Epileptic seizure detection;Feature extraction and classification;Feature extraction techniques;Local centroid pattern (LCP);Local ternary patterns;},
URL = {http://dx.doi.org/10.1007/s40846-017-0286-5},
} 


@article{20122515135057,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Reactive transport modeling of subsurface flow constructed wetlands using the HYDRUS wetland module},
journal = {Vadose Zone Journal},
author = {Langergraber, Gunter and imunek, Jirka},
volume = {11},
number = {2},
year = {2012},
issn = {15391663},
abstract = {Constructed wetlands (CWs) are engineered water treatment systems designed to remove various types of contaminants. A large number of processes simultaneously contribute to water quality improvement in CWs. During the last decade, there has been a wide interest in the understanding of complex "constructed wetland" systems, including the development of numerical process-based models describing these systems. A number of process-based numerical models for subsurface flow (SSF) CWs have been developed during the last few years; however, most of them are either in an early stage of development or are available only in-house. The HYDRUS wetland module is the only implementation of a CW model that is currently publicly available. Version 2 of the HYDRUS wetland module includes two biokinetic model formulations simulating reactive transport in CWs: CW2D and CWM1. In CW2D, aerobic and anoxic transformation and degradation processes for organic matter, N, and P are considered, whereas in CWM1, aerobic, anoxic, and anaerobic processes for organic matter, N, and S are taken into account. We simulated horizontal flow CWs using both biokinetic models. Compared with the CWM1 implementation in the RETRASO code, the HYDRUS implementation was able to simulate fixed biomass, which is of high importance for obtaining realistic predictions for the treatment efficiency of CWs. We also compared simulation results for horizontal flow CWs obtained using both CW2D and CWM1 modules that showed that CWM1 produces more reasonable results because it also considers anaerobic degradation processes. The influence of wetland plants on the simulation results was also investigated. Simulated biomass profiles in the filter were completely different when considering O<inf>2</inf>release from roots, thus indicating the importance of considering plant effects. &copy; Soil Science Society of America.<br/>},
key = {Wetlands},
keywords = {Biogeochemistry;Biological materials;Organic compounds;Water quality;Water treatment;},
note = {Anaerobic degradation;Anoxic transformation;Constructed wetlands;Constructed wetlands (CWs);Reactive transport modeling;Subsurface flow constructed wetlands;Water quality improvements;Water treatment systems;},
URL = {http://dx.doi.org/10.2136/vzj2011.0104},
} 


@inproceedings{20133116549279,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {GraphX: A resilient distributed graph system on spark},
journal = {1st International Workshop on Graph Data Management Experiences and Systems, GRADES 2013 - Co-located with SIGMOD/PODS 2013},
author = {Xin, Reynold S. and Gonzalez, Joseph E. and Franklin, Michael J. and Stoica, Ion},
year = {2013},
pages = {ACM Special Interest Group on Management of Data (SIGMOD) - },
address = {New York, NY, United states},
abstract = {From social networks to targeted advertising, big graphs capture the structure in data and are central to recent advances in machine learning and data mining. Unfortunately, directly applying existing data-parallel tools to graph computation tasks can be cumbersome and inefficient. The need for intuitive, scalable tools for graph computation has lead to the development of new graph-parallel systems (e.g., Pregel, PowerGraph) which are designed to efficiently execute graph algorithms. Unfortunately, these new graph-parallel systems do not address the challenges of graph construction and transformation which are often just as problematic as the subsequent computation. Furthermore, existing graph-parallel systems provide limited fault-tolerance and support for interactive data mining. We introduce GraphX, which combines the advantages of both data-parallel and graph-parallel systems by efficiently expressing graph computation within the Spark data-parallel framework. We leverage new ideas in distributed graph representation to efficiently distribute graphs as tabular data-structures. Similarly, we leverage advances in data-flow systems to exploit in-memory computation and fault-tolerance. We provide powerful new operations to simplify graph construction and transformation. Using these primitives we implement the PowerGraph and Pregel abstractions in less than 20 lines of code. Finally, by exploiting the Scala foundation of Spark, we enable users to interactively load, transform, and compute on massive graphs. Copyright &copy; 2013 ACM.<br/>},
key = {Information management},
keywords = {Computational efficiency;Data mining;Fault tolerance;Learning systems;},
note = {Data flow systems;Graph algorithms;Graph computations;Graph construction;Graph representation;Interactive data mining;Memory computations;Targeted advertising;},
URL = {http://dx.doi.org/10.1145/2484425.2484427},
} 


@article{20153201157672,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Open issues in model transformations for multimodal applications},
journal = {Journal on Multimodal User Interfaces},
author = {Lengyel, Laszlo and Charaf, Hassan},
volume = {9},
number = {4},
year = {2015},
pages = {377 - 385},
issn = {17837677},
abstract = {Multimodal human&ndash;computer interaction refers to the interaction with the virtual and physical environment through natural modes of communication. Multimodal input user interfaces have significant role in different domains, for example industrial plants or hospitals, also they have implications for accessibility. To develop multimodal applications in formally rigorous settings, software developer teams may use tools or a software development kit to increase the efficiency and the quality of the resulted software artifacts. Such a technique is performing the design with software modeling and applying model transformations to generate well-defined components of the software. Furthermore, representation-bridging communication is a discipline of cognitive infocommunications, where the sensory information transferred to the receiver entity is filtered and/or converted. Whenever such approaches are used, the challenges associated with the modeling of information requirements, user capabilities and cross-model interactions are compounded and further increase the need for formal design and verification tools. Applying model transformations is a way to support this activity. Communication-intensive solutions often require complex methods, i.e. significant model transformation efforts between the different representations. Important semantic information should be preserved and not misinterpreted in a complex model transformations. Therefore, methods are required to verify that the semantics used during the application generation and analysis are indeed preserved across the transformation. As a case in point, such a model transformation could yield embedded code for a given type of electronic driver assistant system based on a high-level characterizations of the information to be transferred and the driver&rsquo;s cognitive capabilities. Later, a multimodal interactions expert could easily modify those characterizations on demand, and regenerate a modified version of the software without having to know about the low-level details of the embedded platform. This paper provides a strong motivation regarding the necessity of methods to support verification and validation of model transformations supporting multimodal application development and cognitive infocommunications. As the main result of the paper, we compile a list of open issues in the field of verification/validation of model transformations, and link those issues to the development of multimodal interfaces. Through its discussions, the paper makes the point that the design practices behind multimodal interfaces could strongly benefit from the use of formal modeling techniques in general, and model transformation approaches in particular.<br/> &copy; 2015, OpenInterface Association.},
key = {Software design},
keywords = {Application programs;Automobile drivers;Computer software selection and evaluation;Human computer interaction;Industrial plants;Information filtering;Interactive computer systems;Semantics;User interfaces;Virtual reality;},
note = {Cognitive infocommunications;Information requirement;Model transformation;Multi-Modal Interactions;Multi-modal interfaces;Multimodal application;Software development kit;Verification-and-validation;},
URL = {http://dx.doi.org/10.1007/s12193-015-0192-5},
} 


@inproceedings{20133716716607,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Investigation of the effects of soluble boron tracking on coupled CTF / NEM, LWR simulations},
journal = {International Conference on Mathematics and Computational Methods Applied to Nuclear Science and Engineering, M and C 2013},
author = {Biery, M. and Avramova, M. and Ivanov, K.},
volume = {3},
year = {2013},
pages = {1786 - 1796},
address = {Sun Valley, ID, United states},
abstract = {The primary objective of this study is to evaluate the effects of introducing a boron tracking capability to the COBRA-TF / NEM code coupling. The Pennsylvania State University (PSU) versions of COBRA-TF - CTF, and Nodal Expansion Method (NEM) codes are utilized. Previous implementations of the CTF / NEM coupled code had no capability to model soluble boron feedback effects due to boron transport. This study builds upon the validation and qualification efforts of the boron tracking model implementation in CTF by modeling the boron feedback calculated by the CTF boron tracking model in NEM. The core model chosen for this study is the Purdue MOX/UO<inf>2</inf>core model used in the 2007 OECD/NRC code benchmark study. Following the implementation of an explicit online coupling scheme and accompanying k-search routine, the newly coupled CTF / NEM code version with boron tracking is compared to prior results of the non-boron tracking CTF / NEM code version at steady-state hot full power and hot zero power conditions. It was found that the boron tracking model exhibited little influence on the hot zero power result as expected due to a smaller heat flux, which does not significantly change the moderator density and boron concentration as the moderator travels up the axial core length. Meanwhile the boron tracking model had a much greater impact on the hot full power results, predicting the critical inlet boron concentration to be 9.9 ppm below the non-boron tracking result due to greater and more rapid changes in boron concentration corresponding to the reduction in moderator density from being more rapidly heated.<br/>},
key = {Boron},
keywords = {Codes (symbols);Computational methods;Heat flux;Light water reactors;Moderators;},
note = {Boron concentrations;Coupled code;CTF/NEM;Nodal expansion method;Pennsylvania State University;Primary objective;Subchannels;Tracking capability;},
} 


@article{20140217185337,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Numerical modelling of the tailored tempering process applied to 22MnB5 sheets},
journal = {Finite Elements in Analysis and Design},
author = {Tang, B.T. and Bruschi, S. and Ghiotti, A. and Bariani, P.F.},
volume = {81},
year = {2014},
pages = {69 - 81},
issn = {0168874X},
abstract = {In order to enhance the crash characteristics and geometrical accuracy, components hot formed in a fully martensitic state have gained in the last few years more and more importance. However, the very high strength exhibited by these components makes subsequent operations such as cutting difficult due to the high process forces and associated high wear of the cutting tools. Moreover, for some applications, such as B-pillars and other automotive components that may undergo impact loading, it may be desirable to create regions of the part with softer and more ductile microstructures. The novel process called the tailored tempering process allows doing this by suppressing the martensitic transformation in those zones of the sheet located under heated parts of the tools. In the paper, a numerical model of the tailored tempering process was developed, accurately calibrated and validated through a laboratory-scale hot forming process. Using the commercial FE code Forge<sup>&trade;</sup>a fully coupled thermo-mechanical-metallurgical model of the process was set up. The influence of the phase transformation kinetics was taken into account by implementing in the model phase transformation data, namely the shift of the TTT curves due to the applied stress and the transformation plasticity coefficients, gained from an extensive dilatometric experimental campaign and analysis. A laboratory-scale hot-formed U-channel was produced using segmented tools with heated and cooled zones so that the cooling rate of the blank can be locally controlled during the hot forming process. The part Vickers hardness distribution and microstructural evolution predicted by FORGE<sup>&trade;</sup>were then compared with the experimental results, proving the validation of the numerical model by taking into account the influence of the transformation plasticity and deformation history on the phase transformation kinetics. &copy; 2013 Elsevier B.V.<br/>},
key = {Numerical models},
keywords = {Boron compounds;Cutting tools;Kinetics;Manganese compounds;Martensitic transformations;Metadata;Tempering;Vickers hardness;},
note = {Boron steels;Crash characteristics;Hardness distribution;Modeling phase transformation;Phase transformation kinetics;Tailored tempering;Transformation Plasticity;Transformation plasticity coefficient;},
URL = {http://dx.doi.org/10.1016/j.finel.2013.11.009},
} 


@inproceedings{20180904845452,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A set of patterns for concurrent and parallel programming teaching},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Capel, Manuel I. and Tomeu, Antonio J. and Salguero, Alberto G.},
volume = {10659 LNCS},
year = {2018},
pages = {203 - 215},
issn = {03029743},
address = {Santiago de Compostela, Spain},
abstract = {The use of key parallel-programming patterns has proved to be extremely helpful for mastering difficult concurrent and parallel programming concepts and the associated syntactical constructs. The method suggested here consists of a substantial change of more traditional teaching and learning approaches to teach programming. According to our approach, students are first introduced to concurrency problems through a selected set of preliminar program code-patterns. Each pattern also has a series of tests with selected samples to enable students to discover the most common cases that cause problems and then the solutions to be applied. In addition, this paper presents the results obtained from an informal assessment realized by the students of a course on concurrent and real-time programming that belongs to the computer engineering (CE) degree. The obtained results show that students feel now to be more actively involved in lectures, practical lessons, and thus students make better use of their time and gain a better understanding of concurrency topics that would not have been considered possible before the proposed method was implemented at our University.<br/> &copy; Springer International Publishing AG, part of Springer Nature 2018.},
key = {Teaching},
keywords = {Parallel programming;Students;},
note = {Blended learning;Computer engineering;Concurrent programming;Informal assessments;Parallel design patterns;Real time programming;Traditional teachings;Virtual campus;},
URL = {http://dx.doi.org/10.1007/978-3-319-75178-8_17},
} 


@article{20163602767450,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Adaptive Online Sequential ELM for Concept Drift Tackling},
journal = {Computational Intelligence and Neuroscience},
author = {Budiman, Arif and Fanany, Mohamad Ivan and Basaruddin, Chan},
volume = {2016},
year = {2016},
issn = {16875265},
abstract = {A machine learning method needs to adapt to over time changes in the environment. Such changes are known as concept drift. In this paper, we propose concept drift tackling method as an enhancement of Online Sequential Extreme Learning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by adding adaptive capability for classification and regression problem. The scheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier scheme that works well to handle real drift, virtual drift, and hybrid drift. The AOS-ELM also works well for sudden drift and recurrent context change type. The scheme is a simple unified method implemented in simple lines of code. We evaluated AOS-ELM on regression and classification problem by using concept drift public data set (SEA and STAGGER) and other public data sets such as MNIST, USPS, and IDS. Experiments show that our method gives higher kappa value compared to the multiclassifier ELM ensemble. Even though AOS-ELM in practice does not need hidden nodes increase, we address some issues related to the increasing of the hidden nodes such as error condition and rank values. We propose taking the rank of the pseudoinverse matrix as an indicator parameter to detect "underfitting" condition.<br/> &copy; 2016 Arif Budiman et al.},
key = {Learning systems},
keywords = {Classification (of information);Network security;},
note = {Adaptive capabilities;Indicator parameters;Machine learning methods;Multi-classifier;Online sequential elms;Online sequential extreme learning machine;Pseudo-inverse matrixes;Regression problem;},
URL = {http://dx.doi.org/10.1155/2016/8091267},
} 


@inproceedings{20181404983438,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An experience report on (Auto-)tuning of mesh-based PDE solvers on shared memory systems},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Charrier, Dominic E. and Weinzierl, Tobias},
volume = {10778 LNCS},
year = {2018},
pages = {3 - 13},
issn = {03029743},
address = {Czestochowa, Poland},
abstract = {With the advent of manycore systems, shared memory parallelisation has gained importance in high performance computing. Once a code is decomposed into tasks or parallel regions, it becomes crucial to identify reasonable grain sizes, i.e. minimum problem sizes per task that make the algorithm expose a high concurrency at low overhead. Many papers do not detail what reasonable task sizes are, and consider their findings craftsmanship not worth discussion. We have implemented an autotuning algorithm, a machine learning approach, for a project developing a hyperbolic equation system solver. Autotuning here is important as the grid and task workload are multifaceted and change frequently during runtime. In this paper, we summarise our lessons learned. We infer tweaks and idioms for general autotuning algorithms and we clarify that such a approach does not free users completely from grain size awareness.<br/> &copy; Springer International Publishing AG, part of Springer Nature 2018.},
key = {Learning systems},
keywords = {Artificial intelligence;Grain size and shape;Memory architecture;},
note = {Auto-tuning algorithms;Autotuning;Grain size;High performance computing;Machine learning approaches;Shared memory;Shared memory parallelisation;Shared memory system;},
URL = {http://dx.doi.org/10.1007/978-3-319-78054-2_1},
} 


@inproceedings{20135017079241,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Recording and analyzing in-browser programming sessions},
journal = {ACM International Conference Proceeding Series},
author = {Helminen, Juha and Ihantola, Petri and Karavirta, Ville},
year = {2013},
pages = {13 - 22},
address = {Koli, Finland},
abstract = {In this paper, we report on the analysis of a novel type of automatically recorded detailed programming session data collected on a university-level web programming course. We present a method and an implementation of collecting rich data on how students learning to program edit and execute code and explore its use in examining learners' behavior. The data collection instrument is an in-browser Python programming environment that integrates an editor, an execution environment, and an interactive Python console and is used to deliver programming assignments with automatic feedback. Most importantly, the environment records learners' interaction within it. We have implemented tools for viewing these traces and demonstrate their potential in learning about the programming processes of learners and of benefiting computing education research and the teaching of programming. &copy; 2013 ACM.<br/>},
key = {Education computing},
keywords = {Data acquisition;High level languages;Learning systems;Teaching;},
note = {Computer Science Education;Computing education;Programming assignments;Python;Web-based programming;},
URL = {http://dx.doi.org/10.1145/2526968.2526970},
} 


@article{20165203194914,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {There and back again: Can you compile that snapshot?},
journal = {Journal of Software: Evolution and Process},
author = {Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and DiPenta, Massimiliano and Oliveto, Rocco and DeLucia, Andrea and Poshyvanyk, Denys},
volume = {29},
number = {4},
year = {2017},
issn = {20477481},
abstract = {A broken snapshot represents a snapshot from a project's change history that cannot be compiled. Broken snapshots can have significant implications for researchers, as they could hinder any analysis of the past project history that requires code to be compiled. Noticeably, while some broken snapshots may be observable in change history repositories (e.g., no longer available dependencies), some of them may not necessarily happen during the actual development. In this paper, we systematically study the compilability of broken snapshots in 219 395 snapshots belonging to 100 Java projects from the Apache Software Foundation, all relying on Maven as an automated build tool. We investigated broken snapshots from 2 different perspectives: (1) how frequently they happen and (2) likely causes behind them. The empirical results indicate that broken snapshots occur in most (96%) of the projects we studied and that they are mainly due to problems related to the resolution of dependencies. On average, only 38% of the change history of the analyzed systems is currently successfully compilable.<br/> Copyright &copy; 2016 John Wiley & Sons, Ltd.},
key = {Computer software selection and evaluation},
keywords = {Computer software;},
note = {Apache software foundations;broken snapshots;Change history;Compilability;Empirical studies;Mining software repositories;Software Quality;},
URL = {http://dx.doi.org/10.1002/smr.1838},
} 


@inproceedings{20174104260894,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Relationship between design and defects for software in evolution},
journal = {CEUR Workshop Proceedings},
author = {Mileti, Matija and Vukui, Monika and Maus, Goran and Grbac, Tihana Galinac},
volume = {1938},
year = {2017},
issn = {16130073},
address = {Belgrade, Serbia},
abstract = {Successful prediction of defects at an early stage is one of the main goals of software quality assurance. Having an indicator of the severity of defect occurrence may bring further benefit to allocation of testing resources. Code churn in terms of modified lines of code was found to be a good indicators of bugs. However, that metric does not reveal the structural changes of the code design. The idea of this project is to analyze the relationship between the evolution of object oriented software metrics and the appearance of defects. To achieve this, the absolute and relative differences between the initial version of a class and its current version were calculated for each metrics as indicators of design change. Then, the correlation between these differences and the number of defects were analyzed. Our case study showed that certain metrics have no influence on defect occurrence, while several of them exhibit moderate level of correlation. In addition, we concluded that the relative differences were inappropriate indicator for determining the relationship.<br/> &copy; Copyright 2017 by the paper's authors.},
key = {Object oriented programming},
keywords = {Application programs;Codes (symbols);Computer software selection and evaluation;Correlation methods;Defects;Quality assurance;Quality control;},
note = {Code designs;Design change;Lines of code;Object-oriented software metrics;Software quality assurance;Testing resources;},
} 


@inproceedings{20151900835652,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {15th International Conference on Enterprise Information Systems, ICEIS 2013},
journal = {Lecture Notes in Business Information Processing},
volume = {190},
year = {2014},
pages = {1 - 541},
issn = {18651348},
address = {Angers, France},
abstract = {The proceedings contain 40 papers. The special focus in this conference is on Databases, Information Systems Integration, Artificial Intelligence, Decision Support Systems, Information Systems Analysis and Specification. The topics include: An evaluation of multi-way joins for relational database systems; applying semantic web tools and techniques to the textile traceability; estimating sufficient sample sizes for approximate decision support queries; a data-driven prediction framework for analyzing and monitoring business process performances; an overview of experimental studies on software inspection process; optimizing power, heating, and cooling capacity on a decision-guided energy investment framework; business intelligence for improving supply chain risk management; Bayesian prediction of fault-proneness of agile-developed object-oriented system; foundation for fine-grained security and DRM control based on a service call graph context identification; capturing semiotic and social factors of organizational evolution; the change impact analysis in BPM based software applications; reengineering of object-oriented software into aspect-oriented ones supported by class models; re-learning of business process models from legacy system using incremental process mining; an analysis of the usage of a micro blogging system; cloud-based collaborative business services provision; an architecture for health information exchange in pervasive healthcare environment; dealing with usability in model-driven development method; self-service classroom capture generating interactive multivideo objects; from gaps to transformation paths in enterprise architecture planning; an automated architectural evaluation approach based on metadata and code analysis and blueprint of a semantic business process-aware enterprise information architecture.},
} 


@inproceedings{20173003975031,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards Automatic Generation of Short Summaries of Commits},
journal = {IEEE International Conference on Program Comprehension},
author = {Jiang, Siyuan and McMillan, Collin},
volume = {0},
year = {2017},
pages = {320 - 323},
address = {Buenos Aires, Argentina},
abstract = {Committing to a version control system means submitting a software change to the system. Each commit can have a message to describe the submission. Several approaches have been proposed to automatically generate the content of such messages. However, the quality of the automatically generated messages falls far short of what humans write. In studying the differences between auto-generated and human-written messages, we found that 82% of the human-written messages have only one sentence, while the automatically generated messages often have multiple lines. Furthermore, we found that the commit messages often begin with a verb followed by an direct object. This finding inspired us to use a 'verb+object' format in this paper to generate short commit summaries. We split the approach into two parts: verb generation and object generation. As our first try, we trained a classifier to classify a diff to a verb. We are seeking feedback from the community before we continue to work on generating direct objects for the commits.<br/> &copy; 2017 IEEE.},
key = {Natural language processing systems},
keywords = {Computer programming;Control systems;Information management;},
note = {code differencing;commit log;commit message;Program comprehension;Version control system;},
URL = {http://dx.doi.org/10.1109/ICPC.2017.12},
} 


@inproceedings{20184305986612,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Tackling androids native library malware with robust, efficient and accurate similarity measures},
journal = {ACM International Conference Proceeding Series},
author = {Kalysch, Anatoli and Protsenko, Mykolai and Milisterfer, Oskar and Muller, Tilo},
year = {2018},
pages = {Universitat Hamburg - },
address = {Hamburg, Germany},
abstract = {Code similarity measures create a comparison metric showing to what degree two code samples have the same functionality, e.g., to statically detect the use of known libraries in binary code. They are both an indispensable part of automated malware analysis, as well as a helper for the detection of plagiarism (IP protection) and the illegal use of open-source libraries in commercial apps. The centroid similarity metric extracts control-flow features from binary code and encodes them as geometric structures before comparing them. In our paper, we propose novel improvements to the centroid approach and apply it to the ARM architecture for the first time. We implement our approach as a plug-in for the IDA Pro disassembler and evaluate it regarding efficiency, accuracy and robustness on Android. Based on a dataset of 508,745 APKs, collected from 18 third-party app markets, we achieve a detection rate of 89% for the use of native code libraries, with an FPR of 10.8%. To test the robustness of our approach against the compiler version, optimization level, and other code transformations, we obfuscate and recompile known open-source libraries to evaluate which code transformations are resisted. Based on our results, we discuss how code re-use can be hidden by obfuscation and conclude with possible improvements.<br/> &copy; 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
key = {Static analysis},
keywords = {Android (operating system);Binary codes;Computer crime;Cosine transforms;Libraries;Malware;Open systems;Reverse engineering;},
note = {Code similarities;Code transformation;Geometric structure;Malware analysis;Open-source libraries;Optimization levels;Similarity measure;Similarity metrics;},
URL = {http://dx.doi.org/10.1145/3230833.3232828},
} 


@article{20132216384348,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Mission-type education programs with smart device facilitating LBS},
journal = {International Journal of Multimedia and Ubiquitous Engineering},
author = {Dong, Uk Im and Lee, Jong Oh},
volume = {8},
number = {2},
year = {2013},
pages = {81 - 88},
issn = {19750080},
abstract = {The assessment of Korean students' academic achievement was in higher rank among OECD member countries. However, in terms of spontaneity, interest and confidence, they are in lower rank due to the oppressive schooling system. To raise the preference for study and develop creativity, the new kind of mission-type education programs using smart devices and ICT are drawing attention. For example, Ins-Edu Institute obtained a patent on learning programs using iPads and students on field trips were each given a mission with a QR code and instructed to take pictures, write reports and edit photos using the smart device and send them via SMS. At the 5<sup>th</sup> AEMM Meeting, the Institute of APEC Collaborative Education handed out iPads with task information and questions for a tour around the city We analyzed the content and system of these apps, which perform a role to cultivate students' creativity and interest.},
key = {Students},
keywords = {Artificial intelligence;Education;Engineering;Industrial engineering;Multimedia systems;Museums;},
note = {AEMM;APEC;Field trips;Gyeongju;iPad;Korea;Mission-type;QR codes;Science museum;Smart devices;Smart learning;SNS;},
} 


@article{20141717625943,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Genetic optimization of a PCM enhanced storage tank for Solar Domestic Hot Water Systems},
journal = {Solar Energy},
author = {Padovan, Roberta and Manzan, Marco},
volume = {103},
year = {2014},
pages = {563 - 573},
issn = {0038092X},
abstract = {The scope of this work is to ascertain if the inclusion of Phase Change Materials (PCM) in the thermal energy storage of a Solar Domestic Hot Water System (SDHW) could be beneficial for increasing the energy savings of the system or in reducing the space occupied by the thermal energy storage. A simple SDHW plant is studied which features a plane solar collector, a boiler and a PCM enhanced tank. The PCM improved storage tank has been optimized using mono and multi-objective genetic algorithms. The optimization has been carried out with the modeFRONTIER optimization tool, while the system plant has been analyzed by means of a modified version of the building energy simulation code ESP-r. In parallel with the optimization a sensitivity analysis has been carried on in order to find out the relation between the design parameters of the tank (geometry, phase change temperature of the PCM, and insulation) and the performance of the system. Thanks to the multi-objective optimization of the system different solutions with different rankings of the optimized variables have been presented. The main result is that for this application the PCM has not the major impact on the results, while other parameters play a more significant role. &copy; 2014 Elsevier Ltd.<br/>},
key = {Phase change materials},
keywords = {Energy conservation;Genetic algorithms;Heat storage;Hot water distribution systems;Multiobjective optimization;Optimization;Pulse code modulation;Sensitivity analysis;Solar energy;Storage (materials);Tanks (containers);Thermal energy;Water;},
note = {Building energy simulations;Design parameters;Enhanced storage;Genetic optimization;Multi-objective genetic algorithm;Optimization tools;Phase change temperature;Solar domestic hot water systems;},
URL = {http://dx.doi.org/10.1016/j.solener.2013.12.034},
} 


@article{20180904847416,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Selecting a climate model subset to optimise key ensemble properties},
journal = {Earth System Dynamics},
author = {Herger, Nadja and Abramowitz, Gab and Knutti, Reto and Angelil, Oliver and Lehmann, Karsten and Sanderson, Benjamin M.},
volume = {9},
number = {1},
year = {2018},
pages = {135 - 151},
issn = {21904979},
abstract = {End users studying impacts and risks caused by human-induced climate change are often presented with large multi-model ensembles of climate projections whose composition and size are arbitrarily determined. An efficient and versatile method that finds a subset which maintains certain key properties from the full ensemble is needed, but very little work has been done in this area. Therefore, users typically make their own somewhat subjective subset choices and commonly use the equally weighted model mean as a best estimate. However, different climate model simulations cannot necessarily be regarded as independent estimates due to the presence of duplicated code and shared development history. &lt;br&gt;&lt;br&gt; Here, we present an efficient and flexible tool that makes better use of the ensemble as a whole by finding a subset with improved mean performance compared to the multi-model mean while at the same time maintaining the spread and addressing the problem of model interdependence. Out-of-sample skill and reliability are demonstrated using model-as-truth experiments. This approach is illustrated with one set of optimisation criteria but we also highlight the flexibility of cost functions, depending on the focus of different users. The technique is useful for a range of applications that, for example, minimise present-day bias to obtain an accurate ensemble mean, reduce dependence in ensemble spread, maximise future spread, ensure good performance of individual models in an ensemble, reduce the ensemble size while maintaining important ensemble characteristics, or optimise several of these at the same time. As in any calibration exercise, the final ensemble is sensitive to the metric, observational product, and pre-processing steps used.<br/> &copy; Author(s) 2018.},
key = {Climate models},
keywords = {Climate change;Cost functions;Set theory;},
note = {Climate model simulations;Climate projection;Development history;Individual models;Multi-model ensemble;Pre-processing step;Versatile methods;Weighted models;},
URL = {http://dx.doi.org/10.5194/esd-9-135-2018},
} 


@inproceedings{20173404067519,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {One-Way Wave Equation Migration at Scale on GPUs Using Directive Based Programming},
journal = {Proceedings - 2017 IEEE 31st International Parallel and Distributed Processing Symposium, IPDPS 2017},
author = {Mehta, Kshitij and Hugues, Maxime and Hernandez, Oscar and Bernholdt, David E. and Calandra, Henri},
year = {2017},
pages = {224 - 233},
address = {Orlando, FL, United states},
abstract = {One-Way Wave Equation Migration (OWEM) is a depth migration algorithm used for seismic imaging. A parallel version of this algorithm is widely implemented using MPI. Heterogenous architectures that use GPUs have become popular in the Top 500 because of their performance/power ratio. In this paper, we discuss the methodology and code transformations used to port OWEM to GPUs using OpenACC, along with the code changes needed for scaling the application up to 18,400 GPUs (more than 98%) of the Titan leadership class supercomputer at Oak Ridget National Laboratory. For the individual OpenACC kernels, we achieved an average of 3X speedup on a test dataset using one GPU as compared with an 8-core Intel Sandy Bridge CPU. The application was then run at large scale on the Titan supercomputer achieving a peak of 1.2 petaflops using an average of 5.5 megawatts. After porting the application to GPUs, we discuss how we dealt with other challenges of running at scale such as the application becoming more I/O bound and prone to silent errors. We believe this work will serve as valuable proof that directive-based programming models are a viable option for scaling HPC applications to heterogenous architectures.<br/> &copy; 2017 IEEE.},
key = {Program processors},
keywords = {Cosine transforms;Imaging techniques;Seismology;Statistical tests;Supercomputers;Wave equations;},
note = {Code transformation;Depth migration algorithms;National laboratory;One way wave equations;one-way;Openacc;Programming models;seismic;},
URL = {http://dx.doi.org/10.1109/IPDPS.2017.82},
} 


@inproceedings{20180604764423,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A multipurpose framework for model-based reuse-oriented software integration synthesis},
journal = {CEUR Workshop Proceedings},
author = {Perucci, Alexander and Autili, Marco and Tivoli, Massimo},
volume = {2019},
year = {2017},
pages = {38 - 44},
issn = {16130073},
address = {Austin, TX, United states},
abstract = {Systems are increasingly built by reusing and integrating existing software. This paper presents the preliminary version of a multipurpose framework for software integration synthesis. The objective is to provide both researchers and practitioners with an easily accessible environment that, integrating different kinds of software synthesizers, permit to perform different kinds of analyses, verification, model-to-model and model-to-code transformations, all oriented to the reuse and the integration of existing, possibly third-party, software.<br/>},
key = {Computer software reusability},
keywords = {Cosine transforms;Integration;Verification;},
note = {Code transformation;Model-based OPC;Software integration;Third parties;Verification-, modeling;},
} 


@inproceedings{20125215831126,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An industrial study on the risk of software changes},
journal = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012},
author = {Shihab, Emad and Hassan, Ahmed E. and Adams, Bram and Jiang, Zhen Ming},
year = {2012},
pages = {Assoc. Comput. Mach., Spec. Interest; Group Softw. Eng. (ACM SIGSOFT) - },
address = {Cary, NC, United states},
abstract = {Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, i.e., the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, i.e., changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and/or more testing. Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able to accurately identify risky changes with a recall of more than 67%, and a precision improvement of 87% (using developer specific models) and 37% (using team specific models), over a random model. We find that the number of lines and chunks of code added by the change, the bugginess of the files being changed, the number of bug reports linked to a change and the developer experience are the best indicators of change risk. In addition, we find that when a change has many related changes, the reliability of developers in marking risky changes is negatively affected. Our findings and models are being used today in practice to manage the risk of software projects. &copy; 2012 ACM.<br/>},
key = {Program debugging},
keywords = {Codes (symbols);Product design;Risk management;Risks;Software engineering;},
note = {bug inducing changes;change metrics;Code metrics;Design reviewing;Large enterprise;Precision improvement;Software change;Software project;},
URL = {http://dx.doi.org/10.1145/2393596.2393670},
} 


@inproceedings{20164502984710,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Boxlib with tiling: An adaptive mesh refinement software framework},
journal = {SIAM Journal on Scientific Computing},
author = {Zhang, Weiqun and Almgren, Ann and Day, Marcus and Nguyen, Tan and Shalf, John and Unat, Didem},
volume = {38},
number = {5},
year = {2016},
pages = {S156 - S172},
issn = {10648275},
abstract = {In this paper we introduce a block-structured adaptive mesh refinement software framework that incorporates tiling, a well-known loop transformation. Because the multiscale, multiphysics codes built in BoxLib are designed to solve complex systems at high resolution, performance on current and next generation architectures is essential. With the expectation of many more cores per node on next generation architectures, the ability to effectively utilize threads within a node is essential, and the current model for parallelization will not be sufficient. We describe a new version of BoxLib in which the tiling constructs are embedded so that BoxLib-based applications can easily realize expected performance gains without extra effort on the part of the application developer. We also discuss a path forward to enable future versions of BoxLib to take advantage of NUMA-aware optimizations using the TiDA portable library.<br/> &copy; 2016 Society for Industrial and Applied Mathematics.},
key = {Computer programming},
keywords = {Mesh generation;Numerical analysis;},
note = {Adaptive mesh refinement;Application developers;High performance computing;Loop transformation;Multi-physics code;Portable libraries;Software frameworks;Tiling;},
URL = {http://dx.doi.org/10.1137/15M102616X},
} 


@inproceedings{20173504089256,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Using LLVM for optimized lightweight binary re-writing at runtime},
journal = {Proceedings - 2017 IEEE 31st International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2017},
author = {Engelke, Alexis and Weidendorfer, Josef},
year = {2017},
pages = {785 - 794},
address = {Orlando, FL, United states},
abstract = {Providing new parallel programming models/abstractions as a set of library functions has the huge advantage that it allows for an relatively easy incremental porting path for legacy HPC applications, in contrast to the huge effort needed when novel concepts are only provided in new programming languages or language extensions. However, performance issues are to be expected with fine granular usage of library functions. In previous work, we argued that binary rewriting can bridge the gap by tightly coupling application and library functions at runtime. We showed that runtime specialization at the binary level, starting from a compiled, generic stencil code can help in approaching performance of manually written, statically compiled version. In this paper, we analyze the benefits of post-processing the re-written binary code using standard compiler optimizations as provided by LLVM. To this end, we present our approach for efficiently converting x86-64 binary code to LLVM-IR. Using the mentioned generic code for arbitrary 2d stencils, we present performance numbers with and without LLVM postprocessing. We find that we can now achieve the performance of variants specialized by hand.<br/> &copy; 2017 IEEE.},
key = {Program compilers},
keywords = {Binary codes;Parallel programming;},
note = {Binary transformation;Compiler optimizations;Dynamic code generation;Dynamic optimization;High performance computing;Language extensions;Parallel programming model;Runtime specialization;},
URL = {http://dx.doi.org/10.1109/IPDPSW.2017.103},
} 


@inproceedings{20140217192378,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {GCad: A near-miss clone genealogy extractor to support clone evolution analysis},
journal = {IEEE International Conference on Software Maintenance, ICSM},
author = {Saha, Ripon K. and Roy, Chanchal K. and Schneider, Kevin A.},
year = {2013},
pages = {488 - 491},
address = {Eindhoven, Netherlands},
abstract = {Understanding the evolution of code clones is important for both developers and researchers to understand the maintenance implications of clones and to design robust clone management systems. Generally, a study of clone evolution starts with extracting clone genealogies across multiple versions of a program and classifying them according to their change patterns. Although these tasks are straightforward for exact clones, extracting the history of near-miss clones and classifying their change patterns automatically is challenging due to the potential diverse variety of clone fragments even in the same clone class. In this tool demonstration paper we describe the design and implementation of a near-miss clone genealogy extractor, gCad, that can extract and classify both exact and near-miss clone genealogies. Developers and researchers can compute a wide range of popular metrics regarding clone evolution by simply post processing the gCad results. gCad scales well to large subject systems, works for different granularities of clones, and adapts easily to popular clone detection tools. &copy; 2013 IEEE.<br/>},
key = {Cloning},
keywords = {Computer software maintenance;History;},
note = {Change patterns;Clone detection;Clone management;Design and implementations;Different granularities;Evolution analysis;Post processing;Tool demonstration;},
URL = {http://dx.doi.org/10.1109/ICSM.2013.79},
} 


@inproceedings{20181905150715,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Interleaver with high dimensional encoding principle using hybrid group search optimizer},
journal = {Proceedings of the 2017 International Conference on Wireless Communications, Signal Processing and Networking, WiSPNET 2017},
author = {Deshmukh, Rutuja Abhishek and Panat, A.R.},
volume = {2018-January},
year = {2018},
pages = {2629 - 2635},
address = {Chennai, India},
abstract = {Handling high dimensional data and an effective error correcting code with low complexity are the two main challenges in the communication system. In the first paper, we focused on the second problem of error correcting code and introduced a turbo encoder with an interleaver which uses a hybrid meta-heuristic search algorithm by combining renowned Genetic Algorithm (GA) and Group Search Optimizer (GSO) in the name of hybrid GSO (HGSO) and that performs the high dimensional data transmission. Such a high dimensional data has transformed to low dimensional data by introducing a new high dimension interleaver in the conventional design in which data transformation and optimal pattern generation has taken place using the recent meta-heuristic search algorithm. The performance of the proposed system has compared with the existing GSO, GA, FA, ABC and random interleaver design in terms of signal to noise ratio (SNR), BER and FER and also the computation time is computed for each component of both existing and proposed system. Finally, the experimental results shows that the proposed high dimension interleaver design using HGSO approach performs well than any other methods.<br/> &copy; 2017 IEEE.},
key = {Signal to noise ratio},
keywords = {Clustering algorithms;Data handling;Genetic algorithms;Heuristic algorithms;Learning algorithms;Metadata;Modular robots;Signal encoding;Wireless telecommunication systems;},
note = {BER and FER;Error correcting code;Group search optimizer (GSO);High-dimensional encoding;hybrid GSO;Interleavers;Meta-heuristic search algorithms;Turbo encoder;},
URL = {http://dx.doi.org/10.1109/WiSPNET.2017.8300240},
} 


@inproceedings{20155201733096,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A Large Scale Study of License Usage on GitHub},
journal = {Proceedings - International Conference on Software Engineering},
author = {Vendome, Christopher},
volume = {2},
year = {2015},
pages = {772 - 774},
issn = {02705257},
address = {Florence, Italy},
abstract = {The open source community relies upon licensing in order to govern the distribution, modification, and reuse of existing code. These licenses evolve to better suit the requirements of the development communities and to cope with unaddressed or new legal issues. In this paper, we report the results of a large empirical study conducted over the change history of 16,221 open source Java projects mined from Git Hub. Our study investigates how licensing usage and adoption changes over a period of ten years. We consider both the distribution of license usage within projects of a rapidly growing forge and the extent that new versions of licenses are introduced in these projects.<br/> &copy; 2015 IEEE.},
key = {Open source software},
keywords = {Open systems;},
note = {Change history;Development community;Empirical studies;Large-scale studies;Mining software repositories;Open source communities;Open sources;Software license;},
URL = {http://dx.doi.org/10.1109/ICSE.2015.245},
} 


@article{20121714967460,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Numerical simulation of impact tests on reinforced concrete beams},
journal = {Materials and Design},
author = {Jiang, Hua and Wang, Xiaowo and He, Shuanhai},
volume = {39},
year = {2012},
pages = {111 - 120},
issn = {02613069},
abstract = {This paper focuses on numerical simulation of impact tests of reinforced concrete (RC) beams by the LS-DYNA finite element (FE) code. In the FE model, the elasto-plastic damage cap (EPDC) model, which is based on continuum damage mechanics in combination with plasticity theory, is used for concrete, and the reinforcement is assumed to be elasto-plastic. The numerical results compares well with the experimental values reported in the literature, in terms of impact force history, mid-span deflection history and crack patterns of RC beams. By comparing the numerical and experimental results, several important behavior of concrete material is investigated, which includes: damage variable to describe the strain softening section of stress-strain curve; the cap surface to describe the plastic volume change; the shape of the meridian and deviatoric plane to describe the yield surface as well as two methods of incorporating rebar into concrete mesh. This study gives a good example of using EPDC model and can be utilized for the development new constitutive models for concrete in future. &copy; 2012 Elsevier Ltd.},
key = {Computer simulation},
keywords = {Concrete beams and girders;Elastoplasticity;Numerical methods;Reinforced concrete;Stress-strain curves;},
note = {Concrete materials;Crack patterns;Damage variables;E. Impact and ballistic;Elasto-plastic;Elasto-plastic damage;Experimental values;F. Plastic behavior;FE model;Finite element codes;Impact force history;Impact test;LS-DYNA;Mid-span deflection;Numerical results;Plasticity theory;RC beams;Reinforced concrete beams;Strain softening;Volume change;Yield surface;},
URL = {http://dx.doi.org/10.1016/j.matdes.2012.02.018},
} 


@inproceedings{20145100338428,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automatic and portable mapping of data parallel programs to OpenCL for GPU-based heterogeneous systems},
journal = {ACM Transactions on Architecture and Code Optimization},
author = {Wang, Zheng and Grewe, Dominik and O'boyle, Michael F. P.},
volume = {11},
number = {4},
year = {2014},
issn = {15443566},
abstract = {Machine-learning mapping General-purpose GPU-based systems are highly attractive, as they give potentially massive performance at little cost. Realizing such potential is challenging due to the complexity of programming. This article presents a compiler-based approach to automatically generate optimized OpenCL code from data parallel OpenMP programs for GPUs. A key feature of our scheme is that it leverages existing transformations, especially data transformations, to improve performance on GPU architectures and uses automatic machine learning to build a predictive model to determine if it is worthwhile running the OpenCL code on the GPU or OpenMP code on themulticore host.We applied our approach to the entireNAS parallel benchmark suite and evaluated it on distinct GPU-based systems.We achieved average (up to) speedups of 4.51&times; and 4.20&times; (143&times; and 67&times;) on Core i7/NVIDIA GeForce GTX580 and Core i7/AMD Radeon 7970 platforms, respectively, over a sequential baseline. Our approach achieves, on average, greater than 10&times; speedups over two state-of-the-art automatic GPU code generators.<br/>},
key = {Program compilers},
keywords = {Application programming interfaces (API);Artificial intelligence;Benchmarking;Codes (symbols);Graphics processing unit;Learning systems;Mapping;},
note = {Data parallel programs;Data transformation;General purpose gpu;Heterogeneous systems;Improve performance;OpenCL;Parallel benchmarks;Predictive modeling;},
URL = {http://dx.doi.org/10.1145/2677036},
} 


@inproceedings{20171703599459,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Using a visual programming environment and custom robots to learn C programming and K-12 STEM concepts},
journal = {ACM International Conference Proceeding Series},
author = {Krishnamoorthy, Sai Prasanth and Kapila, Vikram},
year = {2016},
pages = {41 - 48},
address = {Stanford, CA, United states},
abstract = {This paper presents a robot-Visual Programming Environment (VPE) interface that can support K-12 students to learn science, technology, engineering, and math (STEM) concepts. Specifically, we employ Google's Blockly VPE to construct a blocks-based visual programming tool to facilitate easy programming of and interaction with physical robots. Through a careful and intentional integration of the Blockly VPE and physical robots, we illustrate that many K-12 level STEM concepts, which are traditionally treated through lectures and problem-solving, can be explored in a hands-on manner. The use of Blockly VPE obviates the need for prior experience with computer programming or familiarity with advanced programming concepts. Moreover, it permits students to learn various programming constructs, sequentially, starting from the fundamentals and gradually progressing to advanced concepts. The web-based Blockly VPE provides an interface that allows the user to browse through a block library and construct a block code for which a corresponding C program is automatically generated. The default web-based Blockly interface has been modified to permit the user to edit the resulting C program or to create an entirely new C program. Moreover, the Blockly VPE allows the user to wirelessly upload the C program to a Linux server running on a Raspberry Pi computer hosted on the robot. The Raspberry Pi compiles the received C program and serially transfers corresponding instructions to the robot's embedded hardware. The efficacy of the proposed robot-VPE interface is examined through students' experiences in conducting several illustrative robot-based STEM learning activities. The results of content quizzes and surveys show gains in students' understanding of STEM concepts after participation in robotics activities with the VPE interface.<br/> &copy; 2016 ACM.},
key = {C (programming language)},
keywords = {Computer operating systems;Engineering education;Interfaces (materials);Problem solving;Robot programming;Robotics;Robots;STEM (science, technology, engineering and mathematics);Students;Websites;},
note = {Automatically generated;Blockly;K-12 STEM education;Learning Activity;Programming concepts;Science, technology, engineering, and maths;Visual programming;Visual programming environments;},
URL = {http://dx.doi.org/10.1145/3003397.3003403},
} 


@inproceedings{20145200381202,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Interactive shadow removal and ground truth for variable scene categories},
journal = {BMVC 2014 - Proceedings of the British Machine Vision Conference 2014},
author = {Gong, Han and Cosker, Darren},
year = {2014},
pages = {Microsoft; NVIDIA; Qualcomm; Springer - },
address = {Nottingham, United kingdom},
abstract = {We present an interactive, robust and high quality method for fast shadow removal. To perform detection we use an on-the-fly learning approach guided by two rough user inputs for the pixels of the shadow and the lit area. From this we derive a fusion image that magnifies shadow boundary intensity change due to illumination variation. After detection, we perform shadow removal by registering the penumbra to a normalised frame which allows us to efficiently estimate non-uniform shadow illumination changes, resulting in accurate and robust removal. We also present the first reliable, validated and multi-scene category ground truth for shadow removal algorithms which overcomes limitations in existing data sets-such as inconsistencies between shadow and shadow-free images and limited variations of shadows. Using our data, we perform the most thorough comparison of state of the art shadow removal methods to date. Our algorithm outperforms the state of the art, and we supply our P-code and evaluation data and scripts to encourage future open comparisons.<br/> &copy; 2014. The copyright of this document resides with its authors.},
note = {Illumination changes;Illumination variation;Intensity change;Learning approach;Scene categories;Shadow boundaries;Shadow removal;State of the art;},
} 


@inproceedings{20162902623440,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An extendible design exploration tool for supporting approximate computing techniques},
journal = {Proceedings - 2016 11th IEEE International Conference on Design and Technology of Integrated Systems in Nanoscale Era, DTIS 2016},
author = {Barbareschi, Mario and Iannucci, Federico and Mazzeo, Antonino},
year = {2016},
pages = {Bahcesehir University (BU); Enis; et al.; Hacettepe University; Politecnico di Torino; TEI - },
address = {Istanbul, Turkey},
abstract = {Today, Approximate Computing represents one of the most important breakthrough in many scientific research areas. It exploits the inherent tolerance property of algorithms in order to perform approximate computations that have an acceptable accuracy and, at the same time, better performance than original versions. Previous work in the literature demonstrated the effectiveness of the trade-off between accuracy and performance, such as energy consumption, time and occupied area for integrated circuits, and many approximate computing methodologies were proposed. Unfortunately, introduced approaches fit in specific application domains and a general and systematic methodology to automatically define approximate algorithms is still an open challenge. Taking into account previous considerations, we make a key contribution in this direction by introducing a novel Approximate Computing methodology. Indeed, in this paper, we present an extended version of idea IDEA, a design exploration tool, which makes use of a source-to-source manipulation tool in order to apply code transformations that approximate the computation of a C/C++ algorithm. IDEA searches for configurations of a given algorithm that are characterized by an approximation error which is less than a user-defined threshold. Aiming at show the efficacy of IDEA, we configure it to find approximate variants of an algorithm by applying rounding techniques on numeric operations and we optimize two image processing algorithms, reducing the hardware overhead of their equivalent implementations.<br/> &copy; 2016 IEEE.},
key = {Approximation algorithms},
keywords = {C++ (programming language);Cosine transforms;Economic and social effects;Energy utilization;Green computing;Image processing;Integrated control;Nanotechnology;Natural resources exploration;},
note = {Approximate algorithms;Approximate computation;Approximate computing;Approximation errors;Code transformation;Image processing algorithm;Scientific researches;Systematic methodology;},
URL = {http://dx.doi.org/10.1109/DTIS.2016.7483888},
} 


@inproceedings{20154601537603,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Bioacoustic approaches to biodiversity monitoring and conservation in Kenya},
journal = {2015 IST-Africa Conference, IST-Africa 2015},
author = {Maina, Ciira Wa},
year = {2015},
address = {Lilongwe, Malawi},
abstract = {Kenya's rich biodiversity faces a number of threats including human encroachment, poaching and climate change. Since Kenya is a developing country, there is need to manage the sometimes competing interests of development, such as infrastructure development, and conservation. To achieve this, tools to effectively monitor the state of Kenya's various ecosystems are essential. In this paper we propose a biodiversity monitoring software tool that integrates acoustic indices of biodiversity, recognition of species of interest based on their vocalizations and acoustic census. This tool can be used by non-experts to determine the current state of their ecosystems by monitoring the state of bird species that serve as indicator taxa and whose abundance is related to the abundance of other terrestrial vertebrates including the 'big five'. The tool we propose exploits state-of-the art advances in signal processing and machine learning to perform biodiversity monitoring, bird species detection and census in a joint framework. Using publicly available data we demonstrate how current acoustic indices of biodiversity can be improved by incorporating machine learning based audio segmentation algorithms. We also show how open source toolkits can be used to build bird species recognition systems. Code to reproduce the experiments in this paper is available on Github at https://github.com/ciiram/BirdPy.<br/> &copy; 2015 IIMC International Information Management Corporation Ltd.},
key = {Conservation},
keywords = {Artificial intelligence;Audio acoustics;Bioacoustics;Biodiversity;Birds;Climate change;Developing countries;Ecosystems;Learning systems;Open source software;Open systems;Signal processing;Surveys;},
note = {Audio segmentation;Biodiversity monitoring;Bird species;Indicator taxon;Infrastructure development;Non-experts;Open sources;State of the art;},
URL = {http://dx.doi.org/10.1109/ISTAFRICA.2015.7190558},
} 


@article{20130916070133,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Adjusting effort estimation using micro-productivity profiles},
title = {Tookoormuse hinnangu tapsustamine mikroproduktiivsuse profiili abil},
journal = {Proceedings of the Estonian Academy of Sciences},
author = {Toth, Gabriella and Vegh, Adam Zoltan and Beszedes, Arpad and Schrettner, Lajos and Gergely, Tamas and Gyimothy, Tibor},
volume = {62},
number = {1},
year = {2013},
pages = {71 - 80},
issn = {17366046},
abstract = {We investigate a phenomenon we call micro-productivity decrease, which is expected to be found in most development or maintenance projects and has a specific profile that depends on the project, the development model, and the team. Micro-productivity decrease refers to the observation that the cumulative effort to implement a series of changes is larger than the effort that would be needed if we made the same modification in only one step. The reason for the difference is that the same sections of code are usually modified more than once in the series of (sometimes imperfect) atomic changes. Hence, we suggest that effort estimation methods based on atomic change estimations should incorporate these profiles when being applied to larger modification tasks. We verify the concept on industrial development projects with our metrics-based machine learning models extended with statistical data. We show that the calculated Micro-Productivity Profile for these projects could be used for effort estimation of larger tasks with more accuracy than a naive atomic change-oriented estimation.<br/>},
key = {Productivity},
keywords = {Atoms;Estimation;},
note = {Atomic changes;Change estimation;Development model;Effort Estimation;Effort estimation methods;Industrial development;Machine learning models;Maintenance projects;Prediction model;Specific profile;Statistical datas;},
URL = {http://dx.doi.org/10.3176/proc.2013.1.08},
} 


@article{20134716990597,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A BPMN-based design and maintenance framework for ETL processes},
journal = {International Journal of Data Warehousing and Mining},
author = {El Akkaoui, Zineb and Zimanyi, Esteban and Mazon, Jose-Norberto and Trujillo, Juan},
volume = {9},
number = {3},
year = {2013},
pages = {46 - 72},
issn = {15483924},
abstract = {Business Intelligence (BI) applications require the design, implementation, and maintenance of processes that extract, transform, and load suitable data for analysis. The development of these processes (known as ETL) is an inherently complex problem that is typically costly and time consuming. In a previous work, the authors have proposed a vendor-independent language for reducing the design complexity due to disparate ETL languages tailored to specific design tools with steep learning curves. Nevertheless, the designer still faces two major issues during the development of ETL processes: (i) how to implement the designed processes in an executable language, and (ii) how to maintain the implementation when the organization data infrastructure evolves. In this paper, the authors propose a model-driven framework that provides automatic code generation capability and ameliorate maintenance support of our ETL language. They present a set of model-to-text transformations able to produce code for different ETL commercial tools as well as model-to-model transformations that automatically update the ETL models with the aim of supporting the maintenance of the generated code according to data source evolution. A demonstration using an example is conducted as an initial validation to show that the framework covering modeling, code generation and maintenance could be used in practice. Copyright &copy; 2013, IGI Global.<br/>},
key = {Automatic programming},
keywords = {Codes (symbols);Data warehouses;Maintenance;Systems analysis;},
note = {Automatic code generations;Code Generation;Conceptual model;ETL process;Maintenance supports;Model driven development;Model to model transformation;Model to text transformations;},
URL = {http://dx.doi.org/10.4018/jdwm.2013070103},
} 


@inproceedings{20152600980601,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {HERCULES: Strong patterns towards more intelligent predictive modeling},
journal = {Proceedings of the International Conference on Parallel Processing},
author = {Park, Eunjung and Kartsaklis, Christos and Cavazos, John},
volume = {2014-November},
number = {November},
year = {2014},
pages = {172 - 181},
issn = {01903918},
address = {Minneapolis, MN, United states},
abstract = {Recent work has shown that program analysis techniques to select meaningful code features of programs are important in the task of deciding the best compiler optimizations. Although, there are many successful state-of-the-art program analysis techniques, they often do not provide a simple method to extract the most expressive information about loops, especially when a target program is computationally intensive with complex loops and data dependencies. In this paper, we introduce a static technique to characterize a program using a pattern-driven system named HERCULES. This characterization technique not only helps a user to understand programs by searching pattern-of-interests, but also can be used for a predictive model that effectively selects the proper compiler optimizations. We formulated 35 loop patterns, then evaluated our characterization technique by comparing the predictive models constructed using HERCULES to three other state-of-the-art characterization methods. We show that our models outperform three state-of-the-art program characterization techniques on two multicore systems in selecting the best optimization combination from a given loop transformation space. We achieved up to 67% of the best possible speedup achievable with the optimization search space we evaluated.<br/> &copy; 2014 IEEE.},
key = {Program compilers},
keywords = {Characterization;Learning systems;Multicore programming;},
note = {Characterization methods;Characterization techniques;Compiler optimizations;Loop transformation;Optimization combination;Pattern-based;Predictive modeling;Searching patterns;},
URL = {http://dx.doi.org/10.1109/ICPP.2014.26},
} 


@article{20164302927266,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Moriarty: Improving 'time to market' in big data and artificial intelligence applications},
journal = {International Journal of Design and Nature and Ecodynamics},
author = {Pena, P. and Del Hoyo, R. and Vea-Murguia, J. and Rodrigalvarez, V. and Calvo, J.I. and Martin, J.M.},
volume = {11},
number = {3},
year = {2016},
pages = {230 - 238},
issn = {17557437},
abstract = {The objective of this paper is to present the Moriarty framework and show one use case of the recommendation of entertainment events. Moriarty is a tool that can generate Big Data near real-Time analytics solutions (Streaming Analytics). This new tool makes possible the collaboration among the data scientist and the software engineer. Through Moriarty, they join forces for the rapid generation of new software solutions. The data scientist works with algorithms and data transformations using a visual interface, while the software engineer works with the idea of services to be invoked. The underlying idea is that a user can build projects of Artificial Intelligence and Data Analytics without having to make any line of code. The main power of the tool is to reduce the 'time to market' in an application which embeds complex algorithms of Artificial Intelligence. It is based on different Artificial Intelligence algorithms (like Deep Learning, Natural Language Processing and Semantic Web) and Big Datamodules (Spark as a distributed data engine and access to NoSQL databases). Moriarty is divided into several layers; its core is a BPMN engine, which executes the processing and defines data analytics process, called workflows. Each workflow is defined by the standard BPMN model and is linked to a set of reusable functions or Artificial Intelligence algorithms written following a service-oriented architecture. An example of service presented is a recommendation application of restaurants, concerts, entertainment and events in general, where information is collected from social networks and websites, is processed by Natural Language Processingalgorithms and finally introduced into a graph database.<br/> &copy; 2016 WIT Press.},
key = {Big data},
keywords = {Artificial intelligence;Commerce;Computer software reusability;Concurrent engineering;Data handling;Deep learning;Engines;Information services;Natural language processing systems;Service oriented architecture (SOA);},
note = {Artificial intelligence algorithms;Complex algorithms;Data transformation;Moriarty;Rapid generations;Software solution;User profiling;Work-flows;},
URL = {http://dx.doi.org/10.2495/DNE-V11-N3-230-238},
} 


@inproceedings{20163602776716,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An open-source object-graph-mapping framework for Neo4j and Scala: Renesca},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Dietze, Felix and Karoff, Johannes and Valdez, Andre Calero and Ziefle, Martina and Greven, Christoph and Schroeder, Ulrik},
volume = {9817 LNCS},
year = {2016},
pages = {204 - 218},
issn = {03029743},
address = {Salzburg, Austria},
abstract = {The usage and application of graph databases is increasing. Many research problems are based on understanding relationships between data entities. This is where graph databases are powerful. Nevertheless, software developers model and think in object-oriented software. Combining both approaches leads to a paradigm mismatch. This mismatch can be addressed by using object graph mappers (OGM). OGM adapt graph databases for object-oriented code, to relieve the developer. Most graph database access frameworks only support table based result outputs. This defeats one of the strongest purposes of using graph databases. In order to harness both the power of graph databases and object-oriented modeling (e.g. type-safety, inheritance, etc.) we propose an open-source framework with two libraries: (1) renesca, which is a graph database driver providing graph-query-results and change tracking. (2) renesca-magic, a macro-based ER-modeling domain specific language (DSL). Both were tested in a graph-based application and lead to dramatic improvements in code size (factor 10) and extensibility of the code, with no significant effect on performance.<br/> &copy; IFIP International Federation for Information Processing 2016.},
key = {Object oriented programming},
keywords = {Artificial intelligence;Codes (symbols);Data privacy;Graph Databases;Graphic methods;Learning systems;Modeling languages;Object-oriented databases;Open source software;Problem oriented languages;Query processing;},
note = {Neo4j;Object graphs;Object oriented model;Object oriented software;Object-oriented code;Open source frameworks;Rest api;Scala;},
URL = {http://dx.doi.org/10.1007/978-3-319-45507-5_14},
} 


@inproceedings{20185006239713,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Text steganography in font color of MS excel sheet},
journal = {ACM International Conference Proceeding Series},
author = {Alsaadi, Husam Ibrahiem and Al-Anni, Maad Kamal and Almuttairi, Rafah M. and Bayat, Oguz and Ucan, Osman Nuri},
year = {2018},
pages = {International Association of Researchers (IARES); UDIMA - },
address = {Madrid, Spain},
abstract = {One of the ways to maintain the security of data transfer over the network is to encrypt the data before sending it, for the purpose of increasing data privacy and its importance. Steganography, is used to hide encrypted data in various documents, the art of concealment is embedded Secret Messages (SM) to be passed confidentially within a file without influence on the properties of the file and this can't be anyone's hand is to know that there is confidential data encapsulated by transport data. Previous research used the Excel file to hide data in different ways, but the data masking and packaging by changing the color content writing is what distinguishes our research and make it alone in this area. The Microsoft Excel file size is being used to hide data remains constant in spite of the file containing the confidential data. The secret for the survival of file size unchanged despite the addition of encrypted data, is that confidential data is embedding in the value of the color pallet of the font format used in the cells, leading to the font color change slightly. The main advantage for scathing the Code Message (CM) upon the Cover Excel File (CEF) is making the brute force more confusing and has no possible to visualize the hiding message in normal circumstance, it is somehow rather than embedding it into either MS Word or PPT while the latter done sequentially, in this approach: each Full cell can hide a character with coding process (from 4 bits to 8 bits) In this study, we have had advantages through applying a new approach of Text to Excel hiding techniques, likewise both the conductivity ratio, and the degree of disappearances are indicated perfectly.<br/> &copy; 2018 Association for Computing Machinery. ACM},
key = {Color},
keywords = {Codes (symbols);Cryptography;Data privacy;Data transfer;E-learning;Information systems;Information use;Network security;Steganography;},
note = {Code streams;Conductivity ratio;Confidential data;Encrypted data;Excel;Microsoft excel;Secret messages;Text steganography;},
URL = {http://dx.doi.org/10.1145/3279996.3280006},
} 


@inproceedings{20164502985308,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Patch verification via multiversion interprocedural control flow graphs},
journal = {Proceedings - International Conference on Software Engineering},
author = {Le, Wei and Pattison, Shannon D.},
volume = {0},
number = {1},
year = {2014},
pages = {1047 - 1058},
issn = {02705257},
address = {Hyderabad, India},
abstract = {Software development is inherently incremental; however, it is challenging to correctly introduce changes on top of existing code. Recent studies show that 15%-24% of the bug fixes are incorrect, and the most important yet hard-to-acquire information for programming changes is whether this change breaks any code elsewhere. This paper presents a framework, called Hydrogen, for patch verification. Hydrogen aims to automatically determine whether a patch correctly fixes a bug, a new bug is introduced in the change, a bug can impact multiple software releases, and the patch is applicable for all the impacted releases. Hydrogen consists of a novel program representation, namely multiversion interprocedural control flow graph (MVICFG), that integrates and compares control flow of multiple versions of programs, and a demand-driven, path-sensitive symbolic analysis that traverses the MVICFG for detecting bugs related to software changes and versions. In this paper, we present the definition, construction and applications of MVICFGs. Our experimental results show that Hydrogen correctly builds desired MVICFGs and is scalable to real-life programs such as libpng, tightvnc and putty. We experimentally demonstrate that MVICFGs can enable efficient patch verification. Using the results generated by Hydrogen, we have found a few documentation errors related to patches for a set of open-source programs.<br/> &copy; 2014 ACM.},
key = {Program debugging},
keywords = {Data flow analysis;Embedded systems;Flow graphs;Graphic methods;Hydrogen;Open source software;Software design;},
note = {Control flow graphs;Inter-procedural;Multi-version;Open source projects;Program representations;Software change;Symbolic analysis;Verfication;},
URL = {http://dx.doi.org/10.1145/2568225.2568304},
} 


@inproceedings{20182805544289,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Online interactive educational system for submission and evaluation of programming assignments},
journal = {Proceedings of 2017 International Conference on Intelligent Computing and Control, I2C2 2017},
author = {Varat, Abhishek and Vetal, Mayur and Bawadkar, Pooja and Shinde, Shubham and Naik, Varsha},
volume = {2018-January},
year = {2018},
pages = {1 - 4},
address = {Coimbatore, India},
abstract = {Nowadays in educational institutions teachers provide problem statement to students and students submit their solution in accordance with problem statement provided. Grading of these solutions done manually. It is a time-consuming task. To find out the correctness of the solution they required to run multiple test cases. Also, teachers have to ensure that each computer in the lab has editor and compiler to run the code. This paper will provide guidelines for system interface to teachers in which teachers are able to create new programming assignments and assign it to a particular class. Also, this system helps teachers for grading of programming assignments. It will provide a detailed report of class for each programming assignments. It will also reduce the overhead of installing the software required to edit and run the code. This the system provide an easy interface to students to edit and to compile programs. It will also provide the detailed analysis of evaluated assignments.<br/> &copy; 2017 IEEE.},
key = {Teaching},
keywords = {Cloud computing;E-learning;Grading;Intelligent computing;Program compilers;Students;},
note = {Educational institutions;Educational systems;Online Compiler;Plagiarism detection;Problem statement;Programming assignments;System interfaces;Time-consuming tasks;},
URL = {http://dx.doi.org/10.1109/I2C2.2017.8321792},
} 


@inproceedings{20125215831118,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Seeking the ground truth: A retroactive study on the evolution and migration of software libraries},
journal = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012},
author = {Cossette, Bradley E. and Walker, Robert J.},
year = {2012},
pages = {Assoc. Comput. Mach., Spec. Interest; Group Softw. Eng. (ACM SIGSOFT) - },
address = {Cary, NC, United states},
abstract = {Application programming interfaces (APIs) are a common and industrially-relevant means for third-party software developers to reuse external functionality. Several techniques have been proposed to help migrate client code between library versions with incompatible APIs, but it is not clear how well these perform in an absolute sense. We present a retroactive study into the presence and nature of API incompatibilities between several versions of a set of Java-based software libraries; for each, we perform a detailed, manual analysis to determine what the correct adaptations are to migrate from the older to the newer version. In addition, we investigate whether any of a set of adaptation recommender techniques is capable of identifying the correct adaptations for library migration. We find that a given API incompatibility can typically be addressed by only one or two recommender techniques, but sometimes none serve. Furthermore, those techniques give correct recommendations, on average, in only about 20% of cases. &copy; 2012 ACM.<br/>},
key = {Application programming interfaces (API)},
keywords = {Application programs;Computer software reusability;Digital libraries;Recommender systems;},
note = {adaptive change;Client code;Ground truth;Manual analysis;Software libraries;Third party software;},
URL = {http://dx.doi.org/10.1145/2393596.2393661},
} 


@article{20181705048240,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Preparation and enhanced thermal performance of novel (solid to gel) form-stable eutectic PCM modified by nano-graphene platelets},
journal = {Journal of Energy Storage},
author = {Saeed, Rami M. and Schlegel, J.P. and Castano, C. and Sawafta, R.},
volume = {15},
year = {2018},
pages = {91 - 102},
issn = {2352152X},
abstract = {This study presents the development of form-stable eutectic mixtures, modified with nanoscale structures for enhanced thermal performance. These additives may result in the next generation of phase change materials (PCMs) for thermal energy storage systems. An appropriate gelling or thickening agent (2-hydroxypropyl ether cellulose) is introduced so that the PCM will lose its fluidity, become form-stable, and the liquid leakage problem will be overcome. Nano-graphene platelets (NGPs) are added in order to enhance the thermal properties and overall heat transfer. Differential scanning calorimetry (DSC) was carried out for the thermal analysis of the PCMs. The paper experimentally studied in detail the enhanced thermo-physical properties required for stimulating and modelling the PCM in energy storage applications such as specific heat, thermal diffusivity, thermal conductivity, enthalpy, and density. The principle of the T-history method was applied using a parallel plate heating/cooling guarded plate apparatus to determine the true phase transition temperatures of bulk PCM. The supercooling of the enhanced shape stable mixture was found to be less than 0.1 &deg;C. The thermal reliability test indicated that the enhanced form-stable eutectic mixture had reliable thermal performance over a postulated lifetime of 80 years. As a result, the developed form stable PCM eutectic mixture is a promising material for thermal energy storage.<br/> &copy; 2017},
key = {Thermal conductivity},
keywords = {Differential scanning calorimetry;Eutectics;Graphene;Heat storage;Mixtures;Phase change materials;Platelets;Plates (structural components);Pulse code modulation;Specific heat;Storage (materials);Supercooling;Thermal energy;Thermoanalysis;},
note = {Energy storage applications;Graphene platelets;Nanoscale structure;Thermal energy storage systems;Thermal Performance;Thermal reliability;Thermo-physical property;Thickening agents;},
URL = {http://dx.doi.org/10.1016/j.est.2017.11.003},
} 


@inproceedings{20172303742340,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A new semantics-based android malware detection},
journal = {2016 2nd IEEE International Conference on Computer and Communications, ICCC 2016 - Proceedings},
author = {Zhang, Xiaohan and Jin, Zhengping},
year = {2016},
pages = {1412 - 1416},
address = {Chengdu, China},
abstract = {With its high market share, the Android platform has become a growing target for mobile malware, which posed great threat to customers' safety. Meanwhile, malwares employed various techniques, take code obfuscation for example, to evade detection. The commercial mobile anti-malware products, however, are vulnerable to common code transformation techniques. This paper proposes an enhanced malware detection approach which combines advantage of static analysis and performance of ensemble learning to improve Android malware detection accuracy. The model extracts semantics-based features which can resist common obfuscation techniques, and also uses feature collection from code and app characteristics through static analysis. Real-world malware samples are used to evaluate the model and the results of experiments have proved that this approach improved the efficiency with AUC of 2.06% higher than previous approach.<br/> &copy; 2016 IEEE.},
key = {Android (operating system)},
keywords = {Codes (symbols);Competition;Computer crime;Cosine transforms;Malware;Semantics;Static analysis;},
note = {Android;Android malware;Android platforms;Code obfuscation;Code transformation;Ensemble learning;Malware detection;Mobile malware;},
URL = {http://dx.doi.org/10.1109/CompComm.2016.7924936},
} 


@article{20114914576870,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Linear and kernel methods for multivariate change detection},
journal = {Computers and Geosciences},
author = {Canty, Morton J. and Nielsen, Allan A.},
volume = {38},
number = {1},
year = {2012},
pages = {107 - 114},
issn = {00983004},
abstract = {The iteratively reweighted multivariate alteration detection (IR-MAD) algorithm may be used both for unsupervised change detection in multi- and hyperspectral remote sensing imagery and for automatic radiometric normalization of multitemporal image sequences. Principal components analysis (PCA), as well as maximum autocorrelation factor (MAF) and minimum noise fraction (MNF) analyses of IR-MAD images, both linear and kernel-based (nonlinear), may further enhance change signals relative to no-change background. IDL (Interactive Data Language) implementations of IR-MAD, automatic radiometric normalization, and kernel PCA/MAF/MNF transformations are presented that function as transparent and fully integrated extensions of the ENVI remote sensing image analysis environment. The train/test approach to kernel PCA is evaluated against a Hebbian learning procedure. Matlab code is also available that allows fast data exploration and experimentation with smaller datasets. New, multiresolution versions of IR-MAD that accelerate convergence and that further reduce no-change background noise are introduced. Computationally expensive matrix diagonalization and kernel image projections are programmed to run on massively parallel CUDA-enabled graphics processors, when available, giving an order of magnitude enhancement in computational speed. The software is available from the authors' Web sites. &copy; 2011 Elsevier Ltd.<br/>},
key = {Remote sensing},
keywords = {Image analysis;Image enhancement;Iterative methods;MATLAB;Principal component analysis;Radiometry;},
note = {CUDA;ENVI;IMAD;Kernel methods;Multi resolutions;Radiometric normalization;},
URL = {http://dx.doi.org/10.1016/j.cageo.2011.05.012},
} 


@inproceedings{20124915767458,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automating object transformations for dynamic software updating},
journal = {ACM SIGPLAN Notices},
author = {Magill, Stephen and Hicks, Michael and Subramanian, Suriya and McKinley, Kathryn S.},
volume = {47},
number = {10},
year = {2012},
pages = {265 - 280},
issn = {15232867},
abstract = {Dynamic software updating (DSU) systems eliminate costly downtime by dynamically fixing bugs and adding features to executing programs. Given a static code patch, most DSU systems construct runtime code changes automatically. However, a dynamic update must also specify how to change the running program's execution state, e.g., the stack and heap, to make it compatible with the new code. Constructing such state transformations correctly and automatically remains an open problem. This paper presents a solution called Targeted Object Synthesis (TOS). TOS first executes the same tests on the old and new program versions separately, observing the program heap state at a few corresponding points. Given two corresponding heap states, TOS matches objects in the two versions using key fields that uniquely identify objects and correlate old and new-version objects. Given example object pairs, TOS then synthesizes the simplest-possible function that transforms an old-version object to its new-version counterpart. We show that TOS is effective on updates to four open-source server programs for which it generates non-trivial transformation functions that use conditionals, operate on collections, and fix memory leaks. These transformations help programmers understand their changes and apply dynamic software updates. Copyright &copy; 2012 ACM.<br/>},
key = {Program debugging},
keywords = {Codes (symbols);Open source software;Software testing;},
note = {Dynamic software update;Hot-swapping;Object matching;Program synthesis;State transformation;},
URL = {http://dx.doi.org/10.1145/2398857.2384636},
} 


@inproceedings{20124815720738,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automating object transformations for dynamic software updating},
journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
author = {Magill, Stephen and Hicks, Michael and Subramanian, Suriya and McKinley, Kathryn S.},
year = {2012},
pages = {265 - 280},
address = {Tucson, AZ, United states},
abstract = {Dynamic software updating (DSU) systems eliminate costly downtime by dynamically fixing bugs and adding features to executing programs. Given a static code patch, most DSU systems construct runtime code changes automatically. However, a dynamic update must also specify how to change the running program's execution state, e.g., the stack and heap, to make it compatible with the new code. Constructing such state transformations correctly and automatically remains an open problem. This paper presents a solution called Targeted Object Synthesis (TOS). TOS first executes the same tests on the old and new program versions separately, observing the program heap state at a few corresponding points. Given two corresponding heap states, TOS matches objects in the two versions using key fields that uniquely identify objects and correlate old and new-version objects. Given example object pairs, TOS then synthesizes the simplest-possible function that transforms an old-version object to its new-version counterpart. We show that TOS is effective on updates to four open-source server programs for which it generates non-trivial transformation functions that use conditionals, operate on collections, and fix memory leaks. These transformations help programmers understand their changes and apply dynamic software updates.<br/>},
key = {Object oriented programming},
keywords = {Codes (symbols);Computer systems programming;Open source software;Program debugging;Software testing;},
note = {Dynamic software update;Hot-swapping;Object matching;Program synthesis;State transformation;},
URL = {http://dx.doi.org/10.1145/2384616.2384636},
} 


@inproceedings{20162202436888,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Causes of architecture changes: An empirical study through the communication in OSS mailing lists},
journal = {Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
author = {Ding, Wei and Liang, Peng and Tang, Antony and Van Vliet, Hans},
volume = {2015-January},
year = {2015},
pages = {403 - 408},
issn = {23259000},
address = {Pittsburgh, PA, United states},
abstract = {Understanding the causes of architecture changes allows us to devise means to prevent architecture knowledge vaporization and architecture degeneration. But the causes are not always known, especially in open source software (OSS) development. This makes it very hard to understand the underlying reasons for the architecture changes and design appropriate modifications. Architecture information is communicated in development mailing lists of OSS projects. To explore the possibility of identifying and understanding the causes of architecture changes, we conducted an empirical study to analyze architecture information (i.e., architectural threads) communicated in the development mailing lists of two popular OSS projects: Hibernate and ArgoUML, verified architecture changes with source code, and identified the causes of architecture changes from the communicated architecture information. The main findings of this study are: (1) architecture information communicated in OSS mailing lists does lead to architecture changes in code; (2) the major cause for architecture changes in both Hibernate and ArgoUML is preventative changes. (3) more than 45% of architecture changes in both projects happened before the first stable version was released, which indicates that the architectures of the investigated OSS projects are relatively stable after the first stable release.<br/>},
key = {Open source software},
keywords = {Communication;Knowledge engineering;Open systems;},
note = {Cause of change;Empirical studies;Mailing lists;Source codes;},
URL = {http://dx.doi.org/10.18293/SEKE2015-193},
} 


@article{20161302152239,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Understanding the causes of architecture changes using OSS mailing lists},
journal = {International Journal of Software Engineering and Knowledge Engineering},
author = {Ding, Wei and Liang, Peng and Tang, Antony and Van Vliet, Hans},
volume = {25},
number = {9-10},
year = {2015},
pages = {1633 - 1651},
issn = {02181940},
abstract = {The causes of architecture changes can tell about why architecture changes, and this knowledge can be captured to prevent architecture knowledge vaporization and architecture degeneration. But the causes are not always known, especially in open source software (OSS) development. This makes it very hard to understand the underlying reasons for the architecture changes and design appropriate modifications. Architecture information is communicated in development mailing lists of OSS projects. To explore the possibility of identifying and understanding the causes of architecture changes, we conducted an empirical study to analyze architecture information (i.e. architectural threads) communicated in the development mailing lists of two popular OSS projects: Hibernate and ArgoUML, verified architecture changes with source code, and identified the causes of architecture changes from the communicated architecture information. The main findings of this study are: (1) architecture information communicated in OSS mailing lists does lead to architecture changes in code; (2) the major cause for architecture changes in both Hibernate and ArgoUML is preventative changes, and the causes of architecture changes are further classified to functional requirement, external quality requirement, and internal quality requirement using the coding techniques of grounded theory; (3) more than 45% of architecture changes in both projects happened before the first stable version was released.<br/> &copy; 2015 World Scientific Publishing Company.},
key = {Open source software},
keywords = {Codes (symbols);Communication;Open systems;},
note = {Cause of change;Coding techniques;Empirical studies;External quality;Functional requirement;Grounded theory;Internal quality;Mailing lists;},
URL = {http://dx.doi.org/10.1142/S0218194015400367},
} 


@article{20154201412085,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Large-scale analysis of neuroimaging data on commercial clouds with content-aware resource allocation strategies},
journal = {International Journal of High Performance Computing Applications},
author = {Minervini, Massimo and Rusu, Cristian and Damiano, Mario and Tucci, Valter and Bifone, Angelo and Gozzi, Alessandro and Tsaftaris, Sotirios A.},
volume = {29},
number = {4},
year = {2015},
pages = {473 - 488},
issn = {10943420},
abstract = {The combined use of mice that have genetic mutations (transgenic mouse models) of human pathology and advanced neuroimaging methods (such as magnetic resonance imaging) has the potential to radically change how we approach disease understanding, diagnosis and treatment. Morphological changes occurring in the brain of transgenic animals as a result of the interaction between environment and genotype can be assessed using advanced image analysis methods, an effort described as 'mouse brain phenotyping'. However, the computational methods involved in the analysis of high-resolution brain images are demanding. While running such analysis on local clusters is possible, not all users have access to such infrastructure and even for those that do, having additional computational capacity can be beneficial (e.g. to meet sudden high throughput demands). In this paper we use a commercial cloud platform for brain neuroimaging and analysis. We achieve a registration-based multi-atlas, multi-template anatomical segmentation, normally a lengthy-in-time effort, within a few hours. Naturally, performing such analyses on the cloud entails a monetary cost, and it is worthwhile identifying strategies that can allocate resources intelligently. In our context a critical aspect is the identification of how long each job will take. We propose a method that estimates the complexity of an image-processing task, a registration, using statistical moments and shape descriptors of the image content. We use this information to learn and predict the completion time of a registration. The proposed approach is easy to deploy, and could serve as an alternative for laboratories that may require instant access to large high-performance-computing infrastructures. To facilitate adoption from the community we publicly release the source code.<br/> &copy; SAGE Publications.},
key = {Image analysis},
keywords = {Brain mapping;Clouds;Computer vision;Diagnosis;Economics;Image registration;Learning systems;Magnetic resonance imaging;Mammals;Neuroimaging;Resource allocation;},
note = {Computational capacity;Content-aware resource allocations;Disease understanding;High performance computing;Image analysis method;Large-scale analysis;Morphological changes;Phenotyping;},
URL = {http://dx.doi.org/10.1177/1094342013519483},
} 


@inproceedings{20170703341361,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CLCMiner: Detecting Cross-Language Clones without Intermediates},
journal = {IEICE Transactions on Information and Systems},
author = {Cheng, Xiao and Peng, Zhiming and Jiang, Lingxiao and Zhong, Hao and Yu, Haibo and Zhao, Jianjun},
volume = {E100D},
number = {2},
year = {2017},
pages = {273 - 284},
issn = {09168532},
abstract = {The proliferation of diverse kinds of programming languages and platforms makes it a common need to have the same functionality implemented in different languages for different platforms, such as Java for Android applications and C# forWindows phone applications. Although versions of code written in different languages appear syntactically quite different from each other, they are intended to implement the same software and typically contain many code snippets that implement similar functionalities, which we call cross-language clones. When the version of code in one language evolves according to changing functionality requirements and/or bug fixes, its cross-language clones may also need be changed to maintain consistent implementations for the same functionality. Thus, it is needed to have automated ways to locate and track cross-language clones within the evolving software. In the literature, approaches for detecting cross-language clones are only for languages that share a common intermediate language (such as the .NET language family) because they are built on techniques for detecting single-language clones. To extend the capability of cross-language clone detection to more diverse kinds of languages, we propose a novel automated approach, CLCMiner, without the need of an intermediate language. It mines such clones from revision histories, based on our assumption that revisions to different versions of code implemented in different languages may naturally reflect how programmers change cross-language clones in practice, and that similarities among the revisions (referred to as clones in diffs or diff clones) may indicate actual similar code. We have implemented a prototype and applied it to ten open source projects implementations in both Java and C#. The reported clones that occur in revision histories are of high precisions (89% on average) and recalls (95% on average). Compared with token-based code clone detection tools that can treat code as plain texts, our tool can detect significantly more cross-language clones. All the evaluation results demonstrate the feasibility of revision-history based techniques for detecting cross-language clones without intermediates and point to promising future work.<br/> &copy; 2017 The Institute of Electronics, Information and Communication Engineers.},
key = {Java programming language},
keywords = {Cloning;Codes (symbols);Linguistics;Open source software;},
note = {Code clone;Cross languages;Diff;Revision;Similarity;},
URL = {http://dx.doi.org/10.1587/transinf.2016EDP7334},
} 


@inproceedings{20180104614983,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Exploring the educational potential of QR codes},
journal = {Proceedings - 2016 3rd International Conference on Advances in Computing, Communication and Engineering, ICACCE 2016},
author = {Sungkur, Roopesh Kevin and Neermul, Vasheel and Tauckoor, Varsha},
year = {2016},
pages = {368 - 373},
address = {Durban, South africa},
abstract = {Education through the past years has changed the way teaching and learning is inculcated to the new generation. The rapid growth of technology has accelerated the change in teaching methods and has given birth to new methods like distance learning and online learning, where lectures notes and material are shared via educational platforms. This leaves traditional face to face lecture session to a discussion type session where tutor and student discuss about topics and do more class activity. Technology offers new ways in transferring data and information in large amounts very rapidly and also offers the accessibility not only to large information pools such as the internet but also offers access to online storage remotely accessible. One way education can benefit from this is to make use of Quick Response (QR) Code. With its potential, QR Codes can be exploited to change the ways both tutors and students interact in class. This can be achieved by combining Mobile technology, Quick Response Code technology and Cloud technology.<br/> &copy; 2016 IEEE.},
key = {Engineering education},
keywords = {Codes (symbols);Digital storage;E-learning;Education;Students;Teaching;Telecommunication equipment;},
note = {Cloud technologies;Data and information;Educational platforms;Educational potential;Mobile Technology;QR codes;Quick response code;Teaching and learning;},
URL = {http://dx.doi.org/10.1109/ICACCE.2016.8073777},
} 


@inproceedings{20123415364462,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An empirical investigation of changes in some software properties over time},
journal = {IEEE International Working Conference on Mining Software Repositories},
author = {Gil, Joseph and Goldstein, Maayan and Moshkovich, Dany},
year = {2012},
pages = {227 - 236},
issn = {21601852},
address = {Zurich, Switzerland},
abstract = {Software metrics are easy to define, but not so easy to justify. It is hard to prove that a metric is valid, i.e., that measured numerical values imply anything on the vaguely defined, yet crucial software properties such as complexity and maintainability. This paper employs statistical analysis and tests to check some plausible assumptions on the behavior of software and metrics measured for this software in retrospective on its versions evolution history. Among those are the reliability assumption implicit in the application of any code metric, and the assumption that the magnitude of change, i.e., increase or decrease of its size, in a software artifact is correlated with changes to its version number. Putting a suite of 36 metrics to the trial, we confirm most of the assumptions on a large repository of software artifacts. Surprisingly, we show that a substantial portion of the reliability of some metrics can be observed even in random changes to architecture. Another surprising result is that Boolean-valued metrics tend to flip their values more often in minor software version increments than in major increments. &copy; 2012 IEEE.<br/>},
key = {Software reliability},
keywords = {Application programs;Software testing;},
note = {Empirical investigation;Evolution history;Numerical values;Plausible assumptions;Software artifacts;Software metrics;Software properties;Software versions;},
URL = {http://dx.doi.org/10.1109/MSR.2012.6224285},
} 


@inproceedings{20145200361996,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A Bayesian network approach for compiler auto-tuning for embedded processors},
journal = {2014 IEEE 12th Symposium on Embedded Systems for Real-Time Multimedia, ESTIMedia 2014},
author = {Ashouri, Amir Hossein and Mariani, Giovanni and Palermo, Gianluca and Silvano, Cristina},
year = {2014},
pages = {90 - 97},
address = {Greater Noida, Uttar Pradesh, India},
abstract = {The complexity and diversity of today's architectures require an additional effort from the programmers in porting and tuning the application code across different platforms. The problem is even more complex when considering that also the compiler requires some tuning, since standard optimization options have been customized for specific architectures or designed for the average case. This paper proposes a machine-learning approach for reducing the cost of the compiler autotuning phase and to speedup the application performance in embedded architectures. The proposed framework is based on an application characterization done dynamically with microarchitecture independent features and based on the usage of Bayesian Networks. The main characteristic of the Bayesian Network approach consists of not describing the solution as a strict set of compiler transformations to be applied, but as a complex probability distribution function to be sampled. Experimental results, carried out on an ARM platform and GCC transformation space, proved the effectiveness of the proposed methodology for the selected benchmarks. The selected set of solutions (less than 10% of the search space) demonstrated to be very close to the optimal sequence of transformations, showing also an applications performance speedup up to 2.8 (1.5 on average) with respect to -O2 and -O3 for the cBench suite. Additionally, the proposed method demonstrated a 3&times; speedup in terms of search time with respect to an iterative compilation approach, given the same quality of the solutions<sup>1</sup>.<br/> &copy; 2014 IEEE.},
key = {Program compilers},
keywords = {Bayesian networks;Complex networks;Distribution functions;Embedded systems;Iterative methods;Learning systems;Network architecture;Probability distributions;Real time systems;},
note = {Application performance;Compiler transformations;Embedded architecture;Embedded processors;Iterative compilation;Machine learning approaches;Standard optimization;Transformation spaces;},
URL = {http://dx.doi.org/10.1109/ESTIMedia.2014.6962349},
} 


@inproceedings{20185006225289,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Teaching introductory web development using scrimba: An interactive and cooperative development tool},
journal = {Proceedings of the European Conference on e-Learning, ECEL},
author = {Lauvas, Per and Gonzalez, Rolando},
volume = {2018-November},
year = {2018},
pages = {299 - 307},
issn = {20488637},
address = {Athens, Greece},
abstract = {New software applications may influence the way we teach. Scrimba facilitates live stream coding, ease of sharing code and an interesting new video format. What looks like a video is actually an audio stream combined with a dynamic code display. The viewer hears the audio while the code is generated as if it were a normal video tutorial. However, as the format is audio combined with text, the viewer may at any time stop the "video" and edit the code. How can educators use Scrimba to activate students and engage them in cooperative activities? We have investigated different use cases using Scrimba in two introductory web development courses. After an initial pilot, we evaluated the use of Scrimba in a course with 200 students. Data was collected through a survey (N=107) and semi-structured interviews with ten students. The students provided multiple reasons why the live stream of the coding in a classroom was useful. They also saw the value of being able to watch something that looks like a video, but with the possibility of jumping into the code and start to build upon it. The main finding, however, was how the ease of sharing code within a classroom setting created new opportunities. The entire class could engage in debugging activities, they could display multiple solutions for each other and they could create cooperative assignments. These are not new activities, but the activities were enhanced because of the reduced time in order to be able to cooperate and interact. We argue that the students became live coding participants and not only spectators through the introduction of a new software application. We further discuss these findings in the context of blended learning. Our findings should be relevant and interesting for anyone involved in teaching computer programming topics, and especially within web development.<br/> &copy; The Authors, 2018. All Rights Reserved.},
key = {Students},
keywords = {Application programs;Codes (symbols);Computer programming;Curricula;E-learning;Teaching;Tools;},
note = {Active Learning;Blended learning;Cooperative learning;Scrimba;Web development;},
} 


@inproceedings{20161702283603,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Dynamically testing GUIs using ant colony optimization},
journal = {Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},
author = {Carino, Santo and Andrews, James H.},
year = {2015},
pages = {138 - 148},
address = {Lincoln, NE, United states},
abstract = {In this paper we introduce a dynamic GUI test generator that incorporates ant colony optimization. We created two ant systems for generating tests. Our first ant system implements the normal ant colony optimization algorithm in order to traverse the GUI and find good event sequences. Our second ant system, called AntQ, implements the antq algorithm that incorporates Q-Learning, which is a behavioral reinforcement learning technique. Both systems use the same fitness function in order to determine good paths through the GUI. Our fitness function looks at the amount of change in the GUI state that each event causes. Events that have a larger impact on the GUI state will be favored in future tests. We compared our two ant systems to random selection. We ran experiments on six subject applications and report on the code coverage and fault finding abilities of all three algorithms.<br/> &copy; 2015 IEEE.},
key = {Ant colony optimization},
keywords = {Artificial intelligence;Graphical user interfaces;Reinforcement learning;Software engineering;},
note = {Ant Colony Optimization algorithms;Ant systems;Code coverage;Event sequence;Fault finding;Fitness functions;Random selection;Reinforcement learning techniques;},
URL = {http://dx.doi.org/10.1109/ASE.2015.70},
} 


@inproceedings{20142817911960,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Impact of consecutive changes on later file versions},
journal = {ACM International Conference Proceeding Series},
author = {Dai, Meixi and Shen, Beijun and Zhang, Tao and Zhao, Min},
year = {2014},
pages = {17 - 24},
address = {Nanjing, China},
abstract = {By analyzing histories of program versions, many researches have shown that software quality is associated with history-related metrics, such as code-related metrics, commit-related metrics, developer-related metrics, process-related metrics, and organizational metrics etc. It has also been revealed that consecutive changes on commit-level are strongly associated with software defects. In this paper, we introduce two novel concepts of consecutive changes: CFC (chain of consecutive bug-fixing file versions) and CAC (chain of consecutive file versions where each pair of adjacent versions are submitted by different developers). And then several experiments are conducted to explore the correlation between consecutive changes and software quality by using three open-source projects from Github. Our main findings include: 1) CFCs and CACs widely exist in file version histories; 2) Consecutive changes have a negative and strong impact on the later file versions in a short term, especially when the length of consecutive change chain is 4 or 5. &copy; 2014 ACM.<br/>},
key = {Open systems},
keywords = {Chains;Computer software selection and evaluation;Open source software;},
note = {Bug-fixing;consecutive change;Mining software repositories;Novel concept;Open source projects;Short term;Software defects;Software Quality;},
URL = {http://dx.doi.org/10.1145/2627508.2627512},
} 


@inproceedings{20133316600570,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Wind turbine pitch change simulation with helicoidal vortex model},
journal = {Proceedings of the ASME Turbo Expo},
author = {Chattot, Jean-Jacques and Braaten, Mark E.},
volume = {6},
year = {2012},
pages = {789 - 796},
address = {Copenhagen, Denmark},
abstract = {The vortex model has been extended to account for large changes in power, such as occur with abrupt changes in blade pitch. The wake is treated as a flexible spring, extending from the rotor to the far-field (Trefftz plane), along which the pitch varies according to the convected power history at the rotor. Results of test cases are compared with experimental data available from the Tjaereborg and NREL experiments, indicating that the code gives not only correct power levels asymptotically, but also predicts the transient overshoots and undershoots and recovery time accurately at a very low cost. Copyright &copy; 2012 by ASME.<br/>},
key = {Gas turbines},
keywords = {Gas plants;Solar energy;Solar power plants;Steam power plants;Steam turbines;Vortex flow;Wind power;Wind turbines;},
note = {Blade pitch;Helicoidal vortex model;Pitch changes;Power levels;Recovery time;Test case;Trefftz plane;Vortex model;},
URL = {http://dx.doi.org/10.1115/GT2012-68294},
} 


@article{20124915756157,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A software engineering perspective on environmental modeling framework design: The Object Modeling System},
journal = {Environmental Modelling and Software},
author = {David, O. and Ascough, J.C. and Lloyd, W. and Green, T.R. and Rojas, K.W. and Leavesley, G.H. and Ahuja, L.R.},
volume = {39},
year = {2013},
pages = {201 - 213},
issn = {13648152},
abstract = {The environmental modeling community has historically been concerned with the proliferation of models and the effort associated with collective model development tasks (e.g., code generation, data transformation, etc.). Environmental modeling frameworks (EMFs) have been developed to address this problem, but much work remains before EMFs are adopted as mainstream modeling tools. Environmental model development requires both scientific understanding of environmental phenomena and software developer proficiency. EMFs support the modeling process through streamlining model code development, allowing seamless access to data, and supporting data analysis and visualization. EMFs also support aggregation of model components into functional units, component interaction and communication, temporal-spatial stepping, scaling of spatial data, multi-threading/multi-processor support, and cross-language interoperability. Some EMFs additionally focus on high-performance computing and are tailored for particular modeling domains such as ecosystem, socio-economic, or climate change research. The Object Modeling System Version 3 (OMS3) EMF employs new advances in software framework design to better support the environmental model development process. This paper discusses key EMF design goals/constraints and addresses software engineering aspects that have made OMS3 framework development efficacious and its application practical, as demonstrated by leveraging software engineering efforts outside of the modeling community and lessons learned from over a decade of EMF development. Software engineering approaches employed in OMS3 are highlighted including a non-invasive lightweight framework design supporting component-based model development, use of implicit parallelism in system design, use of domain specific language design patterns, and cloud-based support for computational scalability. The key advancements in EMF design presented herein may be applicable and beneficial for other EMF developers seeking to better support environmental model development through improved framework design. &copy; 2012 Elsevier Ltd.<br/>},
key = {Software design},
keywords = {Application programs;Climate change;Computer aided software engineering;Data visualization;Distributed computer systems;Interoperability;Metadata;Problem oriented languages;Software engineering;},
note = {Component-based modeling;Computational scalability;Domain specific languages;Environmental modeling frameworks;Environmental phenomena;High performance computing;Model and simulation;Object modeling systems;},
URL = {http://dx.doi.org/10.1016/j.envsoft.2012.03.006},
} 


@article{20172503798243,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Examining the stability of logging statements},
journal = {Empirical Software Engineering},
author = {Kabinna, Suhas and Bezemer, Cor-Paul and Shang, Weiyi and Syer, Mark D and Hassan, Ahmed E},
volume = {23},
number = {1},
year = {2018},
pages = {290 - 333},
issn = {13823256},
abstract = {Logging statements (embedded in the source code) produce logs that assist in understanding system behavior, monitoring choke-points and debugging. Prior work showcases the importance of logging statements in operating, understanding and improving software systems. The wide dependence on logs has lead to a new market of log processing and management tools. However, logs are often unstable, i.e., the logging statements that generate logs are often changed without the consideration of other stakeholders, causing sudden failures of log processing tools and increasing the maintenance costs of such tools. We examine the stability of logging statements in four open source applications namely: Liferay, ActiveMQ, Camel and CloudStack. We find that 20&ndash;45% of their logging statements change throughout their lifetime. The median number of days between the introduction of a logging statement and the first change to that statement is between 1 and 17 in our studied applications. These numbers show that in order to reduce maintenance effort, developers of log processing tools must be careful when selecting the logging statements on which their tools depend. In order to effectively mitigate the issues that are caused by unstable logging statements, we make an important first step towards determining whether a logging statement is likely to remain unchanged in the future. First, we use a random forest classifier to determine whether a just-introduced logging statement will change in the future, based solely on metrics that are calculated when it is introduced. Second, we examine whether a long-lived logging statement is likely to change based on its change history. We leverage Cox proportional hazards models (Cox models) to determine the change risk of long-lived logging statements in the source code. Through our case study on four open source applications, we show that our random forest classifier achieves a 83&ndash;91% precision, a 65&ndash;85% recall and a 0.95&ndash;0.96 AUC. We find that file ownership, developer experience, log density and SLOC are important metrics in our studied projects for determining the stability of logging statements in both our random forest classifiers and Cox models. Developers can use our approach to determine the risk of a logging statement changing in their own projects, to construct more robust log processing tools, by ensuring that these tools depend on logs that are generated by more stable logging statements.<br/> &copy; 2017, Springer Science+Business Media New York.},
key = {Well logging},
keywords = {Decision trees;Open source software;Program debugging;Stability;},
note = {Cox proportional hazards models;Log file;Maintenance efforts;Open source application;Processing tools;Random forest classifier;Software systems;System behaviors;},
URL = {http://dx.doi.org/10.1007/s10664-017-9518-0},
} 


@article{20154001339929,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Isotopic, ecological and technological investigations of the land snail record},
journal = {ProQuest Dissertations and Theses Global},
author = {Faber, Meredith L.},
year = {2012},
abstract = {In the ever-evolving landscape of the natural world, change is the only constant. Investigating how life accommodates that change can provide valuable insights into the biological, ecological and geological history of our planet. The fossil record is replete with examples of organisms which failed to survive in the wake of ongoing environmental change. However, for as many organisms as succumbed to extinction, there are just as many that not only survived, but thrived. Adaptability, in the face of ecological adversity, is the key to evolutionary success. This trait is no more evident than in the life history of terrestrial gastropods. This dissertation examines how land snail strategies for survival may reveal valuable paleoenvironmental and paleoecological information and how that information might be more effectively and efficiently managed using mobile technology. The results of stable isotopic analyses of modern and ancient land snail shells from two Algerian archaeological sites and experiments demonstrating the effects of insect and non-insect herbivory on leaf tissue combined with data management techniques exploring the use of Augmented Reality (AR) and Quick Response (QR) Code 2D barcode technology in laboratory research show how dynamic life can be, even when you live it "in the slow lane.". ProQuest Subject Headings: Geochemistry, Paleoecology, Information technology.<br/>  &copy; Citation reproduced with permission of ProQuest LLC.},
key = {Ecology},
keywords = {Augmented reality;Bar codes;Geochemistry;Information management;Information technology;Isotopes;Molluscs;},
note = {Archaeological site;Data management techniques;Environmental change;Geological history;Mobile Technology;Paleoecology;Quick response code;Terrestrial gastropods;},
} 


@inproceedings{20161102102436,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Users beware: Preference inconsistencies ahead},
journal = {2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings},
author = {Behrang, Farnaz and Cohen, Myra B. and Orso, Alessandro},
year = {2015},
pages = {295 - 306},
address = {Bergamo, Italy},
abstract = {The structure of preferences for modern highly-configurable software systems has become extremely complex, usually consisting of multiple layers of access that go from the user interface down to the lowest levels of the source code. This complexity can lead to inconsistencies between layers, especially during software evolution. For example, there may be preferences that users can change through the GUI, but that have no effect on the actual behavior of the system because the related source code is not present or has been removed going from one version to the next. These inconsistencies may result in unexpected program behaviors, which range in severity from mild annoyances to more critical security or performance problems. To address this problem, we present SCIC (Software Configuration Inconsistency Checker), a static analysis technique that can automatically detect these kinds of inconsistencies. Unlike other configuration analysis tools, SCIC can handle software that (1) is written in multiple programming languages and (2) has a complex preference structure. In an empirical evaluation that we performed on 10 years worth of versions of both the widely used Mozilla Core and Firefox, SCIC was able to find 40 real inconsistencies (some determined as severe), whose lifetime spanned multiple versions, and whose detection required the analysis of code written in multiple languages.<br/> &copy; 2015 ACM.},
key = {Static analysis},
keywords = {Codes (symbols);Problem oriented languages;Software engineering;User interfaces;},
note = {Analysis techniques;Complex preferences;Configurable systems;Configuration analysis;Empirical evaluations;Performance problems;Software configuration;Software Evolution;},
URL = {http://dx.doi.org/10.1145/2786805.2786869},
} 


@inproceedings{20184005895124,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Avances en Ingenieria de Software a Nivel Iberoamericano, CIbSE 2018},
journal = {Avances en Ingenieria de Software a Nivel Iberoamericano, CIbSE 2018},
year = {2018},
pages = {630 - },
address = {Bogota, Colombia},
abstract = {The proceedings contain 50 papers. The topics discussed include: a taste of the software industry perception of technical debt and its management in Brazil; knowledge management in agile testing teams: a survey; gender gap in computing: a preliminary empirical study; an exploratory study of academic architectural tactics and patterns in microservices: a systematic literature review; software measurement and estimation: a case study in the financial services industry; towards a developer's reputation model based on source code features; privacy by design in software engineering: a systematic mapping study; challenges in system of systems development: a systematic mapping; an ontology-based approach to identify developers in a software ecosystem based on their skillset; towards using task similarity to recommend stack overflow posts; improving the decision-making support in context-aware applications: the case of an adaptive virtual education learning management system; S&yacute;ntixi - a generative approach to dynamic fusion of software components; towards a simulation-based project monitoring and control learning approach; evaluation of source code in ACM ICPC style programming and training competitions; applying transformation templates to diversify user interfaces generated by model-driven engineering; and MoWeb - a mobile: modeling and generation of the communication of mobile apps with their functions in the cloud.<br/>},
} 


@inproceedings{20185106278914,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Design relativistic charged particle beam transportation channels},
journal = {25th Russian Particle Accelerator Conference, RuPAC 2016},
author = {Averyanov, G.P. and Budkin, V.A. and Osadchuk, I.O.},
year = {2016},
pages = {354 - 356},
address = {St. Petersburg, Russia},
abstract = {This paper contains results of development new version (2016) of program for channels design high-energy beams of charged particles. The program includes application package modeling the dynamics of charged particles in the channel, operational tools to change the channel parameters, channel optimization tools and processing output beam parameters with graphic and digital presentation of its key features. The MATLAB (Scilab) was used as programming tools, allows to make the source code modular, compact and scalable. New object-oriented graphical user interface provides an interactive assembly of new or modernization of previously developed channel - selection and arrangement of its elements, as well as the installation and the variation of their parameters. The relational database, which is part of the new version of program, providing additional functionality to the designer. It is intended for storage of the current development, and to preserve the previously completed projects, as well as other useful designer related information. A multi-output of all the main parameters of the beam at the output, as well as anywhere in the channel. In this case, the developer has the ability to interactively search and setting the optimum mode of operation channel.<br/> Copyright &copy; 2017 CC-BY-3.0 and by the respective authors},
key = {Charged particles},
keywords = {Application programs;Graphical user interfaces;MATLAB;Object oriented programming;Particle accelerators;},
note = {Channel optimization;Channel parameter;Digital presentation;High-energy beams;Interactive assembly;Mode of operations;Relational Database;Transportation channels;},
} 


@inproceedings{20143718163444,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A longitudinal study of programmers' backtracking},
journal = {Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},
author = {Yoon, Young Seok and Myers, Brad A.},
year = {2014},
pages = {101 - 108},
issn = {19436092},
address = {Melbourne, VIC, Australia},
abstract = {Programming often involves reverting source code to an earlier state, which we call backtracking. We performed a longitudinal study of programmers' backtracking, analyzing 1,460 hours of fine-grained code editing logs collected from 21 people. Our analysis method keeps track of the change history of each abstract syntax tree node and looks for backtracking instances within each node. Using this method, we detected a total of 15,095 backtracking instances, which gives an average backtracking rate of 10.3/hour. The size of backtracking varied con-siderably, ranging from a single character to thousands of char-acters. 34% of the backtracking was performed by manually deleting or typing the desired code, and 9.5% of all backtracking was selective, meaning that it could not have been performed using the conventional undo command present in the IDE. The study results show that programmers need better backtracking tools, and also provide design implications for such tools. &copy; 2014 IEEE.<br/>},
key = {Visual languages},
keywords = {Codes (symbols);Human computer interaction;Integrodifferential equations;Trees (mathematics);},
note = {Abstract Syntax Trees;Analysis method;backtracking;Design implications;Empirical studies;Interactive development environment;Longitudinal study;undo;},
URL = {http://dx.doi.org/10.1109/VLHCC.2014.6883030},
} 


@inproceedings{20173404066957,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017},
journal = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017},
year = {2017},
pages = {Association for Computing Machinery; et al.; Exactas UBA 150; IEEE Computer Society; IEEE Technical Council on Software Engineering (TCSE); Special Interest Group on Software Engineering (SIGSOFT) - },
address = {Buenos Aires, Argentina},
abstract = {The proceedings contain 68 papers. The topics discussed include: semantically enhanced software traceability using deep learning techniques; can latent topics in source code predict missing architectural tactics?; preventing defects: the impact of traceability completeness on software quality; imprecise matching of requirements specifications for software services using fuzzy logic; analyzing APIs documentation and code to detect directive defects; detecting user story information in developer-client conversations to generate extractive summaries; keyword search for building service-based systems; clone refactoring with lambda expressions; automated refactoring of legacy Java software to default methods; using cohesion and coupling for software remodularization: is it enough?; recommending and localizing change requests for mobile apps based on user reviews; machine learning-based detection of open source license exceptions; supporting change impact analysis using a recommendation system: an industrial case study in a safety-critical context; becoming agile: a grounded theory of agile transitions in practice; from diversity by numbers to diversity as process: supporting inclusiveness in software development teams with brainstorming; and process aspects and social dynamics of contemporary code review: insights from open source development and industrial practice at Microsoft.},
} 


@inproceedings{20160501868930,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {SARATHI: Characterization study on regression bugs and identification of regression bug inducing changes: A case-study on Google Chromium Project},
journal = {ACM International Conference Proceeding Series},
author = {Khattar, Manisha and Lamba, Yash and Sureka, Ashish},
volume = {18-20-February-2015},
year = {2015},
pages = {50 - 59},
address = {Bangalore, India},
abstract = {As a software system evolves, maintaining the system becomes increasingly difficult. A lot of times code changes or system patches cause an existing feature to misbehave or fail completely. An issue ticket reporting a defect in a feature that was working earlier, is known as a Regression Bug. Running a test suite to validate the new features getting added and faults introduced in previously working code, after every change is impractical. As a result, by the time an issue is identified and reported a lot of changes are made to the source code, which makes it very difficult for the developers to find the regression bug inducing change. Regression bugs are considered to be inevitable and truism in large and complex software systems [1]. Issue Tracking System (ITS) are applications to track and manage issue reports and to archive bug or feature enhancement requests. Version Control System (VCS) are source code control systems recording the author, timestamp, commit message and modified files. We first conduct an in-depth characterization study of regression bugs by mining issue tracking system dataset belonging to a large and complex software system i.e. Google Chromium Project. We then describe our solution approach to find the regression bug inducing change, based on mining ITS and VCS data. We build a recommendation engine Sarathi1 to assist a bug fixer in locating a regression bug inducing change and validate the system on real world Google Chromium project.<br/> Copyright 2015 ACM.},
key = {Program debugging},
keywords = {Codes (symbols);Computer software;Computer software maintenance;Control systems;Regression analysis;Tracking (position);},
note = {Empirical Software Engineering;Issue Tracking;Mining software repositories;Predictive modeling;Regression bugs;},
URL = {http://dx.doi.org/10.1145/2723742.2723747},
} 


@article{20182605384319,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Transforming programs and tests in tandem for fault localization},
journal = {Proceedings of the ACM on Programming Languages},
author = {Li, Xia and Zhang, Lingming},
volume = {1},
number = {OOPSLA},
year = {2017},
issn = {24751421},
abstract = {Localizing failure-inducing code is essential for software debugging. Manual fault localization can be quite tedious, error-prone, and time-consuming. Therefore, a huge body of research eorts have been dedicated to automated fault localization. Spectrum-based fault localization, the most intensively studied fault localization approach based on test execution information, may have limited eectiveness, since a code element executed by a failed tests may not necessarily have impact on the test outcome and cause the test failure. To bridge the gap, mutation-based fault localization has been proposed to transform the programs under test to check the impact of each code element for better fault localization. However, there are limited studies on the eectiveness of mutation-based fault localization on sucient number of real bugs. In this paper, we perform an extensive study to compare mutation-based fault localization techniques with various state-of-the-art spectrum-based fault localization techniques on 357 real bugs from the Defects4J benchmark suite. The study results rstly demonstrate the eectiveness of mutation-based fault localization, as well as revealing a number of guidelines for further improving mutation-based fault localization. Based on the learnt guidelines, we further transform test outputs/messages and test code to obtain various mutation information. Then, we propose TraPT, an automated Learning-to-Rank technique to fully explore the obtained mutation information for eective fault localization. The experimental results show that TraPT localizes 65.12% and 94.52% more bugs within Top-1 than state-of-the-art mutation and spectrum based techniques when using the default setting of LIBSVM.<br/> &copy; 2017 Association for Computing Machinery.},
key = {Program debugging},
keywords = {Codes (symbols);Cosine transforms;},
note = {Automated learning;Benchmark suites;Code transformation;Fault localization;Localizing failures;Mutation testing;Software debugging;State of the art;},
URL = {http://dx.doi.org/10.1145/3133916},
} 


@article{20131516196794,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {SARAH 3.2: Dirac gauginos, UFO output, and more},
journal = {Computer Physics Communications},
author = {Staub, Florian},
volume = {184},
number = {7},
year = {2013},
pages = {1792 - 1809},
issn = {00104655},
abstract = {SARAH is a Mathematica package optimized for the fast, efficient and precise study of supersymmetric models beyond the MSSM: a new model can be defined in a short form and all vertices are derived. This allows SARAH to create model files for FeynArts/FormCalc, CalcHep/CompHep and WHIZARD/O'Mega. The newest version of SARAH now provides the possibility to create model files in the UFO format which is supported by MadGraph 5, MadAnalysis 5, GoSam, and soon by Herwig++. Furthermore, SARAH also calculates the mass matrices, RGEs and 1-loop corrections to the mass spectrum. This information is used to write source code for SPheno in order to create a precision spectrum generator for the given model. This spectrum-generator-generator functionality as well as the output of WHIZARD and CalcHep model files has seen further improvement in this version. Also models including Dirac gauginos are supported with the new version of SARAH, and additional checks for the consistency of the implementation of new models have been created. Program summary: Program title:SARAH Catalogue identifier: AEIB-v2-0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ AEIB-v2-0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 3 22 411 No. of bytes in distributed program, including test data, etc.: 3 629 206 Distribution format: tar.gz Programming language: Mathematica. Computer: All for which Mathematica is available. Operating system: All for which Mathematica is available. Classification: 11.1, 11.6. Catalogue identifier of previous version: AEIB-v1-0 Journal reference of previous version: Comput. Phys. Comm. 182 (2011) 808 Does the new version supersede the previous version?: Yes, the new version includes all known features of the previous version but also provides the new features mentioned below. Nature of problem: To use Madgraph for new models it is necessary to provide the corresponding model files which include all information about the interactions of the model. However, the derivation of the vertices for a given model and putting those into model files which can be used with Madgraph is usually very time consuming. Dirac gauginos are not present in the minimal supersymmetric standard model (MSSM) or many extensions of it. Dirac mass terms for vector superfields lead to new structures in the supersymmetric (SUSY) Lagrangian (bilinear mass term between gaugino and matter fermion as well as new D-terms) and modify also the SUSY renormalization group equations (RGEs). The Dirac character of gauginos can change the collider phenomenology. In addition, they come with an extended Higgs sector for which a precise calculation of the 1-loop masses has not happened so far. Solution method: SARAH calculates the complete Lagrangian for a given model whose gauge sector can be any direct product of SU(N) gauge groups. The chiral superfields can transform as any, irreducible representation with respect to these gauge groups and it is possible to handle an arbitrary number of symmetry breakings or particle rotations. Also the gauge fixing is automatically added. Using this information, SARAH derives all vertices for a model. These vertices can be exported to model files in the UFO which is supported by Madgraph and other codes like GoSam, MadAnalysis or ALOHA. The user can also study models with Dirac gauginos. In that case SARAH includes all possible terms in the Lagrangian stemming from the new structures and can also calculate the RGEs. The entire impact of these terms is then taken into account in the output of SARAH to UFO, CalcHep, WHIZARD, FeynArts and SPheno. Reasons for new version: SARAH provides, with this version, the possibility of creating model files in the UFO format. The UFO format is supposed to become a standard format for model files which should be supported by many different tools in the future. Also models with Dirac gauginos were not supported in earlier versions. Summary of revisions: Support of models with Dirac gauginos. Output of model files in the UFO format, speed improvement in the output of WHIZARD model files, CalcHep output supports the internal diagonalization of mass matrices, output of control files for LHPC spectrum plotter, support of generalized PDG numbering scheme PDG.IX, improvement of the calculation of the decay widths and branching ratios with SPheno, the calculation of new low energy observables are added to the SPheno output, the handling of gauge fixing terms has been significantly simplified. Restrictions: SARAH can only derive the Lagrangian in an automatized way for N=1 SUSY models, but not for those with more SUSY generators. Furthermore, SARAH supports only renormalizable operators in the output of model files in the UFO format and also for CalcHep, FeynArts and WHIZARD. Also color sextets are not yet included in the model files for Monte Carlo tools. Dimension 5 operators are only supported in the calculation of the RGEs and mass matrices. Unusual features: SARAH does not need the Lagrangian of a model as input to calculate the vertices. The gauge structure, particle and content and superpotential as well as rotations stemming from gauge symmetry breaking are sufficient. All further information is derived by SARAH on its own. Therefore, the model files are very short and the implementation of new models is fast and easy. In addition, the implementation of a model can be checked for physical and formal consistency. In addition, SARAH can generate Fortran code for a full 1-loop analysis of the mass spectrum in the context for Dirac gauginos. Running time: Measured CPU time for the evaluation of the MSSM using a Lenovo Thinkpad X220 with i7 processor (2.53 GHz). Calculating the complete Lagrangian: 9 s. Calculating all vertices: 51 s. Output of the UFO model files: 49 s. &copy; 2013 Elsevier B.V. All rights reserved.<br/>},
key = {Iron compounds},
keywords = {Codes (symbols);Fractals;Gages;Lagrange multipliers;Mass spectrometers;Mass spectrometry;Matrix algebra;Software testing;Statistical mechanics;Supersymmetry;},
note = {Catalogue identifiers;Dirac gauginos;Irreducible representations;MadGraph;Minimal supersymmetric standard models;Renormalization group equations;SARAH;Supersymmetric models;},
URL = {http://dx.doi.org/10.1016/j.cpc.2013.02.019},
} 


@inproceedings{20183205678659,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {2018 IEEE 1st International Workshop on Mining and Analyzing Interaction Histories, MAINT 2018 - Proceedings},
journal = {2018 IEEE 1st International Workshop on Mining and Analyzing Interaction Histories, MAINT 2018 - Proceedings},
volume = {2018-January},
year = {2018},
address = {Campobasso, Italy},
abstract = {The proceedings contain 4 papers. The topics discussed include: the cost-benefit analysis of usage data in RobotStudio; CodeCAM: capturing programmer's reaction during coding session; privacy preservation in interaction history on integrated development environments; and integrating source code search into Git client for effective retrieving of change history.<br/>},
} 


@inproceedings{20182105214963,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
journal = {Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
volume = {2017-November},
year = {2018},
pages = {IEEE Beijing Section; The Institute of Electrical and Electronics Engineers - },
issn = {23270586},
address = {Beijing, China},
abstract = {The proceedings contain 212 papers. The topics discussed include: utilizing cluster quality in hierarchical clustering for analogy-based software effort estimation; unconventional service engineering: toward a new paradigm of service engineering for empowering senior citizens in city platform as a service; classification and metrics for replay tools; SuperedgeRank algorithm and its application for core technology identification; performance analysis of CDN-P2P networks based on processer-sharing queues; a test language for avionics system; application of CART decision tree combined with PCA algorithm in intrusion detection; a performance comparison of two versatile frequency transformation approach in texture image retrieval; intrusion detection for engineering vehicles under the transmission line based on deep learning; detecting injection vulnerabilities in executable codes with concolic execution; an abstract method linearization for detecting source code plagiarism in object-oriented environment; adaptive life-cycle control system for overhead transmission lines using forecasting models; intelligent model of decision support system of distributed generation integration; machine learning for predictive maintenance of industrial machines using IoT sensor data; ECG based authentication for remote patient monitoring in IoT by wavelets and template matching; and a model-driven deployment approach for scaling distributed software architectures on a cloud computing platform.<br/>},
} 


@inproceedings{20123415354804,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A basic model for proactive event-driven computing},
journal = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems, DEBS'12},
author = {Engel, Yagil and Etzion, Opher and Feldman, Zohar},
year = {2012},
pages = {107 - 118},
address = {Berlin, Germany},
abstract = {During the movie "Source Code" there is a shift in the plot; from (initially) reacting to a train explosion that already occurred and trying to eliminate further explosions, to (later) changing the reality to avoid the original train explosion. Whereas changing the history after events have happened is still within the science fiction domain, changing the reality to avoid events that have not happened yet is, in many cases, feasible, and may yield significant benefits. We use the term proactive behavior to designate the change of what will be reality in the future. In particular, we focus on proactive event-driven computing: the use of event-driven systems to predict future events and react to them before they occur. In this paper we start our investigation of this large area by constructing a model and end-to-end implementation of a restricted subset of basic proactive applications that is trying to eliminate a single forecasted event, selecting between a finite and relatively small set of feasible actions, known at design time, based on quantified cost functions over time. After laying out the model, we describe the extensions required of the conceptual architecture of event processing to support such applications: supporting proactive agents as part of the model, supporting the derivation of forecasted events, and supporting various aspects of uncertainty; next, we show a decision algorithm that selects among the alternatives. We demonstrate the approach by implementing an example of a basic proactive application in the area of condition based maintenance, and showing experimental results. Copyright &copy; 2012 ACM.<br/>},
key = {Decision making},
keywords = {Cost functions;Software architecture;},
note = {Conceptual architecture;Condition based maintenance;Decision algorithms;Event Processing;Event-driven computing;Event-driven system;Proactive applications;Proactive computing;},
URL = {http://dx.doi.org/10.1145/2335484.2335496},
} 


@inproceedings{20180704793803,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {11th International Andrei Ershov Memorial Conference on Perspectives of System Informatics, PSI 2017},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {10742 LNCS},
year = {2018},
issn = {03029743},
address = {Moscow, Russia},
abstract = {The proceedings contain 31 papers. The special focus in this conference is on Perspectives of System Informatics. The topics include: Lightweight non-intrusive virtual machine introspection; a distributed approach to coreference resolution in multiagent text analysis for ontology population; a framework for dynamical construction of software components; A transformation-based approach to developing high-performance GPU programs; domain engineering the Magnolia way; approximating event system abstractions by covering their states and transitions; implementing the symbolic method of verification in the C-light project; highlights of the rice-shapiro theorem in computable topology; a memory model for deductively verifying linux kernel modules; a human-in-the-loop perspective for safety assessment in robotic applications; indexing of hierarchically organized spatial-temporal data using dynamic regular octrees; An approach to the validation of XML documents based on the model driven architecture and the object constraint language; compositional relational programming with name projection and compositional synthesis; whaleProver: First-order intuitionistic theorem prover based on the inverse method; distributed in situ processing of big raster data in the cloud; statistical approach to increase source code completion accuracy; using the subject area ontology for automating learning processes and scientific investigation; Runtime specialization of postgreSQL query executor; MicroTESK: A tool for constrained random test program generation for microprocessors; Enriching textual Xtext-DSLs with a graphical GEF-based editor; multi-level static analysis for finding error patterns and defects in source code; Towards automated static verification of GNU C programs; domain specific semantic validation of schema.org annotations; pipelined bottom-up evaluation of datalog programs: The push method; PosDB: A distributed column-store engine.<br/>},
} 


@inproceedings{20151300684080,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards a refactoring catalogue for knowledge discovery metamodel},
journal = {Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration, IEEE IRI 2014},
author = {Durelli, Rafael S. and Santibanez, Daniel S.M. and Delamaro, Marcio E. and De Camargo, Valter Vieira},
year = {2014},
pages = {569 - 576},
address = {San Francisco, CA, United states},
abstract = {Refactorings are a well known technique that assist developers in reformulating the overall structure of applications aiming to improve internal quality attributes while preserving their original behavior. One of the most conventional uses of refactorings are in reengineering processes, whose goal is to change the structure of legacy systems aiming to solve previously identified structural problems. Architecture-Driven Modernization (ADM) is the new generation of reengineering processes; relying just on models, rather than source code, as the main artifacts along the process. However, although ADM provides the general concepts for conducting model-driven modernizations, it does not provide instructions on how to create or apply refactorings in the Knowledge Discovery Metamodel (KDM) metamodel. This leads developers to create their own refactoring solutions, which are very hard to be reused in other contexts. One of the most well known and useful refactoring catalogue is the Fowler's one, but it was primarily proposed for source-code level. In order to fill this gap, in this paper we present a model-oriented version of the Fowler's Catalogue, so that it can be applied to KDM metamodel. In this paper we have focused on four categories of refactorings: (j') renaming, (w) moving features between objects, (iii) organizing data, and (iv) dealing with generalization. We have also developed an environment to support the application of our catalogue. To evaluate our solution we conducted an experiment using eight open source Java application. The results showed that our catalogue can be used to improve the cohesion and coupling of the legacy system.<br/> &copy; 2014 IEEE.},
key = {Legacy systems},
keywords = {Delta modulation;Information use;Open source software;Reengineering;},
note = {Architecture-driven modernizations;Cohesion and couplings;Empirical studies;Internal quality attributes;Java applications;Open sources;Refactorings;Structural problems;},
URL = {http://dx.doi.org/10.1109/IRI.2014.7051940},
} 


@inproceedings{20162202438178,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Optimizing irregular applications for energy and performance on the tilera many-core architecture},
journal = {Proceedings of the 12th ACM International Conference on Computing Frontiers, CF 2015},
author = {Chavarria-Miranda, Daniel and Panyala, Ajay and Halappanavar, Mahantesh and Manzano, Joseph B. and Tumeo, Antonino},
year = {2015},
pages = {ACM SIGMICRO - },
address = {Ischia, Italy},
abstract = {Optimizing applications simultaneously for energy and performance is a complex problem. High performance, parallel, irregular applications are notoriously hard to optimize due to their data-dependent memory accesses, lack of structured locality and complex data structures and code patterns. Irregular kernels are growing in importance in applications such as machine learning, graph analytics and combinatorial scientific computing. Performance-and energy-efficient implementation of these kernels on modern, energy efficient, many-core platforms is therefore an important and challenging problem. We present results from optimizing two irregular applications-the Louvain method for community detection (Grappolo), and high-performance conjugate gradient (HPCCG)-on the Tilera many-core system. We have significantly extended MIT's OpenTuner auto-tuning framework to conduct a detailed study of platform-independent and platform-specific optimizations to improve performance as well as reduce total energy consumption. We explore the optimization design space along three dimensions: memory layout schemes, compiler-based code transformations, and optimization of parallel loop schedules. Using auto-tuning, we demonstrate whole-node energy savings of up to 41% relative to a baseline instantiation, and up to 31% relative to manually optimized variants.<br/> &copy; Copyright 2015 ACM.},
key = {Computer architecture},
keywords = {Conjugate gradient method;Cosine transforms;Energy conservation;Energy efficiency;Energy utilization;Learning systems;Object oriented programming;},
note = {Code transformation;Combinatorial scientific computing;Community detection;Complex data structures;Irregular applications;Many-core architecture;Platform independent;Total energy consumption;},
URL = {http://dx.doi.org/10.1145/2742854.2742865},
} 


@inbook{20182705482884,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A Design Methodology for Developing Resilient Cloud Services},
journal = {Handbook of System Safety and Security: Cyber Risk and Risk Management, Cyber Security, Threat Analysis, Functional Safety, Software Systems, and Cyber Physical Systems},
author = {Tunc, Cihan and Hariri, Salim and Battou, Abdella},
year = {2016},
pages = {177 - 197},
abstract = {Cloud computing is emerging as a new paradigm that aims at delivering computing as a utility. For the cloud computing paradigm to be fully adopted and effectively used, it is critical that the security mechanisms are robust and resilient to malicious faults and attacks. Security in cloud computing is of major concern and a challenging research problem since it involves many interdependent tasks including application layer firewalls, configuration management, alert monitoring and analysis, source code analysis, and user identity management. It is widely accepted that we cannot build software and computing systems that are free from vulnerabilities and cannot be penetrated or attacked. Therefore, it is widely accepted that cyber resilient techniques are the most promising solutions to mitigate cyberattacks and change the game to advantage the defender over the attacker. Moving Target Defense (MTD) has been proposed as a mechanism to make it extremely difficult for an attacker to exploit existing vulnerabilities by varying the attack surface of the execution environment. By continuously changing the environment (e.g., software versions, programming language, operating system, connectivity, etc.), we can shift the attack surface and, consequently, evade attacks. In this chapter we present a methodology for designing resilient cloud services that is based on the following capabilities: Redundancy, Diversity, Shuffling, and Autonomic Management. Redundancy is used to tolerate attacks if any redundant version or resource is compromised. The diversity is to use to avoid the software monoculture problem where one attack vector can successfully attack many instances of the same software module. Shuffling is needed to randomly change the execution environment and is achieved by "hot" shuffling of multiple functionally equivalent, behaviorally different software versions (code implementation) at runtime (e.g., the software task can have multiple versions where each version can be a different algorithm implemented in different programming language running on different computing systems). We also present our experimental results and evaluation of the RCS design methodology. We have implemented the applications on an IBM blade server with four blades, where each blade has 24 cores and can run several virtual machines. Our experimental results show that our environment is resilient against attacks with less than 7% in overhead time.<br/> &copy; 2017 Elsevier Inc. All rights reserved.},
key = {Computer system firewalls},
keywords = {Cloud computing;Computer viruses;Design;Distributed database systems;Network security;Problem oriented languages;Redundancy;Web services;},
note = {Autonomic Computing;Configuration management;Diversity;Execution environments;Inter-dependent tasks;Monitoring and analysis;Moving target defense;Software monoculture;},
URL = {http://dx.doi.org/10.1016/B978-0-12-803773-7.00009-7},
} 


@inproceedings{20151000598845,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Vigiles: Fine-grained access control for MapReduce systems},
journal = {Proceedings - 2014 IEEE International Congress on Big Data, BigData Congress 2014},
author = {Ulusoy, Huseyin and Kantarcioglu, Murat and Pattuk, Erman and Hamlen, Kevin},
year = {2014},
pages = {40 - 47},
address = {Anchorage, AK, United states},
abstract = {Security concerns surrounding the rise of Big Data systems have stimulated myriad new Big Data security models and implementations over the past few years. A significant disadvantage shared by most of these implementations is that they customize the underlying system source code to enforce new policies, making the customizations difficult to maintain as these layers evolve over time (e.g., over version updates). This paper demonstrates how a broad class of safety policies, including fine-grained access control policies at the level of key-value data pairs rather than files, can be elegantly enforced on MapReduce clouds with minimal overhead and without any change to the system or OS implementations. The approach realizes policy enforcement as a middleware layer that rewrites the cloud's front-end API with reference monitors. After rewriting, the jobs run on input data authorized by fine-grained access control policies, allowing them to be safely executed without additional system-level controls. Detailed empirical studies show that this more modular approach exhibits just 1% overhead compared to a less modular implementation that customizes MapReduce directly to enforce the same policies.<br/> &copy; 2014 IEEE.},
key = {Access control},
keywords = {Big data;Middleware;},
note = {Access control policies;Empirical studies;Map-reduce;Modular implementation;Policy enforcement;Reference monitors;Security;Underlying systems;},
URL = {http://dx.doi.org/10.1109/BigData.Congress.2014.16},
} 


@inproceedings{20140717317139,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Preventing erosion of architectural tactics through their strategic implementation, preservation, and visualization},
journal = {2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings},
author = {Mirakhorli, Mehdi},
year = {2013},
pages = {762 - 765},
address = {Palo Alto, CA, United states},
abstract = {Nowadays, a successful software production is increasingly dependent on how the final deployed system addresses customers' and users' quality concerns such as security, reliability, availability, interoperability, performance and many other types of such requirements. In order to satisfy such quality concerns, software architects are accountable for devising and comparing various alternate solutions, assessing the trade-offs, and finally adopting strategic design decisions which optimize the degree to which each of the quality concerns is satisfied. Although designing and implementing a good architecture is necessary, it is not usually enough. Even a good architecture can deteriorate in subsequent releases and then fail to address those concerns for which it was initially designed. In this work, we present a novel traceability approach for automating the construction of traceabilty links for architectural tactics and utilizing those links to implement a change impact analysis infrastructure to mitigate the problem of architecture degradation. Our approach utilizes machine learning methods to detect tactic-related classes. The detected tactic-related classes are then mapped to a Tactic Traceability Pattern. We train our trace algorithm using code extracted from fifty performance-centric and safety-critical open source software systems and then evaluate it against a real case study. &copy; 2013 IEEE.<br/>},
key = {Open source software},
keywords = {Architecture;Artificial intelligence;Economic and social effects;Interoperability;Learning systems;Object oriented programming;Safety engineering;Software architecture;Software reliability;},
note = {Change impact analysis;Machine learning methods;Open source software systems;Software architects;Software production;tactics;traceability;traceability patterns;},
URL = {http://dx.doi.org/10.1109/ASE.2013.6693152},
} 


@article{20153801292692,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Improving the user experience of the rCUDA remote GPU virtualization framework},
journal = {Concurrency Computation},
author = {Reano, Carlos and Silla, Federico and Castello, Adrian and Pena, Antonio J. and Mayo, Rafael and Quintana-Orti, Enrique S. and Duato, Jose},
volume = {27},
number = {14},
year = {2015},
pages = {3746 - 3770},
issn = {15320626},
abstract = {Graphics processing units (GPUs) are being increasingly embraced by the high-performance computing community as an effective way to reduce execution time by accelerating parts of their applications. remote CUDA (rCUDA) was recently introduced as a software solution to address the high acquisition costs and energy consumption of GPUs that constrain further adoption of this technology. Specifically, rCUDA is a middleware that allows a reduced number of GPUs to be transparently shared among the nodes in a cluster. Although the initial prototype versions of rCUDA demonstrated its functionality, they also revealed concerns with respect to usability, performance, and support for new CUDA features. In response, in this paper, we present a new rCUDA version that (1) improves usability by including a new component that allows an automatic transformation of any CUDA source code so that it conforms to the needs of the rCUDA framework, (2) consistently features low overhead when using remote GPUs thanks to an improved new communication architecture, and (3) supports multithreaded applications and CUDA libraries. As a result, for any CUDA-compatible program, rCUDA now allows the use of remote GPUs within a cluster with low overhead, so that a single application running in one node can use all GPUs available across the cluster, thereby extending the single-node capability of CUDA.<br/> &copy; 2014 John Wiley & Sons, Ltd.},
key = {Program processors},
keywords = {Application programs;Energy utilization;Graphics processing unit;Middleware;Virtual reality;Virtualization;},
note = {Automatic transformations;clusters;Communication architectures;CUDA;GPGPU;Gpu virtualization;High performance computing;Multi-threaded application;},
URL = {http://dx.doi.org/10.1002/cpe.3409},
} 


@inproceedings{20183005607034,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Beyond precision and recall: Understanding uses (and Misuses) of similarity hashes in binary analysis},
journal = {CODASPY 2018 - Proceedings of the 8th ACM Conference on Data and Application Security and Privacy},
author = {Pagani, Fabio and DellAmico, Matteo and Balzarotti, Davide},
volume = {2018-January},
year = {2018},
pages = {354 - 365},
address = {Tempe, AZ, United states},
abstract = {Fuzzy hashing algorithms provide a convenient way of summarizing in a compact form the content of files, and of looking for similarities between them. Because of this, they are widely used in the security and forensics communities to look for similarities between binary program files; one version of them, ssdeep, is the de facto standard to share information about known malware. Fuzzy hashes are quite pervasive, but no study so far answers conclusively the question of which (if any) fuzzy hashing algorithms are suited to detect similarities between programs, where we consider as similar those programs that have code or libraries in common. We measure how four popular algorithms perform in different scenarios: when they are used to correlate statically-compiled files with the libraries they use, when compiled with different flags or different compilers, and when applied to programs that share a large part of their source code. Perhaps more importantly, we provide interpretations that explain the reasons why results vary, sometimes widely, among apparently very similar use cases. We find that the low-level details of the compilation process, together with the technicalities of the hashing algorithms, can explain surprising results such as similarities dropping to zero with the change of a single assembly instruction. More in general, we see that ssdeep, the de facto standard for this type of analysis, performs definitely worse than alternative algorithms; we also find that the best choice of algorithm to use varies depending on the particularities of the use case scenario.<br/> &copy; 2018 Copyright held by the owner/author(s).},
key = {Program compilers},
keywords = {Computer crime;Data privacy;Libraries;Malware;},
note = {Alternative algorithms;Approximate matching;Assembly instructions;Binary analysis;Compilation process;Fuzzy hash;Hashing algorithms;Precision and recall;},
URL = {http://dx.doi.org/10.1145/3176258.3176306},
} 


@article{20185006231506,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Heterogeneous Face Recognition Using Domain Specific Units},
journal = {IEEE Transactions on Information Forensics and Security},
author = {Pereira, Tiago de Freitas and Anjos, Andre and Marcel, Sebastien},
year = {2018},
issn = {15566013},
abstract = {The task of Heterogeneous Face Recognition consists in matching face images that are sensed in different domains, such as sketches to photographs (visual spectra images), thermal images to photographs or near-infrared images to photographs. In this work we suggest that high level features of Deep Convolutional Neural Networks trained on visual spectra images are potentially domain independent and can be used to encode faces sensed in different image domains. A generic framework for Heterogeneous Face Recognition is proposed by adapting Deep Convolutional Neural Networks low level features in, so called, &amp;#x201C;Domain Specific Units&amp;#x201D;. The adaptation using Domain Specific Units allow the learning of shallow feature detectors specific for each new image domain. Furthermore, it handles its transformation to a generic face space shared between all image domains. Experiments carried out with four different face databases covering three different image domains show substantial improvements, in terms of recognition rate, surpassing the state-of-the-art for most of them. This work is made reproducible: all the source code, scores and trained models of this approach are made publicly available.<br/> OAPA},
key = {Face recognition},
keywords = {Convolution;Deep neural networks;Feature extraction;Image enhancement;Infrared devices;Infrared imaging;Neural networks;Photography;},
note = {Deep convolutional neural networks;Domain adaptation;Domain independents;Heterogeneous face recognition;High-level features;Low-level features;Near- infrared images;Reproducible research;},
URL = {http://dx.doi.org/10.1109/TIFS.2018.2885284},
} 


@inproceedings{20161202123801,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Sensor fault identification based on Error-Correcting Output Codes method},
journal = {Proceedings of the 2015 7th IEEE International Conference on Cybernetics and Intelligent Systems, CIS 2015 and Robotics, Automation and Mechatronics, RAM 2015},
author = {Zhou, Rui and Chen, Jie and Deng, Fang},
year = {2015},
pages = {131 - 136},
address = {Siem Reap, Cambodia},
abstract = {In this paper, we proposed a method within the framework of Error-Correcting Output Codes (ECOC) to solve sensor fault feature extraction and online identification problem. Time and frequency domain signal features are selected as the initial fault characteristics, and we enhance the separability of initial characteristics by a nonlinear transformation based on the probabilistic confidence of the first ECOC outputs. Then construct the second ECOC to classify the fault state taking the feature extracted from the first ECOC as the input. We compared the accuracy and efficiency with this method between different ECOC coding strategies. The results indicate that some certain coding combination show better results on efficiency and accuracy, being able to meet the demand of sensor online fault identification. And generalization experiment on UCI Machine Learning Repository show that this method can also be promoted to other problems.<sup>1</sup><br/> &copy; 2015 IEEE.},
key = {Codes (symbols)},
keywords = {Efficiency;Frequency domain analysis;Intelligent systems;Learning systems;Mathematical transformations;Natural language processing systems;Robotics;},
note = {Coding strategy;Error correcting output code;Fault identifications;Initial faults;Non-linear transformations;On-line identification;Time and frequency domains;UCI machine learning repository;},
URL = {http://dx.doi.org/10.1109/ICCIS.2015.7274561},
} 


@inproceedings{20140217177326,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Towards indicators of instabilities in software product lines: An empirical evaluation of metrics},
journal = {International Workshop on Emerging Trends in Software Metrics, WETSoM},
author = {Cafeo, Bruno B. P. and Dantas, Francisco and Cirilo, Elder J. R. and Garcia, Alessandro},
year = {2013},
pages = {69 - 75},
issn = {23270950},
address = {San Francisco, CA, United states},
abstract = {A Software Product Line (SPL) is a set of software systems (products) that share common functionalities, so-called features. The success of a SPL design is largely dependent on its stability; otherwise, a single implementation change will cause ripple effects in several products. Therefore, there is a growing concern in identifying means to either indicate or predict design instabilities in the SPL source code. However, existing studies up to now rely on conventional metrics as indicators of SPL instability. These conventional metrics, typically used in standalone systems, are not able to capture the properties of SPL features in the source code, which in turn might neglect frequent causes of SPL instabilities. On the other hand, there is a small set of emerging software metrics that take into account specific properties of SPL features. The problem is that there is a lack of empirical validation of the effectiveness of metrics in indicating quality attributes in the context of SPLs. This paper presents an empirical investigation through two set of metrics regarding their power of indicating instabilities in evolving SPLs. A set of conventional metrics was confronted with a set of metrics we instantiated to capture important properties of SPLs. The software evolution history of two SPLs were analysed in our studies. These SPLs are implemented using two different programming techniques and all together they encompass 30 different versions under analysis. Our analysis confirmed that conventional metrics are not good indicators of instabilities in the context of evolving SPLs. The set of employed feature dependency metrics presented a high correlation with instabilities proving its value as indicator of SPL instabilities. &copy; 2013 IEEE.<br/>},
key = {Product design},
keywords = {Computer software;Convergence of numerical methods;Software design;},
note = {Empirical evaluations;Empirical investigation;Experimentation;Feature Dependency;Implementation changes;Metrics;Software Product Line;Software product line (SPL);},
URL = {http://dx.doi.org/10.1109/WETSoM.2013.6619339},
} 


@inproceedings{20151800813955,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Evolution analysis for Accessibility Excessiveness in Java},
journal = {2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
author = {Kobori, Kazuo and Matsushita, Makoto and Inoue, Katsuro},
year = {2015},
pages = {83 - 90},
address = {Montreal, QC, Canada},
abstract = {In Java programs, access modifiers are used to control the accessibility of fields and methods from other objects. Choosing appropriate access modifiers is one of the key factors to improve program quality and to reduce potential vulnerability. In our previous work, we presented a static analysis method named Accessibility Excessiveness (AE) detection for each field and method in Java program. We have also developed an AE analysis tool named ModiChecker that analyzes each field and method of the input Java programs, and reports their excessiveness. In this paper, we have applied ModiChecker to several OSS repositories to investigate the evolution of AE over versions, and identified transition of AE status and the difference in the amount of AE change between major version releases and minor ones. Also we propose when to evaluate source code with AE analysis.<br/> &copy; 2015 IEEE.},
key = {Java programming language},
keywords = {Acoustic emission testing;Computer software;Static analysis;},
note = {Access modifiers;AE analysis;Evolution analysis;Java program;Program quality;Source codes;Static analysis method;},
URL = {http://dx.doi.org/10.1109/SANER.2015.7081818},
} 


@article{20150900567122,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Residential energy efficiency standards in Australia: where to next?},
journal = {Energy Efficiency},
author = {Berry, Stephen and Marker, Tony},
volume = {8},
number = {5},
year = {2015},
pages = {963 - 974},
issn = {1570646X},
abstract = {Increasing the energy and carbon efficiency of homes has been at the forefront of international climate change mitigation policy. In Australia, recent policy action led to the introduction of minimum energy efficiency standards for new homes within the Building Code of Australia in 2003, with subsequent stringency increases in 2006 and 2010. Although not yet reflecting international best regulatory practice, these standards represent substantial progress in addressing the energy and carbon emission impact of new homes, yet there are a number of energy policy challenges that highlight the need for further change. This paper documents the history of house energy standards in Australia and examines the post-occupancy evidence of that policy outcome. The paper examines international and domestic issues pointing to a possible future direction for Australian house energy regulation, highlighting the key drivers for change. In particular, we investigate the concepts of net zero carbon and net zero energy homes which have recently been adopted internationally, examining the technical and economic evidence that would support such a policy position in Australia.<br/> &copy; 2015, Springer Science+Business Media Dordrecht.},
key = {Energy efficiency},
keywords = {Carbon;Climate change;Emission control;Energy policy;Houses;},
note = {Building Code of Australia;Carbon efficiency;Climate change mitigation;Energy regulation;Performance-based regulation;Residential energy efficiency;Zero carbon homes;Zero energy homes;},
URL = {http://dx.doi.org/10.1007/s12053-015-9336-4},
} 


@inproceedings{20133016526644,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {CU2rCU: Towards the complete rCUDA remote GPU virtualization and sharing solution},
journal = {2012 19th International Conference on High Performance Computing, HiPC 2012},
author = {Reano, C. and Pena, A.J. and Silla, F. and Duato, J. and Mayo, R. and Quintana-Orti, E.S.},
year = {2012},
pages = {ACM; CSIR; et al.; IEEE Comput. Soc. Tech. Comm. Parallel Process. (TCPP); IISc; IISER - },
address = {Pune, India},
abstract = {GPUs are being increasingly embraced by the high performance computing and computational communities as an effective way of considerably reducing execution time by accelerating significant parts of their application codes. However, despite their extraordinary computing capabilities, the adoption of GPUs in current HPC clusters may present certain negative side-effects. In particular, to ease job scheduling in these platforms, a GPU is usually attached to every node of the cluster. In addition to increasing acquisition costs this favors that GPUs may frequently remain idle, as applications usually do not fully utilize them. On the other hand, idle GPUs consume non-negligible amounts of energy, which translates into very poor energy efficiency during idle cycles. rCUDA was recently developed as a software solution to address these concerns. Specifically, it is a middleware that allows transparently sharing a reduced number of GPUs among the nodes in a cluster. rCUDA thus increases the GPU-utilization rate, taking care of job scheduling. While the initial prototype versions of rCUDA demonstrated its functionality, they also revealed several concerns related with usability and performance. With respect to usability, in this paper we present a new component of the rCUDA suite that allows an automatic transformation of any CUDA source code, so that it can be effectively accommodated within this technology. In response to performance, we briefly show some interesting results, which will be deeply analyzed in future publications. The net outcome is a new version of rCUDA that allows, for any CUDA-compatible program, to use remote GPUs in a cluster with minimum overhead. &copy; 2012 IEEE.<br/>},
key = {Program processors},
keywords = {Energy efficiency;Graphics processing unit;Middleware;Scheduling;Virtualization;},
note = {Acquisition costs;Application codes;Automatic transformations;Computing capability;Gpu virtualization;High performance computing;Negative side effects;Prototype versions;},
URL = {http://dx.doi.org/10.1109/HiPC.2012.6507485},
} 


@inproceedings{20144200110491,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Debsources: Live and historical views on macro-level software evolution},
journal = {International Symposium on Empirical Software Engineering and Measurement},
author = {Caneill, Matthieu and Zacchiroli, Stefano},
year = {2014},
pages = {IEEE Software; Microsoft Research; Politecnico di Torino; Telecom Italia JOL (Joint Open Lab); Telecom Italia Lab - },
issn = {19493770},
address = {Torino, Italy},
abstract = {Context. Software evolution has been an active field of research in recent years, but studies on macro-level software evolution-i.e., on the evolution of large software collections over many years-are scarce, despite the increasing popularity of intermediate vendors as a way to deliver software to final users. Goal. We want to ease the study of both day-by-day and long-term Free and Open Source Software (FOSS) evolution trends at the macro-level, focusing on the Debian distribution as a proxy of relevant FOSS projects. Method. We have built Debsources, a software platform to gather, search, and publish on the Web all the source code of Debian and measures about it. We have set up a public Debsources instance at http://sources.debian.net, integrated it into the Debian infrastructure to receive live updates of new package releases, and written plugins to compute popular source code metrics. We have injected all current and historical Debian releases into it. Results. The obtained dataset and Web portal provide both long term-views over the past 20 years of FOSS evolution and live insights on what is happening at sub-day granularity. By writing simple plugins (~100 lines of Python each) and adding them to our Debsources instance we have been able to easily replicate and extend past empirical analyses on metrics as diverse as lines of code, number of packages, and rate of change-and make them perennial. We have obtained slightly different results than our reference study, but confirmed the general trends and updated them in light of 7 extra years of evolution history. Conclusions. Debsources is a flexible platform to monitor large FOSS collections over long periods of time. Its main instance and dataset are valuable resources for scholars interested in macro-level software evolution.<br/> &copy; 2014 ACM.},
key = {Open source software},
keywords = {Codes (symbols);Computer programming languages;Macros;Open systems;Portals;},
note = {debian;Free software;Open sources;Software Evolution;Source codes;},
URL = {http://dx.doi.org/10.1145/2652524.2652528},
} 


@article{20181705055490,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A Distributed Classifier for MicroRNA Target Prediction with Validation Through TCGA Expression Data},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
author = {Ghoshal, Asish and Zhang, Jinyi and Roth, Michael A. and Xia, Kevin Muyuan and Grama, Ananth Y. and Chaterji, Somali},
volume = {15},
number = {4},
year = {2018},
pages = {1037 - 1051},
issn = {15455963},
abstract = {Background: MicroRNAs (miRNAs) are approximately 22-nucleotide long regulatory RNA that mediate RNA interference by binding to cognate mRNA target regions. Here, we present a distributed kernel SVM-based binary classification scheme to predict miRNA targets. It captures the spatial profile of miRNA-mRNA interactions via smooth B-spline curves. This is accomplished separately for various input features, such as thermodynamic and sequence-based features. Further, we use a principled approach to uniformly model both canonical and non-canonical seed matches, using a novel seed enrichment metric. Finally, we verify our miRNA-mRNA pairings using an Elastic Net-based regression model on TCGA expression data for four cancer types to estimate the miRNAs that together regulate any given mRNA. Results: We present a suite of algorithms for miRNA target prediction, under the banner Avishkar, with superior prediction performance over the competition. Specifically, our final kernel SVM model, with an Apache Spark backend, achieves an average true positive rate (TPR) of more than 75 percent, when keeping the false positive rate of 20 percent, for non-canonical human miRNA target sites. This is an improvement of over 150 percent in the TPR for non-canonical sites, over the best-in-class algorithm. We are able to achieve such superior performance by representing the thermodynamic and sequence profiles of miRNA-mRNA interaction as curves, devising a novel seed enrichment metric, and learning an ensemble of miRNA family-specific kernel SVM classifiers. We provide an easy-to-use system for large-scale interactive analysis and prediction of miRNA targets. All operations in our system, namely candidate set generation, feature generation and transformation, training, prediction, and computing performance metrics are fully distributed and are scalable. Conclusions: We have developed an efficient SVM-based model for miRNA target prediction using recent CLIP-seq data, demonstrating superior performance, evaluated using ROC curves for different species (human or mouse), or different target types (canonical or non-canonical). We analyzed the agreement between the target pairings using CLIP-seq data and using expression data from four cancer types. To the best of our knowledge, we provide the first distributed framework for miRNA target prediction based on Apache Hadoop and Spark. Availability: All source code and sample data are publicly available at https://bitbucket.org/cellsandmachines/avishkar. Our scalable implementation of kernel SVM using Apache Spark, which can be used to solve large-scale non-linear binary classification problems, is available at https://bitbucket.org/cellsandmachines/kernelsvmspark.<br/> &copy; 2004-2012 IEEE.},
key = {RNA},
keywords = {Curve fitting;Nucleic acids;Regression analysis;},
note = {Binary classification;Expression data;Microrna target predictions;Regression model;Regulatory rnas;RNA interference;Sequence based features;Spatial profiles;},
URL = {http://dx.doi.org/10.1109/TCBB.2018.2828305},
} 


@article{20173304049115,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {TouchTerrain: A simple web-tool for creating 3D-printable topographic models},
journal = {Computers and Geosciences},
author = {Hasiuk, Franciszek J. and Harding, Chris and Renner, Alex Raymond and Winer, Eliot},
volume = {109},
year = {2017},
pages = {25 - 31},
issn = {00983004},
abstract = {An open-source web-application, TouchTerrain, was developed to simplify the production of 3D-printable terrain models. Direct Digital Manufacturing (DDM) using 3D Printers can change how geoscientists, students, and stakeholders interact with 3D data, with the potential to improve geoscience communication and environmental literacy. No other manufacturing technology can convert digital data into tangible objects quickly at relatively low cost; however, the expertise necessary to produce a 3D-printed terrain model can be a substantial burden: knowledge of geographical information systems, computer aided design (CAD) software, and 3D printers may all be required. Furthermore, printing models larger than the build volume of a 3D printer can pose further technical hurdles. The TouchTerrain web-application simplifies DDM for elevation data by generating digital 3D models customized for a specific 3D printer's capabilities. The only required user input is the selection of a region-of-interest using the provided web-application with a Google Maps-style interface. Publically available digital elevation data is processed via the Google Earth Engine API. To allow the manufacture of 3D terrain models larger than a 3D printer's build volume the selected area can be split into multiple tiles without third-party software. This application significantly reduces the time and effort required for a non-expert like an educator to obtain 3D terrain models for use in class. The web application is deployed at http://touchterrain.geol.iastate.edu, while source code and installation instructions for a server and a stand-alone version are available at Github: https://github.com/ChHarding/TouchTerrain_for_CAGEO.<br/> &copy; 2017 Elsevier Ltd},
key = {Three dimensional computer graphics},
keywords = {3D printers;Computer aided design;Data handling;Engineering education;Engines;HTTP;Image segmentation;Landforms;Open source software;Printing presses;Topography;},
note = {3-D printing;Digital manufacturing;Google earths;Python;Terrain;},
URL = {http://dx.doi.org/10.1016/j.cageo.2017.07.005},
} 


@inproceedings{20150400457054,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Integrating security, analytics and application management into the mobile development lifecycle},
journal = {MobileDeLi 2014 - Proceedings of the 2nd International Workshop on Mobile Development Lifecycle, Part of SPLASH 2014},
author = {Pistoia, Marco and Tripp, Omer},
year = {2014},
pages = {17 - 18},
address = {Portland, OR, United states},
abstract = {The advent of mobile devices has revolutionized many aspects of the software lifecycle. Unlike Web applications, which delegate most of the business logic to the server and use the client side for the presentation logic, mobile apps are client intensive. Another crucial difference is that the client side of Web applications is typically written using a combination of platform-independentWeb languages, whereas most mobile apps have native clients written in platform-specific languages. Though nativity hinders the portability of mobile apps across different platforms and even different devices inside the same platform, it enables smooth high-fidelity experience, high performance and compliance with the platform's UI style-requirements that can only be satisfied natively. The main cost, beyond the initial coding of a mobile app, is to maintain its different variants in the presence of updates. Indeed, mobile code is typically updated frequently, with bug fixes and new features integrated into each new version. In the enterprise setting, these new features often revolve around security, analytics and Mobile App Management (MAM). This paper presents Enceladus, an app-level instrumentation framework that addresses these high-maintenance costs by transparently enriching any mobile enterprise app with new analytics, security andMAMcapabilities not otherwise present in the original app source code. With Enceladus, the mobile app lifecycle is significantly reduced because the instrumentation is visually configurable, and any change to the instrumentation policy can be pushed transparently without requiring a full app update.<br/> Copyright &copy; 2014 ACM.},
key = {Application programs},
keywords = {Codes (symbols);Life cycle;Mobile computing;Mobile devices;Mobile security;},
note = {Application management;Integrating security;Mobile analytics;Mobile applications;Mobile enterprise;Mobile softwares;Software life cycles;Specific languages;},
URL = {http://dx.doi.org/10.1145/2688412.2688419},
} 


@inproceedings{20184706115793,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ACM International Conference Proceeding Series},
journal = {ACM International Conference Proceeding Series},
year = {2018},
pages = {University of Oulu - },
address = {Oulu, Finland},
abstract = {The proceedings contain 11 papers. The topics discussed include: accurately predicting the location of code fragments in programming video tutorials using deep learning; a public unified bug dataset for Java; an empirical study of metric-based comparisons of software libraries; cross-version defect prediction using cross-project defect prediction approaches: does it work?; how effectively is defective code actually tested? an analysis of JUnit tests in seven open source systems; using Bayesian networks to estimate strategic indicators in the context of rapid software development; modeling relationship between post-release faults and usage in mobile software; are software dependency supply chain metrics useful in predicting change of popularity of NPM packages?; and mining communication patterns in software development: a GitHub analysis.<br/>},
} 


@inproceedings{20140117153686,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {The impact of requirements on software quality across three product generations},
journal = {2013 21st IEEE International Requirements Engineering Conference, RE 2013 - Proceedings},
author = {Terzakis, John},
year = {2013},
pages = {284 - 289},
address = {Rio de Janeiro, Brazil},
abstract = {In a previous case study, we presented data demonstrating the impact that a well-written and well-reviewed set of requirements had on software defects and other quality indicators between two generations of an Intel product. The first generation was coded from an unorganized collection of requirements that were reviewed infrequently and informally. In contrast, the second was developed based on a set of requirements stored in a Requirements Management database and formally reviewed at each revision. Quality indicators for the second software product all improved dramatically even with the increased complexity of the newer product. This paper will recap that study and then present data from a subsequent Intel case study revealing that quality enhancements continued on the third generation of the product. The third generation software was designed and coded using the final set of requirements from the second version as a starting point. Key product differentiators included changes to operate with a new Intel processor, the introduction of new hardware platforms and the addition of approximately fifty new features. Software development methodologies were nearly identical, with only the change to a continuous build process for source code check-in added. Despite the enhanced functionality and complexity in the third generation software, requirements defects, software defects, software sightings, feature commit vs. delivery (feature variance), defect closure efficiency rates, and number of days from project commit to customer release all improved from the second to the third generation of the software. &copy; 2013 IEEE.<br/>},
key = {Software design},
keywords = {Computer software selection and evaluation;Defects;Requirements engineering;Reviews;},
note = {Quality enhancement;Quality indicators;Requirements management;Requirements specifications;Software defects;Software development methodologies;Software products;Software Quality;},
URL = {http://dx.doi.org/10.1109/RE.2013.6636731},
} 


@inproceedings{20133716718412,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Language model rest costs and space-efficient storage},
journal = {EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference},
author = {Heafield, Kenneth and Koehn, Philipp and Lavie, Alon},
year = {2012},
pages = {1169 - 1178},
address = {Jeju Island, Korea, Republic of},
abstract = {Approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments. We contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant. Common practice uses lower-order entries in an N-gram model to score the first few words of a fragment; this violates assumptions made by common smoothing strategies, including Kneser-Ney. Instead, we use a unigram model to score the first word, a bigram for the second, etc. This improves search at the expense of memory. Conversely, we show how to save memory by collapsing probability and backoff into a single value without changing sentence-level scores, at the expense of less accurate estimates for sentence fragments. These changes can be stacked, achieving better estimates with unchanged memory usage. In order to interpret changes in search accuracy, we adjust the pop limit so that accuracy is unchanged and report the change in CPU time. In a German-English Moses system with target-side syntax, improved estimates yielded a 63% reduction in CPU time; for a Hiero-style version, the reduction is 21%. The compressed language model uses 26% less RAM while equivalent search quality takes 27% more CPU. Source code is released as part of KenLM. &copy; 2012 Association for Computational Linguistics.<br/>},
key = {Natural language processing systems},
keywords = {Computational linguistics;Computer aided language translation;Random access storage;Syntactics;},
note = {Common practices;Machine translations;N-gram modeling;Search accuracy;Search Algorithms;Sentence fragments;Space efficient;Unigram models;},
} 


@inproceedings{20144900299137,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Upgrading your Android, elevating my malware: Privilege escalation through mobile OS updating},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
author = {Xing, Luyi and Pan, Xiaorui and Wang, Rui and Yuan, Kan and Wang, Xiao Feng},
year = {2014},
pages = {393 - 408},
issn = {10816011},
address = {San Jose, CA, United states},
abstract = {Android is a fast evolving system, with new updates coming out one after another. These updates often completely overhaul a running system, replacing and adding tens of thousands of files across Android's complex architecture, in the presence of critical user data and applications (apps for short). To avoid accidental damages to such data and existing apps, the upgrade process involves complicated program logic, whose security implications, however, are less known. In this paper, we report the first systematic study on the Android updating mechanism, focusing on its Package Management Service (PMS). Our research brought to light a new type of security-critical vulnerabilities, called Pileup flaws, through which a malicious app can strategically declare a set of privileges and attributes on a low-version operating system (OS) and wait until it is upgraded to escalate its privileges on the new system. Specifically, we found that by exploiting the Pileup vulnerabilities, the app can not only acquire a set of newly added system and signature permissions but also determine their settings (e.g., protection levels), and it can further substitute for new system apps, contaminate their data (e.g., cache, cookies of Android default browser) to steal sensitive user information or change security configurations, and prevent installation of critical system services. We systematically analyzed the source code of PMS using a program verification tool and confirmed the presence of those security flaws on all Android official versions and over 3000 customized versions. Our research also identified hundreds of exploit opportunities the adversary can leverage over thousands of devices across different device manufacturers, carriers and countries. To mitigate this threat without endangering user data and apps during an upgrade, we also developed a new detection service, called SecUP, which deploys a scanner on the user's device to capture the malicious apps designed to exploit Pileup vulnerabilities, based upon the vulnerability-related information automatically collected from newly released Android OS images.<br/> &copy; 2014 IEEE.},
key = {Mobile security},
keywords = {Android (operating system);Malware;Security of data;},
note = {Accidental damage;Android;Complex architectures;Package managements;Privilege escalations;Program Verification;Security configurations;Security implications;},
URL = {http://dx.doi.org/10.1109/SP.2014.32},
} 


@inproceedings{20142217759344,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Application of Group Method of Data Handling model for software maintainability prediction using object oriented systems},
journal = {International Journal of Systems Assurance Engineering and Management},
author = {Malhotra, Ruchika and Chug, Anuradha},
volume = {5},
number = {2},
year = {2014},
pages = {165 - 173},
issn = {09756809},
abstract = {Object-oriented methodology has emerged as most prominent in software industry for application development. Maintenance phase begins once the product is delivered and by software maintainability we mean the ease with which existing software could be modified during maintenance phase. We can improve and control software maintainability if we can predict it in the early phases of software life cycle using design metrics. Predicting the maintainability of any software has become critical with the increasing importance of software maintenance. Many authors have practiced and proved theoretical validation followed by empirical evaluation using statistical and experimental techniques for evaluating the relevance of any given metrics suite using many models. In this paper, we have presented an empirical study to evaluate the effectiveness of novel technique called Group Method of Data Handling (GMDH) for the prediction of maintainability over other models. Although many metrics have been proposed in the literature, software design metrics suite proposed by Chidamber et al. and revised by Li et al. have been selected for this study. Two web-based customized softwares developed using C# Language have been used for empirical study. Source code of old and new versions for both applications were collected and analysed against modifications made in every class. The changes were counted in terms of number of lines added, deleted or modified in the classes belonging to new version with respect to the classes of old version. Finally values of metrics were combined with "change" in order to generate data points. Hence, in this study an attempt has been made to evaluate and examine the effectiveness of prediction models for the purpose of software maintainability using real life web based projects. Three models using Feed Forward 3-Layer Back Propagation Network (FF3LBPN), General Regression Neural Network (GRNN) and GMDH are developed and performance of GMDH is compared against two others i.e. FF3LBPN and GRNN. With the aid of this empirical analysis, we can safely suggest that software professionals can use OO metric suite to predict the maintainability of software using GMDH technique with least error and best precision in an object oriented paradigm. &copy; 2014 The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.<br/>},
key = {Object oriented programming},
keywords = {Application programs;Backpropagation;Computer software maintenance;Data handling;Experimental reactors;Forecasting;Life cycle;Maintainability;Neural networks;Software design;Websites;},
note = {Backpropagation network;Empirical validation;General regression neural network;Group method of data handling;Software maintainability;},
URL = {http://dx.doi.org/10.1007/s13198-014-0227-4},
} 


@inproceedings{20183605776109,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {How to Evaluate Software Architectures: Tutorial on Practical Insights on Architecture Evaluation Projects with Industrial Customers},
journal = {Proceedings - 2018 IEEE 15th International Conference on Software Architecture Companion, ICSA-C 2018},
author = {Naab, Matthias and Rost, Dominik},
year = {2018},
pages = {6 - 7},
address = {Seattle, WA, United states},
abstract = {Thorough and continuous architecting is the key to overall success in software engineering, and architecture evaluation is a crucial part of it. This tutorial presents a pragmatic architecture evaluation approach and insights gained from its application in more than 75 projects with industrial customers in the past decade. It presents context factors, empirical data, and example cases, as well as lessons learned on mitigating the risk of change through architecture evaluation. By providing comprehensive answers to many typical questions and discussing more frequent mistakes and lessons learned, the tutorial allows the audience to not only learn how to conduct architecture evaluations and interpret its results, but also to become aware of risks such as false conclusions, manipulating data, and unsound lines of argument. It equips the audience to become confident in assessing quantitative measurement results and recognize when it is better to rely on qualitative expertise. The target audience includes both practitioners and researchers. By demonstrating its impact and providing clear guidelines, data, and examples, it encourages practitioners to conduct architecture evaluations. At the same time, it offers researchers insights into industrial architecture evaluations, which serve as the basis for guiding research in this area and will inspire future research directions. Both groups will get an overview of the foundations and history of architecture evaluation. The tutorial covers the following important aspects of architecture evaluation &bull; Architecture drivers: types of drivers, importance of drivers, elicitation of drivers, documentation of drivers as architecture scenarios &bull; Solution Adequacy Check: check whether an architecture is adequate for its drivers, what are risks, assumptions, tradeoffs (based on ATAM (architecture tradeoff analysis method) and enhanced with other techniques for increasing confidence of results) &bull; Documentation Quality Check: How adequate is the documentation of an architecture to be understandable and to serve its purposes? &bull; Architecture Compliance Check: How to check whether an intended architecture is consistently reflected in the source code? For all aspects, pragmatic methodical support is provided in the tutorial and all checks are well integrated in the overall architecture evaluation method.<br/> &copy; 2018 IEEE.},
key = {Quality control},
keywords = {C (programming language);Compliance control;Industrial research;Petroleum reservoir evaluation;Risk assessment;Risk perception;Sales;Software architecture;Software engineering;},
note = {Architecture compliance;Architecture evaluation;Architecture tradeoff analysis methods;Future research directions;Industrial customer;practical insights;Quantitative measurement;tutorial;},
URL = {http://dx.doi.org/10.1109/ICSA-C.2018.00008},
} 


@article{20143900074188,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Intelligent GPGPU classification in volume visualization: A framework based on error-correcting output codes},
journal = {Computer Graphics Forum},
author = {Escalera, S. and Puig, A. and Amoros, O. and Salamo, M.},
volume = {30},
number = {7},
year = {2014},
pages = {2107 - 2115},
issn = {01677055},
abstract = {In volume visualization, the definition of the regions of interest is inherently an iterative trial-and-error process finding out the best parameters to classify and render the final image. Generally, the user requires a lot of expertise to analyze and edit these parameters through multi-dimensional transfer functions. In this paper, we present a framework of intelligent methods to label on-demand multiple regions of interest. These methods can be split into a two-level GPU-based labelling algorithm that computes in time of rendering a set of labelled structures using the Machine Learning Error-Correcting Output Codes (ECOC) framework. In a pre-processing step, ECOC trains a set of Adaboost binary classifiers from a reduced pre-labelled data set. Then, at the testing stage, each classifier is independently applied on the features of a set of unlabelled samples and combined to perform multi-class labelling. We also propose an alternative representation of these classifiers that allows to highly parallelize the testing stage. To exploit that parallelism we implemented the testing stage in GPU-OpenCL. The empirical results on different data sets for several volume structures shows high computational performance and classification accuracy.<br/> &copy; 2011 The Author(s).},
key = {Classification (of information)},
keywords = {Adaptive boosting;Errors;Graphics processing unit;Iterative methods;Learning systems;Program processors;Rendering (computer graphics);Visualization;},
note = {Classification accuracy;Computational performance;Error correcting output code;Iterative trial and error;Multi dimensional transfer functions;Pre-processing step;Regions of interest;Volume visualization;},
URL = {http://dx.doi.org/10.1111/j.1467-8659.2011.02043.x},
} 


@article{20121314893657,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ES-MPICH2: A message passing interface with enhanced security},
journal = {IEEE Transactions on Dependable and Secure Computing},
author = {Ruan, Xiaojun and Yang, Qing and Alghamdi, Mohammed I. and Yin, Shu and Qin, Xiao},
volume = {9},
number = {3},
year = {2012},
pages = {361 - 374},
issn = {15455971},
abstract = {An increasing number of commodity clusters are connected to each other by public networks, which have become a potential threat to security sensitive parallel applications running on the clusters. To address this security issue, we developed a Message Passing Interface (MPI) implementation to preserve confidentiality of messages communicated among nodes of clusters in an unsecured network. We focus on MPI rather than other protocols, because MPI is one of the most popular communication protocols for parallel computing on clusters. Our MPI implementation - called ES-MPICH2 - was built based on MPICH2 developed by the Argonne National Laboratory. Like MPICH2, ES-MPICH2 aims at supporting a large variety of computation and communication platforms like commodity clusters and high-speed networks. We integrated encryption and decryption algorithms into the MPICH2 library with the standard MPI interface and; thus, data confidentiality of MPI applications can be readily preserved without a need to change the source codes of the MPI applications. MPI-application programmers can fully configure any confidentiality services in MPICHI2, because a secured configuration file in ES-MPICH2 offers the programmers flexibility in choosing any cryptographic schemes and keys seamlessly incorporated in ES-MPICH2. We used the Sandia Micro Benchmark and Intel MPI Benchmark suites to evaluate and compare the performance of ES-MPICH2 with the original MPICH2 version. Our experiments show that overhead incurred by the confidentiality services in ES-MPICH2 is marginal for small messages. The security overhead in ES-MPICH2 becomes more pronounced with larger messages. Our results also show that security overhead can be significantly reduced in ES-MPICH2 by high-performance clusters. The executable binaries and source code of the ES-MPICH2 implementation are freely available at http:// www.eng.auburn.edu/&sim;xqin/ software/es-mpich2/. &copy; 2006 IEEE.<br/>},
key = {Message passing},
keywords = {Benchmarking;Computer programming;Cryptography;HIgh speed networks;Interfaces (computer);Network security;Parallel processing systems;Security of data;},
note = {Argonne National Laboratory;Communication platforms;Cryptographic schemes;Data confidentiality;Encryption and decryption;High performance cluster;Message passing interface;Parallel application;},
URL = {http://dx.doi.org/10.1109/TDSC.2012.9},
} 


@inproceedings{20133116558626,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Improving scenario selection for simulations by run-time control-flow analysis},
journal = {Simulation Series},
author = {Berger, Christian},
volume = {45},
number = {11},
year = {2013},
pages = {94 - 101},
issn = {07359276},
address = {Toronto, ON, Canada},
abstract = {Cyber-physical systems like active safety systems in recent vehicles are significantly driven by software and rely predominantly on data that is perceived by cameras, laser scanners, and the like from the system's environment. For example, these sensor-based systems realize pedestrian protection functionalities, which cannot be tested under simplified conditions on proving grounds only or by arbitrary test-runs on public roads anymore. Instead, simulative environments are used nowadays, which provide the virtual surroundings for such a system where its real input sources are replaced with simplified sensor models. Thus, interactive and hazard-free system tests and automated system evaluations can be carried out easily. However, the simple strategy to run all available modeled traffic scenarios in the simulation on any change of the implementation would consume too much computation time to provide effective and fast feedback for developers. In this article, an improved strategy for selecting scenarios that shall be run in a simulation based on run-time control-flow analysis is proposed, which resulted from the in-depth analysis of the revision history of the source code and their accompanying simulations for two self-driving vehicles. The outlined strategy is evaluated on a self-driving miniature vehicle.<br/>},
key = {Automobile safety devices},
keywords = {Active safety systems;Automation;Cyber Physical System;Embedded systems;Laser safety;},
note = {Automated systems;In-depth analysis;Pedestrian protection;Regression simulation;Runtimes;Self-driving cars;Self-driving vehicles;Sensor based systems;},
} 


@inproceedings{20162002400326,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Relationship between features volatility and software architecture design stability in object-oriented software: Preliminary analysis},
journal = {2015 International Conference on Information Technology Systems and Innovation, ICITSI 2015 - Proceedings},
author = {Handani, Felix and Rochimah, Siti},
year = {2015},
pages = {IEEE Indonesia Join Chapter of Education Society/Electron Devices Society/Power Electronics Society/Signal Processing Society; IEEE Indonesia Joint Chapter of Control System Society/Robotics and Automation Society (CSS/RAS) - },
address = {Bandung, Bali, Indonesia},
abstract = {Software architecture is the core structure of a system. Software architecture describes the functionality and the size of system to be built. Software architecture is illustrated as packages diagram, class diagram or Enterprise Architecture diagram. To make a robust software, it's important to know quality of architecture. Architecture Quality is reflected in its design. There are various topics of research on the quality aspect of the architectural design, from enviroment adaption of architectural design to design stability maintenance. The concept of reuse elements of the system is one of the topics to maintain the stability of the software design. Aversano and Constantinou introduce the method of measuring the stability of the architectural design by taking into account external and internal elements of architecture built. Both just look at the number of packets that undergo additions and deletions to the pair versions. Quantitative research to assess an architectural stability by looking at environmental factors needed to complete measurement. Before implementing this factor, it is necessary to measure the relationship between variables the stability and environmental factors. We introduced a quantitative analysis of the mechanisms related to the extent to which the relationship between features volatility and architecture stability. Architecture design stability is measured by metrics Constantinou, and the calculation of features volatility depend on change of features from consecutive version. We applied this analysis into one project. The source code in the repository extracted to be converted into data according to metrics Constantinou, then the results are validated by experts selected. Datasets that have been validated measured by metrics and measurable correlation with Pearson-Product-Moment analysis.<br/> &copy; 2015 IEEE.},
key = {Object oriented programming},
keywords = {Architectural design;Computer architecture;Computer software reusability;Software architecture;Software design;Stability;},
note = {Architecture stabilities;Correlation analysis;Enterprise Architecture;Environmental factors;features volatility;Object oriented software;Product moments;Software architecture design;},
URL = {http://dx.doi.org/10.1109/ICITSI.2015.7437736},
} 


@article{20183805845542,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Bayesian denoising hashing for robust image retrieval},
journal = {Pattern Recognition},
author = {Wang, Dong and Song, Ge and Tan, Xiaoyang},
volume = {86},
year = {2019},
pages = {134 - 142},
issn = {00313203},
abstract = {Learning to hash is one of the most popular techniques in image retrieval, but few work investigates its robustness to noise corrupted images in which the unknown pattern of noise would heavily deteriorate the performance. To deal with this issue, we present in this paper a Bayesian denoising hashing algorithm whose output can be regarded a denoised version of the input hash code. We show that our method essentially seeks to reconstruct a new but more robust hash code by preserving the original input information while imposing extra constraints so as to correct the corrupted bits. We optimized this model in variational Bayes framework which has a closed-form update in each iteration that is more efficient than numerical optimization. Furthermore, our method can be added at the top of any original hashing layer, serving as a post-processing denoising layer with no change to previous training procedure. Experiments on three popular datasets demonstrate that the proposed method yields robust and meaningful hash code, which significantly improves the performance of state-of-the-art hash learning methods on challenging tasks such as large-scale natural image retrieval and retrieval with corrupted images.<br/> &copy; 2018 Elsevier Ltd},
key = {Image retrieval},
keywords = {Codes (symbols);Hash functions;Image enhancement;Iterative methods;Optimization;},
note = {De-noising;Hashing algorithms;Numerical optimizations;Probabilistic modeling;Robustness to noise;State of the art;Training procedures;Variational bayes;},
URL = {http://dx.doi.org/10.1016/j.patcog.2018.09.006},
} 


@inproceedings{20162002394183,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {GREENTHUMB: Superoptimizer construction framework},
journal = {Proceedings of CC 2016: The 25th International Conference on Compiler Construction},
author = {Phothilimthana, Phitchaya Mangpo and Thakur, Aditya and Bodik, Rastislav and Dhurjati, Dinakar},
year = {2016},
pages = {261 - 262},
address = {Barcelona, Spain},
abstract = {Developing an optimizing compiler backend remains a laborious process, especially for nontraditional ISAs that have been appearing recently. Superoptimization sidesteps the need for many code transformations by searching for the most optimal instruction sequence semantically equivalent to the original code fragment. Even though superoptimization discovers the best machine-specific code optimizations, it has yet to become widely-used. We propose GREENTHUMB, an extensible framework that reduces the cost of constructing superoptimizers and provides a fast search algorithm that can be reused for any ISA, exploiting the unique strengths of enumerative, stochastic, and symbolic (SAT-solver-based) search algorithms. To extend GREENTHUMB to a new ISA, it is only necessary to implement an emulator for the ISA and provide some ISA-specific search utility functions.<br/> &copy; 2016 ACM.},
key = {Optimal systems},
keywords = {Codes (symbols);Cosine transforms;Decision theory;Learning algorithms;Program compilers;Stochastic systems;Surface mount technology;},
note = {Code optimization;Code transformation;Extensible framework;Fast search algorithm;Optimizing compilers;Program synthesis;Superoptimization;Utility functions;},
URL = {http://dx.doi.org/10.1145/2892208.2892233},
} 


@inproceedings{20143818165282,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Design considerations of synaptic device for neuromorphic computing},
journal = {Proceedings - IEEE International Symposium on Circuits and Systems},
author = {Yu, Shimeng and Kuzum, Duygu and Wong, H.-S. Philip},
year = {2014},
pages = {1062 - 1065},
issn = {02714310},
address = {Melbourne, VIC, Australia},
abstract = {Hardware implementation of neuromorphic computing is attractive as a computing paradigm beyond the conventional digital Boolean computing. Recently, two-terminal emerging memory devices that show electrically-triggered resistance modulation have been proposed as synaptic devices for neuromorphic computing. The synaptic device candidates include phase change memory (PCM), resistive RAM (RRAM) and conductive bridge RAM (CBRAM), etc. In this paper, we discuss the general design considerations of synaptic devices for plasticity and learning. As a rule of thumb for performance metrics assessment, an ideal synaptic device should have characteristics such as dimension, energy consumption, operation frequency, dynamic range, etc. that are scalable to biological systems with comparable complexity. &copy; 2014 IEEE.<br/>},
key = {Phase change memory},
keywords = {Design;Energy utilization;Hardware;Plasticity;Pulse code modulation;RRAM;},
note = {CBRAM;Design considerations;Hardware implementations;learning;Neuromorphic computing;Phase change memory (pcm);Resistive rams (RRAM);synaptice device;},
URL = {http://dx.doi.org/10.1109/ISCAS.2014.6865322},
} 


@inproceedings{20164102893817,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {XB+-tree: A novel index for PCM/DRAM-Based hybrid memory},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Li, Lu and Jin, Peiquan and Yang, Chengcheng and Wan, Shouhong and Yue, Lihua},
volume = {9877 LNCS},
year = {2016},
pages = {357 - 368},
issn = {03029743},
address = {Sydney, NSW, United states},
abstract = {Phase Change Memory (PCM) has emerged as a new kind of future memories that can be used as an alternative of DRAM. PCM has a number of special properties such as non-volatility, high density, read/write asymmetry, and byte addressability. Specially, PCM has higher write latency than DRAM but has comparable read latency with DRAM. This makes it difficult to directly replace DRAM with PCM in current memory hierarchy. Thus, in this paper, we propose to construct hybrid memory architecture that involves both PCM and DRAM, which is a practical and feasible way to utilize PCM. Such hybrid memory architecture introduces many new issues for database researches, as existing algorithms have to be revised to be suitable for hybrid memory. In this paper, we study the indexing issue on PCM/DRAM-based hybrid memory and propose an improved version of the B+-tree called XB+-tree (eXtended B+- tree). The key idea of the XB+-tree is to detect the read/write tendency of the nodes in the tree index and organize write-intensive nodes on PCM while putting read-intensive nodes on DRAM. We propose a new node management and migration algorithm in the XB+-tree to effectively move nodes between DRAM and PCM. With this mechanism, we can reduce the read and write operations on PCM and improve the overall performance. We conduct trace-driven experiments and compare our proposal with three existing indices including the B+-tree, the OB+-tree (B+-tree with the overflow scheme), and the CB+-tree. The results in terms of PCM read/write count and run time suggest the efficiency of our proposal.<br/> &copy; Springer International Publishing AG 2016.},
key = {Trees (mathematics)},
keywords = {Database systems;Dynamic random access storage;Forestry;Memory architecture;Phase change memory;Pulse code modulation;},
note = {B+-trees;Hybrid memory;Hybrid memory architectures;Index;Migration algorithms;Phase change memory (pcm);Special properties;Trace driven experiments;},
URL = {http://dx.doi.org/10.1007/978-3-319-46922-5_28},
} 


@article{20131816302787,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Nonwovens with thermal storage properties based on paraffin-modified polypropylene fibres},
journal = {Fibres and Textiles in Eastern Europe},
author = {Tomaszewski, Waclaw and Twarowska-Schmidt, Krystyna and Moraczewski, Andrzej and Kudra, Michal and Szadkowski, Marek and Palys, Beata},
volume = {96},
number = {6 B},
year = {2012},
pages = {64 - 69},
issn = {12303666},
abstract = {A series of nonwovens with various mechanical and thermal properties was prepared by a textile technique based on melt-spun continuous PP fibres modified with paraffin as a phase change material (PCM). The PCM is not encapsulated; it forms a structure like "islands in the sea" in the PP fibres. This permits the addition of a larger amount of the active substance to the fibre than in the encapsulated version. The nonwovens made of such fibres retained high thermal resistance. Paraffin was added to the PP fibre in amounts of 10 - 30 wt%; 20 wt% appeared to be best in terms of thermal properties and processability. To prevent fibre to fibre sticking at elevated temperatures, the nonwovens were made of a blend of paraffin-modified and standard PP fibres in variable proportions. The thermal activity determined by the kind of paraffin used was estimated to be in the range of 30 - 60 &deg;C. The fibre heat accumulation capacity stemming from the phase transition in the PCM was in the range of 3.6 - 19.4 kJ/m<sup>2</sup>, at a thermal regulation factor (TRF) from 0.8 to 0.4.<br/>},
key = {Fibers},
keywords = {Heat storage;Melt spinning;Nonwoven fabrics;Paraffins;Phase change materials;Polypropylenes;Pulse code modulation;Thermodynamic properties;Weaving;},
note = {Elevated temperature;Heat accumulation;Mechanical and thermal properties;Non-wovens;Phase Change Material (PCM);Polypropylene fibres;Thermal regulation;TRF coefficient;},
} 


@article{20181104894014,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Scaling modified condition/decision coverage using distributed concolic testing for Java programs},
journal = {Computer Standards and Interfaces},
author = {Godboley, Sangharatna and Dutta, Arpita and Mohapatra, Durga Prasad and Mall, Rajib},
volume = {59},
year = {2018},
pages = {61 - 86},
issn = {09205489},
abstract = {Object-Oriented languages such as Java language introduce advantageous features which overcome the demerits of procedural languages to some extent. Therefore, Java language is now going to be used by the industries to develop their critical safety system software products. In this paper, we propose some code transformation methodologies, which are implemented in Java language to test Java written code. We apply Java Distributed Concolic testing technique to improve the code coverage, which is more powerful than non-distributed concolic testing in terms of speed of test case generation. We develop a Java coverage analyzer according to the test cases produced by Java distributed concolic testers. This version of the MC/DC analyzer is more powerful than that of procedural languages. Our core idea is to integrate the existing and developed modules to produce a single tool for measuring MC/DC score. This novel idea automates the flow of testing 100%. Our experimental results present different scenarios, and suggest the stronger one. On an average, for forty-five Java programs using three nodes in the client&ndash;server architecture, we achieved higher MC/DC score.<br/> &copy; 2018 Elsevier B.V.},
key = {Java programming language},
keywords = {Accident prevention;Client server computer systems;Codes (symbols);Computer software;Cosine transforms;Object oriented programming;Testing;},
note = {Client-server architectures;Code transformation;Concolic testing;Java language;Modified condition/decision coverages;Procedural languages;System softwares;Test case generation;},
URL = {http://dx.doi.org/10.1016/j.csi.2018.02.005},
} 


@inproceedings{20173304043368,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Type-Safe Evolution of Web Services},
journal = {Proceedings - 2017 IEEE/ACM 2nd International Workshop on Variability and Complexity in Software Design, VACE 2017},
author = {Campinhos, Joao and Seco, Joao Costa and Cunha, Jacome},
year = {2017},
pages = {20 - 26},
address = {Buenos Aires, Argentina},
abstract = {Applications based on micro or web services have had significant growth due to the exponential increase in the use of mobile devices. However, using such kind of loosely coupled interfaces provides almost no guarantees to the developer in terms of evolution. Changes to service interfaces can be introduced at any moment, which may cause the system to fail due to mismatches between communicating parts. In this paper, we present a programming model that allows the development of web service applications, server end-points and their clients, in such a way that the evolution of services' implementation does not cause the disruption of the client. Our approach is based on a type based code slicing technique that ensures that each version only refers to type compatible code, of the same version or of a compatible version, and that each client request is redirected to the most recent type compatible version implemented by the server. We abstract the notion of version and parametrize type compatibility on the relation between versions. The relation between versions is tagged with compatibility levels, so to capture the common conventions used in software development. Our implementation allows multiple versions of a service to be deployed simultaneously, while reusing code between versions in a type safe way. We describe a prototype framework, based on code transformation, for server-side JavaScript code, and using Flow as verification tool.<br/> &copy; 2017 IEEE.},
key = {Web services},
keywords = {Codes (symbols);Cosine transforms;High level languages;Software design;Websites;},
note = {API evolution;Code transformation;Compatibility levels;Exponential increase;Javascript;Service interfaces;type safe;Web service applications;},
URL = {http://dx.doi.org/10.1109/VACE.2017.6},
} 


@article{20154301437565,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An Optimal Page-Level Power Management Strategy in PCMDRAM Hybrid Memory},
journal = {International Journal of Parallel Programming},
author = {Zhang, Jinbao and Liao, Xiaofei and Jin, Hai and Liu, Dong and Lin, Li and Zhao, Kao},
volume = {45},
number = {1},
year = {2017},
pages = {4 - 16},
issn = {08857458},
abstract = {The new emergence of data-intensive applications raises a huge requirement on the capacity of memory. However, an obvious increase of memory makes the corresponding energy consumption unacceptable in practice. To address this question, any effort only to reduce the energy consumption of dynamic random access memory (DRAM) is ineffective. Recently, combining DRAM and phase change memory (PCM) to construct a hybrid main memory is recognized as a promising solution to distinctly reduce the energy consumption. In this paper, we propose a new page-level energy management strategy to optimize the energy consumption of the hybrid main memory. The proposed strategy records pages&rsquo; local and global access information by a new data structure, and then classifies pages by the access history, at last adaptively places PCM or DRAM pages according to the memory characteristics and remaps the migrated pages. Our experimental results show that our strategy can achieve 9.4 % of energy saving and 9.6 % of performance improvement at most compared with APG and PDRAM, which were proposed respectively by conferences RACS&rsquo;12 and DAC&rsquo;09.<br/> &copy; 2015, Springer Science+Business Media New York.},
key = {Dynamic random access storage},
keywords = {Energy conservation;Energy utilization;Phase change memory;Power management;Pulse code modulation;},
note = {Data-intensive application;Dynamic random access memory;Energy management strategies;Hybrid main memory;Hybrid memory;Performance improvements;Phase change memory (pcm);Power management strategies;},
URL = {http://dx.doi.org/10.1007/s10766-015-0382-5},
} 


@article{20131916311144,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Climate model genealogy: Generation CMIP5 and how we got there},
journal = {Geophysical Research Letters},
author = {Knutti, Reto and Masson, David and Gettelman, Andrew},
volume = {40},
number = {6},
year = {2013},
pages = {1194 - 1199},
issn = {00948276},
abstract = {A new ensemble of climate models is becoming available and provides the basis for climate change projections. Here, we show a first analysis indicating that the models in the new ensemble agree better with observations than those in older ones and that the poorest models have been eliminated. Most models are strongly tied to their predecessors, and some also exchange ideas and code with other models, thus supporting an earlier hypothesis that the models in the new ensemble are neither independent of each other nor independent of the earlier generation. On the basis of one atmosphere model, we show how statistical methods can identify similarities between model versions and complement process understanding in characterizing how and why a model has changed. We argue that the interdependence of models complicates the interpretation of multimodel ensembles but largely goes unnoticed. &copy;2013 American Geophysical Union. All Rights Reserved.<br/>},
key = {Climate models},
keywords = {Climate change;History;},
note = {Atmosphere modeling;Climate change projections;independence;Multi-model;Multi-model ensemble;Process understanding;},
URL = {http://dx.doi.org/10.1002/grl.50256},
} 


@inproceedings{20164703036621,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Statistical deobfuscation of Android applications},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
author = {Bichsel, Benjamin and Raychev, Veselin and Tsankov, Petar and Vechev, Martin},
volume = {24-28-October-2016},
year = {2016},
pages = {343 - 355},
issn = {15437221},
address = {Vienna, Austria},
abstract = {This work presents a new approach for deobfuscating Android APKs based on probabilistic learning of large code bases (termed "Big Code"). The key idea is to learn a probabilistic model over thousands of non-obfuscated Android applications and to use this probabilistic model to deobfuscate new, unseen Android APKs. The concrete focus of the paper is on reversing layout obfuscation, a popular transformation which renames key program elements such as classes, packages and methods, thus making it difficult to understand what the program does. Concretely, the paper: (i) phrases the layout deobfuscation problem of Android APKs as structured prediction in a probabilistic graphical model, (ii) instantiates this model with a rich set of features and constraints that capture the Android setting, ensuring both semantic equivalence and high prediction accuracy, and (iii) shows how to leverage powerful inference and learning algorithms to achieve overall precision and scalability of the probabilistic predictions. We implemented our approach in a tool called DeGuard and used it to: (i) reverse the layout obfuscation performed by the popular ProGuard system on benign, open-source applications, (ii) predict third-party libraries imported by benign APKs (also obfuscated by ProGuard), and (iii) rename obfuscated program elements of Android malware. The experimental results indicate that DeGuard is practically effective: it recovers 79.1% of the program element names obfuscated with ProGuard, it predicts third-party libraries with accuracy of 91.3%, and it reveals string decoders and classes that handle sensitive data in Android malware.<br/> &copy; 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
key = {Android (operating system)},
keywords = {Application programs;Computer crime;Concretes;Forecasting;Inference engines;Learning algorithms;Libraries;Malware;Open systems;Semantics;},
note = {Android applications;Open source application;Probabilistic graphical models;Probabilistic Learning;Probabilistic modeling;Probabilistic prediction;Semantic equivalences;Structured prediction;},
URL = {http://dx.doi.org/10.1145/2976749.2978422},
} 


@inproceedings{20182805538948,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Improved airfoil polar predictions with data-driven boundary-layer closure relations},
journal = {Journal of Physics: Conference Series},
author = {De Oliveira, Gael and Pereira, Ricardo and Timmer, Nando and Van Rooij, Ruud},
volume = {1037},
number = {2},
year = {2018},
issn = {17426588},
address = {Milan, Italy},
abstract = {The accuracy of airfoil polar predictions is limited by the usage of imperfect turbulence models. Can machine-learning improve this situation? Will airfoil polars teach the effect of turbulence on skin-friction? We try to answer these questions by refining turbulence treatment in the Rfoil code: boundary layer closure relations are learned from airfoil polar data. Two turbulent closure relations, for skin friction and energy shape factor, are parametrized with a class-shape transformation. An experimental database is then used to define code inaccuracy measures that are minimized with an interior point gradient algorithm. Results show that airfoil polars contain exploitable information about turbulent phenomena. Inferred closures agree with direct numerical simulation results of skin friction and the new code predicts drag more accurately. Maximum lift remains under-predicted but Rfoil maintains its robustness and suitability for optimization of wind energy airfoils.<br/> &copy; Published under licence by IOP Publishing Ltd.},
key = {Atmospheric thermodynamics},
keywords = {Airfoils;Boundary layers;Codes (symbols);Friction;Learning systems;Skin friction;Torque;Turbulence models;Wind power;},
note = {Data driven;Experimental database;Gradient algorithm;Interior point;Shape factor;Shape transformation;Turbulent closures;},
URL = {http://dx.doi.org/10.1088/1742-6596/1037/2/022009},
} 


@article{20182405298788,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {A Survey on Cross-Project Software Defect Prediction Methods},
journal = {Jisuanji Xuebao/Chinese Journal of Computers},
author = {Chen, Xiang and Wang, Li-Ping and Gu, Qing and Wang, Zan and Ni, Chao and Liu, Wang-Shu and Wang, Qiu-Ping},
volume = {41},
number = {1},
year = {2018},
pages = {254 - 274},
issn = {02544164},
abstract = {Software defect prediction firstly analyzes and mines software historical repositories to extract program modules and label them. It secondly designs novel metrics, which have strong correlation with defects, based on the analysis on code complexity or development process. Then it uses these metrics to measure these program modules. It finally uses a specific machine learning algorithm to construct software defect prediction models, which are trained on these datasets. Therefore software defect prediction can optimize the software testing resource allocation by identifying the potential defect modules in advance. However in real software development, a project, which needs defect prediction, maybe a new project or it maybe has less training data. A simple solution is directly using training data from other projects to construct the model. However application domain, development process, used programming language, developer experience of different projects may be not same. This will cause the distribution of corresponding datasets to be large and result in the poor performance of defect prediction. Therefore, how to effectively transfer the knowledge of the source project to build a defect prediction model for the target project has attracted the attention of researchers, and this problem is called cross-project defect prediction (CPDP).We conduct a comprehensive survey on this topic and classify existing methods into three categories: supervised learning based methods, unsupervised learning based methods, and semi-supervised learning based methods. In particular, the supervised learning based methods will use the modules of the source project to construct the model. These methods can be further classified into two categories: homogeneous cross-project defect learning and heterogeneous cross-project defect prediction based on whether the source project and the target project use the same metric set. For the former, researchers design novel methods by using metric value transformation, instance selection and weight setting, feature mapping and selection, ensemble learning, class imbalance learning. For the latter, the issue is more challenging and researchers design novel methods by using feature mapping and canonical correlation analysis. The unsupervised learning based methods will attempt to make a prediction on the modules of the target project immediately. The assumption of these methods is that the metric value of defective modules has the tendency to be higher than the metric value of non-defective modules. Researchers design novel methods by using cluster algorithms. The semi-supervised learning based methods will use the modules of the source project and some labeled programs in the target project together to construct the model. These methods try to improve the performance of CPDP by identifying some representative program modules in the target project and label them manually. Researchers design novel methods by using ensemble learning and TrAdaBoost. We summarize and comment the existing research work for each category in sequence. Then we analyze the commonly used performance metrics and benchmarks in empirical studies in CPDP for other researchers to better design empirical studies. Finally we conclude this paper and discuss some potentially future research work from four dimensions: dataset gathering, dataset preprocessing, CPDP model construction and evaluation, and CPDP model application.<br/> &copy; 2018, Science Press. All right reserved.},
key = {Software design},
keywords = {Benchmarking;Defects;Engineering research;Forecasting;Learning algorithms;Mapping;Software testing;Supervised learning;Surveys;Unsupervised learning;},
note = {Defect prediction;Empirical Software Engineering;Empirical studies;Software defect prediction;Transfer learning;},
URL = {http://dx.doi.org/10.11897/SP.J.1016.2018.00254},
} 


@inproceedings{20183805815514,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Android apps and user feedback: A dataset for software evolution and quality improvement},
journal = {WAMA 2017 - Proceedings of the 2nd ACM SIGSOFT International Workshop on App Market Analytics, Co-located with FSE 2017},
author = {Grano, Giovanni and Di Sorbo, Andrea and Mercaldo, Francesco and Visaggio, Corrado A. and Canfora, Gerardo and Panichella, Sebastiano},
year = {2017},
pages = {8 - 11},
address = {Paderborn, Germany},
abstract = {Nowadays, Android represents the most popular mobile platform with a market share of around 80%. Previous research showed that data contained in user reviews and code change history of mobile apps represent a rich source of information for reducing software maintenance and development effort, increasing customers' satisfaction. Stemming from this observation, we present in this paper a large dataset of Android applications belonging to 23 different apps categories, which provides an overview of the types of feedback users report on the apps and documents the evolution of the related code metrics. The dataset contains about 395 applications of the F-Droid repository, including around 600 versions, 280,000 user reviews and more than 450,000 user feedback (extracted with specific text mining approaches). Furthermore, for each app version in our dataset, we employed the Paprika tool and developed several Python scripts to detect 8 different code smells and compute 22 code quality indicators. The paper discusses the potential usefulness of the dataset for future research in the field.<br/> &copy; 2017 - Proceedings of the 2nd ACM SIGSOFT International Workshop on App Market Analytics, Co-located with FSE 2017. All rights reserved.},
key = {Android (operating system)},
keywords = {Application programs;Codes (symbols);Commerce;Competition;Computer software maintenance;Computer software selection and evaluation;Customer satisfaction;Data mining;Natural language processing systems;},
note = {Android applications;Customers' satisfaction;Mobile applications;Mobile platform;Quality improvement;Software Evolution;Software maintenance and evolution;Software Quality;},
URL = {http://dx.doi.org/10.1145/3121264.3121266},
} 


@inproceedings{20164302939096,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Predicting Defectiveness of Software Patches},
journal = {International Symposium on Empirical Software Engineering and Measurement},
author = {Soltanifar, Behjat and Erdem, Atakan and Bener, Ayse},
volume = {08-09-September-2016},
year = {2016},
pages = {ACM Special Interest Group on Software Engineering (SIGSOFT); IEEE CS - },
issn = {19493770},
address = {Ciudad Real, Spain},
abstract = {Context: Software code review, as an engineering best practice, refers to the inspection of the code change in order to find possible defects and ensure change quality. Code reviews, however, may not guarantee finding the defects. Thus, there is a risk for a defective code change in a given patch, to pass the review process and be submitted. Goal: In this research, we aim to apply different machine learning algorithms in order to predict the defectiveness of a patch after being reviewed, at the time of its submission. Method: We built three models using three different machine learning algorithms: Logistic Regression, Nave Bayes, and Bayesian Network model. To build the models, we consider different factors involved in review process in terms of Product, Process and People (3P). Results: Our empirical results show that, Bayesian Networks is able to better predict the defectiveness of the changed code with 76% accuracy. Conclusions: Predicting defectiveness of change code is beneficial in making patch release decisions. The Bayesian Network model outperforms the others since it capturs the relationship among the factors in the review process.<br/> &copy; 2016 ACM.},
key = {Learning algorithms},
keywords = {Artificial intelligence;Bayesian networks;Codes (symbols);Defects;Forecasting;Learning systems;Software engineering;},
note = {Bayesian network models;Code review;Defect prediction;Engineering best practice;Logistic regressions;Review process;Software codes;Software patches;},
URL = {http://dx.doi.org/10.1145/2961111.2962601},
} 


@inproceedings{20134817031955,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Refactoring with synthesis},
journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
author = {Raychev, Veselin and Schafer, Max and Sridharan, Manu and Vechev, Martin},
year = {2013},
pages = {339 - 354},
address = {Indianapolis, IN, United states},
abstract = {Refactoring has become an integral part of modern software development, with wide support in popular integrated development environments (IDEs). Modern IDEs provide a fixed set of supported refactorings, listed in a refactoring menu. But with IDEs supporting more and more refactorings, it is becoming increasingly difficult for programmers to discover and memorize all their names and meanings. Also, since the set of refactorings is hard-coded, if a programmer wants to achieve a slightly different code transformation, she has to either apply a (possibly non-obvious) sequence of several built-in refactorings, or just perform the transformation by hand. We propose a novel approach to refactoring, based on synthesis from examples, which addresses these limitations. With our system, the programmer need not worry how to invoke individual refactorings or the order in which to apply them. Instead, a transformation is achieved via three simple steps: The programmer first indicates the start of a code refactoring phase; then she performs some of the desired code changes manually; and finally, she asks the tool to complete the refactoring. Our system completes the refactoring by first extracting the difference between the starting program and the modified version, and then synthesizing a sequence of refactorings that achieves (at least) the desired changes. To enable scalable synthesis, we introduce local refactorings, which allow for first discovering a refactoring sequence on small program fragments and then extrapolating it to a full refactoring sequence. We implemented our approach as an Eclipse plug-in, with an architecture that is easily extendable with new refactorings. The experimental results are encouraging: with only minimal user input, the synthesizer was able to quickly discover complex refactoring sequences for several challenging realistic examples.<br/>},
key = {Object oriented programming},
keywords = {Codes (symbols);Computer systems programming;Cosine transforms;Software design;Synthesis (chemical);},
note = {Code changes;Code re-factoring;Code transformation;Integral part;Integrated development environment;Program fragments;Refactorings;Scalable synthesis;},
URL = {http://dx.doi.org/10.1145/2509136.2509544},
} 


@article{20134917058585,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Refactoring with synthesis},
journal = {ACM SIGPLAN Notices},
author = {Raychev, Veselin and Schafer, Max and Vechev, Martin and Sridharan, Manu},
volume = {48},
number = {10},
year = {2013},
pages = {339 - 354},
issn = {15232867},
abstract = {Refactoring has become an integral part of modern software development, with wide support in popular integrated development environments (IDEs). Modern IDEs provide a fixed set of supported refactorings, listed in a refactoring menu. But with IDEs supporting more and more refactorings, it is becoming increasingly difficult for programmers to discover and memorize all their names and meanings. Also, since the set of refactorings is hard-coded, if a programmer wants to achieve a slightly different code transformation, she has to either apply a (possibly non-obvious) sequence of several built-in refactorings, or just perform the transformation by hand. We propose a novel synthesis system which addresses these limitations. Our system employs a recently proposed refactoring interface, in which a refactoring is achieved via three simple steps: the programmer first indicates the start of a code refactoring phase; then she performs some of the desired code changes manually; and finally, she asks the tool to complete the refactoring. Given the initial and modified programs, our synthesis system completes the refactoring by first extracting the difference between the starting program and the modified version, and then synthesizing a sequence of refactorings that achieves (at least) the desired changes. To enable scalable synthesis, we introduce local refactorings, which allow for first discovering a refactoring sequence on small program fragments and then extrapolating it to a full refactoring sequence. Copyright &copy; 2013. Copyright &copy; 2013 ACM.<br/>},
key = {Software design},
keywords = {Codes (symbols);Cosine transforms;Synthesis (chemical);},
note = {Code changes;Code re-factoring;Code transformation;Integral part;Integrated development environment;Program fragments;Refactorings;Scalable synthesis;},
} 


@inproceedings{20184706087451,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Generating reusable web components from mockups},
journal = {ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
author = {Bajammal, Mohammad and Mazinanian, Davood and Mesbah, Ali},
year = {2018},
pages = {601 - 611},
address = {Montpellier, France},
abstract = {The transformation of a user interface mockup designed by a graphic designer to web components in the final app built by a web developer is often laborious, involving manual and time consuming steps. We propose an approach to automate this aspect of web development by generating reusable web components from a mockup. Our approach employs visual analysis of the mockup, and unsupervised learning of visual cues to create reusable web components (e.g., React components). We evaluated our approach, implemented in a tool called VizMod, on five real-world web mockups, and assessed the transformations and generated components through comparison with web development experts. The results show that VizMod achieves on average 94% precision and 75% recall in terms of agreement with the developers' assessment. Furthermore, the refactorings yielded 22% code reusability, on average.<br/> &copy; 2018 Association for Computing Machinery.},
key = {Machine components},
keywords = {Computer vision;Learning systems;Mockups;Reusability;Software engineering;User interfaces;},
note = {Code reusability;Graphic designers;Real world web;Refactorings;Visual analysis;Web components;Web development;Web UI;},
URL = {http://dx.doi.org/10.1145/3238147.3238194},
} 


@inproceedings{20124615674262,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Transforming SQLITE to run on a bare PC},
journal = {ICSOFT 2012 - Proceedings of the 7th International Conference on Software Paradigm Trends},
author = {Okafor, Uzo and Karne, Ramesh K. and Wijesinha, Alexander L. and Rawal, Bharat S.},
year = {2012},
pages = {311 - 314},
address = {Rome, Italy},
abstract = {SQLITE is a popular small open-source database management system with many versions that run on popular platforms. However, there is currently no version of the SQLITE application that runs on a bare PC. Since a bare PC does not provide any form of operating system (or kernel) support, bare PC applications need to be completely self-contained with their own interfaces to the hardware. Such applications are characterized by small code size, and have inherent security and performance advantages due to the absence of a conventional operating system. We describe a general transformation approach that can be used to transform the SQLITE application to a lean SQLITE application that runs on a bare PC. We present the current state of this work and identify several important issues that need further research.<br/>},
key = {Open systems},
keywords = {Cosine transforms;Database systems;Open source software;},
note = {Bare machine computing;Bare pc;Code size;Code transformation;Open source database management system;Popular platform;Security and performance;SQLITE;},
} 


@inproceedings{20180804811911,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Fast feature extraction method for faults detection system},
journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
author = {Wang, Hongmin and Zhu, Xiaohui and Niu, Xiangyong and Xue, Ping},
volume = {219},
year = {2018},
pages = {297 - 306},
issn = {18678211},
address = {Harbin, China},
abstract = {The feature extraction based on machine learning is significant in the detection system. The boundary information, the circumference and the area are the essential features in the identification and the classification of flaws. In order to get those information, this paper proposed a novel algorithm to get the boundary information using the boundary tracking, and to make each flaw independent by establishing a balanced binary search tree for data storage. By scanning the image and the image boundaries based on binarization transformation, there is no need to fill the region, nor need to use the chain code to count the number of regions and the boundary information. According to the established balanced binary search tree, we can calculate the number of the pixel of the area of each fault, the edge information of the boundary, and the circumference. The algorithm has the advantages of fast speed, less computation, better noise suppression and accurate results.<br/> &copy; ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018.},
key = {Feature extraction},
keywords = {Binary trees;Chains;Classification (of information);Codes (symbols);Digital storage;Extraction;Forestry;Image processing;Learning systems;Trees (mathematics);},
note = {Balanced binary searches;Boundary information;Boundary tracking;Detection system;Essential features;Feature extraction methods;Freeman chain code;Noise suppression;},
URL = {http://dx.doi.org/10.1007/978-3-319-73317-3_35},
} 


@article{20153301159390,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {BinComp: A stratified approach to compiler provenance Attribution},
journal = {Digital Investigation},
author = {Rahimian, Ashkan and Shirani, Paria and Alrbaee, Saed and Wang, Lingyu and Debbabi, Mourad},
volume = {14},
number = {S1},
year = {2015},
pages = {S146 - S155},
issn = {17422876},
abstract = {Abstract Compiler provenance encompasses numerous pieces of information, such as the compiler family, compiler version, optimization level, and compiler-related functions. The extraction of such information is imperative for various binary analysis applications, such as function fingerprinting, clone detection, and authorship attribution. It is thus important to develop an efficient and automated approach for extracting compiler provenance. In this study, we present BinComp, a practical approach which, analyzes the syntax, structure, and semantics of disassembled functions to extract compiler provenance. BinComp has a stratified architecture with three layers. The first layer applies a supervised compilation process to a set of known programs to model the default code transformation of compilers. The second layer employs an intersection process that disassembles functions across compiled binaries to extract statistical features (e.g., numerical values) from common compiler/linker-inserted functions. This layer labels the compiler-related functions. The third layer extracts semantic features from the labeled compiler-related functions to identify the compiler version and the optimization level. Our experimental results demonstrate that BinComp is efficient in terms of both computational resources and time.<br/> &copy; 2015 The Authors.},
key = {Program compilers},
keywords = {Cosine transforms;Digital forensics;Reverse engineering;Semantics;},
note = {Authorship attribution;Binary programs;Code transformation;Compilation process;Compiler provenance;Computational resources;Optimization levels;Statistical features;},
URL = {http://dx.doi.org/10.1016/j.diin.2015.05.015},
} 


@inproceedings{20162902617642,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Using qualitative data analysis to measure user experience in a serious game for premed students},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Zielke, Marjorie A. and Zakhidov, Djakhangir and Jacob, Daniel and Lenox, Sean},
volume = {9740},
year = {2016},
pages = {92 - 103},
issn = {03029743},
address = {Toronto, ON, Canada},
abstract = {The University of Texas Transformation in Medical Education Portal (UT TIME Portal) is a game-based learning platform for select premed students, with a particular emphasis on communication and professionalism. In addition to quantitative data on system usage and user performance, the UT TIME Portal generates rich sets of qualitative data collected through discussion board posts and pre- and post- surveys. Using NVivo 10&rsquo;s built-in tools, our team used this qualitative data to measure game experience outcomes in many ways by building and testing out hypotheses about our user experience design. The ability to tag, code and organize themes to then be analyzed in the context of quantitative data generated by the UT TIME Portal adds an important dimension to understanding the user experience and generates insights not possible to glean from quantitative data alone.<br/> &copy; Springer International Publishing Switzerland 2016.},
key = {Serious games},
keywords = {E-learning;Human computer interaction;Intelligent agents;Medical education;Mixed reality;Online systems;Students;},
note = {Adaptive learning;Asynchronous access;Asynchronous practice;Discussion boards;Education and training;Emergent learning;Nvivo;Online learning;Qualitative analysis;},
URL = {http://dx.doi.org/10.1007/978-3-319-39907-2_9},
} 


@inproceedings{20183205661175,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Assessing technical debt in automated tests with codescene},
journal = {Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2018},
author = {Tornhill, Adam},
year = {2018},
pages = {122 - 125},
address = {Vasteras, Sweden},
abstract = {Test automation promises several advantages such as shorter lead times, higher code quality, and an executable documentation of the system's behavior. However, test automation won't deliver on those promises unless the quality of the automated test code itself is maintained, and to manually inspect the evolution of thousands of tests that change on a daily basis is impractical at best. This paper investigates how CodeScene - a tool for predictive analyses and visualizations - could be used to identify technical debt in automated test code. CodeScene combines repository mining, static code analysis, and machine learning to prioritize potential code improvements based on the most likely return on investment.<br/> &copy; 2018 IEEE.},
key = {Software testing},
keywords = {Automation;Codes (symbols);Learning systems;Testing;Verification;},
note = {Code improvement;Code quality;Executable documentation;Repository mining;Return on investments;Static code analysis;Technical debts;Test Automation;},
URL = {http://dx.doi.org/10.1109/ICSTW.2018.00039},
} 


@article{20150200404785,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Neural acceleration for generalpurpose approximate programs},
journal = {Communications of the ACM},
author = {Esmaeilzadeh, Hadi and Sampson, Adrian and Ceze, Luis and Burger, Doug},
volume = {58},
number = {1},
year = {2015},
pages = {105 - 115},
issn = {00010782},
abstract = {As improvements in per-transistor speed and energy efficiency diminish, radical departures from conventional approaches are needed to continue improvements in the performance and energy efficiency of general-purpose processors. One such departure is approximate computing, where error in computation is acceptable and the traditional robust digital abstraction of near-perfect accuracy is relaxed. Conventional techniques in energy-efficient computing navigate a design space defined by the two dimensions of performance and energy, and traditionally trade one for the other. General-purpose approximate computing explores a third dimension-error-and trades the accuracy of computation for gains in both energy and performance. Techniques to harvest large savings from small errors have proven elusive. This paper describes a new approach that uses machine learning-based transformations to accelerate approximation-tolerant programs. The core idea is to train a learning model how an approximable region of code-code that can produce imprecise but acceptable results-behaves and replace the original code region with an efficient computation of the learned model. We use neural networks to learn code behavior and approximate it. We describe the Parrot algorithmic transformation, which leverages a simple programmer annotation ("approximable") to transform a code region from a von Neumann model to a neural model. After the learning phase, the compiler replaces the original code with an invocation of a low-power accelerator called a neural processing unit (NPU). The NPU is tightly coupled to the processor pipeline to permit profitable acceleration even when small regions of code are transformed. Offloading approximable code regions to NPUs is faster and more energy efficient than executing the original code. For a set of diverse applications, NPU acceleration provides whole-application speedup of 2.3&times; and energy savings of 3.0&times; on average with average quality loss of at most 9.6%. NPUs form a new class of accelerators and show that significant gains in both performance and efficiency are achievable when the traditional abstraction of near-perfect accuracy is relaxed in general-purpose computing.<br/> &copy; 2015 ACM.},
key = {Energy efficiency},
keywords = {Abstracting;Acceleration;Codes (symbols);Commerce;Energy conservation;Errors;General purpose computers;Learning systems;Pipeline processing systems;},
note = {Algorithmic transformation;Approximate computing;Conventional approach;Conventional techniques;Efficient computation;Energy efficient computing;General purpose processors;General-purpose computing;},
URL = {http://dx.doi.org/10.1145/2589750},
} 


@article{20133616694605,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Automatic optimization of stream programs via source program operator graph transformations},
journal = {Distributed and Parallel Databases},
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
volume = {31},
number = {4},
year = {2013},
pages = {543 - 599},
issn = {09268782},
abstract = {Distributed data stream processing is a data analysis paradigm where massive amounts of data produced by various sources are analyzed online within real-time constraints. Execution performance of a stream program/query executed on such middleware is largely dependent on the ability of the programmer to fine tune the program to match the topology of the stream processing system. However, manual fine tuning of a stream program is a very difficult, error prone process that demands huge amounts of programmer time and expertise which are expensive to obtain. We describe an automated process for stream program performance optimization that uses semantic preserving automatic code transformation to improve stream processing job performance. We first identify the structure of the input program and represent the program structure in a Directed Acyclic Graph. We transform the graph using the concepts of Tri-OP Transformation and Bi-Op Transformation. The resulting sample program space is pruned using both empirical as well as profiling information to obtain a ranked list of sample programs which have higher performance compared to their parent program. We successfully implemented this methodology on a prototype stream program performance optimization mechanism called Hirundo. The mechanism has been developed for optimizing SPADE programs which run on System S stream processing run-time. Using five real world applications (called VWAP, CDR, Twitter, Apnoea, and Bargain) we show the effectiveness of our approach. Hirundo was able to identify a 31.1 times higher performance version of the CDR application within seven minutes time on a cluster of 4 nodes. &copy; 2013 Springer Science+Business Media New York.<br/>},
key = {Data handling},
keywords = {Automation;Clock and data recovery circuits (CDR circuits);Cosine transforms;Directed graphs;Graph theory;Metadata;Middleware;Semantics;},
note = {Automatic tuning;Code transformation;Data-intensive computing;Performance optimizations;Stream processing;},
URL = {http://dx.doi.org/10.1007/s10619-013-7130-x},
} 


@inproceedings{20140817357258,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Numerical simulation of an U-channel component tailored tempering process considering the effect of stress on the shift of TTT curves},
journal = {Applied Mechanics and Materials},
author = {Tang, B.T. and Wang, Q.L. and Bruschi, S.},
volume = {496-500},
year = {2014},
pages = {425 - 428},
issn = {16609336},
address = {Hong Kong, China},
abstract = {A numerical model of the tailored hot stamping process was developed in the framework of the commercial FE code Forge&trade; and accurately calibrated in order to take into account the influence of applied stress and strain on the phase transformation kinetics. The calibration was carried out by introducing in the numerical model data on the shift of the TTT curves due to applied stress and the transformation plasticity coefficients, which were obtained through an extensive dilatometric analysis. The numerical model was validated through a laboratory-scale hot-formed U-channel produced using a segmented die with local heating and cooling zones. The predicted distribution of Vickers hardness and evolution of microstructure given by the numerical model was compared with the experimental results to show the significant predictive improvements introduced by considering the influence of the transformation plasticity and deformation history on the phase transformation kinetics. &copy; (2014) Trans Tech Publications, Switzerland.},
key = {Numerical models},
keywords = {Design;Manufacture;Microstructural evolution;Phase transitions;},
note = {Deformation history;Dilatometric analysis;Hot stamping process;Mechanical behavior;Phase transformation kinetics;Tailored tempering;Transformation Plasticity;Transformation plasticity coefficient;},
URL = {http://dx.doi.org/10.4028/www.scientific.net/AMM.496-500.425},
} 


@inproceedings{20164603010818,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings - 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN-W 2016},
journal = {Proceedings - 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN-W 2016},
year = {2016},
pages = {IEEE Computer Society; IFIP - },
address = {Toulouse, France},
abstract = {The proceedings contain 60 papers. The topics discussed include: use of similarity measure to suggest the existence of duplicate user stories in the Srum process; code change history and software vulnerabilities; D-MBTDD: an approach for reusing test artefacts in evolving system; comparing detection capabilities of antivirus products: an empirical study with different versions of products from the same vendors; an application of unsupervised fraud detection to passenger name records; MimeoDroid: large scale dynamic app analysis on cloned devices via machine learning classifiers; error monitoring for legacy mission-critical systems; classifying virtual machine managers by overhead; hunting killer tasks for cloud system through behavior pattern learning; ground control to major faults: towards a fault tolerant and adaptive sdn control network; experience with 3 SDN controllers in an enterprise setting; NetCo: reliable routing with unreliable routers; an architecture for semi-automatic collaborative malware analysis for CIs; quantification and analysis of interdependency in cyber-physical systems; and on the feasibility of distinguishing between process disturbances and intrusions in process control systems using multivariate statistical process control.},
} 


@article{20172403788295,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An investigation of the fault-proneness of clone evolutionary patterns},
journal = {Software Quality Journal},
author = {Barbour, Liliane and An, Le and Khomh, Foutse and Zou, Ying and Wang, Shaohua},
volume = {26},
number = {4},
year = {2018},
pages = {1187 - 1222},
issn = {09639314},
abstract = {Two identical or similar code fragments form a clone pair. Previous studies have identified cloning as a risky practice. Therefore, a developer needs to be aware of any clone pairs in order to properly propagate any changes between clones. A clone pair may experience many changes during the creation and maintenance of a software system. A change can either maintain or remove the similarity between clones in a clone pair. If a change maintains the similarity between clones, the clone pair is left in a consistent state. When a change makes the clones no longer similar, the clone pair is left in an inconsistent state. The set of states and changes experienced by clone pairs over time form an evolution history known as a clone genealogy. In this paper, we examine clone genealogies to identify fault-prone &ldquo;patterns&rdquo; of states and changes. We explore the use of clone genealogy information in fault prediction. We conduct a quasi-experiment with four long-lived software systems (i.e., Apache Ant, ArgoUML, JEdit, Maven) and identify clones using the NiCad and iClones clone detection tools. Overall, we find that the size of the clone can impact the fault-proneness of a clone pair. However, there is no clear impact of the time interval between changes to a clone pair on the fault-proneness of the clone pair. We also discover that adding clone genealogy information can increase the explanatory power of fault prediction models.<br/> &copy; 2017, Springer Science+Business Media New York.},
key = {Cloning},
keywords = {Computer software;History;},
note = {Evolution history;Explanatory power;Fault prediction;Fault prediction models;Fault proneness;Metrics;Quasi-experiments;Software systems;},
URL = {http://dx.doi.org/10.1007/s11219-017-9375-5},
} 


@article{20163002639245,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Robust visual tracking via sparse representation under subclass discriminant constraint},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
author = {Qian, Cheng and Xu, Zezhong},
volume = {26},
number = {7},
year = {2016},
pages = {1293 - 1307},
issn = {10518215},
abstract = {In this paper, we propose a method for visual tracking based on local sparse representation. Image patches from the object and the background are split into image blocks to construct local representations. Within the subclass discriminant framework, a discriminative subspace is learned to distinguish the object image blocks from the background image blocks while preserving their multimodal structure. A dictionary is constructed using the centers of the object subclasses. With this dictionary, sparse coding is implemented on the projected vectors corresponding to the image blocks, and the sparse coefficients are concatenated to obtain a local sparse code as the feature that represents the image patch. Considering the subclass discriminant constraint and the sparsity constraint imposed on the sparse coding, the subspace learning and sparse representation problems are converted into a joint optimization problem with respect to a transformation matrix and sparse coefficients. To enhance the tracking accuracy, two dictionaries are devised, one to incorporate the original observations of the target and the other to incorporate the latest observations, thereby providing two templates to characterize the appearance of the target. Histogram intersection over the local sparse codes provides an evaluation of the confidence. Finally, the candidate with the maximal confidence is selected as the object image patch. Compared with several state-of-the-art algorithms, our method demonstrates a superior performance when applied to challenging sequences.<br/> &copy; 1991-2012 IEEE.},
key = {Image coding},
keywords = {Concatenated codes;Linear transformations;Vectors;},
note = {Dictionary learning;Histogram intersection;Sparse representation;Sparsity constraints;State-of-the-art algorithms;subclass discriminant constraint (SDC);Transformation matrices;Visual Tracking;},
URL = {http://dx.doi.org/10.1109/TCSVT.2015.2424091},
} 


@article{20165103136435,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Seismic Evaluation of a Multitower Connected Building by Using Three Software Programs with Experimental Verification},
journal = {Shock and Vibration},
author = {Zhou, Deyuan and Guo, Changtuan and Wu, Xiaohan and Zhang, Bo},
volume = {2016},
year = {2016},
issn = {10709622},
abstract = {Shanghai International Design Center (SHIDC) is a hybrid structure of steel frame and reinforced concrete core tube (SF-RCC). It is a building of unequal height two-tower system and the story lateral stiffness of two towers is different, which may result in the torsion effect. To fully evaluate structural behaviors of SHIDC under earthquakes, NosaCAD, ABAQUS, and Perform-3D, which are widely applied for nonlinear structure analysis, were used to perform elastoplastic time history analyses. Numerical results were compared with those of shake table testing. NosaCAD has function modules for transforming the nonlinear analysis model to Perform-3D and ABAQUS. These models were used in ABAQUS or Perform-3D directly. With the model transformation, seismic performances of SHIDC were fully investigated. Analyses have shown that the maximum interstory drift can satisfy the limits specified in Chinese code and the failure sequence of structural members was reasonable. It meant that the earthquake input energy can be well dissipated. The structure keeps in an undamaged state under frequent earthquakes and it does not collapse under rare earthquakes; therefore, the seismic design target is satisfied. The integrated use of multisoftware with the validation of shake table testing provides confidence for a safe design of such a complex structure.<br/> &copy; 2016 Deyuan Zhou et al.},
key = {Seismic design},
keywords = {Earthquake effects;Nonlinear analysis;Rare earths;Reinforced concrete;Verification;},
note = {Earthquake input energy;Elasto-plastic time-history analysis;Experimental verification;International designs;Model transformation;Nonlinear analysis model;Reinforced concrete core;Structural behaviors;},
URL = {http://dx.doi.org/10.1155/2016/8215696},
} 


@inproceedings{20184406015587,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Modeling the injection of carbon dioxide and nitrogen into a methane hydrate reservoir and the subsequent production of methane gas on the North slope of Alaska},
journal = {Unconventional Resources Technology Conference 2013, URTC 2013},
author = {Garapati, Nagasree and McGuire, Patrick and Anderson, Brian J.},
year = {2013},
address = {Denver, CO, United states},
abstract = {HydrateResSim (HRS) is an open-source finite-difference reservoir simulation code capable of simulating the behavior of gas hydrate in porous media. The original version of HRS was developed to simulate pure methane hydrates, and the relationship between equilibrium temperature and pressure is given by a simple, 1-D regression expression. In this work, we have modified HydrateResSim to allow for the formation and dissociation of gas hydrates made from gas mixtures. This modification allows one to model the ConocoPhillips Ignik Sikumi #1 field test performed in early 2012 on the Alaska North Slope. The Ignik Sikumi #1 test is the first field-based demonstration of gas production through the injection of a mixture of carbon dioxide and nitrogen gases into a methane hydrate reservoir and thereby sequestering the greenhouse gas CO<inf>2</inf> into hydrate form. The primary change to the HRS software is the added capability of modeling a ternary mixture consisting of CH<inf>4</inf> + CO<inf>2</inf> + N<inf>2</inf> instead of only one hydrate guest molecule (CH<inf>4</inf>), therefore the new software is called Mix3HydrateResSim. This Mix3HydrateResSim upgrade to the software was accomplished by adding primary variables (for the concentrations of CO<inf>2</inf> and N<inf>2</inf>), governing equations (for the mass balances of CO<inf>2</inf> and N<inf>2</inf>), and phase equilibrium data. The phase equilibrium data in Mix3HydrateResSim is given as an input table [1] obtained using a statistical mechanical method developed in our research group called the cell potential method. An additional phase state describing a two-phase Gas-Hydrate (GsH) system was added to consider the possibility of converting all available free water to form hydrate with injected gas. Using Mix3HydrateResSim, a methane hydrate reservoir with coexisting pure-CH<inf>4</inf>-hydrate and aqueous phases at 7.0 MPa and 5.5&deg;C was modeled after the conditions of the Ignik Sikumi #1 test: (i) 14-day injection of CO<inf>2</inf> and N<inf>2</inf> followed by (ii) 30-day production of CH<inf>4</inf> (by depressurization of the well). During the injection phase, the injection well is modeled as a fixed-condition boundary maintained as a gas phase (23% CO<inf>2</inf> + 77% N<inf>2</inf>) at 9.65 MPa and 5.5 &deg;C. Initially, there is an increase in the saturation of hydrate indicating the formation of secondary hydrate due to the injected gas and the available free water. There is also a slight increase in the temperature due to the exothermic reaction of hydrate formation. As the hydrate becomes saturated with the injected gases it releases CH<inf>4</inf>. After the initial 14 days of injection, a mixture of the three gases was produced through depressurization. This was modeled by maintaining the well as a fixed-state boundary at the bottom-hole pressure. The amount of CH<inf>4</inf> released from the hydrate phase during the injection and production phases and the amount of CO<inf>2</inf> and N<inf>2</inf> gases sequestered as hydrates have been examined in this study. A model-based history-matching of the gas flow rates from the ConocoPhillips field test will be conducted to validate the code.<br/> &copy; Copyright 2013, Unconventional Resources Technology Conference (URTeC)},
key = {Gas hydrates},
keywords = {Bottom hole pressure;Carbon dioxide;Flow of gases;Greenhouse gases;Hydration;Injection (oil wells);Methane;Nitrogen;Open source software;Open systems;Petroleum reservoirs;Phase equilibria;Porous materials;Resource valuation;},
note = {Cell potential methods;Equilibrium temperatures;Governing equations;North Slope of Alaska;Phase equilibrium data;Regression expressions;Reservoir simulation;Statistical mechanical methods;},
} 


@inproceedings{20185106254766,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Periodic developer metrics in software defect prediction},
journal = {Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018},
author = {Ozcan Kini, Seldag and Tosun, Ayse},
year = {2018},
pages = {72 - 81},
address = {Madrid, Spain},
abstract = {Defect prediction studies have proposed several data-driven approaches, and recently, this field has put more emphasis on whether the people factor is associated software defects. Developer metrics can capture experience, code ownership, coding skills and techniques, and commit activities. These metrics have so far been measured at a specified snapshot of the codebase although developer's knowledge on a source module could change over time. In this paper, we propose to measure periodic developer experience with regard to contextual knowledge on files and directories. We extract periodic experience metrics capturing the previous activities of developers on source files and investigate the explanatory effect of these metrics on defects. We also use activity-based (churn) metrics to observe the performance of both metric types on defect prediction. We used two large-scale open source projects, Lucene and Jackrabbit, for model evaluation. We calculate periodic developer experience metrics and churn metrics at two granularity levels: File level and commit level. We build the models using five popular machine learning algorithms in defect prediction literature. The models with the two best performing algorithms are assessed in terms of Precision, Recall, False Positive Rate, and F-measure. The set of metrics that explains software defects the best is also identified using correlation-based feature selection method. Results show that periodic developer experience metrics extracted at file level are good merits for defect prediction, accompanied with churn. When there is not enough data to extract the contextual knowledge of developers on source files, churn metrics play an important role on defect prediction.<br/> &copy; 2018 IEEE.},
key = {Defects},
keywords = {Codes (symbols);Forecasting;Learning algorithms;Learning systems;Open source software;},
note = {Associated softwares;Churn metrics;Code ownership;Contextual knowledge;Correlation based feature selections;Data-driven approach;Periodic developer experience;Software defect prediction;},
URL = {http://dx.doi.org/10.1109/SCAM.2018.00016},
} 


@inproceedings{20161902346228,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Fault prediction model for software using soft computing techniques},
journal = {ICOSST 2015 - 2015 International Conference on Open Source Systems and Technologies, Proceedings},
author = {Nisa, Ishrat Un and Ahsan, Syed Nadeem},
year = {2015},
pages = {78 - 83},
address = {Lahore, Pakistan},
abstract = {Faulty modules of any software can be problematic in terms of accuracy, hence may encounter more costly redevelopment efforts in later phases. These problems could be addressed by incorporating the ability of accurate prediction of fault prone modules in the development process. Such ability of the software enables developers to reduce the faults in the whole life cycle of software development, at the same time it benefits automation process, and reduces the overall cost and efforts of the software maintenance. In this paper, we propose to design fault prediction model by using a set of code and design metrics; applying various machine learning (ML) classifiers; also used transformation techniques for feature reduction and dealing class imbalance data to improve fault prediction model. The data sets were obtained from publicly available PROMISE repositories. The results of the study revealed that there was no significant impact on the ability to accurately predict the fault-proneness of modules by applying PCA in reducing the dimensions; the results were improved after balancing data by SMOTE, Resample techniques, and by applying PCA with Resample in combination. It has also been seen that Random Forest, Random Tree, Logistic Regression, and Kstar machine learning classifiers have relatively better consistency in prediction accuracy as compared to other techniques.<br/> &copy; 2015 IEEE.},
key = {Open systems},
keywords = {Artificial intelligence;Data mining;Decision trees;Forecasting;Learning systems;Life cycle;Metadata;Open source software;Soft computing;Software design;},
note = {Class imbalance;Development process;Fault prediction models;Feature reduction;Logistic regressions;Softcomputing techniques;Software metrics;Transformation techniques;},
URL = {http://dx.doi.org/10.1109/ICOSST.2015.7396406},
} 


@inproceedings{20164603007120,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Enhance embedded system E-leaming experience with sensors},
journal = {IEEE Global Engineering Education Conference, EDUCON},
author = {Malchow, Martin and Renz, Jan and Bauer, Matthias and Meinel, Christoph},
volume = {10-13-April-2016},
year = {2016},
pages = {175 - 183},
issn = {21659559},
address = {Abu Dhabi, United arab emirates},
abstract = {Earlier research shows that using an embedded LED system motivates students to learn programming languages in massive open online courses (MOOCs) efficiently. Since this earlier approach was very successful the system should be improved to increase the learning experience for students during programming exercises. The problem of the current system is that only a static image was shown on the LED matrix controlled by students' array programming over the embedded system. The idea of this paper is to change this static behavior into a dynamic display of information on the LED matrix by the use of sensors which are connected with the embedded system. For this approach a light sensor and a temperature sensor are connected to an analog-to-digital converter (ADC) port of the embedded system. These sensors' values can be read by the students to compute the correct output for the LED matrix. The result is captured and sent back to the students for direct feedback. Furthermore, unit tests can be used to automatically evaluate the programming results. The system was evaluated during a MOOC course about Web Technologies using JavaScript. Evaluation results are taken from the student's feedback and an evaluation of the students' code executions on the system. The positive feedback and the evaluation of the students' executions, which shows a higher amount of code executions compared to standard programming tasks and the fact that students solving these tasks have overall better course results, highlight the advantage of the approach. Due to the evaluation results, this approach should be used in e-learning e.g. MOOCs teaching programming languages to increase the learning experience and motivate students to learn programming.<br/> &copy; 2016 IEEE.},
key = {Students},
keywords = {Analog to digital conversion;Distance education;E-learning;Embedded systems;Engineering education;Feedback;Light emitting diodes;Teaching;},
note = {Analog to digital converters;Evaluation results;Learning experiences;Massive open online course;Programming exercise;Tele-Lecturing;Tele-teaching;Virtual lab;},
URL = {http://dx.doi.org/10.1109/EDUCON.2016.7474550},
} 


@inproceedings{20142217755430,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An empirical study of faults in late propagation clone genealogies},
journal = {Journal of software: Evolution and Process},
author = {Barbour, Liliane and Khomh, Foutse and Zou, Ying},
volume = {25},
number = {11},
year = {2013},
pages = {1139 - 1165},
issn = {20477481},
abstract = {Two similar code segments, or clones, form a clone pair within a software system. The changes to the clones over time create a clone evolution history. In this work, we study late propagation, a specific pattern of clone evolution. In late propagation, one clone in a clone pair is modified, causing the clone pair to diverge. The code segments are then reconciled in a later commit. Existing work has established late propagation as a clone evolution pattern and suggested that the pattern is related to a high number of faults. In this study, we examine the characteristics of late propagation in three long-lived software systems using the Simian (Simon Harris, Victoria, Australia, http://www.harukizaemon.com/simian), CCFinder, and NiCad (Software Technology Laboratory, Queen's University, Kingston, ON, Canada) clone detection tools. We define eight types of late propagation and compare them to other forms of clone evolution. Our results not only verify that late propagation is more harmful to software systems but also establish that some specific types of late propagations are more harmful than others. Specifically, two types are most risky: (1) when a clone experiences diverging changes and then a reconciling change without any modification to the other clone in a clone pair; and (2) when two clones undergo a diverging modification followed by a reconciling change that modifies both the clones in a clone pair. We also observe that the reconciliation in the former case is more prone to faults than in the latter case. We determine that the size of the clones experiencing late propagation has an effect on the fault proneness of specific types of late propagation genealogies. Lastly, we cannot report a correlation between the delay of the propagation of changes and its faults, as the fault proneness of each delay period is system dependent. Copyright &copy; 2013 John Wiley &amp; Sons, Ltd.<br/>},
key = {Cloning},
keywords = {Computer software;Crack propagation;History;},
note = {Empirical studies;Evolution history;Evolution patterns;Fault proneness;Propagation of changes;Software systems;Software technology;Victoria, Australia;},
URL = {http://dx.doi.org/10.1002/smr.1597},
} 


@inproceedings{20165103131879,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Suspiciously structured entropy: Wavelet decomposition of software entropy reveals symptoms of malware in the energy spectrum},
journal = {Proceedings of the 29th International Florida Artificial Intelligence Research Society Conference, FLAIRS 2016},
author = {Wojnowicz, Michael and Chisholm, Glenn and Wolff, Matt},
year = {2016},
pages = {288 - 293},
address = {Key Largo, FL, United states},
abstract = {Sophisticated malware authors can sneak hidden malicious code into portable executable files, and this code can be hard to detect, especially if it is encrypted or compressed. However, when an executable file shifts between native code, encrypted or compressed code, and padding, there are corresponding shifts in the file's representation as an entropy signal. In this paper, we develop a method for automatically quantifying the extent to which the patterned variations in a file's entropy signal makes it "suspicious." A corpus of n = 39,968 portable executable files were studied, 50% of which were malicious. Each portable executable file was represented as an entropy stream, where each value in the entropy stream describes the amount of entropy at a particular locations in the file. Wavelet transforms were then applied to this entropy signal in order to extract the amount of entropie energy at multiple scales of code resolution. Based on this entropie energy spectrum, we derive a Suspiciously Structured Entropie Change Score (SSECS), a single scalar feature which quantifies the extent to which a given file's entropie energy spectrum makes the file suspicious as possible malware. We found that, based on SSECS alone, it was possible to predict with 68.7% accuracy whether a file in this corpus was malicious or legitimate (a 18.7% gain over random guessing). Moreover, we found that SSECS contains predictive information not contained in mean entropy alone. Thus, we argue that SSECS could be a useful single feature for machine learning models which attempt to identify malware based on millions of file features.<br/> &copy; 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
key = {Malware},
keywords = {Artificial intelligence;Codes (symbols);Computer crime;Cryptography;Entropy;Learning systems;Spectroscopy;Wavelet decomposition;},
note = {Compressed codes;Executable files;Machine learning models;Malicious codes;Multiple scale;Portable Executable files;Predictive information;Software entropies;},
} 


@inproceedings{20164803071899,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Discovering bug patterns in Javascript},
journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
author = {Hanam, Quinn and Brito, Fernando S. De M. and Mesbah, Ali},
volume = {13-18-November-2016},
year = {2016},
pages = {144 - 156},
address = {Seattle, WA, United states},
abstract = {JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug findings tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-Automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using languageconstruct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript.<br/> &copy; 2016 ACM.},
key = {Program debugging},
keywords = {Data mining;High level languages;Learning systems;Software engineering;Static analysis;},
note = {Bug Patterns;Javascript;Large-scale studies;Multiple projects;Node.Js;Semi-automatics;Tools and techniques;Unsupervised machine learning;},
URL = {http://dx.doi.org/10.1145/2950290.2950308},
} 


@inproceedings{20161102085907,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Deploying an enterprise-class Software Lifecycle Management solution for Test Program Sets},
journal = {AUTOTESTCON (Proceedings)},
author = {Davis, Timothy W. and Kane, Gary S.},
volume = {2015-December},
year = {2015},
pages = {450 - 455},
address = {National Harbor, MD, United states},
abstract = {Information Technology tools for Software Lifecycle Management have advanced in recent years to enable organizations to implement a more centralized, enterprise-level lifecycle management solution. Despite these tools having become commonly used in the software development world, the Test Program Set (TPS) community remained behind the times. This paper presents the history and implementation of TPS Lifecycle Management as deployed by the TPS community for the Consolidated Automated Support System (CASS) Family of Testers (FoT). Prior to any efforts to enact centralized TPS lifecycle (i.e. configuration) management, TPS owners controlled their software in a variety of ways. The results of their efforts could be characterized as barely effective and often problematic. The initial attempt at solving the TPS lifecycle management problem was to mandate the use of a commercially available product which was not well supported by the developer of the application. That, coupled with both political issues and technical problems, relegated this system to becoming simply a repository for data. After addressing many of the problems with the initial solution and moving to a more widely-used product, the next iteration of TPS lifecycle management has been much more successful. It has become a legitimate enterprise solution providing both version control and implementing change management principles to include traceability between software change requests and released code. Many of the specific design details are discussed in the paper and include: a flexible workflow, a defined code branching strategy, an ownership and protection scheme and a process for release. The paper concludes with a detailed example of how TPS lifecycle management has benefitted the CASS team and, in particular, those who provide support of TPSs. It also shows how those benefits can be attained by other TPS support teams looking to effectively manage their test programs on other types of Automatic Test Equipment.<br/> &copy; 2015 IEEE.},
key = {Life cycle},
keywords = {Automatic testing;Computer software;Enterprise software;Equipment testing;Human resource management;Management;Software design;Software testing;Supports;Test facilities;},
note = {Automated support systems;Automatic test equipment;cass;configuration;Flexible workflows;Information technology tools;Life-cycle management;Software lifecycle management;},
URL = {http://dx.doi.org/10.1109/AUTEST.2015.7356532},
} 


@inproceedings{20163002625956,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {SWAT: A programmable, in-memory, distributed, high-performance computing platform},
journal = {HPDC 2016 - Proceedings of the 25th ACM International Symposium on High-Performance Parallel and Distributed Computing},
author = {Grossman, Max and Sarkar, Vivek},
year = {2016},
pages = {81 - 92},
address = {Kyoto, Japan},
abstract = {The field of data analytics is currently going through a renaissance as a result of ever-increasing dataset sizes, the value of the models that can be trained from those datasets, and a surge in exible, distributed programming models. In particular, the Apache Hadoop [1] and Spark [5] programming systems, as well as their supporting projects (e.g. HDFS, SparkSQL), have greatly simplified the analysis and transformation of datasets whose size exceeds the capacity of a single machine. While these programming models facilitate the use of distributed systems to analyze large datasets, they have been plagued by performance issues. The I/O performance bottlenecks of Hadoop are partially responsible for the creation of Spark. Performance bottlenecks in Spark due to the JVM object model, garbage collection, interpreted/-managed execution, and other abstraction layers are responsible for the creation of additional optimization layers, such as Project Tungsten [4]. Indeed, the Project Tungsten issue tracker states that the "majority of Spark workloads are not bottlenecked by I/O or network, but rather CPU and memory" [20]. In this work, we address the CPU and memory performance bottlenecks that exist in Apache Spark by accelerating user-written computational kernels using accelerators. We refer to our approach as Spark With Accelerated Tasks (SWAT). SWAT is an accelerated data analytics (ADA) framework that enables programmers to natively execute Spark applications on high performance hardware platforms with co-processors, while continuing to write their applications in a JVM-based language like Java or Scala. Runtime code generation creates OpenCL kernels from JVM bytecode, which are then executed on OpenCL accelerators. In our work we emphasize 1) full compatibility with a modern, existing, and accepted data analytics platform, 2) an asynchronous, event-driven, and resource-aware runtime, 3) multi-GPU memory management and caching, and 4) ease-of-use and programmability. Our performance evaluation demonstrates up to 3.24&times; overall application speedup relative to Spark across six machine learning benchmarks, with a detailed investigation of these performance improvements.<br/> Copyright &copy; 2016 by the Association for Computing Machinery, Inc. (ACM).},
key = {Distributed computer systems},
keywords = {Abstracting;Ada (programming language);Benchmarking;Computer software;Electric sparks;Graphics processing unit;Information management;Learning systems;Tungsten;},
note = {Data analytics;Distributed;Distributed programming model;Heterogeneous;High performance computing;High-performance hardware;OpenCL;Run-time code generation;},
URL = {http://dx.doi.org/10.1145/2907294.2907307},
} 


@article{20180704806503,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Incremental dynamic analysis of sdof by using nonlinear earthquake accelerograms based on modified inverse Fourier transform},
journal = {Journal of Vibroengineering},
author = {Faroughi, Alireza and Hosseini, Mahmood},
volume = {19},
number = {8},
year = {2017},
pages = {6170 - 6182},
issn = {13928716},
abstract = {In evaluation of structures, performing nonlinear response of model over a time analysis or incremental dynamic analysis needs more time. Hence, it can be beneficial if the event history report is carried out with long time steps without loss of accuracy. This study includes a method to simplify of accelerograms meant on the change of their Fourier reports. So, the Fourier Spectrum of the accelerogram is initially determined. Next applying a PC code generation, the similar Inverse Fourier Convert is computed utilizing a comparatively large time stage, depending on the structure&rsquo;s times; that is ordinarily five to ten times bigger than primary accelerogram&rsquo;s duration stage to generate the visible accelerogram. This application from the simplified accelerogram apparently takes much less time. Results indicates that the analysis time can be reduced up to 80 % by using the proposed method. While the maximum response shows an error of merely five to ten percent, about the sort of structure and the characteristics of the records used.<br/> &copy; JVE INTERNATIONAL LTD.},
key = {Fourier transforms},
keywords = {Dynamic analysis;Graphic methods;Inverse problems;Inverse transforms;},
note = {Code Generation;Event history;Fourier spectra;Incremental dynamic analysis;Inverse Fourier transforms;Loss of accuracy;Non-linear response;Time stage size;},
URL = {http://dx.doi.org/10.21595/jve.2017.18322},
} 


@inproceedings{20141617597930,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {An empirical study on the fault-proneness of clone migration in clone genealogies},
journal = {2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014 - Proceedings},
author = {Xie, Shuai and Khomh, Foutse and Zou, Ying and Keivanloo, Iman},
year = {2014},
pages = {94 - 103},
address = {Antwerp, Belgium},
abstract = {Copy and paste activities create clone groups in software systems. The evolution of a clone group across the history of a software system is termed as clone genealogy. During the evolution of a clone group, developers may change the location of the code fragments in the clone group. The type of the clone group may also change (e.g., from Type-1 to Type-2). These two phenomena have been referred to as clone migration and clone mutation respectively. Previous studies have found that clone migration occur frequently in software systems, and suggested that clone migration can induce faults in a software system. In this paper, we examine how clone migration phenomena affect the risk for faults in clone segments, clone groups, and clone genealogies from three long-lived software systems JBoss, APACHE-ANT, and ARGOUML. Results show that: (1) migrated clone segments, clone groups, and clone genealogies are not equally fault-prone; (2) when a clone mutation occurs during a clone migration, the risk for faults in the migrated clone is increased; (3) migrating a clone that was not changed for a longer period of time is risky. &copy; 2014 IEEE.<br/>},
key = {Cloning},
keywords = {Computer software;Computer software maintenance;Copying;History;Reengineering;Reverse engineering;},
note = {Clone mutation;Code fragments;Copy-and-paste;Empirical studies;Fault proneness;Fault-prone;Migration phenomena;Software systems;},
URL = {http://dx.doi.org/10.1109/CSMR-WCRE.2014.6747229},
} 


@article{20172803942366,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {Design and analysis of online micro-course of Garden Architecture design based on CPC model},
journal = {International Journal of Emerging Technologies in Learning},
author = {Liu, Lili},
volume = {12},
number = {7},
year = {2017},
pages = {44 - 55},
issn = {18688799},
abstract = {In the network information age, micro-course is a brand-new teaching method which integrates conciseness, vividness, visualizability and distinctiveness. It is also a means of educational informationization. Especially in architecture design course, micro-course has been effectively applied. Based on CPC model, online micro-course of Garden Architecture Design was constructed again by combining patent 2D code technology in this paper. The design and analysis of online micro-course of Garden Architecture Design based on CPC model can basically achieve classroom flipping, promote equal interaction between teachers and students, enhance communications among classmates, feedback and evaluate teaching effect in time, promote students' learning interest, change students' learning attitude and boost their academic performance so as to reach the expected teaching objective.<br/>},
key = {Teaching},
keywords = {Curricula;E-learning;Network architecture;Students;},
note = {2D codes;Academic performance;Architecture designs;Design and analysis;Learning attitudes;Micro-course;Network information;Teaching objectives;},
URL = {http://dx.doi.org/10.3991/ijet.v12i07.7214},
} 


@inproceedings{20182105216071,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events},
journal = {Advances in Neural Information Processing Systems},
author = {Racah, Evan and Beckham, Christopher and Maharaj, Tegan and Kahou, Samira Ebrahimi and Prabhat and Pal, Christopher},
volume = {2017-December},
year = {2017},
pages = {3403 - 3414},
issn = {10495258},
address = {Long Beach, CA, United states},
abstract = {Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https://github.com/eracah/hur-detect.<br/> &copy; 2017 Neural information processing systems foundation. All rights reserved.},
key = {Climate change},
keywords = {Decision making;Hurricanes;Learning systems;Neural networks;Risk management;Storms;Weather information services;},
note = {Convolutional neural network;Detection and identifications;Exploratory data analysis;Extra-tropical cyclones;Extreme weather events;Governmental policies;Machine learning research;Temporal information;},
} 


@inproceedings{20141017436000,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright = {Compendex},
title = {2013 International Conference on Sensors, Mechatronics and Automation, ICSMA 2013},
journal = {Applied Mechanics and Materials},
volume = {511-512},
year = {2014},
pages = {Queensland University of Technology; Korea Maritime University; Hong Kong Industrial Technology Research Centre; Inha University; National Chengchi University, Taiwan; et al - },
issn = {16609336},
address = {Shenzhen, China},
abstract = {The proceedings contain 245 papers. The special focus in this conference is on Sensors, Mechatronics and Automation. The topics include: A novel method to dicing anodically bonded silicon glass MEMS wafers based on UV laser technique; a microfluidic device for sample pretreatment by laminar flow extraction; mechanical properties and microstructure analysis of aluminum and steel dissimilar material weld joint; experimental study on axial tensile performance of low volume fraction of ternary hybrid fiber reinforced concrete; mechanical properties of nanocrystalline coatings prepared by thermal spraying process; preparation and ascertainment of ingredient contents of uranium ore sandstone standard substance; study on corrosion behavior of A3 carbon steel in the manicipal sludge; study on effect of high oxygen treatment on storage quality and anti-browning of fresh-cut apples; study on wall slip in pipeline flow of dense paste; adsorption characteristic research of sediments of lake baiyangdian; high temperature deformation behavior and dynamic recrystallization law of 33Mn2V steel for oil well tube; finite element analysis of 1-3-2 piezoelectric composite resonance frequency characteristics; study on novel relaxor ferroelectric single crystal PMNT/epoxy composite transducer array; influence of sulfur on the long-term durability of SOFC cathode; distribution function for nonlinear potential gradient high pressure plasma; analysis of copper wires short circuited melted mark; a new approach for autonomous robot obstacle avoidance using PSD infrared sensor combined with digital compass; design and build of a nine-axis digital output gyroscope-based percutaneous puncture ultrasonic navigation system; construction principle and application test of 220kV self-healing optical voltage sensor; a survey on QoS-aware secure data transmission in multimedia sensor networks; the influence of implantable actuators on residual hearing; wireless sensor networks for structural healthy monitoring applications-a design proposal; dynamic estimation of water hyacinth area by fusing data from satellite and GPS sensors; optimal ecological restoration of degraded wetland ecosystem by using satellite sensors; PZT based ultrasonic wave force detecting sensor; research on routing protocol for large-scale wireless sensor networks; research on wireless MAC protocols oriented to smart clothing; sensor node identifier resolution of IP-based and non-IP-based WSN; study on the conductivity of DMEM by PASCO sensors; study on transient temperature generator and dynamic compensation technology; substation equipment temperature monitoring system based on heterogeneous wireless sensor network; tamper detection in RFID tags using zero-watermarks; tuning characteristics of DFB diode laser and its application to TDLAS gas sensor design; theoretical modeling and error analysis for reflection structure fiber optical current transformer; the calibration techniques of paper basis weight sensor; modeling of collaborative context-awareness for smart home; sensor fault diagnosis based on SOFM neural network; the design of a basketball goal automatic detector; numerical analysis of the effects caused by ground reflection on aperture coupling; acoustic wave resonance of stimulated brillouin scattering in a single mode fiber ring; rules of harmonic longitudinal propagation in various voltage classes; study on the multi-optical theodolites positioning method based on function restriction; weight size determined variable step size LMS method for identifying under damped systems; a fuzzy curve detection method based on lateral acceleration; research and performance analysis on pedestrian detection based on optimized hog and SVM; a new method for calculating the volume of debris flow using GIS and RS technology; design of thermocouple verification data records table based on labview; development of a transducer for dental implant stability measurement; temperature influence on real-time ultrasonic detection of multi-planetary gear transmission; application of frequency-voltage converter chip in the vehicle data acquisition; multi-point temperature monitoring system for the LNG storage tank; research on the impact of CNC servo dynamic characteristic for the on-line detection precision; weak signal detection and application in electronic experiment based on virtual instrument; design of a constant probing force system for profilometer; research of film thickness online measurement system; a new method based on lyapunov exponent to determine the threshold of chaotic systems; application of barcode information and acquisition system for the machining process technology; capability assessment of satellite communication system based on multi-granular modeling; research on discharge signals feature extraction of sealing packaging food existing pinholes based on EEMD; dynamic displacement response characteristics of a solenoid actuator for electromagnetic separation; error processing techniques of integral velocity based on frequency spectrum forecasting; FH-FSK underwater acoustic control and communication system based on LDPC code; monitoring system for plant growth environment based on zigbee/RS485; feature extraction of the small leakage diagnosis of oil pipeline based on acoustic signal; pseudo two-hop distributed consensus with noise; research on MWD mud pulse signal waveform identification algorithm; ultralow frequency standard vibration calibration system based on DSP; analysis on graduate employment based on data warehouse; air quality evaluation based on local normalized image contrast; inexact distributed reconstruction via alternating direction method; reasoning method of remote sensing imagery based on topological transformation; extraction of the palm vein texture features based on Gabor wavelet transforms; facial expression recognition based on monogenic binary coding; HVS-based low bit-rate image compression; a novel method of macro block motion prediction using particle filter; an effective multi-feature extraction method for manually plotted well-log curve line; an image segmentation method of underwater targets based on active contour model; removal of glass reflection using TV-retinex model; recognition method of cotton blind stinkbug hazard level based on image processing; stereoscopic images enhancement based on edge sharpening of wavelet coefficients; the bistatic MIMO imaging algorithm based on waveform diversity; the algal blooms prediction in the lakes based on remote sensing information; the image information recognition of large structures based on CAD model characteristics; topological equivalence in multi-scale remote sensing image segmentation; study on regression model of measuring weld position; the change and management for surveying achievement of real estate using GIS techno logy; intelligent video surveillance system based on sound source localization technology; decision method of optimal X-ray digital imaging parameters; gesture acquisition and tracking with kinect under complex background; a novel approach of edge detection based on vector morphology; edge detection of color images based on improved morphological gradient operators; the filtering and streamline of three-dimensional point-cloud data; compressive strength analysis of MWD microchip tracer; analysis of mechanical fatigue behavior for MEMS structures; feature based parametric design of piston reliability; numerical study on effect of wall on upwelling flow; influence of technical parameters on contact pressure in straight bevel gear meshing; modeling of electrostatic chuck and simulation of electrostatic force; research on finite element static analysis in mechanical vehicle structure design; research on vault plate installation technology of LNG storage tank; seal optimization technology research of large LNG storage tank dome gas lift; study on elevator drive system dynamics simulation of rail transport conveyer; multiple instance space description method in the application research of complex product conceptual design; design of multi-mode PID controller and application in time-delay process; remote household appliance control system based on internet; the simulation investigation of matlab in intelligent control course; optimal power management of final load and electrolyser in a solar hydrogen power generation system; design of absorbing wave maker control system based on trio controller; design of a chaos phenomenon exhibition system; dynamic modeling of a new version of cylindrical planetary gear used for a robotic arm; modular design based on design rules; study on the automatic spraying system of dust suppressant and speed adaptive algorithms; research and design of shortwave and ultra-short wave SDR engineering in urban emergency system; design of the embedded control system of laser engraving machine; research of IOT in the application of wisdom agricultural; research of RAM on the all-electronic computer interlocking system; the research on embedded database system of wireless network; drive characteristic analysis and test system design for water hydraulic artificial muscle; a type of equipment TV goniometer angle deviation signal simulation research; design of a new dual-core intelligent PLC for Ethernet communication; design and implementation of an automatic hydrological telemetry system; improvement and design of single coil vibrating-wire sensor signal acquisition system based on STM32; application of three-dimensional fine geological modeling in complex fault-block reservoir with low permeability; decentralised sanitation and energy reuse in the built environment; the empirical research on industry informatization; manufacturing defects and safety technology of automobile product; research on SSCR technology to reduce NOx emissions for diesel engines; a new technique to improve estimation of position for serial robots.},
} 



