@article{ANDERSEN201545,
title = "Evolutionary history and adaptive significance of the polymorphic Pan I in migratory and stationary populations of Atlantic cod (Gadus morhua)",
journal = "Marine Genomics",
volume = "22",
pages = "45 - 54",
year = "2015",
issn = "1874-7787",
doi = "https://doi.org/10.1016/j.margen.2015.03.009",
url = "http://www.sciencedirect.com/science/article/pii/S1874778715000495",
author = "Øivind Andersen and Hanne Johnsen and Maria Cristina De Rosa and Kim Præbel and Suzana Stjelja and Tina Graceline Kirubakaran and Davide Pirolli and Sissel Jentoft and Svein-Erik Fevolden",
keywords = "Pan I, Rhodopsin, Synaptic vesicle, Atlantic cod, Polymorphism, Ecotypes",
abstract = "The synaptophysin (SYP) family comprises integral membrane proteins involved in vesicle-trafficking events, but the physiological function of several members has been enigmatic for decades. The presynaptic SYP protein controls neurotransmitter release, while SYP-like 2 (SYPL2) contributes to maintain normal Ca2+-signaling in the skeletal muscles. The polymorphic pantophysin (Pan I) of Atlantic cod shows strong genetic divergence between stationary and migratory populations, which seem to be adapted to local environmental conditions. We have investigated the functional involvement of Pan I in the different ecotypes by analyzing the 1) phylogeny, 2) spatio-temporal gene expression, 3) structure–function relationship of the Pan IA and IB protein variants, and 4) linkage to rhodopsin (rho) recently proposed to be associated with different light sensitivities in Icelandic populations of Atlantic cod. We searched for SYP family genes in phylogenetic key species and identified a single syp-related gene in three invertebrate chordates, while four members, Syp, Sypl1, Sypl2 and synaptoporin (Synpr), were found in tetrapods, Comoran coelacanth and spotted gar. Teleost fish were shown to possess duplicated syp, sypl2 and synpr genes of which the sypl2b paralog is identical to Pan I. The ubiquitously expressed cod Pan I codes for a tetra-spanning membrane protein possessing five amino acid substitutions in the first intravesicular loop, but only minor structural differences were shown between the allelic variants. Despite sizable genomic distance (>2.5Mb) between Pan I and rho, highly significant linkage disequilibrium was found by genotyping shallow and deep water juvenile settlers predominated by the Pan IA–rhoA and Pan IB–rhoB haplotypes, respectively. However, the predicted rhodopsin protein showed no amino acid changes, while multiple polymorphic sites in the upstream region might affect the gene expression and pigment levels in stationary and migratory cod. Alternatively, other strongly linked genes might be responsible for the sharp settling stratification of juveniles and the different vertical behavior patterns of adult Atlantic cod."
}
@article{2016e361,
title = "Estimates of global, regional, and national incidence, prevalence, and mortality of HIV, 1980–2015: the Global Burden of Disease Study 2015",
journal = "The Lancet HIV",
volume = "3",
number = "8",
pages = "e361 - e387",
year = "2016",
issn = "2352-3018",
doi = "https://doi.org/10.1016/S2352-3018(16)30087-X",
url = "http://www.sciencedirect.com/science/article/pii/S235230181630087X",
author = "Haidong Wang and Tim M Wolock and Austin Carter and Grant Nguyen and Hmwe Hmwe Kyu and Emmanuela Gakidou and Simon I Hay and Edward J Mills and Adam Trickey and William Msemburi and Matthew M Coates and Meghan D Mooney and Maya S Fraser and Amber Sligar and Joshua Salomon and Heidi J Larson and Joseph Friedman and Amanuel Alemu Abajobir and Kalkidan Hassen Abate and Kaja M Abbas and Mohamed Magdy Abd El Razek and Foad Abd-Allah and Abdishakur M Abdulle and Semaw Ferede Abera and Ibrahim Abubakar and Laith J Abu-Raddad and Niveen M E Abu-Rmeileh and Gebre Yitayih Abyu and Akindele Olupelumi Adebiyi and Isaac Akinkunmi Adedeji and Ademola Lukman Adelekan and Koranteng Adofo and Arsène Kouablan Adou and Oluremi N Ajala and Tomi F Akinyemiju and Nadia Akseer and Faris Hasan Al Lami and Ziyad Al-Aly and Khurshid Alam and Noore K M Alam and Deena Alasfoor and Saleh Fahed S Aldhahri and Robert William Aldridge and Miguel Angel Alegretti and Alicia V Aleman and Zewdie Aderaw Alemu and Rafael Alfonso-Cristancho and Raghib Ali and Ala'a Alkerwi and François Alla and Rajaa Mohammad and Salem Al-Raddadi and Ubai Alsharif and Elena Alvarez and Nelson Alvis-Guzman and Azmeraw T Amare and Alemayehu Amberbir and Adeladza Kofi Amegah and Walid Ammar and Stephen Marc Amrock and Carl Abelardo T Antonio and Palwasha Anwari and Johan Ärnlöv and Al Artaman and Hamid Asayesh and Rana Jawad Asghar and Reza Assadi and Suleman Atique and Lydia S Atkins and Euripide Frinel G Arthur Avokpaho and Ashish Awasthi and Beatriz Paulina Ayala Quintanilla and Umar Bacha and Alaa Badawi and Aleksandra Barac and Till Bärnighausen and Arindam Basu and Tigist Assefa Bayou and Yibeltal Tebekaw Bayou and Shahrzad Bazargan-Hejazi and Justin Beardsley and Neeraj Bedi and Derrick A Bennett and Isabela M Bensenor and Balem Demtsu Betsu and Addisu Shunu Beyene and Eesh Bhatia and Zulfiqar A Bhutta and Sibhatu Biadgilign and Boris Bikbov and Sait Mentes Birlik and Donal Bisanzio and Michael Brainin and Alexandra Brazinova and Nicholas J K Breitborde and Alexandria Brown and Michael Burch and Zahid A Butt and Julio Cesar Campuzano and Rosario Cárdenas and Juan Jesus Carrero and Carlos A Castañeda-Orjuela and Jacqueline Castillo Rivas and Ferrán Catalá-López and Hsing-Yi Chang and Jung-chen Chang and Laxmikant Chavan and Wanqing Chen and Peggy Pei-Chia Chiang and Mirriam Chibalabala and Vesper Hichilombwe Chisumpa and Jee-Young Jasmine Choi and Devasahayam Jesudas Christopher and Liliana G Ciobanu and Cyrus Cooper and Tukur Dahiru and Solomon Abrha Damtew and Lalit Dandona and Rakhi Dandona and José das Neves and Pieter de Jager and Diego De Leo and Louisa Degenhardt and Robert P Dellavalle and Kebede Deribe and Amare Deribew and Don C Des Jarlais and Samath D Dharmaratne and Eric L Ding and Pratik Pinal Doshi and Kerrie E Doyle and Tim R Driscoll and Manisha Dubey and Yousef Mohamed Elshrek and Iqbal Elyazar and Aman Yesuf Endries and Sergey Petrovich Ermakov and Babak Eshrati and Alireza Esteghamati and Imad D A Faghmous and Carla Sofia e Sa Farinha and Andre Faro and Maryam S Farvid and Farshad Farzadfar and Seyed-Mohammad Fereshtehnejad and Joao C Fernandes and Florian Fischer and Joseph Robert Anderson Fitchett and Nataliya Foigt and Nancy Fullman and Thomas Fürst and Fortuné Gbètoho Gankpé and Teshome Gebre and Amanuel Tesfay Gebremedhin and Alemseged Aregay Gebru and Johanna M Geleijnse and Bradford D Gessner and Peter W Gething and Tsegaye Tewelde Ghiwot and Maurice Giroud and Melkamu Dedefo Gishu and Elizabeth Glaser and Shifalika Goenka and Amador Goodridge and Sameer Vali Gopalani and Atsushi Goto and Harish Chander Gugnani and Mark D C Guimaraes and Rahul Gupta and Rajeev Gupta and Vipin Gupta and Juanita Haagsma and Nima Hafezi-Nejad and Holly Hagan and Gessessew Bugssa Hailu and Randah Ribhi Hamadeh and Samer Hamidi and Mouhanad Hammami and Graeme J Hankey and Yuantao Hao and Hilda L Harb and Sivadasanpillai Harikrishnan and Josep Maria Haro and Kimani M Harun and Rasmus Havmoeller and Mohammad T Hedayati and Ileana Beatriz Heredia-Pi and Hans W Hoek and Masako Horino and Nobuyuki Horita and H Dean Hosgood and Damian G Hoy and Mohamed Hsairi and Guoqing Hu and Hsiang Huang and John J Huang and Kim Moesgaard Iburg and Bulat T Idrisov and Kaire Innos and Veena J Iyer and Kathryn H Jacobsen and Nader Jahanmehr and Mihajlo B Jakovljevic and Mehdi Javanbakht and Achala Upendra Jayatilleke and Panniyammakal Jeemon and Vivekanand Jha and Guohong Jiang and Ying Jiang and Tariku Jibat and Jost B Jonas and Zubair Kabir and Ritul Kamal and Haidong Kan and André Karch and Corine Kakizi Karema and Dimitris Karletsos and Amir Kasaeian and Anil Kaul and Norito Kawakami and Jeanne Françoise Kayibanda and Peter Njenga Keiyoro and Andrew Haddon Kemp and Andre Pascal Kengne and Chandrasekharan Nair Kesavachandran and Yousef Saleh Khader and Ibrahim Khalil and Abdur Rahman Khan and Ejaz Ahmad Khan and Young-Ho Khang and Jagdish Khubchandani and Yun Jin Kim and Yohannes Kinfu and Miia Kivipelto and Yoshihiro Kokubo and Soewarta Kosen and Parvaiz A Koul and Ai Koyanagi and Barthelemy Kuate Defo and Burcu Kucuk Bicer and Veena S Kulkarni and G Anil Kumar and Dharmesh Kumar Lal and Hilton Lam and Jennifer O Lam and Sinead M Langan and Van C Lansingh and Anders Larsson and James Leigh and Ricky Leung and Yongmei Li and Stephen S Lim and Steven E Lipshultz and Shiwei Liu and Belinda K Lloyd and Giancarlo Logroscino and Paulo A Lotufo and Raimundas Lunevicius and Hassan Magdy Abd El Razek and Mahdi Mahdavi and P A Mahesh and Marek Majdan and Azeem Majeed and Carla Makhlouf and Reza Malekzadeh and Chabila C Mapoma and Wagner Marcenes and Jose Martinez-Raga and Melvin Barrientos Marzan and Felix Masiye and Amanda J Mason-Jones and Bongani M Mayosi and Martin McKee and Peter A Meaney and Man Mohan Mehndiratta and Alemayehu B Mekonnen and Yohannes Adama Melaku and Peter Memiah and Ziad A Memish and Walter Mendoza and Atte Meretoja and Tuomo J Meretoja and Francis Apolinary Mhimbira and Ted R Miller and Joseph Mikesell and Mojde Mirarefin and Karzan Abdulmuhsin Mohammad and Shafiu Mohammed and Ali H Mokdad and Lorenzo Monasta and Maziar Moradi-Lakeh and Rintaro Mori and Ulrich O Mueller and Brighton Murimira and Gudlavalleti Venkata Satyanarayana Murthy and Aliya Naheed and Luigi Naldi and Vinay Nangia and Denis Nash and Haseeb Nawaz and Chakib Nejjari and Frida Namnyak Ngalesoni and Jean de Dieu Ngirabega and Quyen Le Nguyen and Muhammad Imran Nisar and Ole F Norheim and Rosana E Norman and Luke Nyakarahuka and Felix Akpojene Ogbo and In-Hwan Oh and Foluke Adetola Ojelabi and Bolajoko Olubukunola Olusanya and Jacob Olusegun Olusanya and John Nelson Opio and Eyal Oren and Erika Ota and Hye-Youn Park and Jae-Hyun Park and Snehal T Patil and Scott B Patten and Vinod K Paul and Katherine Pearson and Emmanuel Kwame Peprah and David M Pereira and Norberto Perico and Konrad Pesudovs and Max Petzold and Michael Robert Phillips and Julian David Pillay and Dietrich Plass and Suzanne Polinder and Farshad Pourmalek and David M Prokop and Mostafa Qorbani and Anwar Rafay and Kazem Rahimi and Vafa Rahimi-Movaghar and Mahfuzar Rahman and Mohammad Hifz Ur Rahman and Sajjad Ur Rahman and Rajesh Kumar Rai and Sasa Rajsic and Usha Ram and Saleem M Rana and Paturi Vishnupriya Rao and Giuseppe Remuzzi and David Rojas-Rueda and Luca Ronfani and Gholamreza Roshandel and Ambuj Roy and George Mugambage Ruhago and Mohammad Yahya Saeedi and Rajesh Sagar and Muhammad Muhammad Saleh and Juan R Sanabria and Itamar S Santos and Rodrigo Sarmiento-Suarez and Benn Sartorius and Monika Sawhney and Aletta E Schutte and David C Schwebel and Soraya Seedat and Sadaf G Sepanlou and Edson E Servan-Mori and Masood Ali Shaikh and Rajesh Sharma and Jun She and Sara Sheikhbahaei and Jiabin Shen and Kenji Shibuya and Hwashin Hyun Shin and Inga Dora Sigfusdottir and Naris Silpakit and Diego Augusto Santos Silva and Dayane Gabriele Alves Silveira and Edgar P Simard and Shireen Sindi and Jasvinder A Singh and Om Prakash Singh and Prashant Kumar Singh and Vegard Skirbekk and Karen Sliwa and Samir Soneji and Reed J D Sorensen and Joan B Soriano and David O Soti and Chandrashekhar T Sreeramareddy and Vasiliki Stathopoulou and Nicholas Steel and Bruno F Sunguya and Soumya Swaminathan and Bryan L Sykes and Rafael Tabarés-Seisdedos and Roberto Tchio Talongwa and Mohammad Tavakkoli and Bineyam Taye and Bemnet Amare Tedla and Tesfaye Tekle and Girma Temam Shifa and Awoke Misganaw Temesgen and Abdullah Sulieman Terkawi and Fisaha Haile Tesfay and Gizachew Assefa Tessema and Kiran Thapa and Alan J Thomson and Andrew L Thorne-Lyman and Ruoyan Tobe-Gai and Roman Topor-Madry and Jeffrey Allen Towbin and Bach Xuan Tran and Zacharie Tsala Dimbuene and Nikolaos Tsilimparis and Abera Kenay Tura and Kingsley Nnanna Ukwaja and Chigozie Jesse Uneke and Olalekan A Uthman and N Venketasubramanian and Sergey K Vladimirov and Vasiliy Victorovich Vlassov and Stein Emil Vollset and Linhong Wang and Elisabete Weiderpass and Robert G Weintraub and Andrea Werdecker and Ronny Westerman and Tissa Wijeratne and James D Wilkinson and Charles Shey Wiysonge and Charles D A Wolfe and Sungho Won and John Q Wong and Gelin Xu and Ajit Kumar Yadav and Bereket Yakob and Ayalnesh Zemene Yalew and Yuichiro Yano and Mehdi Yaseri and Henock Gebremedhin Yebyo and Paul Yip and Naohiro Yonemoto and Seok-Jun Yoon and Mustafa Z Younis and Chuanhua Yu and Shicheng Yu and Zoubida Zaidi and Maysaa El Sayed Zaki and Hajo Zeeb and Hao Zhang and Yong Zhao and Sanjay Zodpey and Leo Zoeckler and Liesl Joanna Zuhlke and Alan D Lopez and Christopher J L Murray",
abstract = "Summary
Background
Timely assessment of the burden of HIV/AIDS is essential for policy setting and programme evaluation. In this report from the Global Burden of Disease Study 2015 (GBD 2015), we provide national estimates of levels and trends of HIV/AIDS incidence, prevalence, coverage of antiretroviral therapy (ART), and mortality for 195 countries and territories from 1980 to 2015.
Methods
For countries without high-quality vital registration data, we estimated prevalence and incidence with data from antenatal care clinics and population-based seroprevalence surveys, and with assumptions by age and sex on initial CD4 distribution at infection, CD4 progression rates (probability of progression from higher to lower CD4 cell-count category), on and off antiretroviral therapy (ART) mortality, and mortality from all other causes. Our estimation strategy links the GBD 2015 assessment of all-cause mortality and estimation of incidence and prevalence so that for each draw from the uncertainty distribution all assumptions used in each step are internally consistent. We estimated incidence, prevalence, and death with GBD versions of the Estimation and Projection Package (EPP) and Spectrum software originally developed by the Joint United Nations Programme on HIV/AIDS (UNAIDS). We used an open-source version of EPP and recoded Spectrum for speed, and used updated assumptions from systematic reviews of the literature and GBD demographic data. For countries with high-quality vital registration data, we developed the cohort incidence bias adjustment model to estimate HIV incidence and prevalence largely from the number of deaths caused by HIV recorded in cause-of-death statistics. We corrected these statistics for garbage coding and HIV misclassification.
Findings
Global HIV incidence reached its peak in 1997, at 3·3 million new infections (95% uncertainty interval [UI] 3·1–3·4 million). Annual incidence has stayed relatively constant at about 2·6 million per year (range 2·5–2·8 million) since 2005, after a period of fast decline between 1997 and 2005. The number of people living with HIV/AIDS has been steadily increasing and reached 38·8 million (95% UI 37·6–40·4 million) in 2015. At the same time, HIV/AIDS mortality has been declining at a steady pace, from a peak of 1·8 million deaths (95% UI 1·7–1·9 million) in 2005, to 1·2 million deaths (1·1–1·3 million) in 2015. We recorded substantial heterogeneity in the levels and trends of HIV/AIDS across countries. Although many countries have experienced decreases in HIV/AIDS mortality and in annual new infections, other countries have had slowdowns or increases in rates of change in annual new infections.
Interpretation
Scale-up of ART and prevention of mother-to-child transmission has been one of the great successes of global health in the past two decades. However, in the past decade, progress in reducing new infections has been slow, development assistance for health devoted to HIV has stagnated, and resources for health in low-income countries have grown slowly. Achievement of the new ambitious goals for HIV enshrined in Sustainable Development Goal 3 and the 90-90-90 UNAIDS targets will be challenging, and will need continued efforts from governments and international agencies in the next 15 years to end AIDS by 2030.
Funding
Bill & Melinda Gates Foundation, and National Institute of Mental Health and National Institute on Aging, National Institutes of Health."
}
@article{SATARIC2016411,
title = "Hybrid OpenMP/MPI programs for solving the time-dependent Gross–Pitaevskii equation in a fully anisotropic trap",
journal = "Computer Physics Communications",
volume = "200",
pages = "411 - 417",
year = "2016",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2015.12.006",
url = "http://www.sciencedirect.com/science/article/pii/S0010465515004440",
author = "Bogdan Satarić and Vladimir Slavnić and Aleksandar Belić and Antun Balaž and Paulsamy Muruganandam and Sadhan K. Adhikari",
keywords = "Bose–Einstein condensate, Gross–Pitaevskii equation, Split-step Crank–Nicolson scheme, Real- and imaginary-time propagation, C program, MPI, OpenMP, Partial differential equation",
abstract = "We present hybrid OpenMP/MPI (Open Multi-Processing/Message Passing Interface) parallelized versions of earlier published C programs (Vudragović et al. 2012) for calculating both stationary and non-stationary solutions of the time-dependent Gross–Pitaevskii (GP) equation in three spatial dimensions. The GP equation describes the properties of dilute Bose–Einstein condensates at ultra-cold temperatures. Hybrid versions of programs use the same algorithms as the C ones, involving real- and imaginary-time propagation based on a split-step Crank–Nicolson method, but consider only a fully-anisotropic three-dimensional GP equation, where algorithmic complexity for large grid sizes necessitates parallelization in order to reduce execution time and/or memory requirements per node. Since distributed memory approach is required to address the latter, we combine MPI programming paradigm with existing OpenMP codes, thus creating fully flexible parallelism within a combined distributed/shared memory model, suitable for different modern computer architectures. The two presented C/OpenMP/MPI programs for real- and imaginary-time propagation are optimized and accompanied by a customizable makefile. We present typical scalability results for the provided OpenMP/MPI codes and demonstrate almost linear speedup until inter-process communication time starts to dominate over calculation time per iteration. Such a scalability study is necessary for large grid sizes in order to determine optimal number of MPI nodes and OpenMP threads per node.
New version program summary
Program title: GP-SCL-HYB package, consisting of: (i) imagtime3d-hyb, (ii) realtime3d-hyb. Catalogue identifier: AEDU_v3_0 Program Summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDU_v3_0.html Program obtainable from: CPC Program Library, Queen’s University of Belfast, N. Ireland. Licensing provisions: Apache License 2.0 No. of lines in distributed program, including test data, etc.: 26397. No. of bytes in distributed program, including test data, etc.: 161195. Distribution format: tar.gz. Programming language: C/OpenMP/MPI. Computer: Any modern computer with C language, OpenMP- and MPI-capable compiler installed. Operating system: Linux, Unix, Mac OS X, Windows. RAM: Total memory required to run programs with the supplied input files, distributed over the used MPI nodes: (i) 310 MB, (ii) 400 MB. Larger grid sizes require more memory, which scales with Nx*Ny*Nz. Number of processors used: No limit, from one to all available CPU cores can used on all MPI nodes. Number of nodes used: No limit on the number of MPI nodes that can be used. Depending on the grid size of the physical problem and communication overheads, optimal number of MPI nodes and threads per node can be determined by a scalability study for a given hardware platform. Classification: 2.9, 4.3, 4.12. Catalogue identifier of previous version: AEDU_v2_0. Journal reference of previous version: Comput. Phys. Commun. 183 (2012) 2021. Does the new version supersede the previous version?: No. Nature of problem: These programs are designed to solve the time-dependent Gross–Pitaevskii (GP) nonlinear partial differential equation in three spatial dimensions in a fully anisotropic trap using a hybrid OpenMP/MPI parallelization approach. The GP equation describes the properties of a dilute trapped Bose–Einstein condensate. Solution method: The time-dependent GP equation is solved by the split-step Crank–Nicolson method using discretization in space and time. The discretized equation is then solved by propagation, in either imaginary or real time, over small time steps. The method yields solutions of stationary and/or non-stationary problems. Reasons for the new version: Previous C [1] and Fortran [2] programs are widely used within the ultracold atoms and nonlinear optics communities, as well as in various other fields [3]. This new version represents extension of the two previously OpenMP-parallelized programs (imagtime3d-th and realtime3d-th) for propagation in imaginary and real time in three spatial dimensions to a hybrid, fully distributed OpenMP/MPI programs (imagtime3d-hyb and realtime3d-hyb). Hybrid extensions of previous OpenMP codes enable interested researchers to numerically study Bose–Einstein condensates in much greater detail (i.e., with much finer resolution) than with OpenMP codes. In OpenMP (threaded) versions of programs, numbers of discretization points in X, Y, and Z directions are bound by the total amount of available memory on a single computing node where the code is being executed. New, hybrid versions of programs are not limited in this way, as large numbers of grid points in each spatial direction can be evenly distributed among the nodes of a cluster, effectively distributing required memory over many MPI nodes. This is the first reason for development of hybrid versions of 3d codes. The second reason for new versions is speedup in the execution of numerical simulations that can be gained by using multiple computing nodes with OpenMP/MPI codes. Summary of revisions: Two C/OpenMP programs in three spatial dimensions from previous version [1] of the codes (imagtime3d-th and realtime3d-th) are transformed and rewritten into a hybrid OpenMP/MPI programs and named imagtime3d-hyb and realtime3d-hyb. The overall structure of two programs is identical. The directory structure of the GP-SCL-HYB package is extended compared to the previous version and now contains a folder scripts, where examples of scripts that can be used to run the programs on a typical MPI cluster are given. The corresponding readme.txt file contains more details. We have also included a makefile with tested and verified settings for most popular MPI compliers, including OpenMPI (Open Message Passing Interface) [4] and MPICH (Message Passing Interface Chameleon) [5]. Transformation from pure OpenMP to a hybrid OpenMP/MPI approach has required that the array containing condensate wavefunction is distributed among MPI nodes of a computer cluster. Several data distribution models have been considered for this purpose, including block distribution and block cyclic distribution of data in a 2d matrix. Finally, we decided to distribute the wavefunction values across different nodes so that each node contains only one slice of the X-dimension data, while containing the complete corresponding Y- and Z-dimension data, as illustrated in Fig. 1. This allows central functions of our numerical algorithm, calcluy, calcuz, and calcnu to be executed purely in parallel on different MPI nodes of a cluster, without any overhead or communication, as nodes contain all the information for Y- and Z-dimension data in the given X-sub-domain. However, the problem arises when functions calclux, calcrms, and calcmuen need to be executed, as they also operate on the whole X-dimension data. Thus, the need for additional communication arises during the execution of the function calcrms, while in the case of functions calclux and calcmuen also the transposition of data between X- and Y-dimensions is necessary, while data in Z dimension have to stay contiguous. Transposition provides nodes with all the necessary X-dimension data to execute functions calclux and calcmuen. However, this needs to be done in each iteration of numerical algorithm, thus necessarily increasing communication overhead of the simulation. Transposition algorithms that were considered where the ones that account for greatest common divisor (GCD) between number of nodes in columns (designated by N) and rows (designated by M) of a cluster configured as 2d mash of nodes [6]. Two of such algorithms have been tested and tried for implementation: the case when GCD=1 and the case when GCD>1. The trivial situation N=M=1 is already covered by the previous, purely OpenMP programs, and therefore, without any loss of generality, we have considered only configurations with number of nodes in X-dimension satisfying N>1. Only the former algorithm (GCD=1) was found to be sound in case where data matrix is not a 2d, but a 3d structure. Latter case was found to be too demanding implementation-wise, since MPI functions and data-types are bound to certain limitations. Therefore, the algorithm with M=1 nodes in Y-dimension was implemented, as depicted by the wavefunction data structure in Fig. 1. Fig. 1BEC wavefunction data structure in the employed algorithm. Data are sliced so that the complete Y- and Z-dimension data reside on a single node for a given range of data in X-dimension, while data in X-dimension are distributed over N=gsize nodes. Figures show data transposition and MPI indexed datatype creation parameters for the case of: (a) sending side and (b) receiving side. Implementation of the algorithm relies on a sliced distribution of data among the nodes, as explained in Fig. 2. This successfully solves the problem of large RAM consumption of 3d codes, which arises even for moderate grid sizes. However, it does not solve the question of data transposition between the nodes. In order to implement the most effective (GCD=1) transposition algorithm according to Ref. [6], we had to carry out block distribution of data within one data slice contained on a single node. This block distribution of data was done implicitly, i.e. data on one node have been put in a single 1d array (psi) of contiguous memory, in which Z-dimension has stride 1, Y-dimension has stride Nz, and X-dimension has stride Ny*Nz. This is different from previous implementation of the programs, where the wavefunction was represented by an explicit 3d array. This change was also introduced in order to more easily form user MPI datatypes, which allow for implicit block distribution of data, and represent 3d blocks of data within 1d data array. These blocks are then swapped between nodes, effectively performing the transposition in X–Y and Y–X directions. Together with transposition of blocks between the nodes, the block data also have to be redistributed. To illustrate how this works, let us consider example shown in Fig. 1(a), where one data block has size (Nx/gisze)*(Ny/gsize)*Nz. It represents one 3d data block, swapped between two nodes of a cluster (through one non-blocking MPI_Isend and one MPI_Ireceive operation), containing (Nx/gsize)*(Ny/gsize) 1d rods of contiguous Nz data. These rods themselves need to be transposed within the transposed block as well. This means that two levels of transpositions need to be performed. At a single block level, rods have to be transposed (as indicated in upper left corner of Fig. 1(a) for sending index type and in Fig. 1(b) for receiving index type). Second level is transposition of blocks between different nodes, which is depicted by blue arrows connecting different blocks in Fig. 1. The above described transposition is applied whenever needed in the functions calclux and calcmuen, which require calculations to be done on the whole range of data in X-dimension. When performing renormalization of the wavefunction or calculation of its norm, root-mean-square radius, chemical potential, and energy, collective operations MPI_Gather and MPI_Bcast are also used. Fig. 3, Fig. 4 show the scalability results obtained for hybrid versions of programs for small and large grid sizes as a function of number of MPI nodes used. The baseline for calculation of speedups in the execution time for small grid sizes is previous, purely OpenMP programs, while for large grid sizes, which cannot fit onto a single node, the baseline is hybrid programs with minimal configuration runs on 8 nodes. The figures also show efficacies, defined as percentages of measured speedups compared to the ideal ones. We see that an excellent scalability (larger than 80% compared to the ideal one) can be obtained for up to 32 nodes. The tests have been performed on a cluster with nodes containing 2×8-core Sandy Bridge Xeon 2.6 GHz processors with 32 GB of RAM and Infiniband QDR (Quad Data Rate, 40 Gbps) interconnect. We stress that the scalability depends greatly on the ratio between the calculation and communication time per iteration, and has to be studied for a particular type of processors and interconnect technology. Additional comments: This package consists of 2 programs, see Program title above. Both are hybrid, threaded and distributed (OpenMP/MPI parallelized). For the particular purpose of each program, see descriptions below. Fig. 2Creation of a user-defined MPI datatype indextype with the function MPI_Type_indexed. Here, count represents the number of blocks, blocklengths array contains lengths of each block, and displacements array contains the displacement of each block from the beginning of the corresponding data structure. For example, if an array of double precision numbers (designated as buffer in the figure) is sent by MPI_Send with the datatype set to indextype, it is interpreted as a block-distributed data structure, as specified when indextype was created.Fig. 3Speedup in the execution time and efficacy curves of imagtime3d-hyb and realtime3d-hyb programs as a function of the number of MPI nodes used for small grid sizes. The results are obtained on a cluster with nodes containing 2×8-core Sandy Bridge Xeon 2.6 GHz processors with 32 GB of RAM and Infiniband QDR interconnect: (a) speedup of imagtime3d-hyb on a 240×200×160 grid; (b) efficacy of imagtime3d-hyb on a 240×200×160 grid; (c) speedup of realtime3d-hyb on a 200×160×120 grid; (d) efficacy of realtime3d-hyb on a 200×160×120 grid. Shaded areas in graphs (b) and (d) represent high-efficacy regions, where speedup is at least 80% of the ideal one. Running time: All running times given in descriptions below refer to programs compiled with OpenMPI/GCC compiler and executed on 8–32 nodes with 2×8-core Sandy Bridge Xeon 2.6 GHz processors with 32 GB of RAM and Infiniband QDR interconnect. With the supplied input files for small grid sizes, running wallclock times of several minutes are required on 8–10 MPI nodes. Special features: (1) Since the condensate wavefunction data are distributed among the MPI nodes, when writing wavefunction output files each MPI process saves its data into a separate file, to avoid I/O issues. Concatenating the corresponding files from all MPI processes will create the complete wavefunction file. (2) Due to a known bug in OpenMPI up to version 1.8.4, allocation of memory for indexed datatype on a single node for large grids (such as 800×640×480) may fail. The fix for this bug is already in 3c489ea branch and is fixed in OpenMPI as of version 1.8.5. Fig. 4Speedup in the execution time and efficacy curves of imagtime3d-hyb and realtime3d-hyb programs as a function of the number of MPI nodes used for large grid sizes. The results are obtained on a cluster with nodes containing 2×8-core Sandy Bridge Xeon 2.6 GHz processors with 32 GB of RAM and Infiniband QDR interconnect: (a) speedup of imagtime3d-hyb on a 1920×1600×1280 grid; (b) efficacy of imagtime3d-hyb on a 1920×1600×1280 grid; (c) speedup of realtime3d-hyb on a 1600×1280×960 grid; (d) efficacy of realtime3d-hyb on a 1600×1280×960 grid. Shaded areas in graphs (b) and (d) represent high-efficacy regions, where speedup is at least 80% of the ideal one. Program summary (i) Program title: imagtime3d-hyb. Title of electronic files: imagtime3d-hyb.c, imagtime3d-hyb.h. Computer: Any modern computer with C language, OpenMP- and MPI-capable compiler installed. RAM memory requirements: 300 MBytes of RAM for a small grid size 240×200×160, and scales with Nx*Ny*Nz. This is total amount of memory needed, and is distributed over MPI nodes used for execution. Programming language used: C/OpenMP/MPI. Typical running time: Few minutes with the supplied input files for a small grid size 240×200×160 on 8 nodes. Up to one hour for a large grid size 1920×1600×1280 on 32 nodes (1000 iterations). Nature of physical problem: This program is designed to solve the time-dependent GP nonlinear partial differential equation in three space dimensions with an anisotropic trap. The GP equation describes the properties of a dilute trapped Bose–Einstein condensate. Method of solution: The time-dependent GP equation is solved by the split-step Crank–Nicolson method by discretizing in space and time. The discretized equation is then solved by propagation in imaginary time over small time steps. The method yields solutions of stationary problems. Program summary (ii) Program title: realtime3d-hyb. Title of electronic files: realtime3d-hyb.c, realtime3d-hyb.h. Computer: Any modern computer with C language, OpenMP- and MPI-capable compiler installed. RAM memory requirements: 410 MBytes of RAM for a small grid size 200×160×120, and scales with Nx*Ny*Nz. This is total amount of memory needed, and is distributed over MPI nodes used for execution. Programming language used: C/OpenMP/MPI. Typical running time: 10–15 min with the supplied input files for a small grid size 200×160×120 on 10 nodes. Up to one hour for a large grid size 1600×1280×960 on 32 nodes (1000 iterations). Nature of physical problem: This program is designed to solve the time-dependent GP nonlinear partial differential equation in three space dimensions with an anisotropic trap. The GP equation describes the properties of a dilute trapped Bose–Einstein condensate. Method of solution: The time-dependent GP equation is solved by the split-step Crank–Nicolson method by discretizing in space and time. The discretized equation is then solved by propagation in real time over small time steps. The method yields solutions of stationary and non-stationary problems. Acknowledgments B.S., V.S., A.B., and A.B. acknowledge support by the Ministry of Education, Science, and Technological Development of the Republic of Serbia under projects ON171017, III43007, ON171009, ON174027 and IBEC, and by DAAD - German Academic and Exchange Service under project IBEC. P.M. acknowledges support by the Science and Engineering Research Board, Department of Science and Technology, Government of India under project No. EMR/2014/000644. S.K.A. acknowledges support by the CNPq of Brazil under project 303280/2014-0, and by the FAPESP of Brazil under project 2012/00451-0. Numerical simulations were run on the PARADOX supercomputing facility at the Scientific Computing Laboratory of the Institute of Physics Belgrade, supported in part by the Ministry of Education, Science, and Technological Development of the Republic of Serbia under project ON171017. References[1]D. Vudragović, I. Vidanović, A. Balaž, P. Muruganandam, S. K. Adhikari, C programs for solving the time-dependent Gross–Pitaevskii equation in a fully anisotropic trap, Comput. Phys. Commun. 183 (2012) 2021.[2]P. Muruganandam and S. K. Adhikari, Fortran programs for the time-dependent Gross–Pitaevskii equation in a fully anisotropic trap, Comput. Phys. Commun. 180 (2009) 1888.[3]R. K. Kumar and P. Muruganandam, J. Phys. B: At. Mol. Opt. Phys. 45 (2012) 215301;L. E. Young-S. and S. K. Adhikari, Phys. Rev. A 86 (2012) 063611;S. K. Adhikari, J. Phys. B: At. Mol. Opt. Phys. 45 (2012) 235303;I. Vidanović, N. J. van Druten, and M. Haque, New J. Phys. 15 (2013) 035008;S. Balasubramanian, R. Ramaswamy, and A. I. Nicolin, Rom. Rep. Phys. 65 (2013) 820;L. E. Young-S. and S. K. Adhikari, Phys. Rev. A 87 (2013) 013618;H. Al-Jibbouri, I. Vidanovic, A. Balaz, and A. Pelster, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 065303;X. Antoine, W. Bao, and C. Besse, Comput. Phys. Commun. 184 (2013) 2621;B. Nikolić, A. Balaž, and A. Pelster, Phys. Rev. A 88 (2013) 013624;H. Al-Jibbouri and A. Pelster, Phys. Rev. A 88 (2013) 033621;S. K. Adhikari, Phys. Rev. A 88 (2013) 043603;J. B. Sudharsan, R. Radha, and P. Muruganandam, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 155302;R. R. Sakhel, A. R. Sakhel, and H. B. Ghassib, J. Low Temp. Phys. 173 (2013) 177;E. J. M. Madarassy and V. T. Toth, Comput. Phys. Commun. 184 (2013) 1339;R. K. Kumar, P. Muruganandam, and B. A. Malomed, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 175302;W. Bao, Q. Tang, and Z. Xu, J. Comput. Phys. 235 (2013) 423;A. I. Nicolin, Proc. Rom. Acad. Ser. A-Math. Phys. 14 (2013) 35;R. M. Caplan, Comput. Phys. Commun. 184 (2013) 1250;S. K. Adhikari, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 115301;Ž. Marojević, E. Göklü, and C. Lämmerzahl, Comput. Phys. Commun. 184 (2013) 1920;X. Antoine and R. Duboscq, Comput. Phys. Commun. 185 (2014) 2969;S. K. Adhikari and L. E. Young-S, J. Phys. B: At. Mol. Opt. Phys. 47 (2014) 015302;K. Manikandan, P. Muruganandam, M. Senthilvelan, and M. Lakshmanan, Phys. Rev. E 90 (2014) 062905;S. K. Adhikari, Phys. Rev. A 90 (2014) 055601;A. Balaž, R. Paun, A. I. Nicolin, S. Balasubramanian, and R. Ramaswamy, Phys. Rev. A 89 (2014) 023609;S. K. Adhikari, Phys. Rev. A 89 (2014) 013630;J. Luo, Commun. Nonlinear Sci. Numer. Simul. 19 (2014) 3591;S. K. Adhikari, Phys. Rev. A 89 (2014) 043609;K.-T. Xi, J. Li, and D.-N. Shi, Physica B 436 (2014) 149;M. C. Raportaru, J. Jovanovski, B. Jakimovski, D. Jakimovski, and A. Mishev, Rom. J. Phys. 59 (2014) 677;S. Gautam and S. K. Adhikari, Phys. Rev. A 90 (2014) 043619;A. I. Nicolin, A. Balaž, J. B. Sudharsan, and R. Radha, Rom. J. Phys. 59 (2014) 204;K. Sakkaravarthi, T. Kanna, M. Vijayajayanthi, and M. Lakshmanan, Phys. Rev. E 90 (2014) 052912;S. K. Adhikari, J. Phys. B: At. Mol. Opt. Phys. 47 (2014) 225304;R. K. Kumar and P. Muruganandam, Numerical studies on vortices in rotating dipolar Bose–Einstein condensates, Proceedings of the 22nd International Laser Physics Workshop, J. Phys. Conf. Ser. 497 (2014) 012036;A. I. Nicolin and I. Rata, Density waves in dipolar Bose–Einstein condensates by means of symbolic computations, High-Performance Computing Infrastructure for South East Europe’s Research Communities: Results of the HP-SEE User Forum 2012, in Springer Series: Modeling and Optimization in Science and Technologies 2 (2014) 15;S. K. Adhikari, Phys. Rev. A 89 (2014) 043615;R. K. Kumar and P. Muruganandam, Eur. Phys. J. D 68 (2014) 289;J. B. Sudharsan, R. Radha, H. Fabrelli, A. Gammal, and B. A. Malomed, Phys. Rev. A 92 (2015) 053601;S. K. Adhikari, J. Phys. B: At. Mol. Opt. Phys. 48 (2015) 165303;F. I. Moxley III, T. Byrnes, B. Ma, Y. Yan, and W. Dai, J. Comput. Phys. 282 (2015) 303;S. K. Adhikari, Phys. Rev. E 92 (2015) 042926;R. R. Sakhel, A. R. Sakhel, and H. B. Ghassib, Physica B 478 (2015) 68;S. Gautam and S. K. Adhikari, Phys. Rev. A 92 (2015) 023616;D. Novoa, D. Tommasini, and J. A. Nóvoa-López, Phys. Rev. E 91 (2015) 012904;S. Gautam and S. K. Adhikari, Laser Phys. Lett. 12 (2015) 045501;K.-T. Xi, J. Li, and D.-N. Shi, Physica B 459 (2015) 6;R. K. Kumar, L. E. Young-S., D. Vudragović, A. Balaž, P. Muruganandam, and S. K. Adhikari, Comput. Phys. Commun. 195 (2015) 117;S. Gautam and S. K. Adhikari, Phys. Rev. A 91 (2015) 013624;A. I. Nicolin, M. C. Raportaru, and A. Balaž, Rom. Rep. Phys. 67 (2015) 143;S. Gautam and S. K. Adhikari, Phys. Rev. A 91 (2015) 063617;E. J. M. Madarassy and V. T. Toth, Phys. Rev. D 91 (2015) 044041.[4]Open Message Passing Interface (OpenMPI), http://www.open-mpi.org/ (2015).[5]Message Passing Interface Chameleon (MPICH), https://www.mpich.org/ (2015).[6]J. Choi, J. J. Dongarra, D. W. Walker, Parallel matrix transpose algorithms on distributed memory concurrent computers, Parallel Comput. 21 (1995) 1387."
}
@article{ALSHAER2015873,
title = "Numerical investigations of using carbon foam/PCM/Nano carbon tubes composites in thermal management of electronic equipment",
journal = "Energy Conversion and Management",
volume = "89",
pages = "873 - 884",
year = "2015",
issn = "0196-8904",
doi = "https://doi.org/10.1016/j.enconman.2014.10.045",
url = "http://www.sciencedirect.com/science/article/pii/S0196890414009261",
author = "W.G. Alshaer and S.A. Nada and M.A. Rady and Cedric Le Bot and Elena Palomo Del Barrio",
keywords = "Thermal management, Electronic equipment, PCM, Nano carbon tubes",
abstract = "A numerical investigation of predicting thermal characteristics of electronic equipment using carbon foam matrix saturated with phase change material (PCM) and Nano carbon tubes as thermal management modules is presented. To study the effect of insertion of RT65 and Nano carbon tubes in carbon foam matrices of different porosities, three different modules; namely Pure CF-20, CF20+RT65, and CF-20+RT65/Nano carbon modules are numerically tested at different values of carbon foam porosities. Mathematical model is obtained using volume averaging technique based on single-domain energy equation and a control volume based numerical scheme. Interfacial effects influencing heat transfer process at enclosure wall, module surface and different interfacial surfaces within the composite have been addressed. Governing equations have been solved using a CFD code (Thétis, http://thetis.enscbp.fr). Mathematical model is validated by comparing its prediction with previous experimental measurements for pure CF-20 foam and CF-20+RT65 composite modules. The model is used to predict thermal characteristics of CF-20+RT65/Nano carbon tubes composite as a thermal management modules. Results reveal that insertion of RT65/MWCNTs in CF-20 leads to a 11.5% reduction in the module surface temperature for carbon foam porosities less than 75%. The reduction decrease to 7.8% for a porosity of 88%. Numerical results of transient and steady state temperature histories at different depths within the module are compared with previous experimental data and fair agreement is obtained."
}
@article{POORTVLIET201572,
title = "A dated molecular phylogeny of manta and devil rays (Mobulidae) based on mitogenome and nuclear sequences",
journal = "Molecular Phylogenetics and Evolution",
volume = "83",
pages = "72 - 85",
year = "2015",
issn = "1055-7903",
doi = "https://doi.org/10.1016/j.ympev.2014.10.012",
url = "http://www.sciencedirect.com/science/article/pii/S1055790314003637",
author = "Marloes Poortvliet and Jeanine L. Olsen and Donald A. Croll and Giacomo Bernardi and Kelly Newton and Spyros Kollias and John O’Sullivan and Daniel Fernando and Guy Stevens and Felipe Galván Magaña and Bernard Seret and Sabine Wintner and Galice Hoarau",
keywords = "Mitogenome, Phylogenetics, Molecular clock, Divergence times, , ",
abstract = "Manta and devil rays are an iconic group of globally distributed pelagic filter feeders, yet their evolutionary history remains enigmatic. We employed next generation sequencing of mitogenomes for nine of the 11 recognized species and two outgroups; as well as additional Sanger sequencing of two mitochondrial and two nuclear genes in an extended taxon sampling set. Analysis of the mitogenome coding regions in a Maximum Likelihood and Bayesian framework provided a well-resolved phylogeny. The deepest divergences distinguished three clades with high support, one containing Manta birostris, Manta alfredi, Mobula tarapacana, Mobula japanica and Mobula mobular; one containing Mobula kuhlii, Mobula eregoodootenkee and Mobula thurstoni; and one containing Mobula munkiana, Mobula hypostoma and Mobula rochebrunei. Mobula remains paraphyletic with the inclusion of Manta, a result that is in agreement with previous studies based on molecular and morphological data. A fossil-calibrated Bayesian random local clock analysis suggests that mobulids diverged from Rhinoptera around 30 Mya. Subsequent divergences are characterized by long internodes followed by short bursts of speciation extending from an initial episode of divergence in the Early and Middle Miocene (19–17 Mya) to a second episode during the Pliocene and Pleistocene (3.6 Mya – recent). Estimates of divergence dates overlap significantly with periods of global warming, during which upwelling intensity – and related high primary productivity in upwelling regions – decreased markedly. These periods are hypothesized to have led to fragmentation and isolation of feeding regions leading to possible regional extinctions, as well as the promotion of allopatric speciation. The closely shared evolutionary history of mobulids in combination with ongoing threats from fisheries and climate change effects on upwelling and food supply, reinforces the case for greater protection of this charismatic family of pelagic filter feeders."
}
@article{SANDEL2014166,
title = "Interspecific relationships and the evolution of sexual dimorphism in pygmy sunfishes (Centrarchidae: Elassoma)",
journal = "Molecular Phylogenetics and Evolution",
volume = "77",
pages = "166 - 176",
year = "2014",
issn = "1055-7903",
doi = "https://doi.org/10.1016/j.ympev.2014.04.018",
url = "http://www.sciencedirect.com/science/article/pii/S1055790314001468",
author = "Michael Sandel and Fritz C. Rohde and Phillip M. Harris",
keywords = ", Centrarchidae, Cytochrome , Rhodopsin, PICH, Sexual dimorphism",
abstract = "The genus Elassoma represents a small but unique component of the aquatic biodiversity hotspot in southeastern North America. We present the first phylogeny of the seven described species, corroborated by sequence data from mitochondrial and nuclear protein coding genes. This analysis reveals a Coastal Plain clade sister to the geographically isolated, and federally protected, Elassoma alabamae. The Coastal Plain clade contains the widespread E. zonatum, which is sister to a clade primarily restricted to lowland Neogene subprovinces. We analyzed morphometric data in a phylogenetic context to illustrate the evolution of sexual shape dimorphism within the genus. Sixteen univariate and three multivariate traits were tested for significant sexual dimorphism for each species, and relative transformation rates were inferred from the time tree. A simple index of interspecific sexual dimorphism revealed greater disparity among sympatric species comparisons than among allopatric comparisons. Results implicate geology as a primary factor influencing ecological diversification, and sexual selection as a mechanism reinforcing reproductive isolation in areas of secondary contact. We discuss putative roles of geological history and sexual selection in the generation and maintenance of the aquatic biodiversity gradient in southeastern North America."
}
@article{BOSCHMAN2015250,
title = "Exploring teachers' use of TPACK in design talk: The collaborative design of technology-rich early literacy activities",
journal = "Computers & Education",
volume = "82",
pages = "250 - 262",
year = "2015",
issn = "0360-1315",
doi = "https://doi.org/10.1016/j.compedu.2014.11.010",
url = "http://www.sciencedirect.com/science/article/pii/S0360131514002577",
author = "Ferry Boschman and Susan McKenney and Joke Voogt",
keywords = "TPACK, Design talk, Collaborative design, ICT learning activities",
abstract = "Research shows the benefits of collaborative design activity by teachers are that in their conversations (design talk) they develop technological pedagogical content knowledge (TPACK). While more and more teachers engage in collaborative design, little is known about how they use TPACK during design. The main question of this study was: “What is the nature of design talk of a group of teachers during the design of technology-rich early literacy activities?” Using a holistic case study on design talk, the analysis focused on the topics that were under discussion and how these topics were discussed. Three phases of coding were applied: (a) how design represents any of the seven domains of TPACK knowledge (Pedagogical, Content, Technological, Technological Pedagogical, Technological Content, Pedagogical Content or Technological Pedagogical Content Knowledge); (b), how design talk represented three aspects of reasoning (external priorities, practical concerns and existing orientations); and (c), and what levels of inquiry are reached (no-depth; sharing ideas; analyze; and plan). Findings indicate that design talk reflects moments in which teachers reach deeper levels of inquiry. Findings also indicate that TPACK was mostly linked to expressing practical concerns. However when engaging in deeper inquiry, teachers existing orientations featured more prominently in the conversations. External priorities hardly seemed to play any role in design talk. Also, when addressing TPACK or PCK, design talk mostly reflects practical concerns. Pedagogy was addressed not as a single knowledge domain, rather in conjunction with the other two domains. Practical implications are discussed regarding how to support teachers during collaborative design."
}
@article{JOUTSA20121992,
title = "Mesolimbic dopamine release is linked to symptom severity in pathological gambling",
journal = "NeuroImage",
volume = "60",
number = "4",
pages = "1992 - 1999",
year = "2012",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2012.02.006",
url = "http://www.sciencedirect.com/science/article/pii/S1053811912001838",
author = "Juho Joutsa and Jarkko Johansson and Solja Niemelä and Antti Ollikainen and Mika M. Hirvonen and Petteri Piepponen and Eveliina Arponen and Hannu Alho and Valerie Voon and Juha O. Rinne and Jarmo Hietala and Valtteri Kaasinen",
keywords = "Dopamine, Reward, Raclopride, PET, Addiction",
abstract = "Background
Brain dopamine neurons code rewarding environmental stimuli by releasing endogenous dopamine, a transmission signal that is important for reinforcement learning. Human reward-seeking gambling behavior, and especially pathological gambling, has been presumed to be modulated by brain dopamine.
Methods
Striatal dopamine release was studied with [11C]raclopride positron emission tomography (PET) during gambling with an ecologically valid slot machine gambling task. Twenty-four males with and without pathological gambling (DSM-IV) were scanned three times, and the effects of different gambling outcomes (high-reward and low-reward vs. control task) on dopamine release were evaluated.
Results
Striatal dopamine was released in both groups during high-reward but also low-reward tasks. The dopamine release during the low-reward task was located in the associative part of the caudate nucleus. During the high-reward task, the effect was also seen in the ventral striatum and the magnitude of dopamine release was associated with parallel gambling “high”. Furthermore, there was a positive correlation between dopamine release during the low-reward and the high-reward task. There was no general difference in the magnitude of dopamine release between pathological gamblers and controls. However, in pathological gamblers, dopamine release correlated positively with gambling symptom severity.
Conclusions
Striatal dopamine is released during gambling irrespective of gambling outcome suggesting that the mere expectation/prediction of reward is sufficient to induce dopaminergic changes. Although dopamine release during slot machine gambling is comparable between healthy controls and pathological gamblers, greater gambling symptom severity is associated with greater dopaminergic responses. Thus, as the dopamine reward deficiency theory predicts blunted mesolimbic dopamine responses to gambling in addicted individuals, our results question the validity of the reward deficiency hypothesis in pathological gambling."
}
@article{WU2013943,
title = "What factors influence midwives' decision to perform or avoid episiotomies? A focus group study",
journal = "Midwifery",
volume = "29",
number = "8",
pages = "943 - 949",
year = "2013",
issn = "0266-6138",
doi = "https://doi.org/10.1016/j.midw.2012.11.017",
url = "http://www.sciencedirect.com/science/article/pii/S0266613812002264",
author = "Lin Chieh Wu and Désirée Lie and Rahul Malhotra and John C. Allen and Julie S.L. Tay and Thiam Chye Tan and Truls Østbye",
keywords = "Episiotomy practice, Midwife beliefs, Focus group, Singapore",
abstract = "Objective
to explore midwives' reasons for performing or avoiding episiotomies and motivation to change episiotomy practice in a large tertiary maternity hospital.
Design
using purposive sampling, three focus groups were conducted to achieve theme saturation. Open-ended questions elicited personal reasons for performing or avoiding episiotomy, information sources, and opinions about past and future practice trends. Sessions were audiotaped, and transcripts independently examined by three researchers who coded for themes. An iterative process was used to achieve consensus. Grounded theory was used to interpret data and to derive a theoretical framework for understanding the reasoning that influences episiotomy practice.
Setting
a high volume delivery unit in Singapore.
Participants
20 of 79 licensed midwives, aged 28–70, who performed independent deliveries at the delivery unit.
Findings
participants recognised maternal, fetal and other factors affecting their own decision to perform episiotomies. Patient request, better healing, midwife's reputation and job satisfaction were cited as main reasons to avoid episiotomy. Key sources informing practice were past training, delivery experience, anecdotal learning and lack of a protocol. There was no consensus on current trends in episiotomy practice. There was an absence of recognition of individual roles in reducing episiotomy rates. Clinicians were perceived as having both positive and negative influence.
Conclusions
midwives' reasons for performing episiotomies were attributed to midwifery training, fear of doing harm and perceived clinician expectation, and were not consistent with current international practice guidelines. Reasons for avoiding episiotomies were associated with patient-centeredness and job satisfaction. Midwives agreed on the need to reduce episiotomy rates.
Implications for practice
with reduction in episiotomy rates as a goal, a combination of guideline education, feedback, peer coaching and collaborative care with doctors may be needed to achieve desired outcomes. Views and experiences of midwives should also be incorporated into strategies to change episiotomy practice."
}
@article{FULLER2014106,
title = "Application of a theoretical framework for behavior change to hospital workers’ real-time explanations for noncompliance with hand hygiene guidelines",
journal = "American Journal of Infection Control",
volume = "42",
number = "2",
pages = "106 - 110",
year = "2014",
issn = "0196-6553",
doi = "https://doi.org/10.1016/j.ajic.2013.07.019",
url = "http://www.sciencedirect.com/science/article/pii/S0196655313011565",
author = "Chris Fuller and Sarah Besser and Joanne Savage and John McAteer and Sheldon Stone and Susan Michie",
keywords = "Behavioral theory, Noncompliance",
abstract = "Background
Insufficient use of behavioral theory to understand health care workers’ (HCWs) hand hygiene compliance may result in suboptimal design of hand hygiene interventions and limit effectiveness. Previous studies examined HCWs’ intended, rather than directly observed, compliance and/or focused on just 1 behavioral model. This study examined HCWs’ explanations of noncompliance in “real time” (immediately after observation), using a behavioral theory framework, to inform future intervention design.
Methods
HCWs were directly observed and asked to explain episodes of noncompliance in “real-time.” Explanations were recorded, coded into 12 behavioral domains, using the Theory Domains Framework, and subdivided into themes.
Results
Over two-thirds of 207 recorded explanations were explained by 2 domains. These were “Memory/Attention/Decision Making” (87, 44%), subdivided into 3 themes (memory, loss of concentration, and distraction by interruptions), and “Knowledge” (55, 26%), with 2 themes relating to specific hand hygiene indications. No other domain accounted for more than 18 (9%) explanations.
Conclusion
An explanation of HCW’s “real-time” explanations for noncompliance identified “Memory/Attention/Decision Making” and “Knowledge” as the 2 behavioral domains commonly linked to noncompliance. This suggests that hand hygiene interventions should target both automatic associative learning processes and conscious decision making, in addition to ensuring good knowledge. A theoretical framework to investigate HCW’s “real-time” explanations of noncompliance provides a coherent way to design hand hygiene interventions."
}
@article{MARTINELLI2014293,
title = "Phylogeny and population dynamics of respiratory syncytial virus (Rsv) A and B",
journal = "Virus Research",
volume = "189",
pages = "293 - 302",
year = "2014",
issn = "0168-1702",
doi = "https://doi.org/10.1016/j.virusres.2014.06.006",
url = "http://www.sciencedirect.com/science/article/pii/S0168170214002494",
author = "Marianna Martinelli and Elena Rosanna Frati and Alessandra Zappa and Erika Ebranati and Silvia Bianchi and Elena Pariani and Antonella Amendola and Gianguglielmo Zehender and Elisabetta Tanzi",
keywords = "Respiratory syncytial virus, G protein, Phylogenetic analysis, Selective pressure, Phylodynamic analysis, Evolutionary rate",
abstract = "Respiratory syncytial virus (RSV) is a major cause of lower respiratory tract infections in infants and young children. RSV is characterised by high variability, especially in the G glycoprotein, which may play a significant role in RSV pathogenicity by allowing immune evasion. To reconstruct the origin and phylodynamic history of RSV, we evaluated the genetic diversity and evolutionary dynamics of RSV A and RSV B isolated from children under 3 years old infected in Italy from 2006 to 2012. Phylogenetic analysis revealed that most of the RSV A sequences clustered with the NA1 genotype, and RSV B sequences were included in the Buenos Aires genotype. The mean evolutionary rates for RSV A and RSV B were estimated to be 2.1×10−3 substitutions (subs)/site/year and 3.03×10−3 subs/site/year, respectively. The time of most recent common ancestor for the tree root went back to the 1940s (95% highest posterior density—HPD: 1927–1951) for RSV A and the 1950s (95%HPD: 1951–1960) for RSV B. The RSV A Bayesian skyline plot (BSP) showed a decrease in transmission events ending in about 2005, when a sharp growth restored the original viral population size. RSV B BSP showed a similar trend. Site-specific selection analysis identified 10 codons under positive selection in RSV A sequences and only one site in RSV B sequences. Although RSV remains difficult to control due to its antigenic diversity, it is important to monitor changes in its coding sequences, to permit the identification of future epidemic strains and to implement vaccine and therapy strategies."
}
@article{ESTEVES201552,
title = "Incremental dataflow execution, resource efficiency and probabilistic guarantees with Fuzzy Boolean nets",
journal = "Journal of Parallel and Distributed Computing",
volume = "79-80",
pages = "52 - 66",
year = "2015",
note = "Special Issue on Scalable Systems for Big Data Management and Analytics",
issn = "0743-7315",
doi = "https://doi.org/10.1016/j.jpdc.2015.03.001",
url = "http://www.sciencedirect.com/science/article/pii/S0743731515000507",
author = "Sérgio Esteves and João Nuno Silva and João Paulo Carvalho and Luís Veiga",
keywords = "Workflow, Dataflow, Incremental processing, Continuous processing, Data-intensive, NoSQL, Quality-of-service, Machine learning, Fuzzy logic",
abstract = "Currently, there is a strong need for organizations to analyze and process ever-increasing volumes of data in order to answer to real-time processing demands. Such continuous and data-intensive processing is often achieved through the composition of complex data-intensive workflows (i.e., dataflows). Dataflow management systems typically enforce strict temporal synchronization across the various processing steps. Non-synchronous behavior often has to be explicitly programmed on an ad-hoc basis, which requires additional lines of code in programs and thus the possibility of errors. More so, in a large set of scenarios for continuous and incremental processing, the output of dataflow applications at each execution can suffer almost no difference when comparing to the previous execution, and therefore resources, energy and computational power are unknowingly wasted. To face such lack of efficiency, transparency, and generality, we introduce the notion of Quality-of-Data (QoD), which describes the level of changes required on a data store that cause the triggering of processing steps. This, so that the dataflow (re-)execution is reduced until its outcome would reach a significant and meaningful variation, which is inside a specified freshness limit. Based on the QoD notion, we propose a novel dataflow model, with framework (Fluxy), for orchestrating data-intensive processing steps, which communicate data via a NoSQL storage, and whose triggering semantics is driven by dynamic QoD constraints automatically defined for different datasets by means of Fuzzy Boolean Nets. These nets give probabilistic guarantees about the prediction of the cumulative error between consecutive dataflow executions. With Fluxy, we demonstrate how dataflows can be leveraged to respond to quality boundaries (that can be seen as SLAs) to deliver controlled and augmented performance, rationalization of resources, and task prioritization."
}
@article{WILBERTZ2012353,
title = "Orbitofrontal reward sensitivity and impulsivity in adult attention deficit hyperactivity disorder",
journal = "NeuroImage",
volume = "60",
number = "1",
pages = "353 - 361",
year = "2012",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2011.12.011",
url = "http://www.sciencedirect.com/science/article/pii/S1053811911014133",
author = "Gregor Wilbertz and Ludger Tebartz van Elst and Mauricio R. Delgado and Simon Maier and Bernd Feige and Alexandra Philipsen and Jens Blechert",
keywords = "ADHD, Reward, fMRI, OFC, Striatum, Impulsivity",
abstract = "Impulsivity symptoms of adult attention deficit hyperactivity disorder (ADHD) such as increased risk taking have been linked with impaired reward processing. Previous studies have focused on reward anticipation or on rewarded executive functioning tasks and have described a striatal hyporesponsiveness and orbitofrontal alterations in adult and adolescent ADHD. Passive reward delivery and its link to behavioral impulsivity are less well understood. To study this crucial aspect of reward processing we used functional magnetic resonance imaging (fMRI) combined with electrodermal assessment in male and female adult ADHD patients (N=28) and matched healthy control participants (N=28) during delivery of monetary and non-monetary rewards. Further, two behavioral tasks assessed risky decision making (game of dice task) and delay discounting. Results indicated that both groups activated ventral and dorsal striatum and the medial orbitofrontal cortex (mOFC) in response to high-incentive (i.e. monetary) rewards. A similar, albeit less strong activation pattern was found for low-incentive (i.e. non-monetary) rewards. Group differences emerged when comparing high and low incentive rewards directly: activation in the mOFC coded for the motivational change in reward delivery in healthy controls, but not ADHD patients. Additionally, this dysfunctional mOFC activity in patients correlated with risky decision making and delay discounting and was paralleled by physiological arousal. Together, these results suggest that the mOFC codes reward value and type in healthy individuals whereas this function is deficient in ADHD. The brain–behavior correlations suggest that this deficit might be related to behavioral impulsivity. Reward value processing difficulties in ADHD should be considered when assessing reward anticipation and emotional learning in research and applied settings."
}
@article{HUANG2015208,
title = "Self-organizing maps based on limit cycle attractors",
journal = "Neural Networks",
volume = "63",
pages = "208 - 222",
year = "2015",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2014.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0893608014002718",
author = "Di-Wei Huang and Rodolphe J. Gentili and James A. Reggia",
keywords = "Self-organizing maps, Limit cycle attractors, Oscillatory dynamics, Multi-self-organizing map architectures",
abstract = "Recent efforts to develop large-scale brain and neurocognitive architectures have paid relatively little attention to the use of self-organizing maps (SOMs). Part of the reason for this is that most conventional SOMs use a static encoding representation: each input pattern or sequence is effectively represented as a fixed point activation pattern in the map layer, something that is inconsistent with the rhythmic oscillatory activity observed in the brain. Here we develop and study an alternative encoding scheme that instead uses sparsely-coded limit cycles to represent external input patterns/sequences. We establish conditions under which learned limit cycle representations arise reliably and dominate the dynamics in a SOM. These limit cycles tend to be relatively unique for different inputs, robust to perturbations, and fairly insensitive to timing. In spite of the continually changing activity in the map layer when a limit cycle representation is used, map formation continues to occur reliably. In a two-SOM architecture where each SOM represents a different sensory modality, we also show that after learning, limit cycles in one SOM can correctly evoke corresponding limit cycles in the other, and thus there is the potential for multi-SOM systems using limit cycles to work effectively as hetero-associative memories. While the results presented here are only first steps, they establish the viability of SOM models based on limit cycle activity patterns, and suggest that such models merit further study."
}
@article{WOJCIECHOWSKI2012204,
title = "Influence of the power density on a conversion ratio in Accelerated Driven System (ADS)",
journal = "Annals of Nuclear Energy",
volume = "46",
pages = "204 - 212",
year = "2012",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2012.04.006",
url = "http://www.sciencedirect.com/science/article/pii/S0306454912001235",
author = "Andrzej Wojciechowski",
keywords = "Thorium, Breeding, U-233, ADS, Conversion ratio",
abstract = "The U-233 Conversion Ratio (CR) calculation results for Yalina Thermal assembly are presented as a function of burnup, power density, irradiation history and U-233 concentration. The Yalina Thermal assembly is an ADS which makes configuration changes possible. These various configurations enable investigation and comparison of CR in a Thorium cycle (Th cycle) and a mixed Uranium–Thorium cycle (U–Th cycle) of burning. The power density has no influence on the U-233 concentration dependence of CR and on the burnup dependence of CR. The calculations were done with a MCNPX code."
}
@article{KOOB201573,
title = "The dark side of emotion: The addiction perspective",
journal = "European Journal of Pharmacology",
volume = "753",
pages = "73 - 87",
year = "2015",
note = "Mood disorders-preclinical, clinical and translational aspects",
issn = "0014-2999",
doi = "https://doi.org/10.1016/j.ejphar.2014.11.044",
url = "http://www.sciencedirect.com/science/article/pii/S0014299915000151",
author = "George F. Koob",
keywords = "Opponent process, Extended amygdala, Corticotropin-releasing factor, Dynorphin, Incentive salience, Allostasis",
abstract = "Emotions are “feeling” states and classic physiological emotive responses that are interpreted based on the history of the organism and the context. Motivation is a persistent state that leads to organized activity. Both are intervening variables and intimately related and have neural representations in the brain. The present thesis is that drugs of abuse elicit powerful emotions that can be interwoven conceptually into this framework. Such emotions range from pronounced euphoria to a devastating negative emotional state that in the extreme can create a break with homeostasis and thus an allostatic hedonic state that has been considered key to the etiology and maintenance of the pathophysiology of addiction. Drug addiction can be defined as a three-stage cycle—binge/intoxication, withdrawal/negative affect, and preoccupation/anticipation—that involves allostatic changes in the brain reward and stress systems. Two primary sources of reinforcement, positive and negative reinforcement, have been hypothesized to play a role in this allostatic process. The negative emotional state that drives negative reinforcement is hypothesized to derive from dysregulation of key neurochemical elements involved in the brain incentive salience and stress systems. Specific neurochemical elements in these structures include not only decreases in incentive salience system function in the ventral striatum (within-system opponent processes) but also recruitment of the brain stress systems mediated by corticotropin-releasing factor (CRF), dynorphin-κ opioid systems, and norepinephrine, vasopressin, hypocretin, and substance P in the extended amygdala (between-system opponent processes). Neuropeptide Y, a powerful anti-stress neurotransmitter, has a profile of action on compulsive-like responding for drugs similar to a CRF1 receptor antagonist. Other stress buffers include nociceptin and endocannabinoids, which may also work through interactions with the extended amygdala. The thesis argued here is that the brain has specific neurochemical neurocircuitry coded by the hedonic extremes of pleasant and unpleasant emotions that have been identified through the study of opponent processes in the domain of addiction. These neurochemical systems need to be considered in the context of the framework that emotions involve the specific brain regions now identified to differentially interpreting emotive physiological expression."
}
@article{LUO2015125,
title = "Identification and characterization of microRNAs by deep-sequencing in Hyalomma anatolicum anatolicum (Acari: Ixodidae) ticks",
journal = "Gene",
volume = "564",
number = "2",
pages = "125 - 133",
year = "2015",
issn = "0378-1119",
doi = "https://doi.org/10.1016/j.gene.2015.01.019",
url = "http://www.sciencedirect.com/science/article/pii/S0378111915000359",
author = "Jin Luo and Guang-Yuan Liu and Ze Chen and Qiao-Yun Ren and Hong Yin and Jian-Xun Luo and Hui Wang",
keywords = "Tick,  (.. ), MicroRNA (miRNA), Deep-sequencing",
abstract = "Hyalomma anatolicum anatolicum (H.a. anatolicum) (Acari: Ixodidae) ticks are globally distributed ectoparasites with veterinary and medical importance. These ticks not only weaken animals by sucking their blood but also transmit different species of parasitic protozoans. Multiple factors influence these parasitic infections including miRNAs, which are non-coding, small regulatory RNA molecules essential for the complex life cycle of parasites. To identify and characterize miRNAs in H.a. anatolicum, we developed an integrative approach combining deep sequencing, bioinformatics and real-time PCR analysis. Here we report the use of this approach to identify miRNA expression, family distribution, and nucleotide characteristics, and discovered novel miRNAs in H.a. anatolicum. The result showed that miR-1-3p, miR-275-3p, and miR-92a were expressed abundantly. There was a strong bias on miRNA, family members, and nucleotide compositions at certain positions in H.a. anatolicum miRNA. Uracil was the dominant nucleotide, particularly at positions 1, 6, 16, and 18, which were located approximately at the beginning, middle, and end of conserved miRNAs. Analysis of the conserved miRNAs indicated that miRNAs in H.a. anatolicum were concentrated along three diverse phylogenetic branches of bilaterians, insects and coelomates. Two possible roles for the use of miRNA in H.a. anatolicum could be presumed based on its parasitic life cycle: to maintain a large category of miRNA families of different animals, and/or to preserve stringent conserved seed regions with active changes in other places of miRNAs mainly in the middle and the end regions. These might help the parasite to undergo its complex life style in different hosts and adapt more readily to the host changes. The present study represents the first large scale characterization of H.a. anatolicum miRNAs, which could further the understanding of the complex biology of this zoonotic parasite, as well as initiate miRNA studies in other related species such as Haemaphysalis longicornis and Rhipicephalus sanguineus of human and animal health significance."
}
@article{OUG201294,
title = "Biological traits analyses in the study of pollution gradients and ecological functioning of marine soft bottom species assemblages in a fjord ecosystem",
journal = "Journal of Experimental Marine Biology and Ecology",
volume = "432-433",
pages = "94 - 105",
year = "2012",
issn = "0022-0981",
doi = "https://doi.org/10.1016/j.jembe.2012.07.019",
url = "http://www.sciencedirect.com/science/article/pii/S0022098112002821",
author = "Eivind Oug and Annelise Fleddum and Brage Rygg and Frode Olsgard",
keywords = "Biological traits analysis, Ecological functioning, Marine benthic fauna, Pollution, Soft sediment infauna",
abstract = "In the present study, biological traits analysis (BTA) was used to explore and characterise effects of pollution on functional attributes of soft bottom infaunal species assemblages. The data comprised 38 sampling stations in the Oslofjord, Norway, ranging from heavily polluted to minimally impacted areas. At each station, species composition (113 taxa in total), contaminants (cadmium, mercury, lead, DDT, PCB) and sediment parameters were determined. Species functions were analysed for eight biological traits defined for activity and life history features. Traits were scored according to the fuzzy coding technique. The most distinct patterns were shown for mobility, size, sediment dwelling depth, feeding type and larval development in relation to contaminants, sediment physical structure and sediment oxidation status. At high levels of contaminants, particularly cadmium, features such as shallow sediment dwelling depth, small size, subsurface deposit feeding and lecitotroph larval development prevailed, while at low contaminant levels characteristic features included deeper sediment dwelling depth, larger size, surface deposit feeding and permanent attachment. Deep sediment dwelling depth (>15cm) was related to minimally contaminated oxidised sediments at greater water depths. Mobility and carnivorous feeding prevailed in coarser sediments. The study showed that BTA detected and depicted specific features that correlated with gradients in pollution and may be important for sediment reworking and nutrient cycling. As part of the present work, trait information for >500 macrofaunal taxa have been assembled and entered in a comprehensive database."
}
@article{UMUHOZA201349,
title = "Advocating for safe abortion in Rwanda: how young people and the personal stories of young women in prison brought about change",
journal = "Reproductive Health Matters",
volume = "21",
number = "41",
pages = "49 - 56",
year = "2013",
issn = "0968-8080",
doi = "https://doi.org/10.1016/S0968-8080(13)41690-7",
url = "http://www.sciencedirect.com/science/article/pii/S0968808013416907",
author = "Chantal Umuhoza and Barbara Oosters and Miranda van Reeuwijk and Ine Vanwesenbeeck",
keywords = "adolescents and young people, advocacy and political process, abortion law and policy, criminalization, Rwanda",
abstract = "In June 2012, a new abortion law came into effect in Rwanda as part of a larger review of Rwanda's penal code. This was a significant step in a country where it was previously taboo even to discuss abortion. This article describes some of the crucial elements in how this success was achieved in Rwanda, which began through a project launched by Rutgers WPF on “sensitive issues in young people's sexuality” in several countries. This paper describes how the Rwandan Youth Action Movement decided to work on unsafe abortion as part of this project. They gathered data on the extent of unsafe abortion and testimonies of young Rwandan women in prison for abortions; organized debates, values clarification exercises, interviews and a survey in four universities; launched a petition for law reform; produced awareness-raising materials; worked with the media; and met with representatives from government ministries, the national women's and youth councils, and parliamentarians – all of which played a significant role in the advocacy process for amendment of the law, which was revised when the penal code came up for review in June 2012. This history shows how important the role of young people can be in producing change and exposes, through personal stories, the need for a better abortion law, not only in Rwanda but also elsewhere.
Résumé
En juin 2012, une nouvelle loi sur l'avortement est entrée en vigueur au Rwanda, dans le cadre d'un examen élargi du code pénal rwandais. C'était un progrès important dans un pays où il était auparavant tabou même de parler d'avortement. Cet article décrit certains des éléments essentiels pour parvenir à ce succès, qui a commencé par un projet lancé par Rutgers WPF sur « les questions sensibles de la sexualité des jeunes » dans plusieurs pays. L'article décrit comment le Mouvement rwandais d'action des jeunes a décidé de travailler sur l'avortement à risque au titre de ce projet. Il a recueilli des données sur l'ampleur de ce phénomène et des témoignages de jeunes Rwandaises en prison pour avortement, et a organisé des débats, des exercices de clarification des valeurs, des entretiens et une enquête dans quatre universités. Il a lancé une pétition pour la réforme de la loi, a produit du matériel de sensibilisation et a collaboré avec les médias. Le Mouvement a rencontré des représentants de ministères, des conseils nationaux des femmes et des jeunes, ainsi que des parlementaires qui tous ont contribué grandement au processus de plaidoyer pour l'amendement de la loi, qui a été révisée lors de l'examen du code pénal en juin 2012. Cette histoire montre le rôle important que les jeunes peuvent jouer pour déclencher le changement et expose, avec des récits personnels, la nécessité d'une meilleure loi sur l'avortement au Rwanda, mais aussi ailleurs.
Resumen
En junio de 2012 entró en vigor una nueva ley referente al aborto en Ruanda como parte de una modificación general del Código Penal ruandés. Éste fue un paso importante en un país donde anteriormente era tabú incluso hablar sobre aborto. En este artículo se describen algunos de los elementos cruciales de este logro, que comenzó por medio de un proyecto iniciado por Rutgers WPF sobre “temas delicados relacionados con la sexualidad en la adolescencia” en varios países. Se describe cómo el Movimiento Ruandés de Juventud en Acción decidió trabajar en asuntos de aborto inseguro como parte de este proyecto. Recolectaron datos sobre la magnitud del aborto inseguro y testimonios de mujeres jóvenes ruandesas encarceladas por tener abortos; organizaron debates, ejercicios de aclaración de valores, entrevistas y una encuesta en cuatro universidades; entablaron una petición de reforma legislativa; produjeron material de concientización; trabajaron con los medios de comunicación; y se reunieron con representantes de ministerios gubernamentales, consejos nacionales de mujeres y jóvenes, y parlamentarios. Todos ellos desempeñaron un papel decisivo en el proceso para modificar la ley, lo cual sucedió cuando el código penal fue revisado en junio de 2012. Esta historia demuestra la importancia del rol de la juventud para producir cambios y expone, mediante historias personales, la necesidad de tener mejores leyes de aborto, no solo en Ruanda sino también en otros países."
}
@article{MEHOLIC2015302,
title = "A comprehensive, mechanistic heat transfer modeling package for dispersed flow film boiling—Part 2—Implementation and assessment",
journal = "Nuclear Engineering and Design",
volume = "291",
pages = "302 - 311",
year = "2015",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2015.07.014",
url = "http://www.sciencedirect.com/science/article/pii/S0029549315002848",
author = "Michael J. Meholic and David L. Aumiller and Fan-Bill Cheung",
abstract = "A mechanistic, first-principles based Dispersed Flow Film Boiling (DFFB) heat transfer package has been implemented within the existing framework of an in-house version of COBRA-TF, called COBRA-IE. Several sensitivities studies were performed on the proposed model to determine how to characterize the droplet size distribution and to validate the use of a quasi-static Lagrangian subscale trajectory calculation within COBRA-IE. Assessing the proposed model against experimental data from 118 experimental runs in four separate facilities has shown a marked improvement over the predictions of the base line COBRA-IE DFFB model set. Over the entire assessment database, the proposed model has reduced the mean error, RMS error, and standard deviation of the error in the wall temperature predictions and improved the prediction in the axial variation of the wall temperature. The proposed DFFB model marks a step-change in the use of mechanistically based DFFB models in reactor safety analysis codes resulting in improved predictive capabilities."
}
@article{JOHNSON20121786,
title = "Hidden costs: The direct and indirect impact of user fees on access to malaria treatment and primary care in Mali",
journal = "Social Science & Medicine",
volume = "75",
number = "10",
pages = "1786 - 1792",
year = "2012",
issn = "0277-9536",
doi = "https://doi.org/10.1016/j.socscimed.2012.07.015",
url = "http://www.sciencedirect.com/science/article/pii/S0277953612005564",
author = "Ari Johnson and Adeline Goss and Jessica Beckerman and Arachu Castro",
keywords = "Mali, Health financing, Fee-for-service, Access to health care, Health seeking, Primary health care, Malaria",
abstract = "About 20 years after initial calls for the introduction of user fees in health systems in sub-Saharan Africa, a growing coalition is advocating for their removal. Several African countries have abolished user fees for health care for some or all of their citizens. However, fee-for-service health care delivery remains a primary health care funding model in many countries in sub-Saharan Africa. Although the impact of user fees on utilization of health services and household finances has been studied extensively, further research is needed to characterize the multi-faceted health and social problems associated with charging user fees. This ethnographic study aims to identify consequences of user fees on gender inequality, food insecurity, and household decision-making for a group of women living in poverty. Ethnographic life history interviews were conducted with 24 women in Yirimadjo, Mali in 2007. Purposive sampling selected participants across a broad socio-economic spectrum. Semi-structured interviews addressed participants' past medical history, socio-economic status, social and family history, and access to health care. Interview transcripts were coded using the guiding analytical framework of structural violence. Interviews revealed that user fees for health care not only decreased utilization of health services, but also resulted in delayed presentation for care, incomplete or inadequate care, compromised food security and household financial security, and reduced agency for women in health care decision making. The effects of user fees were amplified by conditions of poverty, as well as gender and health inequality; user fees in turn reinforced the inequalities created by those very conditions. The qualitative data reveal multi-faceted health and socioeconomic effects of user fees, and illustrate that user fees for health care may impact quality of care, health outcomes, food insecurity, and gender inequality, in addition to impacting health care utilization and household finances. As many countries consider user fee abolition policies, these findings indicate the need to create a broader evaluation framework—one that can measure the health and socioeconomic impacts of user fee polices and of their removal."
}
@article{PANZERA2014105,
title = "Evolutionary and dispersal history of Triatoma infestans, main vector of Chagas disease, by chromosomal markers",
journal = "Infection, Genetics and Evolution",
volume = "27",
pages = "105 - 113",
year = "2014",
issn = "1567-1348",
doi = "https://doi.org/10.1016/j.meegid.2014.07.006",
url = "http://www.sciencedirect.com/science/article/pii/S1567134814002354",
author = "Francisco Panzera and María J. Ferreiro and Sebastián Pita and Lucía Calleros and Ruben Pérez and Yester Basmadjián and Yenny Guevara and Simone Frédérique Brenière and Yanina Panzera",
keywords = ", rDNA variability, C-heterochromatic polymorphism, Pyrethroid resistance, Hybrid zone, Hybridization",
abstract = "Chagas disease, one of the most important vector-borne diseases in the Americas, is caused by Trypanosoma cruzi and transmitted to humans by insects of the subfamily Triatominae. An effective control of this disease depends on elimination of vectors through spraying with insecticides. Genetic research can help insect control programs by identifying and characterizing vector populations. In southern Latin America, Triatoma infestans is the main vector and presents two distinct lineages, known as Andean and non-Andean chromosomal groups, that are highly differentiated by the amount of heterochromatin and genome size. Analyses with nuclear and mitochondrial sequences are not conclusive about resolving the origin and spread of T. infestans. The present paper includes the analyses of karyotypes, heterochromatin distribution and chromosomal mapping of the major ribosomal cluster (45S rDNA) to specimens throughout the distribution range of this species, including pyrethroid-resistant populations. A total of 417 specimens from seven different countries were analyzed. We show an unusual wide rDNA variability related to number and chromosomal position of the ribosomal genes, never before reported in species with holocentric chromosomes. Considering the chromosomal groups previously described, the ribosomal patterns are associated with a particular geographic distribution. Our results reveal that the differentiation process between both T. infestans chromosomal groups has involved significant genomic reorganization of essential coding sequences, besides the changes in heterochromatin and genomic size previously reported. The chromosomal markers also allowed us to detect the existence of a hybrid zone occupied by individuals derived from crosses between both chromosomal groups. Our genetic studies support the hypothesis of an Andean origin for T. infestans, and suggest that pyrethroid-resistant populations from the Argentinean-Bolivian border are most likely the result of recent secondary contact between both lineages. We suggest that vector control programs should make a greater effort in the entomological surveillance of those regions with both chromosomal groups to avoid rapid emergence of resistant individuals."
}
@article{APEL201376,
title = "Comparing LOPES measurements of air-shower radio emission with REAS 3.11 and CoREAS simulations",
journal = "Astroparticle Physics",
volume = "50-52",
pages = "76 - 91",
year = "2013",
issn = "0927-6505",
doi = "https://doi.org/10.1016/j.astropartphys.2013.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S092765051300145X",
author = "W.D. Apel and J.C. Arteaga-Velázquez and L. Bähren and K. Bekk and M. Bertaina and P.L. Biermann and J. Blümer and H. Bozdog and I.M. Brancus and E. Cantoni and A. Chiavassa and K. Daumiller and V. de Souza and F. Di Pierro and P. Doll and R. Engel and H. Falcke and B. Fuchs and D. Fuhrmann and H. Gemmeke and C. Grupen and A. Haungs and D. Heck and J.R. Hörandel and A. Horneffer and D. Huber and T. Huege and P.G. Isar and K.-H. Kampert and D. Kang and O. Krömer and J. Kuijpers and K. Link and P. Łuczak and M. Ludwig and H.J. Mathes and M. Melissas and C. Morello and J. Oehlschläger and N. Palmieri and T. Pierog and J. Rautenberg and H. Rebel and M. Roth and C. Rühle and A. Saftoiu and H. Schieler and A. Schmidt and F.G. Schröder and O. Sima and G. Toma and G.C. Trinchero and A. Weindl and J. Wochele and J. Zabierowski and J.A. Zensus",
keywords = "Cosmic rays, Extensive air showers, Radio emission, LOPES, Lateral distribution",
abstract = "Cosmic ray air showers emit radio pulses at MHz frequencies, which can be measured with radio antenna arrays – like LOPES at the Karlsruhe Institute of Technology in Germany. To improve the understanding of the radio emission, we test theoretical descriptions with measured data. The observables used for these tests are the absolute amplitude of the radio signal, and the shape of the radio lateral distribution. We compare lateral distributions of more than 500 LOPES events with two recent and public Monte Carlo simulation codes, REAS 3.11 and CoREAS (v 1.0). The absolute radio amplitudes predicted by REAS 3.11 are in good agreement with the LOPES measurements. The amplitudes predicted by CoREAS are lower by a factor of two, and marginally compatible with the LOPES measurements within the systematic scale uncertainties. In contrast to any previous versions of REAS, REAS 3.11 and CoREAS now reproduce the shape of the measured lateral distributions correctly. This reflects a remarkable progress compared to the situation a few years ago, and it seems that the main processes for the radio emission of air showers are now understood: The emission is mainly due to the geomagnetic deflection of the electrons and positrons in the shower. Less important but not negligible is the Askaryan effect (net charge variation). Moreover, we confirm that the refractive index of the air plays an important role, since it changes the coherence conditions for the emission: Only the new simulations including the refractive index can reproduce rising lateral distributions which we observe in a few LOPES events. Finally, we show that the lateral distribution is sensitive to the energy and the mass of the primary cosmic ray particles."
}
@article{ERMINI2012265,
title = "Complement polymorphisms: Geographical distribution and relevance to disease",
journal = "Immunobiology",
volume = "217",
number = "2",
pages = "265 - 271",
year = "2012",
note = "Special Issue COMPLEMENT UK",
issn = "0171-2985",
doi = "https://doi.org/10.1016/j.imbio.2011.07.020",
url = "http://www.sciencedirect.com/science/article/pii/S0171298511001604",
author = "L. Ermini and I.J. Wilson and T.H.J. Goodship and N.S. Sheerin",
keywords = "Complement polymorphisms, Human diseases, Evolutionary and population genetics",
abstract = "The evolution of man has been characterised by recurrent episodes of migration and settlement with infectious disease a constant threat. This long history of demographic change, together with the action of evolutionary forces such as natural selection and genetic drift, has shaped human genetic diversity. In particular, the interaction between humans, pathogens and the environment has played a crucial role in generating patterns of human genetic variation. The complement system plays a crucial role in the early protective immune response after exposure to a pathogen. Pathogens, over time, have developed mechanisms to circumvent the effects of complement which in turn has led to development of a more complex complement system. During the evolution of the complement system genes coding complement proteins have evolved polymorphisms, some of which have a functional effect, and this may reflect human–pathogen interaction and geographical origin. An example is the polymorphism Ile62Val (rs800292 (A>G)) in the complement regulator Factor H gene which alters the susceptibility to age-related macular degeneration (AMD), with the Ile62 polymorphism protecting against AMD. When sub-Saharan African and European populations are compared, the frequency of this polymorphism shows a very marked geographical distribution. Polymorphisms in other complement genes such as complement factor B show similar trends. This paper describes the geographical variation present in complement genes and discusses the implications of these observations. The analysis of genetic variation in complement genes is a promising tool to unravel mechanisms of host–pathogen interaction and can provide new insights into the evolution of the human immune system."
}
@article{CASINI2013171,
title = "GEOTHERM: A finite difference code for testing metamorphic P–T–t paths and tectonic models",
journal = "Computers & Geosciences",
volume = "59",
pages = "171 - 180",
year = "2013",
issn = "0098-3004",
doi = "https://doi.org/10.1016/j.cageo.2013.05.017",
url = "http://www.sciencedirect.com/science/article/pii/S0098300413001647",
author = "Leonardo Casini and Antonio Puccini and Stefano Cuccuru and Matteo Maino and Giacomo Oggiano",
keywords = "Numerical modeling, Finite differences methods, Variscan, HT Metamorphism, P–T–t paths",
abstract = "Here, time-dependent solutions for the heat conduction equation are numerically evaluated in 1D space using a fully implicit algorithm based on the finite difference method, assuming temperature-dependence of thermal conductivity. The method is implemented using the package ‘GEOTHERM’, comprising 13 MATLAB-derived scripts and 3 Excel spreadsheets. In the package, the initial state of the modeled crust, including its thickness, average density, and average heat production rate, can be configured by the user. The exhumation/burial history and metamorphic evolution of the crust are simulated by changing these initial values to fit the vertical displacement rates of the crust imposed by the user. Once the inputs have been made, the variations with depth of temperature, proportion of melt, and shear stress, as well as average values of heat flow at the surface and across the Moho, are calculated and displayed in five separate plots. The code is demonstrated with respect to the Carboniferous evolution of the South Variscan Belt. The best fit to independent petrologic constraints derived from thermobarometry is obtained with an early Carboniferous (342Ma) slab break-off and a shear strain rate of 10−13s−1 between 318 and 305Ma."
}
@article{DASGUPTA2015127,
title = "SCADOP: Phenomenological modeling of dryout in nuclear fuel rod bundles",
journal = "Nuclear Engineering and Design",
volume = "293",
pages = "127 - 137",
year = "2015",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2015.07.042",
url = "http://www.sciencedirect.com/science/article/pii/S0029549315003143",
author = "Arnab Dasgupta and D.K. Chandraker and P.K. Vijayan",
abstract = "Analysis and prediction of dryout is of important consequence to safety of nuclear fuel clusters of boiling water type of reactors. Traditionally, experimental correlations are used for dryout predictions. Since these correlations are based on operating parameters and do not aim to model the underlying phenomena, there has been a proliferation of the correlations, each catering to some specific bundle geometry under a specific set of operating conditions. Moreover, such experiments are extremely costly. In general, changes in tested bundle geometry for improvement in thermal-hydraulic performance would require re-experimentation. Understanding and modeling the basic processes leading to dryout in flow boiling thus has great incentive. Such a model has the ability to predict dryout in any rod bundle geometry, unlike the operating parameter based correlation approach. Thus more informed experiments can be carried out. A good model can, reduce the number of experiments required during the iterations in bundle design. In this paper, a phenomenological model as indicated above is presented. The model incorporates a new methodology to estimate the Initial Entrained Fraction (IEF), i.e., entrained fraction at the onset of annular flow. The incorporation of this new methodology is important since IEF is often assumed ad-hoc and sometimes also used as a parameter to tune the model predictions to experimental data. It is highlighted that IEF may be low under certain conditions against the general perception of a high IEF due to influence of churn flow. It is shown that the same phenomenological model is applicable to tubes as well as rod bundles. For application to rod bundles, the flow field was calculated using subchannel methodology. The model developed has been validated against experimental data in tubes and rod bundles. In the process a computer code SCADOP has been developed for analysis of dryout in rod bundles."
}
@article{GROSSU20131812,
title = "Hyper-Fractal Analysis v04: Implementation of a fuzzy box-counting algorithm for image analysis of artistic works",
journal = "Computer Physics Communications",
volume = "184",
number = "7",
pages = "1812 - 1813",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.02.026",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513000970",
author = "I.V. Grossu and S.A. El-Shamali",
keywords = "Fuzzy fractal dimension, Craquelure",
abstract = "This work presents a new version of a Visual Basic 6.0 application for estimating the fractal dimension of images and 4D objects (Grossu et al. 2013 [1]). Following our attempt of investigating artistic works by fractal analysis of craquelure, we encountered important difficulties in filtering real information from noise. In this context, trying to avoid a sharp delimitation of “black” and “white” pixels, we implemented a fuzzy box-counting algorithm.
New version program summary
Program title: Hyper-Fractal Analysis v04 Catalogue identifier: AEEG_v4_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEEG_v4_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 745999 No. of bytes in distributed program, including test data, etc.: 12844235 Distribution format: tar.gz Programming language: MS Visual Basic 6.0 Computer: PC Operating system: MS Windows 98 or later RAM: 100M Classification: 14 Catalogue identifier of previous version: AEEG_v3_0 Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 1344 Does the new version supercede the previous version?: Yes Nature of problem: estimating the fractal dimension of images Solution method: fuzzy box-counting algorithm Reasons for new version: Following the idea [2, 3] of investigating old paintings by fractal analysis of craquelure [4, 5], we faced with significant difficulties involved by the band-pass filter limitations. Trying to find a smoother way of separating information from noise, we implemented a fuzzy box-counting algorithm [6–8]. The fractal dimension [9] can be defined as: (1)df=limr→0logN(r)log(1/r) where N(r) represents the number of boxes, with length r, needed to cover the object. The main change considered is related to the significance of N(r). As opposed to the classical approach, where each box contributes to N(r) with either 1 (black), or 0 (white), in the fuzzy version (Fig. 1) each box contributes to N(r) with a rational number p=1−color code/(total number of colors−1). Summary of revisions:1.Implementation of a fuzzy box-counting algorithm for estimating the fractal dimension of images2.Optimization of the file open procedure.Fig. 1Hyper-Fractal Analysis v04 example of use. Fuzzy fractal dimension of painting craquelure. Running time: In a first approximation, the algorithm is linear [2]. References: [1]I.V. Grossu, I. Grossu, D. Felea, C. Besliu, Al. Jipa, T. Esanu, C.C. Bordeianu, E. Stan, Computer Physics Communications, 184 (2013) 1344–1345.[2]I.V. Grossu, C. Besliu, M.V. Rusu, Al. Jipa, C. C. Bordeianu, D. Felea, Computer Physics Communications, 180 (2009) 1999–2001.[3]I.V. Grossu, M.V. Rusu, A. Teodosiu, Fractals in a particular process. Fractals in the investigation of artistic works, in National Conference of Physics, Romania, Constanta, 21–23 September 2000.[4]A. Teodosiu, Din universul ascuns al operei de arta, Allfa, Romania (2001) pp. 113–122.[5]S Bucklow, Consensus in the Classification of Craquelure, Hamilton Kerr Institute Bulletin, number 3, ed. A Massing, Hamilton Kerr Institute, University of Cambridge 2000: pp. 61–73.[6]X. Zhuang, Q. Meng, Artificial Intelligence in Medicine (2004) 32, 29–36.[7]D. Dumitrescu, Hariton Costin, Retele Neuronale. Teorie si aplicatii, Teora, Bucuresti, 1996, pp. 228–262.[8]D. Dumitrescu, Fuzzy Measures and the entropy of fuzzy partitions, J. Math, Anal. Appl., 176 (1993b) 359–373.[9]R.H. Landau, M.J. Paez and C.C. Bordeianu, Computational physics: Problem solving with computers, Wiley-VCH-Verlag, Weinheim, 2007, pp. 293–306."
}
@article{BORN2015161,
title = "Cortical magnification plus cortical plasticity equals vision?",
journal = "Vision Research",
volume = "111",
pages = "161 - 169",
year = "2015",
note = "Sight restoration: prosthetics, optogenetics and gene therapy",
issn = "0042-6989",
doi = "https://doi.org/10.1016/j.visres.2014.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S0042698914002351",
author = "Richard T. Born and Alexander R. Trott and Till S. Hartmann",
keywords = "Primary visual cortex, V1, Magnification factor, Vision restoration, Visual prosthesis, Plasticity",
abstract = "Most approaches to visual prostheses have focused on the retina, and for good reasons. The earlier that one introduces signals into the visual system, the more one can take advantage of its prodigious computational abilities. For methods that make use of microelectrodes to introduce electrical signals, however, the limited density and volume occupying nature of the electrodes place severe limits on the image resolution that can be provided to the brain. In this regard, non-retinal areas in general, and the primary visual cortex in particular, possess one large advantage: “magnification factor” (MF)—a value that represents the distance across a sheet of neurons that represents a given angle of the visual field. In the foveal representation of primate primary visual cortex, the MF is enormous—on the order of 15–20mm/deg in monkeys and humans, whereas on the retina, the MF is limited by the optical design of the eye to around 0.3mm/deg. This means that, for an electrode array of a given density, a much higher-resolution image can be introduced into V1 than onto the retina (or any other visual structure). In addition to this tremendous advantage in resolution, visual cortex is plastic at many different levels ranging from a very local ability to learn to better detect electrical stimulation to higher levels of learning that permit human observers to adapt to radical changes to their visual inputs. We argue that the combination of the large magnification factor and the impressive ability of the cerebral cortex to learn to recognize arbitrary patterns, might outweigh the disadvantages of bypassing earlier processing stages and makes V1 a viable option for the restoration of vision."
}
@article{MILLER2015329,
title = "Concrete slab comparison and embodied energy optimisation for alternate design and construction techniques",
journal = "Construction and Building Materials",
volume = "80",
pages = "329 - 338",
year = "2015",
issn = "0950-0618",
doi = "https://doi.org/10.1016/j.conbuildmat.2015.01.071",
url = "http://www.sciencedirect.com/science/article/pii/S0950061815001051",
author = "Dane Miller and Jeung-Hwan Doh and Mitchell Mulvey",
keywords = "Optimisation, Sustainable structural design, Slab construction techniques, Embodied energy",
abstract = "Construction material consumption is greater than any time in history. Australia produces approximately 30 million tonnes of finished building products each year, with over 56% of this quantity, by mass, being attributed to concrete and a further 6%, steel. Globally, 23trillion kilograms of concrete alone is consumed annually, with growing population driving increasing demands. This study assesses the environmental performance of various concrete slab systems. Historically, the focus of environmental performance in buildings has been Operation Energy (OE) requirements, however Zero Energy Buildings (ZEB) are changing this. Specifically the study investigates the environmental performance of concrete structures varying design parameters and construction techniques to optimise its embodied energy (EE). These structures are designed in accordance with all relevant Australian codes and standards. The various slab systems investigated include: beam & slab, flat slab and flat plates while concurrently considering the use of conventionally reinforced and post-tensioned construction methods. Designs were compared in terms of EE outcomes given fixed design criteria, with results indicating reductions between 23.7% and 49.1% when utilising post-tensioned construction methods."
}
@article{HERNANDEZZAPATA2018151,
title = "Lupus, «un cáncer pero más chiquito». Percepciones del lupus eritematoso sistémico en adolescentes próximos a la transición",
journal = "Revista Colombiana de Reumatología",
volume = "25",
number = "3",
pages = "151 - 160",
year = "2018",
issn = "0121-8123",
doi = "https://doi.org/10.1016/j.rcreu.2018.06.002",
url = "http://www.sciencedirect.com/science/article/pii/S0121812318300604",
author = "Lady J. Hernández Zapata and Sandra I. Alzate Vanegas and Ruth M. Eraso and Carlos E. Yepes Delgado",
keywords = "Lupus eritematoso sistémico, Percepción social, Transición a la atención de adultos, Salud del adolescente, Acontecimientos que cambian la vida, Systemic lupus erythematosus, Social perception, Transition to adult care, Adolescent health, Life change events",
abstract = "Resumen
Introducción
Se espera que pacientes con lupus pediátrico próximos a la transición asuman responsabilidades de su cuidado, pero muchos no están preparados, lo cual empobrece su pronóstico. Según el modelo autorregulatorio, las percepciones de la enfermedad determinan las respuestas emocionales, guían las conductas de afrontamiento, su evaluación, retroalimentación y estrategias de asimilación.
Objetivo
Describir las percepciones sobre el lupus en adolescentes próximos a la transición.
Materiales y métodos
Desde un enfoque hermenéutico y utilizando técnicas de la teoría fundada, se realizaron 11 entrevistas semiestructuradas entre junio de 2013 y septiembre de 2014 a 9 adolescentes con diagnóstico de lupus.
Resultados
Se obtuvieron 1.800 códigos. Emergieron como categorías preliminares: «Intentando explicar el origen», donde se interpreta la causa de la enfermedad en términos de inmunosupresión y autoinmunidad, asociación con cáncer, culpa e influencia de factores emocionales, además el proceso diagnóstico. «Lo que se pierde», enmarcado en cambios, trato diferencial y límites. Finalmente, los «aspectos positivos»: enfermedad como modulador de la conducta, adquisición de cualidades, aprendizaje sobre el funcionamiento corporal y ganancia secundaria.
Conclusión
Las percepciones de la enfermedad en estos adolescentes, expresan en la experiencia de cambio las implicaciones del diagnóstico, las cuales impactan múltiples aspectos de sus vidas y traen consigo incertidumbre y necesidad de ajustes que les llevan a la búsqueda de explicaciones. Se requiere mayor conciencia sobre estas percepciones, debido a que, junto con otros factores, determinarán las estrategias que los adolescentes desarrollen para garantizar su autocuidado y adaptación a situaciones derivadas del vivir con la enfermedad.
Introduction
Patients with paediatric lupus nearing transition to adult care are expected to take responsibility for their care. Nevertheless, many are not prepared for this, and thus have a poorer prognosis. Using the self-regulation model, the perception of a condition determines the emotional responses and guides coping efforts, appraisal, feedback, and assimilation strategies.
Objective
To describe how adolescents nearing transition perceive lupus.
Materials and methods
Eleven semi-structured interviews were conducted using a hermeneutic approach with techniques from grounded theory. Interviews took place between July 2013 and September 2014. The participants were nine adolescents with diagnosed lupus.
Results
A total of 1,800 codes were obtained that emerged as the following preliminary categories: “Attempting to explain the origin”, where the cause of the disease is interpreted as immunosuppression, autoimmunity, association with cancer, guilt and influence of emotional factors, along with the diagnosis process. “What is lost”, which includes changes, being treated differently, and having limitations. The last category was “positive aspects” deals with illness as a behavioural moderator, acquiring qualities, learning about bodily functioning and secondary gain.
Conclusion
Adolescents perceive their condition based on the implications of the changes experienced in their lives as a result of the diagnosis. Lupus affects several aspects of their lives and brings uncertainty and a need to adjust, leading them to look for explanations. More awareness of these perceptions is required because the latter, along with other factors, determine the strategies that adolescents develop to ensure their self-care and adaptation to any situations arising from living with the condition."
}
@article{SAVARDI201813,
title = "Automatic hemolysis identification on aligned dual-lighting images of cultured blood agar plates",
journal = "Computer Methods and Programs in Biomedicine",
volume = "156",
pages = "13 - 24",
year = "2018",
issn = "0169-2607",
doi = "https://doi.org/10.1016/j.cmpb.2017.12.017",
url = "http://www.sciencedirect.com/science/article/pii/S0169260717307113",
author = "Mattia Savardi and Alessandro Ferrari and Alberto Signoroni",
keywords = "Digital Microbiology Imaging, Full Laboratory Automation, Hemolysis identification, Machine learning, Image alignment, Image classification",
abstract = "Background and Objective: The recent introduction of Full Laboratory Automation systems in clinical microbiology opens to the availability of streams of high definition images representing bacteria culturing plates. This creates new opportunities to support diagnostic decisions through image analysis and interpretation solutions, with an expected high impact on the efficiency of the laboratory workflow and related quality implications. Starting from images acquired under different illumination settings (top-light and back-light), the objective of this work is to design and evaluate a method for the detection and classification of diagnostically relevant hemolysis effects associated with specific bacteria growing on blood agar plates. The presence of hemolysis is an important factor to assess the virulence of pathogens, and is a fundamental sign of the presence of certain types of bacteria. Methods: We introduce a two-stage approach. Firstly, the implementation of a highly accurate alignment of same-plate image scans, acquired using top-light and back-light illumination, enables the joint spatially coherent exploitation of the available data. Secondly, from each segmented portion of the image containing at least one bacterial colony, specifically designed image features are extracted to feed a SVM classification system, allowing detection and discrimination among different types of hemolysis. Results: The fine alignment solution aligns more than 98.1% images with a residual error of less than 0.13 mm. The hemolysis classification block achieves a 88.3% precision with a recall of 98.6%. Conclusions: The results collected from different clinical scenarios (urinary infections and throat swab screening) together with accurate error analysis demonstrate the suitability of our system for robust hemolysis detection and classification, which remains feasible even in challenging conditions (low contrast or illumination changes)."
}
@article{JACOBS2016498,
title = "Late Ebola virus relapse causing meningoencephalitis: a case report",
journal = "The Lancet",
volume = "388",
number = "10043",
pages = "498 - 503",
year = "2016",
issn = "0140-6736",
doi = "https://doi.org/10.1016/S0140-6736(16)30386-5",
url = "http://www.sciencedirect.com/science/article/pii/S0140673616303865",
author = "Michael Jacobs and Alison Rodger and David J Bell and Sanjay Bhagani and Ian Cropley and Ana Filipe and Robert J Gifford and Susan Hopkins and Joseph Hughes and Farrah Jabeen and Ingolfur Johannessen and Drosos Karageorgopoulos and Angie Lackenby and Rebecca Lester and Rebecca S N Liu and Alisdair MacConnachie and Tabitha Mahungu and Daniel Martin and Neal Marshall and Stephen Mepham and Richard Orton and Massimo Palmarini and Monika Patel and Colin Perry and S Erica Peters and Duncan Porter and David Ritchie and Neil D Ritchie and R Andrew Seaton and Vattipally B Sreenu and Kate Templeton and Simon Warren and Gavin S Wilkie and Maria Zambon and Robin Gopal and Emma C Thomson",
abstract = "Summary
Background
There are thousands of survivors of the 2014 Ebola outbreak in west Africa. Ebola virus can persist in survivors for months in immune-privileged sites; however, viral relapse causing life-threatening and potentially transmissible disease has not been described. We report a case of late relapse in a patient who had been treated for severe Ebola virus disease with high viral load (peak cycle threshold value 13·2).
Methods
A 39-year-old female nurse from Scotland, who had assisted the humanitarian effort in Sierra Leone, had received intensive supportive treatment and experimental antiviral therapies, and had been discharged with undetectable Ebola virus RNA in peripheral blood. The patient was readmitted to hospital 9 months after discharge with symptoms of acute meningitis, and was found to have Ebola virus in cerebrospinal fluid (CSF). She was treated with supportive therapy and experimental antiviral drug GS-5734 (Gilead Sciences, San Francisco, Foster City, CA, USA). We monitored Ebola virus RNA in CSF and plasma, and sequenced the viral genome using an unbiased metagenomic approach.
Findings
On admission, reverse transcriptase PCR identified Ebola virus RNA at a higher level in CSF (cycle threshold value 23·7) than plasma (31·3); infectious virus was only recovered from CSF. The patient developed progressive meningoencephalitis with cranial neuropathies and radiculopathy. Clinical recovery was associated with addition of high-dose corticosteroids during GS-5734 treatment. CSF Ebola virus RNA slowly declined and was undetectable following 14 days of treatment with GS-5734. Sequencing of plasma and CSF viral genome revealed only two non-coding changes compared with the original infecting virus.
Interpretation
Our report shows that previously unanticipated, late, severe relapses of Ebola virus can occur, in this case in the CNS. This finding fundamentally redefines what is known about the natural history of Ebola virus infection. Vigilance should be maintained in the thousands of Ebola survivors for cases of relapsed infection. The potential for these cases to initiate new transmission chains is a serious public health concern.
Funding
Royal Free London NHS Foundation Trust."
}
@article{BARKER2014738,
title = "Aztreonam for inhalation solution in patients with non-cystic fibrosis bronchiectasis (AIR-BX1 and AIR-BX2): two randomised double-blind, placebo-controlled phase 3 trials",
journal = "The Lancet Respiratory Medicine",
volume = "2",
number = "9",
pages = "738 - 749",
year = "2014",
issn = "2213-2600",
doi = "https://doi.org/10.1016/S2213-2600(14)70165-1",
url = "http://www.sciencedirect.com/science/article/pii/S2213260014701651",
author = "Alan F Barker and Anne E O'Donnell and Patrick Flume and Philip J Thompson and Jonathan D Ruzi and Javier de Gracia and Wim G Boersma and Anthony De Soyza and Lixin Shao and Jenny Zhang and Laura Haas and Sandra A Lewis and Sheila Leitzinger and A Bruce Montgomery and Matthew T McKevitt and David Gossage and Alexandra L Quittner and Thomas G O'Riordan",
abstract = "Summary
Background
The clinical benefit of inhaled antibiotics in non-cystic fibrosis bronchiectasis has not been established in randomised controlled trials. We aimed to assess safety and efficacy of aztreonam for inhalation solution (AZLI) in patients with non-cystic fibrosis bronchiectasis and Gram-negative bacterial colonisation.
Methods
AIR-BX1 and AIR-BX2 were two double-blind, multicentre, randomised, placebo-controlled phase 3 trials, which included patients aged 18 years or older who had bronchiectasis and history of positive sputum or bronchoscopic culture for target Gram-negative organisms. Patients were randomly assigned to receive either AZLI or placebo (1:1). Randomisation was done without stratification and the code was generated by a Gilead designee. In both studies, two 4-week courses of AZLI 75 mg or placebo (three-times daily; eFlow nebulizer) were each followed by a 4-week off-treatment period. Primary endpoint was change from baseline Quality of Life-Bronchiectasis Respiratory Symptoms scores (QOL-B-RSS) at 4 weeks. These trials are registered with ClinicalTrials.gov, numbers are NCT01313624 for AIR-BX1 and NCT01314716 for AIR-BX2.
Findings
We recruited participants from 47 ambulatory clinics for AIR-BX1 and 65 ambulatory clinics for AIR-BX2; studies were done between April 25, 2011, and July 1, 2013. In AIR-BX1, of the 348 patients screened, 134 were randomly assigned to receive AZLI and 132 to receive placebo. In AIR-BX2, of the 404 patients screened, 136 were randomly assigned to receive AZLI and 138 to receive placebo. The difference between AZLI and placebo for adjusted mean change from baseline QOL-B-RSS was not significant at 4 weeks (0·8 [95% CI −3·1 to 4·7], p=0·68) in AIR-BX1, but was significant (4·6 [1·1 to 8·2], p=0·011) in AIR-BX2. The 4·6 point difference in QOL-B-RSS after 4 weeks in AIR-BX2 was not deemed clinically significant. In both studies, treatment-related adverse events were more common in the AZLI group than in the placebo group, as were discontinuations from adverse events. The most commonly reported treatment-emergent adverse events were dyspnea, cough, and increased sputum. Each was more common for AZLI-treated than for placebo-treated patients, but the incidences were more balanced in AIR-BX2.
Interpretation
AZLI treatment did not provide significant clinical benefit in non-cystic fibrosis bronchiectasis, as measured by QOL-B-RSS, suggesting a continued need for placebo-controlled studies to establish the clinical benefit of inhaled antibiotics in patients with this disorder.
Funding
Gilead Sciences."
}
@article{FRANCESCHINI2013235,
title = "Development of a control rod depletion methodology for the Westinghouse NEXUS system",
journal = "Progress in Nuclear Energy",
volume = "68",
pages = "235 - 242",
year = "2013",
issn = "0149-1970",
doi = "https://doi.org/10.1016/j.pnucene.2012.12.001",
url = "http://www.sciencedirect.com/science/article/pii/S0149197012001424",
author = "Fausto Franceschini and Baocheng Zhang and Larry Mayhue and Erwin Müller and Petri Forslund Guimarães",
keywords = "NEXUS, Control rod depletion, Pin power recovery",
abstract = "The NEXUS project is an effort to merge and modernize the methods employed in Westinghouse PWR and BWR steady-state reactor physics codes. The NEXUS system relies on a once-through nodal cross-section generation methodology with an innovative and efficient technique for pin power recovery. The pin power methodology overcomes a well-known limitation of existing methodologies, namely the incapacity to properly account for heterogeneity changes due to the depletion environment. The so-called control rod history problem where control rods are repeatedly inserted and withdrawn during core depletion is a good example of such a case. In addition to the control rod history impact on pin power distributions, the insertion of control rods during extended periods leads to significant control rod depletion that affects the reactivity worth of the control rods which in turn can have a significant impact on pin powers. The importance of accurately predicting pin powers, combined with the need to adequately estimate the reactivity worth and nuclear end of life of control rods in BWRs and in generation III+ PWRs, has motivated the development of a novel control rod depletion model. This methodology and its numerical qualification, initially for PWR application only, is the topic of this paper. The focus is on describing the salient features of the model and on illustrating its performance by means of numerical experiments. It is shown that together with the NEXUS pin power recovery model, the control rod depletion methodology accurately predicts the reactivity feedback from repeated control rod insertions in a PWR core."
}
@article{LONCAR2016190,
title = "OpenMP, OpenMP/MPI, and CUDA/MPI C programs for solving the time-dependent dipolar Gross–Pitaevskii equation",
journal = "Computer Physics Communications",
volume = "209",
pages = "190 - 196",
year = "2016",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2016.07.029",
url = "http://www.sciencedirect.com/science/article/pii/S0010465516302272",
author = "Vladimir Lončar and Luis E. Young-S. and Srdjan Škrbić and Paulsamy Muruganandam and Sadhan K. Adhikari and Antun Balaž",
keywords = "Bose–Einstein condensate, Dipolar atoms, Gross–Pitaevskii equation, Split-step Crank–Nicolson scheme, C program, OpenMP, GPU, CUDA program, MPI",
abstract = "We present new versions of the previously published C and CUDA programs for solving the dipolar Gross–Pitaevskii equation in one, two, and three spatial dimensions, which calculate stationary and non-stationary solutions by propagation in imaginary or real time. Presented programs are improved and parallelized versions of previous programs, divided into three packages according to the type of parallelization. First package contains improved and threaded version of sequential C programs using OpenMP. Second package additionally parallelizes three-dimensional variants of the OpenMP programs using MPI, allowing them to be run on distributed-memory systems. Finally, previous three-dimensional CUDA-parallelized programs are further parallelized using MPI, similarly as the OpenMP programs. We also present speedup test results obtained using new versions of programs in comparison with the previous sequential C and parallel CUDA programs. The improvements to the sequential version yield a speedup of 1.1–1.9, depending on the program. OpenMP parallelization yields further speedup of 2–12 on a 16-core workstation, while OpenMP/MPI version demonstrates a speedup of 11.5–16.5 on a computer cluster with 32 nodes used. CUDA/MPI version shows a speedup of 9–10 on a computer cluster with 32 nodes.
New version program summary
Program Title: DBEC-GP-OMP-CUDA-MPI: (1) DBEC-GP-OMP package: (i) imag1dX-th, (ii) imag1dZ-th, (iii) imag2dXY-th, (iv) imag2dXZ-th, (v) imag3d-th, (vi) real1dX-th, (vii) real1dZ-th, (viii) real2dXY-th, (ix) real2dXZ-th, (x) real3d-th; (2) DBEC-GP-MPI package: (i) imag3d-mpi, (ii) real3d-mpi; (3) DBEC-GP-MPI-CUDA package: (i) imag3d-mpicuda, (ii) real3d-mpicuda. Program Files doi:http://dx.doi.org/10.17632/j3z9z379m8.1 Licensing provisions: Apache License 2.0 Programming language: OpenMP C; CUDA C. Computer: DBEC-GP-OMP runs on any multi-core personal computer or workstation with an OpenMP-capable C compiler and FFTW3 library installed. MPI versions are intended for a computer cluster with a recent MPI implementation installed. Additionally, DBEC-GP-MPI-CUDA requires CUDA-aware MPI implementation installed, as well as that a computer or a cluster has Nvidia GPU with Compute Capability 2.0 or higher, with CUDA toolkit (minimum version 7.5) installed. Number of processors used: All available CPU cores on the executing computer for OpenMP version, all available CPU cores across all cluster nodes used for OpenMP/MPI version, and all available Nvidia GPUs across all cluster nodes used for CUDA/MPI version. Journal reference of previous version: Comput. Phys. Commun. 195 (2015) 117; ibid.200 (2016) 406. Does the new version supersede the previous version?: Not completely. OpenMP version does supersede previous AEWL_v1_0 version, while MPI versions do not supersede previous versions and are meant for execution on computer clusters and multi-GPU workstations. Nature of problem: These programs are designed to solve the time-dependent nonlinear partial differential Gross–Pitaevskii (GP) equation with contact and dipolar interaction in a harmonic anisotropic trap. The GP equation describes the properties of a dilute trapped Bose–Einstein condensate. OpenMP package contains programs for solving the GP equation in one, two, and three spatial dimensions, while MPI packages contain only three-dimensional programs, which are computationally intensive or memory demanding enough to require such level of parallelization. Solution method: The time-dependent GP equation is solved by the split-step Crank–Nicolson method by discretizing in space and time. The discretized equation is then solved by propagation, in either imaginary or real time, over small time steps. The contribution of the dipolar interaction is evaluated by a Fourier transformation to momentum space using a convolution theorem. MPI parallelization is done using the domain decomposition. The method yields the solution of stationary and/or non-stationary problems. Reasons for the new version: Previously published C and Fortran programs [1] for solving the dipolar GP equation are sequential in nature and do not exploit the multiple cores or CPUs found in typical modern computers. A parallel implementation exists, using Nvidia CUDA [2], and both versions are already used within the ultra-cold atoms community [3]. However, CUDA version requires special hardware, which limits its usability. Furthermore, many researchers have access to high performance computer clusters, which could be used to either further speed up the computation, or to work with problems which cannot fit into a memory of a single computer. In light of these observations, we have parallelized all programs using OpenMP, and then extended the parallelization of three-dimensional programs using MPI to distributed-memory clusters. Since the CUDA implementation uses the same algorithm, and thus has the same structure and flow, we have applied the same data distribution scheme to provide the distributed-memory CUDA/MPI implementation of three-dimensional programs. Summary of revisions: Package DBEC-GP-OMP: Previous serial C programs [1] are here improved and then parallelized using OpenMP (package DBEC-GP-OMP). The main improvement consists of switching to real-to-complex (R2C) Fourier transform, which is possible due to the fact that input of the transform is purely real. In this case the result of the transform has Hermitian symmetry, where one half of the values are complex conjugates of the other half. The fast Fourier transformation (FFT) libraries we use can exploit this to compute the result faster, using half the memory. To parallelize the programs, we have used OpenMP with the same approach as described in [4], and extended the parallelization routines to include the computation of the dipolar term. The FFT, used in computation of the dipolar term, was also parallelized in a straightforward manner, by using the built-in support for OpenMP in FFTW3 library [5]. With the introduction of multiple threads memory usage has increased, driven by the need to have some variables private to each thread. To reduce the memory consumed, we resorted to using techniques similar to the ones used in our CUDA implementation [2], i.e., we have reduced the memory required for FFT by exploiting the aforementioned R2C FFT, and reused the memory with pointer aliases whenever possible. Package DBEC-GP-MPI: Next step in the parallelization (package DBEC-GP-MPI) was to extend the programs to run on distributed-memory systems, i.e., on computer clusters using domain decomposition with MPI programming paradigm. We chose to use the newly-implemented threaded versions of the programs as the starting point. Alternatively, we could have used serial versions, and attempt a pure MPI parallelization, however we have found that OpenMP-parallelized routines better exploit the data locality and thus outperform the pure MPI implementation. Therefore, our OpenMP/MPI-parallelized programs are intended to run one MPI process per cluster node, and each process would spawn the OpenMP threads as needed on its cluster node. Note that this is not a requirement, and users may run more than one MPI process per node, but we advise against it due to performance reasons. With the suggested execution strategy (one MPI process per cluster node, each spawning as many threads as CPU cores available), OpenMP threads perform most of the computation, and MPI is used for data exchanges between processes. There are numerous ways to distribute the data between MPI processes, and we decided to use a simple one-dimensional data distribution, also known as slab decomposition. Data is distributed along the first (slowest changing) dimension, which corresponds to NX spatial dimension in our programs (see Fig. 1). Each process is assigned a different portion of the NX dimension, and contains the entire NY and NZ spatial dimensions locally. This allows each process to perform computation on those two dimensions in the same way as before, without any data exchanges. In case the computation requires whole NX dimension to be local to each process, we transpose the data, and after the computation, we transpose the data back. Fig. 1Illustration of data distribution between MPI processes. On the left, the data are distributed along the NX dimension, while on the right the same data are redistributed along the NY dimension. Transpose routine can be implemented in many ways using MPI, most commonly using MPI_Alltoall function, or using transpose routines from external libraries, like FFTW3 [5] or 2DECOMP&FFT [6]. Since we already rely on FFTW3 library for FFT, we have utilized its dedicated transpose interface to perform the necessary transformations. To speed up transpose operation, we do not perform full transposition of data, but rather leave it locally transposed. That is, we transform from local_NX × NY × NZ, stored in row-major order, to NX × local_NY × NZ in row-major order (where local_NX = NX / number_of_processes, and equivalently for local_NY). This approach has an additional benefit that we do not have to make significant changes in the way array elements are processed, and in most cases we only have to adjust the loop limit of the non-local dimension. Package DBEC-GP-MPI-CUDA: The aforementioned data distribution scheme can be also applied to the CUDA version of programs [2]. However, there is no support for CUDA in FFTW3, and cuFFT (used in CUDA programs for FFT) does not provide equivalent MPI or transpose interface. Instead, we developed our own transpose routines, and used them in FFT computation. One example of manual implementation of transpose routines is shown in Ref. [7], and while we could readily use the same code, we wanted to have the same result as when using FFTW3. To achieve this, we use the same basic principle as in Ref. [7], first we create a custom MPI data type that maps to portions of the data to be exchanged, followed by an all-to-all communication to exchange the data between processes, see Fig. 2 for details. Fig. 2Example of a transpose routine of a 4×4×4 data between four MPI processes. Initially, all processes have 1/4 of the NX dimension, and whole NY and NZ dimensions. After transposing, each process has full NX and NZ dimensions, and 1/4 of the NY dimension. The implemented transpose routines are also used to compute a distributed-memory FFT, performed over all MPI processes. To divide the computation of a multidimensional FFT, in our case three-dimensional, we use a well-known row–column algorithm. The basic idea of the algorithm is perhaps best explained on a two-dimensional FFT of N×M data, stored in row-major order, illustrated in Fig. 3. First the N one-dimensional FFTs of length M are performed (along the row of data), followed by a transpose, after which data are stored as M×N in row-major format. Now M FFTs of length N can be performed along what used to be a column of original data, but are stored as rows after transposing. Finally, an optional transpose can be performed to return the data in their original N×M form. In three dimensions, we can perform a two-dimensional FFT, transpose the data, and perform the FFT along the third dimension. This algorithm can be easily adapted for distributed memory systems. We use advanced cuFFT interface for local computation of FFT, and use our transpose routine to redistribute the data. Note that DBEC-GP-MPI-CUDA programs can be easily modified to work on a single workstation with multiple GPU cards, or a computer cluster with multiple GPU cards per node. In that case, for each GPU card a separate MPI process should be launched and the programs should be modified to assign a separate GPU card for processes on the same cluster node. Fig. 3Illustration of four stages of row–column FFT algorithm. The last transpose operation may be omitted, and often yields better performance. MPI output format: Given that the distributed memory versions of the programs can be used for much larger grid sizes, the output they produce (i.e., the density profiles) can be much larger and difficult to handle. To alleviate this problem somewhat, we have switched to a binary output instead of the textual. This allowed us to reduce the size of files, while still retaining precision. All MPI processes will write the output to the same file, at the corresponding offset, relieving the user of the task of combining the files. The binary output can be subsequently converted to textual, for example by using hexdump command on UNIX-like systems. We have developed a simple script which converts the output from binary to textual format and included it in the software package. Testing results: We have tested all programs on the PARADOX supercomputing facility at the Scientific Computing Laboratory of the Institute of Physics Belgrade. Nodes used for testing had two Intel Xeon E5-2670 CPUs (with a total of 2×8=16 CPU cores) with 32 GB of RAM and one Nvidia Tesla M2090 GPU with 6 GB of RAM, each connected by Infiniband QDR interconnect. The presented results are obtained for arbitrary grid sizes, which are not tailored to maximize performance of the programs. We also stress that execution times and speedups reported here are calculated for critical parallelized parts of the programs performing iterations over imaginary or real time steps, and they exclude time spent on initialization (threads initialization, MPI environment, allocation/deallocation of memory, creating/destroying FFTW plans, I/O operations). As a part of its output, each program separately prints initialization time and time spent on iterations for GP propagation. The latter time is used to calculate a speedup, as a speedup obtained this way does not depend on the number of iterations and is more useful for large numbers of iterations. The testing of OpenMP versions of programs DBEC-GP-OMP was performed with the number of threads varying from 1 to 16. Table 1 and Fig. 4 show the obtained absolute wall-clock times, speedups, and scaling efficiencies, as well as comparison with the previous serial version of programs [1]. As we can see from the table, improvements in the FFT routine used already yield a speedup of 1.3 to 1.9 for single-threaded (T=1) 2d and 3d programs compared to the previous serial programs, and somewhat smaller speedup for 1d programs, 1.1 to 1.3. The use of additional threads brings about further speedup of 2 to 2.5 for 1d programs, and 9 to 12 for 2d and 3d programs. From Fig. 4 we see that for 1d programs, although speedup increases with the number of threads used, the efficiency decreases due to insufficient size of the problem, and one can achieve almost maximal value of speedup already with T=4 threads, while still keeping the efficiency around 50%. We also see, as expected, that speedup and efficiency of 2d and 3d programs behave quite well as we increase the numbers of threads. In particular, we note that the efficiency is always above 60%, making the use of all available CPU cores worthwhile. Fig. 4Speedup in the execution time and scaling efficiency of DBEC-GP-OMP programs compared to single-threaded runs: (a) imag1dX-th, (b) real1dX-th, (c) imag2dXY-th, (d) real2dXY-th, (e) imag3d-th, (f) real3d-th. Scaling efficiency is calculated as a fraction of the obtained speedup compared to a theoretical maximum. Grid sizes used for testing are the same as in Table 1. Speedups and efficiencies of imag1dZ-th, real1dZ-th, imag2dXZ-th, and real2dXZ-th (not reported here) are similar to those of imag1dX-th, real1dX-th, imag2dXY-th, and real2dXY-th, respectively. For testing of MPI versions we have used a similar methodology to measure the strong scaling performance. For OpenMP/MPI programs DBEC-GP-MPI, the obtained wall-clock times are shown in Table 2, together with the corresponding wall-clock times for the OpenMP programs DBEC-GP-OMP that served as a baseline to calculate speedups. The testing was done for varying number of cluster nodes, from 4 to 32, and the measured speedup ranged from 11 to 16.5. The corresponding graphs of speedups and efficiencies are shown in Fig. 5, where we can see that the speedup grows linearly with the number of nodes used, while the efficiency remains mostly constant in the range between 40% and 60%, thus making the use of OpenMP/MPI programs highly advantageous for problems with large grid sizes. Fig. 5Speedup in the execution time and scaling efficiency of DBEC-GP-MPI programs compared to single-node OpenMP runs: (a) imag3d-mpi, (b) real3d-mpi. Scaling efficiency is calculated as a fraction of the obtained speedup compared to a theoretical maximum. Grid size used for testing is the same as in Table 2. For CUDA/MPI programs DBEC-GP-MPI-CUDA we observe similar behavior in Table 3 and in Fig. 6. The obtained speedup with N=32 nodes here ranges from 9 to 10, with the efficiency between 30% and 40%. While the efficiency is slightly lower than in the case of OpenMP/MPI programs, which could be expected due to a more complex memory hierarchy when dealing with the multi-GPU system distributed over many cluster nodes, the speedup still grows linearly and makes CUDA/MPI programs ideal choice for use on GPU-enabled computer clusters. Additional benefit of using these programs is their low CPU usage (up to one CPU core), allowing for the possibility that same cluster nodes are used for other CPU-intensive simulations. Fig. 6Speedup in the execution time and scaling efficiency of DBEC-GP-MPI-CUDA programs compared to single-node runs of previous CUDA programs [2]: (a) imag3d-mpicuda, (b) real3d-mpicuda. Scaling efficiency is calculated as a fraction of the obtained speedup compared to a theoretical maximum. Grid size used for testing is the same as in Table 3. The introduction of distributed transposes of data creates some overhead, which negatively impacts scaling efficiency. This is more evident in the CUDA/MPI version, as the transpose algorithm is inferior to the one provided by FFTW3. In our tests, both MPI versions of programs failed to achieve speedup on less than 4 nodes, due to the introduction of the transpose routines. We therefore recommend using MPI versions only on 4 or more cluster nodes. The MPI versions are highly dependent not only on the configuration of the cluster, mainly on the speed of interconnect, but also on the distribution of processes and threads, NUMA configuration, etc. We recommend that users experiment with several different configurations to achieve the best performance. The results presented are obtained without extensive tuning, with the aim to show the base performance. Finally, we note that the best performance can be achieved by evenly distributing the workload among the MPI processes and OpenMP threads, and by using grid sizes which are optimal for FFT. In particular, the programs in DBEC-GP-OMP package have the best performance if NX, NY, and NZ are divisible by the number of OpenMP threads used. Similarly, for DBEC-GP-MPI programs the best performance is achieved if NX and NY are divisible by a product of the number of MPI processes and the number of OpenMP threads used. For DBEC-GP-MPI-CUDA programs, the best performance is achieved if NX and NY are divisible by a product of the number of MPI processes and the number of Streaming Multiprocessors (SM) in the GPU used. For all three packages, the best FFT performance is obtained if NX, NY and NZ can be expressed as 2a3b5c7d11e13f, where e and f are either 0 or 1, and the other exponents are non-negative integer numbers [8]. Additional comments, restrictions, and unusual features: MPI programs require that grid size (controlled by input parameters NX, NY and NZ) can be evenly distributed between the processes, i.e., that NX and NY are divisible by the number of MPI processes. Since the data is never distributed along the NZ dimension, there is no such requirement on NZ. Programs will test if these conditions are met, and inform the user if not (by reporting an error). Additionally, MPI versions of CUDA programs require CUDA-aware MPI implementation. This allows the MPI runtime to directly access GPU memory pointers and avoid having to copy the data to main RAM. List of CUDA-aware MPI implementations can be found in Ref. [9]. Acknowledgments V.L., S.Š. and A.B. acknowledge support by the Ministry of Education, Science, and Technological Development of the Republic of Serbia under projects ON171017, OI1611005, and III43007, as well as SCOPES project IZ74Z0-160453. L.E. Y.-S. acknowledges support by the FAPESP of Brazil under project 2012/21871-7 and 2014/16363-8. P.M. acknowledges support by the Science and Engineering Research Board, Department of Science and Technology, Government of India under project No.  EMR/2014/000644. S.K.A. acknowledges support by the CNPq of Brazil under project 303280/2014-0, and by the FAPESP of Brazil under project 2012/00451-0. References:[1]R. Kishor Kumar, L. E. Young-S., D. Vudragović, A. Balaž, P. Muruganandam, and S. K. Adhikari, Comput. Phys. Commun. 195 (2015) 117.[2]V. Lončar, A. Balaž, A. Bogojević, S. Škrbić, P. Muruganandam, S. K. Adhikari, Comput. Phys. Commun. 200 (2016) 406.[3]R. Kishor Kumar, P. Muruganandam, and B. A. Malomed, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 175302;H. Al-Jibbouri, I. Vidanović, A. Balaž, and A. Pelster, J. Phys. B: At. Mol. Opt. Phys. 46 (2013) 065303;R. R. Sakhel, A. R. Sakhel, and H. B. Ghassib, J. Low Temp. Phys. 173 (2013) 177;B. Nikolić, A. Balaž, and A. Pelster, Phys. Rev. A 88 (2013) 013624;X. Antoine and R. Duboscq, Comput. Phys. Commun. 185 (2014) 2969;J. Luo, Commun. Nonlinear Sci. Numer. Simul. 19 (2014) 3591;K.-T. Xi, J. Li, and D.-N. Shi, Physica B 436 (2014) 149;S. K. Adhikari, Phys. Rev. A 90 (2014) 055601;M. C. Raportaru, J. Jovanovski, B. Jakimovski, D. Jakimovski, and A. Mishev, Rom. J. Phys. 59 (2014) 677;A. I. Nicolin, A. Balaž, J. B. Sudharsan, and R. Radha, Rom. J. Phys. 59 (2014) 204;A. Balaž, R. Paun, A. I. Nicolin, S. Balasubramanian, and R. Ramaswamy, Phys. Rev. A 89 (2014) 023609;A. I. Nicolin and I. Rata, High-Performance Computing Infrastructure for South East Europe’s Research Communities: Results of the HP-SEE User Forum 2012, in Springer Series: Modeling and Optimization in Science and Technologies 2 (2014) 15;S. K. Adhikari, J. Phys. B: At. Mol. Opt. Phys. 48 (2015) 165303;S. K. Adhikari, Phys. Rev. E 92 (2015) 042926;T. Khellil and A. Pelster, arXiv:1512.04870 (2015);H. L. C. Couto and W. B. Cardoso, J. Phys. B: At. Mol. Opt. Phys. 48 (2015) 025301;R. R. Sakhel, A. R. Sakhel, and H. B. Ghassib, Physica B 478 (2015) 68;L. Salasnich and S. K. Adhikari, Acta Phys. Pol. A 128 (2015) 979;X. Antoine and R. Duboscq, Lecture Notes Math. 2146 (2015) 49;E. Chiquillo, J. Phys. A: Math. Theor. 48 (2015) 475001;S. Sabari, C. P. Jisha, K. Porsezian, and V. A. Brazhnyi, Phys. Rev. E 92 (2015) 032905;W. Wen, T. K. Shui, Y. F. Shan, and C. P. Zhu, J. Phys. B: At. Mol. Opt. Phys. 48 (2015) 175301;P. Das and P. K. Panigrahi, Laser Phys. 25 (2015) 125501;Y. S. Wang, S. T. Ji, Y. E. Luo, and Z. Y. Li, J. Korean. Phys. Soc. 67 (2015) L1504;A. I. Nicolin, M. C. Raportaru, and A. Balaž, Rom. Rep. Phys. 67 (2015) 143;V. S. Bagnato, D. J. Frantzeskakis, P. G. Kevrekidis, B. A. Malomed, and D. Mihalache, Rom. Rep. Phys. 67 (2015) 5;J. B. Sudharsan, R. Radha, H. Fabrelli, A. Gammal, and B. A. Malomed, Phys. Rev. A 92 (2015) 053601;K.-T. Xi, J. Li, and D.-N. Shi, Physica B 459 (2015) 6;E. J. M. Madarassy and V. T. Toth, Phys. Rev. D 91 (2015) 044041;F. I. Moxley III, T. Byrnes, B. Ma, Y. Yan, and W. Dai, J. Comput. Phys. 282 (2015) 303;D. Novoa, D. Tommasini, J. A. Nóvoa-López, Phys. Rev. E 91 (2015) 012904;Y. H. Wang, A. Kumar, F. Jendrzejewski, R. M. Wilson, M. Edwards, S. Eckel, G. K. Campbell, and C. W. Clark, New J. Phys. 17 (2015) 125012;T. Khellil, A. Balaž, and A. Pelster, New J. Phys. 18 (2016) 063003;T. Khellil and A. Pelster, J. Stat. Mech.-Theory Exp. (2016) 063301;J. Akram and A. Pelster, Phys. Rev. A 93 (2016) 023606;S. K. Adhikari, Laser Phys. Lett. 13 (2016) 035502;J. Akram and A. Pelster, Phys. Rev. A 93 (2016) 033610;J. Akram, B. Girodias, and A. Pelster, J. Phys. B: At. Mol. Opt. Phys. 49 (2016) 075302;S. K. Adhikari and S. Gautam, Phys. Rev. A 93 (2016) 013630;Ž. Marojević, E. Göklü, and C. Lämmerzahl, Comput. Phys. Commun. 202 (2016) 216;A. Paredes and H. Michninel, Phys. Dark Universe 12 (2016) 50;J. Akram and A. Pelster, Laser Phys. 26 (2016) 065501;T. Mithun, K. Porsezian, and B. Dey, Phys. Rev. A 93 (2016) 013620;C.-Y. Lai and C.-C. Chien, Phys. Rev. Appl. 5 (2016) 034001;S. K. Adhikari, Laser Phys. Lett. 13 (2016) 085501;K. Manikandan, P. Muruganandam, M. Senthilvelan, and M. Lakshmanan, Phys. Rev. E 93 (2016) 032212;R. R. Sakhel, A. R. Sakhel, H. B. Ghassib, and A. Balaž, Eur. Phys. J. D 70 (2016) 66;W. Bao, Q. Tang, and Y. Zhang, Commun. Comput. Phys. 19 (2016) 1141;R. Kishor Kumar, T. Sriraman, H. Fabrelli, P. Muruganandam, and A. Gammal, J. Phys. B: At. Mol. Opt. Phys. 49 (2016) 155301;A. Bogojević, A. Balaž, and A. Belić, Phys. Rev. E 72 (2005) 036128;A. Bogojević, I. Vidanović, A. Balaž, and A. Belić, Phys. Lett. A 372 (2008) 3341;I. Vidanović, A. Bogojević, A. Balaž, and A. Belić, Phys. Rev. E 80 (2009) 066706;A. Balaž, A. Bogojević, I. Vidanović, and A. Pelster, Phys. Rev. E 79 (2009) 036701;A. Balaž, I. Vidanović, A. Bogojević, and A. Pelster, Phys. Lett. A 374 (2010) 1539;A. I. Nicolin, Physica A 391 (2012) 1062;I. Vasić and A. Balaž, arXiv:1602.03538 (2016);O. Voronych, A. Buraczewski, M. Matuszewski, and M. Stobińska, arXiv:1603.02570 (2016);A. M. Martin, N. G. Marchant, D. H. J. O’Dell, and N. G. Parker, arXiv:1606.07107 (2016).[4]D. Vudragović, I. Vidanović, A. Balaž, P. Muruganandam, and S. K. Adhikari, Comput. Phys. Commun. 183 (2012) 2021.[5]FFTW3 library, http://www.fftw.org/ (2016).[6]2DECOMP&FFT library, http://www.2decomp.org/ (2016).[7]B. Satarić, V. Slavnić, A. Belić, A. Balaž, P. Muruganandam, S. K. Adhikari, Comput. Phys. Commun. 200 (2016) 411. [8]Real-data DFTs with FFTW3, http://www.fftw.org/fftw3_doc/Real_002ddata-DFTs.html (2014);Nvidia’s cuFFT accuracy and performance, http://docs.nvidia.com/cuda/cufft/#accuracy-and-performance (2015).[9]Nvidia’s MPI Solutions for GPUs, https://developer.nvidia.com/mpi-solutions-gpus (2016)."
}
@article{SAMUEL2018,
title = "Expertise psychiatrique : quelle est la responsabilité pénale en cas de consommation de cannabis ?",
journal = "Annales Médico-psychologiques, revue psychiatrique",
year = "2018",
issn = "0003-4487",
doi = "https://doi.org/10.1016/j.amp.2018.09.009",
url = "http://www.sciencedirect.com/science/article/pii/S0003448718302737",
author = "Mariana Samuel and Michel Benoit and Nidal Nabhan Abou",
keywords = "Addiction, Cannabis, Dépendance, Discernement, Expertise psychiatrique, Pathologie psychiatrique, Région PACA, Responsabilité pénale, Addiction, Cannabis, Criminal liability, Discernment, Psychiatric expertise, Psychiatric pathology, PACA region",
abstract = "Résumé
Introduction
La consommation de cannabis est depuis quelques années un problème de santé publique en France, notamment par la démocratisation de cette drogue chez les jeunes. Bien que la législation soit claire en ce qui concerne la responsabilité des personnes sous l’emprise de stupéfiants en cas d’infractions, il n’est pas rare que les personnes soient reconnues irresponsables ou partiellement responsables. L’objectif de cette étude est de déterminer si une consommation de cannabis au moment des faits est retenue par les experts comme un facteur de d’altération ou d’abolition du discernement.
Méthode
Une étude descriptive rétrospective réalisée dans la région PACA auprès de six experts a recueilli 96 expertises pénales en responsabilité pour des infractions commises sous l’emprise de cannabis entre 2016 et 2018. Le critère d’évaluation principal était le discernement, défini soit comme aboli, altéré ou non modifié. Les objectifs secondaires étaient d’observer les facteurs associés à une modification de celui-ci.
Résultats
La consommation de cannabis n’a pas d’impact sur le degré de discernement. La dépendance au cannabis chez l’auteur des faits n’est pas significativement associée à une atteinte du discernement. En revanche, le sexe masculin, l’existence d’un trouble de la personnalité, une immaturité affective, les antécédents psychiatriques, un suivi psychiatrique ou addictologique au moment des faits et une décompensation psychotique sont significativement associés à une atteinte du discernement.
Conclusion
Ces résultats soulignent l’importance des mesures de prévention et de réduction des risques liés au cannabis, notamment dans ce contexte de légalisation ou dépénalisation du cannabis.
Objectives
In recent years, cannabis use has been a public health problem in France, particularly through the democratization of this drug among young people. The impact of chronic cannabis use on impulsivity and risk taking therefore questions the role of cannabis in offenses. Although the legislation is clear as regards the liability of persons under the influence of drugs in the event of an offense, it is not unusual for individuals to be found irresponsible or partially responsible. The purpose of this study is to determine whether cannabis use at the time of the facts is retained by the experts as a factor of alteration or abolition of discernment, but also to identify the elements associated with an impairment of discernment.
Patients or materials and methods
A retrospective descriptive study conducted in the PACA region with 6 experts collected 96 criminal appraisals of responsibility for offenses committed under the influence of cannabis between 2016 and 2018. The inclusion criteria include the age of the alleged perpetrators that had to be between 18 and 65 years old and cannabis intoxication at the time of the facts. The expert reports for which the expert did not mention a possible cannabis intoxication at the time of the events but which he emphasized a significant daily consumption were included. The criterion of non-inclusion was the impossibility for the expert to pronounce on the existence or not of a psychic or neuropsychic disorder having abolished or impaired the discernment or control of the acts. Appraisals for which no use of cannabis at the time of the facts was found were excluded, likewise for the expertises whose question of discernment at the time of the facts according to article 122-1 of the Penal Code was not raised. (postsentencing expertise). The primary outcome measure was to determine whether or not the alleged perpetrator is responsible for the alleged acts. It then corresponds to the expert's conclusion concerning the question of the discernment and control of acts at the material time. Thus, three possible answers were obtained: discernment abolished, discernment altered or unmodified discernment. The secondary objectives were to determine the factors that may have an impact on the assessment of judgment and thus on criminal responsibility. Secondary outcomes included biographical information (age, sex, geography, placement, abuse or rape in childhood, parenthood, marital status, socioprofessional category), personal psychiatric history (personality disorder, bipolar disorder, and psychotic disorder, acute delirium and cannabic psychosis, impulsivity) and family psychiatric history. A criminal record as a victim but especially as an author was sought. If so, is the subject subject to or has been sentenced to socio-judicial follow-up (care ordered by the penitentiary, followed by Prison and Probation Services (SPIP), electronic surveillance)? Is the act currently being reproached committed in a state of legal recidivism? Finally, the toxicological profile was analyzed to determine whether cannabis use at the time of the event occurred in a context of cannabis addiction or harmful use, but also to assess concomitant use of other illicit substances or alcohol at the time facts.
Results
Associations were searched for bivariate analysis (Chi2 test or Fisher's exact test) and then multivariate analysis (logistic regression). The variables selected for the multivariate analysis are those for which P<0.05 in bivariate analysis. Discernment was abolished in 6 expert reports (6.25%), impaired in 19 expert reports (19.79%) and unmodified in 71 reports (73.96%). Cannabis dependence accounted for 81.25% of the sample and the harmful use of cannabis for 18.75% of the sample. Among the individuals with a cannabis addiction, the experts concluded that discernment was abolished in 4.17% of cases, discernment was impaired in 17.71% of cases, and there was no discernment or abolition of discernment. 59.38% of cases. In the case of harmful use of cannabis, the results were respectively 14.58%, 2.08% and 2.08%. The cannabis poisoning at the time of the facts is not therefore retained by the experts as a factor of alteration or abolition of discernment. An impairment of discernment was significantly associated with the presence of psychiatric history (P=6.973.10 ^ -5), psychiatric or addictological management (P=0.017), personality disorder (P=0.032), emotional immaturity (P=0.022) and psychiatric comorbidity (P=1.194.10 ^ -5) at the time of the events. Age was also a factor correlated with a change in discernment (P=0.001). However, in multivariate analysis, only age (P=0.017, OR=1.113, 95% CI [1.016, 1.175]) and the existence of psychiatric pathology at the time of the events (P=0.004, OR=1.262.10–1 95% CI [2966; 6956.10–1]) were statistically significant.
Conclusion
The results of the study are in line with those expected from the literature. Psychiatric co-morbidities at the time of the events and in particular psychoses (decompensation of schizophrenia and paranoid delusion) are the first causes of abolition of the discernment, with 2/3 of the people addicted to cannabis. With regard to the causes of discernment, psychoses are still strongly represented, but to these are added, among others, personality disorders (antisocial and borderline personality disorder) and intellectual deficit. It is important to note in this category, however, the high proportion of cannabis addicts (41.67%) and addictive comorbidities (33%). The vast majority of those whose discernment was neither altered nor abolished at the time of the incident, suffered no psychiatric pathology at the time of the offense. Where appropriate, the experts primarily diagnosed antisocial and borderline personality disorders. These results underline the importance of prevention and risk reduction measures related to cannabis, particularly in this context of legalization or decriminalization of cannabis."
}
@article{CIMO2018,
title = "Tailoring Diabetes Education to Meet the Needs of Adults With Type 2 Diabetes and Mental Illness: Client and Health-Care Provider Perspectives From an Exploratory Pilot Study",
journal = "Canadian Journal of Diabetes",
year = "2018",
issn = "1499-2671",
doi = "https://doi.org/10.1016/j.jcjd.2018.09.008",
url = "http://www.sciencedirect.com/science/article/pii/S1499267118302442",
author = "Adriana Cimo and Carolyn S. Dewa",
keywords = "diabetes self-management education, mental illness, self-care, solutions, tailored care, enseignement sur la prise en charge autonome du diabète, maladie mentale, autosoins, solutions, soins adaptés",
abstract = "Objectives
People with mental illness are more likely to experience poorer outcomes with type 2 diabetes than the general population. Diabetes management can be improved when lifestyle-intervention content is tailored to the learning needs of individuals or groups. The purpose of this pilot study was to explore the perspectives of clients and providers involved with mental health care with regard to how diabetes education can effectively address the challenges that may be faced when people with mental illness engage in diabetes self-care behaviours.
Methods
Focus groups included 17 people with mental illness and type 2 diabetes and 21 mental health clinicians. Data were transcribed verbatim, assessed for quality and saturation and coded to identify relationships and meanings among identified themes.
Results
Participants described strategies concerning how to consider symptoms of mental illness and address the psychosocial challenges that people with mental illness may be more likely to experience. Teaching strategies identified by clinicians and clients that were perceived to be effective included allowing clients to guide education session content, and being flexible when providing support, identifying education topics to discuss and teaching about diabetes. Participants also emphasized the importance of empowering clients by helping them to see how sustainable behaviour changes can be achieved. Differences between the perspectives of the clients receiving mental health care and the clinicians were often related to neglecting to begin with client-driven needs assessments.
Conclusions
Our study offers diabetes educators a strategy for applying Diabetes Canada's self-management education guidelines to the needs of people with mental illness by using suggestions from clients and clinicians.
Résumé
Objectifs
Les personnes atteintes d'une maladie mentale sont plus susceptibles que la population générale de connaître de moins bons résultats si elles sont atteintes de diabète de type 2. Il est possible d'avoir une meilleure prise en charge du diabète lorsque le contenu de l'intervention sur le mode de vie est adapté aux besoins d'apprentissage des individus ou des groupes. L'objectif de la présente étude pilote était d'examiner les points de vue des clients et des prestataires impliqués dans les soins de santé mentale en ce qui concerne la façon dont l'enseignement sur le diabète peut aborder efficacement les difficultés à relever lorsque les personnes atteintes d'une maladie mentale adoptent des comportements d'autosoins liés au diabète.
Méthodes
Les groupes de discussion comptaient 17 personnes atteintes d'une maladie mentale et du diabète de type 2, et 21 cliniciens en santé mentale. Les données étaient transcrites textuellement, faisaient l'objet d'une évaluation de la qualité et de la saturation, et étaient codées pour cerner les liens et les significations parmi les thèmes retenus.
Résultats
Les participants décrivaient les stratégies sur la façon d'envisager les symptômes de santé mentale et d'aborder les difficultés psychosociales que les personnes atteintes d'une maladie mentale sont plus susceptibles de connaître. Les stratégies d'enseignement perçues comme efficaces que les cliniciens et les clients avaient déterminées étaient de permettre aux clients de suggérer le contenu des séances d'enseignement, de faire preuve de souplesse lors qu'ils offrent du soutien, de déterminer les sujets d'enseignement à traiter et d'offrir un enseignement sur le diabète. Les participants soulignaient également l'importance de rendre les clients autonomes en les aidant à découvrir comment atteindre des changements de comportements durables. Les différences entre les points de vue des clients et des cliniciens en soins de santé mentale étaient souvent liées à l'omission de faire des évaluations initiales des besoins axés sur les clients.
Conclusions
Notre étude fournit aux éducateurs en diabète une stratégie d'application des lignes directrices en matière d'enseignement sur les autosoins de Diabète Canada aux besoins des personnes atteintes d'une maladie mentale en tenant compte des suggestions des clients et des cliniciens."
}
@article{STEWART2014655,
title = "Maintenance of a Physically Active Lifestyle After Pulmonary Rehabilitation in Patients With COPD: A Qualitative Study Toward Motivational Factors",
journal = "Journal of the American Medical Directors Association",
volume = "15",
number = "9",
pages = "655 - 664",
year = "2014",
issn = "1525-8610",
doi = "https://doi.org/10.1016/j.jamda.2014.05.003",
url = "http://www.sciencedirect.com/science/article/pii/S1525861014002862",
author = "Kelly F.J. Stewart and Jessie J.M. Meis and Coby van de Bool and Daisy J.A. Janssen and Stef P.J. Kremers and Annemie M.W.J. Schols",
keywords = "Chronic obstructive pulmonary disease, pulmonary rehabilitation, behavior maintenance, autonomous motivation",
abstract = "Objectives
To explore determinants of behavior change maintenance of a physically active lifestyle in patients with chronic obstructive pulmonary disease (COPD) 8–11 months after completion of a 4-month outpatient pulmonary rehabilitation program.
Design
A qualitative descriptive study of semistructured interviews.
Setting
Pulmonary rehabilitation assessment center.
Participants
Patients with COPD.
Measurements
Semistructured interviews until data saturation, coded by 2 independent researchers. Patients were classified as responder (maintenance or improvement) or nonresponder (relapse or decrease), based on 3 quantitative variables reflecting exercise capacity (Constant Work Rate Test), health-related quality of life (Short-Form health survey [SF-36]), and self-management abilities (Self-Management Ability Scale [SMAS-30/Version 2]).
Results
Mean (SD) forced expiratory volume in the first second (FEV1) among interviewees was 52.5% (14.4%) predicted and the mean age was 63.5 years (range: 45–78). The group consisted of 15 responders and 7 nonresponders. Physical limitations reduced competence to engage in an active lifestyle and responders appeared to experience higher levels of perceived competence. Social support was found important and the experienced understanding from fellow patients made exercising together enjoyable. Particularly, responders expressed autonomous motivation and said they exercised because of the benefits they gain from it. Unexpectedly, only responders also experienced controlled motivation.
Conclusion
Perceived competence and autonomous motivation are important determinants for maintenance of an active lifestyle in patients with COPD. In contrast to common theoretical assumptions, a certain threshold level of controlled motivation may remain important in maintaining a physically active lifestyle after a pulmonary rehabilitation program."
}
@article{JABLONSKI2013440,
title = "Improved algorithm for calculating the Chandrasekhar function",
journal = "Computer Physics Communications",
volume = "184",
number = "2",
pages = "440 - 442",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2012.08.020",
url = "http://www.sciencedirect.com/science/article/pii/S001046551200286X",
author = "A. Jablonski",
keywords = "Chandrasekhar function, Isotropic scattering",
abstract = "Theoretical models of electron transport in condensed matter require an effective source of the Chandrasekhar H(x,omega) function. A code providing the H(x,omega) function has to be both accurate and very fast. The current revision of the code published earlier [A. Jablonski, Comput. Phys. Commun. 183 (2012) 1773] decreased the running time, averaged over different pairs of arguments x and omega, by a factor of more than 20. The decrease of the running time in the range of small values of the argument x, less than 0.05, is even more pronounced, reaching a factor of 30. The accuracy of the current code is not affected, and is typically better than 12 decimal places.
New version program summary
Program title: CHANDRAS_v2 Catalogue identifier: AEMC_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEMC_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 976 No. of bytes in distributed program, including test data, etc.: 11416 Distribution format: tar.gz Programming language: Fortran 90 Computer: Any computer with a Fortran 90 compiler Operating system: Windows 7, Windows XP, Unix/Linux RAM: 0.7 MB Classification: 2.4, 7.2 Catalogue identifier of previous version: AEMC_v1_0 Journal reference of previous version: Comput. Phys. Commun. 183 (2012) 1773 Does the new version supersede the old program: Yes Nature of problem: An attempt has been made to develop a subroutine that calculates the Chandrasekhar function with high accuracy, of at least 10 decimal places. Simultaneously, this subroutine should be very fast. Both requirements stem from the theory of electron transport in condensed matter. Solution method: Two algorithms were developed, each based on a different integral representation of the Chandrasekhar function. The final algorithm is edited by mixing these two algorithms by selecting ranges of the argument omega in which the performance is the fastest. Reasons for the new version: Some of the theoretical models describing electron transport in condensed matter need a source of the Chandrasekhar H function values with an accuracy of at least 10 decimal places. Additionally, calculations of this function should be as fast as possible since frequent calls to a subroutine providing this function are made (e.g., numerical evaluation of a double integral with a complicated integrand containing the H function). Both conditions were satisfied in the algorithm previously published [1]. However, it has been found that a proper selection of the quadrature in an integral representation of the Chandrasekhar function may considerably decrease the running time. By suitable selection of the number of abscissas in Gauss–Legendre quadrature, the execution time was decreased by a factor of more than 20. Simultaneously, the accuracy of results has not been affected. Summary of revisions: (1) As in previous work [1], two integral representations of the Chandrasekhar function, H(x,omega), were considered: the expression published by Dudarev and Whelan [2] and the expression published by Davidović et al. [3]. The algorithms implementing these representations were designated A and B, respectively. All integrals in these implementations were previously calculated using Romberg quadrature. It has been found, however, that the use of Gauss–Legendre quadrature considerably improved the performance of both algorithms. Two conditions have to be satisfied. (i) The number of abscissas, N, has to be rather large, and (ii) the abscissas and corresponding weights should be determined with accuracy as high as possible. The abscissas and weights are available for N=16, 20, 24, 32, 40, 48, 64, 80, and 96 with accuracy of 20 decimal places [4], and all these values were introduced into a new procedure GAUSS replacing procedure ROMBERG. Due to the fact that the implemented tables are rather extensive, they were recalculated using the Rybicki algorithm (Ref. [5], pp. 183–184) and rechecked. No errors or misprints were found. (2) In the integral representation of the H function derived by Davidović et al. [3], the positive root ν0 of the so-called dispersion function needs to be calculated with accuracy of at least 10 decimal places (see. Ref [6], pp. 61–64 and Ref. [1], Eqs. (5) and (29)). For small values of the argument omega and values of omega close to unity, the nonlinear equation in one unknown, ν0, can be solved analytically. New simple analytical expressions were derived here that can be efficiently used in calculations of the root. (3) The above modifications of the code considerably decreased the time of calculation of both algorithms A and B. The results are summarized in Fig. 1. The time of calculations is in fact the CPU time in microseconds for a computer equipped with an Inter Xeon processor (3.46 GHz) using Lahey–Fujitsu Fortran v. 7.2. Fig. 1Time of calculations of the H(x,omega) function averaged over different pairs of arguments x and omega. (a) 400 pairs uniformly distributed in the ranges 0<=x<=0.05 and 0<=omega<=1; (b) 400 pairs uniformly distributed in the ranges 0.05<=x<=1 and 0<=omega<=1. The shortest execution time averaged over values of the argument x exceeding 0.05 has been observed for algorithm B and Gauss–Legendre quadrature with the number of abscissas equal to 64 (23.2 μs). As compared with Romberg quadrature, the execution time was shortened by a factor of 22.5. For small x values, below 0.05, both algorithms A and B are considerably faster if Gauss–Legendre quadrature is used. For N=64, the average time of execution of algorithm B is decreased with respect to Romberg quadrature by a factor close to 30. However, in that range of argument x, algorithm A exhibits much faster performance. Furthermore, the average execution time of algorithm A, equal to about 100 μs, is practically independent of the number of abscissas N. (4) For Romberg quadrature, to optimize the performance, the mixed algorithm C was proposed in which algorithm A is used for argument x smaller than or equal to x0=0.4, while algorithm B is used for x larger than 0.4 [1]. For Gauss–Legendre quadrature, the limit x0 was found to depend on the number of abscissas N. For each value of N considered, the time of calculations of the H function was determined for pairs of arguments uniformly distributed in the ranges 0<=x<=0.05 and 0<=omega<=1, and for pairs of arguments uniformly distributed in the ranges 0.05<=x<=1 and 0<=omega<=1. As shown in Fig. 2 for N=64, algorithm A is faster than algorithm B for x smaller than or equal to 0.0225. Fig. 2Comparison of the running times of algorithms A and B. Open circles: algorithm B is faster than the algorithm A; full circles: algorithm A is faster than algorithm B. Thus, the value of x0=0.0225 is proposed for the mixed algorithm C when Gauss–Legendere quadrature with N=64 is used. Similar computer experiments performed for other values of N are summarized below. LNx01160.252200.153240.104320.0505400.0306480.0457640.0225–Recommended8800.01259960.020 The flag L is one of the input parameters for the subroutine GAUSS. In the programs implementing algorithms A, B, and C (CHANDRA, CHANDRB, and CHANDRC), Gauss–Legendre quadrature with N=64 is currently set. As follows from Fig. 1, algorithm B (and consequently algorithm C) is the fastest in that case. It is still possible to change the number of abscissas; the flag L then has to be modified in lines 165, 169, 185, 189, and 304 of program CHANDRAS_v2, and the value of x0 in line 111 has to be adjusted according to the table above. (5) The above modifications of the code did not affect the accuracy of the calculated Chandrasekhar function, as compared to the original code [1]. For the pairs of arguments shown in Fig. 2, the accuracy of the H function, calculated from algorithms A and B, reached at least 12 decimal digits; however, in the majority of cases, the accuracy is equal to 13 decimal digits. Restrictions: Two input parameters for the Chandrasekhar function, x and omega, are restricted to the ranges 0<=x<=1 and 0<=omega<=1, which is sufficient in numerous applications. Running time: between 15 and 100 μs for one pair of arguments of the Chandrasekhar function. References:[1]A. Jablonski, Comput. Phys. Comm. 183 (2012) 1773.[2]S.L. Dudarev, M.J. Whelan, Surf. Sci. 311 (1994) L687.[3]D.M. Davidović, J. Vukanić, D. Arsenović, Icarus 194 (2008) 389.[4]M. Abramowitz, I.A. Stegun (Eds.), Handbook of Mathematical Functions, Dover Publications, Inc., New York, 1972, pp. 916–919.[5]W. H. Press, S. A. Teukolsky, W. T. Vetterling, B. P. Flannery, Numerical recipes, in: The Art of Scientific Computing, Cambridge University Press, Cambridge, 2007.[6]K.M. Case, P.F. Zweifel, Linear Transport Theory, Addison-Wesley, Reading, MA, 1967, pp. 61–65."
}
@article{ABRAHAO2017138,
title = "A method for cohort selection of cardiovascular disease records from an electronic health record system",
journal = "International Journal of Medical Informatics",
volume = "102",
pages = "138 - 149",
year = "2017",
issn = "1386-5056",
doi = "https://doi.org/10.1016/j.ijmedinf.2017.03.015",
url = "http://www.sciencedirect.com/science/article/pii/S138650561730076X",
author = "Maria Tereza Fernandes Abrahão and Moacyr Roberto Cuce Nobre and Marco Antonio Gutierrez",
keywords = "Secondary use of data, Medical informatics, Electronic health records, Cohort studies, Retrospective studies and data mining",
abstract = "Introduction
An electronic healthcare record (EHR) system, when used by healthcare providers, improves the quality of care for patients and helps to lower costs. Information collected from manual or electronic health records can also be used for purposes not directly related to patient care delivery, in which case it is termed secondary use. EHR systems facilitate the collection of this secondary use data, which can be used for research purposes like observational studies, taking advantage of improvement in the structuring and retrieval of patient information. However, some of the following problems are common when conducting a research using this kind of data: (i) Over time, systems and data storage methods become obsolete; (ii) Data concerns arise since the data is being used in a context removed from its original intention; (iii) There are privacy concerns when sharing data about individual subjects; (iv) The partial availability of standard medical vocabularies and natural language processing tools for non-English language limits information extraction from structured and unstructured data in the EHR systems. A systematic approach is therefore needed to overcome these, where local data processing is performed prior to data sharing.
Method
The proposed study describes a local processing method to extract cohorts of patients for observational studies in four steps: (1) data reorganization from an existing local logical schema into a common external schema over which information can be extracted; (2) cleaning of data, generation of the database profile and retrieval of indicators; (3) computation of derived variables from original variables; (4) application of study design parameters to transform longitudinal data into anonymized data sets ready for statistical analysis and sharing. Mapping from the local logical schema into a common external schema must be performed differently for each EHR and is not subject of this work, but step 2, 3 and 4 are common to all EHRs. The external schema accepts parameters that facilitate the extraction of different cohorts for different studies without having to change the extraction algorithms, and ensures that, given an immutable data set, can be done by the idempotent process. Statistical analysis is part of the process to generate the results necessary for inclusion in reports. The generation of indicators to describe the database allows description of its characteristics, highlighting study results. The set extraction/statistical processing is available in a version controlled repository and can be used at any time to reproduce results, allowing the verification of alterations and error corrections. This methodology promotes the development of reproducible studies and allows potential research problems to be tracked upon extraction algorithms and statistical methods
Results
This method was applied to an admissions database, SI3, from the InCor-HCFMUSP, a tertiary referral hospital for cardiovascular disease in the city of São Paulo, as a source of secondary data with 1116848 patients records from 1999 up to 2013. The cleaning process resulted in 313894 patients records and 27698 patients in the cohort selection, with the following criteria: study period: 2003–2013, gender: Male, Female, age:≥18years old, at least 2 outpatient encounters, diagnosis of cardiovascular disease (ICD-10 codes: I20-I25, I64-I70 and G45). An R script provided descriptive statistics of the extracted cohort.
Conclusion
This method guarantees a reproducible cohort extraction for use of secondary data in observational studies with enough parameterization to support different study designs and can be used on diverse data sources. Moreover it allows observational electronic health record cohort research to be performed in a non-English language with limited international recognized medical vocabulary."
}
@article{ABEL2013382,
title = "Running-induced epigenetic and gene expression changes in the adolescent brain",
journal = "International Journal of Developmental Neuroscience",
volume = "31",
number = "6",
pages = "382 - 390",
year = "2013",
note = "Special Issue on Developmental Aspects of Epigenetic Research",
issn = "0736-5748",
doi = "https://doi.org/10.1016/j.ijdevneu.2012.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S0736574812005850",
author = "Jean LeBeau Abel and Emilie F. Rissman",
keywords = "Epigenetic, Exercise, Puberty, Adolescent, Cerebellum, BDNF",
abstract = "Physical exercise is associated with positive neural functioning. Here we examined the gene expression consequences of 1 week of voluntary wheel running in adolescent male mice. We assayed expression levels of genes associated with synaptic plasticity, signaling pathways, and epigenetic modifying enzymes. Two regions were examined: the hippocampus, which is typically examined in exercise studies, and the cerebellum, an area directly involved in motor control and learning. After 1 week of exercise, global acetylation of histone 3 was increased in both brain regions. Interestingly this was correlated with increased brain derived neural growth factor in the hippocampus, as noted in many other studies, but only a trend was found in cerebellum. Differences and similarities between the two areas were noted for genes encoding functional proteins. In contrast, the expression pattern of DNA methyltransferases (Dnmts) and histone deacetylases (Hdacs), genes that influence DNA methylation and histone modifications in general, decreased in both regions with exercise. We hypothesize that epigenetic mechanisms, involving many of the genes assessed here, are essential for the positive affects of exercise on behavior and suspect these data have relevance for adolescent boys."
}
@article{BOLAM201447,
title = "Macrofaunal production and biological traits: Spatial relationships along the UK continental shelf",
journal = "Journal of Sea Research",
volume = "88",
pages = "47 - 58",
year = "2014",
issn = "1385-1101",
doi = "https://doi.org/10.1016/j.seares.2014.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S1385110114000057",
author = "S.G. Bolam and J.D. Eggleton",
keywords = "Benthic Communities, Biological Traits, Secondary Production, Continental Shelf, Fuzzy Coding",
abstract = "Biological trait analysis (BTA) is increasingly being employed to improve our understanding of the ecological functioning of marine benthic invertebrate communities. However, changes in trait composition are seldomly compared with concomitant changes in metrics of ecological function. Consequently, inferences regarding the functional implications of any changes are often anecdotal; we currently have a limited understanding of the functional significance of the traits commonly used. In this study, we quantify the relationship between benthic invertebrate trait composition and secondary production estimates using data spanning almost the breadth of the UK continental shelf. Communities described by their composition of 10 traits representing life history, morphology and behaviour showed strong relationships with variations in total secondary production. A much weaker relationship was observed for community productivity (or P:B), a measure of rate of energy turnover. Furthermore, the relationship between total production and multivariate taxonomic community composition was far weaker than that for trait composition. Indeed, the similarities between communities as defined by taxonomy were very different from those depicted by their trait composition. That is, as many studies have demonstrated, taxonomically different communities may display similar trait compositions, and vice versa. Finally, we found that descriptions of community trait composition vary greatly depending on whether abundance or biomass is used as the enumeration weighting method during BTA, and trait assessments based on biomass produced better relations with secondary production than those based on abundance. We discuss the significance of these findings with respect to BTA using marine benthic invertebrates."
}
@article{GONZALES2012244,
title = "Seismic performance of buildings with thin RC bearing walls",
journal = "Engineering Structures",
volume = "34",
pages = "244 - 258",
year = "2012",
issn = "0141-0296",
doi = "https://doi.org/10.1016/j.engstruct.2011.10.007",
url = "http://www.sciencedirect.com/science/article/pii/S0141029611004044",
author = "H. Gonzales and F. López-Almansa",
keywords = "Seismic vulnerability, Thin supporting walls, Reinforced concrete, Push-over analysis, Nonlinear time history analysis",
abstract = "In Latin America there is an urgent need for housing; thus, during the last few years a relevant number of mid-height buildings (usually, up to five stories) with thin RC shear–walls have been constructed for low-cost dwellings in Bolivia, Colombia, Ecuador, Mexico, Peru, Venezuela, and other countries located in seismic-prone regions. These walls are 10cm thick and their reinforcement consists mainly of a single layer of welded wire mesh. This construction technology offers two main advantages: economy and rapidity of construction. These buildings do not fulfill the international seismic codes but some national regulations are less demanding, not preventing the use of thin bearing walls. These buildings might be vulnerable to earthquakes because of their low ductility, the insufficiency of the experimental information, the absence of observed damages and, in some cases, poor construction quality. This work describes the initial steps of a wider research aiming at providing reliable seismic design guidelines for thin-wall buildings; the initial objectives are analyzing the seismic performance of these buildings, proposing preliminary design criteria and identifying further research needs. This research focuses on buildings located in Peru, being representative of the situations in the other countries. The vulnerability is numerically evaluated by push-over and nonlinear time history analyses; the structural parameters are obtained from available testing information. The obtained results show that the seismic strength of the analyzed buildings is insufficient; however, minor changes in the structural design might improve significantly their seismic performance. Cheap and easy-to-implement design recommendations are issued."
}
@article{FENG2015192,
title = "A 3-D model for void evolution in viscous materials under large compressive deformation",
journal = "International Journal of Plasticity",
volume = "74",
pages = "192 - 212",
year = "2015",
issn = "0749-6419",
doi = "https://doi.org/10.1016/j.ijplas.2015.06.012",
url = "http://www.sciencedirect.com/science/article/pii/S0749641915001084",
author = "Chao Feng and Zhenshan Cui",
keywords = "Voids and inclusions, Viscoplastic material, Numerical algorithms, Representative volume element",
abstract = "The paper presents a study on the evolution of dilute ellipsoidal voids in power-law viscous materials under triaxial loading condition. Firstly, referring to the work of Eshelby (1957), a semi-analytical expression is deduced to evaluate the deformation of ellipsoidal void in linear viscous material. Then, for the non-linear viscous materials, the concept of mesoscopic representative volume element (RVE) is applied to study the voids deformation under different stress states, and a rigid visco-plastic finite element (FE) procedure is applied to solve the RVE model. For the condition of stress triaxiality ranging from −1 to +1, it is found that the voids deformation behaves similarly in both linear and non-linear viscous materials. Due to this fact, the framework of the expression of void deformation in linear viscous material is inferred to describe the void evolution in non-linear viscous materials, while the parameters of the expression are re-evaluated for the specific materials. The results show that the void shapes and loading conditions take important roles in the void evolution. Therefore, for an ellipsoidal void, the void radius strain rate is expressed as a function of the void shape index, the macroscopic stress and strain-rate. Meanwhile, the void volume strain rate is obtained as a function of the void radius strain rate. This void evolution model is integrated into FE code and applied to study the void closure problem in the metal forming process. The FE simulation provides the evolution of macroscopic stress, strain and strain-rate, and then the model is used to calculate the changes of void shape and volume in each step of the deformation history. It can be found that the results predicted by this model agree well with the analytical solution, experiment measurements and numerical simulations with embedded void shapes, which demonstrates that this method can be appropriately used to predict the void evolution during the large compressive deformation process."
}
@article{NISHI201642,
title = "Assessment for innovative combustion on HCCI engine by controlling EGR ratio and engine speed",
journal = "Applied Thermal Engineering",
volume = "99",
pages = "42 - 60",
year = "2016",
issn = "1359-4311",
doi = "https://doi.org/10.1016/j.applthermaleng.2015.11.126",
url = "http://www.sciencedirect.com/science/article/pii/S1359431115013897",
author = "Mina Nishi and Masato Kanehara and Norimasa Iida",
keywords = "Internal combustion engine, Premixed combustion, Numerical analysis, HCCI (homogeneous charge compression ignition) engine, EGR (exhaust gas recirculation), DME (dimethyl ether)",
abstract = "HCCI (homogeneous charge compression ignition) combustion offers both high efficiency and very low NOx and particulate matter emissions. However, the operating range of HCCI engines is limited by an excessive PRR (pressure rise rate) in high load region, which is the main reason of engine knock. For this problem shown in HCCI combustion, ignition timing should be retarded after TDC (top dead center) by controlling gas temperature properly, and in addition, combustion duration also needs to be ensured adequately. EGR (exhaust gas recirculation) gas is known to have impacts on the history of gas temperature, and the objective of this study is to investigate the influence of EGR ratio on combustion characteristics. The computational modeling work is conducted by using a single-zone code with detailed chemical kinetics. Then, in order to investigate the influence of EGR ratio and the engine speed, contribution matrix has been used to extract important reaction paths from a reaction mechanism."
}
@article{MALYARCHUK201456,
title = "A mitogenomic phylogeny and genetic history of sable (Martes zibellina)",
journal = "Gene",
volume = "550",
number = "1",
pages = "56 - 67",
year = "2014",
issn = "0378-1119",
doi = "https://doi.org/10.1016/j.gene.2014.08.015",
url = "http://www.sciencedirect.com/science/article/pii/S0378111914009287",
author = "Boris Malyarchuk and Miroslava Derenko and Galina Denisova",
keywords = "Mitochondrial DNA, , Phylogeny, Phylogeography, Adaptive evolution",
abstract = "We assessed phylogeny of sable (Martes zibellina, Linnaeus, 1758) by sequence analysis of nearly complete, new mitochondrial genomes in 36 specimens from different localities in northern Eurasia (Primorye, Khabarovsk and Krasnoyarsk regions, the Kamchatka Peninsula, the Kuril Islands and the Urals). Phylogenetic analysis of mtDNA sequences demonstrates that two clades, A and BC, radiated about 200–300thousandyears ago (kya) according to results of Bayesian molecular clock and RelTime analyses of different mitogenome alignments (nearly complete mtDNA sequences, protein-coding region, and synonymous sites), while the age estimates of clades A, B and C fall within the Late Pleistocene (~50–140kya). Bayesian skyline plots (BSPs) of sable population size change based on analysis of nearly complete mtDNAs show an expansion around 40kya in the warm Karganian time, without a decline of population size around the Last Glacial Maximum (21kya). The BSPs based on synonymous clock rate indicate that M. zibellina experienced demographic expansions later, approximately 22kya. The A2a clade that colonized Kamchatka ~23–50kya (depending on the mutation rate used) survived the last glaciation there as demonstrated by the BSP analysis. In addition, we have found evidence of positive selection acting at ND4 and cytochrome b genes, thereby suggesting adaptive evolution of the A2a clade in Kamchatka."
}
@article{MUELLER2012217,
title = "Incentive effect on inhibitory control in adolescents with early-life stress: An antisaccade study",
journal = "Child Abuse & Neglect",
volume = "36",
number = "3",
pages = "217 - 225",
year = "2012",
issn = "0145-2134",
doi = "https://doi.org/10.1016/j.chiabu.2011.10.010",
url = "http://www.sciencedirect.com/science/article/pii/S0145213412000312",
author = "Sven C. Mueller and Michael G. Hardin and Katherine Korelitz and Teresa Daniele and Jessica Bemis and Mary Dozier and Elizabeth Peloso and Francoise S. Maheu and Daniel S. Pine and Monique Ernst",
keywords = "Reward, Antisaccade, Cognitive control, Early adversity, Stress",
abstract = "Objective
Early-life stress (ES) such as adoption, change of caregiver, or experience of emotional neglect may influence the way in which affected individuals respond to emotional stimuli of positive or negative valence. These modified responses may stem from a direct alteration of how emotional stimuli are coded, and/or the cognitive function implicated in emotion modulation, such as self-regulation or inhibition. These ES effects have been probed on tasks either targeting reward and inhibitory function. Findings revealed deficits in both reward processing and inhibitory control in ES youths. However, no work has yet examined whether incentives can improve automatic response or inhibitory control in ES youths.
Method
To determine whether incentives would only improve self-regulated voluntary actions or generalize to automated motoric responses, participants were tested on a mixed eye movement task that included reflex-like prosaccades and voluntary controlled antisaccade eye movements. Seventeen adopted children (10 females, mean age 11.3years) with a documented history of neglect and 29 typical healthy youths (16 females, mean age 11.9years) performed the mixed prosaccade/antisaccade task during monetary incentive conditions or during no-incentive conditions.
Results
Across both saccade types, ES adolescents responded more slowly than controls. As expected, control participants committed fewer errors on antisaccades during the monetary incentive condition relative to the no-incentive condition. By contrast, ES youths failed to show this incentive-related improvement on inhibitory control. No significant incentive effects were found with prepotent prosaccades trials in either group. Finally, co-morbid psychopathology did not modulate the findings.
Conclusions
These data suggest that youths with experience of early stress exhibit deficient modulation of inhibitory control by reward processes, in tandem with a reward-independent deficit in preparation for both automatic and controlled responses. These data may be relevant to interventions in ES youths."
}
@article{LO201316,
title = "A Systematic Review of the Mysterious Caterpillar Fungus Ophiocordyceps sinensis in DongChongXiaCao (冬蟲夏草 Dōng Chóng Xià Cǎo) and Related Bioactive Ingredients",
journal = "Journal of Traditional and Complementary Medicine",
volume = "3",
number = "1",
pages = "16 - 32",
year = "2013",
issn = "2225-4110",
doi = "https://doi.org/10.1016/S2225-4110(16)30164-X",
url = "http://www.sciencedirect.com/science/article/pii/S222541101630164X",
author = "Hui-Chen Lo and Chienyan Hsieh and Fang-Yi Lin and Tai-Hao Hsu",
keywords = "Bioactive Ingredients, , DongChongXiaCao, , ",
abstract = "ABSTRACT
The caterpillar fungus Ophiocordyceps sinensis (syn.††The term “Cordyceps sinensis” has been renamed to its synonym “Ophiocordyceps sinensis” by Sung et al. in 2007. In the discussion, “Cordyceps sinensis” is still used to represent “Ophiocordyceps sinensis” out of respect to the original authors of the articles that we cited.Cordyceps sinensis), which was originally used in traditional Tibetan and Chinese medicine, is called either “yartsa gunbu” or “DongChongXiaCao (冬蟲夏草 Dōng Chóng Xià Cǎo)” (“winter worm-summer grass”), respectively. The extremely high price of DongChongXiaCao, approximately USD $20,000 to 40,000 per kg, has led to it being regarded as “soft gold” in China. The multi-fungi hypothesis has been proposed for DongChongXiaCao; however, Hirsutella sinensis is the anamorph of O. sinensis. In Chinese, the meaning of “DongChongXiaCao” is different for O. sinensis, Cordyceps spp.,‡‡Cordyceps spp. indicates any species that belongs to the genus Cordyceps. and Cordyceps spƒƒCordyceps sp. indicates the unidentified species that belong to the genus Cordyceps. Over 30 bioactivities, such as immunomodulatory, antitumor, anti-inflammatory, and antioxidant activities, have been reported for wild DongChongXiaCao and for the mycelia and culture supernatants of O. sinensis. These bioactivities derive from over 20 bioactive ingredients, mainly extracellular polysaccharides, intracellular polysaccharides, cordycepin, adenosine, mannitol, and sterols. Other bioactive components have been found as well, including two peptides (cordymin and myriocin), melanin, lovastatin, γ-aminobutyric acid, and cordysinins. Recently, the bioactivities of O. sinensis were described, and they include antiarteriosclerosis, antidepression, and antiosteoporosis activities, photoprotection, prevention and treatment of bowel injury, promotion of endurance capacity, and learning-memory improvement. H. sinensis has the ability to accelerate leukocyte recovery, stimulate lymphocyte proliferation, antidiabetes, and improve kidney injury. Starting January 1st, 2013, regulation will dictate that one fungus can only have one name, which will end the system of using separate names for anamorphs. The anamorph name “H. sinensis” has changed by the International Code of Nomenclature for algae, fungi, and plants to O. sinensis."
}
@article{VASSILIADIS201724,
title = "Gravitating to rigidity: Patterns of schema evolution – and its absence – in the lives of tables",
journal = "Information Systems",
volume = "63",
pages = "24 - 46",
year = "2017",
issn = "0306-4379",
doi = "https://doi.org/10.1016/j.is.2016.06.010",
url = "http://www.sciencedirect.com/science/article/pii/S030643791630120X",
author = "Panos Vassiliadis and Apostolos V. Zarras and Ioannis Skoulis",
keywords = "Schema evolution, Database evolution, Analysis of evolution history, Patterns in schema evolution, Software rigidity, Software repository mining, Exploratory study, Software maintenance",
abstract = "Like all software maintenance, schema evolution is a process that can severely impact the lifecycle of a data-intensive software projects, as schema updates can drive depending applications crushing or delivering incorrect data to end users. In this paper, we study the schema evolution of eight databases that are part of larger open source projects, publicly available through open source repositories. In particular, the focus of our research was the understanding of which tables evolve and how. We report on our observations and patterns on how evolution related properties, like the possibility of deletion, or the amount of updates that a table undergoes, are related to observable table properties like the number of attributes or the time of birth of a table. A study of the update profile of tables, indicates that they are mostly rigid (without any updates to their schema at all) or quiet (with few updates), especially in databases that are more mature and heavily updated. Deletions are significantly outnumbered by table insertions, leading to schema expansion. Delving deeper, we can highlight four patterns of schema evolution. The Γ pattern indicating that tables with large schemata tend to have long durations and avoid removal, the Comet pattern indicating that the tables with most updates are the ones with medium schema size, the Inverse Γ pattern, indicating that tables with medium or small durations produce amounts of updates lower than expected, and, the Empty Triangle pattern indicating that deletions involve mostly early born, quiet tables with short lives, whereas older tables are unlikely to be removed. Overall, we believe that the observed evidence strongly indicates that databases are rigidity-prone rather than evolution-prone. We call the phenomenon gravitation to rigidity and we attribute it to the implied impact to the surrounding code that a modification to the schema of a database has."
}
@article{THUMS2013156,
title = "Tracking sea turtle hatchlings — A pilot study using acoustic telemetry",
journal = "Journal of Experimental Marine Biology and Ecology",
volume = "440",
pages = "156 - 163",
year = "2013",
issn = "0022-0981",
doi = "https://doi.org/10.1016/j.jembe.2012.12.006",
url = "http://www.sciencedirect.com/science/article/pii/S0022098112004182",
author = "Michele Thums and Scott D. Whiting and Julia W. Reisser and Kellie L. Pendoley and Chari B. Pattiaratchi and Robert G. Harcourt and Clive R. McMahon and Mark G. Meekan",
keywords = "Acoustic telemetry, In-water movement, VR2W positioning system",
abstract = "Understanding the movements of turtle hatchings is essential for improved understanding of dispersal behaviour and ultimately survivorship, life history strategies and population connectivity. Yet investigation of in-water movement has been hampered by the small size of hatchlings relative to the size of available tracking technologies. This has resulted in the use of labour intensive visual tracking methods, or active tracking methods with high transmitter to body weight ratios. These methods are confounded by the presence of the observer, the size of the tag, usual small treatment sample sizes and studies that are constrained to daylight hours when turtles hatch predominantly at night. Passive acoustic monitoring using new miniature tags can overcome these limitations. We tested the effectiveness of active and passive acoustic tracking in monitoring turtle hatchling movement in order to measure the influence of artificial light on newly hatched turtles once they enter the water. A Vemco VR2W Positioning System (VPS) comprising an array of 18 VR2W receivers was deployed in the surf zone to detect signals from acoustic-coded transmitters (1.14±0.06% of body mass) attached to 26 flatback turtle hatchlings released into the array. A total of 1328 detections were recorded for 22 hatchlings with turtles spending a mean of 16.63±5.89min in the array. The test detection range for this technology in the surf-zone was 50–100m and was influenced by wave noise and shallow deployment. Cyclonic conditions hampered the experiment and resulted in an inconclusive test of light effects. Three additional instrumented flatback hatchlings were followed in a small boat using a mobile acoustic receiver and directional hydrophone up to 2km from shore. Passive acoustic monitoring is a viable technology for tracking small marine animals and removes many of the confounding effects of other telemetry methods. It has great potential to examine natural and anthropogenic factors influencing orientation and behaviour during a crucial stage in turtle life history — their initial movement from the beach through predator-rich, near shore waters. While the data obtained by passive acoustic monitoring is limited in its spatio-temporal coverage, being constrained by the size of the array, active acoustic tracking can be applied over larger scales. Such studies will be particularly important for assessing the impacts of anthropogenic pressures that have changed the natural light, noise or wave environments and for providing behavioural data to improve and validate bio-physical models of the migration and dispersal of young turtles."
}
@article{HIGGINS201249,
title = "Beyond contrastive analysis and codeswitching: Student documentary filmmaking as a challenge to linguicism in Hawai‘i",
journal = "Linguistics and Education",
volume = "23",
number = "1",
pages = "49 - 61",
year = "2012",
issn = "0898-5898",
doi = "https://doi.org/10.1016/j.linged.2011.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S0898589811001057",
author = "Christina Higgins and Richard Nettell and Gavin Furukawa and Kent Sakoda",
keywords = "Critical language awareness, Translingual practices, Pidgin, Hawai‘i Creole, Documentary film, Linguicism, Students-as-knowledge-producers, Contrastive analysis",
abstract = "This article discusses a documentary film project33The project was made possible with a grant from the Hawai‘i Council for the Humanities. produced by high school students in Hawai‘i that investigated the value of Pidgin (Hawai‘i Creole) in schools and society, and which ultimately aimed to address the problem of linguicism (Skutnabb-Kangas, 1990). The project was carried out within a critical language awareness framework that treated students as knowledge producers and which provided them with the opportunity to use their own communities and languages as repositories of knowledge and as sites for learning about the relationship between language and society. Through exploring the meanings and values of their language, the students produced a documentary that ended up challenging many of their own assumptions about Pidgin, and which revealed the importance of translingual practices (Pennycook, 2007). This article draws on material from the documentary and interviews with the students to illustrate how the students’ views towards Pidgin changed during the course of the project, with a particular focus on the language's legitimacy. The results suggest that a students-as-knowledge-producers approach may offer more potential to challenge linguicism than many contrastive analysis approaches currently being used. By treating non-mainstream languages as subject matter in their own right, without reference or comparison to the dominant language, we argue that these languages earn more respect and acknowledgment in school settings and beyond."
}
@article{NEWGARDEN201522,
title = "An eco-dialogical study of second language learners' World of Warcraft (WoW) gameplay",
journal = "Language Sciences",
volume = "48",
pages = "22 - 41",
year = "2015",
issn = "0388-0001",
doi = "https://doi.org/10.1016/j.langsci.2014.10.004",
url = "http://www.sciencedirect.com/science/article/pii/S0388000114001302",
author = "Kristi Newgarden and Dongping Zheng and Min Liu",
keywords = "Values realizing, Ecological psychology, Dialogicality, Languaging, Skilled linguistic action, Video games",
abstract = "This exploratory research proceeds from the perspective that language is ecological and dialogical. We examined variables derived from eco-dialogical coding of an episode of World of Warcraft play involving three English learners. According to the Eco-dialogical model (Zheng, 2012), second language (L2) learners need to learn to take skilled linguistic action (Cowley, 2013), a process of realizing the values of physical, sociocultural and dialogical affordances in the environment. We employed Multinomial Logistic Regression to determine which of our variables were predictors for three types of values realizing; namely, wayfinding orienting to sociocultural norms and synergized values realizing of both wayfinding and orientation to sociocultural norms. The model we developed suggested that when communicative projects collectively entailed players’ a) verbalizing with synchronized avatar action, b) attending to game rules and c) coordinating in anticipation of good future prospects, players were more likely to realize both values realizing types synergistically. In other words, players' skilled linguistic action of prospective coordination, combined with multimodal languaging and constrained by WoW game rules, together, were more likely to lead to dual values realizing. This finding suggests that dual values realizing evokes connections between real-time first-order physical movements and multimodal languaging with situation transcending practices (Linell, 2009) which are second-order rules, and other sociocultural and linguistic norms. Coupling this finding with our Eco-dialogical unit of analysis, communicative projects, we suggest that these language learners developed co-agency. We conclude that our model should be tested in future studies that seek to illuminate the contribution of a new Eco-dialogical understanding of L2 learning and the potential for learners to have high quality languaging experiences in multiplayer 3D game environments and other social semiotically rich contexts."
}
@article{SHARIFI20151393,
title = "Engaging children in the development of obesity interventions: Exploring outcomes that matter most among obesity positive outliers",
journal = "Patient Education and Counseling",
volume = "98",
number = "11",
pages = "1393 - 1401",
year = "2015",
issn = "0738-3991",
doi = "https://doi.org/10.1016/j.pec.2015.06.007",
url = "http://www.sciencedirect.com/science/article/pii/S0738399115002803",
author = "Mona Sharifi and Gareth Marshall and Roberta E. Goldman and Courtney Cunningham and Richard Marshall and Elsie M. Taveras",
keywords = "Obesity, Overweight, Positive deviance, Children, Attitude to health, Qualitative",
abstract = "Objective
To explore outcomes and measures of success that matter most to ‘positive outlier’ children who improved their body mass index (BMI) despite living in obesogenic neighborhoods.
Methods
We collected residential address and longitudinal height/weight data from electronic health records of 22,657 children ages 6–12 years in Massachusetts. We defined obesity “hotspots” as zip codes where >15% of children had a BMI ≥95th percentile. Using linear mixed effects models, we generated a BMI z-score slope for each child with a history of obesity. We recruited 10–12 year-olds with negative slopes living in hotspots for focus groups. We analyzed group transcripts and discussed emerging themes in iterative meetings using an immersion/crystallization approach.
Results
We reached thematic saturation after 4 focus groups with 21 children. Children identified bullying and negative peer comparisons related to physical appearance, clothing size, and athletic ability as motivating them to achieve a healthier weight, and they measured success as improvement in these domains. Positive relationships with friends and family facilitated both behavior change initiation and maintenance.
Conclusions
The perspectives of positive outlier children can provide insight into children's motivations leading to successful obesity management.
Practice implications
Child/family engagement should guide the development of patient-centered obesity interventions."
}
@article{LEE201653,
title = "Repeated exposure to neurotoxic levels of chlorpyrifos alters hippocampal expression of neurotrophins and neuropeptides",
journal = "Toxicology",
volume = "340",
pages = "53 - 62",
year = "2016",
issn = "0300-483X",
doi = "https://doi.org/10.1016/j.tox.2016.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S0300483X16300014",
author = "Young S. Lee and John A. Lewis and Danielle L. Ippolito and Naissan Hussainzada and Pamela J. Lein and David A. Jackson and Jonathan D. Stallings",
keywords = "Chlorpyrifos, Cholinesterase inhibition, Gene expression, Microarray, miRNA, Organophosphorus pesticide",
abstract = "Chlorpyrifos (CPF), an organophosphorus pesticide (OP), is one of the most widely used pesticides in the world. Subchronic exposures to CPF that do not cause cholinergic crisis are associated with problems in cognitive function (i.e., learning and memory deficits), but the biological mechanism(s) underlying this association remain speculative. To identify potential mechanisms of subchronic CPF neurotoxicity, adult male Long Evans (LE) rats were administered CPF at 3 or 10mg/kg/d (s.c.) for 21 days. We quantified mRNA and non-coding RNA (ncRNA) expression profiles by RNA-seq, microarray analysis and small ncRNA sequencing technology in the CA1 region of the hippocampus. Hippocampal slice immunohistochemistry was used to determine CPF-induced changes in protein expression and localization patterns. Neither dose of CPF caused overt clinical signs of cholinergic toxicity, although after 21 days of exposure, cholinesterase activity was decreased to 58% or 13% of control levels in the hippocampus of rats in the 3 or 10mg/kg/d groups, respectively. Differential gene expression in the CA1 region of the hippocampus was observed only in the 10mg/kg/d dose group relative to controls. Of the 1382 differentially expressed genes identified by RNA-seq and microarray analysis, 67 were common to both approaches. Differential expression of six of these genes (Bdnf, Cort, Crhbp, Nptx2, Npy and Pnoc) was verified in an independent CPF exposure study; immunohistochemistry demonstrated that CRHBP and NPY were elevated in the CA1 region of the hippocampus at 10mg/kg/d CPF. Gene ontology enrichment analysis suggested association of these genes with receptor-mediated cell survival signaling pathways. miR132/212 was also elevated in the CA1 hippocampal region, which may play a role in the disruption of neurotrophin-mediated cognitive processes after CPF administration. These findings identify potential mediators of CPF-induced neurobehavioral deficits following subchronic exposure to CPF at a level that inhibits hippocampal cholinesterase to less than 20% of control. An equally significant finding is that subchronic exposure to CPF at a level that produces more moderate inhibition of hippocampal cholinesterase (approximately 50% of control) does not produce a discernable change in gene expression."
}
@article{WITRY2013654,
title = "A qualitative investigation of protégé expectations and proposition of an evaluation model for formal mentoring in pharmacy education",
journal = "Research in Social and Administrative Pharmacy",
volume = "9",
number = "6",
pages = "654 - 665",
year = "2013",
issn = "1551-7411",
doi = "https://doi.org/10.1016/j.sapharm.2012.08.003",
url = "http://www.sciencedirect.com/science/article/pii/S1551741112001404",
author = "Matthew J. Witry and Brandon J. Patterson and Bernard A. Sorofman",
keywords = "Mentor, Model, Professional development",
abstract = "Background
Student pharmacist mentoring programs have gained attention from colleges of pharmacy as a way to enhance the student experience. However, no evaluative models have been proposed or theoretical explanations described for use in improving formal mentoring programs in pharmacy or for guiding the construction of a literature base.
Objectives
The objectives of this study were to investigate student expectations and preferences for formal mentoring programs and propose a model for evaluating formal mentoring programs in pharmacy education.
Methods
Five, 60-minute focus groups were conducted in September 2009. Participants were PharmD candidates in their first 3 years of professional education. Discussion was facilitated using a question guide. Following transcription, an initial iteration of the model was used to code the data. A consensus-forming process was used to derive themes and identify representative quotes. Elaboration and specification of the final proposed model is presented.
Results
In all, 28 students participated. Emergent constructs were identified from the data. Structures or inputs of the formal mentoring program included mentor and protégé characteristics and program structure. Mentoring processes included mentor functions, mentoring activities, and relationship development. Outcomes included both proximal outcomes in the form of mentor and protégé change, program satisfaction, and organizational learning; and distal outcomes comprised mentor, protégé, and organizational outcomes.
Conclusions
This formal mentoring evaluation model was useful in guiding analysis of protégé experiences and preferences for a college-sponsored program. The model can be used to guide college administrators and researchers on future theory-based inquiry into protégé; mentor; and organizational structures, processes, and outcomes for formal mentoring programs."
}
@article{KOLOKOLOVA20122567,
title = "Polarization of light scattered by large aggregates",
journal = "Journal of Quantitative Spectroscopy and Radiative Transfer",
volume = "113",
number = "18",
pages = "2567 - 2572",
year = "2012",
note = "Electromagnetic and Light Scattering by non-spherical particles XIII",
issn = "0022-4073",
doi = "https://doi.org/10.1016/j.jqsrt.2012.02.002",
url = "http://www.sciencedirect.com/science/article/pii/S0022407312000465",
author = "Ludmilla Kolokolova and Daniel Mackowski",
keywords = "Aggregates, Polarization, Comet dust, Porosity, Modeling, T-matrix, Parallel computing",
abstract = "Study of cosmic dust and planetary aerosols indicate that some of them contain a large number of aggregates of the size that significantly exceeds the wavelengths of the visible light. In some cases such large aggregates may dominate in formation of the light scattering characteristics of the dust. In this paper we present the results of computer modeling of light scattering by aggregates that contain more than 1000 monomers of submicron size and study how their light scattering characteristics, specifically polarization, change with phase angle and wavelength. Such a modeling became possible due to development of a new version of Multi Sphere T-Matrix (MSTM) code for parallel computing. The results of the modeling are applied to the results of comet polarimetric observations to check if large aggregates dominate in formation of light scattering by comet dust. We compare aggregates of different structure and porosity. We show that large aggregates of more than 98% porosity (e.g. ballistic cluster–cluster aggregates) have angular dependence of polarization almost identical to the Rayleigh one. Large compact aggregates (less than 80% porosity) demonstrate the curves typical for solid particles. This rules out too porous and too compact aggregates as typical comet dust particles. We show that large aggregates not only can explain phase angle dependence of comet polarization in the near infrared but also may be responsible for the wavelength dependence of polarization, which can be related to their porosity."
}
@article{GONCALVES2014138,
title = "KDM5C mutational screening among males with intellectual disability suggestive of X-Linked inheritance and review of the literature",
journal = "European Journal of Medical Genetics",
volume = "57",
number = "4",
pages = "138 - 144",
year = "2014",
issn = "1769-7212",
doi = "https://doi.org/10.1016/j.ejmg.2014.02.011",
url = "http://www.sciencedirect.com/science/article/pii/S1769721214000366",
author = "Thainá Fernandez Gonçalves and Andressa Pereira Gonçalves and Natalia Fintelman Rodrigues and Jussara Mendonça dos Santos and Márcia Mattos Gonçalves Pimentel and Cíntia Barros Santos-Rebouças",
keywords = "X-linked intellectual disability, , , Histone demethylase, Chromatin remodeling, Epigenetics",
abstract = "An increasing number of neurodevelopmental diseases have been associated with disruption of chromatin remodeling in eukaryotes. Lysine(K)-specific demethylase 5C (KDM5C) is a versatile epigenetic regulator that removes di- and tri-methyl groups of lysine 4 on histone H3 (H3K4) from transcriptional targets and is essential for neuronal survival and dendritic growth. Mutations in KDM5C gene, located at Xp11.22, have been reported as an important cause of both syndromic and non-syndromic X-linked intellectual disability (XLID) in males. The aim of this study was to evaluate the prevalence and spectrum of KDM5C mutations among Brazilian patients with XLID. To access the impact of KDM5C variants on XLID, a cohort of 143 males with a family history of intellectual disability (ID) suggestive of X-linked inheritance were enrolled. Common genetic causes of XLID were previously excluded and the entire coding and flanking intronic sequences of KDM5C gene were screened by direct Sanger sequencing. Seven nucleotide changes were observed: one pathogenic mutation (c.2172C>A, p.Cys724*), one novel variant with unknown value (c.633G>C, p.Arg211Arg) and five apparently benign sequence changes. In silico analysis of the variants revealed a putative creation of an Exonic Splicing Enhancer sequence by the silent c.633G>C mutation, which co-segregates with the ID phenotype. Our results point out to a KDM5C pathogenic mutational frequency of 0.7% among males with probable XLID. This is the first KDM5C screening among ID males from a country in Latin America and provides new clues about the significance of KDM5C mutations for genetic counseling."
}
@article{LIU201644,
title = "A mesoscopic reaction rate model for shock initiation of multi-component PBX explosives",
journal = "Journal of Hazardous Materials",
volume = "317",
pages = "44 - 51",
year = "2016",
issn = "0304-3894",
doi = "https://doi.org/10.1016/j.jhazmat.2016.05.052",
url = "http://www.sciencedirect.com/science/article/pii/S0304389416304903",
author = "Y.R. Liu and Z.P. Duan and Z.Y. Zhang and Z.C. Ou and F.L. Huang",
keywords = "Mesoscopic reaction rate model, Shock initiation, Multi-component PBX, Hot-spot ignition, Explosive mixture ratios",
abstract = "The primary goal of this research is to develop a three-term mesoscopic reaction rate model that consists of a hot-spot ignition, a low-pressure slow burning and a high-pressure fast reaction terms for shock initiation of multi-component Plastic Bonded Explosives (PBX). Thereinto, based on the DZK hot-spot model for a single-component PBX explosive, the hot-spot ignition term as well as its reaction rate is obtained through a “mixing rule” of the explosive components; new expressions for both the low-pressure slow burning term and the high-pressure fast reaction term are also obtained by establishing the relationships between the reaction rate of the multi-component PBX explosive and that of its explosive components, based on the low-pressure slow burning term and the high-pressure fast reaction term of a mesoscopic reaction rate model. Furthermore, for verification, the new reaction rate model is incorporated into the DYNA2D code to simulate numerically the shock initiation process of the PBXC03 and the PBXC10 multi-component PBX explosives, and the numerical results of the pressure histories at different Lagrange locations in explosive are found to be in good agreements with previous experimental data."
}
@article{BAZIN2013280,
title = "Mapping of quick clay by electrical resistivity tomography under structural constraint",
journal = "Journal of Applied Geophysics",
volume = "98",
pages = "280 - 287",
year = "2013",
issn = "0926-9851",
doi = "https://doi.org/10.1016/j.jappgeo.2013.09.002",
url = "http://www.sciencedirect.com/science/article/pii/S0926985113001948",
author = "S. Bazin and A.A. Pfaffhuber",
keywords = "ERT, RCPT, Constrained inversion, Quick clay, Case histories",
abstract = "Geotechnical projects usually rely on traditional sounding and drilling investigations. Drilling only provides point information and the geology needs to be interpolated between these points. Near surface geophysical methods can provide information to fill those gaps. Norwegian case studies are presented to illustrate how two-dimensional electrical resistivity tomography (ERT) can be used to accurately map the extent of quick clay deposits. Quick clay may be described as highly sensitive marine clay that changes from a relatively stiff condition to a liquid mass when disturbed. Quick clay slides present a geo-hazard and therefore layers of sensitive clay need to be mapped in detail. They are usually characterized by higher resistivity than non-sensitive clay and ERT is therefore a suitable approach to identify their occurrence. However, our experience shows that ERT cannot resolve this small resistivity contrast near large anomalies such as a bedrock interface. For this reason, a constrained inversion of ERT data was applied to delineate quick clay extent both vertically and laterally. As compared to the conventional unconstrained inversions, the constrained inversion models exhibit sharper resistivity contrasts and their resistivity values agree better with in situ measurements."
}
@article{AVELLAR2015221,
title = "Improving a family of Darboux methods for rational second order ordinary differential equations",
journal = "Computer Physics Communications",
volume = "195",
pages = "221 - 223",
year = "2015",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2015.04.028",
url = "http://www.sciencedirect.com/science/article/pii/S0010465515001927",
author = "J. Avellar and L.G.S. Duarte and L.A.C.P. da Mota",
keywords = "Second order ordinary differential equations, Darboux polynomials, Computer algebra",
abstract = "We have been working in many aspects of the problem of analyzing, understanding and solving ordinary differential equations (first and second order). As we have extensively mentioned, while working in the Darboux type methods, the most costly step of our methods and algorithms of solution is the determination of Darboux polynomials for the associated differential operators. Here, we are going to apply a procedure to greatly reduce the time expenditure in determining these needed Darboux polynomials for a class of second order differential equations.
New version program summary
Program Title: FiOrDi Catalogue identifier: AEQL_v2_0 Program Summary URL:http://cpc.cs.qub.ac.uk/summaries/AEQL_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 1784 No. of bytes in distributed program, including test data, etc.: 35174 Distribution format: tar.gz Programming language: Maple (release 17). Computer: PC. Operating system: Windows 7. Windows Vista RAM: 128 Mb Keywords: First integrals, Second order ordinary differential equations, Darboux type approach, Computer algebra, Darboux polynomials. PACS: 02.30.Hq. Classification: 4.3, 5. Catalogue identifier of previous version: AEQL_v1_0 Journal reference of previous version: Comput. Phys. Commun. 185(2014)307 Does the new version supersede the previous version?: Yes Nature of problem: Determination of first order differential invariants for rational second order ordinary differential equations. Solution method: The method of solution is based on a Darboux type approach. Reasons for the new version: We have been working on analyzing and solving systems of first and second order differential equations (1ODEs and 2ODEs, respectively) from a numerical point of view, using Lie methods and Darboux type approaches. For this latter class of methods, we have been developing (semi) algorithms to deal with classes of ODEs. In these algorithms, one fact has been always present: the most (computationally) costly step is the determination of the associated Darboux polynomials. Based on this realization, here we will be focused on speeding the process of finding Darboux polynomials for a class of ODEs of our interest. In particular, in this paper, we will talk about a class of rational 2ODEs. Summary of revisions: We have realized that one can extract information regarding the Darboux polynomials, correspondent to the D-operator related to the 2ODE (please see [1]) being studied in a very straightforward way. This, although a simple procedure, will prove essential to solve (or at least reduce) some 2ODEs. As mentioned in [1], the fact that our method uses the differential operator defines as in Eq. (1) is very advantageous. So, let us first define the D-operator we have been using: (1)D≡N∂x+zN∂y+M∂z, where z=dy(x)dx, y=y(x), and M and N are polynomials in (x,y,z). Considering this D-operator, one can see, by inspection on (1), that the cases that will be of interest to us are the ones listed below: 1.Darboux polynomials as factors of the numerator (in M) that are polynomials on (z) only.2.Darboux polynomials as factors of the denominator (in N) that are polynomials without z. So, the main revision we introduce here is to implement routines, in our new Maple code, to look for the needed Darboux polynomials for 2ODEs belonging to the classes mentioned above, just by inspecting the general expression for the 2ODE. With this, we make it available for the researcher, using our package, a more powerful weapon. These implementation is basically done via a modification to the command Invar, now one can use an extra argument MNDarboux in order to make use of the ideas explained above. Apart from that, some bugs were removed. Restrictions: If, for the 2ODE under consideration, the Darboux Polynomials are of high degree (>3) in the dependent and independent variables, the package may spend an impractical amount of time to obtain the solution. That restriction is in part lifted by the modifications hereby introduced. Unusual features: Since our package is based on our theoretical developments [1], it can successfully reduce some rational 2ODEs that were not solved (or reduced) by some of the best-known methods available. This situation is even enhanced via the improvements that we have made here. Let us present an example that shows the power of the change introduced. The command now is able to (automatically) find Darboux polynomials from M (depending on (z)) and N (depending on (x,y)): (2)d2dx2y=−1/7(z7−3)2(−9y8zx7+5y9x6+8y9zx+y10−6x5+z)z6(−x6+y)(xy9−1). Using what we have been learning, we can see that we have Darboux polynomials coming from both M and N. Below we will display them and their corresponding co-factors: v1=z7−3→g1=−7z6(z7−3)(9x7y8z−5x6y9−8xy9z−y10+6x5−z)v2=x6−y→g2=7(xy9−1)(6x5−z)z6(3)v3=xy9−1→g3=7(x6−y)(9xz+y)y8z6. Using the method briefly described above one conclude that, for this ODE, we have the following results for the parameters and functions needed to find the differential invariant for the ODE: P=181(z7−3)2(9x7y8−8xy9−1)Q=181(4)R=1(z7−3)2(x6−y)(xy9−1) and, using the theory described in [1], we finally find the first order invariant given by: (5)181ln(−x6+y)z7−ln(xy9−1)z7−3ln(−x6+y)+3ln(xy9−1)+1z7−3. It worth mention that the presence of the Darboux polynomials of such a high degree (as can be seen above), with terms up to the power of 10, makes the regular process of determining it very “expensive” in time expenditure and memory. After applying the method here presented, which very quickly determined the needed Darboux polynomials, the algorithm we introduced in [1] finds the results (3) and (5). For this particular instance, the in-built Maple (very powerful) dsolve command fails to reduce this ode. Our procedure takes some minutes but reduces it. Running time: This depends strongly on the ODE, but usually under 4 seconds. References: [1]L.G.S. Duarte and L.A.C.P. da Mota, Finding Elementary First Integrals for Rational Second Order Ordinary Differential Equations, Journal of Mathematical Physics, Volume 50, Issue 1, pp. 013514-013514-17 (2009)."
}
@article{CORPINO201613,
title = "E-st@r-I experience: Valuable knowledge for improving the e-st@r-II design",
journal = "Acta Astronautica",
volume = "121",
pages = "13 - 22",
year = "2016",
issn = "0094-5765",
doi = "https://doi.org/10.1016/j.actaastro.2015.12.027",
url = "http://www.sciencedirect.com/science/article/pii/S0094576515004695",
author = "S. Corpino and G. Obiols-Rabasa and R. Mozzillo and F. Nichele",
keywords = "CubeSat, CubeSat verification, Spacecraft design, Spacecraft lifecycle, Assembly Integration and Verification, Space engineering education",
abstract = "Many universities all over the world have now established hands-on education programs based on CubeSats. These small and cheap platforms are becoming more and more attractive also for other-than-educational missions, such as technology demonstration, science applications, and Earth observation. This new paradigm requires the development of adequate technology to increase CubeSat performance and mission reliability, because educationally-driven missions have often failed. In 2013 the ESA Education Office launched the Fly Your Satellite! Programme which aims at increasing CubeSat mission reliability through several actions: to improve design implementation, to define best practices for conducting the verification process, and to make the CubeSat community aware of the importance of verification. Within this framework, the CubeSat team at Politecnico di Torino developed the e-st@r-II CubeSat as follow-on of the e-st@r-I satellite, launched in 2012 on the VEGA Maiden Flight. E-st@r-I and e-st@r-II are both 1U satellites with educational and technology demonstration objectives: to give hands-on experience to university students and to test an active attitude determination and control system based on inertial and magnetic measurements with magnetic actuation. This paper describes the know-how gained thanks to the e-st@r-I mission, and how this heritage has been translated into the improvement of the new CubeSat in several areas and lifecycle phases. The CubeSat design has been reviewed to reduce the complexity of the assembly procedure and to deal with possible failures of the on-board computer, for example re-coding the software in the communications subsystem. New procedures have been designed and assessed for the verification campaign accordingly to ECSS rules and with the support of ESA specialists. Different operative modes have been implemented to handle some anomalies observed during the operations of the first satellite. A new version of the on-board software is one of the main modifications. In particular, the activation sequence of the satellite has been modified to have a stepwise switch-on of the satellite. In conclusion, the e-st@r-I experience has provided valuable lessons during its development, verification and on-orbit operations. This know-how has become crucial for the development of the e-st@r-II CubeSat as illustrated in this article."
}
@article{HARIHARAN201839,
title = "Improved binary dragonfly optimization algorithm and wavelet packet based non-linear features for infant cry classification",
journal = "Computer Methods and Programs in Biomedicine",
volume = "155",
pages = "39 - 51",
year = "2018",
issn = "0169-2607",
doi = "https://doi.org/10.1016/j.cmpb.2017.11.021",
url = "http://www.sciencedirect.com/science/article/pii/S0169260717307666",
author = "M. Hariharan and R. Sindhu and Vikneswaran Vijean and Haniza Yazid and Thiyagar Nadarajaw and Sazali Yaacob and Kemal Polat",
keywords = "Infant cry signal, Feature extraction, Feature selection, Optimization and classification",
abstract = "Background and objective
Infant cry signal carries several levels of information about the reason for crying (hunger, pain, sleepiness and discomfort) or the pathological status (asphyxia, deaf, jaundice, premature condition and autism, etc.) of an infant and therefore suited for early diagnosis. In this work, combination of wavelet packet based features and Improved Binary Dragonfly Optimization based feature selection method was proposed to classify the different types of infant cry signals.
Methods
Cry signals from 2 different databases were utilized. First database contains 507 cry samples of normal (N), 340 cry samples of asphyxia (A), 879 cry samples of deaf (D), 350 cry samples of hungry (H) and 192 cry samples of pain (P). Second database contains 513 cry samples of jaundice (J), 531 samples of premature (Prem) and 45 samples of normal (N). Wavelet packet transform based energy and non-linear entropies (496 features), Linear Predictive Coding (LPC) based cepstral features (56 features), Mel-frequency Cepstral Coefficients (MFCCs) were extracted (16 features). The combined feature set consists of 568 features. To overcome the curse of dimensionality issue, improved binary dragonfly optimization algorithm (IBDFO) was proposed to select the most salient attributes or features. Finally, Extreme Learning Machine (ELM) kernel classifier was used to classify the different types of infant cry signals using all the features and highly informative features as well.
Results
Several experiments of two-class and multi-class classification of cry signals were conducted. In binary or two-class experiments, maximum accuracy of 90.18% for H Vs P, 100% for A Vs N, 100% for D Vs N and 97.61% J Vs Prem was achieved using the features selected (only 204 features out of 568) by IBDFO. For the classification of multiple cry signals (multi-class problem), the selected features could differentiate between three classes (N, A & D) with the accuracy of 100% and seven classes with the accuracy of 97.62%.
Conclusion
The experimental results indicated that the proposed combination of feature extraction and selection method offers suitable classification accuracy and may be employed to detect the subtle changes in the cry signals."
}
@article{HOPPENREIJS2013106,
title = "Evaluation of Condylar Resorption Before and After Orthognathic Surgery",
journal = "Seminars in Orthodontics",
volume = "19",
number = "2",
pages = "106 - 115",
year = "2013",
note = "Progressive Condylar Resorption and Dentofacial Deformities",
issn = "1073-8746",
doi = "https://doi.org/10.1053/j.sodo.2012.11.006",
url = "http://www.sciencedirect.com/science/article/pii/S1073874612001223",
author = "Theo J.M. Hoppenreijs and Thomas Maal and Tong Xi",
abstract = "Malpositioned condyles during osteotomy can cause remodeling of the condyles, but can also initiate condylar resorption (CR). The radiological signs of CR are similar to juvenile osteoarthritis and osteoarthrosis. In the 1980s, conventional transcranial and infracranial radiographs were used to evaluate the position of the condyle in the fossa. An orthopantomogram can be used to describe the contour or morphology of the condyles, but it is not applicable for measurements. Magnetic resonance imaging is useful in evaluation of the disks, condyles, and synovia. Both conventional multislice computed tomography and cone-beam computed tomography (CBCT) can provide an excellent visualization of the condyles in 3 planes. With CBCT, condylar position and condylar changes can be assessed as a color-coded map, or as mesh transparencies, which provide higher accuracy. The pretreatment assessment of past or potential temporomandibular joint (TMJ) issues consists of a detailed history of previous TMJ symptoms, as well as a clinical and radiological examination. An orthopantomogram is helpful to make a risk profile based on the contour of a condyle and the stage of osteoarthritic degeneration. After orthognathic surgery, the surgeon must be aware of TMJ dysfunction symptoms, occlusal relapse, reduction of form and volume of the condyle, and loss of mandibular ramus height. In patients with a high risk for CR or when a suspicion of CR occurs, a CBCT is indicated. The incorporation of an automated postscan image enhancement protocol and subsequent 3-dimensional rendering of condyles into the 3-dimensional virtual head model of patients will provide a powerful tool for analysis of CR."
}
@article{SCHLOTZ2012449,
title = "The potential of dietary polyunsaturated fatty acids to modulate eicosanoid synthesis and reproduction in Daphnia magna: A gene expression approach",
journal = "Comparative Biochemistry and Physiology Part A: Molecular & Integrative Physiology",
volume = "162",
number = "4",
pages = "449 - 454",
year = "2012",
issn = "1095-6433",
doi = "https://doi.org/10.1016/j.cbpa.2012.05.004",
url = "http://www.sciencedirect.com/science/article/pii/S1095643312001286",
author = "Nina Schlotz and Jesper Givskov Sørensen and Dominik Martin-Creuzburg",
keywords = "Arachidonic acid, , Eicosanoids, Eicosapentaenoic acid, Food quality, Gene expression, Nutrition, Vitellogenin",
abstract = "Nutritional ecology of the aquatic model genus Daphnia has received much attention in past years in particular with regard to dietary polyunsaturated fatty acids (PUFAs) which are crucial for growth and reproduction. Besides their significant role as membrane components, C20 PUFAs serve as precursors for eicosanoids, hormone-like mediators of reproduction, immunity and ion transport physiology. In the present study we investigate transcriptomic changes in Daphnia magna in response to different algal food organisms substantially differing in their PUFA composition using quantitative real-time PCR and relate them to concomitantly documented life history data. The selection of target genes includes representatives that have previously been shown to be responsive to the eicosanoid biosynthesis inhibitor ibuprofen. The beneficial effect of C20 PUFA-rich food on reproduction and population growth rates was accompanied by an increased vitellogenin (DmagVtg1) gene expression in D. magna. Additionally, genes involved in eicosanoid signaling were particularly influenced by dietary C20 PUFA availability. For example, the cyclooxygenase gene (Cox), coding for a central enzyme in the eicosanoid pathway, was highly responsive to the food treatments. Our results suggest that dietary PUFAs are fundamental in D. magna physiology as substrate for eicosanoid synthesis and that these eicosanoids are important for D. magna reproduction."
}
@article{HARVEY2012108,
title = "Comparison of the relative efficiencies of stereo-BRUVs and traps for sampling tropical continental shelf demersal fishes",
journal = "Fisheries Research",
volume = "125-126",
pages = "108 - 120",
year = "2012",
issn = "0165-7836",
doi = "https://doi.org/10.1016/j.fishres.2012.01.026",
url = "http://www.sciencedirect.com/science/article/pii/S0165783612000859",
author = "Euan S. Harvey and Stephen J. Newman and Dianne L. McLean and Mike Cappo and Jessica J. Meeuwig and Craig L. Skepper",
keywords = "Demersal fish assemblages, Stereo-BRUVs, Diurnal, Nocturnal, Statistical power, Sampling efficiency",
abstract = "The sampling efficiencies of commercial standard fish traps and baited remote underwater stereo-video systems (stereo-BRUVs) were compared by examining the diversity and relative abundance of tropical demersal fish that each method sampled on the north-western shelf (40–60m) of Western Australia. Stereo-BRUVs recorded many more species (91 species from 32 families) than commercial fish traps (30 species and 15 families). Stereo-BRUVs also sampled many more individuals (mean 36.55±5.91 SE) than fish traps (mean 12.30±1.40 SE). This suggests stereo-BRUVs would be more capable of detecting changes in the relative abundance of species over time. Data from four commercially important species (Epinephelus bilobatus, Epinephelus multinotatus, Lethrinus punctulatus and Lutjanus russelli) revealed that stereo-BRUVs had much greater statistical power to detect change than an equivalent number of samples from fish traps. In contrast, fish traps had a greater statistical power to detect change for a fifth target species, Lutjanus sebae. For two commonly sampled species, Abalistes stellatus11Usage follows CAABcodes (Rees, A.J.J., Yearsley, G.K., Gowlett-Holmes, K. and Pogonoski, J. Codes for Australian Aquatic Biota (on-line version). CSIRO Marine and Atmospheric Research, World Wide Web electronic publication, 1999 onwards. Available at: http://www.cmar.csiro.au/caab/.). and Lethrinus punctulatus, stereo-BRUVs sampled a smaller mean length than fish traps while for a third species, Lutjanus sebae, stereo-BRUVs recorded a larger mean length. The length frequencies for these species were not significantly different between methods, although stereo-BRUVs sampled a much larger range of lengths than was captured in traps. This study demonstrates that stereo-BRUVs are potentially a much more powerful technique than fish traps for assessing species richness, relative abundance and size structure in multi-species fisheries in north-western Australia."
}
@article{KAR201258,
title = "Craziness based Particle Swarm Optimization algorithm for FIR band stop filter design",
journal = "Swarm and Evolutionary Computation",
volume = "7",
pages = "58 - 64",
year = "2012",
issn = "2210-6502",
doi = "https://doi.org/10.1016/j.swevo.2012.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S2210650212000375",
author = "Rajib Kar and Durbadal Mandal and Sangeeta Mondal and Sakti Prasad Ghoshal",
keywords = "FIR band stop filter, RGA, PSO, CLPSO, CRPSO, Parks and McClellan (PM) Algorithm",
abstract = "In this paper, an improved particle swarm optimization technique called Craziness based Particle Swarm Optimization (CRPSO) is proposed and employed for digital finite impulse response (FIR) band stop filter design. The design of FIR filter is generally nonlinear and multimodal. Hence gradient based classical optimization methods are not suitable for digital filter design due to sub-optimality problem. So, global optimization techniques are required to avoid local minima problem. Several heuristic approaches are available in the literatures. The Particle Swarm Optimization (PSO) algorithm is a heuristic approach with two main advantages: it has fast convergence, and it uses only a few control parameters. But the performance of PSO depends on its parameters and may be influenced by premature convergence and stagnation problem. To overcome these problems the PSO algorithm has been modified in this paper and is used for FIR filter design. In birds' flocking or fish schooling, a bird or a fish often changes directions suddenly. This is described by using a “craziness” factor and is modelled in the technique by using a craziness variable. A craziness operator is introduced in the proposed technique to ensure that the particle would have a predefined craziness probability to maintain the diversity of the particles. The algorithm's performance is studied with the comparison of real coded genetic algorithm (RGA), conventional PSO, comprehensive learning particle swarm optimization (CLPSO) and Parks and McClellan (PM) Algorithm. The simulation results show that the CRPSO is superior or comparable to the other algorithms for the employed examples and can be efficiently used for FIR filter design."
}
@article{IRVINE2015728,
title = "Experimental evaluation of the applicability of phase, amplitude, and combined methods to determine water flux and thermal diffusivity from temperature time series using VFLUX 2",
journal = "Journal of Hydrology",
volume = "531",
pages = "728 - 737",
year = "2015",
issn = "0022-1694",
doi = "https://doi.org/10.1016/j.jhydrol.2015.10.054",
url = "http://www.sciencedirect.com/science/article/pii/S0022169415008252",
author = "Dylan J. Irvine and Laura K. Lautz and Martin A. Briggs and Ryan P. Gordon and Jeffrey M. McKenzie",
keywords = "Heat tracing, Groundwater–surface water interaction, Thermal diffusivity, Hyporheic, Dynamic harmonic regression",
abstract = "Summary
Vertical fluid exchange between surface water and groundwater can be estimated using diurnal signals from temperature time series methods based on amplitude ratios (Ar), phase shifts (Δϕ), or combined use of both (ArΔϕ). The Ar, Δϕ, and ArΔϕ methods are typically applied in conditions where one or more of their underlying assumptions are violated, and the reliability of the various methods in response to non-ideal conditions is unclear. Additionally, ArΔϕ methods offer the ability to estimate thermal diffusivity (κe) without assuming any thermal parameters, although the value of such output has not been broadly tested. The Ar, Δϕ, and ArΔϕ methods are tested under non-steady, 1D flows in sand column experiments, and multi-dimensional flows in heterogeneous media in numerical modeling experiments. Results show that, in non-steady flow conditions, estimated κe values outside of a plausible range for streambed materials (0.028–0.180m2d−1) coincide with time periods with erroneous flux estimates. In heterogeneous media, sudden changes of κe with depth also coincide with erroneous flux estimates. When (known) fluxes are variable in time, poor identification of Δϕ leads to poor flux estimates from Δϕ and ArΔϕ methods. However, when fluxes are steady, or near zero, ArΔϕ methods provide the most accurate flux estimates. This comparison of Ar, Δϕ and ArΔϕ methods under non-ideal conditions provides guidance on their use. In this study, ArΔϕ methods have been coded into a new version of VFLUX, allowing users easy access to recent advances in heat tracing."
}
@article{POTLURI2014760,
title = "The role of angioplasty in patients with acute coronary syndrome and previous coronary artery bypass grafting",
journal = "International Journal of Cardiology",
volume = "176",
number = "3",
pages = "760 - 763",
year = "2014",
issn = "0167-5273",
doi = "https://doi.org/10.1016/j.ijcard.2014.07.097",
url = "http://www.sciencedirect.com/science/article/pii/S0167527314013072",
author = "Rahul Potluri and Mudassar Baig and Jaskaran Singh Mavi and Noman Ali and Amir Aziz and Hardeep Uppal and Suresh Chandran",
keywords = "Angioplasty, Acute coronary syndrome, Mortality, CABG",
abstract = "Introduction
Angioplasty has changed the management of acute coronary syndrome (ACS). However, in patients with previous coronary artery bypass grafting (CABG), the role of angioplasty in the management of ACS is widely debated. Lack of clear guidelines leads to subjective and often stereotypical assessments based on clinician preferences. We sought to investigate if angioplasty affected all cause mortality in ACS patients with previous CABG.
Methods
Completely anonymous information on patients with ACS with a background of previous CABG, co-morbidities and procedures attending three multi-ethnic general hospitals in the North West of England, United Kingdom in the period 2000–2012 was traced using the ACALM (Algorithm for Comorbidities, Associations, Length of stay and Mortality) study protocol using ICD-10 and OPCS-4 coding systems. Predictors of mortality and survival analyses were performed using SPSS version 20.0.
Results
Out of 12,227 patients with ACS, there were 1172 (19.0%) cases of ACS in patients with previous coronary artery bypass grafting. Of these 83 (7.1%) patients underwent angioplasty. Multi-nominal logistic regression, accounting for differences in age and co-morbidities, revealed that having angioplasty conferred a 7.96 times improvement in mortality (2.36–26.83 95% CI) compared to not having angioplasty in this patient group.
Conclusions
We have shown that angioplasty confers significantly improved all cause mortality in the management of ACS in patients with previous CABG. The findings of this study highlight the need for clinicians to conscientiously think about the individual benefits and risks of angioplasty for every patient rather than confining to age related stereotypes."
}
@article{MORTON201637,
title = "Developmental study of treatment fidelity, safety and acceptability of a Symptoms Clinic intervention delivered by General Practitioners to patients with multiple medically unexplained symptoms",
journal = "Journal of Psychosomatic Research",
volume = "84",
pages = "37 - 43",
year = "2016",
issn = "0022-3999",
doi = "https://doi.org/10.1016/j.jpsychores.2016.03.008",
url = "http://www.sciencedirect.com/science/article/pii/S0022399916300447",
author = "LaKrista Morton and Alison Elliott and Ruth Thomas and Jennifer Cleland and Vincent Deary and Christopher Burton",
keywords = "Medically unexplained symptoms, Observational study, Intervention, Primary care",
abstract = "Background
There is a need for primary care interventions for patients with multiple medically unexplained symptoms (MUS). We examined whether GPs could be taught to deliver one such intervention, the Symptoms Clinic Intervention (SCI), to patients. The intervention includes recognition and validation of patients' symptoms, explanation of symptoms and actions to manage symptoms.
Methods
We conducted an uncontrolled observational study in Northeast Scotland. GPs were recruited and received two days of structured training. Patients were identified via a two stage process (database searching followed by postal questionnaire) and received the SCI intervention from a GP in their practice. Treatment fidelity was assessed by applying a coding framework to consultation transcripts. Safety was assessed by examining changes in patient symptoms (PHQ-15) and checking for unexpected events. Acceptability was primarily assessed by patient interview.
Results
Four GPs delivered the SCI to 23 patients. GPs delivered all core components of the SCI, and used the components flexibly across the consultations and between patients. They spent more time on recognition than either explanation or actions components. 10 out of 17 patients interviewed described feeling validated, receiving useful explanation and learning actions. 9 out of 20 patients (45%) reported an improvement in PHQ-15 of between 3 and 8 points. Patients who reported the most improvement also described receiving all three components of the intervention.
Conclusions
GPs can be taught to deliver the SCI with reasonable fidelity, safety and acceptability, although some items were inconsistently delivered: further training would be needed before use."
}
@article{PADROSA201513,
title = "La transformación del modelo asistencial en Cataluña para mejorar la calidad de la atención",
journal = "Medicina Clínica",
volume = "145",
pages = "13 - 19",
year = "2015",
note = "Salud en todas las políticas",
issn = "0025-7753",
doi = "https://doi.org/10.1016/S0025-7753(15)30032-4",
url = "http://www.sciencedirect.com/science/article/pii/S0025775315300324",
author = "Josep Maria Padrosa and Àlex Guarga and Francesc Brosa and Josep Jiménez and Roger Robert",
keywords = "Sistema sanitario catalán, Plan de Salud de Cataluña, Modelo asistencial integral, Reorganización de processos, Reordenación de servicios, Catalan health system, Health plan for Catalonia, Integrated healthcare model, Process reorganization, Services reordering",
abstract = "Resumen
Los cambios que se están produciendo en los países occidentales obligan a los sistemas sanitarios a adaptarse a las nuevas necesidades y expectativas de la población. En Cataluña se está produciendo una profunda transformación del modelo asistencial, con el fin de poder dar una respuesta adecuada a esta nueva situación y a la vez garantizar la sostenibilidad del sistema en un contexto de crisis económica. Esta transformación se basa en convertir el actual modelo asistencial centrado en la enfermedad y fraccionado por niveles en otro centrado en la persona, integrado y de base territorial, que promueva el trabajo compartido en red de los diferentes profesionales, dispositivos y niveles asistenciales, estableciendo objetivos comunes explicitados en acuerdos y pactos territoriales. Los cambios que ha llevado a cabo el Servei Català de la Salut (CatSalut) pasan principalmente por incrementar la capacidad de resolución de la atención primaria, reducir la variabilidad de la práctica clínica, evolucionar hacia hospitales más quirúrgicos, potenciar las alternativas a la hospitalización convencional, desarrollar modalidades de atención no presencial, concentrar y sectorizar territorialmente la atención de alta complejidad y diseñar códigos sanitarios específicos, como respuesta a situaciones de emergencia. La finalidad de estas actuaciones es mejorar la efectividad, la calidad, la seguridad y la eficiencia del sistema asegurando la equidad de acceso de la población y el equilibrio territorial. Entre los instrumentos que deben facilitar y promover estos cambios cabe destacar la historia clínica compartida, el nuevo modelo de contratación y pago por resultados, los pactos territoriales, las alianzas entre centros, el aprovechamiento de las potencialidades de las tecnologías de la información y la comunicación, y la evaluación de resultados.
The changes taking place in western countries require health systems to adapt to the public's evolving needs and expectations. The healthcare model in Catalonia is undergoing significant transformation in order to provide an adequate response to this new situation while ensuring the system's sustainability in the current climate of economic crisis. This transformation is based on converting the current diseasecentred model which is fragmented into different levels, to a more patient-centred integrated and territorial care model that promotes the use of a shared network of the different specialities, the professionals, resources and levels of care, entering into territorial agreements and pacts which stipulate joint goals or objectives. The changes the Catalan Health Service (CatSalut) has undergone are principally focused on increasing resolution capacity of the primary level of care, eliminating differences in clinical practice, evolving towards more surgery-centred hospitals, promoting alternatives to conventional hospitalization, developing remote care models, concentrating and organizing highly complex care into different sectors at a territorial level and designing specific health codes in response to health emergencies. The purpose of these initiatives is to improve the effectiveness, quality, safety and efficiency of the system, ensuring equal access for the public to these services and ensuring a territorial balance. These changes should be facilitated and promoted using several different approaches, including implementing shared access to clinical history case files, the new model of results-based contracting and payment, territorial agreements, alliances between centres, harnessing the potential of information and communications technology and evaluation of results."
}
@article{KUHNER20122232,
title = "Progress on standardization and automation in software development on W7X",
journal = "Fusion Engineering and Design",
volume = "87",
number = "12",
pages = "2232 - 2237",
year = "2012",
note = "Proceedings of the 8th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research",
issn = "0920-3796",
doi = "https://doi.org/10.1016/j.fusengdes.2012.06.003",
url = "http://www.sciencedirect.com/science/article/pii/S0920379612003067",
author = "Georg Kühner and Torsten Bluhm and Peter Heimann and Christine Hennig and Hugo Kroiss and Jon Krom and Heike Laqua and Marc Lewerentz and Josef Maier and Jörg Schacht and Anett Spring and Andreas Werner and Manfred Zilker",
keywords = "W7-X, Software development, Quality management, Standardization, ISO/IEC 15504",
abstract = "For a complex experiment like W7X being subject to changes all along its projected lifetime the advantages of a formalized software development method have already been stated [1]. Quality standards like ISO/IEC-12207 provide a guideline for structuring of development work and improving process and product quality. A considerable number of tools has emerged supporting and automating parts of development work. On W7X progress has been made during the last years in exploiting the benefit of automation and management during software development:–Continuous build, integration and automated test of software artefacts.∘Syntax checks and code quality metrics.∘Documentation generation.∘Feedback for developers by temporal statistics.–Versioned repository for build products (libraries, executables).–Separate snapshot and release repositories and automatic deployment.–Semi-automatic provisioning of applications.–Feedback from testers and feature requests by ticket system. This toolset is working efficiently and allows the team to concentrate on development. The activity there is presently focused on increasing the quality of the existing software to become a dependable product. Testing of single functions and qualities must be simplified. So a restructuring is underway which relies more on small, individually testable components with standardized interfaces providing the capability to construct arbitrary function aggregates for dedicated tests of quality attributes as availability, reliability, performance. A further activity is on improving the development cycle. The use of release cycles has already provided favourable concentration of work and predictability of delivery times. However, the demand has risen, to react quickly on priority changes from W7X-project management. So a more agile development cycle is being prepared relying on smaller working packages, shorter release cycles and an associated release plan giving the software development responsible the possibility to react on a shorter time scale."
}
@article{VAERET201274,
title = "Holocene dynamics of the salt–fresh groundwater interface under a sand island, Inhaca, Mozambique",
journal = "Quaternary International",
volume = "257",
pages = "74 - 82",
year = "2012",
note = "Palaeogroundwater dynamics and their importance for past human settlements and today's water management",
issn = "1040-6182",
doi = "https://doi.org/10.1016/j.quaint.2011.11.020",
url = "http://www.sciencedirect.com/science/article/pii/S1040618211006604",
author = "Lars Været and Anton Leijnse and Fortunato Cuamba and Sylvi Haldorsen",
abstract = "The configuration of coastal groundwater systems in southeast Africa was strongly controlled by the Holocene sea-level changes, with an Early Holocene transgression ∼15 m (10,000–5000 cal BP), and two assumed high-stand events in the Middle and Late Holocene with levels higher than the present. The fluctuation of the salt–fresh groundwater interface under Inhaca Island in Mozambique during the Holocene has been studied using an adapted version of the numerical code SUTRA (Saturated-Unsaturated Transport). In this study, small-scale variations such as tidal effects have not been considered. A number of transient simulations were run with constant boundary conditions until the steady state condition was reached in order to study the sensitivity of response time, salt–fresh interface position, and thickness of the transition zone to different parameters such as hydraulic conductivity, porosity, recharge, and dispersivity. A 50% increase in horizontal hydraulic conductivity yields a rise in the location of the interface of >15 m, while an increase in recharge from 8% to 20% of mean annual precipitation (MAP) causes a downward shift in the interface position of >40 m. A full transient simulation of the Holocene dynamics of the salt–fresh groundwater interface showed a response time of several hundred years, with a duration sensitive to porosity, hydraulic conductivity and recharge and a position determined by the recharge rate and the hydraulic conductivity. Dispersivity controls the thickness of the transition zone in this non-tidal model. Physical processes, such as changes in recharge and/or the sea level, may cause rapid shifts in the interface position and affect the thickness of the transition zone."
}
@article{CHAPELLON201855,
title = "Le simulateur face à l’expert",
journal = "Annales Médico-psychologiques, revue psychiatrique",
volume = "176",
number = "1",
pages = "55 - 62",
year = "2018",
issn = "0003-4487",
doi = "https://doi.org/10.1016/j.amp.2017.06.001",
url = "http://www.sciencedirect.com/science/article/pii/S0003448717302068",
author = "Sébastien Chapellon and Frédéric Bondil",
keywords = "Discernement, Étude critique, Expertise médico-légale, Historique, Interprétation psychanalytique, Mensonge, Pathologie psychiatrique, Responsabilité pénale, Simulation, Criminal liability, Critical study, Discernment, Forensic expertise, History, Lie, Psychiatric pathology, Psychoanalytic interpretation, Simulation",
abstract = "Résumé
Depuis l’apparition de l’article 64 du Code pénal, le manque de discernement lié à une maladie mentale est devenu une source d’immunité pénale. Les personnes atteintes d’un trouble psychique ou neuropsychique ayant aboli leur discernement n’ont pas à répondre de leur acte devant la loi. Cette immunité pénale a cependant eu un effet pernicieux, puisque certains prévenus simulent la maladie mentale pour se prémunir des peines qu’ils encourent. La simulation fait partie du paysage juridique. Pour distinguer les vrais malades des simulateurs, la justice fait appel au supposé savoir des experts. Afin de fonder sa sentence, le magistrat requiert de ces derniers des informations médicales et psychologiques visant à vérifier la réalité de « l’altération du discernement » de la personne poursuivie. Or, cette collaboration entre les praticiens de la justice et de la santé mentale n’échappe pas à des problèmes techniques et éthiques. Le clinicien intervenant en tant qu’expert court autant le risque d’une instrumentalisation par la justice que celui d’une manipulation par le sujet simulateur. Cette attitude consistant à tirer profit d’un trouble présumé fournit l’occasion de questionner le fonctionnement du cadre de l’expertise médico-légale. En se jouant de ce cadre, à dessein, les sujets mettent indirectement en lumière certaines de ses failles, autrement imperceptibles. Cet article met ainsi en lumière la complexité des enjeux éthiques et techniques liés à la question de la simulation. Les auteurs rappellent notamment les réflexions émises par Freud lorsqu’il fut mandaté en tant qu’expert pour se pencher sur ce thème. Ensuite, ils s’intéressent aux difficultés que le mensonge sur l’état mental du prévenu pose au niveau juridique. Après quoi, une discussion s’ouvre quant à la possibilité que la simulation puisse révéler l’existence d’un trouble chez les sujets concernés. L’intention qui préside à ce comportement peut suggérer qu’il est le fruit d’une conduite totalement raisonnée. Or, ici, des exemples remettent en question cette conception. En effet, bien que le sujet ait conscience de tromper ses interlocuteurs, il peut néanmoins chercher inconsciemment à masquer un trouble dont il ignore lui-même l’existence.
Since 1810 and the enactment of article 64 of the Penal Code, the lack of discernment due to a mental disease has become a cause of penal immunity. People suffering from a mental or a neuropsychic disorder and having abolished their discernment cannot be sentenced as being criminally responsible for the crime they allegedly committed. Nevertheless, this protection status has had a pernicious effect as some accused are trying to exempt themselves from legal trials by dissimulating their mischiefs under a crazy action. Like McMurphy in One Flew Over the Cuckoo's Nest, they are simulating craziness to preserve themselves from the sentences they could receive. Hence, simulating a mental disorder is prevalent in the legal landscape. To make the difference between the real and the pretending sick cases, the magistrate uses the supposed experts’ knowledge. In order to help him building his sentence, they give him medical and psychological information aiming to identify “the diminished mental capacity” of the accused person. Thus, we are observing a close cooperation between the legal and the mental health practitioners. Nevertheless, this collaboration raises technical and ethical problems. The clinician who intervenes as an expert runs the risk to face the orders of Justice, without avoiding the opposite risk of being manipulated by an individual trying to benefit from his alleged craziness. What is more, the expertise scope is radically different from the therapy one where the individual is expected, according to Freud's formula, to work “side by side” with the therapist. For the latter, it is not a question of finding the “truth”, but of listening how the patient's psychic reality is expressing itself through its slips, its blackouts or its modified memories. Yet, in the case of a simulation, the individual changes the reality, deliberately, to abuse the Law in his favor. However, this behavior consisting in simulating craziness provides the opportunity to question the forensic expertise framework functioning. By playing with this framework, these individuals are indirectly highlighting some of its faults, which otherwise go unnoticed. This article describes the ethical and technical pitfalls the specialists are facing when confronted to simulation cases. To do so, the authors, a legal expert and a psychologist, remind the comments made by Freud at the time he was mandated as an expert to study this field. Then, they are saying more about the difficulties lying on the mental state of the accused can become a problem on a legal scale. Then, they open a discussion on the exact health level of the simulating people. Their intention can lead to the conclusion of their perfect rationality. Nevertheless, resounding scandals are challenging this conception. Even though the individuals are aware of defying the Justice, their behavior may unconsciously hide a mental disorder."
}
@article{KONDO20131,
title = "Modulation of synaptic plasticity by the coactivation of spatially distinct synaptic inputs in rat hippocampal CA1 apical dendrites",
journal = "Brain Research",
volume = "1526",
pages = "1 - 14",
year = "2013",
issn = "0006-8993",
doi = "https://doi.org/10.1016/j.brainres.2013.05.023",
url = "http://www.sciencedirect.com/science/article/pii/S0006899313007415",
author = "Masashi Kondo and Tatsuo Kitajima and Satoshi Fujii and Minoru Tsukada and Takeshi Aihara",
keywords = "Synaptic plasticity, Back-propagating action potential, Dendritic integration, Voltage-sensitive dye, Optical imaging",
abstract = "The phenomenon whereby the relative timing between presynaptic and postsynaptic spiking determines the direction and extent of synaptic changes in a critical temporal window is known as spike timing-dependent synaptic plasticity (STDP). We have previously reported that STDP profiles can be classified into two types depending on their layer-specific location along CA1 pyramidal neuron dendrites in the rat hippocampus, suggesting that there are differences in information processing between the proximal dendrite (PD) and distal dendrite (DD). However, how the different types of information processing interact at different dendritic locations remains unclear. To investigate how the temporal information of inputs to PD influences information processing at DD, PD stimulation was applied while the STDP protocol was simultaneously applied at DDs of CA1 pyramidal neurons. Synaptic plasticity induced by the STDP protocol at DDs was enhanced or depressed depending on the timing of the back-propagating action potentials (bAPs) and the excitatory and inhibitory postsynaptic potentials elicited by PD stimulation. These results suggested that bAPs function as carriers of temporal information of PD inputs to DD. Next, the influence of DD on PD was investigated using the same protocol. Synaptic plasticity at PD was modulated only if the pairing stimuli were applied to elicit coincidental timing of bAP and the excitatory postsynaptic potential. Such coding modulations could provide the basis for a novel learning rule and may be important factors in the integration of spatiotemporal input information in neural networks in the brain."
}
@article{DARABI2012100,
title = "A modified viscoplastic model to predict the permanent deformation of asphaltic materials under cyclic-compression loading at high temperatures",
journal = "International Journal of Plasticity",
volume = "35",
pages = "100 - 134",
year = "2012",
issn = "0749-6419",
doi = "https://doi.org/10.1016/j.ijplas.2012.03.001",
url = "http://www.sciencedirect.com/science/article/pii/S0749641912000411",
author = "Masoud K. Darabi and Rashid K. Abu Al-Rub and Eyad A. Masad and Chien-Wei Huang and Dallas N. Little",
keywords = "Hardening-relaxation, Viscoplastic-softening, Viscoelasticity, Asphalt concrete, Constitutive modeling",
abstract = "When subjected to cyclic creep (ratcheting) loading with rest periods between the loading cycles, the viscoplastic behavior of asphaltic materials changes such the rate of accumulation of the viscoplastic strain at the beginning of the subsequent loading cycle increases comparing to that at the end of the preceding loading cycle. This phenomenon is referred to as the hardening-relaxation (or viscoplastic-softening) and is a key element in predicting the permanent deformation (rutting) of asphalt pavements which is one of the most important distresses in asphalt pavements. This paper presents a phenomenological-based rate-dependent hardening-relaxation model to significantly enhance the prediction of the permanent deformation in asphaltic materials subjected to cyclic-compression loadings at high temperatures. A hardening-relaxation memory surface is defined in the viscoplastic strain space as the general condition for the initiation and evolution of the hardening-relaxation (or viscoplastic-softening). The memory surface is formulated to be a function of an internal state variable memorizing the maximum viscoplastic strain for which the softening has been occurred during the deformation history. The evolution function for the hardening-relaxation model is then defined as a function of the hardening-relaxation internal state variable. The proposed viscoplastic-softening model is coupled to the nonlinear Schapery’s viscoelastic and Perzyna’s viscoplastic models. The numerical algorithms for the proposed model are implemented in the well-known finite element code Abaqus via the user material subroutine UMAT. The model is then calibrated and verified by comparing the model predictions and experimental data that includes cyclic creep-recovery loadings at different stress levels, loading times, rest periods, and confinement levels. Model predictions show that the proposed approach provides a promising tool for constitutive modeling of cyclic hardening-relaxation in asphaltic materials and in general in time- and rate-dependent materials."
}
@article{ZARRETT2012417,
title = "A qualitative study of staff's perspectives on implementing an after school program promoting youth physical activity",
journal = "Evaluation and Program Planning",
volume = "35",
number = "3",
pages = "417 - 426",
year = "2012",
issn = "0149-7189",
doi = "https://doi.org/10.1016/j.evalprogplan.2011.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S0149718911001170",
author = "Nicole Zarrett and Brittany Skiles and Dawn K. Wilson and Lauren McClintock",
keywords = "Adolescent health, Intervention research, Physical activity, Qualitative methods, Social environment",
abstract = "Minimal effects found across youth physical activity (PA) interventions, and increased attention to circumstances that impede adequate delivery of program components, has highlighted the importance of learning from staff what is needed to foster staff comprehension and engagement for developing, adopting, and successfully implementing PA-based youth interventions. The purpose of this study is to address this knowledge gap by conducting a qualitative assessment of school staff perspectives on the positive aspects and challenges of implementing the 17-week ACT program, an after-school intervention that integrated motivational and behavioral components to promote PA in underserved adolescents. Interviews were conducted with one school staff member from each participating school for all four trial cohorts (N=12). Transcripts were coded by independent coders (r=.84) and content analyses of themes was performed using QSR NVivo. Themes were organized into five meta-themes: (1) Logistics; (2) Essential Elements; (3) Staff and Child Challenges; (4) Staff Comprehension, Value, and Enjoyment; (5) Spill-Over Effects. Findings indicate that staff can be successful at understanding, valuing, and reaching fidelity in implementing climate-based mediation components. The insight gained from this study lays the foundation for understanding the components needed for establishing well-implemented, effective, and generalizable interventions for increasing youth PA."
}
@article{PERINI2013344,
title = "High-dimensional, unsupervised cell clustering for computationally efficient engine simulations with detailed combustion chemistry",
journal = "Fuel",
volume = "106",
pages = "344 - 356",
year = "2013",
issn = "0016-2361",
doi = "https://doi.org/10.1016/j.fuel.2012.11.015",
url = "http://www.sciencedirect.com/science/article/pii/S0016236112008915",
author = "Federico Perini",
keywords = "Cell clustering, Detailed chemistry, High-dimensional space, -Means, Internal combustion engines",
abstract = "A novel approach for computationally efficient clustering of chemically reacting environments with similar reactive conditions is presented, and applied to internal combustion engine simulations. The methodology relies on a high-dimensional representation of the chemical state space, where the independent variables (i.e. temperature and species mass fractions) are normalized over the whole dataset space. An efficient bounding-box-constrained k-means algorithm has been developed and used for obtaining optimal clustering of the dataset points in the high-dimensional domain box with maximum computational accuracy, and with no need to iterate the algorithm in order to identify the desired number of clusters. The procedure has been applied to diesel engine simulations carried out with a custom version the KIVA4 code, provided with detailed chemistry capability. Here, the cells of the computational grid are clustered at each time step, in order to reduce the computational time needed by the integration of the chemistry ODE system. After the integration, the changes in species mass fractions of the clusters are redistributed to the cells accordingly. The numerical results, tested over a variety of engine conditions featuring both single- and multiple-pulse injection operation with fuel being injected at 50° BTDC allowed significant computational time savings of the order of 3–4 times, showing the accuracy of the high-dimensional clustering approach in catching the variety of reactive conditions within the combustion chamber."
}
@article{ECKHOFF2015271,
title = "Scaling of dust explosion violence from laboratory scale to full industrial scale – A challenging case history from the past",
journal = "Journal of Loss Prevention in the Process Industries",
volume = "36",
pages = "271 - 280",
year = "2015",
issn = "0950-4230",
doi = "https://doi.org/10.1016/j.jlp.2014.12.020",
url = "http://www.sciencedirect.com/science/article/pii/S0950423014002368",
author = "Rolf K. Eckhoff",
keywords = "Dust explosions, Pressure development, Explosion venting, Silo cells, Wheat grain dust, Soya meal",
abstract = "The standardized KSt parameter still seems to be widely used as a universal criterion for ranking explosion violence to be expected from various dusts in given industrial situations. However, this may not be a generally valid approach. In the case of dust explosion venting, the maximum pressure Pmax generated in a given vented industrial enclosure is not only influenced by inherent dust parameters (dust chemistry including moisture, and sizes and shapes of individual dust particles). Process-related parameters (degree of dust dispersion, cloud turbulence, and dust concentration) also play key roles. This view seems to be confirmed by some results from a series of large scale vented dust explosion experiments in a 500 m3 silo conducted in Norway by CMI, (now GexCon AS) during 1980–1982. Therefore, these results have been brought forward again in the present paper. The original purpose of the 500 m3 silo experiments was to obtain correlations between Pmax in the vented silo and the vent area in the silo top surface, for two different dusts, viz. a wheat grain dust collected in a Norwegian grain import silo facility, and a soya meal used for production of fish farming food. Both dusts were tested in the standard 20-L-sphere in two independent laboratories, and also in the Hartmann bomb in two independent laboratories. Pmax and (dP/dt)max were significantly lower for the soya meal than for the wheat grain dust in all laboratory tests. Because the available amount of wheat grain dust was much larger than the quite limited amount of available soya meal, a complete series of 16 vented silo experiments was first performed with the wheat grain dust, starting with the largest vent area and ending with the smallest one. Then, to avoid unnecessary laborious changes of vent areas, the first experiment with soya dust was performed with the smallest area. The dust cloud in the silo was produced in exactly the same way as with the wheat grain dust. However, contrary to expectations based on the laboratory-scale tests, the soya meal exploded more violently in the large silo than the wheat grain dust, and the silo was blown apart in the very first experiment with this material. The probable reason is that the two dusts responded differently to the dust cloud formation process in the silo on the one hand and in the laboratory-scale apparatuses on the other. This re-confirms that a differentiated philosophy for design of dust explosion vents is indeed needed. Appropriate attention must be paid to the influence of the actual dust cloud generation process on the required vent area. The location and type of the ignition source also play important roles. It may seem that tailored design has to become the future solution for tackling this complex reality, not least for large storage silos. It is the view of the present author that the ongoing development of CFD-based computer codes offers the most promising line of attack. This also applies to design of systems for dust explosion isolation and suppression."
}
@article{LI201548,
title = "A comprehensive photometric study of the eclipsing binary EP Aurigae",
journal = "New Astronomy",
volume = "35",
pages = "48 - 52",
year = "2015",
issn = "1384-1076",
doi = "https://doi.org/10.1016/j.newast.2014.09.004",
url = "http://www.sciencedirect.com/science/article/pii/S1384107614001389",
author = "H.-L. Li and J.-Y. Wei and Y.-G. Yang and K. Li and X.-B. Zhang",
keywords = "Binaries: close, Binaries: eclipsing, Stars: individual (EP Aur)",
abstract = "We present new observations for the eclipsing binary EP Aurigae, which were performed by using three small telescopes in China from 2003 December to 2014 January. With the updated 2003 version of the Wilson–Devinney code, the photometric elements were deduced from three sets of light curves. Based on all available eclipsing times, the orbital period changes were investigated. It is discovered that the (O–C) curve may show an existence of light-time effect due to an unseen third body, which was weakly identified by the photometric solution. The modulated period and amplitude of the cyclic variation are P3=71.2(±8.0)yr and A=0.0101(±0.0008)day, respectively. In the co-planar orbit with the binary system, the mass of the third body is M3=0.18(±0.02)M⊙. The photometric results imply that EP Aur is an Algol-type binary with a mass ratio of q=0.831(±0.004). Its primary component almost fills its Roche lobe. Therefore, EP Aur may consist of a normal main-sequence star and a cool Roche-lobe filling subgiant, which may be undergoing rapid mass transfer."
}
@article{LI2016265,
title = "Autophagy ameliorates cognitive impairment through activation of PVT1 and apoptosis in diabetes mice",
journal = "Behavioural Brain Research",
volume = "305",
pages = "265 - 277",
year = "2016",
issn = "0166-4328",
doi = "https://doi.org/10.1016/j.bbr.2016.03.023",
url = "http://www.sciencedirect.com/science/article/pii/S0166432816301486",
author = "Zhigui Li and Shuang Hao and Hongqiang Yin and Jing Gao and Zhuo Yang",
keywords = "Autophagy, Hippocampus, Diabetes, Cognitive impairment, Synaptic plasticity, PVT1, Long noncoding RNA, Apoptosis, Regulated necrosis",
abstract = "The underlying mechanisms of cognitive impairment in diabetes remain incompletely characterized. Here we show that the autophagic inhibition by 3-methyladenine (3-MA) aggravates cognitive impairment in streptozotocin-induced diabetic mice, including exacerbation of anxiety-like behaviors and aggravation in spatial learning and memory, especially the spatial reversal memory. Further neuronal function identification confirmed that both long term potentiation (LTP) and depotentiation (DPT) were exacerbated by autophagic inhibition in diabetic mice, which indicating impairment of synaptic plasticity. However, no significant change of pair-pulse facilitation (PPF) was recorded in diabetic mice with autophagic suppression compared with the diabetic mice, which indicated that presynaptic function was not affected by autophagic inhibition in diabetes. Subsequent hippocampal neuronal cell death analysis showed that the apoptotic cell death, but not the regulated necrosis, significantly increased in autophagic suppression of diabetic mice. Finally, molecular mechanism that may lead to cell death was identified. The long non-coding RNA PVT1 (plasmacytoma variant translocation 1) expression was analyzed, and data revealed that PVT1 was decreased significantly by 3-MA in diabetes. These findings show that PVT1-mediated autophagy may protect hippocampal neurons from impairment of synaptic plasticity and apoptosis, and then ameliorates cognitive impairment in diabetes. These intriguing findings will help pave the way for exciting functional studies of autophagy in cognitive impairment and diabetes that may alter the existing paradigms."
}
@article{RAMIREZLEYVA201695,
title = "De la promoción de la lectura por placer a la formación integral de lectores",
journal = "Investigación Bibliotecológica: Archivonomía, Bibliotecología e Información",
volume = "30",
number = "69",
pages = "95 - 120",
year = "2016",
issn = "0187-358X",
doi = "https://doi.org/10.1016/j.ibbai.2016.04.014",
url = "http://www.sciencedirect.com/science/article/pii/S0187358X16300181",
author = "Elsa Margarita Ramírez Leyva",
keywords = "Formación de Lectores y Bibliotecología, Lectura y Sociedades del Conocimiento, , Alfabetización Múltiple, Reader's Education, Library Science, Reading and Knowledge Societies, , Multiple Literacies",
abstract = "RESUMEN
En artículo tiene el objetivo de proponer elementos para renovar la formación de lectores ante la complejidad de la lectura que hoy exigen las sociedades del conocimiento. En este contexto surgen concepciones sobre la función de la biblioteca como espacio de formación, aprendizaje, cultura y construcción de sociabilidades; por ello la bibliotecología debe desarrollar una propuesta con una perspectiva de lectura más amplia que la de los modelos pedagógicos en las instituciones educativas. Se identifican en la Bildung aportaciones para una formación de lectores integral, se incorporan además una variedad de recursos que favorecen la lectura de diferentes códigos para ampliar el capital cultural y léxico que a la vez generan modalidades de lectura dirigidas al desarrollo de capacidades de pensamiento crítico y de reflexión involucradas en la construcción de conocimiento, así como para causar experiencias estéticas necesarias en la formación y transformación subjetiva de los ciudadanos a lo largo de su vida.
ABSTRACT
This paper proposes an approach to renewing the teaching of readers how to cope with the complex demands of the information society. In this context, the role of the library in training, learning, and construction of sociability and culture requires examination; and librarianship needs to develop broadly conceived proposals that move beyond pedagogical reading models offered schools. Bildung provides guidance for the comprehensive training of readers across a variety of codes, thereby equipping trainees with reading skills needed to develop broader lexicons and other associated tools and cultural capital that are useful in the construction of knowledge. Moreover, such training enriches the subject with aesthetic experiences that have the potential to effectuate subjective transformation of citizens over the long term."
}
@article{RAMIREZLEYVA201693,
title = "Encouraging reading for pleasure and the comprehensive training for readers",
journal = "Investigación Bibliotecológica: Archivonomía, Bibliotecología e Información",
volume = "30",
number = "69, Supplement ",
pages = "93 - 116",
year = "2016",
issn = "0187-358X",
doi = "https://doi.org/10.1016/j.ibbai.2016.10.018",
url = "http://www.sciencedirect.com/science/article/pii/S0187358X16300624",
author = "Elsa Margarita Ramírez-Leyva",
keywords = "Reader's Education, Library Science, Reading and Knowledge Societies, , Multiple Literacies, Formación de Lectores y Bibliotecología, Lectura y Sociedades del Conocimiento, , Alfabetización Múltiple",
abstract = "ABSTRACT
This paper proposes an approach to renewing the teaching of readers how to cope with the complex demands of the information society. In this context, the role of the library in training, learning, and construction of sociability and culture requires examination; and librarianship needs to develop broadly conceived proposals that move beyond pedagogical reading models offered schools. Bildung provides guidance for the comprehensive training of readers across a variety of codes, thereby equipping trainees with reading skills needed to develop broader lexicons and other associated tools and cultural capital that are useful in the construction of knowledge. Moreover, such training enriches the subject with aesthetic experiences that have the potential to effectuate subjective transformation of citizens over the long term.
RESUMEN
En artículo tiene el objetivo de proponer elementos para renovar la formación de lectores ante la complejidad de la lectura que hoy exigen las sociedades del conocimiento. En este contexto surgen concepciones sobre la función de la biblioteca como espacio de formación, aprendizaje, cultura y construcción de sociabilidades; por ello la bibliotecología debe desarrollar una propuesta con una perspectiva de lectura más amplia que la de los modelos pedagógicos en las instituciones educativas. Se identifican en la Bildung aportaciones para una formación de lectores integral, se incorporan además una variedad de recursos que favorecen la lectura de diferentes códigos para ampliar el capital cultural y léxico que a la vez generan modalidades de lectura dirigidas al desarrollo de capacidades de pensamiento crítico y de reflexión involucradas en la construcción de conocimiento, así como para causar experiencias estéticas necesarias en la formación y transformación subjetiva de los ciudadanos a lo largo de su vida."
}
@article{ZANCHETTA20131026,
title = "Undergraduate nursing students integrating health literacy in clinical settings",
journal = "Nurse Education Today",
volume = "33",
number = "9",
pages = "1026 - 1033",
year = "2013",
issn = "0260-6917",
doi = "https://doi.org/10.1016/j.nedt.2012.05.008",
url = "http://www.sciencedirect.com/science/article/pii/S0260691712001384",
author = "Margareth Zanchetta and Yasmin Taher and Suzanne Fredericks and Janice Waddell and Carol Fine and Rona Sales",
keywords = "Health literacy promotion, Health educator, Undergraduate nursing students, Canada",
abstract = "Summary
Background
Analyzing students' performance and self-criticism of their roles in promoting health literacy can inform nursing education in a social environment that expects new graduates to be health promoters.
Objectives
The pilot study reported here aimed to a) analyze students' understanding of and sensitivity to issues of health literacy, (b) identify students' perceptions of structural, organizational, and political barriers to the promotion of health literacy in social and health care organizations, and (c) document students' suggestions for curriculum changes that would develop their skills and competencies as health-literacy promoters.
Design
A qualitative pilot study.
Setting
A collaborative undergraduate nursing degree program in the metropolitan area of Toronto, Canada.
Participants
Sixteen undergraduate, Year 4 nursing students.
Methods
Signed informed consent was obtained from the participants. Participation was unpaid and voluntary. Recruitment was through an email invitation sent by the School of Nursing Student Affairs Coordinator. Three, one-time individual interviews and three focus groups were conducted. All were audio-recorded. Recordings were transcribed, and the transcriptions were coded using the qualitative software ATLAS ti 6.0. The interview data were submitted to thematic analysis. Additional data were gathered from the two-page self-assessments in students' academic portfolios.
Results
Sensitivity to health literacy was documented. Students performed best as health promoters in supportive teaching hospitals. Their performance was hindered by clinical settings unsupportive of health education, absence of role models, and insufficient theoretical preparation for health teaching. Students' sensitivity to their clients' diversity reportedly reinforced the interconnection, in multicultural healthcare settings, between health literacy and other social determinants of health and a growing demand for educating future nurses in expanding their role also as health promoters.
Conclusions
Students recommended more socially inclusive and experiential learning initiatives related to health teaching to address education gaps in classrooms and practice."
}
@article{FLATEN2016245,
title = "CYP2C19 drug-drug and drug-gene interactions in ED patients",
journal = "The American Journal of Emergency Medicine",
volume = "34",
number = "2",
pages = "245 - 249",
year = "2016",
issn = "0735-6757",
doi = "https://doi.org/10.1016/j.ajem.2015.10.055",
url = "http://www.sciencedirect.com/science/article/pii/S0735675715009481",
author = "Hanna K. Flaten and Howard S. Kim and Jenny Campbell and Lisa Hamilton and Andrew A. Monte",
abstract = "Background
CYP450 polymorphisms result in variable rates of drug metabolism. CYP drug-drug interactions can contribute to altered drug effectiveness and safety.
Study objectives
The primary objective was to determine the percentage of emergency department (ED) patients with cytochrome 2C19 (CYP2C19) drug-drug interactions. The secondary objective was to determine the prevalence of CYP2C19 polymorphisms in a US ED population.
Methods
We conducted a prospective observational study in an urban academic ED with 72,000 annual visits. Drug ingestion histories for the 48 hours preceding ED visit were obtained; each drug was coded as CYP2C19 substrate, inhibitor, inducer, or not CYP2C19 dependent. Ten percent of patients were randomized to undergo CYP2C19 genotyping using the Roche Amplichip.
Results
A total of 502 patients were included; 61% were female, 65% were white, and median age was 39 years (interquartile range, 22-53). One hundred thirty-one (26.1%) patients had taken at least 1 CYP2C19-dependent home drug. Eighteen (13.7%) patients who were already taking a CYP2C19-dependent drug were given or prescribed a CYP2C19-dependent drug while in the ED. Among the 53 patients genotyped, 52 (98%) were extensive metabolizers and 1 was a poor metabolizer.
Conclusions
In a population of ED patients, more than a quarter had taken a CYP2C19-dependent drug in the preceding 48 hours, but few were given or prescribed another CYP2C19-dependent drug in the ED. On genotyping analysis, CYP2C19 polymorphisms were uncommon in our cohort. We conclude that changing prescribing practice due to CYP2C19 drug-drug interaction or genotype is unlikely to be useful in most US ED populations."
}
@article{BEUTHE2015109,
title = "Tides on Europa: The membrane paradigm",
journal = "Icarus",
volume = "248",
pages = "109 - 134",
year = "2015",
issn = "0019-1035",
doi = "https://doi.org/10.1016/j.icarus.2014.10.027",
url = "http://www.sciencedirect.com/science/article/pii/S0019103514005740",
author = "Mikael Beuthe",
keywords = "Europa, Tides, Tectonics, Planetary dynamics",
abstract = "Jupiter’s moon Europa has a thin icy crust which is decoupled from the mantle by a subsurface ocean. The crust thus responds to tidal forcing as a deformed membrane, cold at the top and near melting point at the bottom. In this paper I develop the membrane theory of viscoelastic shells with depth-dependent rheology with the dual goal of predicting tidal tectonics and computing tidal dissipation. Two parameters characterize the tidal response of the membrane: the effective Poisson’s ratio ν¯ and the membrane spring constant Λ, the latter being proportional to the crust thickness and effective shear modulus. I solve membrane theory in terms of tidal Love numbers, for which I derive analytical formulas depending on Λ,ν¯, the ocean-to-bulk density ratio and the number k2∘ representing the influence of the deep interior. Membrane formulas predict h2 and k2 with an accuracy of a few tenths of percent if the crust thickness is less than one hundred kilometers, whereas the error on l2 is a few percents. Benchmarking with the thick-shell software SatStress leads to the discovery of an error in the original, uncorrected version of the code that changes stress components by up to 40%. Regarding tectonics, I show that different stress-free states account for the conflicting predictions of thin and thick shell models about the magnitude of tensile stresses due to nonsynchronous rotation. Regarding dissipation, I prove that tidal heating in the crust is proportional to Im(Λ) and that it is equal to the global heat flow (proportional to Im(k2)) minus the core-mantle heat flow (proportional to Im(k2∘)). As an illustration, I compute the equilibrium thickness of a convecting crust. More generally, membrane formulas are useful in any application involving tidal Love numbers such as crust thickness estimates, despinning tectonics or true polar wander."
}
@article{PECH20152083,
title = "Optical Dissection of Experience-Dependent Pre- and Postsynaptic Plasticity in the Drosophila Brain",
journal = "Cell Reports",
volume = "10",
number = "12",
pages = "2083 - 2095",
year = "2015",
issn = "2211-1247",
doi = "https://doi.org/10.1016/j.celrep.2015.02.065",
url = "http://www.sciencedirect.com/science/article/pii/S2211124715002429",
author = "Ulrike Pech and Natalia H. Revelo and Katharina J. Seitz and Silvio O. Rizzoli and André Fiala",
abstract = "Summary
Drosophila represents a key model organism for dissecting neuronal circuits that underlie innate and adaptive behavior. However, this task is limited by a lack of tools to monitor physiological parameters of spatially distributed, central synapses in identified neurons. We generated transgenic fly strains that express functional fluorescent reporters targeted to either pre- or postsynaptic compartments. Presynaptic Ca2+ dynamics are monitored using synaptophysin-coupled GCaMP3, synaptic transmission is monitored using red fluorescent synaptophysin-pHTomato, and postsynaptic Ca2+ dynamics are visualized using GCaMP3 fused with the postsynaptic matrix protein, dHomer. Using two-photon in vivo imaging of olfactory projection neurons, odor-evoked activity across populations of synapses is visualized in the antennal lobe and the mushroom body calyx. Prolonged odor exposure causes odor-specific and differential experience-dependent changes in pre- and postsynaptic activity at both levels of olfactory processing. The approach advances the physiological analysis of synaptic connections across defined groups of neurons in intact Drosophila."
}
@article{KERSEVAN2013919,
title = "The Monte Carlo event generator AcerMC versions 2.0 to 3.8 with interfaces to PYTHIA 6.4, HERWIG 6.5 and ARIADNE 4.1",
journal = "Computer Physics Communications",
volume = "184",
number = "3",
pages = "919 - 985",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2012.10.032",
url = "http://www.sciencedirect.com/science/article/pii/S001046551200375X",
author = "Borut Paul Kersevan and Elzbieta Richter-Wa̧s",
keywords = "SM backgrounds at LHC, Massive matrix elements, Monte Carlo generator, Heavy flavour production, Multi-channel phase-space generation",
abstract = "The AcerMC Monte Carlo generator is dedicated to the generation of Standard Model background processes which were recognised as critical for the searches at LHC, and generation of which was either unavailable or not straightforward so far. The program itself provides a library of the massive matrix elements (coded by MADGRAPH) and native phase space modules for generation of a set of selected processes. The hard process event can be completed by the initial and the final state radiation, hadronisation and decays through the existing interface with either PYTHIA, HERWIG or ARIADNE event generators and (optionally) TAUOLA and PHOTOS. Interfaces to all these packages are provided in the distribution version. The phase-space generation is based on the multi-channel self-optimising approach using the modified Kajantie–Byckling formalism for phase space construction and further smoothing of the phase space was obtained by using a modified ac-VEGAS algorithm. An additional improvement in the recent versions is the inclusion of the consistent prescription for matching the matrix element calculations with parton showering for a select list of processes.
Program summary
Program title: AcerMC version 3.8 Catalogue identifier: ADQQ_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/ADQQ_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 3853309 No. of bytes in distributed program, including test data, etc.: 68045728 Distribution format: tar.gz Programming language: FORTRAN 77 with popular extensions (g77, gfortran). Computer: All running Linux. Operating system: Linux. Classification: 11.2, 11.6. External routines: CERNLIB (http://cernlib.web.cern.ch/cernlib/), LHAPDF (http://lhapdf.hepforge.org/) Catalogue identifier of previous version: ADQQ_v1_0 Journal reference of previous version: Comput. Phys. Comm. 149(2003)142 Does the new version supersede the previous version?: Yes Nature of problem: Despite a large repertoire of processes implemented for generation in event generators like PYTHIA [1] or HERWIG [2] a number of background processes, crucial for studying the expected physics of the LHC experiments, is missing. For some of these processes the matrix element expressions are rather lengthy and/or to achieve a reasonable generation efficiency it is necessary to tailor the phase space selection procedure to the dynamics of the process. That is why it is not practical to imagine that any of the above general purpose generators will contain every, or even only observable, processes which will occur at LHC collisions. A more practical solution can be found in a library of dedicated matrix-element-based generators, with the standardised interfaces like that proposed in [3], to the more universal one which is used to complete the event generation. Solution method: The AcerMC EventGenerator provides a library of the matrix-element-based generators for several processes. The initial- and final-state showers, beam remnants and underlying events, fragmentation and remaining decays are supposed to be performed by the other universal generator to which this one is interfaced. We will call it a supervising generator. The interfaces to PYTHIA 6.4, ARIADNE 4.1 and HERWIG 6.5, as such generators, are provided. Provided is also an interface to TAUOLA [4] and PHOTOS [5] packages for τ-lepton decays (including spin correlations treatment) and QED radiations in decays of particles. At present, the following matrix-element-based processes have been implemented: gg,qq̄→tt̄bb̄, qq̄→W(→ℓν)bb̄; qq̄→W(→ℓν)tt̄; gg,qq̄→Z/γ∗(→ℓℓ)bb̄; gg,qq̄→Z/γ∗(→ℓℓ,νν,bb̄)tt̄; complete EW gg,qq̄→(Z/W/γ∗→)tt̄bb̄; gg,qq̄→tt̄tt̄; gg,qq̄→(tt̄→)ff̄bff̄b̄; gg,qq̄→(WWbb→)ff̄ff̄bb̄. Both interfaces allow the use of the LHAPDF/LHAGLUE library of parton density functions. Provided is also a set of control processes: qq̄→W→ℓν; qq̄→Z/γ∗→ℓℓ; gg,qq̄→tt̄ and gg→(tt̄→)WbWb̄; Reasons for new version: Implementation of several new processes and methods. Summary of revisions: Each version added new processes or functionalities, a detailed list is given in the section “Changes since AcerMC 1.0”. Restrictions: The package is optimised for the 14 TeV pp collision simulated in the LHC environment and also works at the achieved LHC energies of 7 TeV and 8 TeV. The consistency between results of the complete generation using PYTHIA 6.4 or HERWIG 6.5 interfaces is technically limited by the different approaches taken in both these generators for evaluating αQCD and αQED couplings and by the different models for fragmentation/hadronisation. For the consistency check, in the AcerMC library contains native coded definitions of the QCD and αQED. Using these native definitions leads to the same total cross-sections both with PYTHIA 6.4 or HERWIG 6.5 interfaces. Additional comments: !!!!! The distribution file for this program is over 67 Mbytes and therefore is not delivered directly when download or Email is requested. Instead an html file giving details of how the program can be obtained is sent. !!!!! Running time: On an PIII 800 MHz PC it amounts to ∼0.05→1.1 events/sec, depending on the choice of process. References:[1]T. Sjostrand et al., High energy physics generation with PYTHIA 6.2, eprint hep-ph/0108264, LU-TP 01-21, August 2001.[2]G. Julyesini et al., Comp. Phys. Commun. 67 (1992) 465, G. Corcella et al., JHEP 0101 (2001) 010.[3]E. Boos at al., Generic user process interface for event generators, hepph /0109068.[4]S. Jadach, J. H. Kuhn, Z. Was, Comput. Phys. Commun. 64 (1990) 275; M. Jezabek, Z. Was, S. Jadach, J. H. Kuhn, Comput. Phys. Commun. 70 (1992) 69; R. Decker, S. Jadach, J. H. Kuhn, Z. Was, Comput. Phys. Commun. 76 (1993) 361.[5]E. Barberio and Z. Was, Comp. Phys. Commun. 79 (1994) 291."
}
@article{RODRIGUEZ2014958,
title = "Clinical features of leiomyosarcoma of the urinary bladder: Analysis of 183 cases",
journal = "Urologic Oncology: Seminars and Original Investigations",
volume = "32",
number = "7",
pages = "958 - 965",
year = "2014",
issn = "1078-1439",
doi = "https://doi.org/10.1016/j.urolonc.2014.01.025",
url = "http://www.sciencedirect.com/science/article/pii/S1078143914000313",
author = "Dayron Rodríguez and Mark A. Preston and Glen W. Barrisford and Aria F. Olumi and Adam S. Feldman",
keywords = "Cancer, Urinary bladder, Leiomyosarcoma, SEER, Outcomes, Prognostic factors",
abstract = "Introduction
Experience with management of urinary bladder leiomyosarcoma (LMS) is rare. Therefore, to better elucidate the disease characteristics of urinary bladder LMS, we utilized a large population-based cancer registry to examine the epidemiology, natural history, pathological characteristics, prognostic factors, and treatment outcomes.
Material and methods
The Surveillance, Epidemiology, and End Results database (1973–2010) was used to identify cases by tumor site and histology codes. The association between clinical and demographic characteristics and long-term survival was examined.
Results
A total of 183 histologically confirmed cases were identified between 1973 and 2010. The annual age-adjusted incidence rate was 0.23 cases per 1,000,000 and did not significantly change over time. Median age of the patients was 65 years (interquartile range: 47–78y). Of the patients with a known pathologic tumor stage (n = 164), 50% had a regional or distant disease. Overall, 63.2% of patients with known histologic grade (n = 106), had poorly differentiated or undifferentiated histology. Most patients (92.9%) received cancer-directed surgery (CDS), with 34.4% having radical or partial cystectomy. Only 7.7% of patients received radiation therapy in combination with surgery. The median disease-specific survival was 46 months. Five- and 10-year cancer-specific survival rates were 47%, and 35%, respectively. On multivariate analysis, a worse outcome was associated with an undifferentiated tumor grade, distant disease, and failure to undergo CDS.
Conclusion
This series represents the largest cohort of LMS of the urinary bladder studied to date. LMS commonly presented as high grade and advanced stage with a poor prognosis. Reduced disease-specific survival was associated with increasing age, undifferentiated tumor grade, distant disease, and failure to undergo CDS."
}
@article{DROBYSHEV2012137,
title = "Detecting changes in climate forcing on the fire regime of a North American mixed-pine forest: A case study of Seney National Wildlife Refuge, Upper Michigan",
journal = "Dendrochronologia",
volume = "30",
number = "2",
pages = "137 - 145",
year = "2012",
note = "WORLD DENDRO 2010",
issn = "1125-7865",
doi = "https://doi.org/10.1016/j.dendro.2011.07.002",
url = "http://www.sciencedirect.com/science/article/pii/S1125786511000786",
author = "Igor Drobyshev and P. Charles Goebel and Yves Bergeron and R. Gregory Corace",
keywords = "Historical fire regimes, Fire scars, Top-down controls, Fire suppression, Natural disturbances, Hemi-boreal, Climate variability",
abstract = "The study of forests dominated by red pine (Pinus resinosa Ait.), one of the few fire-resistant tree species of eastern North America, provides an opportunity to reconstruct long-term fire histories and examine the temporal dynamics of climate forcing upon forest fire regimes. We used a 300-year long spatially explicit dendrochronological reconstruction of the fire regime for Seney National Wildlife Refuge (SNWR, 38,531ha), eastern Upper Michigan to: (1) identify fire size thresholds with strong vs. weak climate controls, (2) evaluate effect of landform type (outwash channel vs. sand ridges) in modifying climate–fire associations, and (3) check for the presence of temporal changes in the climate control of large fire events over the time period 1700–1983. We used a summer drought sensitive red pine chronology (ITRDB code can037) as a proxy of past fire-related climate variability. Results indicated that fires >60ha in sand-ridge-dominated portions of SNWR and >100ha in outwash channels were likely climatically driven events. Climate–fire associations varied over time with significant climate–fire linkages observed for the periods 1700–1800 (pre-EuroAmerican), 1800–1900 (EuroAmerican settlement) and 1900–1983 (modern era). Although an increase in fire activity at the turn of 20th century is commonly associated with human sources of ignitions, our results suggest that such an increase was also likely a climatically driven episode."
}
@article{FERNANDEZDELGADO201460,
title = "Direct Kernel Perceptron (DKP): Ultra-fast kernel ELM-based classification with non-iterative closed-form weight calculation",
journal = "Neural Networks",
volume = "50",
pages = "60 - 71",
year = "2014",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2013.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S0893608013002530",
author = "Manuel Fernández-Delgado and Eva Cernadas and Senén Barro and Jorge Ribeiro and José Neves",
keywords = "Kernel-based classification, Extreme learning machine, Support vector machine, Analytical weight calculation, Closed-form solution, Margin maximization, Parallel Delta rule",
abstract = "The Direct Kernel Perceptron (DKP) (Fernández-Delgado et al., 2010) is a very simple and fast kernel-based classifier, related to the Support Vector Machine (SVM) and to the Extreme Learning Machine (ELM) (Huang, Wang, & Lan, 2011), whose α-coefficients are calculated directly, without any iterative training, using an analytical closed-form expression which involves only the training patterns. The DKP, which is inspired by the Direct Parallel Perceptron, (Auer et al., 2008), uses a Gaussian kernel and a linear classifier (perceptron). The weight vector of this classifier in the feature space minimizes an error measure which combines the training error and the hyperplane margin, without any tunable regularization parameter. This weight vector can be translated, using a variable change, to the α-coefficients, and both are determined without iterative calculations. We calculate solutions using several error functions, achieving the best trade-off between accuracy and efficiency with the linear function. These solutions for the α coefficients can be considered alternatives to the ELM with a new physical meaning in terms of error and margin: in fact, the linear and quadratic DKP are special cases of the two-class ELM when the regularization parameter C takes the values C=0 and C=∞. The linear DKP is extremely efficient and much faster (over a vast collection of 42 benchmark and real-life data sets) than 12 very popular and accurate classifiers including SVM, Multi-Layer Perceptron, Adaboost, Random Forest and Bagging of RPART decision trees, Linear Discriminant Analysis, K-Nearest Neighbors, ELM, Probabilistic Neural Networks, Radial Basis Function neural networks and Generalized ART. Besides, despite its simplicity and extreme efficiency, DKP achieves higher accuracies than 7 out of 12 classifiers, exhibiting small differences with respect to the best ones (SVM, ELM, Adaboost and Random Forest), which are much slower. Thus, the DKP provides an easy and fast way to achieve classification accuracies which are not too far from the best one for a given problem. The C and Matlab code of DKP are freely available.11http://www.gsi.dec.usc.es/~delgado/papers/dkp."
}
@article{BUSA20122494,
title = "ARVO-CL: The OpenCL version of the ARVO package — An efficient tool for computing the accessible surface area and the excluded volume of proteins via analytical equations",
journal = "Computer Physics Communications",
volume = "183",
number = "11",
pages = "2494 - 2497",
year = "2012",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2012.04.019",
url = "http://www.sciencedirect.com/science/article/pii/S0010465512001580",
author = "Ján Buša and Shura Hayryan and Ming-Chya Wu and Ján Buša and Chin-Kun Hu",
keywords = "ARVO, Proteins, Solvent accessible area, Excluded volume, Stereographic projection, OpenCL package",
abstract = "Introduction of Graphical Processing Units (GPUs) and computing using GPUs in recent years opened possibilities for simple parallelization of programs. In this update, we present the modernized version of program ARVO [J. Buša, J. Dzurina, E. Hayryan, S. Hayryan, C.-K. Hu, J. Plavka, I. Pokorný, J. Skivánek, M.-C. Wu, Comput. Phys. Comm. 165 (2005) 59]. The whole package has been rewritten in the C language and parallelized using OpenCL. Some new tricks have been added to the algorithm in order to save memory much needed for efficient usage of graphical cards. A new tool called ‘input_structure’ was added for conversion of pdb files into files suitable for work with the C and OpenCL version of ARVO.
New version program summary
Program title: ARVO-CL Catalog identifier: ADUL_v2_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ADUL_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 11834 No. of bytes in distributed program, including test data, etc.: 182528 Distribution format: tar.gz Programming language: C, OpenCL. Computer: PC Pentium; SPP’2000. Operating system: All OpenCL capable systems. Has the code been vectorized or parallelized?: Parallelized using GPUs. A serial version (non GPU) is also included in the package. Classification: 3. External routines: cl.hpp (http://www.khronos.org/registry/cl/api/1.1/cl.hpp) Catalog identifier of previous version: ADUL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 165(2005)59 Does the new version supercede the previous version?: Yes Nature of problem: Molecular mechanics computations, continuum percolation Solution method: Numerical algorithm based on the analytical formulas, after using the stereographic transformation. Reasons for new version: During the past decade we have published a number of protein structure related algorithms and software packages [1,2,3,4,5,6] which have received considerable attention from researchers and interesting applications of such packages have been found. For example, ARVO [4] has been used to find that ratios of volume V to surface area A, for proteins in Protein Data Bank (PDB) distribute in a narrow range [7]. Such a result is useful for finding native structures of proteins. Therefore, we consider that there is a demand to revise and modernize these tools and to make them more efficient. Here we present the new version of the ARVO package. The original ARVO package was written in the FORTRAN language. One of the reasons for the new version is to rewrite it in C in order to make it more friendly to the young researchers who are not familiar with FORTRAN. Another, more important reason is to use the possibilities for speeding-up provided by modern graphical cards. We also want to eliminate the necessity of re-compiling the program for every molecule. For this purpose, we have added the possibility of using general pdb [8] files as an input. Once compiled, the program can receive any number of input files successively. Also, we found it necessary to go through the algorithm and to make some tricks for avoiding unnecessary memory usage so that the package becomes more efficient. Summary of revisions: 1. New tool. ARVO is designed to calculate the volume and accessible surface area of an arbitrary system of overlapping spheres (representing atoms), the biomolecules being just one albeit important, application. The user provides the coordinates and radii of the spheres as well as the radius of the probe sphere (water molecule for biomolecules). In the old version of ARVO the input of data was organized immediately in the code, which made it necessary to re-compile the program after every change in the input data. In the current version a module called ‘input_structure’ has been created to input the data from an independent external file. The coordinates and radii are stored in the file with extension *.ats (see the directory ‘input’ in the package). Each line in the file corresponds to one sphere (atom) and has the format 24.733−4.992−13.2562.800. The first three numbers are the (x,y,z) coordinates of the atom and the last one is the radius. It is important to remember that the radius of the probe sphere must be already added to this number. In the above example, the value 2.800 is obtained by the formula “sphere radius+probe sphere radius”. In the case of the arbitrary system of spheres the file *.ats is created by the user. In the case of proteins the ‘input_structure’ takes as an input a file in the format compatible with Protein Data Bank (pdb) format [8] and creates a corresponding *.ats file. It also assigns automatically, radii to individual spheres and (optionally) adds to all radii the probe sphere (water molecule) radius. As output, it produces a file containing coordinates of spheres together with radii. This file works automatically as an input for ARVO. Using an external tool allows users to create their own mappings of atoms and radii without the need to re-compile the tool ‘input_structure’ or program ARVO. It is again the user’s responsibility to assign proper radii to each type of atom. One can use any of the published standard sets of radii (see for example, [9,10,11,12,13]). Alternatively, the user can assign his own values for radii immediately in the module input_structure. The radii are assigned in a special file with extension *pds (see the documentation) which consists of lines like this: ATOM CA ALA 2.0 which is read as “the Calpha atom of Alanine has radius 2.0 Angstroms”. Here we provide for testing of the file rashin.pds where the radii are assigned according to [12]. The output file contains only recognized atoms. Atoms that were not recognized (are not part of mapping) are written to a separate log file allowing the user to review and correct the mapping files later. 2. The Language. Implementing the program in C is a natural first step when translating a program into OpenCL. This implementation is rewritten line-by-line from the original FORTRAN version of ARVO. 3. OpenCL implementation. OpenCL [14] is an open standard for parallel programming of heterogeneous systems. Unlike other parallelization technologies like CUDA [15] or ATI Stream [16] which are interconnected with specific hardware (produced by NVIDIA or ATI, respectively), OpenCL is vendor-independent, and programs written in OpenCL can be run on any hardware of companies supporting this standard, including AMD, INTEL, and NVIDIA. Programs written in OpenCL can be run without much change both on CPUs and GPUs. Improvements as compared with the original version: Support for files in the format as created by ‘input_structure’; input of parameters (name of input file) via command line; dynamic size of arrays—removal of the necessity to re-compile the program after any change in size of structures; memory allocation according to the real demands of the application; replacing north pole test by slight reduction of the radius (see below). To compile an OpenCL program, one needs to download and install the appropriate driver and software development kit (SDK). The program itself consists of two parts: a part running on the CPU and a part running on the GPU. The CPU initializes communication between the computer and the GPU, load data, processes and exports results. The GPU does the parallel part of calculation, consisting of the search for neighboring atoms and calculating the contribution of the area and volume of the individual atom to the total area and volume of the molecule. For details of the algorithm, please read Refs. [3,4]. In programming using OpenCL, more attention must be given to memory used than in a classical approach. Memory of the device is usually limited and therefore, some changes to the original algorithm are necessary. First, unlike in the FORTRAN version of the program, no structures containing the list of neighbor atoms are created. The search for the neighbors is done on-line, when the calculation of the contribution from individual atoms is being performed. Table 1Comparison of volumes and surface areas of different proteins obtained by original ARVO and by the new version. Different strategies for dealing with the “north pole” are applied. The first column contains the PDB ID of the protein and the number of atoms. Second column contains the volume of the protein obtained with original ARVO (upper number) and the difference with the new approach (lower number). Third column contains the same as in the second column for the surface area. Fourth column contains the number of rotations of the molecule in original ARVO (upper number) and the number of atoms whose radii have been reduced in the new version (lower number). Fifth column contains the relative errors for the volume (upper number) and the area (lower number).Protein atoms #Volume diffArea diffRotat. reduct.δvolume (%) δarea (%)3rn323,951.1804696858.3226363−1.04⋅10−7957−0.000025−0.0000071−1.02⋅10−73cyt40,875.86739511,455.4748323−3.85⋅10−61600−0.0015750.00141541.24⋅10−42act38,608.2430389054.00735041.28⋅10−416570.0494800.00173321.91⋅10−52brd43,882.73547910,918.20352921−7.84⋅10−71738−0.000344−0.0000971−8.88⋅10−78tln56,698.98888312,496.97806415−1.70⋅10−62455−0.0009660.00045943.67⋅10−61rr8105,841.50219227,983.15977218−6.60⋅10−74108−0.000699−0.0002144−7.65⋅10−71xi51743,445.092001863,139.88270314.42⋅10−715,6960.0077090.00007018.11⋅10−9 The strategy behind the North Pole check and molecule rotation [4, Sec. 4.7] has been changed. If during the north pole test, the north pole of the active sphere lies close to the surface of a neighboring sphere, the radius of such a neighboring sphere is multiplied by 0.9999 instead of rotating the whole molecule. This allows the algorithm to continue normally. Changing the radius of one atom changes the area and the volume of this atom by 0.02% and 0.03%, respectively. As the atom’s contribution to the total area (volume) of the protein is usually only a part of the atom’s total area (volume) and since there are many atoms in the protein itself, the change of total area (volume) is much smaller than 0.02% (0.03%). Testings showed relative errors ranging from 10−4 down to 10−8. An additional benefit of this approach is, that the whole molecule is not rotated and therefore no errors are introduced there which would occur during such rotation. We were even able to find a protein (1S1I having 31,938 atoms), where, after several hundreds of rotations, ARVO was not able to find such a position that the original north pole test could pass. For such proteins the new approach is the only one possible. Some data obtained using the north pole test (with rotation) and those without the north pole test (with radii reduction) are summarized in Table 1. The radius of water molecule was set to 1.4 Å, and Rashin’s set of the van der Waals radii of atoms [12] was used. The first column contains the protein name and the number of atoms. Each cell of the second and the third columns contains two numbers. The upper number is the volume (surface area) obtained using the original ARVO algorithm [4] with conventional north pole test and rotation. The lower number shows the difference coming from using the new approach. The upper number in the fourth column shows the number of rotations when using the original version and the second number is the number of atoms for which the radius has been reduced. The relative error of volume (upper number) and area (lower number) obtained by using radius reduction are shown in the last column. It can be seen clearly that the error is negligible. The disadvantage is that calculations using OpenCL are done with single precision only. This comes from the fact that the OpenCL standard does not support double precision float number operations as a basic part but as an extension only. This means that availability of double precision calculations depends on the device (CPU, GPU) vendor. Switching to double precision calculations downgrades speed performance (calculations in double precision are 8–2 times slower than the same calculations in single precision). Another problem is that after using the double precision switch, all calculations are done with double precision which leads to problems with insufficient memory. This problem can be bypassed by explicitly switching to single precision where possible but this requires careful modification of the whole program source. Since on our GPU (NVIDIA GTX 480) double precision was available, we have decided to use the double precision only for the critical parts of algorithm (s.a. integral calculation), leaving non-critical parts in single precision. This allowed us to speed up the calculation and to obtain acceptable results. Results of the test calculations are given in Table 2. All calculations except for 2brd0 have been performed using water radius 1.4 Å. The first column contains the protein name and the number of atoms. The second column contains computation time in seconds (in FORTRAN/CPU—upper part and OpenCL/GPU—lower part). The third column is a speed-up (time on the CPU divided by time on the GPU). The fourth and fifth columns contain the volume and area calculated in FORTRAN (upper number) and the difference when compared to results obtained by OpenCL (lower number). As one can see, the area and the volume obtained using FORTRAN (in double precision) and the OpenCL implementation (combination of single and double precisions) are practically the same. This is even more clear from the relative error of the OpenCL implementation as shown in the last column (upper number for volume, and lower number for area). As to computational time, FORTRAN (C) implementation is appropriate in the case when the calculation takes approximately less than 2 s. This is because in the case of OpenCL some time–about 0.3–1.5 s on testing configuration–is needed for the initialization of the device and for starting the communication. Speed-up is clearly visible for large proteins when the parallel approach can be exploited, but complexity of protein needs to be taken into account as well. Compare the times for 2brd (water radius 1.4 Å) and 2brd0 (water radius 0 Å). The difference is in the number of neighbors (overlapping spheres). While, for water radius 1.4 Å the number of neighbors is high and using the GPU is efficient, for water radius 0 Å it is better to use CPU. All results were obtained on a test configuration with CPU Intel Core i7 930 processor running at 2.8 GHz and a GPU NVIDIA GeForce GTX 480. Table 2The table contains comparative data on precision and computational times obtained by FORTRAN vs. OpenCL implementations of ARVO. The structure of the columns is similar to Table 1. Note that last protein (1s1i) was not calculated using FORTRAN implementation and comparison presented is between C and OpenCL version. This is because we were not able to find such rotation that north pole test would pass.Protein atoms #Time F95 (s) OpenCLSpeed upVolume diffArea diffδvolume (%) δarea (%)1eca8.236.0126,072.0030697004.1681381.65⋅10−510311.370.0043100.0004987.11⋅10−62ptn13.729.0139,273.2209339227.570716−2.01⋅10−516291.52−0.007906−0.005795−6.28⋅10−52brd15.779.9143,882.73513610,918.203432−1.44⋅10−517381.59−0.0063260.0014711.35⋅10−52brd00.290.9122,412.82580722,546.123881−9.13⋅10−517380.32−0.020471−0.008437−9.17⋅10−48tln23.3213.7456,698.98855012,496.977990−5.34⋅10−624551.70−0.003028−0.008708−4.64⋅10−41rr830.8917.67105,841.50149227,983.1595581.93⋅10−541081.750.020445−0.000802−2.87⋅10−61s1i286.8133.95816,980.348702253,160.674893−1.40⋅10−431,9388.45−1.1407630.0494781.95⋅10−5 At the time of writing, OpenCL allowed the allocation of only 1/4 of the total memory of the devices (CPU, GPU) by one call to malloc. This can be bypassed by four individual calls of memory allocation requesting 1/4 of the total devices’ memory. It is advisable to use a dedicated GPU for the calculations since sharing a GPU for calculations and displaying graphics can lead to unexpected results due to common access to the memory of devices. Restrictions: The program does not account for possible cavities inside the molecule. The current version works in a combination of single and double precisions (see Summary of revisions for details). Running time: Depends on the size of the molecule under consideration. For molecules whose running time was less than 2 s in the old version the performance is likely to decrease. This changes considerably when larger molecules are calculated (in test configuration speed-ups up to 34 were obtained). References:[1]F. Eisenmenger, U.H.E. Hansmann, S. Hayryan, C.-K. Hu, Comput. Phys. Commun. 138 (2001) 192.[2]F. Eisenmenger, U.H.E. Hansmann, S. Hayryan, C.-K. Hu, Comput. Phys. Commun. 174 (2006) 422.[3]S. Hayryan, C.-K. Hu, J. Skrivánek, E. Hayryan, I. Pokorný, J. Comput. Chem. 26 (2005) 334.[4]J. Busa, J. Dzurina, E. Hayryan, S. Hayryan, C.-K. Hu, J. Plavka, I. Pokorný, J. Skrivánek, M.-C. Wu, Comput. Phys. Commun. 165 (2005) 59.[5]J. Busa, S. Hayryan, C.-K. Hu, J. Skrivánek, M.-C. Wu, J. Comput. Chem. 30 (2009) 346.[6]J. Busa, S. Hayryan, C.-K. Hu, J. Skrivánek, M.-C. Wu, Comput. Phys. Commun. 181 (2010) 2116.[7]M.-C. Wu, M.S. Li, W.-J. Ma, M. Kouza, C.-K. Hu, EPL 96 (2011) 68005.[8]http://www.rcsb.org.[9]B. Lee, F.M. Richards, J. Mol. Biol. 55 (1971) 379.[10]F.M. Richards, Annu. Rev. Bipohys. Bioeng. 6 (1977) 151.[11]A. Shrake, J.A. Rupley, J. Mol. Biol. 79 (1973) 351.[12]A.A. Rashin, M. Iofin, B. Honig, Biochemistry 25 (1986) 3619.[13]C. Chotia, Nature 248 (1974) 338.[14]http://www.khronos.org/opencl/.[15]http://www.nvidia.com/object/cuda_home_new.html.[16]http://www.amd.com/stream."
}
@article{MOIGNIER2014287,
title = "Determination of small field output factors and correction factors using a Monte Carlo method for a 1000 MU/min CyberKnife® system equipped with fixed collimators",
journal = "Radiation Measurements",
volume = "71",
pages = "287 - 292",
year = "2014",
note = "Proceedings of the 17th Solid State Dosimetry Conference (SSD17)",
issn = "1350-4487",
doi = "https://doi.org/10.1016/j.radmeas.2014.05.009",
url = "http://www.sciencedirect.com/science/article/pii/S1350448714001371",
author = "C. Moignier and C. Huet and V. Barraux and C. Bassinet and M. Baumann and K. Sebe-Mercier and C. Loiseau and A. Batalla and L. Makovicka",
keywords = "Small fields, Output factor, Monte Carlo, CyberKnife",
abstract = "A new formalism for small field dosimetry has been proposed (Alfonso et al., 2008) with the concept of an additional correction factor (kQclin,Qmsrfclin,fmsr) which accounts for possible changes in detector response with field size. The aim of this work was to evaluate the response of eight commercially available detectors, then to provide a set of correction factors for a 1000 MU/min CyberKnife® equipped with fixed collimators and to compare them with those obtained for the 800 MU/min CyberKnife® version. Measurements were performed on a 1000 MU/min CyberKnife® with several active detectors designed for small field dosimetry (two chambers (PTW 31014 and 31018), three high resolution diodes (PTW 60016, 60017 and Sun Nuclear EDGE), a natural diamond (PTW 60003)) and two passive dosimeters (Harshaw TLD-700 7LiF:Mg,Ti thermoluminescent micro-cube and EBT3 radiochromic films). The CyberKnife® as well as the diode detectors, the PinPoint chamber, the diamond and the LiF micro-cubes were modeled with the PENELOPE Monte Carlo code in order to calculate the output factors in a point-like voxel of water (OFMC,w). A set of kQclin,Qmsrfclin,fmsr correction factors for the active detectors investigated is provided for the 1000 MU/min CyberKnife® in order to be used with the new formalism. A difference up to 2.4%, 2.0 and 1.7% in the correction factor obtained for the two different CyberKnife® models is found for the PTW 60003, the PTW 60016 and the PTW 60017 respectively. Although this difference is small, we recommend using specific kQclin,Qmsrfclin,fmsr correction factors for the 1000 MU/min CyberKnife® when they are available."
}
@article{MOYLE2017766,
title = "Use of a Robotic Seal as a Therapeutic Tool to Improve Dementia Symptoms: A Cluster-Randomized Controlled Trial",
journal = "Journal of the American Medical Directors Association",
volume = "18",
number = "9",
pages = "766 - 773",
year = "2017",
issn = "1525-8610",
doi = "https://doi.org/10.1016/j.jamda.2017.03.018",
url = "http://www.sciencedirect.com/science/article/pii/S1525861017301895",
author = "Wendy Moyle and Cindy J. Jones and Jenny E. Murfield and Lukman Thalib and Elizabeth R.A. Beattie and David K.H. Shum and Siobhan T. O'Dwyer and M. Cindy Mervin and Brian M. Draper",
keywords = "Agitation, BPSD, engagement, PARO, mood state, older people",
abstract = "Objectives
To test the effects of individual, nonfacilitated sessions with PARO (version 9), when compared against a look-alike plush toy and usual care, on the emotional and behavioral symptoms of dementia for people living in long-term care facilities.
Design
Parallel, 3-group, cluster-randomized controlled trial conducted between June 14, 2014, and May 16, 2015.
Setting
Twenty-eight long-term care facilities operated by 20 care organizations located in South-East Queensland, Australia.
Participants
Four hundred fifteen participants aged ≥60 years, with a documented diagnosis of dementia.
Intervention
Stratified by private/not-for-profit status and randomized using a computer-generated sequence, 9 facilities were randomized to the PARO group (individual, nonfacilitated, 15-minute sessions 3 times per week for 10 weeks); 10 to plush toy (same, but given PARO with robotic features disabled); and 9 to usual care. Treatment allocation was masked to assessors.
Measurements
Primary outcomes were changes in levels of engagement, mood states, and agitation after a 10-week intervention, assessed by coded video observations (baseline, weeks 1, 5, 10, and 15) and Cohen-Mansfield Agitation Inventory–Short Form (baseline, weeks 10 and 15). Analyses followed intention-to-treat, using repeated measures mixed effects models. Australian New Zealand Clinical Trials Registry (ACTRN12614000508673).
Results
Video data showed that participants in the PARO group were more verbally [3.61, 95% confidence interval (CI): 6.40–0.81, P = .011] and visually engaged (13.06, 95% CI: 17.05–9.06, P < .0001) than participants in plush toy. Both PARO (−3.09, 95% CI: −0.45 to −5.72, P = .022) and plush toy (−3.58, 95% CI: −1.26 to −5.91, P = .002) had significantly greater reduced neutral affect compared with usual care, whilst PARO was more effective than usual care in improving pleasure (1.12, 95% CI: 1.94–0.29, P = .008). Videos showed that PARO was more effective than usual care in improving agitation (3.33, 95% CI: 5.79–0.86, P = .008). When measured using the CMAI-SF, there was no difference between groups.
Conclusions
Although more effective than usual care in improving mood states and agitation, PARO was only more effective than a plush toy in encouraging engagement."
}
@article{UZUN20141,
title = "A preliminary examination technique for audio evidence to distinguish speech from non-speech using objective speech quality measures",
journal = "Speech Communication",
volume = "61-62",
pages = "1 - 16",
year = "2014",
issn = "0167-6393",
doi = "https://doi.org/10.1016/j.specom.2014.03.003",
url = "http://www.sciencedirect.com/science/article/pii/S016763931400017X",
author = "Erkam Uzun and Husrev T. Sencar",
keywords = "Preliminary analysis of audio evidence, Speech and non-speech discrimination, Objective speech quality assessment, Audio encoding, Audio effects, Surveillance",
abstract = "Forensic practitioners are faced more and more with large volumes of data. Therefore, there is a growing need for computational techniques to aid in evidence collection and analysis. With this study, we introduce a technique for preliminary analysis of audio evidence to discriminate between speech and non-speech. The novelty of our approach lies in the use of well-established speech quality measures for characterizing speech signals. These measures rely on models of human perception of speech to provide objective and reliable measurements of changes in characteristics that influence speech quality. We utilize this capability to compute quality scores between an audio and its noise-suppressed version and to model variations of these scores in speech as compared to those in non-speech audio. Tests performed on 11 datasets with widely varying characteristics show that the technique has a high discrimination capability, achieving an identification accuracy of 96 to 99% in most test cases, and offers good generalization properties across different datasets. Results also reveal that the technique is robust against encoding at low bit-rates, application of audio effects and degradations due to varying degrees of background noise. Performance comparisons made with existing studies show that the proposed method improves the state-of-the-art in audio content identification."
}
@article{PRICE2015131,
title = "The role of iPads in pre-school children's mark making development",
journal = "Computers & Education",
volume = "87",
pages = "131 - 141",
year = "2015",
issn = "0360-1315",
doi = "https://doi.org/10.1016/j.compedu.2015.04.003",
url = "http://www.sciencedirect.com/science/article/pii/S0360131515001025",
author = "Sara Price and Carey Jewitt and Lucrezia Crescenzi",
keywords = "iPads, Mark making, Finger painting, Pre-school, Children",
abstract = "The increased acquisition of touch-screen technologies, such as tablet computers, in both homes and schools raises important questions about their role for very young children's learning and development. Their inherent touch-based interaction offers new opportunities for mark making practices, which are linked to literacy development, through the emergent process of using marks as symbolic representation. This paper reports a comparative study of touch-based interaction using a tablet computer versus traditional physical paint and paper. Children aged 2–3 years engaged in a free finger painting activity and colouring in activity in both paper and digital environments. Video data of their interactions was used to develop a coding scheme for analyzing touch-based interaction, providing insight into how the use of fingers and hands differed in each environment, the different types and qualities of touch that were engendered, and the composition of the final paintings produced. Findings show that while the tablet computer limited the number of fingers used for interaction, its material affordances supported speed and continuity, which led to more mark making, and different ‘scales’ of mark making extending the range of mark making practices. At the same time it limited the sensory experience of physical paint and resulted in more uniform final compositions. The findings are discussed in terms of shaping young children's mark making, the implications of the use of touch screen technologies in literacy development for educational practitioners and technology design, and key future research directions."
}
@article{HOFFMANN20142138,
title = "kmos: A lattice kinetic Monte Carlo framework",
journal = "Computer Physics Communications",
volume = "185",
number = "7",
pages = "2138 - 2150",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2014.04.003",
url = "http://www.sciencedirect.com/science/article/pii/S001046551400126X",
author = "Max J. Hoffmann and Sebastian Matera and Karsten Reuter",
keywords = "Lattice kinetic Monte Carlo, Microkinetic modeling, First-principles multi-scale modeling, Heterogeneous catalysis, Graphical user interface, Python, Fortran90, Open source",
abstract = "Kinetic Monte Carlo (kMC) simulations have emerged as a key tool for microkinetic modeling in heterogeneous catalysis and other materials applications. Systems, where site-specificity of all elementary reactions allows a mapping onto a lattice of discrete active sites, can be addressed within the particularly efficient lattice kMC approach. To this end we describe the versatile kmos software package, which offers a most user-friendly implementation, execution, and evaluation of lattice kMC models of arbitrary complexity in one- to three-dimensional lattice systems, involving multiple active sites in periodic or aperiodic arrangements, as well as site-resolved pairwise and higher-order lateral interactions. Conceptually, kmos achieves a maximum runtime performance which is essentially independent of lattice size by generating code for the efficiency-determining local update of available events that is optimized for a defined kMC model. For this model definition and the control of all runtime and evaluation aspects kmos offers a high-level application programming interface. Usage proceeds interactively, via scripts, or a graphical user interface, which visualizes the model geometry, the lattice occupations and rates of selected elementary reactions, while allowing on-the-fly changes of simulation parameters. We demonstrate the performance and scaling of kmos with the application to kMC models for surface catalytic processes, where for given operation conditions (temperature and partial pressures of all reactants) central simulation outcomes are catalytic activity and selectivities, surface composition, and mechanistic insight into the occurrence of individual elementary processes in the reaction network.
Program summary
Program title: kmos Catalogue identifier: AESU_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AESU_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 27450 No. of bytes in distributed program, including test data, etc.: 2777387 Distribution format: tar.gz Programming language: Python 16.4%, fortran90: 83.6%. Computer: PC, Mac. Operating system: Linux, Mac, Windows. RAM: 100 MB+ Classification: 7.8. External routines: ASE, Numpy, f2py, python-lxml Nature of problem: Microkinetic simulations of complex reaction networks with all elementary processes occurring at active sites of a static lattice. Solution method: Efficient lattice kinetic Monte Carlo solution of the Markovian master equation underlying the reaction network. Unusual features: The framework implements a Fortran90 code generator Running time: From 10 s to 10 h"
}
@article{BENSMAINE2013519,
title = "A non-dominated sorting genetic algorithm based approach for optimal machines selection in reconfigurable manufacturing environment",
journal = "Computers & Industrial Engineering",
volume = "66",
number = "3",
pages = "519 - 524",
year = "2013",
note = "Special Issue: The International Conferences on Computers and Industrial Engineering (ICC&IEs) - series 41",
issn = "0360-8352",
doi = "https://doi.org/10.1016/j.cie.2012.09.008",
url = "http://www.sciencedirect.com/science/article/pii/S0360835212002240",
author = "Abderrahmane Bensmaine and Mohammed Dahane and Lyes Benyoucef",
keywords = "Reconfigurable manufacturing systems, RMS design, Machines selection, Multi-objective metaheuristic, NSGA-II",
abstract = "This paper deals with a problem of reconfigurable manufacturing systems (RMSs) design based on products specifications and reconfigurable machines capabilities. A reconfigurable manufacturing environment includes machines, tools, system layout, etc. Moreover, the machine can be reconfigured to meet the changing needs in terms of capacity and functionality, which means that the same machine can be modified in order to perform different tasks depending on the offered axes of motion in each configuration and the availability of tools. This problem is related to the selection of candidate reconfigurable machines among an available set, which will be then used to carry out a certain product based on the product characteristics. The selection of the machines considers two main objectives respectively the minimization of the total cost (production cost, reconfiguration cost, tool changing cost and tool using cost) and the total completion time. An adapted version of the non- dominated sorting genetic algorithm (NSGA-II) is proposed to solve the problem. To demonstrate the effectiveness of the proposed approach on RMS design problem, a numerical example is presented and the obtained results are discussed with suggested future research."
}
@article{CHRISTIANSEN2013652,
title = "Environmental Factors That Impact the Eating Behaviors of Low-income African American Adolescents in Baltimore City",
journal = "Journal of Nutrition Education and Behavior",
volume = "45",
number = "6",
pages = "652 - 660",
year = "2013",
issn = "1499-4046",
doi = "https://doi.org/10.1016/j.jneb.2013.05.009",
url = "http://www.sciencedirect.com/science/article/pii/S1499404613004843",
author = "Karina M.H. Christiansen and Farah Qureshi and Alex Schaible and Sohyun Park and Joel Gittelsohn",
keywords = "eating behavior, environment, African American, child, qualitative research, overweight",
abstract = "Objective
To understand environmental factors influencing the food-related habits of low-income urban African American adolescents.
Design
Qualitative research was conducted between February and April, 2010, using in-depth interviews, focus groups, and direct observation.
Setting
The study was conducted in low-income, predominantly African American neighborhoods of Baltimore City.
Participants
A total of 20 adolescents were interviewed in 18 in-depth interviews (n = 13) and 2 focus groups (n = 7). Participants were recruited from Baltimore City recreation centers and were eligible if they were African American and aged 10–16 years.
Phenomenon of Interest
The food-related habits of low-income, African American, urban adolescents and reported perceptions of their food environments.
Analysis
Interviews were audio recorded, transcribed, coded, and analyzed for emerging themes.
Results
Six thematic categories emerged and were organized into 4 environmental contexts: the neighborhood context (accessibility of food and safety of neighborhood), the school context (school food environment), the family context (family health history, role modeling, and monitoring) and the peer context (peer behaviors).
Conclusions and Implications
Future efforts to reduce the obesity epidemic among low-income African American adolescents should address the social environment of the family; however, positive behavior change may not be sustainable without neighborhood or school food environment modifications."
}
@article{BLAIMER2012421,
title = "Acrobat ants go global – Origin, evolution and systematics of the genus Crematogaster (Hymenoptera: Formicidae)",
journal = "Molecular Phylogenetics and Evolution",
volume = "65",
number = "2",
pages = "421 - 436",
year = "2012",
issn = "1055-7903",
doi = "https://doi.org/10.1016/j.ympev.2012.06.028",
url = "http://www.sciencedirect.com/science/article/pii/S1055790312002540",
author = "Bonnie B. Blaimer",
keywords = ", Ants, South-East Asia, Madagascar, Transoceanic dispersal, Biogeography",
abstract = "This study unravels the evolution and biogeographic history of the globally distributed ant genus Crematogaster on the basis of a molecular phylogeny, reconstructed from five nuclear protein-coding genes and a total of 3384bp of sequence data. A particular emphasis is placed on the evolutionary history of these ants in the Malagasy region. Bayesian and likelihood analyses performed on a dataset of 124 Crematogaster ingroup taxa lend strong support for three deeply diverging phylogenetic lineages within the genus: the Orthocrema clade, the Global Crematogaster clade and the Australo-Asian Crematogaster clade. The 15 previous subgenera within Crematogaster are mostly not monophyletic. Divergence dating analyses and ancestral range reconstructions suggest that Crematogaster evolved in South-East Asia in the mid-Eocene (40–45ma). The three major lineages also originated in this region in the late Oligocene/early Miocene (∼24–30ma). A first dispersal out of S-E Asia by an Orthocrema lineage is supported for 22–30ma to the Afrotropical region. Successive dispersal events out of S-E Asia began in the early, and continued throughout the late Miocene. The global distribution of Crematogaster was achieved by subsequent colonizations of all major biogeographic regions by the Orthocrema and the Global Crematogaster clade. Molecular dating estimates and ancestral range evolution are discussed in the light of palaeogeographic changes in the S-E Asian region and an evolving ocean circulation system throughout the Eocene, Oligocene and Miocene. Eight dispersal events to/from Madagascar by Crematogaster are supported, with most events occurring in the late Miocene to Pliocene (5.0–9.5ma). These results suggest that Crematogaster ants possess exceptional dispersal and colonization abilities, and emphasize the need for detailed investigations of traits that have contributed to the global evolutionary success of these ants."
}
@article{MALDONADO2016400,
title = "Mesenteric vein thrombosis can be safely treated with anticoagulation but is associated with significant sequelae of portal hypertension",
journal = "Journal of Vascular Surgery: Venous and Lymphatic Disorders",
volume = "4",
number = "4",
pages = "400 - 406",
year = "2016",
issn = "2213-333X",
doi = "https://doi.org/10.1016/j.jvsv.2016.05.003",
url = "http://www.sciencedirect.com/science/article/pii/S2213333X16300695",
author = "Thomas S. Maldonado and Sheila N. Blumberg and Sharvil U. Sheth and Gabriel Perreault and Mikel Sadek and Todd Berland and Mark A. Adelman and Caron B. Rockman",
abstract = "Background
Mesenteric venous thrombosis (MVT) is a relatively uncommon but potentially lethal condition associated with bowel ischemia and infarction. The natural history and long-term outcomes are poorly understood and under-reported.
Methods
A single-institution retrospective review of noncirrhotic patients diagnosed with MVT from 1999 to 2015 was performed using International Classification of Diseases, Ninth Revision and radiology codes. Patients were excluded if no radiographic imaging was available for review. Eighty patients were identified for analysis. Demographic, clinical, and radiographic data on presentation and at long-term follow-up were collected. Long-term sequelae of portal venous hypertension were defined as esophageal varices, portal vein cavernous transformation, splenomegaly, or hepatic atrophy, as seen on follow-up imaging.
Results
There were 80 patients (57.5% male; mean age, 57.9 ± 15.6 years) identified; 83.3% were symptomatic, and 80% presented with abdominal pain. Median follow-up was 480 days (range, 1-6183 days). Follow-up radiographic and clinical data were available for 50 patients (62.5%). The underlying causes of MVT included cancer (41.5%), an inflammatory process (25.9%), the postoperative state (20.7%), and idiopathic cases (18.8%). Pancreatic cancer was the most common associated malignant neoplasm (53%), followed by colon cancer (15%). Twenty patients (26%) had prior or concurrent lower extremity deep venous thromboses. Most patients (68.4%) were treated with anticoagulation; the rest were treated expectantly. Ten (12.5%) had bleeding complications related to anticoagulation, including one death from intracranial hemorrhage. Four patients underwent intervention (three pharmacomechanical thrombolysis and one thrombectomy). One patient died of intestinal ischemia. Two patients had recurrent MVT, both on discontinuing anticoagulation. Long-term imaging sequelae of portal hypertension were noted in 25 of 50 patients (50%) who had follow-up imaging available. Patients with long-term sequelae had lower recanalization rates (36.8% vs 65%; P = .079) and significantly higher rates of complete as opposed to partial thrombosis at the initial event (73% vs 43.3%; P < .005). Long-term sequelae were unrelated to the initial cause or treatment with anticoagulation (P = NS).
Conclusions
Most cases of MVT are associated with malignant disease or an inflammatory process, such as pancreatitis. A diagnosis of malignant disease in the setting of MVT has poor prognosis, with a 5-year survival of only 25%. MVT can be effectively treated with anticoagulation in the majority of cases. Operative or endovascular intervention is rarely needed but important to consider in patients with signs of severe ischemia or impending bowel infarction. There is a significant incidence of radiographically noted long-term sequelae from MVT related to portal venous hypertension, especially in cases of initial complete thrombosis of the mesenteric vein."
}