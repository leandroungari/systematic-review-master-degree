@article{JONSSON20132197,
title = "New version: Grasp2K relativistic atomic structure package",
journal = "Computer Physics Communications",
volume = "184",
number = "9",
pages = "2197 - 2203",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.02.016",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513000738",
author = "P. Jönsson and G. Gaigalas and J. Bieroń and C. Froese Fischer and I.P. Grant",
keywords = "Relativistic atomic structure calculations, Multi-configurational wavefunctions, Energy levels, Transition probabilities, Quantum electrodynamic corrections, Zeeman effect, Hyperfine structure, Isotope shift",
abstract = "A revised version of Grasp2K [P. Jönsson, X. He, C. Froese Fischer, I.P. Grant, Comput. Phys. Commun. 177 (2007) 597] is presented. It supports earlier non-block and block versions of codes as well as a new block version in which the njgraf library module [A. Bar-Shalom, M. Klapisch, Comput. Phys. Commun. 50 (1988) 375] has been replaced by the librang angular package developed by Gaigalas based on the theory of [G. Gaigalas, Z.B. Rudzikas, C. Froese Fischer, J. Phys. B: At. Mol. Phys. 30 (1997) 3747, G. Gaigalas, S. Fritzsche, I.P. Grant, Comput. Phys. Commun. 139 (2001) 263]. Tests have shown that errors encountered by njgraf do not occur with the new angular package. The three versions are denoted v1, v2, and v3, respectively. In addition, in v3, the coefficients of fractional parentage have been extended to j=9/2, making calculations feasible for the lanthanides and actinides. Changes in v2 include minor improvements. For example, the new version of rci2 may be used to compute quantum electrodynamic (QED) corrections only from selected orbitals. In v3, a new program, jj2lsj, reports the percentage composition of the wave function in LSJ and the program rlevels has been modified to report the configuration state function (CSF) with the largest coefficient of an LSJ expansion. The bioscl2 and bioscl3 application programs have been modified to produce a file of transition data with one record for each transition in the same format as in Atsp2K [C. Froese Fischer, G. Tachiev, G. Gaigalas, M.R. Godefroid, Comput. Phys. Commun. 176 (2007) 559], which identifies each atomic state by the total energy and a label for the CSF with the largest expansion coefficient in LSJ intermediate coupling. All versions of the codes have been adapted for 64-bit computer architecture.
Program Summary
Program title: Grasp2K, version 1_1 Catalogue identifier: ADZL_v1_1 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ADZL_v1_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 730252 No. of bytes in distributed program, including test data, etc.: 14808872 Distribution format: tar.gz Programming language: Fortran. Computer: Intel Xeon, 2.66 GHz. Operating system: Suse, Ubuntu, and Debian Linux 64-bit. RAM: 500 MB or more Classification: 2.1. Catalogue identifier of previous version: ADZL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 177 (2007) 597 Does the new version supersede the previous version?: Yes Nature of problem: Prediction of atomic properties — atomic energy levels, oscillator strengths, radiative decay rates, hyperfine structure parameters, Landé gJ-factors, and specific mass shift parameters — using a multiconfiguration Dirac–Hartree–Fock approach. Solution method: The computational method is the same as in the previous Grasp2K [1] version except that for v3 codes the njgraf library module [2] for recoupling has been replaced by librang [3,4]. Reasons for new version: New angular libraries with improved performance are available. Also methodology for transforming from jj- to LSJ-coupling has been developed. Summary of revisions: New angular libraries where the coefficients of fractional parentage have been extended to j=9/2, making calculations feasible for the lanthanides and actinides. Inclusion of a new program jj2lsj, which reports the percentage composition of the wave function in LSJ. Transition programs have been modified to produce a file of transition data with one record for each transition in the same format as Atsp2K [C. Froese Fischer, G. Tachiev, G. Gaigalas and M.R. Godefroid, Comput. Phys. Commun. 176 (2007) 559], which identifies each atomic state by the total energy and a label for the CSF with the largest expansion coefficient in LSJ intermediate coupling. Updated to 64-bit architecture. A comprehensive user manual in pdf format for the program package has been added. Restrictions: The packing algorithm restricts the maximum number of orbitals to be ≤214. The tables of reduced coefficients of fractional parentage used in this version are limited to subshells with j≤9/2 [5]; occupied subshells with j>9/2 are, therefore, restricted to a maximum of two electrons. Some other parameters, such as the maximum number of subshells of a CSF outside a common set of closed shells are determined by a parameter.def file that can be modified prior to compile time. Unusual features: The bioscl3 program reports transition data in the same format as in Atsp2K [6], and the data processing program tables of the latter package can be used. The tables program takes a name.lsj file, usually a concatenated file of all the .lsj transition files for a given atom or ion, and finds the energy structure of the levels and the multiplet transition arrays. The tables posted at the website http://atoms.vuse.vanderbilt.edu are examples of tables produced by the tables program. With the extension of coefficients of fractional parentage to j=9/2, calculations for the lanthanides and actinides become possible. Running time: CPU time required to execute test cases: 70.5 s. References: [1]P. Jönsson, X. He, C. Froese Fischer, I.P. Grant, Comput. Phys. Commun. 177 (2007) 597.[2]A. Bar-Shalom, M. Klapisch, Comput. Phys. Commun. 50 (1988) 375.[3]G. Gaigalas, Z.B. Rudzikas, C. Froese Fischer, J. Phys. B: At. Mol. Phys. 30 (1997) 3747.[4]G. Gaigalas, S. Fritzsche, I.P. Grant, Comput. Phys. Commun. 139 (2001) 263.[5]G. Gaigalas, S. Fritzsche, Z. Rudzikas, At. Data Nucl. Data Tables 76 (2000) 235.[6]C. Froese Fischer, G. Tachiev, G. Gaigalas, M.R. Godefroid, Comput. Phys. Commun. 176 (2007) 559."
}
@article{GALINOS2016319,
title = "Vertical Axis Wind Turbine Design Load Cases Investigation and Comparison with Horizontal Axis Wind Turbine",
journal = "Energy Procedia",
volume = "94",
pages = "319 - 328",
year = "2016",
note = "13th Deep Sea Offshore Wind R&D Conference, EERA DeepWind'2016",
issn = "1876-6102",
doi = "https://doi.org/10.1016/j.egypro.2016.09.190",
url = "http://www.sciencedirect.com/science/article/pii/S1876610216308426",
author = "Christos Galinos and Torben J. Larsen and Helge A. Madsen and Uwe S. Paulsen",
keywords = "VAWT, Darrieus, DLC, HAWT, onshore wind turbine",
abstract = "The paper studies the applicability of the IEC 61400-1 ed.3, 2005 International Standard of wind turbine minimum design requirements in the case of an onshore Darrieus VAWT and compares the results of basic Design Load Cases (DLCs) with those of a 3-bladed HAWT. The study is based on aeroelastic computations using the HAWC2 aero-servo-elastic code A 2-bladed 5 MW VAWT rotor is used based on a modified version of the DeepWind rotor For the HAWT simulations the NREL 3-bladed 5 MW reference wind turbine model is utilized Various DLCs are examined including normal power production, emergency shut down and parked situations, from cut-in to cut-out and extreme wind conditions. The ultimate and 1 Hz equivalent fatigue loads of the blade root and turbine base bottom are extracted and compared in order to give an insight of the load levels between the two concepts. According to the analysis the IEC 61400-1 ed.3 can be used to a large extent with proper interpretation of the DLCs and choice of parameters such as the hub-height. In addition, the design drivers for the VAWT appear to differ from the ones of the HAWT. Normal operation results in the highest tower bottom and blade root loads for the VAWT, where parked under storm situation (DLC 6.2) and extreme operating gust (DLC 2.3) are more severe for the HAWT. Turbine base bottom and blade root edgewise fatigue loads are much higher for the VAWT compared to the HAWT. The interpretation and simulation of DLC 6.2 for the VAWT lead to blade instabilities, while extreme wind shear and extreme wind direction change are not critical in terms of loading of the VAWT structure. Finally, the extreme operating gust wind condition simulations revealed that the emerging loads depend on the combination of the rotor orientation and the time stamp that the frontal passage of gust goes through the rotor plane."
}
@article{LU201588,
title = "Experimental investigation of heat conduction performance and development of automatic temperature measurement device on modular artillery charge system",
journal = "Measurement",
volume = "59",
pages = "88 - 95",
year = "2015",
issn = "0263-2241",
doi = "https://doi.org/10.1016/j.measurement.2014.09.043",
url = "http://www.sciencedirect.com/science/article/pii/S0263224114004230",
author = "Xin Lu and Yanhuang Zhou and Jincao Chen and Dianjin Liu",
keywords = "Automatic temperature measurement device, Modular artillery charge system, Heat conduction performance, Similarity theory",
abstract = "The heat conduction performance experiment is conducted on the modular artillery charge system (MACS). Utilizing the experimental measurement system, the change history of the modular charge temperature is obtained. On the basis of the heat conduction performance experiment of modular propellant charge, an unsteady-state heat conduction model describing the temperature change of the MACS is built and the finite-difference implicit schemes are theoretically deduced using the volume equilibrium method for numerical simulation. The validation of the numerical model is checked through compared with the experimental results. An automatic online temperature measurement device on the MACS is developed, based on the non-contact measurement method which is proposed in this paper. As a part of the device, an initial charge temperature sensor (ICTS) is also developed according to the similarity principle. The temperature measurement device where the numerical codes are embedded is assembled in the turret to calculation successively the temperature change of the modular charge with the environment temperature of ammunition rack. Meanwhile, for trajectory calculation and firing data correction, the temperature information obtained from the device may be simultaneously transferred to the gunner task terminal computer through a Controller Area Network (CAN) bus interface."
}
@article{MAITY20121056,
title = "An Adaptive SIC Technique in DS-CDMA using Neural Network",
journal = "Procedia Engineering",
volume = "30",
pages = "1056 - 1063",
year = "2012",
note = "International Conference on Communication Technology and System Design 2011",
issn = "1877-7058",
doi = "https://doi.org/10.1016/j.proeng.2012.01.963",
url = "http://www.sciencedirect.com/science/article/pii/S1877705812009733",
author = "Santi P. Maity and Sumanta Hati",
keywords = "SIC, Neural Network, DS-CDMA, JPOE",
abstract = "In this paper, we propose an efficient successive interference cancellation (SIC) method in direct sequence code division multiple access (DS-CDMA) system using neural network (NN). Neural network is used here to estimate the amplitudes of different users signals under frequency selective Rayleigh fading channel. The correlation values between the received signal and the signature /spreading waveforms for different users are given as the inputs to a NN and the output acts as an estimation of corresponding user's signal amplitude. A closed mathematical form of joint probability of error (JPOE) is developed to determine the number of active users needs to be canceled to achieve a desired bit error rate (BER) value. Simulation results strongly support the mathematical results. Mathematical analysis shows that better performance results can be achieved through large change in weight up-gradation (w) for the strong users with a particular change in learning rate (η). Performance of the SIC system has been studied for initial wrong ordering of a couple of pairs of interfering users based on the correlation values. Simulation results show that BER performance is better when users are ordered based on signal to interference ratio (SIR) values rather than correlation values."
}
@article{COTEARSENAULT2016100,
title = "“Have no regrets:” Parents' experiences and developmental tasks in pregnancy with a lethal fetal diagnosis",
journal = "Social Science & Medicine",
volume = "154",
pages = "100 - 109",
year = "2016",
issn = "0277-9536",
doi = "https://doi.org/10.1016/j.socscimed.2016.02.033",
url = "http://www.sciencedirect.com/science/article/pii/S0277953616300880",
author = "Denise Côté-Arsenault and Erin Denney-Koelsch",
keywords = "USA, Phenomenology, Longitudinal, Prenatal diagnosis, Pregnancy, Developmental task, Lethal fetal diagnosis, Perinatal palliative care",
abstract = "Significance
Lethal fetal diagnoses are made in 2% of all pregnancies. The pregnancy experience is certainly changed for the parents who choose to continue the pregnancy with a known fetal diagnosis but little is known about how the psychological and developmental processes are altered.
Methods
This longitudinal phenomenological study of 16 mothers and 14 fathers/partners sought to learn the experiences and developmental needs of parents who continue their pregnancy despite the lethal diagnosis. The study was guided by Merleau-Ponty's philosophic view of embodiment. Interviews (N = 90) were conducted with mothers and fathers over time, from mid-pregnancy until 2–3 months post birth. Data analysis was iterative, through a minimum of two cycles of coding, theme identification, within- and cross-case analysis, and the writing of results.
Results
Despite individual differences, parents were quite consistent in sharing that their overall goal was to “Have no regrets” when all was said and done. Five stages of pregnancy were identified: Pre-diagnosis, Learning Diagnosis, Living with Diagnosis, Birth & Death, and Post Death. Developmental tasks of pregnancy that emerged were 1) Navigating Relationships, 2) Comprehending Implication of the Condition, 3) Revising Goals of Pregnancy, 4) Making the Most of Time with Baby, 5) Preparing for Birth and Inevitable Death, 6) Advocating for Baby with Integrity, and 7) Adjusting to Life in Absence of Baby. Prognostic certainty was found to be highly influential in parents' progression through developmental tasks.
Conclusion
The framework of parents' pregnancy experiences with lethal fetal diagnosis that emerged can serve as a useful guide for providers who care for families, especially in perinatal palliative care. Providing patient-centered care that is matched to the stage and developmental tasks of these families may lead to improved care and greater parent satisfaction."
}
@article{BIAGIOLI2013149,
title = "Surgery Clerkship Evaluations Drive Improved Professionalism",
journal = "Journal of Surgical Education",
volume = "70",
number = "1",
pages = "149 - 155",
year = "2013",
issn = "1931-7204",
doi = "https://doi.org/10.1016/j.jsurg.2012.06.020",
url = "http://www.sciencedirect.com/science/article/pii/S1931720412001730",
author = "Frances E. Biagioli and Rebecca E. Rdesinski and Diane L. Elliot and Kathryn G. Chappelle and Karen L. Kwong and William L. Toffler",
keywords = "codes of professional ethics, clinical clerkship, education, faculty, medical, surveys, Professionalism, Interpersonal and Communication Skills, Practice-Based Learning and Improvement",
abstract = "Purpose
To determine whether a brief student survey can differentiate among third-year clerkship student's professionalism experiences and whether sharing specific feedback with surgery faculty and residents can lead to improvements.
Methods
Medical students completed a survey on professionalism at the conclusion of each third-year clerkship specialty rotation during academic years 2007-2010.
Results
Comparisons of survey items in 2007-2008 revealed significantly lower ratings for the surgery clerkship on both Excellence (F = 10.75, p < 0.001) and Altruism/Respect (F = 15.59, p < 0.001) subscales. These data were shared with clerkship directors, prompting the surgery department to discuss student perceptions of professionalism with faculty and residents. Postmeeting ratings of surgery professionalism significantly improved on both Excellence and Altruism/Respect dimensions (p < 0.005 for each).
Conclusions
A brief survey can be used to measure student perceptions of professionalism and an intervention as simple as a surgery department openly sharing results and communicating expectations appears to drive positive change in student experiences."
}
@article{SHARIFI2014646,
title = "Exploring Innovative Approaches and Patient-Centered Outcomes From Positive Outliers in Childhood Obesity",
journal = "Academic Pediatrics",
volume = "14",
number = "6",
pages = "646 - 655",
year = "2014",
issn = "1876-2859",
doi = "https://doi.org/10.1016/j.acap.2014.08.001",
url = "http://www.sciencedirect.com/science/article/pii/S1876285914002782",
author = "Mona Sharifi and Gareth Marshall and Roberta Goldman and Sheryl L. Rifas-Shiman and Christine M. Horan and Renata Koziol and Richard Marshall and Thomas D. Sequist and Elsie M. Taveras",
keywords = "attitude to health, obesity, overweight, parents, positive deviance, qualitative",
abstract = "Objective
New approaches for obesity prevention and management can be gleaned from positive outliers—that is, individuals who have succeeded in changing health behaviors and reducing their body mass index (BMI) in the context of adverse built and social environments. We explored perspectives and strategies of parents of positive outlier children living in high-risk neighborhoods.
Methods
We collected up to 5 years of height/weight data from the electronic health records of 22,443 Massachusetts children, ages 6 to 12 years, seen for well-child care. We identified children with any history of BMI in the 95th percentile or higher (n = 4007) and generated a BMI z-score slope for each child using a linear mixed effects model. We recruited parents for focus groups from the subsample of children with negative slopes who also lived in zip codes where >15% of children were obese. We analyzed focus group transcripts using an immersion/crystallization approach.
Results
We reached thematic saturation after 5 focus groups with 41 parents. Commonly cited outcomes that mattered most to parents and motivated change were child inactivity, above-average clothing sizes, exercise intolerance, and negative peer interactions; few reported BMI as a motivator. Convergent strategies among positive outlier families were family-level changes, parent modeling, consistency, household rules/limits, and creativity in overcoming resistance. Parents voiced preferences for obesity interventions that include tailored education and support that extend outside clinical settings and are delivered by both health care professionals and successful peers.
Conclusions
Successful strategies learned from positive outlier families can be generalized and tested to accelerate progress in reducing childhood obesity."
}
@article{COOPER201374,
title = "Event-related potentials reveal modelling of auditory repetition in the brain",
journal = "International Journal of Psychophysiology",
volume = "88",
number = "1",
pages = "74 - 81",
year = "2013",
issn = "0167-8760",
doi = "https://doi.org/10.1016/j.ijpsycho.2013.02.003",
url = "http://www.sciencedirect.com/science/article/pii/S0167876013000354",
author = "Rowena Jane Cooper and Rebbekah Josephine Atkinson and Rosemary Ann Clark and Patricia Therese Michie",
keywords = "Auditory event-related potential (ERP), Repetition positivity (RP), Mismatch negativity (MMN), Repetition, Auditory sensory memory, Predictive coding",
abstract = "Two auditory event-related potential (ERP) waveforms, mismatch negativity (MMN) and repetition positivity (RP), are sensitive to repetition of auditory stimuli. Increasing repetition of standards produces larger MMN amplitudes to deviant stimuli in an oddball paradigm, known as the memory trace effect, and attributed to increasing strength of the memory trace for standards. RP to standards also increases as a function of repetition in a ‘roving’ oddball paradigm where the standard changes in pitch following presentation of a deviant tone. As the sensory memory trace representing standard stimuli must be continually updated in the roving paradigm, RP has been proposed to reflect memory trace formation. Given that RP to date has only been observed in roving oddball paradigms, we examined whether RP and the MMN memory trace effect are present in both roving and standard oddball paradigms in 24 young adults (mean age: 22.4±5years). Four, 8, or 16 standards preceded a deviant. We observed RP at Fz in standard ERPs in the roving but not constant paradigm. At mastoid sites, RP was observed in both paradigms. A memory trace effect was not observed at Fz in either paradigm. Our findings suggest that different generator sites in the brain model local and global auditory information with generators of mastoid activity primarily sensitive to local or short term stimulus history of auditory regularities while generators of frontal site activity retain more global information regarding stimulus history over a longer time period."
}
@article{PALANISWAMY20142056,
title = "Catheter ablation of postinfarction ventricular tachycardia: Ten-year trends in utilization, in-hospital complications, and in-hospital mortality in the United States",
journal = "Heart Rhythm",
volume = "11",
number = "11",
pages = "2056 - 2063",
year = "2014",
issn = "1547-5271",
doi = "https://doi.org/10.1016/j.hrthm.2014.07.012",
url = "http://www.sciencedirect.com/science/article/pii/S1547527114007541",
author = "Chandrasekar Palaniswamy and Dhaval Kolte and Prakash Harikrishnan and Sahil Khera and Wilbert S. Aronow and Marjan Mujib and William Michael Mellana and Paul Eugenio and Seth Lessner and Aileen Ferrick and Gregg C. Fonarow and Ali Ahmed and Howard A. Cooper and William H. Frishman and Julio A. Panza and Sei Iwai",
keywords = "Catheter ablation, Outcomes, Registry, Trends, Ventricular tachycardia",
abstract = "Background
There is a paucity of data regarding the complications and in-hospital mortality after catheter ablation for ventricular tachycardia (VT) in patients with ischemic heart disease.
Objective
The purpose of this study was to determine the temporal trends in utilization, in-hospital mortality, and complications of catheter ablation of postinfarction VT in the United States.
Methods
We used the 2002–2011 Nationwide Inpatient Sample (NIS) database to identify all patients ≥18 years of age with a primary diagnosis of VT (International Classification of Diseases, Ninth Edition, Clinical Modification [ICD-9-CM] code 427.1) and who also had a secondary diagnosis of prior history of myocardial infarction (ICD-9-CM 412). Patients with supraventricular arrhythmias were excluded. Patients who underwent catheter ablation were identified using ICD-9-CM procedure code 37.34. Temporal trends in catheter ablation, in-hospital complications, and in-hospital mortality were analyzed.
Results
Of 81,539 patients with postinfarct VT, 4653 (5.7%) underwent catheter ablation. Utilization of catheter ablation increased significantly from 2.8% in 2002 to 10.8% in 2011 (Ptrend < .001). The overall rate of any in-hospital complication was 11.2% (523/4653), with vascular complications in 6.9%, cardiac in 4.3%, and neurologic in 0.5%. In-hospital mortality was 1.6% (75/4653). From 2002 to 2011, there was no significant change in the overall complication rates (8.4% to 10.2%, Ptrend = .101; adjusted odds ratio [per year] 1.02, 95% confidence interval 0.98–1.06) or in-hospital mortality (1.3% to 1.8%, Ptrend = .266; adjusted odds ratio [per year] 1.03, 95% confidence interval 0.92–1.15).
Conclusion
The utilization rate of catheter ablation as therapy for postinfarct VT has steadily increased over the past decade. However, procedural complication rates and in-hospital mortality have not changed significantly during this period."
}
@article{GILL2016467,
title = "Photovoice: A Strategy to Better Understand the Reproductive and Sexual Health Needs of Young Mothers",
journal = "Journal of Pediatric and Adolescent Gynecology",
volume = "29",
number = "5",
pages = "467 - 475",
year = "2016",
issn = "1083-3188",
doi = "https://doi.org/10.1016/j.jpag.2016.03.001",
url = "http://www.sciencedirect.com/science/article/pii/S108331881600187X",
author = "Roopan Gill and Amanda Black and Tania Dumont and Nathalie Fleming",
keywords = "Adolescent girls, Qualitative, Policy, Reproductive and sexual health, Social determinants of health, Photovoice",
abstract = "Study Objective
Adolescent women face significant sexual and reproductive health challenges and are more vulnerable than their male peers. Photovoice methodology might allow them to provide more meaningful and accurate representations of the health challenges they encounter. Our objectives were to: (1) use Photovoice to understand how young mothers frame reproductive and sexual health within the context of their lives; (2) explore how they define reproductive and sexual health; (3) identify youth perspectives on how their life situations influence their ability to affect their health; and (4) connect their perspectives to social determinants of health framework to facilitate implementation of effective programs and policies to address their needs.
Design, Setting, Participants, Interventions, and Main Outcome Measures
This was a prospective qualitative community-based participatory research study involving young women (ages 15-25 years) recruited from a local youth outreach center. A 9-step validated qualitative participatory approach that combined documentary photography with focus groups was used. Qualitative analysis was conducted with NVivo version 10 software (QSR International Inc., Burlington, MA, USA). Data were coded and themes were developed.
Results
Thirty women were recruited and nine women completed the study. Key themes included: personal sexual health practices and coping skills, influence of poverty, physical environments, community resources and sexual health services, education, and stigma of pregnancy. Participating in community-based participatory research empowered participants to advocate for their own health.
Conclusion
Photovoice methodology contributes to understanding complex factors influencing sexual and reproductive health of young mothers. This participatory-based methodology highlights their individual situations, allowing us to seek connections, create analytical perspectives from which to relate their situations to root causes, and consider strategies for change."
}
@article{ROSIEK2015208,
title = "SUSY FLAVOR v2.5: A computational tool for FCNC and CP-violating processes in the MSSM",
journal = "Computer Physics Communications",
volume = "188",
pages = "208 - 210",
year = "2015",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2014.10.003",
url = "http://www.sciencedirect.com/science/article/pii/S0010465514003385",
author = "J. Rosiek",
keywords = "Supersymmetry, K physics, B physics, Rare decays, CP-violation",
abstract = "We present SUSY_FLAVOR version 2.5—a program that calculates over 30 low-energy flavor observables in the general R-parity conserving MSSM. Compared to previous versions, in SUSY_FLAVOR v2.5 parameter initialization in SLHA2 formats has been significantly generalized, so that the program accepts most of the output files produced by other libraries analyzing the MSSM phenomenology. A number of bugs and inconsistencies have been fixed, based on users feedback. Calculations of several processes implemented in the earlier version have been corrected. New processes of rare decays of the top quark to Higgs boson have been included. Variables controlling contributions from various MSSM sectors have been added.
New Version Program Summary
Program title: SUSY FLAVOR v2.5 Catalogue identifier: AEGV_v2_5 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEGV_v2_5.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 22623 No. of bytes in distributed program, including test data, etc.: 614938 Distribution format: tar.gz Programming language: Fortran 77. Computer: Any. Operating system: Any, tested on Linux. Classification: 11.6. Catalogue identifier of previous version: AEGV_v2_0 Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 1004 Does the new version supersede the previous version?: Yes Nature of problem: Predicting CP-violating observables, meson mixing parameters and branching ratios for a set of rare processes in the general R-parity conserving MSSM. Solution method: We use standard quantum theoretical methods to calculate Wilson coefficients in MSSM at one loop and including QCD corrections at higher orders when this is necessary and possible. Reasons for new version: The input/output routines have been rewritten to make them more flexible and compatible with the SLHA2 standard [1]. Calculations of the several processes implemented in earlier SUSY_FLAVOR versions have been corrected. New observables have been added. A number of bugs have been corrected. Summary of revisions:1.Modified initialization routines. Currently the program should be able to read without modifications most of the SLHA2-compatible output files produced by other publicly available libraries calculating observables related to the MSSM phenomenology. In addition, new optional input block SFLAV_HADRON has been defined to facilitate modifications of the parameters related to the hadronic and QCD sector.The initialization sequence now goes through the following steps: •Before reading the file, all parameters are set to some initial values (which can be changed by editing the values given in the subroutine sflav_defaults in file sflav_io.f).•Subsequently, the user-defined data are read from the file with the default name susy_flavor.in. Data are grouped in Blocks following the SLHA2 specification or extensions described in [2]. Blocks are read in the following order: SOFTINP, SMINPUTS, VCKMIN, MINPAR (tanβ only, other entries are ignored), EXTPAR, IMEXTPAR, MSL2IN, IMMSL2IN, MSE2IN, IMMSE2IN, TEIN, IMTEIN, TEINH, IMTEINH, MSQ2IN, IMMSQ2IN, MSU2IN, IMMSU2IN, MSD2IN, IMMSD2IN, TUIN, IMTUIN, TUINH, IMTUINH, TDIN, IMTDIN, TDINH, IMTDINH, SFLAV_HADRON.•The presence of any Block is optional—if some Block is absent, the program falls back to the default parameter values. At least flavor-diagonal SUSY mass parameters have to be defined, otherwise the vanishing default values cause the program to crash.•If a parameter is multiply defined in several Blocks, the value from Block read as latest in the list above overwrites (without warning!) the values from preceding Blocks.•Blocks do not need to be complete and to contain all the entries described in the SLHA2 specification—it is sufficient to define a minimal set of the parameters relevant for a given problem, others are filled with the default values.•The “non-holomorphic” LR mixing terms are not included in the SLHA2 specification and by default are set to 0. They can be initialized to the non-trivial values in the blocks TXINH and IMTXINH (X=E,D,U)•The new input block SFLAV_HADRON allows the modification of the hadronic- and QCD-related quantities used by SUSY_FLAVOR. The structure of this block and the default values of the hadronic parameters are shown at the end of the sample input file susy_flavor.in attached to the SUSY_FLAVOR distribution.2.New control variables have been added, allowing the separate switching of contributions from various MSSM sectors on or off. They can be set by the following statement at the beginning of the driver program: call set_active_sector(ih,ic,in,ig) where the variables ih, ic, in, ig can take values 0 or 1 and they control, respectively, the inclusion in the total result of the diagrams with gauge and Higgs bosons, charginos, neutralinos and gluinos exchanged in the loops. Note that diagrams with Higgs and gauge bosons are always added together and currently cannot be disentangled, so setting ih=1, ic=in=ig=0 does not reproduce the SM result. By default, if no call to set_active_sector is made, all control variables are assumed to be equal to 1, so that all contributions are included.3.Added or modified processes: •The expressions used to calculate the neutron Electric Dipole Moment have been modified.•The branching ratios for the radiative decays of the heavy lepton into the lighter lepton and the photon, μ→eγ and τ→eγ, μγ, are now normalized to the total heavy lepton decay width (previously they were normalized to the decay width into leptonic channels).•The routines calculating branching ratios of B→τν and B→Dτν decays have been generalized to include more general structure of the effective Higgs boson–fermion couplings. In addition the routine calculating Br(B→D∗τν) has been added.•The routines for rare decays of the top quark to the CP-even Higgs boson and the light quarks, t→ch, uh, have been added, based on Ref. [3] (program can calculate also the decay rates of the top quark to the heavier CP-even Higgs boson H, assuming that such decays are allowed kinematically).•The routine calculating the approximate 2-loop estimate of the neutral CP-even Higgs mass mh has been added, based on Ref. [4]. Note that for the more precise calculations of this mass other publicly available SUSY codes should be used.•Default values of numerous quantities which are treated by SUSY_FLAVOR as the external parameters, mainly the values of hadronic parameters obtained from lattice calculations and results of experimental measurements, have been updated to accommodate the latest published results.4.SUSY_FLAVOR’s output is now written to the file named susy_flavor.out. It has ”SLHA-like” structure, i.e. it is divided into “data blocks”, however these blocks are SUSY_FLAVOR specific and do not follow the common SLHA2 standards. The output file contains the following data blocks: •SFLAV_CONTROL: control variables and error code status.•SFLAV_MASS: MSSM mass spectrum after mass matrix diagonalization.•SFLAV_CHIRAL_YUKAWA: relative size of the resummed chiral corrections to the Yukawa couplings.•SFLAV_CHIRAL_CKM: relative size of the resummed chiral corrections to the CKM matrix elements.•SFLAV_DELTA_F0: ΔF=0 observables: leptonic EDMs and g−2 anomalies, neutron EDM.•SFLAV_DELTA_F1: ΔF=1 observables: decay rates of l→l′γ, K→πν̄ν, B+→τ+ν, B→Dτν, B→D∗τν, B→Xsγ, Bd,s→li+lj−, t→uh, t→ch.•SFLAV_DELTA_F2: ΔF=2 observables: εK, ΔmK, ΔmD, ΔmBd, ΔmBs.Blocks SFLAV_CHIRAL_YUKAWA and SFLAV_CHIRAL_CKM show the relative differences of bare and physical Yukawa couplings and CKM matrix elements after the resummation of chiral corrections. If they are large, ≥O(1), the perturbation expansion is not reliable and the remaining program output may not be correct.5.The new integrated manual for SUSY_FLAVOR_v2.5 has been created, including the detailed description of the modifications listed above. It is attached to the SUSY_FLAVOR distribution. Regular code distribution updates and bug fixes (between the major revisions submitted to Computer Physics Communications) can be found on the program web page www.fuw.edu.pl/susy_flavor. Restrictions: The results apply only to the case of MSSM with R-parity conservation and without heavy right neutrino sector [5]. Additional comments: This program has been cataloged as AEGV_v2_5 to conform to the manuscript version number. There are no programs cataloged as, AEGV_v2_1, AEGV_v2_2, AEGV_v2_3, AEGV_v2_4, in the CPC Program Library. Running time: For a single parameter set, under 1s on a personal computer. References: [1] B. Allanach et al., Comput. Phys. Commun. 180 (2009) 8 [arXiv:0801.0045 [hep-ph]]. [2] J. Rosiek, P Chankowski, A. Dedes, S. Jager and P. Tanedo, Comput. Phys. Commun. 181 (2010) 2180 [arXiv:1003.4260 [hep-ph]]; A. Crivellin, J. Rosiek et. al., Comput. Phys. Commun. 184 (2013) 1004, [arXiv:1203.5023” [hep-ph]]. [3] A. Dedes, M. Paraskevas, J. Rosiek, K. Suxho and K. Tamvakis, arXiv:1409.6546 [hep-ph]. [4] S. Heinemeyer, W. Hollik and G. Weiglein, Phys. Lett. B455 (1999) 179–191 [hep-ph/9903404] [5] A. Dedes, H. Haber and J. Rosiek, JHEP 0711 (2007) 059, [arXiv:0707.3718 [hep-ph]]."
}
@article{TRIVEDI2016838,
title = "National Trends and Outcomes of Transjugular Intrahepatic Portosystemic Shunt Creation Using the Nationwide Inpatient Sample",
journal = "Journal of Vascular and Interventional Radiology",
volume = "27",
number = "6",
pages = "838 - 845",
year = "2016",
issn = "1051-0443",
doi = "https://doi.org/10.1016/j.jvir.2015.12.013",
url = "http://www.sciencedirect.com/science/article/pii/S105104431501249X",
author = "Premal S. Trivedi and Paul J. Rochon and Janette D. Durham and Robert K. Ryu",
abstract = "ABSTRACT
Purpose
To elucidate trends in transjugular intrahepatic portosystemic shunt (TIPS) use and outcomes over the course of a decade, including predictors of inpatient mortality and extended length of hospital stay.
Materials and Methods
The Nationwide Inpatient Sample was interrogated for the most recent 10 years available: 2003–2012. TIPS procedures and associated diagnoses were identified via International Classification of Diseases (version 9) codes, with the latter categorized into primary diagnoses in a hierarchy of disease severity. Linear regression analysis was used to determine trends of TIPS use and outcomes over time. Independent predictors of mortality and extended length of stay were determined by logistic regression.
Results
A total of 55,145 TIPS procedures were captured during the study period. Annual procedural volume did not change significantly (5,979 in 2003, 5,880 in 2012). The majority of TIPSs were created for ascites and/or varices (84%). Inpatient mortality (12.5% in 2003, 10.6% in 2012; P < .05) decreased but varied considerably by diagnosis (from 3.7% to 59.3%), with a disparity between bleeding and nonbleeding varices (18.7% vs 3.8%; P < .01). Multivariate predictors of mortality (P < .001 for all) included primary diagnoses (bleeding varices, hepatorenal and abdominal compartment syndromes), patient characteristics (age > 80 y, black race), and sequelae of advanced cirrhosis (comorbid hepatocellular carcinoma, spontaneous bacterial peritonitis, encephalopathy, and coagulopathy).
Conclusions
National TIPS inpatient mortality has decreased since 2003 while procedural volume has not changed. Postprocedural outcome is a function of patient demographic and socioeconomic factors and associated diagnoses. Independent predictors of poor outcome identified in this large national population study may aid clinicians in better assessing preprocedural risk."
}
@article{HALAWANI2016458,
title = "100 lines of code for shape-based object localization",
journal = "Pattern Recognition",
volume = "60",
pages = "458 - 472",
year = "2016",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2016.06.003",
url = "http://www.sciencedirect.com/science/article/pii/S0031320316301212",
author = "Alaa Halawani and Haibo Li",
keywords = "Object detection, Boundary localization, Deformable templates, Dynamic programming, Contour segments, One-pixel segment (OPS)",
abstract = "We introduce a simple and effective concept for localizing objects in densely cluttered edge images based on shape information. The shape information is characterized by a binary template of the object's contour, provided to search for object instances in the image. We adopt a segment-based search strategy, in which the template is divided into a set of segments. In this work, we propose our own segment representation that we call one-pixel segment (OPS), in which each pixel in the template is treated as a separate segment. This is done to achieve high flexibility that is required to account for intra-class variations. OPS representation can also handle scale changes effectively. A dynamic programming algorithm uses the OPS representation to realize the search process, enabling a detailed localization of the object boundaries in the image. The concept's simplicity is reflected in the ease of implementation, as the paper's title suggests. The algorithm works directly with very noisy edge images extracted using the Canny edge detector, without the need for any preprocessing or learning steps. We present our experiments and show that our results outperform those of very powerful, state-of-the-art algorithms."
}
@article{ABRAMOVICH2012551,
title = "Studying teacher selection of resources in an ultra-large scale interactive system: Does metadata guide the way?",
journal = "Computers & Education",
volume = "58",
number = "1",
pages = "551 - 559",
year = "2012",
issn = "0360-1315",
doi = "https://doi.org/10.1016/j.compedu.2011.09.001",
url = "http://www.sciencedirect.com/science/article/pii/S036013151100217X",
author = "Samuel Abramovich and Christian Schunn",
keywords = "Computer-mediated communication, Cooperative/collaborative learning, Human-computer interface, Improving classroom teaching",
abstract = "Ultra-large-scale interactive systems on the Internet have begun to change how teachers prepare for instruction, particularly in regards to resource selection. Consequently, it is important to look at how teachers are currently selecting resources beyond content or keyword search. We conducted a two-part observational study of an existing popular system called TeachersPayTeachers hypothesizing that ‘evaluative metadata’ (i.e. comments, ratings, and popularity measures) would drive selection of resources. The first part examined patterns in tens of thousands of sales overall, and the second part focused on patterns of sales in one focal topic that could be expert coded. We find that there are significant gaps in available metadata, that some aspects of metadata are closely associated with sales, and that metadata are weak correlates of expert-determined quality. We conclude by making suggestions for additional research and suggesting how ultra-large scale-interactive systems such as TeachersPayTeachers could be used to improve teacher education."
}
@article{DELCOSO2015246,
title = "Mixing numerical and categorical data in a Self-Organizing Map by means of frequency neurons",
journal = "Applied Soft Computing",
volume = "36",
pages = "246 - 254",
year = "2015",
issn = "1568-4946",
doi = "https://doi.org/10.1016/j.asoc.2015.06.058",
url = "http://www.sciencedirect.com/science/article/pii/S1568494615004512",
author = "Carmelo del Coso and Diego Fustes and Carlos Dafonte and Francisco J. Nóvoa and José M. Rodríguez-Pedreira and Bernardino Arcay",
keywords = "Self-Organizing Map, Categorical data, Mixed data, Big data",
abstract = "Even though Self-Organizing Maps (SOMs) constitute a powerful and essential tool for pattern recognition and data mining, the common SOM algorithm is not apt for processing categorical data, which is present in many real datasets. It is for this reason that the categorical values are commonly converted into a binary code, a solution that unfortunately distorts the network training and the posterior analysis. The present work proposes a SOM architecture that directly processes the categorical values, without the need of any previous transformation. This architecture is also capable of properly mixing numerical and categorical data, in such a manner that all the features adopt the same weight. The proposed implementation is scalable and the corresponding learning algorithm is described in detail. Finally, we demonstrate the effectiveness of the presented algorithm by applying it to several well-known datasets."
}
@article{SHEEN201661,
title = "The experience and impact of traumatic perinatal event experiences in midwives: A qualitative investigation",
journal = "International Journal of Nursing Studies",
volume = "53",
pages = "61 - 72",
year = "2016",
issn = "0020-7489",
doi = "https://doi.org/10.1016/j.ijnurstu.2015.10.003",
url = "http://www.sciencedirect.com/science/article/pii/S0020748915003041",
author = "Kayleigh Sheen and Helen Spiby and Pauline Slade",
keywords = "Indirect exposure to trauma, Midwives, Posttraumatic stress, Template analysis",
abstract = "Background
Through their work midwives may experience distressing events that fulfil criteria for trauma. However, there is a paucity of research examining the impact of these events, or what is perceived to be helpful/unhelpful by midwives afterwards.
Objective
To investigate midwives’ experiences of traumatic perinatal events and to provide insights into experiences and responses reported by midwives with and without subsequent posttraumatic stress symptoms.
Design
Semi-structured telephone interviews were conducted with a purposive sample of midwives following participation in a previous postal survey.
Methods
35 midwives who had all experienced a traumatic perinatal event defined using the Diagnostic and Statistical Manual of Mental Disorders (version IV) Criterion A for posttraumatic stress disorder were interviewed. Two groups of midwives with high or low distress (as reported during the postal survey) were purposefully recruited. High distress was defined as the presence of clinical levels of PTSD symptomatology and high perceived impairment in terms of impacts on daily life. Low distress was defined as any symptoms of PTSD present were below clinical threshold and low perceived life impairment. Interviews were analysed using template analysis, an iterative process of organising and coding qualitative data chosen for this study for its flexibility. An initial template of four a priori codes was used to structure the analysis: event characteristics, perceived responses and impacts, supportive and helpful strategies and reflection of change over time codes were amended, integrated and collapsed as appropriate through the process of analysis. A final template of themes from each group is presented together with differences outlined where applicable.
Results
Event characteristics were similar between groups, and involved severe, unexpected episodes contributing to feeling ‘out of a comfort zone.’ Emotional upset, self-blame and feelings of vulnerability to investigative procedures were reported. High distress midwives were more likely to report being personally upset by events and to perceive all aspects of personal and professional lives to be affected. Both groups valued talking about the event with peers, but perceived support from senior colleagues and supervisors to be either absent or inappropriate following their experience; however, those with high distress were more likely to endorse this view and report a perceived need to seek external input.
Conclusion
Findings indicate a need to consider effective ways of promoting and facilitating access to support, at both a personal and organisational level, for midwives following the experience of a traumatic perinatal event."
}
@article{FONKEN2016362,
title = "MicroRNA-155 deletion reduces anxiety- and depressive-like behaviors in mice",
journal = "Psychoneuroendocrinology",
volume = "63",
pages = "362 - 369",
year = "2016",
issn = "0306-4530",
doi = "https://doi.org/10.1016/j.psyneuen.2015.10.019",
url = "http://www.sciencedirect.com/science/article/pii/S0306453015300019",
author = "Laura K. Fonken and Andrew D. Gaudet and Kristopher R. Gaier and Randy J. Nelson and Phillip G. Popovich",
keywords = "Depression, Learning and memory, Hippocampus, MicroRNA, Cytokine",
abstract = "Depressive disorders have complex and multi-faceted underlying mechanisms, rendering these disorders difficult to treat consistently and effectively. One under-explored therapeutic strategy for alleviating mood disorders is the targeting of microRNAs (miRs). miRs are small non-coding RNAs that cause sequestration/degradation of specific mRNAs, thereby preventing protein translation and downstream functions. miR-155 has validated and predicted neurotrophic factor and inflammatory mRNA targets, which led to our hypothesis that miR-155 deletion would modulate affective behaviors. To evaluate anxiety-like behavior, wildtype (wt) and miR-155 knockout (ko) mice (littermates; both male and female) were assessed in the open field and on an elevated plus maze. In both tests, miR-155 ko mice spent more time in open areas, suggesting they had reduced anxiety-like behavior. Depressive-like behaviors were assessed using the forced swim test. Compared to wt mice, miR-155 ko mice exhibited reduced float duration and increased latency to float. Further, although all mice exhibited a strong preference for a sucrose solution over water, this preference was enhanced in miR-155 ko mice. miR-155 ko mice had no deficiencies in learning and memory (Barnes maze) or social preference/novelty suggesting that changes in mood were specific. Finally, compared to wt hippocampi, miR-155 ko hippocampi had a reduced inflammatory signature (e.g., decreased IL-6, TNF-a) and female miR-155 ko mice increased ciliary neurotrophic factor expression. Together, these data highlight the importance of studying microRNAs in the context of anxiety and depression and identify miR-155 as a novel potential therapeutic target for improving mood disorders."
}
@article{FADWA201434,
title = "Reinforced concrete wide and conventional beam–column connections subjected to lateral load",
journal = "Engineering Structures",
volume = "76",
pages = "34 - 48",
year = "2014",
issn = "0141-0296",
doi = "https://doi.org/10.1016/j.engstruct.2014.06.029",
url = "http://www.sciencedirect.com/science/article/pii/S014102961400385X",
author = "Issa Fadwa and Tasnimi Abbas Ali and Eilouch Nazih and Mirzabagheri Sara",
keywords = "Wide beam, Conventional beam, Lateral load, Beam–column connection, Drift ratio",
abstract = "In order to illustrate the structural performance of reinforced concrete wide beam–column connection, an experimental research was carried out to compare the behavior of two RC wide beam–column connections and two conventional beam–column connections when subjected to quasi-static cyclic loading. The specimens were full-scale connections and they were composed of two sets of interior and exterior joints. These specimens were designed in accordance with Syrian code of practice version 2006 (dependent on ACI 318 and ACI 352-R02 codes). Transverse beams in the wide beam–column joints, were also wider than the columns. Experimental results indicated that the hysteresis response of the wide beams was likely exhibited remarkable enhancement compared to that of conventional beams and the total energy dissipating capacity of a wide beam–column connection was higher than the conventional joint. Also it was found that by presence of the longitudinal reinforcement of the spandrel beam which was also a wide beam in the wide beam–column joints, flexural hinging mechanism in the wide beam was occurred instead of torsion brittle mode of failure. The experimental behavior of the sub-assemblages is reported within this paper."
}
@article{MENDENHALL201238,
title = "A probability-conserving cross-section biasing mechanism for variance reduction in Monte Carlo particle transport calculations",
journal = "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
volume = "667",
pages = "38 - 43",
year = "2012",
issn = "0168-9002",
doi = "https://doi.org/10.1016/j.nima.2011.11.084",
url = "http://www.sciencedirect.com/science/article/pii/S0168900211021541",
author = "Marcus H. Mendenhall and Robert A. Weller",
keywords = "Non-analog Monte Carlo, Non-Boltzmann tally, Variance reduction, Geant4, Cross-section biasing, Particle splitting, History splitting, Radiation transport, Transport theory",
abstract = "In Monte Carlo particle transport codes, it is often important to adjust reaction cross-sections to reduce the variance of calculations of relatively rare events, in a technique known as non-analog Monte Carlo. We present the theory and sample code for a Geant4 process which allows the cross-section of a G4VDiscreteProcess to be scaled, while adjusting track weights so as to mitigate the effects of altered primary beam depletion induced by the cross-section change. This makes it possible to increase the cross-section of nuclear reactions by factors exceeding 104 (in appropriate cases), without distorting the results of energy deposition calculations or coincidence rates. The procedure is also valid for bias factors less than unity, which is useful in problems that involve the computation of particle penetration deep into a target (e.g. atmospheric showers or shielding studies)."
}
@article{HUANG20151,
title = "Object tracking using discriminative sparse appearance model",
journal = "Signal Processing: Image Communication",
volume = "37",
pages = "1 - 18",
year = "2015",
issn = "0923-5965",
doi = "https://doi.org/10.1016/j.image.2015.06.012",
url = "http://www.sciencedirect.com/science/article/pii/S0923596515001095",
author = "Dandan Huang and Yi Sun",
keywords = "Visual tracking, Sparse representation, Dictionary learning, Adaptive update, Bayesian inference framework",
abstract = "Object tracking based on sparse representation formulates tracking as searching the candidate with minimal reconstruction error in target template subspace. The key problem lies in modeling the target robustly to vary appearances. The appearance model in most sparsity-based trackers has two main problems. The first is that global structural information and local features are insufficiently combined because the appearance is modeled separately by holistic and local sparse representations. The second problem is that the discriminative information between the target and the background is not fully utilized because the background is rarely considered in modeling. In this study, we develop a robust visual tracking algorithm by modeling the target as a model for discriminative sparse appearance. A discriminative dictionary is trained from the local target patches and the background. The patches display the local features while their position distribution implies the global structure of the target. Thus, the learned dictionary can fully represent the target. The incorporation of the background into dictionary learning also enhances its discriminative capability. Upon modeling the target as a sparse coding histogram based on this learned dictionary, our tracker is embedded into a Bayesian state inference framework to locate a target. We also present a model update scheme in which the update rate is adjusted automatically. In conjunction with the update strategy, the proposed tracker can handle occlusion and alleviate drifting. Comparative results on challenging benchmark image sequences show that the tracking method performs favorably against several state-of-the-art algorithms."
}
@article{HOLLINS2016187,
title = "Alteration of transcriptional networks in the entorhinal cortex after maternal immune activation and adolescent cannabinoid exposure",
journal = "Brain, Behavior, and Immunity",
volume = "56",
pages = "187 - 196",
year = "2016",
issn = "0889-1591",
doi = "https://doi.org/10.1016/j.bbi.2016.02.021",
url = "http://www.sciencedirect.com/science/article/pii/S0889159116300411",
author = "Sharon L. Hollins and Katerina Zavitsanou and Frederick Rohan Walker and Murray J. Cairns",
keywords = "Maternal infection, Cannabinoid, Brain, Entorhinal cortex, Gene expression, Schizophrenia",
abstract = "Maternal immune activation (MIA) and adolescent cannabinoid exposure (ACE) have both been identified as major environmental risk factors for schizophrenia. We examined the effects of these two risk factors alone, and in combination, on gene expression during late adolescence. Pregnant rats were exposed to the viral infection mimic polyriboinosinic-polyribocytidylic acid (poly I:C) on gestational day (GD) 15. Adolescent offspring received daily injections of the cannabinoid HU210 for 14days starting on postnatal day (PND) 35. Gene expression was examined in the left entorhinal cortex (EC) using mRNA microarrays. We found prenatal treatment with poly I:C alone, or HU210 alone, produced relatively minor changes in gene expression. However, following combined treatments, offspring displayed significant changes in transcription. This dramatic and persistent alteration of transcriptional networks enriched with genes involved in neurotransmission, cellular signalling and schizophrenia, was associated with a corresponding perturbation in the expression of small non-coding microRNA (miRNA). These results suggest that a combination of environmental exposures during development leads to significant genomic remodeling that disrupts maturation of the EC and its associated circuitry with important implications as the potential antecedents of memory and learning deficits in schizophrenia and other neuropsychiatric disorders."
}
@article{MEENA2018299,
title = "PAREMD: A parallel program for the evaluation of momentum space properties of atoms and molecules",
journal = "Computer Physics Communications",
volume = "224",
pages = "299 - 310",
year = "2018",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2017.12.002",
url = "http://www.sciencedirect.com/science/article/pii/S0010465517304010",
author = "Deep Raj Meena and Shridhar R. Gadre and P. Balanarayan",
keywords = "EMD: Electron momentum density, ED: Electron density",
abstract = "The present work describes a code for evaluating the electron momentum density (EMD), its moments and the associated Shannon information entropy for a multi-electron molecular system. The code works specifically for electronic wave functions obtained from traditional electronic structure packages such as GAMESS and GAUSSIAN. For the momentum space orbitals, the general expression for Gaussian basis sets in position space is analytically Fourier transformed to momentum space Gaussian basis functions. The molecular orbital coefficients of the wave function are taken as an input from the output file of the electronic structure calculation. The analytic expressions of EMD are evaluated over a fine grid and the accuracy of the code is verified by a normalization check and a numerical kinetic energy evaluation which is compared with the analytic kinetic energy given by the electronic structure package. Apart from electron momentum density, electron density in position space has also been integrated into this package. The program is written in C++ and is executed through a Shell script. It is also tuned for multicore machines with shared memory through OpenMP. The program has been tested for a variety of molecules and correlated methods such as CISD, Møller–Plesset second order (MP2) theory and density functional methods. For correlated methods, the PAREMD program uses natural spin orbitals as an input. The program has been benchmarked for a variety of Gaussian basis sets for different molecules showing a linear speedup on a parallel architecture.
Program summary
Program Title: PAREMD Program Files doi:http://dx.doi.org/10.17632/gcr9gmh6zv.1 Licensing provisions: GPLv3 Programming language: C, Csh External routines/libraries: GSL[1], BLAS[2,3], OpenMP[4], GAMESS[5,6] and GAUSSIAN[7]. Nature of problem: Momentum space properties for a multi-electron system. Solution method: Analytic Fourier transformation of Gaussian basis in position space to get momentum space basis followed by EMD evaluation on spherical or Cartesian grids. A numerical integration procedure is implemented for evaluating moments of EMD and Shannon information entropy on a polar grid. [1] Galassi et al, GNU Scientific Library Reference Manual (3rd Ed.), ISBN 0954612078. [2] L. S. Blackford, J. Demmel, J. Dongarra, I. Duff, S. Hammarling, G. Henry, M. Heroux, L. Kaufman, A. Lumsdaine, A. Petitet, R. Pozo, K. Remington, R. C. Whaley, An Updated Set of Basic Linear Algebra Subprograms (BLAS), ACM Trans. Math. Soft., 28-2 (2002), pp. 135–151. [3] J. Dongarra, Basic Linear Algebra Subprograms Technical Forum Standard, International Journal of High Performance Applications and Supercomputing, 16(1) (2002), pp. 1–111, and International Journal of High Performance Applications and Supercomputing, 16(2) (2002), pp. 115–199. [4] OpenMP Architecture Review Board, ”OpenMP Application Program Interface, Version 4.5”, November 2015. [5] ”General Atomic and Molecular Electronic Structure System” M.W. Schmidt, K.K. Baldridge, J.A. Boatz, S.T. Elbert, M.S. Gordon, J.H. Jensen, S. Koseki, N. Matsunaga, K.A. Nguyen, S.J. Su, T.L. Windus, M. Dupuis, J.A. MontgomeryJ. Comput. Chem. 14 1347–1363, 1993. [6] “Advances in electronic structure theory: GAMESS a decade later M.S. Gordon, M.W. Schmidt pp. 1167–1189, in Theory and Applications of Computational Chemistry: the first forty years C.E. Dykstra, G. Frenking, K.S. Kim, G.E. Scuseria (editors), Elsevier, Amsterdam, 2005. [7] Gaussian 09, Revision C.01, M. J. Frisch, G. W. Trucks, H. B. Schlegel, G. E. Scuseria, M. A. Robb, J. R. Cheeseman, G. Scalmani, V. Barone, G. A. Petersson, H. Nakatsuji, X. Li, M. Caricato, A. Marenich, J. Bloino, B. G. Janesko, R. Gomperts, B. Mennucci, H. P. Hratchian, J. V. Ortiz, A. F. Izmaylov, J. L. Sonnenberg, D. Williams-Young, F. Ding, F. Lipparini, F. Egidi, J. Goings, B. Peng, A. Petrone, T. Henderson, D. Ranasinghe, V. G. Zakrzewski, J. Gao, N. Rega, G. Zheng, W. Liang, M. Hada, M. Ehara, K. Toyota, R. Fukuda, J. Hasegawa, M. Ishida, T. Nakajima, Y. Honda, O. Kitao, H. Nakai, T. Vreven, K. Throssell, J. A. Montgomery, Jr., J. E. Peralta, F. Ogliaro, M. Bearpark, J. J. Heyd, E. Brothers, K. N. Kudin, V. N. Staroverov, T. Keith, R. Kobayashi, J. Normand, K. Raghavachari, A. Rendell, J. C. Burant, S. S. Iyengar, J. Tomasi, M. Cossi, J. M. Millam, M. Klene, C. Adamo, R. Cammi, J. W. Ochterski, R. L. Martin, K. Morokuma, O. Farkas, J. B. Foresman, and D. J. Fox, Gaussian, Inc., Wallingford CT, 2016."
}
@article{LI2018290,
title = "Molecular characterization and expression analysis of glucose transporter 1 and hepatic glycolytic enzymes activities from herbivorous fish Ctenopharyngodon idellus in respond to a glucose load after the adaptation to dietary carbohydrate levels",
journal = "Aquaculture",
volume = "492",
pages = "290 - 299",
year = "2018",
issn = "0044-8486",
doi = "https://doi.org/10.1016/j.aquaculture.2018.04.028",
url = "http://www.sciencedirect.com/science/article/pii/S0044848618302655",
author = "Ruixin Li and Hongyu Liu and Xiaohui Dong and Shuyan Chi and Qihui Yang and Shuang Zhang and Beiping Tan",
keywords = ", Glucose transporter 1, Molecular cloning, Glucose administration, Glycolytic enzymes",
abstract = "In the present study, we test the hypothesis that the ability of Ctenopharyngodon idellu to clear a glucose load is related to nutrient history of different dietary carbohydrate through the transport and glycolysis of glucose. Firstly, we obtained the complete coding sequence from C. idellus GLUT1 (ciGLUT1), composed by 2364 bp with an open reading frame of 1473 bp encoding 490 amino acids. Sequence alignment and phylogenetic analysis revealed a high degree of conservation (80–97%) among most fish and higher vertebrates. The analysis of the 5′ and 3′ UTRs showed the presence of several post-transcriptional regulatory elements. The highest ciGLUT1 expression was observed in the heart followed by brain, whereas relatively low values were detected in the muscle, gill, intestine, kidney, spleen and liver. Then, hepatic and intestine ciGLUT1 expressions and hepatic glycolytic enzymes activities were determined in fish subjected to a glucose load after being fed different dietary carbohydrate levels (20%, 40% and 60%) for 8 weeks. And the result showed that different dietary carbohydrate had no effect on ciGLUT1 mRNA of hepatic and intestine after 8 weeks trail. However, after the adaptation of carbohydrate levels, glucose administration caused a prompt increase of hepatic ciGLUT1 mRNA expressions in all treatments whereas it either had no effect or a negative effect on intestine ciGLUT1 mRNA. In terms of dietary carbohydrate levels, the ciGLUT1 expressions of fish fed high carbohydrate diet (60%) were significantly higher than the other groups after the glucose load. In additon, after the glucose administration, hepatic low Km hexokinases (HK) activities of fish fed high carbohydrate diets increased significantly and was also significantly higher than the other groups. The results indicated that the GLUT1 gene of C. idellus showed a typical structure of the glucose transporters family, supporting the concept that ciGLUT1 functions as a ubiquitous and constitutive basal level glucose transporter. Consistent with the function of the GLUT1 as a constitutive glucose transporter, ciGLUT1 expressions of hepatic and intestine were hardly affected by the changes in dietary carbohydrate. Furthermore, after the glucose load, the fish after a long-term high dietary carbohydrate adaptation showed a more efficient clearance of glucose through the more remarkable enhancements of hepatic ciGLUT1 expressions and HK activities."
}
@article{RAMSEY201473,
title = "Glucagon-like peptide 1 receptor (GLP1R) haplotypes correlate with altered response to multiple antipsychotics in the CATIE trial",
journal = "Schizophrenia Research",
volume = "160",
number = "1",
pages = "73 - 79",
year = "2014",
issn = "0920-9964",
doi = "https://doi.org/10.1016/j.schres.2014.09.038",
url = "http://www.sciencedirect.com/science/article/pii/S0920996414005301",
author = "Timothy L. Ramsey and Mark D. Brennan",
keywords = "Olanzapine, Perphenazine, Quetiapine, Risperidone, Ziprasidone, Pharmacogenetics, Schizophrenia",
abstract = "Glucagon-like peptide 1 receptor (GLP1R) signaling has been shown to have antipsychotic properties in animal models and to impact glucose-dependent insulin release, satiety, memory, and learning in man. Previous work has shown that two coding mutations (rs6923761 and rs1042044) are associated with altered insulin release and cortisol levels. We identified four frequently occurring haplotypes in Caucasians, haplotype 1 through haplotype 4, spanning exons 4–7 and containing the two coding variants. We analyzed response to antipsychotics, defined as predicted change in PANSS-Total (dPANSS) at 18months, in Caucasian subjects from the Clinical Antipsychotic Trial of Intervention Effectiveness treated with olanzapine (n=139), perphenazine (n=78), quetiapine (n=14), risperidone (n=143), and ziprasidone (n=90). Haplotype trend regression analysis revealed significant associations with dPANSS for olanzapine (best p=0.002), perphenazine (best p=0.01), quetiapine (best p=0.008), risperidone (best p=0.02), and ziprasidone (best p=0.007). We also evaluated genetic models for the two most common haplotypes. Haplotype 1 (uniquely including the rs1042044 [Leu260] allele) was associated with better response to olanzapine (p=0.002), and risperidone (p=0.006), and worse response to perphenazine (p=.03), and ziprasidone (p=0.003), with a recessive genetic model providing the best fit. Haplotype 2 (uniquely including the rs6923761 [Ser168] allele) was associated with better response to perphenazine (p=0.001) and worse response to olanzapine (p=.02), with a dominant genetic model providing the best fit. However, GLP1R haplotypes were not associated with antipsychotic-induced weight gain. These results link functional genetic variants in GLP1R to antipsychotic response."
}
@article{SARAI201545,
title = "In vitro levamisole selection pressure on larval stages of Haemonchus contortus over nine generations gives rise to drug resistance and target site gene expression changes specific to the early larval stages only",
journal = "Veterinary Parasitology",
volume = "211",
number = "1",
pages = "45 - 53",
year = "2015",
issn = "0304-4017",
doi = "https://doi.org/10.1016/j.vetpar.2015.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0304401715002216",
author = "Ranbir S. Sarai and Steven R. Kopp and Malcolm R. Knox and Glen T. Coleman and Andrew C. Kotze",
keywords = ", Levamisole, Resistance, Nicotinic agonists, Gene expression",
abstract = "There is some evidence that resistance to levamisole and pyrantel in trichostrongylid nematodes is due to changes in the composition of nicotinic acetylcholine receptors (nAChRs) which represent the drug target site. Altered expression patterns of genes coding for nAChR subunits, as well as the presence of truncated versions of several subunits, have been implicated in observed resistances. The studies have mostly compared target sites in worm isolates of very different genetic background, and hence the ability to associate the molecular changes with drug sensitivity alone have been clouded to some extent. The present study aimed to circumvent this issue by following target site gene expression pattern changes as resistance developed in Haemonchus contortus worms under laboratory selection pressure with levamisole. We applied drug selection pressure to early stage larvae in vitro over nine generations, and monitored changes in larval and adult drug sensitivities and target site gene expression patterns. High level resistance developed in larvae, with resistance factors of 94-fold and 1350-fold at the IC50 and IC95, respectively, in larval development assays after nine generations of selection. There was some cross-resistance to bephenium (70-fold increase in IC95). The expression of all the putative subunit components of levamisole-sensitive nAChRs, as well as a number of ancillary protein genes, particularly Hco-unc-29.1 and –ric-3, were significantly decreased (up to 5.5-fold) in the resistant larvae at generation nine compared to the starting population. However, adult worms did not show any resistance to levamisole, and showed an inverse pattern of gene expression changes, with many target site genes showing increased expression compared to the starting population. A comparison of the larval/adult drug sensitivity data with the known relationships for field-derived isolates indicated that the adults of our selected population should have been highly resistant to the drug if the larval/adult sensitivity relationships were in accordance with previous field isolates. Hence, our selected worms showed a life-stage drug sensitivity pattern quite different to that seen in the field. The present study has highlighted an association between drug target site changes and resistance to levamisole in H. contortus larvae. However, it has also highlighted the artificial nature of the larval selection method with levamisole, as the resistance phenotype and the associated molecular changes were only observed in the drug-pressured life stage. The study therefore reinforces the need for caution in extrapolating larval-based laboratory selection outcomes to field resistances."
}
@article{MIKRAM20132204,
title = "POINCARÉ CODE: A package of open-source implements for normalization and computer algebra reduction near equilibria of coupled ordinary differential equations",
journal = "Computer Physics Communications",
volume = "184",
number = "9",
pages = "2204 - 2213",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.04.003",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513001380",
author = "J. Mikram and F. Zinoun and A. El Abdllaoui",
keywords = " package, Ordinary differential equations, Poincaré–Dulac normal form, Birkhoff–Gustavson normal form, Joint normal form, Lie transform",
abstract = "The Poincaré code is a Maple project package that aims to gather significant computer algebra normal form (and subsequent reduction) methods for handling nonlinear ordinary differential equations. As a first version, a set of fourteen easy-to-use Maple  commands is introduced for symbolic creation of (improved variants of Poincaré’s) normal forms as well as their associated normalizing transformations. The software is the implementation by the authors of carefully studied and followed up selected normal form procedures from the literature, including some authors’ contributions to the subject. As can be seen, joint-normal-form programs involving Lie-point symmetries are of special interest and are published in CPC Program Library for the first time, Hamiltonian variants being also very useful as they lead to encouraging results when applied, for example, to models from computational physics like Hénon–Heiles.
Program summary
Program title: POINCARÉ Catalogue identifier: AEPJ_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEPJ_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 7694 No. of bytes in distributed program, including test data, etc.: 597943 Distribution format: tar.gz Programming language: Maple V11. Computer: See specifications for running Maple V11 or above. Operating system: MS Windows. Classification: 4.3, 5, 16.9. Nature of problem: Computing structure-preserving normal forms near the origin for nonlinear vector fields. Solution method: 14 Maple commands are designed to compute various normal forms as well as their generating functions and associated normalizing transformations for nonlinear systems of ordinary differential equations, including Hamiltonian ones. Further reduction of these normal forms–in the presence of Lie-point symmetries–is also considered. All algorithms are based on Lie transform, thus leading to the structure-preserving normal forms in the sense that all properties that can be formulated in terms of graded Lie algebras are preserved. Restrictions: The semisimple part of the leading matrix about the origin (resp. the quadratic part in the Hamiltonian case) is assumed to be already taken into diagonal form (resp. canonical form) via a linear change of variables. In other words, the eigenvalues must be explicitly known. Unusual features: Further reduction of Poincaré–Dulac normal form via the joint normal form commands, taking profitably Lie-point symmetries, is perhaps the main feature of the software. Besides, to the best of our knowledge, and except the Hamiltonian case, it seems that there is no CPC programs for computation of (structure-preserving) normal forms in the general case. Running time: Depends on the input data, mainly on system size, type of the leading matrix (semisimple or not), type of resonances, order of normalization and required options. Instantaneous for the provided examples."
}
@article{GERMANN201491,
title = "Psychiatrists, criminals, and the law: Forensic psychiatry in Switzerland 1850–1950",
journal = "International Journal of Law and Psychiatry",
volume = "37",
number = "1",
pages = "91 - 98",
year = "2014",
note = "Historical Perspectives on Forensic Psychiatry",
issn = "0160-2527",
doi = "https://doi.org/10.1016/j.ijlp.2013.09.009",
url = "http://www.sciencedirect.com/science/article/pii/S0160252713000848",
author = "Urs Germann",
keywords = "Forensic psychiatry, Criminal justice, History, Criminal responsibility, Security measures, Swiss criminal law",
abstract = "Between 1880 and 1950, Swiss psychiatrists established themselves as experts in criminal courts. In this period, the judicial authorities required psychiatric testimonies in a rising number of cases. As a result, more offenders than ever before were declared mentally deficient and, eventually, sent to psychiatric asylums. Psychiatrists also enhanced their authority as experts at the political level. From the very beginning, they got involved in the preparatory works for a nationwide criminal code. In this article, I argue that these trends toward medicalization of crime were due to incremental processes, rather than spectacular institutional changes. In fact, Swiss psychiatrists gained recognition as experts due to their daily interactions with judges, public prosecutors, and legal counsels. At the same time, the spread of medical expertise had serious repercussions on psychiatric institutions. From 1942 onwards, asylums had to deal with a growing number of “criminal psychopaths,” which affected ward discipline and put psychiatry's therapeutic efficiency into question. The defensive way in which Swiss psychiatrists reacted to this predicament was crucial to the further development of forensic psychiatry. For the most part, it accounts for the subdiscipline's remarkable lack of specialization until the 1990s."
}
@article{JONES201438,
title = "Emotional reactivity and regulation associated with fluent and stuttered utterances of preschool-age children who stutter",
journal = "Journal of Communication Disorders",
volume = "48",
pages = "38 - 51",
year = "2014",
issn = "0021-9924",
doi = "https://doi.org/10.1016/j.jcomdis.2014.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S0021992414000185",
author = "Robin M. Jones and Edward G. Conture and Tedra A. Walden",
keywords = "Stuttering, Preschool, Emotion, Regulation, Behavior",
abstract = "Purpose
The purpose of this study was to assess the relation between emotional reactivity and regulation associated with fluent and stuttered utterances of preschool-age children who stutter (CWS) and those who do not (CWNS).
Participants
Participants were eight 3 to 6-year old CWS and eight CWNS of comparable age and gender.
Methods
Participants were exposed to three emotion-inducing overheard conversations—neutral, angry and happy—and produced a narrative following each overheard conversation. From audio-video recordings of these narratives, coded behavioral analysis of participants’ negative and positive affect and emotion regulation associated with stuttered and fluent utterances was conducted.
Results
Results indicated that CWS were significantly more likely to exhibit emotion regulation attempts prior to and during their fluent utterances following the happy as compared to the negative condition, whereas CWNS displayed the opposite pattern. Within-group assessment indicated that CWS were significantly more likely to display negative emotion prior to and during their stuttered than fluent utterances, particularly following the positive overheard conversation.
Conclusions
After exposure to emotional-inducing overheard conversations, changes in preschool-age CWS's emotion and emotion regulatory attempts were associated with the fluency of their utterances. Learning outcomes: After reading this article, the reader will be able to: (1) describe various measures of emotional reactivity and regulation, including parent-based reports and behavioral coding, and how they may contribute to childhood stuttering; (2) explain emotional differences between the stuttered and fluent utterances of CWS and CWNS; and (3) discuss how emotions may contribute to CWS’ instances of stuttering."
}
@article{DERLET2016902,
title = "Corporate and Hospital Profiteering in Emergency Medicine: Problems of the Past, Present, and Future",
journal = "The Journal of Emergency Medicine",
volume = "50",
number = "6",
pages = "902 - 909",
year = "2016",
issn = "0736-4679",
doi = "https://doi.org/10.1016/j.jemermed.2016.01.006",
url = "http://www.sciencedirect.com/science/article/pii/S073646791600007X",
author = "Robert W. Derlet and Robert M. McNamara and Scott H. Plantz and Matthew K. Organ and John R. Richards",
keywords = "emergency medicine, corporation, finance, billing, Medicare, contract management group",
abstract = "Background
Health care delivery in the United States has evolved in many ways over the past century, including the development of the specialty of Emergency Medicine (EM). With the creation of this specialty, many positive changes have occurred within hospital emergency departments (EDs) to improve access and quality of care of the nation's de facto “safety net.” The specialty of EM has been further defined and held to high standards with regard to board certification, sub-specialization, maintenance of skills, and research. Despite these advances, problems remain.
Objective
This review discusses the history and evolution of for-profit corporate influence on EM, emergency physicians, finance, and demise of democratic group practice. The review also explores federal and state health care financing issues pertinent to EM and discusses potential solutions.
Discussion
The monopolistic growth of large corporate contract management groups and hospital ownership of vertically integrated physician groups has resulted in the elimination of many local democratic emergency physician groups. Potential downsides of this trend include unfair or unlawful termination of emergency physicians, restrictive covenants, quotas for productivity, admissions, testing, patient satisfaction, and the rising cost of health care. Other problems impact the financial outlook for EM and include falling federal, state, and private insurance reimbursement for emergency care, balance-billing, up-coding, unnecessary testing, and admissions.
Conclusions
Emergency physicians should be aware of the many changes happening to the specialty and practice of EM resulting from corporate control, influence, and changing federal and state health care financing issues."
}
@article{REID2016227,
title = "Differential respiratory health effects from the 2008 northern California wildfires: A spatiotemporal approach",
journal = "Environmental Research",
volume = "150",
pages = "227 - 235",
year = "2016",
issn = "0013-9351",
doi = "https://doi.org/10.1016/j.envres.2016.06.012",
url = "http://www.sciencedirect.com/science/article/pii/S001393511630247X",
author = "Colleen E. Reid and Michael Jerrett and Ira B. Tager and Maya L. Petersen and Jennifer K. Mann and John R. Balmes",
keywords = "Wildland fires, Climate change, Vulnerable populations, Asthma, Air pollution",
abstract = "We investigated health effects associated with fine particulate matter during a long-lived, large wildfire complex in northern California in the summer of 2008. We estimated exposure to PM2.5 for each day using an exposure prediction model created through data-adaptive machine learning methods from a large set of spatiotemporal data sets. We then used Poisson generalized estimating equations to calculate the effect of exposure to 24-hour average PM2.5 on cardiovascular and respiratory hospitalizations and ED visits. We further assessed effect modification by sex, age, and area-level socioeconomic status (SES). We observed a linear increase in risk for asthma hospitalizations (RR=1.07, 95% CI=(1.05, 1.10) per 5µg/m3 increase) and asthma ED visits (RR=1.06, 95% CI=(1.05, 1.07) per 5µg/m3 increase) with increasing PM2.5 during the wildfires. ED visits for chronic obstructive pulmonary disease (COPD) were associated with PM2.5 during the fires (RR=1.02 (95% CI=(1.01, 1.04) per 5µg/m3 increase) and this effect was significantly different from that found before the fires but not after. We did not find consistent effects of wildfire smoke on other health outcomes. The effect of PM2.5 during the wildfire period was more pronounced in women compared to men and in adults, ages 20–64, compared to children and adults 65 or older. We also found some effect modification by area-level median income for respiratory ED visits during the wildfires, with the highest effects observed in the ZIP codes with the lowest median income. Using a novel spatiotemporal exposure model, we found some evidence of differential susceptibility to exposure to wildfire smoke."
}
@article{LAZIC2014631,
title = "Arthroscopic washout of the knee: A procedure in decline",
journal = "The Knee",
volume = "21",
number = "2",
pages = "631 - 634",
year = "2014",
issn = "0968-0160",
doi = "https://doi.org/10.1016/j.knee.2014.02.014",
url = "http://www.sciencedirect.com/science/article/pii/S0968016014000416",
author = "Stefan Lazic and Oliver Boughton and Caroline Hing and Jason Bernard",
keywords = "Knee, Arthroscopy, Washout, Arthroscopic washout, Endoscopic irrigation",
abstract = "Background
Osteoarthritis (OA) of the knee is a chronic, progressive condition which often requires surgical intervention. The evidence for the benefits of arthroscopic debridement or washout for knee OA is weak and arthroscopy is currently only indicated in the UK if there is a history of mechanical locking of the knee.
Objectives
To investigate whether there has been any change in the number of arthroscopies performed in the UK since the 2007 NICE guidance on knee arthroscopy and the 2008 Cochrane review of arthroscopic debridement for OA of the knee.
Methods
We interrogated data from the Hospital Episodes Statistics (HES) database with Office of Population Censuses and Surveys-4 (OPSC-4) codes pertaining to therapeutic endoscopic operations in the 60–74year old and 75 and over age groups.
Results
The number of arthroscopic knee interventions in the UK decreased overall from 2000 to 2012, with arthroscopic irrigations decreasing the most by 39.6 per 100,000 population (80%). However, the number of arthroscopic meniscal resections increased by 105.3 per 100,000 (230%) population. These trends were mirrored in both the 60–74 and 75 and over age groups.
Conclusions
Knee arthroscopy in the 60–74 and 75 and over age groups appears to be decreasing but there is still a large and increasing number of arthroscopic meniscal resections being performed."
}
@article{CHANG2015335,
title = "BCVEGPY2.2: A newly upgraded version for hadronic production of the meson Bc and its excited states",
journal = "Computer Physics Communications",
volume = "197",
pages = "335 - 338",
year = "2015",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2015.07.015",
url = "http://www.sciencedirect.com/science/article/pii/S0010465515002842",
author = "Chao-Hsi Chang and Xian-You Wang and Xing-Gang Wu",
keywords = "Event generator, Hadronic production,  meson, Un-weighted events",
abstract = "A newly upgraded version of the BCVEGPY, a generator for hadronic production of the meson Bc and its excited states, is available. In comparison with the previous one (Chang et al., 2006), the new version is to apply an improved hit-and-miss technology to generating the un-weighted events much more efficiently under various simulation environments. The codes for production of 2S-wave Bc states are also given here.
New version program summary
Title of program: BCVEGPY2.2 Catalogue identifier: ADTJ_v2_3 Program obtained from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 323731 No. of bytes in distributed program, including test data, etc.: 4498602 Distribution format: tar.gz Computer: Any LINUX based on PC with FORTRAN 77 or FORTRAN 90 and GNU C compiler as well Operating systems: LINUX Programming language used: FORTRAN 77/90 Memory required to execute with typical data: About 2.0 MB Classification: 11.2, 11.5 Catalogue identifier of previous version: ADTJ_v2_2 Journal reference of previous version: Comput. Phys. Commun. 183 (2012) 442 Does the new version supersede the old version?: Yes Nature of physical problem: Hadronic Production of Bc meson and its excited states. Method of solution: To generate un-weighted events of Bc meson and its excited states by using an improved hit-and-miss technology. Reasons for new version: Responding to the feedback from users, such as those from CMS and LHCb groups, we create a new hit-and-miss algorithm for generating the un-weighted events. Furthermore, the relevant codes for generating the 2S-excited state of Bc meson are added, because the excited state production may be sizable in the new LHC run. Typical running time: It depends on which option is chosen to match PYTHIA when generating the full events and also on which state of Bc meson, either its ground state or its excited states, is to be generated. Typically on a 2.27GHz Intel Xeon E5520 processor machine, for producing the Bc meson ground state: I) If setting [IDWTUP=3 and unwght=.true.], it shall adopt the new hit-and-miss technology to generate the un-weighted events, and to generate 105 events takes 30 minutes; II) If setting [IDWTUP=3 and unwght=.false.] or [IDWTUP=1 and IGENERATE=0], it shall generate the weighted events, and to generate 105 events takes 2 minutes only (the fastest way, for theoretical purpose only); III) As a comparison, if setting [IDWTUP=1 and IGENERATE=1], it shall, as the same as the previous version, adopt the PYTHIA inner hit-and-miss technology to generate the un-weighted events, and to generate 1000 events takes about 22 hours. Thus, the efficiency (and accuracy also) for generating the un-weighted events obviously is greatly increased. Keywords: Event generator; Hadronic production; Bc meson; Un-weighted events Summary of revisions: 1). We improve the approach for generating un-weighted events. 2). Responding to the feedback from users, we adjust part of the codes to make it work more user-friendly. More specifically, we explain main changes in the following : •Event generation.If each simulated event comes with a weight, it will make the data analysis much more complicated. Thus the un-weighted events are usually adopted for Monte Carlo simulations. As an external process of PYTHIA, the generator BCVEGPY  [1], [2], [3], [4] shall call the PYTHIA inner hit-and-miss mechanism to generate the un-weighted events by setting IDWGTUP=1 and IGENERATE=1   [5], i.e. the Von Neumann method is used for generating the un-weighted Bc events.Every events bearing a weight (xwgtup) respectively, when inputting them to PYTHIA, they are suffered from being accepted or rejected, all the fully generated events at the output become to have a common weight. The Von Neumann method states that the event should be accepted by the PYTHIA subroutine PYEVNT with a probability R=xwgtup/xmaxup. This can be achieved by comparing R with a random number that is uniformly distributed within the region of [0,1]. Namely if R is bigger than such a random number then the event is accepted, otherwise it should be rejected. Here xmaxup stands for the maximum event weight.The von Neumann method works effectively for the cases when all the weights of input events are moderate in the whole phase-space. However if the input events’ weights vary greatly, such as varying logarithmically, then its efficiency shall be greatly depressed, since too much time shall be wasted for calculating xwgtup of the rejected events. Thus it is helpful to find a new method for generating un-weighted events.We will adopt the new hit-and-miss strategy suggested by Ref.[6] to do the Bc meson un-weight simulation. Extra switches for calling this new technology are added to BCVEGPY, e.g. the new hit-and-miss technology shall be called by setting IDWTUP=3 and unwght=.true.. Details for this new technology can be found in Ref.[6]. For self-consistency, we repeat its main idea here.To be different from previous versions, BCVEGPY2.2 uses the VEGAS  [7] and the MINT  [8] as a combined way to generate the un-weighted events. The whole phase space shall be separated to a multi-dimensional phase-space grid. The main purpose of VEGAS  [7] is to perform the adaptive Monte Carlo multi-dimensional integration, which uses the importance-sampling method to improve the integration efficiency. Each event shall generally result in a different weight, recorded by xwgtup, and the maximum weight within each grid shall be simultaneously recorded into the importance-sampling grid file (with the suffix .grid). Then following the idea of MINT, the Von Neumann method is used in each phase-space grid. Within this small grid region, the von Neumann algorithm works effectively, thus the efficiency for generating un-weighted events are greatly increased.To implement the new hit-and-miss algorithm into BCVEGPY2.2, we change the original VEGAS subroutine asvegas(fxn,ndim,ncall,itmx,nprn,xint,xmax,imode)Three new variables xint, xmax and imode are added in the VEGAS subroutine. The xmax array is used to record the maximum weights in all cells and imode is a flag. xint stands for the output cross-section when setting imode=0, which shall be used to initialize the xmax array when setting imode=1. For convenience, the generated xmax array will be stored in the same grid file in which the importance sampling function is stored.In the initialization stage, the VEGAS subroutine shall be called by the subroutine evntinit twice by setting imode=0 and imode=1 respectively to generate both the upper bound grid xmax for all cells and the importance sampling function.A subroutine gen(fxn,ndim,xmax,jmode) is defined in the file vegas.F with the purpose to generate the un-weighted events. Three options for calling gen subroutine are defined: jmode=0 is to initializes the parameter; jmode=3 is to print the generation statistics; jmode=1 is the key option, which is to use the new hit-and-miss technology to generate the un-weighted events. More explicitly, by calling gen(fxn,ndim,xmax,jmode=1), three steps shall be executed: 1.Call the phase_gen subroutine to generate a random phase-space point and to calculate its weight xwgtup.2.Judge the point locates in which cell and read from the xmax array and get the upper bound value xmaxup for this particular cell.3.Judge whether such point be kept or not by using the Von Neumann method with the help of the probability xwgtup/xmaxup. To be more flexible, we add one parameter igenmode for generating or using the existed .grid files. When setting igenmode=1, the VEGAS subroutine shall be called to generate the .grid files. When setting igenmode=2, the VEGAS subroutine shall be called to generate more accurate .grid files from the existed .grid files. When setting igenmode=3, one can directly use the existed .grid files to generate events without running VEGAS. Importantly, before using the existed .grid files, one must ensure all the parameters be the same as the previous generation.•A script for setting the parameters and a cross-check of the un-weighted events.We put an additional file, bcvegpy_set_par.nam, in the new version for setting the parameters. This way the user does not need to compile the program again if only the parameter values are changed. Fig. 1Comparison of the normalized Bc transverse momentum (PT) and rapidity (y) distributions derived by setting unwght=.true. (events) and unwght=.false. (differential distributions), which are represented by solid and dotted lines, respectively.As a cross-check of the new technology, we compare the un-weighted Bc event distributions derived by setting unwght=.true. with the weighted Bc differential distributions derived by setting unwght=.false.. The results are shown in Fig. 1. Those two distributions after proper normalization agree well with each other, that shows our present scheme for un-weighted events is correct.•Bc(2S) generation.In 2014 the ATLAS collaboration reported an observation about an excited state of Bc meson, which most probably is Bc(2S) state  [9]. With more data being collected at LHC detectors, it is hopeful that more observations on the excited Bc states will be issued. Therefore in addition to the production via color-singlet Bc(1S), Bc(1P) and color-octet Bc(1S) states, the Bc(2S) production is involved in BCVEGPY2.2. It is achieved by replacing the 1S-wave bound-state parameters pmb, pmc and fbc with those of the 2S-wave one. Here pmb, pmc and fbc are for b-quark mass, c-quark mass and the radial wave function at the zero (|R(0)|), respectively. For the 2S-wave case, their default values are set as pmb=5.234GeV, pmc=1.633 GeV and fbc=0.991GeV3/2   [10] if the mass of the 2S-wave Bc state is 6.867GeV.More explicitly, two new values for ibcstate are added: ibcstate=9 is to generate 21S0 state and ibcstate=10 is to generate 23S1 state. Detailed technologies for deriving the production properties of all the mentioned ten Bc meson states can be found in Refs.[11], [12], [13]. Furthermore, the values for mix_type are rearranged. mix_type=1 is to generate the mixing events for all mentioned states.  mix_type=2 is to generate the mixing events for 11S0 and 13S1 states. mix_type=3 is to generate the mixing events for the four 1P-wave states and the two color-octet 11S0 and 13S1 states. mix_type=4 is to generate the mixing events for 21S0 and 23S1 states."
}
@article{BOURQUIN20121800,
title = "Neural plasticity associated with recently versus often heard objects",
journal = "NeuroImage",
volume = "62",
number = "3",
pages = "1800 - 1806",
year = "2012",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2012.04.055",
url = "http://www.sciencedirect.com/science/article/pii/S105381191200451X",
author = "Nathalie M.-P. Bourquin and Lucas Spierer and Micah M. Murray and Stephanie Clarke",
keywords = "Repetition priming, Sound object, Environmental sound, Semantic representation, Auditory evoked potential (AEP), Human auditory cortex",
abstract = "In natural settings the same sound source is often heard repeatedly, with variations in spectro-temporal and spatial characteristics. We investigated how such repetitions influence sound representations and in particular how auditory cortices keep track of recently vs. often heard objects. A set of 40 environmental sounds was presented twice, i.e. as prime and as repeat, while subjects categorized the corresponding sound sources as living vs. non-living. Electrical neuroimaging analyses were applied to auditory evoked potentials (AEPs) comparing primes vs. repeats (effect of presentation) and the four experimental sections. Dynamic analysis of distributed source estimations revealed i) a significant main effect of presentation within the left temporal convexity at 164–215ms post-stimulus onset; and ii) a significant main effect of section in the right temporo-parietal junction at 166–213ms. A 3-way repeated measures ANOVA (hemisphere×presentation×section) applied to neural activity of the above clusters during the common time window confirmed the specificity of the left hemisphere for the effect of presentation, but not that of the right hemisphere for the effect of section. In conclusion, spatio-temporal dynamics of neural activity encode the temporal history of exposure to sound objects. Rapidly occurring plastic changes within the semantic representations of the left hemisphere keep track of objects heard a few seconds before, independent of the more general sound exposure history. Progressively occurring and more long-lasting plastic changes occurring predominantly within right hemispheric networks, which are known to code for perceptual, semantic and spatial aspects of sound objects, keep track of multiple exposures."
}
@article{WITTEK2015339,
title = "Extended computational kernels in a massively parallel implementation of the Trotter–Suzuki approximation",
journal = "Computer Physics Communications",
volume = "197",
pages = "339 - 340",
year = "2015",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2015.07.017",
url = "http://www.sciencedirect.com/science/article/pii/S0010465515002957",
author = "Peter Wittek and Luca Calderaro",
keywords = "Quantum evolution, Trotter–Suzuki algorithm, Parallel and distributed computing, GPU Computing",
abstract = "We extended a parallel and distributed implementation of the Trotter–Suzuki algorithm for simulating quantum systems to study a wider range of physical problems and to make the library easier to use. The new release allows periodic boundary conditions, many-body simulations of non-interacting particles, arbitrary stationary potential functions, and imaginary time evolution to approximate the ground state energy. The new release is more resilient to the computational environment: a wider range of compiler chains and more platforms are supported. To ease development, we provide a more extensive command-line interface, an application programming interface, and wrappers from high-level languages.
New version program summary
Program title: Trotter–Suzuki-MPI Catalogue identifier: AEXL_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEXL_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 14083 No. of bytes in distributed program, including test data, etc.: 110023 Distribution format: tar.gz Programming language: C++, CUDA, Python, MATLAB. Computer: x86-64. Operating system: Linux. Has the code been vectorized or parallelized?: Yes. Number of processors used: 1–64 in a single node, more in a cluster. RAM: 5 MByte-512 GBytes Journal reference of associated manuscript: Comput. Phys. Comm. 184(2013)1165 Classification: 4.12. External routines: OpenMP, MPI, CUDA Does the new version supersede the previous version?: Yes. The original version is not held in the CPC Program Library but can be obtained from https://github.com/peterwittek/trotter–suzuki-mpi Nature of problem: The evolution of a general quantum system is described by the time-dependent Schrödinger equation. The solution of this equation involves calculating a matrix exponential, which is formally simple, but computer implementations must consider several factors to achieve both high performance and high accuracy. Solution method: The Trotter–Suzuki approximation leads to an efficient algorithm for solving the time-dependent Schrödinger equation [1, 2]. The implementation uses high-performance parallel kernels in a distributed environment to maximize the computational power of this algorithm [3, 4]. Reasons for new version: The computational kernels were generalized to be able to address a much wider range of physics problems. Furthermore, the code has been modularized to make development easier, providing both a command-line and an application programming interface. High-level wrappers from Python and MATLAB provide further ease of use. Summary of revisions: 1.The implementation was generalized to include a richer variety of physics problems. The problem can have periodic boundary conditions. Many-body simulations of non-interacting particles became a possible extension. We can define an arbitrary stationary potential function. The convenience function expect_values helps to obtain expectation values.2.Imaginary time evolution was implemented to find the ground state before starting the simulation. To avoid imposing the overhead of conditional branching in the most computationally intense parts of the code, some of the core kernel functions were duplicated to include the imaginary time evolution.3.Most of the functionality is exposed through a command-line interface (CLI) for convenience. This allows specifying the files of the initial state and the potential, the parameters of the Hamiltonian, and further parameters related to the simulation, such as the computational kernel to use and the frequency at which snapshots should be written to the disk.4.The full functionality of the implementation is exposed as an application programming interface (API) through the ’trotter’ function. This allows for integrating the simulation in a larger MPI programme and it is also useful for initializing the state and the potential without having files on the disk.To demonstrate the use of the API, several examples are provided with the code.5.To further ease development, we redesigned the structure of the implementation, making it more modular. We also introduced a unit testing framework to avoid regression.6.We improved the testing of MPI dependencies by the configure script and allowed compilation without MPI. We also improved the treatment of Intel and Visual C++ compilers.We developed wrappers for Python and MATLAB for the CPU kernel for a high-level interface with the library.Restrictions: The vectorized CPU kernel must have a tile width that is divisible by two. This puts a constraint on the possible matrix sizes for this kernel. For instance, running twelve MPI threads in a 4×3 configuration, the dimensions must be divisible by six and eight. Unusual features: The library currently only supports the CPU kernel under Windows. The Python and MATLAB wrappers support the CPU and SSE kernels. Additional comments: The high-performance kernels were independently extended to study spin dynamics [5]. It remains for future work to include lattice models in this implementation. Running time: The generalization slightly altered the memory access patterns of the computational kernels, yielding performance penalty of approximately 20% compared to the previous version (Table 1). The scaling properties did not change and we see a near-optimal scaling when increasing the number of nodes. The actual running time depends on the system size and the duration to be simulated, and the computational resources. It can range from a few seconds to several days. References: [1]H. Trotter, On the product of semi-groups of operators, Proceedings of the American Mathematical Society 10 (1959) 545–551.[2]M. Suzuki, Decomposition formulas of exponential operators and Lie exponentials with some applications to quantum mechanics and statistical physics, Journal of Mathematical Physics 26 (1985) 601.[3]C. Bederián, A. Dente, Boosting quantum evolutions using Trotter–Suzuki algorithms on GPUs, in: Proceedings of HPCLatAm-11, 4th High-Performance Computing Symposium, Córdoba, Argentina, 2011.[4]P Wittek, F. Cucchietti, A second-order distributed. Trotter–Suzuki solver with a hybrid CPU–GPU kernel, Computer Physics Communications 184 (4) (2013) 1165–1171. doi:10.1016/j.cpc.2012.12.008.[5]A. D. Dente, C. S. Bederián, P R. Zangara, H.M. Pastawski, GPU accelerated Trotter–Suzuki solver for quantum spin dynamics, arXiv:1305.0036."
}
@article{SCOTTPARKER20171,
title = "Nonverbal communication during the learner lesson with a professional driving instructor: A novel investigation",
journal = "Transportation Research Part F: Traffic Psychology and Behaviour",
volume = "47",
pages = "1 - 12",
year = "2017",
issn = "1369-8478",
doi = "https://doi.org/10.1016/j.trf.2017.03.004",
url = "http://www.sciencedirect.com/science/article/pii/S136984781730178X",
author = "B. Scott-Parker",
keywords = "Young driver, Learner, Driving instruction, Nonverbal communication, Verbal communication, Driving lesson",
abstract = "Background
Fundamental for the development of the driving and road use skills of the young driver is learning to drive through driving instruction and, in graduated driver licensing programs such as in Australia, driving supervision. In Queensland young drivers are required to log a minimum of 100h supervised practice, with recent research revealing that parents provide most of this supervision. Queensland also offers young drivers a 10-h 3-for-1 bonus for professional driving instruction, such that one hour of professional instruction can be logged as three hours of practice, to a maximum of 30 logbook hours. Recent research efforts have begun to provide insight into the nature of the verbal instruction of both parents and professional instructors, and into the nonverbal communication between parents and learners. However nothing is known regarding the nonverbal communication between professional instructors and learners.
Method
Ten learner lessons (five male learners) with four professional instructors (four males) were captured via GoPro cameras. The nonverbal communication during the first, middle, and last 10min of each lesson was coded as being posture and body orientation, gestures, facial expressions, proximity, humour, and eye contact, within the context of the accompanying verbal communication according to the value of (a) eager, or (b) cautious; the valence of (a) neutral, (b) positive, or (c) negative; and the purpose of (a) rapport, or (b) communication.
Results
Overall, posture and body orientation was the most common mechanism of nonverbal communication, while facial expressions and proximity were the least common mechanisms of nonverbal communication. In general the beginning, the middle, and the end of the lessons were characterised by a plethora of neutral, cautious interactions, and positive, eager interactions. However it is noteworthy that the rates at which learners and instructors engaged in these behaviours were found to change across the lesson. Specifically learners actively communication nonverbally through mechanisms such as eye contact, facial expressions and humour, while instructors appeared to manage building rapport and communicating safe vehicle and road use through nonverbal communication such as gestures, facial expressions and posture and body orientation, summarised in a model comprising a continuum of instruction.
Discussion
While nonverbal communication is fundamental for effective verbal communication, and on occasion can replace verbal communication, and as such the professional – and the parental – driving lesson should optimise the use of nonverbal communication, at this time the optimal nature of nonverbal communication remains unknown. In addition, optimal verbal and nonverbal communication specifically suited to the driving context which involves a dynamic environment outside the vehicle, and at times a dynamic environment inside the vehicle, remains yet to be identified. The research findings provide unique insight into the nature of the nonverbal communication used by both learner drivers and professional driving instructors, in addition to the continuum of instruction model. As such, the findings provide a solid foundation for future research into, and guidance regarding, optimising the learner driving lesson."
}
@article{ADDESSI2012865,
title = "A multi-scale enriched model for the analysis of masonry panels",
journal = "International Journal of Solids and Structures",
volume = "49",
number = "6",
pages = "865 - 880",
year = "2012",
issn = "0020-7683",
doi = "https://doi.org/10.1016/j.ijsolstr.2011.12.004",
url = "http://www.sciencedirect.com/science/article/pii/S0020768311004069",
author = "Daniela Addessi and Elio Sacco",
keywords = "Multi-scale model, Masonry, Nonlinear homogenization, Damage–friction, Cosserat continuum",
abstract = "A multi-scale model for the structural analysis of the in-plane response of masonry panels, characterized by periodic arrangement of bricks and mortar, is presented. The model is based on the use of two scales: at the macroscopic level the Cosserat micropolar continuum is adopted, while at the microscopic scale the classical Cauchy medium is employed. A nonlinear constitutive law is introduced at the microscopic level, which includes damage, friction, crushing and unilateral contact effects for the mortar joints. The nonlinear homogenization is performed employing the Transformation Field Analysis (TFA) technique, properly extended to the macroscopic Cosserat continuum. A numerical procedure is developed and implemented in a Finite Element (FE) code in order to analyze some interesting structural problems. In particular, four numerical applications are presented: the first one analyzes the response of the masonry Representative Volume Element (RVE) subjected to a cyclic loading history; in the other three applications, a comparison between the numerically evaluated response and the micromechanical or experimental one is performed for some masonry panels."
}
@article{DIN2012227,
title = "Observable behavior of distributed systems: Component reasoning for concurrent objects",
journal = "The Journal of Logic and Algebraic Programming",
volume = "81",
number = "3",
pages = "227 - 256",
year = "2012",
note = "The 22nd Nordic Workshop on Programming Theory (NWPT 2010)",
issn = "1567-8326",
doi = "https://doi.org/10.1016/j.jlap.2012.01.003",
url = "http://www.sciencedirect.com/science/article/pii/S1567832612000045",
author = "Crystal Chang Din and Johan Dovland and Einar Broch Johnsen and Olaf Owe",
keywords = "Distributed systems, Object-orientation, Compositional reasoning, Hoare Logic, Concurrent objects",
abstract = "Distributed and concurrent object-oriented systems are difficult to analyze due to the complexity of their concurrency, communication, and synchronization mechanisms. Rather than performing analysis at the level of code in, e.g., Java or C++, we consider the analysis of such systems at the level of an abstract, executable modeling language. This language, based on concurrent objects communicating by asynchronous method calls, avoids some difficulties of mainstream object-oriented programming languages related to compositionality and aliasing. To facilitate system analysis, compositional verification systems are needed, which allow components to be analyzed independently of their environment. In this paper, a proof system for partial correctness reasoning is established based on communication histories and class invariants. A particular feature of our approach is that the alphabets of different objects are completely disjoint. Compared to related work, this allows the formulation of a much simpler Hoare-style proof system and reduces reasoning complexity by significantly simplifying formulas in terms of the number of needed quantifiers. The soundness and relative completeness of this proof system are shown using a transformational approach from a sequential language with a non-deterministic assignment operator."
}
@article{KOTROCZ201663,
title = "Numerical simulation of soil–cone penetrometer interaction using discrete element method",
journal = "Computers and Electronics in Agriculture",
volume = "125",
pages = "63 - 73",
year = "2016",
issn = "0168-1699",
doi = "https://doi.org/10.1016/j.compag.2016.04.023",
url = "http://www.sciencedirect.com/science/article/pii/S0168169916301545",
author = "Krisztián Kotrocz and Abdul M. Mouazen and György Kerényi",
keywords = "Discrete element method, Cone penetrometer, Soil mechanics",
abstract = "One of the most common methods to measure soil strength in-situ is cone penetrometers. In this paper the development of a three dimensional (3D) discrete element model (DEM) for the simulation of the soil–cone penetrometer interaction in a slightly cohesive loamy sand soil is presented. The aim was to investigate the effects of the soil model’s geometrical (e.g., soil model cross section shape and size and model’s height) changes on variations in the soil penetration resistance. The model area ratio and height ratio values were adopted to analyse the effects of the cross section size and the model’s height, respectively. The results of penetration resistance of the DEM simulations were compared with the in-situ measurement with a cone penetrometer of the same geometry. This comparison allowed the derivation of the contact properties between the elements. To simulate the soil material the so-called Parallel Bond and Linear Models were used in the 3D version of the Particle Flow Code (PFC) software. Finally the mechanical properties of the soil, namely the cohesion and internal friction angle were estimated by DEM simulation of direct shear box. Results showed that the penetration process can be simulated very well using the DEM. The model’s calculated penetration resistance and the corresponding in-situ measurement were in good agreement, with mean error of 14.74%. The best performing models were a rectangular model with an area ratio of 72 and a height ratio of 1.33 and a circular model with an area ratio of 32 and a height ratio of 2. The simulation output of soil material properties with direct shear box resulted in representative values of real loamy sand soils, with cohesion values range of 6.61–8.66kPa and internal friction angle values range of 41.34–41.60°. It can be concluded that the DEM can be successfully used to simulate the interaction between soil and cone penetrometers in agricultural soils."
}
@article{VERGAMINI20141481,
title = "Increase in the Incidence of Differentiated Thyroid Carcinoma in Children, Adolescents, and Young Adults: A Population-Based Study",
journal = "The Journal of Pediatrics",
volume = "164",
number = "6",
pages = "1481 - 1485",
year = "2014",
issn = "0022-3476",
doi = "https://doi.org/10.1016/j.jpeds.2014.01.059",
url = "http://www.sciencedirect.com/science/article/pii/S0022347614000870",
author = "Lucas Bonachi Vergamini and A. Lindsay Frazier and Fernanda Laurinavicius Abrantes and Karina Braga Ribeiro and Carlos Rodriguez-Galindo",
abstract = "Objective
To investigate trends in incidence of differentiated thyroid carcinomas among children and adolescents and young adults.
Study design
In this ecological time-trends study, we selected cases of differentiated thyroid carcinomas (1984-2010) in patients <30 years from Surveillance, Epidemiology, and End Results 9 cancer registries by using International Classification of Diseases for Oncology, 3rd edition, codes for papillary and follicular cancers. Patients with multiple other primary diseases before differentiated thyroid carcinomas were excluded. SEER*Stat software, version 8.0.4 (National Cancer Institute, Bethesda, Maryland) was used to calculate age-standardized rates (estimated per 1 000 000/persons) and annual percentage changes (APCs) were calculated by the Joinpoint model (Joinpoint software, version 4.0.4; National Cancer Institute).
Results
Rates ranged from 2.77 (1990) to 9.63 (2009) and from 18.35 (1987) to 50.99 (2009), for male and female subjects, respectively. A significant increasing trend in incidence was observed for both male (APC 3.44; 95% CI 2.60-4.28) and female (APC 3.81; 95% CI 3.38-4.24) patients. When a stratified analysis on the basis of tumor size was performed, significant increasing trends were noted for the following categories: <0.5 cm (females: APC 5.09, 95% CI 3.54-6.65), 0.5-0.9 cm (females: APC 8.45, 95% CI 7.09-9.82), 1.0-1.9 cm (males: APC 5.09, 95% CI 3.20-7.01; females: APC 3.42, 95% CI 2.78-4.07), and ≥2 cm (males: APC 2.62, 95% CI 1.64-3.60; females: APC 2.96, 95% CI 2.34-3.59).
Conclusions
Incidence rates for differentiated thyroid carcinomas are increasing among children and adolescents and young adults in the US. The increasing trends for larger tumors rules out diagnostic scrutiny as the only explanation for the observed results. Environmental, dietary, and genetic influences should be investigated."
}
@article{ALISSA2017282,
title = "Image-processing of time-averaged interface distributions representing CCFL characteristics in a large scale model of a PWR hot-leg pipe geometry",
journal = "Annals of Nuclear Energy",
volume = "103",
pages = "282 - 293",
year = "2017",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2017.01.021",
url = "http://www.sciencedirect.com/science/article/pii/S0306454916309847",
author = "Suleiman Al Issa and Rafael Macián-Juan",
keywords = "PWR, Hot-leg, CCFL characteristics, Time-averaged interface distribution, CFD validation data, Two-phase flows, Complex geometry, SBLOCA",
abstract = "Countercurrent Flow Limitation (CCFL) was experimentally investigated in a 1/3.9 downscaled COLLIDER facility with a 190mm pipe’s diameter using air/water at 1 atmospheric pressure. Previous investigations provided knowledge over the onset of CCFL mechanisms. In current article, CCFL characteristics at the COLLIDER facility are measured and discussed along with time-averaged distributions of the air/water interface for a selected matrix of liquid/gas velocities. The article demonstrates the time-averaged interface as a useful method to identify CCFL characteristics at quasi-stationary flow conditions eliminating variations that appears in single images, and showing essential comparative flow features such as: the degree of restriction at the bend, the extension and the intensity of the two-phase mixing zones, and the average water level within the horizontal part and the steam generator. Consequently, making it possible to compare interface distributions obtained at different investigations. The distributions are also beneficial for CFD validations of CCFL as the instant chaotic gas/liquid interface is impossible to reproduce in CFD simulations. The current study shows that final CCFL characteristics curve (and the corresponding CCFL correlation) depends upon the covered measuring range of water delivery. It also shows that a hydraulic diameter should be sufficiently larger than 50mm in order to obtain CCFL characteristics comparable to the 1:1 scale data (namely the UPTF data). Finally, the study shows that the change of the flow condition inside the hot-leg is not only related to the water and air inlet velocities, but is also dependent upon the existent interface distribution within the hot-leg, and that several CCFL cases of identical inlet flow conditions can exist with different interface distribution and pressure difference. The last result is of a special importance to the investigation of this phenomenon during SBLOCA accidents, since the entire phenomenon is driven by pressure difference between the steam generator and reactor vessel, as well as by gravity. This result show also that CCFL characteristics cannot be investigated using 1D codes, as the interface distribution within the hot-leg during a SBLOCA accident will depend upon flow history or previous interface distribution. Current investigations support the effort to provide more knowledge over CCFL in order to extrapolate results obtained in downscaled models into the 1:1 scale."
}
@article{CASTILLARHO2015305,
title = "An agent-based platform for simulating complex human–aquifer interactions in managed groundwater systems",
journal = "Environmental Modelling & Software",
volume = "73",
pages = "305 - 323",
year = "2015",
issn = "1364-8152",
doi = "https://doi.org/10.1016/j.envsoft.2015.08.018",
url = "http://www.sciencedirect.com/science/article/pii/S136481521530044X",
author = "J.C. Castilla-Rho and G. Mariethoz and R. Rojas and M.S. Andersen and B.F.J. Kelly",
keywords = "Agent-based models, NetLogo, Groundwater management, Social simulation, Complexity, Participatory modelling",
abstract = "This paper presents and illustrates FlowLogo, an interactive modelling environment for developing coupled agent-based groundwater models (GW-ABMs). It allows users to simulate complex socio-environmental couplings in groundwater systems, and to explore how desirable patterns of groundwater and social development can emerge from agent behaviours and interactions. GW-ABMs can be developed using a single piece of software, addressing common issues around data transfer and model analyses that arise when linking ABMs to existing groundwater codes. FlowLogo is based on a 2D finite-difference solution of the governing groundwater flow equations and a set of procedures to represent the most common types of stresses and boundary conditions of regional aquifer flow. The platform is illustrated using a synthetic example of an expanding agricultural region that depends on groundwater for irrigation. The implementation and analysis of scenarios from this example highlight the possibility to: (i) deploy agents at multiple scales of decision-making (farmers, waterworks, institutions), (ii) model feedbacks between agent behaviours and groundwater dynamics, and (iii) perform sensitivity and multi-realisation analyses on social and physical factors. The FlowLogo interface allows interactively changing parameters using ‘tuneable’ dials, which can adjust agent decisions and policy levers during simulations. This flexibility allows for live interaction with audiences (role-plays), in participatory workshops, public meetings, and as part of learning activities in classrooms. FlowLogo's interactive features and ease of use aim to facilitate the wider dissemination and independent validation of GW-ABMs."
}
@article{NASONOVA2013389,
title = "Analysis on premixed combustion of H2–CH4 mixed fuels for SiO2 particle preparation",
journal = "Chemical Engineering Journal",
volume = "215-216",
pages = "389 - 395",
year = "2013",
issn = "1385-8947",
doi = "https://doi.org/10.1016/j.cej.2012.11.021",
url = "http://www.sciencedirect.com/science/article/pii/S1385894712014192",
author = "Anna Nasonova and Kyo-Seon Kim",
keywords = "Premixed combustion, Mixed H and CH fuels, Preparation of SiO particles, Particle trajectories, Temperature histories of particles",
abstract = "The combustion of premixed H2 and CH4 fuels to prepare SiO2 nanoparticles was analyzed numerically. The commercial CFD-code Fluent was used to calculate the profiles of gas velocities, temperature, species concentrations and reaction rates for various process conditions in the premixed flame reactor. To understand the effect of CH4 addition to H2 fuel, the CH4 mole fraction was changed from 0% to 10%, while keeping the H2 concentration constant. With the increase of CH4 concentration, flame temperature increases, which will affect the properties of SiO2 particles prepared in premixed flame reactor. Using the data of gas temperatures and velocities extracted from Fluent, the trajectories and temperature histories of SiO2 particles were calculated inside the premixed flame reactor. This study shows that particles starting at different initial positions move inside the reactor with different particle trajectories and have different temperature histories, and the properties of SiO2 nanoparticles prepared in the premixed flame reactor will depend on those process variables significantly."
}
@article{LIN20141322,
title = "Test suite reduction methods that decrease regression testing costs by identifying irreplaceable tests",
journal = "Information and Software Technology",
volume = "56",
number = "10",
pages = "1322 - 1344",
year = "2014",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2014.04.013",
url = "http://www.sciencedirect.com/science/article/pii/S0950584914000949",
author = "Chu-Ti Lin and Kai-Wei Tang and Gregory M. Kapfhammer",
keywords = "Software testing, Regression testing, Test suite reduction, Code coverage, Test irreplaceability",
abstract = "Context
In software development and maintenance, a software system may frequently be updated to meet rapidly changing user requirements. New test cases will be designed to ensure the correctness of new or modified functions, thus gradually increasing the test suite’s size. Test suite reduction techniques aim to decrease the cost of regression testing by removing the redundant test cases from the test suite and then obtaining a representative set of test cases that still yield a high level of code coverage.
Objective
Most of the existing reduction algorithms focus on decreasing the test suite’s size. Yet, the differences in execution costs among test cases are usually significant and it may take a lot of execution time to run a test suite consisting of a few long-running test cases. This paper presents and empirically evaluates cost-aware algorithms that can produce the representative sets with lower execution costs.
Method
We first use a cost-aware test case metric, called Irreplaceability, and its enhanced version, called EIrreplaceability, to evaluate the possibility that each test case can be replaced by others during test suite reduction. Furthermore, we construct a cost-aware framework that incorporates the concept of test irreplaceability into some well-known test suite reduction algorithms.
Results
The effectiveness of the cost-aware framework is evaluated via the subject programs and test suites collected from the Software-artifact Infrastructure Repository — frequently chosen benchmarks for experimentally evaluating test suite reduction methods. The empirical results reveal that the presented algorithms produce representative sets that normally incur a low cost to yield a high level of test coverage.
Conclusion
The presented techniques indeed enhance the capability of the traditional reduction algorithms to reduce the execution cost of a test suite. Especially for the additional Greedy algorithm, the presented techniques decrease the costs of the representative sets by 8.10–46.57%."
}
@article{KUCHTA2015454,
title = "Despinning and shape evolution of Saturn’s moon Iapetus triggered by a giant impact",
journal = "Icarus",
volume = "252",
pages = "454 - 465",
year = "2015",
issn = "0019-1035",
doi = "https://doi.org/10.1016/j.icarus.2015.02.010",
url = "http://www.sciencedirect.com/science/article/pii/S0019103515000627",
author = "Miroslav Kuchta and Gabriel Tobie and Katarina Miljković and Marie Běhounková and Ondřej Souček and Gaël Choblet and Ondřej Čadek",
keywords = "Iapetus, Ices, Tides, solid body, Rotational dynamics, Thermal histories",
abstract = "Iapetus possesses two spectacular characteristics: (i) a high equatorial ridge which is unique in the Solar System and (ii) a large flattening (a-c=34km) inconsistent with its current spin rate. These two main characteristics have probably been acquired in Iapetus’ early past as a consequence of coupled interior-rotation evolution. Previous models have suggested that rapid despinning may result either from enhanced internal dissipation due to short-lived radioactive elements or from interactions with a sub-satellite resulting from a giant impact. For the ridge formation, different exogenic and endogenic hypotheses have also been proposed, but most of the proposed scenarios have not been tested numerically. In order to model simultaneously internal heat transfer, tidal despinning and shape evolution, we have developed a two-dimensional axisymmetric thermal convection code with a deformable surface boundary, coupled with a viscoelastic code for tidal dissipation. The model includes centrifugal and buoyancy forces, a composite non-linear viscous rheology as well as an Andrade rheology for the dissipative part. By considering realistic rheological properties and by exploring various grain size values, we show that, in the absence of additional external interactions, despinning of a fast rotating Iapetus is impossible even for warm initial conditions (T>250K). Alternatively, the impact of a single body with a radius of 250–350km at a velocity of 2km/s may be sufficient to slow down the rotation from a period of 6–10h to more than 30h. By combining despinning due to internal dissipation and an abrupt change of rotation due to a giant impact, we determined the parameters leading to a complete despinning and we computed the corresponding shape evolution. We show that stresses arising from shape change affect the viscosity structure by enhancing dislocation creep and can lead to the formation of a large-scale ridge at the equator as a result of rapid rotation change for initial rotation periods of 6h."
}
@article{HEIM2013990,
title = "Early gamma oscillations during rapid auditory processing in children with a language-learning impairment: Changes in neural mass activity after training",
journal = "Neuropsychologia",
volume = "51",
number = "5",
pages = "990 - 1001",
year = "2013",
issn = "0028-3932",
doi = "https://doi.org/10.1016/j.neuropsychologia.2013.01.011",
url = "http://www.sciencedirect.com/science/article/pii/S0028393213000171",
author = "Sabine Heim and Andreas Keil and Naseem Choudhury and Jennifer Thomas Friedman and April A. Benasich",
keywords = "Auditory temporal processing, Computerized training, Electroencephalography, Phase locking, Specific language impairment, Spectral power",
abstract = "Children with language-learning impairment (LLI) have consistently shown difficulty with tasks requiring precise, rapid auditory processing. Remediation based on neural plasticity assumes that the temporal precision of neural coding can be improved by intensive training protocols. Here, we examined the extent to which early oscillatory responses in auditory cortex change after audio-visual training, using combined source modeling and time-frequency analysis of the human electroencephalogram (EEG). Twenty-one elementary school students diagnosed with LLI underwent the intervention for an average of 32 days. Pre- and post-training assessments included standardized language/literacy tests and EEG recordings in response to fast-rate tone doublets. Twelve children with typical language development were also tested twice, with no intervention given. Behaviorally, improvements on measures of language were observed in the LLI group following completion of training. During the first EEG assessment, we found reduced amplitude and phase-locking of early (45–75ms) oscillations in the gamma-band range (29–52Hz), specifically in the LLI group, for the second stimulus of the tone doublet. Amplitude reduction for the second tone was no longer evident for the LLI children post-intervention, although these children still exhibited attenuated phase-locking. Our findings suggest that specific aspects of inefficient sensory cortical processing in LLI are ameliorated after training."
}
@article{KOMMER2015263,
title = "Numerical modeling of flow boiling instabilities using TRACE",
journal = "Annals of Nuclear Energy",
volume = "76",
pages = "263 - 270",
year = "2015",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2014.09.052",
url = "http://www.sciencedirect.com/science/article/pii/S0306454914005313",
author = "Eric M. Kommer",
keywords = "Flow instability, Numerical modeling, Next generation reactor, TRACE, RELAP",
abstract = "Dynamic flow instabilities in two-phase systems are a vitally important area of study due to their effects on a great number of industrial applications, including heat exchangers in nuclear power plants. Several next generation nuclear reactor designs incorporate once through steam generators which will exhibit boiling flow instabilities if not properly designed or when operated outside design limits. A number of numerical thermal hydraulic codes attempt to model instabilities for initial design and for use in accident analysis. TRACE, the Nuclear Regulatory Commission’s newest thermal hydraulic code is used in this study to investigate flow instabilities in both single and dual parallel channel configurations. The model parameters are selected as to replicate other investigators’ experimental and numerical work in order to provide easy comparison. Particular attention is paid to the similarities between analysis using TRACE Version 5.0 and RELAP5/MOD3.3. Comparison of results is accomplished via flow stability maps non-dimensionalized via the phase change and subcooling numbers. Results of this study show that TRACE does indeed model two phase flow instabilities, with the transient response closely mimicking that seen in experimental studies. When compared to flow stability maps generated using RELAP, TRACE shows similar results with differences likely due to the somewhat qualitative criteria used by various authors to determine when the flow is truly unstable."
}
@article{FRANCISCO20142663,
title = "Electron number distribution functions from molecular wavefunctions. Version 2",
journal = "Computer Physics Communications",
volume = "185",
number = "10",
pages = "2663 - 2682",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2014.05.009",
url = "http://www.sciencedirect.com/science/article/pii/S0010465514001635",
author = "E. Francisco and A. Martín Pendás",
keywords = "Quantum theory of atoms in molecules, Electron probability distribution, Molecular wave function, Chemical bonding theory",
abstract = "We present in this article a new and considerably faster version of the edf Fortran 77/90 code that replaces the old one (Francisco et al., 2008). In the new version, given an N-electron molecule and an exhaustive, fuzzy, or orbital-based partition of the physical space R3 into m domains, the probabilities p(S) of all possible distributions S={n1,n2,…,nm} of the N electrons (n1+n2+⋯+nm=N) into m real space domains are computed. The set {p(S)} defines the electron number distribution function (EDF) of the molecule for this specific space partition. The molecule may be described by either a single- or a multi-determinant wavefunction Ψ(1,N). Both spin-resolved and spin-unresolved EDFs are determined. Isopycnic orbital localizations of the natural molecular orbitals (MOs) can be optionally performed to make the use of the core approximation possible. This explicitly eliminates from the calculation those MOs strongly that are localized over one of the m domains, considerably speeding up the process. An optional approximation consisting of assuming that localized MOs are orthogonal to each other in all the domains is shown to give reasonably accurate results and further accelerates the calculation. The new edf code does also allows for the computation of a single probability p(n1,n2,…,nm) instead of the full EDF. Finally, this new version computes multiple-domain covariances of electron populations, particularly relevant for chemical bonding theory.
Program summary/new version program summary
Program title: edf Catalogue identifier: AEAJ_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEAJ_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 16847 No. of bytes in distributed program, including test data, etc.: 206274 Distribution format: tar.gz Programming language: Fortran 77/90. Computer: 2.80 GHz Intel Pentium IV CPU. Operating system: GNU/Linux. RAM: Dynamic Classification: 2.7. External routines: mkl Does the new version supersede the previous version?: Yes Catalogue identifier of the previous version: AEAJ_v1_0 Journal reference of the previous version: Comput. Phys. Comm. 178 (2008) 621 Nature of problem: Given an N-electron molecule described by a single- or multi-determinant wavefunction Ψ(1,N), and a partition of the physical space R3 into m domains Ω1,Ω2,…,Ωm, edf computes the probabilities p(S) of having exactly n1,n2,…, and nm electrons in Ω1,Ω2,…,, and Ωm, respectively, for all possible distributions S≡{n1,n2,…,nm}, being n1,n2,…, and nm integer numbers. Solution method: Given a wavefunction Ψ(1,N)=∑rMcrΨr(1,N), where the Ψr’s are Slater determinants Ψr=det[X1r,…,XNr], and calling (Skrs)ij the overlap integral within the domain Ωk between Xir and Xjs, edf finds all the p(S)’s using a three-step procedure: (1)For each (r,s) pair, solve the linear system ∑{nσi}t1n1σt2n2σ…tmnmσprs({niσ})=det[∑k=1mtkSkrs] in the unknowns prs(niσ), where σ=(α,β), {nσi}(i=1,2,…,m) are the integer electronic populations with spin σ of the domains Ω1,Ω2,…,Ωm, tm=1, and t1,…,tm−1 are arbitrary real numbers,(2)Compute the spin-resolved probabilities p({nα;nβ})≡p({ip})=∑r,sMcrcsprs({niα})prs({niβ}), and(3)obtain the p(S)’s by adding up the p({ip})’s with niα+niβ=niReasons for new version: Dynamic memory allocation instead of static memory allocation is used throughout. Further partitions of the 3D space have been added. Thanks to the change in the algorithm used to solve the problem, the new version is 1–2 orders of magnitude faster than the previous one and can deal with molecules having a greater number of electrons. Approximate calculations as well as exact ones are possible in the new version by making use of the core–valence separability. Summary of revisions: Most data structures are stored in dynamic memory. The basic algorithm has been changed to ensure a much faster computation of the probabilities p(S). Algorithms to obtain the latter in an approximate manner, as well as using different partitions of the 3D space have been included. Restrictions: The number of {niσ} sets in Eq. 1, i.e. the dimension of the linear system to be solved, grows very fast with m and N. This dimension is much smaller than in the previous version of edf but, even so, this restricts the applicability of the method to relatively small systems, unless some drastic approximations are used (excluding, for instance, a large part of the electrons of the system from the calculation). Running time: 0.016 and 0.004 s for the test examples 1 and 2, respectively. However, running times are very variable depending on the molecule, the type of the wavefunction (single- or multi-determinant), the number of fragments (m). etc. References: [1] E. Francisco, A. Martín Pendás, and M.A. Blanco. J. Chem. Phys. 126, 094102 (2007). [2] A. Martín Pendás, E. Francisco, and M.A. Blanco. J. Chem. Phys. 127, 144103 (2007). [3] E. Francisco, A. Martín Pendás, and M.A. Blanco. Computer Physics Commun. 178, 621–634 (2008). [4] A. Martín Pendás, E. Francisco, and M.A. Blanco. Faraday Discuss. 135, 423–438 (2007). [5] A. Martín Pendás, E. Francisco, M.A. Blanco, and C. Gatti. Chemistry: A European Journal. 13, 9362–9371 (2007). [6] A. Martín Pendás, E. Francisco, and M.A. Blanco. Phys. Chem. Chem. Phys. 9, 1087–1092 (2007). [7] E. Francisco, A. Martín Pendás, and M.A. Blanco. J. Chem. Phys. 131, 124125 (2009). [8] E. Francisco, A. Martín Pendás, and M.A. Blanco. Theor. Chemistry Accounts. 128, 433 (2011). [9] E. Francisco, A. Martín Pendás, A. Costales, and M.A. García-Revilla. Comput. Theor. Chem. 975, 2–8 (2011). [10] M.A. García-Revilla, E. Francisco, A. Martín Pendás, J.M. Recio, M. Bartolomei, M.I. Hernández, J. Campos-Martínez, E. Carmona-Novillo, and R. Hernández-Lamoneda. J. Chem. Theory Comput. 9, 2179–2188 (2013)."
}
@article{HUANG2014163,
title = "Dramatic increases in number of cerebellar granule-cell-Purkinje-cell synapses across several mammals",
journal = "Mammalian Biology",
volume = "79",
number = "3",
pages = "163 - 169",
year = "2014",
issn = "1616-5047",
doi = "https://doi.org/10.1016/j.mambio.2013.12.003",
url = "http://www.sciencedirect.com/science/article/pii/S1616504713002371",
author = "Chiming Huang and Samantha J. Gammon and Michael Dieterle and Rosa H. Huang and Lee Likins and R.E. Ricklefs",
keywords = "Cerebellum, Brain evolution, Parallel fiber",
abstract = "The classical comparative literature on mammalian brain evolution has mainly focused on brain mass measurements because larger brains are more likely to have more neurons to process information. The phylogenetic expansion in the mass of the cerebellum is as significant as that of the cerebral cortex. The synapse, however, has recently been recognized as the basic unit of neuronal information processing, including neuroplasticity. Here we hypothesize significant absolute and relative increases in the functionally important granule-cell-Purkinje-cell (gcPc) synapses as a salient feature of the evolving cerebellum. To probe evolutionary constraints, we define the gcPc circuitry with ten degrees of freedom, including number of granule cells, Purkinje cells, lengths of the granule cell axonal segments, linear densities of synapses along them, and physical dimensions of Purkinje as well as granule cell dendritic structures. We show that although only two of the ten parameters are not constrained and therefore can exhibit independent, comparative changes, there is a dramatic increase in the number of gcPc synapses from the rodent to the human cerebellum. By assigning a value of unity for the mouse, the ratio of the number of gcPc synapses from mouse, rat, cat, non-human primate, and human is 1:5.5:236:620:20,000, which greatly exceeds the ratio of increase in cerebellar mass (1:6:48:180:3000). Dramatic changes in the number of gcPc synapses can therefore occur despite evolutionary constraints and only modest changes in parameters of the neuronal circuitry. Increases in the number of gcPc synapses have important functional consequences as these synapses enhance the capacity of the cerebellum to code and process information, which directly impact memory and learning in both motor and non-motor tasks."
}
@article{LESLIE2013S91,
title = "What Influences Changing Practice to Evidence‐Based Care in Individual Providers?",
journal = "Journal of Obstetric, Gynecologic & Neonatal Nursing",
volume = "42",
pages = "S91",
year = "2013",
note = "Proceedings of the AWHONN 2013 Convention",
issn = "0884-2175",
doi = "https://doi.org/10.1111/1552-6909.12184",
url = "http://www.sciencedirect.com/science/article/pii/S0884217515314027",
author = "Mayri Sagady Leslie",
keywords = "practice change, provider change, medical decision making, evidence‐based care, nurse–midwifery, improving maternity care",
abstract = "Paper Presentation
Objective
To explore the experiences of maternity care professionals (midwives and physicians) who changed a practice to one with more scientific support. For the purpose of having the providers share a common phenomenon, the change from early to delayed cord clamping was chosen.
Design
This qualitative study was conducted using a constructivist, grounded theory approach.
Setting
Participants were interviewed by phone. Practice settings ranged from home birth to freestanding and in‐hospital birth centers to the hospital, which was the most common setting with eight of the 17 subjects providing care there.
Sample
The sample was made up of 17 providers (12 midwives and five physicians) from throughout the United States. Participants were in active clinical practice or had been within the past 3 years. They practiced early cord clamping for at least 6 months, changed to delayed cord clamping, and then practiced that for at least 6 months.
Methods
Participant interviews were recorded with permission and then transcribed. Coding and data management was conducted using ATLAS.ti Version6.1. The study was approved by the Institutional Review Board of George Washington University with multiple steps taken to protect the participants during analysis.
Results
Five emergent themes acting as drivers of change included trusting colleagues, believing the evidence, honoring mothers and families, knowing with personal certainty, and protecting the integrity of the mother and the baby. Three domains of influence developed from the background of the stories of change: personal, professional, and institutional. From the findings, the Evolution to Provider Change Model was developed.
Conclusion/Implications for Nursing Practice
In maternity care, a number of current practices are not evidence based while many of those that are supported by evidence remain underutilized. Studies investigating efforts to incorporate evidence into practice reveal that at the organizational level, many initiatives are unsuccessful. Research into provider change at the level of the individual has been sparse in all medical fields. This study provides new understanding about how individual maternity care professionals change practice. A new theoretical model is proposed that may be used in future nursing research on improving practice. Focusing on individual versus group change and on learning from those who have successfully changed practice toward more evidence‐based care offers fertile ground for further study."
}
@article{VIZZINI2017107,
title = "Evolution of Ciona intestinalis Tumor necrosis factor alpha (CiTNFα): Polymorphism, tissues expression, and 3D modeling",
journal = "Developmental & Comparative Immunology",
volume = "67",
pages = "107 - 116",
year = "2017",
issn = "0145-305X",
doi = "https://doi.org/10.1016/j.dci.2016.11.005",
url = "http://www.sciencedirect.com/science/article/pii/S0145305X16303974",
author = "Aiti Vizzini and Maria Giovanna Parisi and Laura Cardinale and Lelia Testasecca and Matteo Cammarata",
keywords = "TNF, , DGGE, Polymorphism, Gene expression",
abstract = "Although the Tumor necrosis factor gene superfamily seems to be very conserved in vertebrates, phylogeny, tissue expression, genomic and gene organization, protein domains and polymorphism analyses showed that a strong change has happened mostly in invertebrates in which protochordates were a constraint during the immune-molecules history and evolution. RT PCR was used to investigate differential gene expression in different tissues. The expression shown was greater in the pharynx. Single-nucleotide polymorphism has been investigated in Ciona intestinalis Tumor necrosis factor alpha (CiTNFα) mRNA isolated from the pharynx of 30 ascidians collected from Licata, Sicily (Italy), by denaturing gradient gel electrophoresis (DGGE). For this analysis, CiTNFα nucleotide sequence was separated into two fragments, TNF-1 and -2, respectively, of 630 and 540 bp. We defined 23 individual DGGE patterns (named 1 to 10 for TNF-1 and 1 to 13 for TNF-2). Five patterns for TNF-1 accounted for <10% of the individuals, whereas the pattern 13 of TNF-2 accounted for >20% of the individuals. All the patterns were verified by direct sequencing. Single base-pair mutations were observed mainly within COOH-terminus, leading to 30 nucleotide sequence variants and 30 different coding sequences segregating in two main different clusters. Although most of the base mutations were silent, four propeptide variants were detected and six amino acid replacements occurred within COOH-terminus. Statistical tests for neutrality indicated negative selection pressure on signal and mature peptide domains, but possible positive selection pressure on COOH-terminus domain. Lastly we displayed the in silico 3D structure analysis including the CiTNFα variable region."
}
@article{CABRERIZO2017412,
title = "ParaTrough v1.0: Librería en Modelica para Simulación de Plantas Termosolares",
journal = "Revista Iberoamericana de Automática e Informática Industrial RIAI",
volume = "14",
number = "4",
pages = "412 - 423",
year = "2017",
issn = "1697-7912",
doi = "https://doi.org/10.1016/j.riai.2017.06.005",
url = "http://www.sciencedirect.com/science/article/pii/S1697791217300481",
author = "Juan A. Romera Cabrerizo and Matilde Santos",
keywords = "Modelado, simulación, planta termosolar, colector cilindro-parabólico, Modelica, energía renovable, Modeling, simulation, solar thermal plant, parabolic trough power plants, Modelica, renewable energy",
abstract = "Resumen
El presente trabajo describe una librería desarrollada en Modelica que utiliza el entorno Dymola 6.1 para modelar y simular plantas termosolares de tecnología de colector cilindro-parabólico. El actual software de modelado y simulación es cada vez más potente gracias a los avances en computación y programación, pudiendo conseguir estimaciones muy precisas del comportamiento de estas plantas térmicas. Como mejora a otras propuestas actuales, la librería ParaTrough se ofrece como una herramienta pública, gratuita bajo licencia Modelica License 2, de código libre, flexible, modular, y por lo tanto fácilmente ampliable y modificable para los requerimientos específicos de cada planta y proceso en particular. En la versión 1.0 contemplada en este artículo, esta librería se puede usar para el modelado y simulación del recurso solar y del sistema de fluido de transferencia calorífica sin cambio de fase. Los modelos han sido validados con datos reales de una planta en operación, Andasol 3, en los términos municipales de Aldeire y La Calahorra (Granada). El objetivo de ParaTrough es poder ser utilizada gratuitamente y de forma amigable por analistas de procesos para uno o varios de los siguientes casos: evaluación del rendimiento, detección de fallos, exploración de nuevos modos de operación y optimización de la planta. Aunque en futuras versiones se puedan añadir otros elementos, esta aportación cubre una nueva área de aplicación específica para el software de Modelica y en su estado actual facilita la operación y mantenimiento de estas plantas.
This paper describes a Modelica-based library developed to the modeling and simulation of solar thermal plants with parabolic trough collectors. The Dymola 6.1 environment has been used. Unlike other commercial tools, the ParaTrough library is offered as a free open source tool, under Modelica License 2. Its modular code makes it easily extensible and modifiable to the requirements of each plant and process in particular. In its current version 1.0, this library can be used for modeling and simulating the solar resource and the heat transfer fluid without phase change. The models have been validated with real data of an operating plant. ParaTrough can be freely used by process analysts for one or more of the following cases: performance assessment, fault detection, exploring new operation modes and plant optimization. While other elements can be added in future extensions, this contribution covers a new specific application area of Modelica and in its current state it facilitates the operation and maintenance of parabolic trough power plants."
}
@article{BONHOMMEAU20141188,
title = "MCMC2 (version 1.1): A Monte Carlo code for multiply-charged clusters",
journal = "Computer Physics Communications",
volume = "185",
number = "3",
pages = "1188 - 1191",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.09.026",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513003305",
author = "David A. Bonhommeau and Marius Lewerenz and Marie-Pierre Gaigeot",
keywords = "Monte Carlo simulations, Coarse-grained models, Charged clusters, Charged droplets, Electrospray ionisation, Parallel Tempering, Parallel Charging",
abstract = "This new version of the MCMC2 program for modeling the thermodynamic and structural properties of multiply-charged clusters by means of parallel classical Monte Carlo methods provides some enhancements and corrections to the earlier version [1]. In particular, histograms for negatively and positively charged particles are separated, parallel Monte Carlo simulations can be performed by attempting exchanges between all the replica pairs and not only one randomly chosen pair, a new random number generator is supplied, and the contribution of Coulomb repulsion to the total heat capacity is corrected. The main functionalities of the original MCMC2 code (e.g., potential-energy surfaces and Monte Carlo algorithms) have not been modified.
New version program summary
Program title: MCMC2. Catalogue identifier: AENZ_v1_1 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AENZ_v1_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland. Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html. No. of lines in distributed program, including test data, etc.: 148028 No. of bytes in distributed program, including test data, etc.: 1501936 Distribution format: tar.gz Programming language: Fortran 90 with MPI extensions for parallelization Computers: x86 and IBM platforms Operating system: 1.CentOS 5.6 Intel Xeon X5670 2.93 GHz, gfortran/ifort(version 13.1.0) + MPICH22.CentOS 5.3 Intel Xeon E5520 2.27 GHz, gfortran/g95/pgf90 + MPICH23.Red Hat Enterprise 5.3 Intel Xeon X5650 2.67 GHz, gfortran + IntelMPI4.IBM Power 6 4.7 GHz, xlf + PESS (IBM parallel library) Has the code been vectorised or parallelized?: Yes, parallelized using MPI extensions. Number of CPUs used: up to 999. RAM (per CPU core): 10–20 MB. The physical memory needed for the simulation depends on the cluster size, the values indicated are typical for small clusters (N≤300−400). Classification: 23 Catalogue identifier of previous version: AENZ_v1_0 Journal reference of previous version: Comput. Phys. Comm. 184 (2013) 873 Nature of problem: We provide a general parallel code to investigate structural and thermodynamic properties of multiply-charged clusters. Solution method: Parallel Monte Carlo methods are implemented for the exploration of the configuration space of multiply-charged clusters. Two parallel Monte Carlo methods were found appropriate to achieve such a goal: the Parallel Tempering method, where replicas of the same cluster at different temperatures are distributed among different CPUs, and Parallel Charging where replicas (at the same temperature) having different particle charges or numbers of charged particles are distributed on different CPUs. Restrictions: The current version of the code uses Lennard-Jones interactions, as the main cohesive interaction between spherical particles, and electrostatic interactions (charge–charge, charge-induced dipole, induced dipole–induced dipole, polarisation). The Monte Carlo simulations can only be performed in the NVT ensemble in the present code. Unusual features: The Parallel Charging methods, based on the same philosophy as Parallel Tempering but with particle charges and number of charged particles as parameters instead of temperature, is an interesting new approach to explore energy landscapes. Splitting of the simulations is allowed and averages are accordingly updated. Running time: The running time depends on the number of Monte Carlo steps, cluster size, and the type of interactions selected (e.g., polarisation turned on or off, and method used for calculating the induced dipoles). Typically a complete simulation can last from a few tens of minutes or a few hours for small clusters (N≤100, not including polarisation interactions), to one week for large clusters (N≥1000 not including polarisation interactions), and several weeks for large clusters (N≥1000) when including polarisation interactions. A restart procedure has been implemented that enables a splitting of the simulation accumulation phase. Reasons for new version: The new version corrects some bugs identified in the previous version. It also provides the user with some new functionalities such as the separation of histograms for positively and negatively charged particles, a new scheme to perform parallel Monte Carlo simulations and a new random number generator. Summary of revisions1.Additional features of MCMC2 version 1.1 (a)Histograms for positively and negatively charged particles. The first version of MCMC2 was able to produce a large variety of histograms to investigate the structure of charged clusters and their propensity to undergo evaporation: angular histograms for charged particles (“angdist-yyy.dat” and “surfang-yyy.dat” files), radial histograms (“rhist-x-yyy.dat” files), and histograms for tracking the number of charged surface particles (“surfnb-yyy.dat” files) or the number of particles that tend to evaporate (“evapnb-yyy.dat” files). Although the program could handle clusters composed of both positively and negatively charged particles, these histograms did not separate these two classes of particles. This has been corrected by the addition of two columns in the histogram files. These columns are labelled with signs “+” and “−” that refer to positively and negatively charged particles, respectively. The study of clusters composed of both positively and negatively charged particles should therefore be made easier [2]. Most of the keywords corresponding to histograms have not been changed except the keyword for radial histograms that now includes the possibility not to print any histogram (see keyword “RADIAL” in the “Modified keywords” section).(b)Full replica pair exchange in Monte Carlo simulations. In previous publications [2, 3] the exchange between replica configurations was based on two steps: the random selection of one replica pair and the calculation of the Parallel Tempering or Parallel Charging criteria to accept or reject the exchange of configurations between replicas. By default these exchanges were attempted every ns=10 Monte Carlo sweeps, where a sweep is composed of N Monte Carlo steps for an N-particle cluster. The main goal of parallel Monte Carlo algorithms is to improve the convergence speed and we might thus be interested in attempting more than one exchange of replica configurations every ns sweeps, provided that ns is large enough for the final configuration (after the ns sweeps) to be decorrelated from the initial configuration (before the ns sweeps). In the present version of the code we have implemented a second way to perform parallel Monte Carlo simulations where N/2 exchanges of replica configurations are attempted every ns sweeps by alternatively selecting odd pairs (i.e., pairs involving replicas 1–2, 3–4, etc.) and even pairs (i.e., pairs involving replicas 2–3, 4–5, etc.). The two methods should be equivalent for large numbers of sweeps although the second method proposed in the present version is deemed to converge faster than the method based on random choices of replica pairs. Modifications brought to keywords “PC” and “PT” are reported in the “Modified keywords” section.(c)Lagged Fibonacci random number generator. Random numbers are generated by means of a LAPACK random number generator in the first version of MCMC2  [1]. Knowing that the main goal of MCMC2 is to handle Monte Carlo simulations on large clusters (from hundreds up to thousands of particles at least), we were particularly concerned in delivering a random number generator already thoroughly tested on neutral or charged clusters for such large systems. One of us (ML) developed a lagged Fibonacci random number generator based on the works by Kirkpatrick et al. [4] and Bhanot et al. [5]. In particular, this random number generator was successfully used in accurate diffusion quantum Monte Carlo investigations of the structure and energetics of small helium clusters [6], a benchmark convergence study of the dissociation energy of the HF dimer [7], and the fragmentation dynamics of ionized doped helium clusters [8, 9]. Modifications brought to keyword “SEED” are reported in the “Modified keywords” section. As an example, heat capacity curves of neutral A100 clusters obtained after performing Parallel Tempering simulations with the two random number generators used in MCMC2 are plotted in Fig. 1 of Supplementary materials. We can notice that the melting peak is perfectly defined by all four methods, some small deviations may occur for the premelting peak whose convergence is harder to achieve.2.Modifications or corrections to MCMC2 version 1.0 (a)In MCMC2 (version 1.0) we have computed the heat capacity related to Coulomb energy fluctuations that we have improperly called the “Coulomb part of heat capacity”. Indeed, when defining the potential energy U of a charged cluster as the sum of the Lennard-Jones (LJ) interactions V and the Coulomb interactions Vc we can define several quantities:Total heat capacity,  CV:  CV=CVCoul+CVLJ=1kBT2(〈U2〉−〈U〉2)Coulomb contribution to  CV:  CVCoul=1kBT2(〈UVc〉−〈U〉〈Vc〉)LJ contribution to  CV:  CVLJ=1kBT2(〈UV〉−〈U〉〈V〉)Coulomb heat capacity:  CV,fluctCoul=1kBT2(〈Vc2〉−〈Vc〉2)LJ heat capacity:CV,fluctLJ=1kBT2(〈V2〉−〈V〉2) where we call “Coulomb heat capacity” and “LJ heat capacity” the heat capacities that are obtained by calculating the fluctuations of Coulomb and LJ interactions, respectively. After some calculations, we can find two formulas to express CV as a function of CVCoul, CVLJ, CV,fluctCoul and CV,fluctLJ: CV=2CVCoul−CV,fluctCoul+CV,fluctLJCV=2CVLJ+CV,fluctCoul−CV,fluctLJ which leads to (1)CV,fluctCoul−CV,fluctLJ=CVCoul−CVLJ. The difference between Coulomb and LJ heat capacities thus matches the difference between the LJ and Coulomb contributions to heat capacity. However, only the latter quantities have a clear physical meaning for charged clusters bound by both LJ and Coulomb interactions and we have therefore replaced the calculation of CV,fluctCoul by CVCoul in the present version of MCMC2. Note that the contribution of polarisation energy to heat capacity has also been added when polarisation is included.(b)Several minor corrections were brought into version 1.1: i.The word “RMS” (that usually stands for Root Mean Square) was wrongly written several times in the MCMC2-yyy.out output files instead of “standard deviation” that is abbreviated “std dev.”. This is corrected in the present version of the code.ii.The syntax used to generate some file names seemed not to be recognized by recent pgf95 compilers and has been modified. This does not affect the user since file names have not been modified.iii.The default minimum temperature for the geometric temperature scale is set to 10−10 instead of 0 that would have led to divergence if not changed by the user.iv.Test cases are slightly modified to take into account the new features of version 1.1 and the README.txt files are more detailed. The START folders and the obsolete open PBS scripts are removed from the distribution.3.Modified keywordsWe present in this section a list of the modified keywords. •PC METHOD imcpc REPLICAS N_repc EVERY pc_every TEMPERATURE kt0 QREF q_ref: Setting of parameters for running MCPC simulations. imcpc is an integer dedicated to the choice of the parallel scheme to be used when performing parallel charging simulations (0 = random choice of one replica pair with MCPC2, 1 = random choice of one replica pair with MCPC1, 2 = use of even or odd replica pairs with MCPC2, and 3 = use of even or odd replica pairs with MCPC1). N_repc is the number of replicas. This number must equal the number of CPUs used for the simulation: the MCMC2 code assumes that one replica is run on one CPU. Any different choice will lead to job abortion. Configuration swapping between replicas is tested every pc_every MC sweeps. kt0 is the reference temperature of the simulation (identical for all the replicas). A MCPC simulation will always use kt0 whatever the originally selected temperature scale (constant, geometric, or adjusted, see keyword “TEMPERATURE”). Defining a temperature scale in the setup file will only result in the replacement of these temperatures by kt0 (beware: if the user does not give a value to kt0, the default will be used). q_ref is the reference charge used for MCPC1 simulations. For MCPC2 simulations, the charges q are read from the input configuration files. The simulation is stopped if the input files have charges different from q_ref. For deactivating MCPC simulations, choose pc_every=0.Default: imcpc=0, N_repc=0, pc_every=0, kt0=0.1, q_ref=0.•PT METHOD imcpt REPLICAS N_rept EVERY pt_every: Setting of parameters for running MCPT simulations. imcpt is an integer dedicated to the choice of the parallel scheme to be used when performing parallel tempering simulations (0 = random choice of one replica pair and 1 = alternative use of even and odd replica pairs). N_rept is the number of replicas. This number must equal the number of CPUs used for the simulation: the MCMC2 code assumes that one replica is run on one CPU. Any different choice will lead to job abortion. Configuration swapping between replicas is tested every pt_every MC sweeps. For deactivating MCPT simulations, choose pt_every=0.Default: imcpt=0, N_rept=0 and pt_every=0.•RADIAL USE lrad GRIDRCOM deltagridstepgrid PARTICLE radtyp: Setting parameters for plotting one-particle radial histograms (whether lrad  = .true.). The radial histograms cannot extend beyond the radius of the Monte Carlo container (see keyword “CONTAINER”), for graphical purposes we however allowed the user to add a small distance deltagrid to the grid size (=radius+deltagrid). stepgrid is the grid step for one-particle radial histograms with respect to the cluster center-of-mass. The grid origin is hardcoded to zero since these histograms are calculated with respect to the cluster center of mass. The number Ngrid of grid points is automatically determined in the code from the grid size and the grid step. radtyp enables the user to specify the type of particles to be considered for plotting radial histograms (0 = no distribution, 1 = all the particles without any distinction, 2 = charged particles only, 3 = neutral particles only, 4 = all the previous histograms (4=1+2+3)).Default: lrad = .false., deltagrid=0, stepgrid=0.1, radtyp=0.•SEED METHOD irand INITIALIZATION seed(1)seed(2)seed(3)seed(4) SCALING nsc(1)nsc(2)nsc(3)nsc(4): choice of the random number generator. irand is an integer that enables the user to select a random number generator (0 = LAPACK random number generator and 1 = lagged Fibonacci random number generator). Depending on the value of irand the random seeds and scaling factors may be different. If irand=0, seed(j) (j∈{1,2,3,4}) are positive integer seeds that must be below 4095 and seed(4) must be odd. N_rep (number of replicas) secondary seeds are generated from these primary seeds and the knowledge of scaling factors nsc(j) (j∈{1,2,3,4}). By default, these secondary seeds are obtained by doing seedi(j)=seed(j)+i×nsc(j) for j∈{1,2,3} and seedi(4)=seed(4)+2i×nsc(4) (since all the seeds seedi(4) must remain odd) where i is the replica number. If irand=1, only one seed (namely seed(1)) is expected by the program and secondary seeds are also produced by doing seedi(1)=seed(1)+i×nsc(1).Default: seed(j)=0 for j∈{1,2,3}, seed(4)=1, nsc(j)=1 for j∈{1,2,3,4} (i.e., seedi(j)=i for j∈{1,2,3} and seedi(4)=1+2i).References [1] D.A. Bonhommeau, M.-P. Gaigeot, Comput. Phys. Commun. 184 (2013) 873–884. [2] D.A. Bonhommeau, R. Spezia, M.-P. Gaigeot, J. Chem. Phys. 136 (2012) 184503. [3] M.A. Miller, D.A. Bonhommeau, C.J. Heard, Y. Shin, R. Spezia, M.-P. Gaigeot, J. Phys.: Condens. Matter. 24 (2012) 284130. [4] S. Kirkpatrick, E. P. Stoll, J. Comp. Phys. 40 (1981) 517. [5] G. Bhanot, D. Duke, R. Salvador, Phys. Rev. B 33 (1986) 7841. [6] M. Lewerenz, J. Chem. Phys. 106 (1997) 4596. [7] M. Mladenović, M. Lewerenz, Chem. Phys. Lett. 321 (2000) 135. [8] D. Bonhommeau, P. T. Lake, Jr., C. Le Quiniou, M. Lewerenz, N. Halberstadt, J. Chem. Phys. 126 (2007) 051104. [9] D. Bonhommeau, M. Lewerenz, N. Halberstadt, J. Chem. Phys. 128 (2008) 054302. Acknowledgements Dr. Mark A. Miller is gratefully acknowledged for providing some routines and valuable advice during the development of the code. The IDRIS national computer center and the ROMEO computer center of Champagne-Ardenne are also acknowledged. This work was supported by the “Génopôle d’Evry” through a post-doctoral fellowship (DAB), by the “Centre National de la Recherche Scientifique” (CNRS) through an excellence chair (DAB), and by a Partenariat Hubert Curien Alliance Program (MPG)."
}
@article{ABELSHAUSEN201542,
title = "Participatory integrated coastal zone management in Vietnam: Theory versus practice case study: Thua Thien Hue province",
journal = "Journal of Marine and Island Cultures",
volume = "4",
number = "1",
pages = "42 - 53",
year = "2015",
issn = "2212-6821",
doi = "https://doi.org/10.1016/j.imic.2015.06.004",
url = "http://www.sciencedirect.com/science/article/pii/S2212682115000190",
author = "Bieke Abelshausen and Tom Vanwing and Wolfgang Jacquet",
keywords = "Integrated coastal zone management, Bi-directional knowledge sharing, Participatory resource management, Social learning, Change management",
abstract = "Sustainable management processes have undergone a shift from a top-down approach to a bottom-up approach. This bottom-up approach allows for a more apprehensive inclusion of stakeholders. In traditional hierarchical societies a combination of both is considered more desirable. This combination is described as a participatory approach that allows for bi-directional knowledge sharing. The question asked is whether this theoretical approach is viable in practice, taking into account different social, political and cultural influences. Qualitative research in bi-directional knowledge sharing and stakeholder participation in Integrated Coastal Zone Management (ICZM) was conducted in the provinces of Thua Thien Hue in Vietnam. Qualitative research was conducted using coding analysis. This analysis showed that in practice a great reluctance for change affects the implementation of ICZM. This reluctance is directly related to the level of power of stakeholders and the level to which stakeholders are embedded in the top-down tradition. Two contradicting results emerged. On the one hand the theoretical understanding of participatory ICZM is highest when reluctance for change is highest and vice versa. On the other hand a decrease in power results in an increase of the sustainability of the implementation of participatory ICZM. This research concluded that a ‘platform or structure’ is essential to achieve sustainability. In the Vietnamese context the tradition of power results in a platform which is both formal and non-formal. A non-formal platform is needed to create social capital, whereas a formal platform will limit the risk for arbitrariness and allow for institutionalisation."
}
@article{HURLEY2014123,
title = "Exploring the connectedness of rural auxiliary midwives to social networks in Koutiala, Mali",
journal = "Midwifery",
volume = "30",
number = "1",
pages = "123 - 129",
year = "2014",
issn = "0266-6138",
doi = "https://doi.org/10.1016/j.midw.2013.03.002",
url = "http://www.sciencedirect.com/science/article/pii/S0266613813000806",
author = "Emily A. Hurley and Nicole E. Warren and Seydou Doumbia and Peter J. Winch",
keywords = "Maternal health, Mali, Social networks",
abstract = "Background
rural auxiliary midwives are central to clinical maternal care in Mali. However, little is known about their social role within the villages they serve. Exploring the social connectedness of midwives in their communities can reveal areas in which they need additional support, and ways they could benefit their communities beyond their clinical role.
Objective
to examine rural auxiliary midwives' social connectedness to the communities they serve.
Design
embedded, mixed methods design combining social network case studies with semi-structured interviews.
Participants and setting
midwives were recruited for semi-structured interviews during technical trainings held in Koutiala in southern Mali. Social network analyses were conducted among all adult women in two small villages purposively sampled from the Koutiala region.
Methods
29 interviews were conducted, transcribed, and coded using NVivo (Version 9) to qualitatively assess social connectedness. In two villages, the complete social networks of women's friendships were analysed using UCINET Version 6 (n=142; 74). Rank-orders of actors according to multiple measures of their centrality within the network were constructed to assess the midwives' position among village women.
Findings
both local and guest midwives reported feeling high levels of social integration, acceptance, and appreciation from the women in their communities. Specific challenges existed for guest or younger midwives, and in midwives' negotiations with men. In the two sociometric analyses, both the local and guest midwives ranked among the most influential social actors in their respective villages.
Key conclusions and implications for practice
though they hold a unique position among other rural women, this study suggests that midwives in Koutiala are well connected socially, and may be capable of becoming effective agents of network based-behavioural health interventions. Additional support is warranted to help midwives affirm a credible professional status in a male-dominated society, especially those of local status and younger age. Programme planners and policy-makers should consider the potential of midwives in communication when designing behaviour change interventions for women in similarly underserved areas."
}
@article{LYONS201736,
title = "Insights into evolution in Andean Polystichum (Dryopteridaceae) from expanded understanding of the cytosolic phosphoglucose isomerase gene",
journal = "Molecular Phylogenetics and Evolution",
volume = "112",
pages = "36 - 46",
year = "2017",
issn = "1055-7903",
doi = "https://doi.org/10.1016/j.ympev.2017.04.010",
url = "http://www.sciencedirect.com/science/article/pii/S1055790316302950",
author = "Brendan M. Lyons and Monique A. McHenry and David S. Barrington",
keywords = "Andes, ferns, Dryopteridaceae, Phosphoglucose isomerase, , Allopolyploidy",
abstract = "Cytosolic phosphoglucose isomerase (pgiC) is an enzyme essential to glycolysis found universally in eukaryotes, but broad understanding of variation in the gene coding for pgiC is lacking for ferns. We used a substantially expanded representation of the gene for Andean species of the fern genus Polystichum to characterize pgiC in ferns relative to angiosperms, insects, and an amoebozoan; assess the impact of selection versus neutral evolutionary processes on pgiC; and explore evolutionary relationships of selected Andean species. The dataset of complete sequences comprised nine accessions representing seven species and one hybrid from the Andes and Serra do Mar. The aligned sequences of the full data set comprised 3376 base pairs (70% of the entire gene) including 17 exons and 15 introns from two central areas of the gene. The exons are highly conserved relative to angiosperms and retain substantial homology to insect pgiC, but intron length and structure are unique to the ferns. Average intron size is similar to angiosperms; intron number and location in insects are unlike those of the plants we considered. The introns included an array of indels and, in intron 7, an extensive microsatellite array with potential utility in analyzing population-level histories. Bayesian and maximum-parsimony analysis of 129 variable nucleotides in the Andean polystichums revealed that 59 (1.7% of the 3376 total) were phylogenetically informative; most of these united sister accessions. The phylogenetic trees for the Andean polystichums were incongruent with previously published cpDNA trees for the same taxa, likely the result of rapid evolutionary change in the introns and contrasting stability in the exons. The exons code a total of seven amino-acid substitutions. Comparison of non-synonymous to synonymous substitutions did not suggest that the pgiC gene is under selection in the Andes. Variation in pgiC including two additional accessions represented by incomplete sequences provided new insights into reticulate relationships among Andean taxa."
}
@article{VILARINHO2014262,
title = "Capability-enhanced AIMSUN with Real-time Signal Timing Control",
journal = "Procedia - Social and Behavioral Sciences",
volume = "111",
pages = "262 - 271",
year = "2014",
note = "Transportation: Can we do more with less resources? – 16th Meeting of the Euro Working Group on Transportation – Porto 2013",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2014.01.059",
url = "http://www.sciencedirect.com/science/article/pii/S1877042814000603",
author = "Cristina Vilarinho and Guilherme Soares and José Macedo and José Pedro Tavares and Rosaldo J.F. Rossetti",
keywords = "Aimsun, traffic simulation, traffic light control, Webster method",
abstract = "In this paper, we enhanced the capability of Aimsun simulation including signal timing optimization of traffic control with real-time information on the network dynamics. The problem is formulated so as to find the duration of maximum green time for each stage in response to recurrent traffic flow fluctuation at an intersection. The approach used a simple version of the Webster method for determining the cycle length and green time split. The resulting algorithm was coded in Java and used TraSMAPI to dynamically link it to Aimsun's API, which allows the user to change the cycle length and green time duration of each traffic light's stage. TraSMAPI is a Traffic Simulation Manager API, designed to provide real-time interaction with traffic simulators. So far, this tool has been only tested with Sumo and Itsumo microscopic traffic simulators. The paper presents an example of the communication protocol using the API module linked to TraSMAPI, and contributes to the implementation of a novel real-time traffic control in Aimsun."
}
@article{PADOVAN2014563,
title = "Genetic optimization of a PCM enhanced storage tank for Solar Domestic Hot Water Systems",
journal = "Solar Energy",
volume = "103",
pages = "563 - 573",
year = "2014",
issn = "0038-092X",
doi = "https://doi.org/10.1016/j.solener.2013.12.034",
url = "http://www.sciencedirect.com/science/article/pii/S0038092X13005616",
author = "Roberta Padovan and Marco Manzan",
keywords = "PCM, Optimization, Solar energy storage",
abstract = "The scope of this work is to ascertain if the inclusion of Phase Change Materials (PCM) in the thermal energy storage of a Solar Domestic Hot Water System (SDHW) could be beneficial for increasing the energy savings of the system or in reducing the space occupied by the thermal energy storage. A simple SDHW plant is studied which features a plane solar collector, a boiler and a PCM enhanced tank. The PCM improved storage tank has been optimized using mono and multi-objective genetic algorithms. The optimization has been carried out with the modeFRONTIER optimization tool, while the system plant has been analyzed by means of a modified version of the building energy simulation code ESP-r. In parallel with the optimization a sensitivity analysis has been carried on in order to find out the relation between the design parameters of the tank (geometry, phase change temperature of the PCM, and insulation) and the performance of the system. Thanks to the multi-objective optimization of the system different solutions with different rankings of the optimized variables have been presented. The main result is that for this application the PCM has not the major impact on the results, while other parameters play a more significant role."
}
@article{VAKILI20171458,
title = "Spinal Epidural Abscess: A Series of 101 Cases",
journal = "The American Journal of Medicine",
volume = "130",
number = "12",
pages = "1458 - 1463",
year = "2017",
issn = "0002-9343",
doi = "https://doi.org/10.1016/j.amjmed.2017.07.017",
url = "http://www.sciencedirect.com/science/article/pii/S0002934317307970",
author = "Martin Vakili and Nancy F. Crum-Cianflone",
keywords = "Epidemiology, Outcome, Risk factors, Spinal epidural abscess, Treatment",
abstract = "Background
Spinal epidural abscesses are uncommon but potentially devastating infections that often elude early diagnosis. An increasing incidence has been suggested; however, few contemporary data are available regarding risk factors and epidemiologic trends over time.
Methods
A retrospective study of spinal epidural abscesses from 2004 to 2014 at a large academic hospital was conducted. Cases were identified using International Classification of Diseases, Ninth Revision (ICD-9) code 324.1, and a review of medical and radiographic records was performed to confirm each case. Data collected included sociodemographics, medical history, suspected route of infection, treatments, and outcome.
Results
The incidence was 5.1 cases for each 10,000 admissions, with no significant changes during the study period. The route of infection was identified in 52% of cases, with bacteremia as the most common (26%), followed by recent surgery/procedure (21%) and spinal injection (6%). An identifiable underlying risk factor was present in 84% of cases, most commonly diabetes and intravenous drug use. A causative organism was identified in 84% of cases, most commonly Staphylococcus aureus; methicillin-resistant isolates accounted for 25% of S. aureus cases. All cases received intravenous antibiotic therapy, and 73% underwent a drainage procedure. Fifteen percent had an adverse outcome (8% paralysis and 7% death).
Conclusions
The incidence of spinal epidural abscesses may be increasing, with the present study demonstrating a ≥5-fold higher rate compared with historical data. Although the outcome in most cases was favorable, spinal epidural abscesses continue to cause substantial morbidity and mortality and should remain a “not to be missed diagnosis.”"
}
@article{SOLISMARTINEZ20141864,
title = "BPMN MUSIM: Approach to improve the domain expert’s efficiency in business processes modeling for the generation of specific software applications",
journal = "Expert Systems with Applications",
volume = "41",
number = "4, Part 2",
pages = "1864 - 1874",
year = "2014",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2013.08.083",
url = "http://www.sciencedirect.com/science/article/pii/S0957417413007045",
author = "Jaime Solís-Martínez and Jordán Pascual Espada and B. Cristina Pelayo G-Bustelo and Juan Manuel Cueva Lovelle",
keywords = "Business process modeling, Domain experts systems, BPMN, Modeling tools, Software generation",
abstract = "Despite the evolution computer science has undergone during the last years, the time consumed by the application development process has not experienced mayor changes. One of the areas where improvements could be made is the requirement engineering process, where the features offered by business process modeling (BPM) could favor the implication of the domain experts. BPMN, the standard notation, is one of the most widely used modeling languages, used by a great number of organizations around the world. Its versatility enables it to be used for any kind of business process, regardless of the domain the problem addresses. However, non-technical domain experts find difficulties when dealing with business process modeling notations, so their participation in this task remains minor. BPMN MUSIM is a simplified business process modeling notation that intends to: lower the difficulty domain experts have for learning and understanding other notations, enable quick modeling of the business processes and use the models to generate specific software applications."
}
@article{SWANN20141184,
title = "Conversion of the Thymus into a Bipotent Lymphoid Organ by Replacement of Foxn1 with Its Paralog, Foxn4",
journal = "Cell Reports",
volume = "8",
number = "4",
pages = "1184 - 1197",
year = "2014",
issn = "2211-1247",
doi = "https://doi.org/10.1016/j.celrep.2014.07.017",
url = "http://www.sciencedirect.com/science/article/pii/S2211124714005841",
author = "Jeremy B. Swann and Annelies Weyn and Daisuke Nagakubo and Conrad C. Bleul and Atsushi Toyoda and Christiane Happe and Nikolai Netuschil and Isabell Hess and Annette Haas-Assenbaum and Yoshihito Taniguchi and Michael Schorpp and Thomas Boehm",
abstract = "Summary
The thymus is a lymphoid organ unique to vertebrates, and it provides a unique microenvironment that facilitates the differentiation of immature hematopoietic precursors into mature T cells. We subjected the evolutionary trajectory of the thymic microenvironment to experimental analysis. A hypothetical primordial form of the thymus was established in mice by replacing FOXN1, the vertebrate-specific master regulator of thymic epithelial cell function, with its metazoan ancestor, FOXN4, thereby resetting the regulatory and coding changes that have occurred since the divergence of these two paralogs. FOXN4 exhibited substantial thymopoietic activity. Unexpectedly, histological changes and a functional imbalance between the lymphopoietic cytokine IL7 and the T cell specification factor DLL4 within the reconstructed thymus resulted in coincident but spatially segregated T and B cell development. Our results identify an evolutionary mechanism underlying the conversion of a general lymphopoietic organ to a site of exclusive T cell generation."
}
@article{GIULIANO201529,
title = "Growth models of dyadic synchrony and mother–child vagal tone in the context of parenting at-risk",
journal = "Biological Psychology",
volume = "105",
pages = "29 - 36",
year = "2015",
issn = "0301-0511",
doi = "https://doi.org/10.1016/j.biopsycho.2014.12.009",
url = "http://www.sciencedirect.com/science/article/pii/S030105111400266X",
author = "Ryan J. Giuliano and Elizabeth A. Skowron and Elliot T. Berkman",
keywords = "Respiratory sinus arrhythmia, RSA, Parenting, Child maltreatment",
abstract = "We used multilevel modeling to examine dynamic changes in respiratory sinus arrhythmia (RSA) and observer-coded interactive synchrony for mother–child dyads engaged in a laboratory interaction, to characterize parenting-at-risk. Seventy-nine preschooler–mother dyads including a subset with documented child maltreatment (CM; n=43) were observed completing a joint puzzle task while physiological measures were recorded. Dyads led by CM mothers showed decreases in positive synchrony over time, whereas no variation was observed in non-CM dyads. Growth models of maternal RSA indicated that mothers who maintained high levels of positive interactive synchrony with their child evidenced greater RSA reactivity, characterized by an initial withdrawal followed by augmentation as the task progressed, after accounting for CM group status. These results help to clarify patterns of RSA responding in the context of caregiver–child interactions, and demonstrate the importance of modeling dynamic changes in physiology over time in order to better understanding biological correlates of parenting-at-risk."
}
@article{GAAMEL2018,
title = "Broker-less middleware for WSAN performance evaluation",
journal = "Future Generation Computer Systems",
year = "2018",
issn = "0167-739X",
doi = "https://doi.org/10.1016/j.future.2018.04.068",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X1732900X",
author = "Awadh Gaamel and Tarek Sheltami and Anas Al-Roubaiey and Elhadi Shakshuki",
keywords = "Wireless Sensor Networks, Data Distribution Service (DDS), Publish/Subscribe Model, TinyDDS, DefTDDS, BLTDDS, HyTDDS, TOSSIM, E-EATDDS",
abstract = "Over the past few years, The cost of electronic devices and sensor networks decreased rapidly enforcing almost all users’ requirements to use those devices. These devices provide low cost processing in wireless sensor networks (WSNs), As well as in wireless sensor and actuator networks (WSANs). The task of data management in WSNs is a vital issue that can be performed with limited resources such as processing, Memory and energy. Data distribution service (DDS) is a prominent standard used in the industry and academia communities to support real-time distributed systems depending on publish/subscribe (pub/sub) model. tinyDDS is a lightweight middleware that is a partial porting of the DDS and implemented over tinyOS code. The original version of tinyDDS is called the default tinyDDS (defTDDS). broker-less tinyDDS (BLTDDS) and hybrid tinyDDS (hyTDDS) are protocols that added several improvements to the defTDDS. The energy aware tinyDDS (EATDDS) protocol is proposed to deal directly with the energy consumption metric. In this paper, We conduct a comparative study between defTDDS, BLTDDS and hyTDDS in terms of throughput, PDR, End-to-end delay and energy consumption. Moreover, We propose a new protocol named an enhanced energy aware tinyDDS (E-EATDDS) that improves the energy consumption of the EATDDS protocol. We use tinyOS simulator in our implementation and evaluation. The results show that E-EATDDS outperforms BLTDDS, hyTDDS and EATDDS in terms of number of packets sent per joule."
}
@article{OLOINGSIGH201429,
title = "The Dust Storm Index (DSI): A method for monitoring broadscale wind erosion using meteorological records",
journal = "Aeolian Research",
volume = "12",
pages = "29 - 40",
year = "2014",
issn = "1875-9637",
doi = "https://doi.org/10.1016/j.aeolia.2013.10.004",
url = "http://www.sciencedirect.com/science/article/pii/S1875963713000773",
author = "T. O’Loingsigh and G.H. McTainsh and E.K. Tews and C.L. Strong and J.F. Leys and P. Shinkfield and N.J. Tapper",
keywords = "Dust storms, Meteorological records, Wind erosion",
abstract = "Wind erosion of soils is a natural process that has shaped the semi-arid and arid landscapes for millennia. This paper describes the Dust Storm Index (DSI); a methodology for monitoring wind erosion using Australian Bureau of Meteorology (ABM) meteorological observational data since the mid-1960s (long-term), at continental scale. While the 46year length of the DSI record is its greatest strength from a wind erosion monitoring perspective, there are a number of technical challenges to its use because when the World Meteorological Organisation (WMO) recording protocols were established the use of the data for wind erosion monitoring was never intended. Data recording and storage protocols are examined, including the effects of changes to the definition of how observers should interpret and record dust events. A method is described for selecting the 180 long-term ABM stations used in this study and the limitations of variable observation frequencies between stations are in part resolved. The rationale behind the DSI equation is explained and the examples of temporal and spatial data visualisation products presented include; a long term national wind erosion record (1965–2011), continental DSI maps, and maps of the erosion event types that are factored into the DSI equation. The DSI is tested against dust concentration data and found to provide an accurate representation of wind erosion activity. As the ABM observational records used here were collected according to WMO protocols, the DSI methodology could be used in all countries with WMO-compatible meteorological observation and recording systems."
}
@article{KERESZTURI201614,
title = "Emplacement conditions of the 1256AD Al-Madinah lava flow field in Harrat Rahat, Kingdom of Saudi Arabia — Insights from surface morphology and lava flow simulations",
journal = "Journal of Volcanology and Geothermal Research",
volume = "309",
pages = "14 - 30",
year = "2016",
issn = "0377-0273",
doi = "https://doi.org/10.1016/j.jvolgeores.2015.11.002",
url = "http://www.sciencedirect.com/science/article/pii/S0377027315003686",
author = "Gábor Kereszturi and Károly Németh and Mohammed R Moufti and Annalisa Cappello and Hugo Murcia and Gaetana Ganci and Ciro Del Negro and Jonathan Procter and Hani Mahmoud Ali Zahran",
keywords = "Scoria cone, Basalt, Lava flow, Lava channel, Effusive curve, Volume",
abstract = "Lava flow hazard modelling requires detailed geological mapping, and a good understanding of emplacement settings and the processes involved in the formation of lava flows. Harrat Rahat, Kingdom of Saudi Arabia, is a large volcanic field, comprising about 1000 predominantly small-volume volcanoes most of which have emitted lava flows of various lengths. A few eruptions took place in this area during the Holocene, and they were located in the northern extreme of the Harrat Rahat, a close proximity to critical infrastructure and population living in Al-Madinah City. In the present study, we combined field work, high resolution digital topography and morphometric analysis to infer the emplacement history of the last historical event in the region represented by the 1256AD Al-Madinah lava flow field. These data were also used to simulate 1256AD-type lava flows in the Harrat Rahat by the MAGFLOW lava flow emplacement model, which is able to relate the flow evolution to eruption conditions. The 1256AD lava flow field extent was mapped at a scale of 1:1000 from a high resolution (0.5m) Light Detection And Ranging (LiDAR) Digital Terrain Model (DTM), aerial photos with field support. The bulk volume of the lava flow field was estimated at 0.4km3, while the source volume represented by seven scoria cone was estimated at 0.023km3. The lava flow covered an area of 60km2 and reached a maximum length of 23.4km. The lava flow field comprises about 20.9% of pāhoehoe, 73.8% of 'a'ā, and 5.3% of late-stage outbreaks. Our field observation, also suggests that the lava flows of the Harrat Rahat region are mainly core-dominated and that they formed large lava flow fields by amalgamation of many single channels. These channels mitigated downslope by topography-lava flow and channel–channel interactions, highlighting this typical process that needs to be considered in the volcanic hazard assessment in the region. A series of numerical lava flow simulations was carried out using a range of water content (0.1–1wt.%), solidification temperature (800–600°C) and effusion curves (simple and complex curves). These simulations revealed that the MAGFLOW code is sensitive to the changes of water content of the erupting lava magma, while it is less sensitive to solidification temperature and the changes of the shape of effusion curve. The advance rate of the simulated lava flows changed from 0.01 to 0.22km/h. Using data and observations from the youngest volcanic event of the Harrat Rahat as input parameters to MAGFLOW code, it is possible to provide quantitative limits on this type of hazard."
}
@article{SRIVASTAVA2013130,
title = "First experiments with PowerPlay",
journal = "Neural Networks",
volume = "41",
pages = "130 - 136",
year = "2013",
note = "Special Issue on Autonomous Learning",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2013.01.022",
url = "http://www.sciencedirect.com/science/article/pii/S0893608013000373",
author = "Rupesh Kumar Srivastava and Bas R. Steunebrink and Jürgen Schmidhuber",
keywords = ", Problem invention, Discovery, Self-delimiting recurrent neural network, Skill learning, Self-modularization",
abstract = "Like a scientist or a playing child, PowerPlay (Schmidhuber, 2011) not only learns new skills to solve given problems, but also invents new interesting problems by itself. By design, it continually comes up with the fastest to find, initially novel, but eventually solvable tasks. It also continually simplifies or compresses or speeds up solutions to previous tasks. Here we describe first experiments with PowerPlay. A self-delimiting recurrent neural network SLIM RNN (Schmidhuber, 2012) is used as a general computational problem solving architecture. Its connection weights can encode arbitrary, self-delimiting, halting or non-halting programs affecting both environment (through effectors) and internal states encoding abstractions of event sequences. Our PowerPlay-driven SLIM RNN learns to become an increasingly general solver of self-invented problems, continually adding new problem solving procedures to its growing skill repertoire. Extending a recent conference paper (Srivastava, Steunebrink, Stollenga, & Schmidhuber, 2012), we identify interesting, emerging, developmental stages of our open-ended system. We also show how it automatically self-modularizes, frequently re-using code for previously invented skills, always trying to invent novel tasks that can be quickly validated because they do not require too many weight changes affecting too many previous tasks."
}
@article{BRAD2014503,
title = "Smart Deployment of Demonstrators into Successful Commercial Solutions",
journal = "Procedia CIRP",
volume = "21",
pages = "503 - 508",
year = "2014",
note = "24th CIRP Design Conference",
issn = "2212-8271",
doi = "https://doi.org/10.1016/j.procir.2014.03.137",
url = "http://www.sciencedirect.com/science/article/pii/S2212827114006775",
author = "Stelian Brad and Mircea Fulea and Emilia Brad and Bogdan Mocan",
keywords = "methodology, software engineering, innovation, product planning, product deployment",
abstract = "Product or service concepts based on emerging technologies are usually results of research projects, be they performed by academic groups or by research departments of companies. Many times, the prototypes or demonstrators that result from such projects are supposed to evolve into commercial products or services, but – at least in the first stage - there are more focus on proving key features of a technology, or the effectiveness / efficiency / applicability of various concepts or algorithms. However, evolving into commercial products is many times at least as challenging as building the prototypes. In case of software-based projects, this means changes in architecture, a lot of code rewriting and important usability improvements. This paper introduces a software-concept product design algorithm which aims to minimize the effort required in turning a demonstrator into a commercial product. This is done by generating two functionality sets: a pure demonstrator and a pure commercial one, then generating a hybrid functionality set with the corresponding architecture, and then assessing each functionality for the demonstrator and the commercial version in terms of development and improvement effort. Through iterations, in which the original functionality sets are improved, the difference between the two perspectives will be reduced until it gets below a reasonable limit in terms of effort. The paper presents a case study in which the algorithm is applied for planning a software platform for supporting SMEs in their innovation processes."
}
@article{WOODS2015323,
title = "Experiences of hearing voices: analysis of a novel phenomenological survey",
journal = "The Lancet Psychiatry",
volume = "2",
number = "4",
pages = "323 - 331",
year = "2015",
issn = "2215-0366",
doi = "https://doi.org/10.1016/S2215-0366(15)00006-1",
url = "http://www.sciencedirect.com/science/article/pii/S2215036615000061",
author = "Angela Woods and Nev Jones and Ben Alderson-Day and Felicity Callard and Charles Fernyhough",
abstract = "Summary
Background
Auditory hallucinations—or voices—are a common feature of many psychiatric disorders and are also experienced by individuals with no psychiatric history. Understanding of the variation in subjective experiences of hallucination is central to psychiatry, yet systematic empirical research on the phenomenology of auditory hallucinations remains scarce. We aimed to record a detailed and diverse collection of experiences, in the words of the people who hear voices themselves.
Methods
We made a 13 item questionnaire available online for 3 months. To elicit phenomenologically rich data, we designed a combination of open-ended and closed-ended questions, which drew on service-user perspectives and approaches from phenomenological psychiatry, psychology, and medical humanities. We invited people aged 16–84 years with experience of voice-hearing to take part via an advertisement circulated through clinical networks, hearing voices groups, and other mental health forums. We combined qualitative and quantitative methods, and used inductive thematic analysis to code the data and χ2 tests to test additional associations of selected codes.
Findings
Between Sept 9 and Nov 29, 2013, 153 participants completed the study. Most participants described hearing multiple voices (124 [81%] of 153 individuals) with characterful qualities (106 [69%] individuals). Less than half of the participants reported hearing literally auditory voices—70 (46%) individuals reported either thought-like or mixed experiences. 101 (66%) participants reported bodily sensations while they heard voices, and these sensations were significantly associated with experiences of abusive or violent voices (p=0·024). Although fear, anxiety, depression, and stress were often associated with voices, 48 (31%) participants reported positive emotions and 49 (32%) reported neutral emotions. Our statistical analysis showed that mixed voices were more likely to have changed over time (p=0·030), be internally located (p=0·010), and be conversational in nature (p=0·010).
Interpretation
This study is, to our knowledge, the largest mixed-methods investigation of auditory hallucination phenomenology so far. Our survey was completed by a diverse sample of people who hear voices with various diagnoses and clinical histories. Our findings both overlap with past large-sample investigations of auditory hallucination and suggest potentially important new findings about the association between acoustic perception and thought, somatic and multisensorial features of auditory hallucinations, and the link between auditory hallucinations and characterological entities.
Funding
Wellcome Trust."
}
@article{RANDESI201833,
title = "Sex differences after chronic stress in the expression of opioid-, stress- and neuroplasticity-related genes in the rat hippocampus",
journal = "Neurobiology of Stress",
volume = "8",
pages = "33 - 41",
year = "2018",
issn = "2352-2895",
doi = "https://doi.org/10.1016/j.ynstr.2018.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S2352289517300450",
author = "Matthew Randesi and Yan Zhou and Sanoara Mazid and Shannon C. Odell and Jason D. Gray and J. Correa da Rosa and Bruce S. McEwen and Teresa A. Milner and Mary Jeanne Kreek",
keywords = "Delta opioid receptor, Neural plasticity, Corticotrophin releasing factor, Drug addiction",
abstract = "Opioid peptides and their receptors re-organize within hippocampal neurons of female, but not male, rats following chronic immobilization stress (CIS) in a manner that promotes drug-related learning. This study was conducted to determine if there are also sex differences in gene expression in the hippocampus following CIS. Adult female and male rats were subjected to CIS (30 min/day) for 10 days. Twenty-four hours after the last stressor, the rats were euthanized, the brains were harvested and the medial (dentate gyrus/CA1) and lateral (CA2/CA3) dorsal hippocampus were isolated. Following total RNA isolation, cDNA was prepared for gene expression analysis using a RT2 Profiler PCR expression array. This custom designed qPCR expression array contained genes for opioid peptides and receptors, as well as genes involved in stress-responses and candidate genes involved in synaptic plasticity, including those upregulated following oxycodone self-administration in mice. Few sex differences are seen in hippocampal gene expression in control (unstressed) rats. In response to CIS, gene expression in the hippocampus was altered in males but not females. In males, opioid, stress, plasticity and kinase/signaling genes were all down-regulated following CIS, except for the gene that codes for corticotropin releasing hormone, which was upregulated. Changes in opioid gene expression following chronic stress were limited to the CA2 and CA3 regions (lateral sample). In conclusion, modest sex- and regional-differences are seen in expression of the opioid receptor genes, as well as genes involved in stress and plasticity responses in the hippocampus following CIS."
}
@article{CONTINILLO201294,
title = "Parallel tools for the bifurcation analysis of large-scale chemically reactive dynamical systems",
journal = "Computers & Chemical Engineering",
volume = "38",
pages = "94 - 105",
year = "2012",
issn = "0098-1354",
doi = "https://doi.org/10.1016/j.compchemeng.2011.12.016",
url = "http://www.sciencedirect.com/science/article/pii/S009813541100353X",
author = "Gaetano Continillo and Artur Grabski and Erasmo Mancusi and Lucia Russo",
keywords = "Periodically forced chemical reactors, Parameter continuation, Bifurcation analysis, Parallel implementation, Parallelism",
abstract = "In this work we propose a set of tools for the parallel application of pseudo-arclength continuation to a class of systems for which the right hand side can be properly represented by a time numerically calculated evolution operator. For example, the reverse flow reactor and the reactors network with periodically switched inlet and outlet sections belong to this class of system. To conduct a dynamical analysis of these systems when the key parameters are changed, it is necessary to compute the eigenvalues of the Jacobian matrix many times. Since the Jacobian can only be obtained numerically, and this in turn takes away really significant computational power, running this operation in parallel saves real time of computation. Examples, solution lines and performance diagrams for selected systems are presented and discussed."
}
@article{FIASCHI2017924,
title = "Improvement of waste heat recuperation on an industrial textile dryer: Redesign of heat exchangers network and components",
journal = "Energy Conversion and Management",
volume = "150",
pages = "924 - 940",
year = "2017",
issn = "0196-8904",
doi = "https://doi.org/10.1016/j.enconman.2017.05.053",
url = "http://www.sciencedirect.com/science/article/pii/S0196890417305022",
author = "Daniele Fiaschi and Giampaolo Manfrida and Luigi Russo and Lorenzo Talluri",
keywords = "Heat recovery, Textile dryer, Finned heat exchangers, CFD, Heat exchanger tests",
abstract = "The improvement of low temperature exhausts heat recovery network of an industrial textile – drying machine (Stenter/Rameuse) is presented. A complete redesign of the layout of the water – gas heat exchangers network was done. The network was improved changing the original serial configuration of the heat recovery cells to a system with parallel manifolds for the water circuit. The heat transfer layout and the related heat exchangers were modelled with a dedicated thermal design code. The limited heat transfer coefficient of the internal gas side in the original configuration was improved with a “twin barrel” solution, with water in the outer annulus and exhaust gas in the inner duct equipped with internal longitudinal fins, an effective solution allowing easy fabrication and cleaning. A second step refinement design of the heat exchangers modules, realized with an OpenFOAM® CFD procedure, allowed the final definition and optimization of the fins size and layout, which were not continuous on the whole length of the module, but staggered on the inner side and shortened to about 1/3 of the length. Compared to the original version, the new heat exchangers network and the improved thermal design allowed an increase of the heat recovery from the exhausts of about 180%. The adoption of three staggered and segmented fins led to an increase of 97% with respect to the bare pipe. Finally, the results of the models were validated on a test bench reproducing one full-scale section of the drying machine: the tests gave positive issues, confirming the model predictions and the correct operability of the unit. Particularly, the accuracy of prediction of water temperature was very good (less than 0.5°C difference between simulation and measurements)."
}
@article{ELLIMOOTTIL201885,
title = "Examining Patient Willingness to Pay for Magnetic Resonance Imaging Guided Prostate Biopsy: Implications in the Era of Health Savings Accounts",
journal = "Urology Practice",
volume = "5",
number = "2",
pages = "85 - 92",
year = "2018",
issn = "2352-0779",
doi = "https://doi.org/10.1016/j.urpr.2017.03.003",
url = "http://www.sciencedirect.com/science/article/pii/S2352077917300572",
author = "Chad Ellimoottil and Marissa Marcotte and Daniel Grace and Alexander Krasnikov and Joan M. Phillips and Marcus L. Quek and Robert Flanigan and Gopal N. Gupta",
keywords = "cost-benefit analysis, patient preference, diagnostic imaging, prostate, biopsy",
abstract = "Introduction
The proliferation of health savings accounts has empowered patients to participate in medical decisions through a direct financial incentive. Using conjoint analysis we examined how much extra patients with a health savings account would be willing to pay for magnetic resonance imaging-transrectal ultrasound fusion guided prostate biopsy over transrectal ultrasound guided prostate biopsy.
Methods
We enrolled men who were 55 to 70 years old from a general urology clinic. We performed a literature review, distributed surveys and conducted semi-structured interviews to develop and rank attributes commonly used to compare magnetic resonance-ultrasound to transrectal ultrasound guided prostate biopsy. Using conjoint surveys we asked participants to select their preferred choice between 2 hypothetical biopsy interventions with differing levels of the attributes and cost. Results of the conjoint surveys were analyzed using a multinomial probit model. We performed a sensitivity analysis to assess the stability of our results after adjusting for age, history of prostate cancer, race, education, marital status, income and Zip Code of residence.
Results
Patients were willing to pay $1,598 more for a biopsy intervention with increased sensitivity to detect all cancer from 43% to 51% and $2,034 more for a negative predictive value improvement from 70% to 90%. Patients were not willing to pay extra for an intervention with improved sensitivity to detect high risk cancer alone. These estimates did not change with our sensitivity analysis.
Conclusions
Our findings suggest that patients are willing to pay approximately $1,500 to $2,000 from a health savings account for a biopsy intervention with a benefit profile similar to that of magnetic resonance-ultrasound guided prostate biopsy."
}
@article{GIM2014633,
title = "FLUID-STRUCTURE INTERACTION IN A U-TUBE WITH SURFACE ROUGHNESS AND PRESSURE DROP",
journal = "Nuclear Engineering and Technology",
volume = "46",
number = "5",
pages = "633 - 640",
year = "2014",
issn = "1738-5733",
doi = "https://doi.org/10.5516/NET.02.2014.001",
url = "http://www.sciencedirect.com/science/article/pii/S1738573315301030",
author = "GYUN-HO GIM and SE-MYONG CHANG and SINYOUNG LEE and GANGWON JANG",
keywords = "U-tube, Steam Generator, Inconel 690, FSI, Fretting Wear, CFD",
abstract = "In this research, the surface roughness affecting the pressure drop in a pipe used as the steam generator of a PWR was studied. Based on the CFD (Computational Fluid Dynamics) technique using a commercial code named ANSYS-FLUENT, a straight pipe was modeled to obtain the Darcy frictional coefficient, changed with a range of various surface roughness ratios as well as Reynolds numbers. The result is validated by the comparison with a Moody chart to set the appropriate size of grids at the wall for the correct consideration of surface roughness. The pressure drop in a full-scale U-shaped pipe is measured with the same code, correlated with the surface roughness ratio. In the next stage, we studied a reduced scale model of a U-shaped heat pipe with experiment and analysis of the investigation into fluid-structure interaction (FSI). The material of the pipe was cut from the real heat pipe of a material named Inconel 690 alloy, now used in steam generators. The accelerations at the fixed stations on the outer surface of the pipe model are measured in the series of time history, and Fourier transformed to the frequency domain. The natural frequency of three leading modes were traced from the FFT data, and compared with the result of a numerical analysis for unsteady, incompressible flow. The corresponding mode shapes and maximum displacement are obtained numerically from the FSI simulation with the coupling of the commercial codes, ANSYS-FLUENT and TRANSIENT_STRUCTURAL. The primary frequencies for the model system consist of three parts: structural vibration, BPF(blade pass frequency) of pump, and fluid-structure interaction."
}
@article{ANSAH2018784,
title = "Integrated microbial enhanced oil recovery (MEOR) simulation: Main influencing parameters and uncertainty assessment",
journal = "Journal of Petroleum Science and Engineering",
volume = "171",
pages = "784 - 793",
year = "2018",
issn = "0920-4105",
doi = "https://doi.org/10.1016/j.petrol.2018.08.005",
url = "http://www.sciencedirect.com/science/article/pii/S0920410518306715",
author = "Eric O. Ansah and Yuichi Sugai and Ronald Nguele and Kyuro Sasaki",
keywords = "Artificial intelligence, Capillary number, Enthalpy, Microbial enhanced oil recovery, Monte Carlo simulation, Response surface methodology",
abstract = "The present study investigated the ability of a thermophilic anaerobic microbe (herein coded as AR80) for MEOR with the further objective to quantify the uncertainty of production forecast in terms of the cumulative probability distribution. A series of core flood experiments conducted in water-flooded Berea sandstone showed that up to 51% of initial oil-in-place was recovered when the plugs were treated with AR80 and shut-in for 14 days. Mainly, the oil recovery mechanisms were attributed to viscosity enhancement, wettability changes, permeability and flow effects. Matching the laboratory data using artificial intelligence: the optimized cumulative oil recovery could be achieved at an enthalpy of 894.2 J/gmol, Arrhenius frequency of 8.3, residual oil saturation of 20%, log of capillary number at microbe flooding stage of −1.26, and also depicted a history match error less than 3%. Therefrom, a sensitivity analysis conducted on reservoir shut-in period effect on oil recovery revealed that a relatively shorter shut-in period is recommended to warrant early incremental oil recovery effect for economical purposes. In addition, MEOR could enhance the oil recovery significantly if a larger capillary number (between 10−5 and 10−3.5) is attained. Per probabilistic estimation, MEOR could sustain already water-flooded well for a set period of time. This study showed that there is a 20% frequency of increasing the oil recovery by above 20% when a mature water-flooded reservoir is further flooded with AR80 for 2 additional years. Lastly, it was demonstrated herein that increasing the nutrient (yeast extract) concentration (from 0.1 to 1% weight) had less or no significant effect on the oil viscosity and subsequent recovery."
}
@article{GOMEZROSADO2013378,
title = "Importance of the Quality of the Discharge Report in the Management of a Surgical Clinical Unit",
journal = "Cirugía Española (English Edition)",
volume = "91",
number = "6",
pages = "378 - 383",
year = "2013",
issn = "2173-5077",
doi = "https://doi.org/10.1016/j.cireng.2013.10.033",
url = "http://www.sciencedirect.com/science/article/pii/S2173507713001907",
author = "Juan-Carlos Gomez-Rosado and María Sanchez-Ramirez and Javier Valdes-Hernandez and Luis C. Capitan-Morales and Marta I. del-Nozal-Nalda and Fernando Oliva-Mompean",
keywords = "Discharge report, Patient discharge, Clinical care management, Avoidable stays, Informe médico de alta, Alta hospitalaria, Gestión clínica, Estancias evitables",
abstract = "Background
The discharge report is a basic document at the end of a care process, and is a key element in the coding process, since its correct wording, reliability and completeness are factors used to determine the hospital production.
Materials and methods
From a hypothesis based on the analysis of the consistency between the discharge report and data collected from the routine clinical notes during admission, we should be able to re-code all those mis-coded, thus placing them in a more appropriate diagnosis-related group (DRG). A total of 24 patient outliers were analysed for the correct filling in of the type and reason for admission, personal history, medication, anamnesis, primary and secondary diagnosis, surgical procedure, outcome, number of diagnostic and procedures cited, concordance between discharge report and history and recoding of the DRG.
Results
From a total of 24 episodes, 6 had precise and valid reports, 4 were valid but not precise enough, 9 were insufficient, and 5 were clearly invalid. The recoded DRG after the documentation review was not significantly different, according to the Wilcoxon test, being changed in only 5 cases (P=.680).
Conclusion
Quality in discharge reports depends on an adequate minimum data set (MDS) in concordance with the source documentation during admission. Discordance can change the DRG, despite it not being significantly different in our series. Self-audit of discharge reports allows quality improvements to be developed along with a reduction in information mistakes.
Resumen
Introducción
El informe de alta es un documento básico al finalizar un proceso asistencial, y es un elemento clave en el proceso de codificación. De su correcta redacción, fiabilidad y exhaustividad dependerán los datos que sirvan para determinar la producción hospitalaria.
Material y métodos
Partimos de la hipótesis de que, analizando la concordancia del informe de alta con los datos cotejados en la documentación del episodio, podremos recodificar todos aquellos casos infracodificados, imputándolos así a un grupo relacionado por el diagnóstico (GRD) más adecuado. Analizamos en 24 pacientes outliers la correcta cumplimentación de tipo y motivo de ingreso, antecedentes personales y medicación, resumen del episodio, diagnósticos principal y secundarios, procedimiento quirúrgico, evolución durante el episodio y número de diagnósticos y procedimientos enumerados, concordancia con la información real del episodio y los cambios teóricos entre los GRD antes y después del análisis.
Resultados
De 24 casos, 6 informes son válidos y claros; 4, válidos aunque poco claros; 9 son insuficientes y 5, claramente inválidos. La comparación de los GRD recalculados tras la interpretación de los datos del episodio no muestra diferencias significativas, mediante test de Wilcoxon, encontrándose tan solo modificaciones en 5 casos (p = 0,680).
Conclusiones
La calidad del informe de alta depende de la correcta inclusión de todos los datos del CMBD, en concordancia con el episodio. Las discordancias historia/informe pueden modificar el GRD que, en nuestra serie, no es estadísticamente significativo. La autoauditoría del informe de alta hospitalaria permite establecer líneas de mejora, al disminuir los errores de información."
}
@article{PULCRANO2015286,
title = "Putting Residents in the Office: An Effective Method to Teach the Systems-Based Practice Competency",
journal = "Journal of Surgical Education",
volume = "72",
number = "2",
pages = "286 - 290",
year = "2015",
issn = "1931-7204",
doi = "https://doi.org/10.1016/j.jsurg.2014.09.001",
url = "http://www.sciencedirect.com/science/article/pii/S1931720414002529",
author = "Marisa Pulcrano and A. Alfred Chahine and Amanda Saratsis and Jamie Divine-Cadavid and Vinod Narra and Stephen R.T. Evans",
keywords = "systems-based practice, general surgery residency, graduate medical education, competencies, Professionalism, Practice-Based Learning and Improvement, Systems-Based Practice",
abstract = "Objectives
Systems-based practice (SBP) was 1 of 6 core competencies established by the Accreditation Council for Graduate Medical Education and has proven to be one of the most difficult to effectively implement. This pilot study presents an immersion workshop as an effective tool to teach the SBP competency in a way that could easily be integrated into a residency curriculum.
Design
In 2006, 16 surgical residents rotated through 3 stations for 30 minutes each: coding and billing, scheduling operations and return appointments, and patient check-in. Participants were administered a pretest and posttest questionnaire evaluating their knowledge of SBP, and were asked to evaluate the workshop.
Setting
Outpatient clinic at MedStar Georgetown University Hospital, Washington, DC.
Participants
Residents in the general surgery residency training program at MedStar Georgetown University Hospital.
Results
Most residents (62.5%) improved their score after the workshop, whereas 31.25% showed no change and 6.25% demonstrated a decrease in score. Overall within their training levels, all groups demonstrated an increase in mean test score. Postgraduate year-2 residents demonstrated the greatest change in mean score (20%), whereas postgraduate year-4 residents demonstrated the smallest change in mean score (3.3%).
Conclusions
An immersion workshop where general surgery residents gained direct exposure to SBP concepts in situ was an effective and practical method of integrating this core competency into the residency curriculum. Such a workshop could complement more formal didactic teaching and be easily incorporated into the curriculum. For example, this workshop could be integrated into the ambulatory care requirement that each resident must fulfill as part of their clinical training."
}
@article{ZHOU201565,
title = "Image-based field monitoring of Cercospora leaf spot in sugar beet by robust template matching and pattern recognition",
journal = "Computers and Electronics in Agriculture",
volume = "116",
pages = "65 - 79",
year = "2015",
issn = "0168-1699",
doi = "https://doi.org/10.1016/j.compag.2015.05.020",
url = "http://www.sciencedirect.com/science/article/pii/S016816991500157X",
author = "Rong Zhou and Shun’ichi Kaneko and Fumio Tanaka and Miyuki Kayamori and Motoshige Shimizu",
keywords = "Image processing, Foliar disease monitoring, Template matching, Pattern recognition, Machine learning, Sugar beets",
abstract = "This paper presents a novel image algorithm using template matching and pattern recognition frameworks for monitoring Cercospora leaf spot (CLS) development on sugar beets on a single leaf scale under real field conditions. Due to the variety and complexity of the open field, it is a great challenge to achieve continuous and robust foliar disease observation in real field conditions. We propose a novel and compact algorithm, composed of two frameworks and a post-processing. The algorithm has continuous and highly discriminative capabilities for observing the process of disease in a single leaf from plant-level time sequence images. The first framework is based on robust template matching by orientation code matching (OCM), which implements successive tracking of a single leaf from a beet plant against severe illumination changes and non-rigid plant movements. The second framework uses a pattern recognition method of support vector machine (SVM) for achieving further disease classification from clutter field background. Prior to SVM, we propose a three feature combination of L∗, a∗, Entropy×Density, which has strong discrimination power to classify CLS disease from the clutter scene containing sandy soil, leaves, leaf stalks, and specular reflection. Additionally, post-processing is introduced to filter false positive noise to enhance the precision of the classification. Field experiment results demonstrate the feasibility and applicability of the proposed algorithm for disease monitoring under real field conditions. Meanwhile, comparative results with other conventional matching methods and feature combinations show the effectiveness of our proposed algorithm in both foliage tracking and disease classification."
}
@article{LAUTENSCHLAGER20153933,
title = "Group Profiling Automation for Crime and Terrorism (GPACT)",
journal = "Procedia Manufacturing",
volume = "3",
pages = "3933 - 3940",
year = "2015",
note = "6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015",
issn = "2351-9789",
doi = "https://doi.org/10.1016/j.promfg.2015.07.922",
url = "http://www.sciencedirect.com/science/article/pii/S2351978915009233",
author = "Jennifer Lautenschlager and Alicia Ruvinsky and Ian Warfield and Brian Kettler",
keywords = "Group profiling, Natural Language Processing (NLP), Data analytics, Event data, Event clustering, Event trending, Narrative generation",
abstract = "The U.S. State Department Bureau of Counterterrorism officially lists 59 foreign terrorist organizations, while the current Terrorism Research & Analysis Consortium (TRAC) database contains over 3,800 groups. The number of actual groups is constantly changing as new groups emerge and existing groups are redefined, thus motivating a need to automate the rapid generation of multi-faceted group profiles to provide on-demand support for analyst understanding. Robust, automated profiles can be generated for these groups by leveraging current Natural Language Processing (NLP) techniques and large-scale analytics over relevant text (e.g., news stories, social media). Information on key individuals, attack history, group interactions, and more can be extracted and assembled into a dynamic organizational profile. Lockheed Martin Advanced Technology Labs (LM ATL) has developed a prototype system for creating such profiles, based on the publicly released Integrated Crisis Early Warning System (ICEWS) Coded Event Data, a set of over 13 million automatically generated events extracted from public news stories. This set of data has proven valuable for situational awareness and event forecasting, and a more actor-centric view of the data can yield rich details about a group's history and modus operandi. Profile generation, then, is based on the following capabilities: (1) event clustering, (2) event trending, and (3) narrative generation. In this paper, we describe both the framework and analytical components of the Group Profiling Automation for Crime and Terrorism (GPACT) prototype that generates terrorist and criminal group profiles. After describing the overall framework we focus on three analytical capabilities. First, event clustering operates over the set of event data to identify clusters of related events relevant to a particular topic of interest (e.g., interactions with other groups, past attack history), similar to how topic and document clustering operates. The second, event trend analysis, performs analytics over event data focusing on clustered topics to provide awareness of aggregate patterns detectable in the data. Third, narrative generation uses a template-based approach to natural language generation to construct a textual overview of the organization. Our results are analyzed, and ideas for potential future research identified."
}
@article{ULRICKSON2012840,
title = "The effect of paramagnetic shift during thermal quench on internal components in fusion devices",
journal = "Fusion Engineering and Design",
volume = "87",
number = "5",
pages = "840 - 844",
year = "2012",
note = "Tenth International Symposium on Fusion Nuclear Technology (ISFNT-10)",
issn = "0920-3796",
doi = "https://doi.org/10.1016/j.fusengdes.2012.02.021",
url = "http://www.sciencedirect.com/science/article/pii/S0920379612000816",
author = "M.A. Ulrickson and J.D. Kotulski",
keywords = "Plasma disruption, Thermal quench, Electromagnetic load, Eddy current, Disruption simulation",
abstract = "A plasma current disruption is usually initiated by impurity influx that causes a rapid decrease in plasma thermal stored energy (thermal quench). Thermal quench occurs in 500–2000μs on a large device like ITER. Depending on the β value, the plasma may be either paramagnetic or diamagnetic. Thermal quench causes a large shift in paramagnetism (or diamagnetism) and a corresponding change in toroidal flux. The flux swing can be 1–2 Weber with the rate of change of the toroidal field between 25 and 150T/s for a device like ITER. The toroidal field shift induces poloidal current in the vessel and possibly in internal components. We have developed a method for simulating the thermal quench field shift that is compatible for use with the electromagnetic simulation codes. The method is based on a radially thin shell having the shape of the last closed flux surface with poloidal current driven to duplicate the toroidal field shift. The magnitude of the current and its time history are adjusted to duplicate the flux change during a disruption thermal quench. We will present the results of using this method to simulate the induced currents in a vacuum vessel having two shells."
}
@article{SCHUCHARD201779,
title = "The timing of spontaneous detection and repair of naming errors in aphasia",
journal = "Cortex",
volume = "93",
pages = "79 - 91",
year = "2017",
issn = "0010-9452",
doi = "https://doi.org/10.1016/j.cortex.2017.05.008",
url = "http://www.sciencedirect.com/science/article/pii/S0010945217301636",
author = "Julia Schuchard and Erica L. Middleton and Myrna F. Schwartz",
keywords = "Speech self-monitoring, Aphasia, Naming, Error detection, Repair",
abstract = "This study examined the timing of spontaneous self-monitoring in the naming responses of people with aphasia. Twelve people with aphasia completed a 615-item naming test twice, in separate sessions. Naming attempts were scored for accuracy and error type, and verbalizations indicating detection were coded as negation (e.g., “no, not that”) or repair attempts (i.e., a changed naming attempt). Focusing on phonological and semantic errors, we measured the timing of the errors and of the utterances that provided evidence of detection. The effects of error type and detection response type on error-to-detection latencies were analyzed using mixed-effects regression modeling. We first asked whether phonological errors and semantic errors differed in the timing of the detection process or repair planning. Results suggested that the two error types primarily differed with respect to repair planning. Specifically, repair attempts for phonological errors were initiated more quickly than repair attempts for semantic errors. We next asked whether this difference between the error types could be attributed to the tendency for phonological errors to have a high degree of phonological similarity with the subsequent repair attempts, thereby speeding the programming of the repairs. Results showed that greater phonological similarity between the error and the repair was associated with faster repair times for both error types, providing evidence of error-to-repair priming in spontaneous self-monitoring. When controlling for phonological overlap, significant effects of error type and repair accuracy on repair times were also found. These effects indicated that correct repairs of phonological errors were initiated particularly quickly, whereas repairs of semantic errors were initiated relatively slowly, regardless of their accuracy. We discuss the implications of these findings for theoretical accounts of self-monitoring and the role of speech error repair in learning."
}
@article{HARBOURNE2013438,
title = "Sit happens: Does sitting development perturb reaching development, or vice versa?",
journal = "Infant Behavior and Development",
volume = "36",
number = "3",
pages = "438 - 450",
year = "2013",
issn = "0163-6383",
doi = "https://doi.org/10.1016/j.infbeh.2013.03.011",
url = "http://www.sciencedirect.com/science/article/pii/S0163638313000441",
author = "Regina T. Harbourne and Michele A. Lobo and Gregory M. Karst and James Cole Galloway",
keywords = "Infant development, Sitting, Reaching",
abstract = "The development of reaching and of sitting during the first year of life is typically studied as separate yet related behaviors. Interestingly, very soon after learning to reach, 4–7-month-old infants start coordinating their arms with their trunk and legs for sitting. In this longitudinal study, we focused, for the first time, on how infants learn to use their arms for the dual tasks of reaching for objects while providing arm support as they learn to sit. We hypothesized that the use of arms for support during sitting development would be a temporary perturbation to reaching and result in a nonlinear progression of reaching skill. Eleven infants were studied monthly from the time they began to prop sit to the time of sitting independence (5–8 months of age). Behavioral coding, kinematics, and electromyography (EMG) characterized reaching and posture while infants sat as independently as possible. Results revealed significant changes across time in trunk movement and hand use as infants transitioned through three stages of sitting: with arm support, sitting briefly without arm support, and sitting independently. Infants used their hands more for contacting objects and less for posture support linearly across time. In contrast, changes in posture control as indicated by pelvis and trunk movement demonstrated a U-shaped curve with more movement of these two body segments during the middle stage of sitting than in the first or last stage. During the middle stage of sitting infants reached persistently even though posture control, measured by pelvis and trunk movement, appeared to be significantly challenged. Muscle activation consisted of tonic and variable combinations of muscle pairings in early sitting. As infants progressed to sitting without hand support, variable but successful strategies utilizing lower extremity muscles in a tight linkage with reach onset emerged to provide prospective control for reaching. Our findings support the contention that reaching both drives the development of sitting in infancy as well as perturbs sitting posture, factoring into the assembly of the complex dual sit–reach behavior that supports and expands flexible interaction with the environment."
}
@article{MARONATI2018,
title = "Assessing I2S-LWR economic competitiveness using systematic differential capital cost evaluation methodology",
journal = "Annals of Nuclear Energy",
year = "2018",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2018.05.057",
url = "http://www.sciencedirect.com/science/article/pii/S0306454918303013",
author = "G. Maronati and B. Petrovic and P. Ferroni",
keywords = "IS-LWR, Nuclear power plants, Economic, Capital cost, Overnight capital cost, Direct cost, Base cost, Total capital investment cost, TCIC, Code of Accounts",
abstract = "One of the main factors impeding nuclear power plant (NPP) construction is their economics. In this study, a systematic differential economics evaluation approach was developed through the use of the Code of Accounts guidelines to assess the costs of nuclear power plants. The methodology, entirely based on publicly available data, may serve as a template to evaluate direct costs for reactors of any size and design at any stage of developments. In particular, this approach was used to assess the costs of the Integral Inherently Safe Light Water Reactor (I2S-LWR). The I2S-LWR is a design concept of a large (∼1000 MWe) light water reactor. One of its key design features promoting inherent safety is implementation of an integral primary circuit configuration, which however necessitates a compact design of the core and primary circuit components. Through the methodology here presented, a representative loop PWR design was taken as a reference and the differential cost was estimated for each individual account based on the design difference, or similarity. Cost scaling techniques were applied to the accounts representing systems that differ from the ones of the reference PWR. Cost estimating techniques were used to evaluate cost of innovative components that are not part of standard PWR designs. By evaluating the cost difference of the I2S-LWR from the standard PWR, rather than the absolute cost, the uncertainty in the estimate is reduced. A similar approach was used by ORNL to estimate the cost of a Fluoride-salt High-temperature Reactor (FHR). A traditional four-loop PWR plant with a core thermal power of 3417 MWth (1144 MWe) was selected as the reference. Costs for that plant were prepared in 1978 by the Department of Energy (DOE) Energy Economics Data Base (EEDB), averaging actual cost incurred in the construction of several nuclear power plants (NPP), itemized with a great level of detail according to the Code of Accounts. This best estimate costs are denoted PWR12-BE. For each account, the cost of equipment, site labor and site material is provided. Industry experts at Westinghouse Electric Company performed a “sanity check” of the cost items, adjusting the cost of several items to match the current market and supply chain data. The detailed cost assessment of I2S-LWR was performed, systematically analyzing cost for each account, and applying the differential economics approach. First, Relative importance of each account, i.e., its contribution to the total cost was established, to help focus analysis on the most significant contributors. Moreover, the accounts describing components that are different than that of the PWR12-BE were identified. The integral configuration of the reactor has important implications on the cost of the reactor plant equipment (accounts 22x). Turbine generator equipment (Accounts 23x) is not believed to be much different than that of the reference design. I2S-LWR structures (Accounts 21x) mainly differ from that of a standard LWR as several buildings (containment building, shield building, annex building, waste processing building, fuel storage building) are integrated into a single building (nuclear island). Yardwork has a higher cost for the I2S-LWR, as the NI is partially below grade. On the other hand, due to its compact Nuclear Island footprint, I2S-LWR facilitates (and includes in its reference version) the use of seismic isolators, which contribute to reducing the direct cost. The total capital investment cost (TCIC), on the $/kW basis, of I2S-LWR is 5.84% lower than that of PWR12-BE, in spite of the lower power output of I2S-LWR. However, for the Western US (0.7 g), benefits of the seismic isolation are more pronounced, and the I2S-LWR total capital investment cost is 13.02% lower than that of PWR12-BE. If the I2S-LWR is compared to a PWR10-BE (traditional loop design, but scaled to the same power level as I2S-LWR), the savings are even higher, in the range 11.12%–17.89%. The analysis indicates that I2S-LWR has potential to offer an economically attractive design, with TCIC lower than that of a nuclear power plant based on a traditional loop PWR design. In other words, I2S-LWR design offers significantly enhanced safety, while at the same time improving economics. The differential economics approach developed in this paper can be used in identifying changes in cost of key components in order to improve the economics of a nuclear reactor design. The method can also help compare the economics of advanced Generation IV reactors and innovative water-cooled SMR (Small Modular Reactors) with respect to standard LWR technologies. In summary, the differential economics approach and can be used as a tool capable of helping stakeholder decisions."
}
@article{WELSH2016220,
title = "A computer code for calculations in the algebraic collective model of the atomic nucleus",
journal = "Computer Physics Communications",
volume = "200",
pages = "220 - 253",
year = "2016",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2015.10.017",
url = "http://www.sciencedirect.com/science/article/pii/S0010465515003975",
author = "T.A. Welsh and D.J. Rowe",
keywords = "Algebraic collective model, Bohr model, SO(5) Clebsch–Gordan coefficients",
abstract = "A Maple code is presented for algebraic collective model (ACM) calculations. The ACM is an algebraic version of the Bohr model of the atomic nucleus, in which all required matrix elements are derived by exploiting the model’s SU(1,1)×SO(5) dynamical group. This paper reviews the mathematical formulation of the ACM, and serves as a manual for the code. The code enables a wide range of model Hamiltonians to be analysed. This range includes essentially all Hamiltonians that are rational functions of the model’s quadrupole moments qˆM and are at most quadratic in the corresponding conjugate momenta πˆN (−2≤M,N≤2). The code makes use of expressions for matrix elements derived elsewhere and newly derived matrix elements of the operators [πˆ⊗qˆ⊗πˆ]0 and [πˆ⊗πˆ]LM. The code is made efficient by use of an analytical expression for the needed SO(5)-reduced matrix elements, and use of SO(5)⊃SO(3)  Clebsch–Gordan coefficients obtained from precomputed data files provided with the code.
Program summary
Program title: ACM Catalogue identifier: AEYO_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEYO_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 3873526 No. of bytes in distributed program, including test data, etc.: 46345414 Distribution format: tar.gz Programming language: Maple 18 (or versions 17, 16, 15). Computer: Any. Operating system: Any which supports Maple; tested under Linux, Max OSX, Windows 7. RAM: 500Mb Classification: 17.20. Nature of problem: The calculation of energy eigenvalues, transition rates and amplitudes of user specified Hamiltonians in the Bohr model of the atomic nucleus. Solution method: Exploit the model’s SU(1,1)×SO(5) dynamical group to calculate analytic (as far as possible) expressions for matrix elements, making use of extensive files (supplied) of SO(5)⊃SO(3)  Clebsch–Gordan coefficients. Diagonalisation of the resulting matrices (once the entries are converted to floating point) is carried out using the Maple library procedure Eigenvectors. (Maple [1] makes use of the NAG [2] and CLAPACK [3] linear algebra libraries.) Additional comments: 1.The dimension of the Hilbert space that can be handled is limited only by the available computer memory and the available SO(5)⊃SO(3)  Clebsch–Gordan coefficients (v1α1L1v2α2L2∥v3α3L3).2.The supplied data files provide coefficients (v1α1L1v2α2L2∥v3α3L3) for 1≤v2≤6, and contain all non-zero coefficients for v1<v3≤50 when v2∈1,3, for v1≤v3≤30 when v2∈2,4, and for v1≤v3≤25 when v2∈5,6. (Once calculated, further coefficients can be readily made available to the code without changing the code.) Thus, depending on the model Hamiltonian being analysed, the states in the Hilbert space used are limited in their seniority. For analysis of the more typical types of model Hamiltonian, only the coefficients with v2∈{1,3} are required, and therefore, with the supplied files, the seniority limit is 50. More exotic Hamiltonians having terms with seniority v2∈{2,4,5,6} would have the seniority limited to 30 or 25 accordingly.3.The code provides lower level procedures that give ready access to the Clebsch–Gordan coefficients and the SU(1, 1) and SO(5) matrix elements. These procedures are described in the manuscript and enable extensions to the code and model to be made easily.4.The accuracy to which Maple performs numerical calculations is determined by the Maple parameter Digits, which specifies the number of significant decimal digits used. The default value of 10 is more than adequate for most ACM calculations. Note, however, that if Digits is increased beyond a certain value (obtained from the Maple command evalhf(Digits), and usually 15 on modern computers) then the code can no longer take advantage of hardware mathematical operations, and is significantly slower. Documents included1.The code makes use of SO(5)⊃SO(3)  Clebsch–Gordan coefficients which are supplied in zip files, and must be installed by the user.2.A Maple worksheet that gives various example calculations and tests carried out using procedures from the code is provided.3.A 162 page PDF file containing everything displayed in the worksheet (input, output and comments, and making use of colour) is also provided. !!!!! The distribution file for this program is over 46 Mbytes and therefore is not delivered directly when download or Email is requested. Instead a html file giving details of how the program can be obtained is sent. !!!!! Running time: For a fixed value of the parameter Digits, the running time depends on the dimension of the Hilbert space on which the diagonalisation is performed, and this in turn is governed by the number of eigenvalues required and the accuracy required. Note that diagonalisation is performed separately in each L-space. For typical ACM calculations (such as those carried out in [4]), the matrices being diagonalised are usually of dimension at most a few hundred, and often much smaller. On a modest personal computer, the computation for the smallest cases takes at most a few seconds. The worksheet contains a range of examples for which the calculation time varies between a few seconds and 750s. In the latter case, diagonalisation is performed on L-spaces for 0≤L≤8, the dimensions of these spaces being between 154 and 616. References: [1]Maplesoft, Waterloo Maple Inc., Waterloo, ON, Canada.[2]NAG, www.nag.com.[3]CLAPACK, www.netlib.org/clapack.[4]D. J. Rowe, T. A. Welsh, M. A. Caprio, Phys. Rev. C 79(2009) 054304."
}
@article{WOJNOWICZ2017301,
title = "SUSPEND: Determining software suspiciousness by non-stationary time series modeling of entropy signals",
journal = "Expert Systems with Applications",
volume = "71",
pages = "301 - 318",
year = "2017",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2016.11.027",
url = "http://www.sciencedirect.com/science/article/pii/S0957417416306625",
author = "Michael Wojnowicz and Glenn Chisholm and Brian Wallace and Matt Wolff and Xuan Zhao and Jay Luan",
keywords = "Malware, Machine learning, Time series, Wavelet, Change points, Detrended fluctuation analysis",
abstract = "Commercial anti-virus software traditionally memorizes specific byte sequences (known as “signatures”) in the file contents of previously encountered malware. However, malware authors can evade signature-based detection in many ways; for instance, by using obfuscation techniques such as “packing” (encryption or compression) to hide snippets of malicious code; by writing metamorphic malware; or by tampering with existing malware. We hypothesize that certain evasion techniques can leave traces in the file’s entropy signal, revealing either similarities to known malware or the presence of tampering per se. To this end, we present SUSPEND (SUSPicious ENtropy signal Detector), an expert system which evaluates the suspiciousness of an executable file’s entropy signal in order to subserve malware classification. Whereas traditionally, entropy analysis has been used for the goal of packer detection (and therefore entropy-based features often merely comprise mean entropy or the entropy of a few file subcomponents), SUSPEND applies non-stationary time series modeling to aid in malware detection. In particular, SUSPEND (a) quantifies the “amount of structure” in the entropy signal (through detrended fluctuation analysis), (b) finds the location and size of sudden jumps in entropy (through mean change point modeling), and (c) computes the distribution of entropic variation across multiple spatial scales (through wavelet decomposition). In addition, SUSPEND (d) summarizes the entropy signal’s empirical probability distribution. Because SUSPEND’s run time can be made to scale linearly in file size, it is well-suited for large-scale malware analysis. We apply SUSPEND to a large-scale malware detection task with 500,000 heterogeneous real-world samples and over 1 million features. We find that SUSPEND boosts the predictive performance of traditional entropy analysis (as found in packer detectors) from 77.02% to 96.62%. Moreover, SUSPEND’s focus on entropy signals makes it a natural candidate for combining with other types of features; for instance, combining SUSPEND with a strings-based feature set boosts predictive accuracy from 97.18% to 98.62%. Thus, whereas traditionally, entropy analysis has focused on detecting that a file is packed, SUSPEND’s more comprehensive representation of the entropy signal helps to determine that a file is malicious. We illustrate the application of SUSPEND by studying 18 pieces of VirRansom, a family of viral ransomware which could cost millions to large organizations. SUSPEND is able to detect 100% of the studied files with over 99% confidence, whereas a more traditional strings-based model was very close to undecided and represents the entire family with a single string."
}
@article{PRUUNSILD2017122,
title = "Networks of Cultured iPSC-Derived Neurons Reveal the Human Synaptic Activity-Regulated Adaptive Gene Program",
journal = "Cell Reports",
volume = "18",
number = "1",
pages = "122 - 135",
year = "2017",
issn = "2211-1247",
doi = "https://doi.org/10.1016/j.celrep.2016.12.018",
url = "http://www.sciencedirect.com/science/article/pii/S2211124716317089",
author = "Priit Pruunsild and C. Peter Bengtson and Hilmar Bading",
keywords = "human neurons, neuronal networks, NMDA receptor, calcium signaling, transcription, genome-wide, species specificity, human adaptogenomics, ",
abstract = "Summary
Long-term adaptive responses in the brain, such as learning and memory, require synaptic activity-regulated gene expression, which has been thoroughly investigated in rodents. Using human iPSC-derived neuronal networks, we show that the human and the mouse synaptic activity-induced transcriptional programs share many genes and both require Ca2+-regulated synapse-to-nucleus signaling. Species-specific differences include the noncoding RNA genes BRE-AS1 and LINC00473 and the protein-coding gene ZNF331, which are absent in the mouse genome, as well as several human genes whose orthologs are either not induced by activity or are induced with different kinetics in mice. These results indicate that lineage-specific gain of genes and DNA regulatory elements affects the synaptic activity-regulated gene program, providing a mechanism driving the evolution of human cognitive abilities."
}
@article{HAYES2017166,
title = "Constructive and Unproductive Processing of Traumatic Experiences in Trauma-Focused Cognitive-Behavioral Therapy for Youth",
journal = "Behavior Therapy",
volume = "48",
number = "2",
pages = "166 - 181",
year = "2017",
note = "Special Issue: Treating Posttraumatic Stress Disorder (PTSD): Innovations and Understanding Processes of Change",
issn = "0005-7894",
doi = "https://doi.org/10.1016/j.beth.2016.06.004",
url = "http://www.sciencedirect.com/science/article/pii/S0005789416300375",
author = "Adele M. Hayes and Carly Yasinski and Damion Grasso and C. Beth Ready and Elizabeth Alpert and Thomas McCauley and Charles Webb and Esther Deblinger",
keywords = "exposure, emotional processing, cognitive processing, PTSD, Trauma-Focused CBT",
abstract = "Although there is substantial evidence to support the efficacy of cognitive-behavioral treatments (CBT) for posttraumatic stress disorder (PTSD), there is some debate about how these treatments have their effects. Modern learning theory and cognitive and emotional processing theories highlight the importance of reducing avoidance, facilitating the constructive processing of feared experiences, and strengthening new inhibitory learning. We examined variables thought to be associated with unproductive and constructive processing of traumatic experiences in a sample of 81 youth with elevated PTSD symptoms, who received Trauma-Focused Cognitive Behavioral Therapy (TF-CBT) for abuse or traumatic interpersonal loss. Sessions during the trauma narrative phase of TF-CBT were coded for indicators of unproductive processing (overgeneralization, rumination, avoidance) and constructive processing (decentering, accommodation of corrective information), as well as levels of negative emotion. In previous analyses of this trial (Ready et al., 2015), more overgeneralization during the narrative phase predicted less improvement in internalizing symptoms at posttreatment and a worsening of externalizing symptoms over the 12-month follow-up. In contrast, more accommodation predicted improvement in internalizing symptoms and also moderated the negative effects of overgeneralization on internalizing and externalizing symptoms. The current study examined correlates of overgeneralization and accommodation. Overgeneralization was associated with more rumination, less decentering, and more negative emotion, suggesting immersion in trauma-related material. Accommodation was associated with less avoidance and more decentering, suggesting a healthy distance from trauma-related material that might allow for processing and cognitive change. Decentering also predicted improvement in externalizing symptoms at posttreatment. Rumination and avoidance showed important associations with overgeneralization and accommodation, respectively, but did not predict treatment outcomes. This study identifies correlates of overgeneralization and accommodation that might shed light on how these variables relate to unproductive and constructive processing of traumatic experiences."
}
@article{MILLER2016115.e1,
title = "Stillbirth evaluation: a stepwise assessment of placental pathology and autopsy",
journal = "American Journal of Obstetrics and Gynecology",
volume = "214",
number = "1",
pages = "115.e1 - 115.e6",
year = "2016",
issn = "0002-9378",
doi = "https://doi.org/10.1016/j.ajog.2015.08.049",
url = "http://www.sciencedirect.com/science/article/pii/S000293781500931X",
author = "Emily S. Miller and Lucy Minturn and Rebecca Linn and Debra E. Weese-Mayer and Linda M. Ernst",
keywords = "autopsy, perinatal pathology, placental pathology, stillbirth",
abstract = "Background
The American Congress of Obstetricians and Gynecologists places special emphasis on autopsy as one of the most important tests for evaluation of stillbirth. Despite a recommendation of an autopsy, many families will decline the autopsy based on religious/cultural beliefs, fear of additional suffering for the child, or belief that no additional information will be obtained or of value. Further, many obstetric providers express a myriad of barriers limiting their recommendation for a perinatal autopsy despite their understanding of its value. Consequently, perinatal autopsy rates have been declining. Without the information provided by an autopsy, many women are left with unanswered questions regarding cause of death for their fetus and without clear management strategies to reduce the risk of stillbirth in future pregnancies. To avoid this scenario, it is imperative that clinicians are knowledgeable about the benefit of autopsy so they can provide clear information on its diagnostic utility and decrease potential barriers; in so doing the obstetrician can ensure that each family has the necessary information to make an informed decision.
Objective
We sought to quantify the contribution of placental pathologic examination and autopsy in identifying a cause of stillbirth and to identify how often clinical management is modified due to each result.
Study Design
This is a cohort study of all cases of stillbirth from 2009 through 2013 at a single tertiary care center. Records were reviewed in a stepwise manner: first the clinical history and laboratory results, then the placental pathologic evaluation, and finally the autopsy. At each step, a cause of death and the certainty of that etiology were coded. Clinical changes that would be recommended by information available at each step were also recorded.
Results
Among the 144 cases of stillbirth examined, 104 (72%) underwent autopsy and these cases constitute the cohort of study. The clinical and laboratory information alone identified a cause of death in 35 (24%). After placental pathologic examination, 88 (61%) cases had a probable cause of death identified. The addition of autopsy resulted in 78 (74%) cases having an identifiable probable cause of death. Placental examination alone changed clinical management in 52 (36%) cases. Autopsy led to additional clinical management changes in 6 (6%) cases.
Conclusion
This stepwise assessment of the benefit of both placental pathological examination and autopsy in changing probable cause of death beyond traditional clinical history and laboratory results emphasizes the need to implement more comprehensive evaluation of all stillbirths. With the aim of providing a cause of stillbirth to the parents, and to prevent future stillbirths, it behooves health care professionals to understand the value of this more comprehensive approach and convey that information to the bereaved parents."
}
@article{WANG201334,
title = "The Limited Utility of Screening Laboratory Tests and Electrocardiograms in the Management of Unintentional Asymptomatic Pediatric Ingestions",
journal = "The Journal of Emergency Medicine",
volume = "45",
number = "1",
pages = "34 - 38",
year = "2013",
issn = "0736-4679",
doi = "https://doi.org/10.1016/j.jemermed.2012.11.056",
url = "http://www.sciencedirect.com/science/article/pii/S0736467912015806",
author = "George Sam Wang and Sara Deakyne and Lalit Bajaj and Shan Yin and Kennon Heard and Genie Roosevelt",
keywords = "pediatric poisonings, ingestions, toxicology, regional poison center, screening laboratory testing, electrocardiogram",
abstract = "Background
Suspected ingestions are a common chief complaint to the emergency department although the majority of ingestions by children are insignificant.
Objective
Assess the utility of screening laboratory tests and Electrocardiograms (ECGs) in unintentional asymptomatic pediatric poisonings.
Methods
Retrospective chart review at a tertiary care children's hospital and a regional poison center of patients less than 12 years of age using ICD-9 codes from January 2005 through December 2008. Laboratory or ECG results requiring intervention and/or direct treatment, a non-RPC subspecialty consultation, and/or prolonged Emergency Department stay was considered changed management.
Results
Five hundred ninety five suspected ingestions met our criteria. The median age was 2.6 years (IQR 1.6, 3.0 years) and 56% were male. One laboratory test or ECG was obtained in 233 patients (39%). Of 24 screening ECGs, 32 complete blood counts and 34 blood gases, none were clinically significant. Fifty-two patients received screening metabolic panels, 3 were abnormal and 2 changed management (anion gap metabolic acidosis with unsuspected salicylate ingestions). None of the 127 (21%) screening acetaminophen levels changed management. Two of sixty-five (13%) screening salicylate levels changed management. Three screening urine toxicology tests on patients with altered mental status were positive without ingestion history. No patient under the age of 12 years with normal vital signs and normal mental status had positive screening tests.
Conclusions
Screening laboratory tests and ECGs were of limited utility and rarely changed management despite being ordered in a significant number of patients. Screening tests are rarely indicated in unintentional overdoses in children who are asymptomatic."
}
@article{FORNACIAI2014123,
title = "Fast Translational Motion, but not Radial, Circular or Biological Motion, Causes Spatially Selective Adaptation of Event Duration",
journal = "Procedia - Social and Behavioral Sciences",
volume = "126",
pages = "123 - 124",
year = "2014",
note = "International Conference on Timing and Time Perception, 31 March – 3 April 2014, Corfu, Greece",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2014.02.339",
url = "http://www.sciencedirect.com/science/article/pii/S1877042814018874",
author = "Michele Fornaciai and Roberto Arrighi and David C. Burr",
keywords = "Adaptation-based time compression, Motion adaptation, Spatiotopic coding",
abstract = "It has been recently shown that adaptation to gratings oscillating at high-frequency compress perceived duration of subsequent, slower, stimuli displayed in the same location. These temporal distortions are spatially selective (Johnston et al., 2006), coded in spatiotopic coordinates (Burr et al., 2007, Burr et al., 2011), and do not result from changes in perceived speed of the adapted stimulus. These findings are important as they support the idea of a distributed framework of multiple mechanisms processing event time across the visual field. However, time distortions induced by motion adaptation have been tested only with simple translational motion, rather than more complex motions. Here we used a similar technique to Burr and coll. (2007) to measure time compression induced by adaptation to four kinds of motion stimuli: a) translating gratings, b) expanding concentric gratings, c) rotating radial gratings, and d) biological motion. We first measured changes in perceived speed caused by the motion adaptation (20Hz for the gratings). We found that perceived speed of the 10Hz test stimuli was dramatically reduced by around 40-50% in all conditions. Subsequently, we used these data to adjust the speed to compensate for the adaptation effects, and measure distortions in perceived duration with equated speed of test and probe gratings. In line with previous reports, perceived duration for translating patterns (displayed for 500ms) was found to be reduced up to 30-40%, even after speed compensation. However, duration estimates for radial and circular moving gratings (500ms) were always veridical. We used two versions of circular motion: with tangential speed constant across the visual patch (non-rigid-motion), and with rigid rotation, but neither caused temporal compression. Similarly, adaptation to a “runner” in biological motion reduced considerably the apparent speed of a briefly presented walker, but when matched for apparent speed, duration estimations were completely unaffected by adaptation. Taken together these results suggest that different mechanisms underlie time processing for simple and complex motion profiles. Why should only translational motion cause changes in temporal duration? fMRI evidence (Morrone et al., 2000) suggests that translational motion stimulates different brain regions from those responding to radial and circular motion, and this could be the basis for the selective effects on duration. Interestingly, however, the duration effects (after matching for apparent speed) are almost entirely spatiotopic, suggesting that they are not occurring at early, retinotopic brain areas. On the other hand it makes sense that the adaption should occur in space-based rather than retinal-based coordinates, if the purpose is related to temporal calibration of objects (in the real world). Why translational motion, but not other more complex forms of motion, should calibrate event duration is far from clear."
}
@article{KNOPF201722,
title = "Zeitliche Entwicklung der Anwendungsprävalenz von Statinen in Deutschland – Ergebnisse der nationalen Interview- und Untersuchungssurveys 1997-1999 und 2008-2011",
journal = "Zeitschrift für Evidenz, Fortbildung und Qualität im Gesundheitswesen",
volume = "122",
pages = "22 - 31",
year = "2017",
issn = "1865-9217",
doi = "https://doi.org/10.1016/j.zefq.2017.04.001",
url = "http://www.sciencedirect.com/science/article/pii/S1865921717300478",
author = "Hildtraud C. Knopf and Markus A. Busch and Yong Du and Julia Truthmann and Anja Schienkiewitz and Christa Scheidt-Nave",
keywords = "Statine, zeitliche Entwicklung, Interview- und Untersuchungssurvey, Erwachsene, kardiovaskuläre Erkrankung, Anwendungsprävalenz, statin use, time trends, national health interview and examination surveys, adults, cardiovascular disease, user prevalence",
abstract = "Zusammenfassung
Hintergrund
Evidenzbasierte Empfehlungen zur Behandlung mit Lipidsenkern, insbesondere mit Statinen nehmen einen zentralen Platz in der Therapie von Fettstoffwechselstörungen und in der Prävention kardiovaskulärer Ereignisse ein. In Deutschland liefern Daten der Gesetzlichen Krankenversicherungen (GKV) Informationen über zeitliche Entwicklungen in der Verordnung von Lipidsenkern. Was fehlt sind bevölkerungsbezogene Daten zu Veränderungen in der Anwendungsprävalenz nach soziodemographischen und gesundheitsrelevanten Merkmalen. Mit bundesweiten Interview- und Untersuchungssurveys bei Erwachsenen in Deutschland 1997-1999 (BGS98) und 2008-2011 (DEGS1) soll diese Informationslücke unter besonderer Berücksichtigung der Statinanwendung geschlossen werden.
Methoden
Die Studienpopulation umfasste 7.099 Teilnehmende in 1997-1999 und 7.091 in 2008-2011, die zum Zeitpunkt der jeweiligen Surveyerhebung 18-79 Jahre alt waren. Primärdaten zur Arzneimittelanwendung innerhalb der letzten 7 Tage vor dem Survey wurden mittels standardisierter Arzneimittelinterviews und Brown-Bag-Verfahren erhoben. Die Pharmazentralnummern auf den Originalverpackungen wurden eingescannt und anhand der aktuellsten Version des anatomisch-therapeutisch-chemischen Klassifikationssystems (ATC) kodiert. Die Krankengeschichte wurde mittels computer-assistierter Interviews erhoben. Schlaganfall und koronare Herzkrankheit (KHK) wurden nur bei Personen im Alter von 40-79 Jahren erfasst und als kardiovaskuläre Erkrankung definiert. Adipositas wurde auf der Grundlage standardisierter Messungen von Körpergewicht und Körpergröße als Body Mass Index (BMI) von >=30kg/m2 definiert. Information zu soziodemographischen Variablen und Art der Krankenversicherung wurde über einen Selbstausfüll-Fragebogen erhoben. In querschnittlichen deskriptiven Analysen wurde die Anwendungsprävalenz von Statinen (ATC-Codes: C10AA, C10BA, C10BX) nach Survey sowie die Veränderung zwischen den Surveys stratifiziert nach relevanten Vorerkrankungen und anderen Kovariablen berechnet. Der Zusammenhang zwischen Erhebungszeitpunkt und Statinanwendung wurde in multivariablen binären logistischen Regressionsmodellen bei Personen im Alter von 40-79 Jahren analysiert. Alle Ergebnisse wurden gewichtet und auf die Bevölkerung von 2010 standardisiert.
Ergebnisse
Zwischen den Surveyperioden 1997-1999 und 2008-2011 stieg die Anwendungsprävalenz von Statinen von 3,2% auf 8,8%. Die Zunahme war besonders ausgeprägt bei Personen im Alter von 65-79 Jahren (7,2% vs. 26,9%) und bei Personen mit relevanten Vorerkrankungen wie KHK (19,1% vs. 54,9%), Schlaganfall (17,1% vs. 50,1%), Diabetes mellitus (10,5% vs. 33,2%) und Fettstoffwechselstörung (12,6% vs. 27,8%). Bei Personen im Alter von 40-79 Jahren stieg die Prävalenz der Statinanwendung unabhängig von Kovariablen signifikant zwischen den beiden Surveyzeitpunkten an (Odds Ratio:3,70; 95% KI: 2,92-4,70). Dies betraf sowohl Personen mit (5,17; 3,50-7,64) als auch ohne kardiovaskuläre Erkrankungen (2,76; 2,07-3,67).
Schlussfolgerung
Die Zunahme in der Anwendungsprävalenz von Statinen in Deutschland zwischen den bundesweiten Gesundheitssurveys 1997-1999 und 2008-2011 reflektiert die Umsetzung aktueller Leitlinienempfehlungen ohne Hinweis auf Ungleichheit nach Geschlecht, Bildung, Art der Krankenversicherung oder Wohnregion. Diese bevölkerungsbezogenen Daten der Gesundheitssurveys ergänzen Informationen zur Verordnung von Statinen auf der Grundlage von GKV-Daten. Limitationen Survey basierter Informationen resultieren aus potentiellem Fehlklassifikations- und Selektionsbias sowie aus den großen zeitlichen Abständen zwischen wiederholten Surveyerhebungen. In weiteren Untersuchungen muss geklärt werden, warum beobachtete Anwendungsprävalenzen für Statine bei Personen mit kardiovaskulärer Erkrankung hinter den aktuellen Leitlinienempfehlungen zur kardiovaskulären Sekundärprävention zurückbleiben.
Background
Evidence-based guideline recommendations on lipid lowering drug treatment, in particular statin treatment, play an essential role in the management of dyslipidemias and in the prevention of cardiovascular disease events. In Germany, statutory health insurance data provide information on time trends in the prescription of lipid lowering drugs. However, population-based data regarding changes in user prevalence according to socio-demographic and health-related characteristics are lacking. Based on data from national health interview and examination surveys for adults in Germany 1997-1999 (GNHIES98) and 2008-2010 (DEGS1), the present analysis aims to close this information gap with a particular focus on the use of statins.
Methods
The study population consisted of 7,099 participants (GNHIES98) and 7,091 participants (DEGS1) aged 18 to 79 years at the time of the respective surveys. Primary data on medication use within 7 days prior to the survey were collected using standardized medication interviews and brown-bag drug review. Unique product identifiers on original drug containers were scanned and coded according to the latest version of the Anatomical Therapeutic Chemical (ATC) classification system. Medical history was obtained in computer-assisted personal interviews. A history of stroke or coronary heart disease (CHD) was assessed among persons aged 40 to 79 years only, and previous stroke or CHD were defined as cardiovascular disease. Obesity was defined as a body mass index (BMI) of ≥ 30kg/m2) based on calculation from standardized measures of body weight and height. Information on socio-demographic variables and type of health insurance was collected using standardized self-administered questionnaires. In cross-sectional descriptive analyses we calculated the prevalence of statin use (ATC codes: C10AA, C10BA, C10BX) by survey as well as the changes between surveys stratified according to relevant preexisting diseases and other co-variables. The association between survey period and statin use was analyzed in multivariable binary logistic regression models among persons aged 40 to 79 years. All results were weighted and standardized for the population of 2010.
Results
Between the two survey periods 1997-1999 and 2008-2011, the prevalence of statin use increased from 3.2 % to 8.8 %. The increase was most pronounced for the age group 65 to 79 years (7.2 % vs. 26.9 %) and among persons with relevant preexisting conditions, such as CHD (19.1 % vs. 54.9 %), stroke (17.1 % vs. 50.1 %), diabetes mellitus (10.5 % vs. 33.2 %), and dyslipidemia (12.6 % vs. 27.8 %). Among persons aged 40 to 79 years, the prevalence of statin use significantly increased between the two surveys, independent of co-variables (Odds Ratio: 3.70; 95 % confidence interval [CI]: 2.92 to 4.70). This applied to persons with cardiovascular disease (5.17; 3.50 to 7.64) and without cardiovascular disease (2.76; 2.07 to 3.67).
Conclusion
The increase in the prevalence of statin use in Germany between the two national health surveys (1997–1999 and 2008–2011) reflects the implementation of current guideline recommendations without evidence for inequalities according to gender, education, type of health insurance or region of residence. These population-based data add to information on statin prescription obtained from statutory health insurance data. Limitations of survey-based information derive from potential misclassification and selection bias as well as large time gaps between the survey periods. Further studies are needed to examine why the observed prevalence of statin use among persons with cardiovascular morbidity lags behind current guideline recommendations for secondary cardiovascular prevention."
}
@article{ABDALA2017341,
title = "Influence of low frequency PSEN1 variants on familial Alzheimer’s disease risk in Brazil",
journal = "Neuroscience Letters",
volume = "653",
pages = "341 - 345",
year = "2017",
issn = "0304-3940",
doi = "https://doi.org/10.1016/j.neulet.2017.05.053",
url = "http://www.sciencedirect.com/science/article/pii/S0304394017304512",
author = "Bianca Barbosa Abdala and Jussara Mendonça dos Santos and Andressa Pereira Gonçalves and Luciana Branco da Motta and Jerson Laks and Margarete Borges de Borges and Márcia Mattos Gonçalves Pimentel and Cíntia Barros Santos-Rebouças",
keywords = "Alzheimer’s disease, , Rare variants, Mutational screening",
abstract = "About 30–70% of familial Alzheimer’s disease (AD) cases are related to mutations in presenilin-1 gene (PSEN1). Although the role of mutations and common variants in AD had been extensively investigated, the contribution of rare or low frequency PSEN1 variants on AD risk remains unclear. In the current study, we performed a mutational screening of PSEN1 coding exons and flanking intronic sequences among 53 index cases with familial history of AD from Rio de Janeiro (Brazil). Two missense variants (rs63750592; rs17125721), one rare and a low frequency variant, and two intronic variants (rs3025786; rs165932) were identified. In silico tools were used to predict the functional impact of the variants, revealing no changes in protein functionality by exonic variants. Otherwise, all variants were predicted to alter splicing signals. Prediction results, together with previous reports, suggest a correlation between rs17125721 and AD. So, a subsequent case-control study to evaluate the role of rs1712572 on AD risk was performed in an additional sample of 120 AD sporadic cases and in 149 elderly healthy controls by TaqMan Genotyping Assay. Our data indicates a risk association for rs17125721 in familial AD cases (OR=6.0; IC95%=1.06–33.79; p=0.042). In addition, we tested the multiplicative interaction between allele ε4 of the apolipoprotein E (APOE) and rs17125721 and no statistical association was found. Taken together, our findings provide new insight about the genetic relevance of low frequency PSEN1 variants for familial AD development."
}
@article{CATTERALL20121336,
title = "An object oriented code for simulating supersymmetric Yang–Mills theories",
journal = "Computer Physics Communications",
volume = "183",
number = "6",
pages = "1336 - 1353",
year = "2012",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2012.01.024",
url = "http://www.sciencedirect.com/science/article/pii/S0010465512000537",
author = "Simon Catterall and Anosh Joseph",
keywords = "Lattice gauge theory, Supersymmetric Yang–Mills, Rational hybrid Monte Carlo, Object oriented programming",
abstract = "We present SUSY_LATTICE – a C++ program that can be used to simulate certain classes of supersymmetric Yang–Mills (SYM) theories, including the well known N=4 SYM in four dimensions, on a flat Euclidean space–time lattice. Discretization of SYM theories is an old problem in lattice field theory. It has resisted solution until recently when new ideas drawn from orbifold constructions and topological field theories have been brought to bear on the question. The result has been the creation of a new class of lattice gauge theories in which the lattice action is invariant under one or more supersymmetries. The resultant theories are local, free of doublers and also possess exact gauge-invariance. In principle they form the basis for a truly non-perturbative definition of the continuum SYM theories. In the continuum limit they reproduce versions of the SYM theories formulated in terms of twisted fields, which on a flat space–time is just a change of the field variables. In this paper, we briefly review these ideas and then go on to provide the details of the C++ code. We sketch the design of the code, with particular emphasis being placed on SYM theories with N=(2,2) in two dimensions and N=4 in three and four dimensions, making one-to-one comparisons between the essential components of the SYM theories and their corresponding counterparts appearing in the simulation code. The code may be used to compute several quantities associated with the SYM theories such as the Polyakov loop, mean energy, and the width of the scalar eigenvalue distributions.
Program summary
Program title: SUSY_LATTICE Catalogue identifier: AELS_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AELS_v1_0.html Program obtainable from: CPC Program Library, Queenʼs University, Belfast, N. Ireland Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 9315 No. of bytes in distributed program, including test data, etc.: 95 371 Distribution format: tar.gz Programming language: C++ Computer: PCs and Workstations Operating system: Any, tested on Linux machines Classification:: 11.6 Nature of problem: To compute some of the observables of supersymmetric Yang–Mills theories such as supersymmetric action, Polyakov/Wilson loops, scalar eigenvalues and Pfaffian phases. Solution method: We use the Rational Hybrid Monte Carlo algorithm followed by a Leapfrog evolution and a Metropolis test. The input parameters of the model are read in from a parameter file. Restrictions: This code applies only to supersymmetric gauge theories with extended supersymmetry, which undergo the process of maximal twisting. (See Section 2 of the manuscript for details.) Running time: From a few minutes to several hours depending on the amount of statistics needed."
}
@article{SELIGMANN201422,
title = "Mitochondrial swinger replication: DNA replication systematically exchanging nucleotides and short 16S ribosomal DNA swinger inserts",
journal = "Biosystems",
volume = "125",
pages = "22 - 31",
year = "2014",
issn = "0303-2647",
doi = "https://doi.org/10.1016/j.biosystems.2014.09.012",
url = "http://www.sciencedirect.com/science/article/pii/S0303264714001555",
author = "Hervé Seligmann",
keywords = "Mitochondrial replication, Swinger DNA polymerization, Invertase, 3′-to-5′ Polymerization, Swinger repeat, Mitochondrial recombination, Gene duplication, Asexual vertebrate reproduction",
abstract = "Assuming systematic exchanges between nucleotides (swinger RNAs) resolves genomic ‘parenthood’ of some orphan mitochondrial transcripts. Twenty-three different systematic nucleotide exchanges (bijective transformations) exist. Similarities between transcription and replication suggest occurrence of swinger DNA. GenBank searches for swinger DNA matching the 23 swinger versions of human and mouse mitogenomes detect only vertebrate mitochondrial swinger DNA for swinger type AT+CG (from five different studies, 149 sequences) matching three human and mouse mitochondrial genes: 12S and 16S ribosomal RNAs, and cytochrome oxidase subunit I. Exchange A<->T+C<->G conserves self-hybridization properties, putatively explaining swinger biases for rDNA, against protein coding genes. Twenty percent of the regular human mitochondrial 16S rDNA consists of short swinger repeats (from 13 exchanges). Swinger repeats could originate from recombinations between regular and swinger DNA: duplicated mitochondrial genes of the parthenogenetic gecko Heteronotia binoei include fewer short A<->T+C<->G swinger repeats than non-duplicated mitochondrial genomes of that species. Presumably, rare recombinations between female and male mitochondrial genes (and in parthenogenetic situations between duplicated genes), favors reverse-mutations of swinger repeat insertions, probably because most inserts affect negatively ribosomal function. Results show that swinger DNA exists, and indicate that swinger polymerization contributes to the genesis of genetic material and polymorphism."
}
@article{HARTUNG2015114,
title = "Changes in Long-acting β-agonist Utilization After the FDA’s 2010 Drug Safety Communication",
journal = "Clinical Therapeutics",
volume = "37",
number = "1",
pages = "114 - 123.e1",
year = "2015",
issn = "0149-2918",
doi = "https://doi.org/10.1016/j.clinthera.2014.10.025",
url = "http://www.sciencedirect.com/science/article/pii/S0149291814006985",
author = "Daniel M. Hartung and Luke Middleton and Sheila Markwardt and Kaylee Williamson and Kathy Ketchum",
keywords = "adrenergic β-agonists, Medicaid, US Food and Drug Administration, utilization",
abstract = "Purpose
In February 2010, the US Food and Drug Administration (FDA) issued new recommendations for the safe use of long-acting β-agonists (LABAs) in patients with asthma. The objective of this study was to determine the impact of the FDA’s 2010 safety advisory on LABA utilization.
Methods
Using administrative data from the Oregon Medicaid program, we performed an interrupted time series regression to evaluate changes in the trend in new LABA prescriptions before and after the FDA’s 2010 advisory. Trends in incident fills were examined among those with and without an asthma diagnosis code and previous respiratory controller medication use; trends were also assessed according to patient age.
Findings
The average age of the 8646 study patients was 37 years, 53% had a diagnosis of asthma, 21% had no respiratory diagnosis, and 32% had not used a respiratory controller medication in the recent past. The trend in new LABA prescriptions declined by 0.09 new start per 10,000 patients per month (95% CI, –0.19 to –0.01) after the FDA’s advisory. Among those with a diagnosis of asthma, there was an immediate drop of 0.48 (95% CI, –0.93 to –0.03) and a 0.10 (95% CI, –0.13 to –0.06) decline in the monthly rate of new starts per 10,000 patients. Immediately after the FDA’s advisory, we observed a statistically significant 4.7% increase (95% CI, 0.8 to 8.7) in the proportion of new LABA starts with history of previous respiratory controller medication use. Utilization of LABAs did not change in those without a diagnosis of asthma.
Implications
The FDA’s 2010 advisory was associated with modest reductions in LABA utilization overall and in ways highlighted in their recommendations."
}
@article{JIANG2012111,
title = "Numerical simulation of impact tests on reinforced concrete beams",
journal = "Materials & Design",
volume = "39",
pages = "111 - 120",
year = "2012",
issn = "0261-3069",
doi = "https://doi.org/10.1016/j.matdes.2012.02.018",
url = "http://www.sciencedirect.com/science/article/pii/S0261306912000751",
author = "Hua Jiang and Xiaowo Wang and Shuanhai He",
keywords = "A. Concrete, E. Impact and ballistic, F. Plastic behavior",
abstract = "This paper focuses on numerical simulation of impact tests of reinforced concrete (RC) beams by the LS-DYNA finite element (FE) code. In the FE model, the elasto-plastic damage cap (EPDC) model, which is based on continuum damage mechanics in combination with plasticity theory, is used for concrete, and the reinforcement is assumed to be elasto-plastic. The numerical results compares well with the experimental values reported in the literature, in terms of impact force history, mid-span deflection history and crack patterns of RC beams. By comparing the numerical and experimental results, several important behavior of concrete material is investigated, which includes: damage variable to describe the strain softening section of stress–strain curve; the cap surface to describe the plastic volume change; the shape of the meridian and deviatoric plane to describe the yield surface as well as two methods of incorporating rebar into concrete mesh. This study gives a good example of using EPDC model and can be utilized for the development new constitutive models for concrete in future."
}
@article{PASQUALI2015932,
title = "Measuring Hospital Performance in Congenital Heart Surgery: Administrative Versus Clinical Registry Data",
journal = "The Annals of Thoracic Surgery",
volume = "99",
number = "3",
pages = "932 - 938",
year = "2015",
issn = "0003-4975",
doi = "https://doi.org/10.1016/j.athoracsur.2014.10.069",
url = "http://www.sciencedirect.com/science/article/pii/S000349751402089X",
author = "Sara K. Pasquali and Xia He and Jeffrey P. Jacobs and Marshall L. Jacobs and Michael G. Gaies and Samir S. Shah and Matthew Hall and J. William Gaynor and Eric D. Peterson and John E. Mayer and Jennifer C. Hirsch-Romano",
abstract = "Background
In congenital heart surgery, hospital performance has historically been assessed using widely available administrative data sets. Recent studies have demonstrated inaccuracies in case ascertainment (coding and inclusion of eligible cases) in administrative versus clinical registry data; however, it is unclear whether this impacts assessment of performance on a hospital level.
Methods
Merged data from The Society of Thoracic Surgeons (STS) database (clinical registry) and the Pediatric Health Information Systems (PHIS) database (administrative data set) for 46,056 children undergoing cardiac operations (2006–2010) were used to evaluate in-hospital mortality for 33 hospitals based on their administrative versus registry data. Standard methods to identify/classify cases were used: Risk Adjustment in Congenital Heart Surgery, version 1 (RACHS-1) in the administrative data and STS–European Association for Cardiothoracic Surgery (STAT) methodology in the registry.
Results
Median hospital surgical volume based on the registry data was 269 cases per year; mortality was 2.9%. Hospital volumes and mortality rates based on the administrative data were on average 10.7% and 4.7% lower, respectively, although this varied widely across hospitals. Hospital rankings for mortality based on the administrative versus registry data differed by 5 or more rank positions for 24% of hospitals, with a change in mortality tertile classification (high, middle, or low mortality) for 18% and a change in statistical outlier classification for 12%. Higher volume/complexity hospitals were most impacted. Agency for Healthcare Quality and Research (AHRQ) methods in the administrative data yielded similar results.
Conclusions
Inaccuracies in case ascertainment in administrative versus clinical registry data can lead to important differences in assessment of hospital mortality rates for congenital heart surgery."
}
@article{BOYLE2012499,
title = "Challenges of standardized continuous quality improvement programs in community pharmacies: The case of SafetyNET-Rx",
journal = "Research in Social and Administrative Pharmacy",
volume = "8",
number = "6",
pages = "499 - 508",
year = "2012",
issn = "1551-7411",
doi = "https://doi.org/10.1016/j.sapharm.2012.01.005",
url = "http://www.sciencedirect.com/science/article/pii/S155174111200006X",
author = "Todd A. Boyle and Neil J. MacKinnon and Thomas Mahaffey and Kellie Duggan and Natalie Dow",
keywords = "Community pharmacy, Continuous quality improvement, Medication error reporting",
abstract = "Background
Research on continuous quality improvement (CQI) in community pharmacies lags in comparison to service, manufacturing, and various health care sectors. As a result, very little is known about the challenges community pharmacies face when implementing CQI programs in general, let alone the challenges of implementing a standardized and technologically sophisticated one.
Objective
This research identifies the initial challenges of implementing a standardized CQI program in community pharmacies and how such challenges were addressed by pharmacy staff.
Methods
Through qualitative interviews, a multisite study of the SafetyNET-Rx CQI program involving community pharmacies in Nova Scotia, Canada, was performed to identify such challenges. Interviews were conducted with the CQI facilitator (ie, staff pharmacist or technician) in 55 community pharmacies that adopted the SafetyNET-Rx program. Of these 55 pharmacies, 25 were part of large national corporate chains, 22 were part of banner chains, and 8 were independent pharmacies. A total of 10 different corporate chains and banners were represented among the 55 pharmacies. Thematic content analysis using well-established coding procedures was used to explore the interview data and elicit the key challenges faced.
Results
Six major challenges were identified, specifically finding time to report, having all pharmacy staff involved in quality-related event (QRE) reporting, reporting apprehensiveness, changing staff relationships, meeting to discuss QREs, and accepting the online technology. Challenges were addressed in a number of ways including developing a manual-online hybrid reporting system, managers paying staff to meet after hours, and pharmacy managers showing visible commitment to QRE reporting and learning.
Conclusions
This research identifies key challenges to implementing CQI programs in community pharmacies and also provides a starting point for future research relating to how the challenges of QRE reporting and learning in community pharmacies change over time."
}
@article{CHEUNG2016e456,
title = "Leukoencephalopathy and long-term neurobehavioural, neurocognitive, and brain imaging outcomes in survivors of childhood acute lymphoblastic leukaemia treated with chemotherapy: a longitudinal analysis",
journal = "The Lancet Haematology",
volume = "3",
number = "10",
pages = "e456 - e466",
year = "2016",
issn = "2352-3026",
doi = "https://doi.org/10.1016/S2352-3026(16)30110-7",
url = "http://www.sciencedirect.com/science/article/pii/S2352302616301107",
author = "Yin Ting Cheung and Noah D Sabin and Wilburn E Reddick and Deepa Bhojwani and Wei Liu and Tara M Brinkman and John O Glass and Scott N Hwang and Deokumar Srivastava and Ching-Hon Pui and Leslie L Robison and Melissa M Hudson and Kevin R Krull",
abstract = "Summary
Background
Leukoencephalopathy is observed in some children undergoing chemotherapy for acute lymphoblastic leukaemia, although its effects on long-term outcomes is unknown. This study examines the associations between acute leukoencephalopathy and neurobehavioural, neurocognitive, and brain white matter imaging outcomes in long-term survivors of childhood acute lymphoblastic leukaemia treated with chemotherapy without cranial radiation.
Methods
In this longitudinal analysis, we used data of children with acute lymphoblastic leukaemia at St Jude Children's Research Hospital (Memphis, TN, USA) who had been treated between June 1, 2000, and Oct 31, 2010. Eligible patients were diagnosed with non-B-cell acute lymphoblastic leukaemia, aged at least 8 years, and survivors with at least 5 years since their initial diagnosis. Brain MRIs obtained during active therapy were systematically coded for leukoencephalopathy using Common Terminology Criteria for Adverse Event version 4. At least 5 years after their diagnosis, survivors completed neurocognitive testing, another brain MRI, and their parents completed neurobehavioural ratings of their child (Behavior Rating Inventory of Executive Function [BRIEF]). Follow-up MRI included diffusion tensor imaging to assess white matter integrity, with indices of fractional anisotropy, axial diffusivity, and radial diffusivity from frontal lobes, parietal lobes, and in the frontostriatal tract. The neuroradiologist, who assessed abnormal MRIs, was masked to both group assignment of survivors and the neurobehavioural and neurocognitive outcomes. The primary outcomes were neurobehavioural function, assessed from completed BRIEF, and neurocognitive performance, measured by direct neurocognitive tests (Delis-Kaplan Executive Function System, Wechsler Intelligence Scale for Children-IV/Wechsler Adult Intelligence Scale-III, Rey-Osterrieth Complex Figure Test, and Lafayette Grooved Pegboard Test). This study had completed enrolment in October, 2014, and is registered as an observational study at ClinicalTrials.gov, number NCT01014195.
Findings
Between Feb 18, 2010, and Oct 22, 2014, 210 (70%) of 301 eligible survivors participated in our study of whom 190 were evaluable, 162 had an MRI. 56 participants had quantitative brain imaging data and were included in evaluable population analyses. 51 (27%) of the 190 evaluable participants had acute leukoencephalopathy. Compared with population norms, survivors were reported to have more neurobehavioural problems with working memory, organisation, initiation, and planning (p<0·001 for all). Survivors had worse scores than the general population on direct measures of memory span, processing speed, and executive function (p<0·05 for all). Survivors with a history of acute leukoencephalopathy had more neurobehavioural problems than survivors with no history of leukoencephalopathy on organisation (adjusted T-score 56·2 [95% CI 53·3–59·1] vs 52·2 [50·4–53·9], p=0·020) and initiation (55·5 [52·7–58·3] vs 52·1 [50·4–53·8], p=0·045). Survivors with acute leukoencephalopathy also had reduced white matter integrity in the frontostriatal tract at follow-up: lower fractional anisotropy (p=0·069), higher axial diffusivity (p=0·020), and higher radial diffusivity (p=0·0077). A one-unit change in the radial diffusivity index corresponded with a 15·0 increase in raw score points on initiation, 30·3 on planning, and 28·0 on working memory (p<0·05 for all).
Interpretation
Acute leukoencephalopathy during chemotherapy treatment, without cranial radiation, for childhood acute lymphoblastic leukaemia predicted higher risk for long-term neurobehavioural problems and reduced white matter integrity in frontal brain regions. Survivors of childhood acute lymphoblastic leukaemia might benefit from preventive cognitive or behavioural interventions, particularly those who develop acute leukoencephalopathy.
Funding
National Institute of Mental Health, National Cancer Institute, American Lebanese Syrian Associated Charities."
}
@article{RYMZHANOV2015462,
title = "Effect of valence holes kinetics on material excitation in tracks of swift heavy ions",
journal = "Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms",
volume = "365",
pages = "462 - 467",
year = "2015",
note = "Swift Heavy Ions in Matter, 18 – 21 May, 2015, Darmstadt, Germany",
issn = "0168-583X",
doi = "https://doi.org/10.1016/j.nimb.2015.08.043",
url = "http://www.sciencedirect.com/science/article/pii/S0168583X15007788",
author = "R.A. Rymzhanov and N.A. Medvedev and A.E. Volkov",
keywords = "Swift heavy ion track, Complex dielectric function, Monte Carlo, Aluminum oxide, Hole diffusion",
abstract = "A considerable part of the excess energy of the electronic subsystem of a solid penetrated by a swift heavy ion (SHI) is accumulated in valence holes. Spatial redistribution of these holes can affect subsequent relaxation, resulting in ionizations of new electrons by hole impacts as well as energy transfer to the target lattice. A new version of the Monte Carlo code TREKIS is applied to study this effect in Al2O3 for SHI tracks. The complex dielectric function (CDF) formalism is used to calculate the cross sections of interaction of involved charged particles (an ion, electrons, holes) with the target giving us ability to take into account collective response of a target to excitations. We compare the radial distributions of the densities and energies of excited electrons and valence holes at different times to those obtained under the assumption of immobile holes used in earlier works. The comparison shows a significant difference between these distributions within the track core, where the majority of slow electrons and valence holes are located at femtosecond timescales after the ion impact. The study demonstrates that the energy deposited by valence holes into the lattice in nanometric tracks is comparable to the energy transferred by excited electrons. Radii of structure transformations in tracks produced by these energy exchange channels are in a good agreement with experiments."
}
@article{MAJOUNIE2012209.e1,
title = "Mutational analysis of the VCP gene in Parkinson's disease",
journal = "Neurobiology of Aging",
volume = "33",
number = "1",
pages = "209.e1 - 209.e2",
year = "2012",
issn = "0197-4580",
doi = "https://doi.org/10.1016/j.neurobiolaging.2011.07.011",
url = "http://www.sciencedirect.com/science/article/pii/S0197458011002892",
author = "Elisa Majounie and Bryan J. Traynor and Adriano Chiò and Gabriella Restagno and Jessica Mandrioli and Michael Benatar and J. Paul Taylor and Andrew B. Singleton",
keywords = "VCP, Parkinson's disease",
abstract = "Mutations in the valosin-containing protein gene (VCP) have been identified in neurological disorders (inclusion body myopathy—early Paget's disease of the bone—frontotemporal dementia and amyotrophic lateral sclerosis) and are thought to play a role in the clearance of abnormally folded proteins. Parkinsonism has been noted in kindreds with VCP mutations. Based on this, we hypothesized that mutations in VCP may also contribute to idiopathic Parkinson's disease (PD). We screened the coding region of the VCP gene in a large cohort of 768 late-onset PD cases (average age at onset, 70 years), both sporadic and with positive family history. We identified a number of rare single nucleotide changes, including a variant previously described to be pathogenic, but no clear disease-causing variants. We conclude that mutations in VCP are not a common cause for idiopathic PD."
}
@article{GOMEZROSADO2013378,
title = "Importancia de la calidad del informe de alta en la gestión de una unidad clínica quirúrgica",
journal = "Cirugía Española",
volume = "91",
number = "6",
pages = "378 - 383",
year = "2013",
issn = "0009-739X",
doi = "https://doi.org/10.1016/j.ciresp.2012.10.015",
url = "http://www.sciencedirect.com/science/article/pii/S0009739X12003910",
author = "Juan-Carlos Gomez-Rosado and María Sanchez-Ramirez and Javier Valdes-Hernandez and Luis C. Capitan-Morales and Marta I. del-Nozal-Nalda and Fernando Oliva-Mompean",
keywords = "Informe médico de alta, Alta hospitalaria, Gestión clínica, Estancias evitables, Discharge report, Patient discharge, Clinical care management, Avoidable stays",
abstract = "Resumen
Introducción
El informe de alta es un documento básico al finalizar un proceso asistencial, y es un elemento clave en el proceso de codificación. De su correcta redacción, fiabilidad y exhaustividad dependerán los datos que sirvan para determinar la producción hospitalaria.
Material y métodos
Partimos de la hipótesis de que, analizando la concordancia del informe de alta con los datos cotejados en la documentación del episodio, podremos recodificar todos aquellos casos infracodificados, imputándolos así a un grupo relacionado por el diagnóstico (GRD) más adecuado. Analizamos en 24 pacientes outliers la correcta cumplimentación de tipo y motivo de ingreso, antecedentes personales y medicación, resumen del episodio, diagnósticos principal y secundarios, procedimiento quirúrgico, evolución durante el episodio y número de diagnósticos y procedimientos enumerados, concordancia con la información real del episodio y los cambios teóricos entre los GRD antes y después del análisis.
Resultados
De 24 casos, 6 informes son válidos y claros; 4, válidos aunque poco claros; 9 son insuficientes y 5, claramente inválidos. La comparación de los GRD recalculados tras la interpretación de los datos del episodio no muestra diferencias significativas, mediante test de Wilcoxon, encontrándose tan solo modificaciones en 5 casos (p = 0,680).
Conclusiones
La calidad del informe de alta depende de la correcta inclusión de todos los datos del CMBD, en concordancia con el episodio. Las discordancias historia/informe pueden modificar el GRD que, en nuestra serie, no es estadísticamente significativo. La autoauditoría del informe de alta hospitalaria permite establecer líneas de mejora, al disminuir los errores de información.
Background
The discharge report is a basic document at the end of a care process, and is a key element in the coding process, since its correct wording, reliability and completeness are factors used to determine the hospital production.
Material and methods
From a hypothesis based on the analysis of the consistency between the discharge report and data collected from the routine clinical notes during admission, we should be able to re-code all those mis-coded, thus placing them in a more appropriate diagnosis-related group (DRG). A total of 24 patient outliers were analysed for the correct filling in of the type and reason for admission, personal history, medication, anamnesis, primary and secondary diagnosis, sugical procedure, outcome, number of diagnostic and procedures cited, concordance between discharge report and history and recoding of the DRG.
Results
From a total of 24 episodes, 6 had precise and valid reports, 4 were valid but not precise enough, 9 were insufficient, and 5 were clearly invalid. The recoded DRG after the documentation review was not significantly different, according to the Wilcoxon test, being changed in only 5 cases (P = .680).
Conclusion
Quality in discharge reports depends on an adequate minimum data set (MDS) in concordance with the source documentation during admission. Discordance can change the DRG, despite it not being significantly different in our series. Self-audit of discharge reports allows quality improvements to be developed along with a reduction in information mistakes."
}