@article{20184706118024,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A dedicated source-position transformation package: PySPT},
journal={Astronomy and Astrophysics},
author={Wertz, Olivier and Orthen, Bastian},
volume={619},
year={2018},
issn={00046361},
abstract={Modern time-delay cosmography aims to infer the cosmological parameters with a competitive precision from observing a multiply imaged quasar. The success of this technique relies upon a robust modeling of the lens mass distribution. Unfortunately strong degeneracies between density profiles that lead to almost the same lensing observables may bias precise estimates of the Hubble constant. The source position transformation (SPT), which covers the well-known mass-sheet transformation (MST) as a special case, defines a new framework to investigate these degeneracies. In this paper, we present pySPT, a python package dedicated to the SPT. We describe how it can be used to evaluate the impact of the SPT on lensing observables. We review most of its capabilities and elaborate on key features that we used in a companion paper regarding SPT and time delays. The pySPT program also comes with a subpackage dedicated to simple lens modeling. This can be used to generate lensing related quantities for a wide variety of lens models independent of any SPT analysis. As a first practical application, we present a correction to the first estimate of the impact on time delays of the SPT, which has been experimentally found in a previous work between a softened power law and composite (baryons + dark matter) lenses. We find that the large deviations previously predicted have been overestimated because of a minor bug in the public lens modeling code lensmodel (v1.99), which is now fixed. We conclude that the predictions for the Hubble constant deviate by &sim;7%, first and foremost as a consequence of an MST. The latest version of pySPT is available on Github, a software development platform, along with some tutorials to describe in detail how making the best use of pySPT.<br/> &copy; 2018 ESO.},
key={Software design},
keywords={Cosmic ray measurement;Galaxies;Time delay;},
note={Cosmological parameters;Density profile;Gravitational lensing: strong;Hubble constant;Large deviations;Mass distribution;Robust modeling;Source position;},
URL={http://dx.doi.org/10.1051/0004-6361/201732242},
}


@inproceedings{20165103138996,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={12th International Conference on Intelligent Information Hiding and Multimedia Signal Processing, IIH-MSP 2016},
journal={Smart Innovation, Systems and Technologies},
volume={64},
year={2017},
pages={1 - 380},
issn={21903018},
address={Kaohsiung, Taiwan},
abstract={The proceedings contain 44 papers. The special focus in this conference is on Image, Video Signal Processing, Multisignal Processing Techniques, Hardware Design, Assisting Systems, Evolutionary Computing and Its Applications. The topics include: The election of spectrum bands in hyper-spectral image classification; evaluating a virtual collaborative environment for interactive distance teaching and learning; the linear transformation image enhancement algorithm based on HSV color space; silhouette imaging for smart fence applications with ZigBee sensors; gender recognition using local block difference pattern; DBN-based classification of spatial-spectral hyperspectral data; multiple kernel-learning based hyperspectral data classification; forensics of operation history including image blurring and noise addition based on joint features; an improvement image subjective quality evaluation model based on just noticeable difference; more efficient algorithm to mine high average-utility patterns; infrared video based sleep comfort analysis using part-based features; fall detection algorithm based on human posture recognition; model-based vehicle make and model recognition from roads; frames motion detection of quantum video; blind quantum computation with two decoy states; a bayesian based finite-size effect analysis of QKD; a novel approach to the quadratic residue code; the reduction of VQ index table size by matching side pixels; a protective device for a motor vehicle battery; a design of genetic programming scheme with VLIW concepts; management of energy saving for wireless sensor network node and optimum design and simulation of new type single phase PMSG.},
}


@inproceedings{20161002051022,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Accelerating an MPI Lattice Boltzmann Code using OpenACC},
journal={Proceedings of WACCPD 2015: 2nd Workshop on Accelerator Programming Using Directives - Held in conjunction with SC 2015: The International Conference for High Performance Computing, Networking, Storage and Analysis},
author={Blair, Stu and Albing, Carl and Grund, Alexander and Jocksch, Andreas},
year={2015},
pages={ACM Special Interest Group on High Performance Computing (SIGHPC); IEEE Computer Society (IEEE-CS\DATC) - },
address={Austin, TX, United states},
abstract={This paper describes our OpenACC porting efforts for a code that we have developed that solves the isothermal in- compressible Navier Stokes equation via the lattice Boltzmann method (LBM). Implemented initially as a hybrid MPI/OpenMP parallel program, we ported our code to use OpenACC in order to obtain the benefit of accelerators in a high performance computing (HPC) environment. We de- scribe the elements of parallelism inherent in the LBM algorithm and the way in which that parallelism can be expressed using OpenACC directives. By setting compile-time flags during the build process, the program alternatively can be compiled for, and continue to run without the benefit of, accelerators. Through this porting process we were able to accelerate our code in an incremental fashion without extensive code transformation. We point out some additional efforts that were required to expose C++ class data members to the OpenACC compiler and describe difficulties encountered in this process. We show an instance where similar code segments decorated with the same OpenACC directives can re- sult in different compiler outputs with significantly different performance properties. The result of the code porting process, which occurred primarily during OpenACC EuroHack, a 5-day intensive computing workshop, was a 5.5x speedup over the non-accelerated version of the code.<br/> &copy; 2015 ACM.},
key={C++ (programming language)},
keywords={Acceleration;Boltzmann equation;Codes (symbols);Computational fluid dynamics;Cosine transforms;Digital storage;Graphics processing unit;Kinetic theory;Mechanical permeability;Navier Stokes equations;Program compilers;},
note={Directives;Lattice Boltzmann method;Openacc;Par- Allel computing;Performance analysis;},
URL={http://dx.doi.org/10.1145/2832105.2832111},
}


@inproceedings{20133416630701,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Improving TelEduc environment with GPL software: The visualization content case for computers and mobile devices},
journal={UBICOMM 2012 - 6th International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies},
author={Da Silva, Andre Constantino and Da Rocha, Heloisa Vieira},
year={2012},
pages={222 - 227},
address={Barcelona, Spain},
abstract={e-Learning environments are proposed to support teaching and learning activities using the Web infrastructure. One example is TelEduc, a GPL e-Learning environment which its developers are mostly students involved in academic projects, making it difficult to have a long-period team with expertise to maintain the code and evolve the software with new requirements and to new and desirable contexts, like mobile access. One solution is use third-party software to improve the software and minimize the code maintain efforts, solution applied in the TelEduc environment to visualize attached documents on computers and smart phones without installed appropriated software, an open problem in the environment before this work. The main result is a TelEduc version with visualization file for desktops and smart phones. Another result is identification of issues dealt when integrates with third party software like impact on usability, accessibility and performance, license compatibility and easily to integrate, update and change.<br/>},
key={Computer software},
keywords={Computer aided instruction;E-learning;Smartphones;Telephone sets;Ubiquitous computing;User interfaces;Visualization;},
note={Academic projects;E-learning environment;Mobile Learning;Mobile user interface;Software integration;Teaching and learning;Third party software;Web infrastructure;},
}


@article{20180804824971,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Enhanced geometric capabilities for the transient analysis code T-ReX and its application to simulating TREAT experiments},
journal={Progress in Nuclear Energy},
author={Mausolff, Zander and DeHart, Mark and Goluoglu, Sedat},
volume={105},
year={2018},
pages={236 - 246},
issn={01491970},
abstract={Advances in computational architecture have prompted a resurgence in the simulation of reactor transients from first principles. Most codes are unable to simulate transient events with complex models, and require numerous approximations. The code T-ReX (Transient-Reactor eXperiment simulator), an extensive update to TDKENO, has been developed as a transient analysis tool with few geometric limitations, and minimal theoretical approximations. T-ReX achieves this by employing the Improved Quasi-Static (IQS) method to solve the time-dependent Boltzmann transport equation with explicit representation of delayed neutrons. The primary change in T-ReX relative to TDKENO is the incorporation of a modified version of the Monte Carlo code KENO-VI to calculate the flux shape and model the geometry of a problem. Using KENO-VI to model systems allows exact representation of the geometry. The changes to T-ReX are verified by comparison of solutions to computational benchmark problems found with a previous version of TDKENO that made use of KENO V.a, and several other codes with time-dependent capabilities. In addition, a three-dimensional KENO-VI model of the Transient Reactor Test Facility (TREAT) core is used in simulations of several temperature-limited transient experiments from the M8 Calibration series. T-ReX produces results that agree with benchmark problems and are in better agreement with TREAT experimental data than TDKENO.<br/> &copy; 2018 Elsevier Ltd},
key={Transient analysis},
keywords={Benchmarking;Boltzmann equation;Clouds;Codes (symbols);Geometry;Temperature;Transients;},
note={IQS method;Neutron transport;T-ReX;Time dependent;TREAT;},
URL={http://dx.doi.org/10.1016/j.pnucene.2018.01.013},
}


@article{20173104009808,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={F-TRIDYN: A Binary Collision Approximation code for simulating ion interactions with rough surfaces},
journal={Journal of Nuclear Materials},
author={Drobny, Jon and Hayes, Alyssa and Curreli, Davide and Ruzic, David N.},
volume={494},
year={2017},
pages={278 - 283},
issn={00223115},
abstract={Fractal TRIDYN (F-TRIDYN) is a modified version of the widely used Monte Carlo, Binary Collision Approximation code TRIDYN that includes an explicit model of surface roughness and additional output modes for coupling to plasma edge and material codes. Surface roughness plays an important role in ion irradiation processes such as sputtering; roughness can significantly increase the angle of maximum sputtering and change the maximum observed sputtering yield by a factor of 2 or more. The complete effect of surface roughness on sputtering and other ion irradiation phenomena is not completely understood. Many rough surfaces can be consistently and realistically modeled by fractals, using the fractal dimension and fractal length scale as the sole input parameters. F-TRIDYN includes a robust fractal surface algorithm that is more computationally efficient than those in previous fractal codes and which reproduces available experimental sputtering data from rough surfaces. Fractals provide a compelling path toward a complete and concise understanding of the effect that surface geometry plays on the behavior of plasma-facing materials. F-TRIDYN is a flexible code for simulating ion-solid interactions and coupling to plasma and material codes for multiscale modeling.<br/> &copy; 2017},
key={Surface roughness},
keywords={Beam plasma interactions;Codes (symbols);Collisional plasmas;Fractal dimension;Fractals;Ion bombardment;Ions;Plasma turbulence;Sputtering;Surface measurement;},
note={Binary collision approximations;Computationally efficient;Ion-solid interactions;Irradiation process;Multi-scale Modeling;Plasma facing materials;Plasma-material interactions;Surface geometries;},
URL={http://dx.doi.org/10.1016/j.jnucmat.2017.07.037},
}


@article{20175104564991,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning Binary Descriptors for Fingerprint Indexing},
journal={IEEE Access},
author={Bai, Chaochao and Li, Mingqiang and Zhao, Tong and Wang, Weiqiang},
volume={6},
year={2017},
pages={1583 - 1594},
issn={21693536},
abstract={Fingerprint indexing is studied widely with the real-valued features, but few works focus on the binary feature descriptors, which are more appropriate to retrieve fingerprints efficiently in the large-scale fingerprint database. In this paper, the binary fingerprint descriptor (BFD), which is an effective and discriminative binary feature representation for fingerprint indexing, is proposed based on minutia cylinder code (MCC). Specifically, we first analyze MCC to find that it has characteristics of the high dimensionality, redundancy, and quantization loss. Accordingly, we propose an optimization model to learn a feature-Transformation matrix, resulting in dimensionality reduction and diminishing quantization loss. Meanwhile, we also incorporate the balance, independence, and similarity-preservation properties in this learning process. Eventually, a multi-index hashing-based fingerprint indexing scheme further accelerate the exact search in Hamming space. The experiments on numerous public databases show that the BFD is discriminative and compact and that the proposed approach is outstanding for fingerprint indexing.<br/> &copy; 2013 IEEE.},
key={Pattern recognition},
keywords={Binary codes;Codes (symbols);Cylinders (shapes);Indexing (of information);Linear transformations;Optimization;Quantization (signal);Robustness (control systems);},
note={Descriptors;Dimensionality reduction;Feature transformations;Fingerprint database;Fingerprint indexing;Fingerprint Recognition;Minutia cylinder codes;Optimization modeling;},
URL={http://dx.doi.org/10.1109/ACCESS.2017.2779562},
}


@article{20121414920338,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Modification and application of the system analysis code ATHLET to trans-critical simulations},
journal={Annals of Nuclear Energy},
author={Fu, S.W. and Liu, X.J. and Zhou, C. and Xu, Z.H. and Yang, Y.H. and Cheng, X.},
volume={44},
year={2012},
pages={40 - 49},
issn={03064549},
abstract={During the loss of coolant accident (LOCA) of supercritical water cooled reactor (SCWR), the pressure in the reactor system will undergo a rapid decrease from supercritical to subcritical condition. This process is called trans-critical transients, which is of crucial importance for the LOCA analysis of SCWR. Using the current version of system code (e.g. ATHLET, REALP), calculation will be terminated due to the abrupt change of void fraction across the critical point (22.064 MPa). To solve this problem, a pseudo two-phase method is proposed by introducing a fictitious region of latent heat (enthalpy of vaporization hfg) at pseudo-critical temperatures. A smooth transition of void fraction can be realized by using liquid-field conservation equations at temperatures lower than the pseudo-critical temperature, and vapor-field conservation equations at temperatures higher than the pseudo-critical temperature. Adopting this method, the system code ATHLET is modified to ATHLET-SC mod 2 on the basic of the previous version ATHLET-SC mod 1 modified by Shanghai Jiao Tong University. When the fictitious region of latent heat is kept as a small region, the code can achieve an acceptable accuracy. Moreover, the ATHLET-SC mod 2 code is applied to simulate the blowdown process of a simplified model. The results achieved so far indicate a good applicability of the new modified code for the trans-critical transient. &copy; 2012 Elsevier Ltd. All rights reserved.},
key={Water cooled reactors},
keywords={Latent heat;Loss of coolant accidents;Systems analysis;Temperature;Two phase flow;Vapors;Void fraction;},
note={ATHLET-SC;Blow down;Conservation equations;Critical points;Enthalpy of vaporization;Pseudo-critical temperature;Reactor systems;Simplified models;Small region;Smooth transitions;Sub-critical condition;Super-critical;Supercritical pressures;Supercritical water cooled reactors;System codes;Two phase method;},
URL={http://dx.doi.org/10.1016/j.anucene.2012.02.005},
}


@inbook{20164102890560,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Verified change},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Havelund, Klaus and Kumar, Rahul},
volume={9960 LNCS},
year={2016},
pages={71 - 89},
issn={03029743},
abstract={We present the textual wide-spectrum modeling and programing language K, which has been designed for representing graphical SysML models, in order to provide semantics to SysML, and pave the way for analysis of SysML models. The current version is supported by the Z3 SMT theorem prover, which allows to prove consistency of constraints. The language is intended to be used by engineers for designing space missions, and in particular NASA&rsquo;s proposed mission to Jupiter&rsquo;s moon Europa. One of the challenges facing software development teams is the notion of change: the fact that code changes over time, and the subsequent problem of demonstrating that no harm has been done due to a change. K is in this paper being applied to demonstrate how change can be perceived as a software verification problem, and hence verified using more traditional software verification techniques.<br/> &copy; Springer International Publishing AG 2016.},
key={Modeling languages},
keywords={Interplanetary flight;Mathematical programming;Models;NASA;Semantics;Software design;Verification;},
note={Change;Constraints;Refinement;Software development teams;Software verification;Space missions;Theorem provers;Wide spectrum;},
URL={http://dx.doi.org/10.1007/978-3-319-46508-1_5},
}


@article{20151700770590,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The impact of tangled code changes on defect prediction models},
journal={Empirical Software Engineering},
author={Herzig, Kim and Just, Sascha and Zeller, Andreas},
volume={21},
number={2},
year={2016},
pages={303 - 336},
issn={13823256},
abstract={When interacting with source control management system, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing version histories, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found between 7 % and 20 % of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6 % of all source files are incorrectly associated with bug reports. These incorrect bug file associations seem to not significantly impact models classifying source files to have at least one bug or no bugs. But our experiments show that untangling tangled code changes can result in more accurate regression bug prediction models when compared to models trained and tested on tangled bug datasets&mdash;in our experiments, the statistically significant accuracy improvements lies between 5 % and 200 %. We recommend better change organization to limit the impact of tangled changes.<br/> &copy; 2015, Springer Science+Business Media New York.},
key={Codes (symbols)},
keywords={Defects;Forecasting;Open source software;},
note={Accuracy Improvement;Bug predictions;Data noise;Defect prediction;Defect prediction models;Impact model;Source control;Untangling;},
URL={http://dx.doi.org/10.1007/s10664-015-9376-6},
}


@article{20175204577108,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={HBS-CRA: scaling impact of change request towards fault proneness: defining a heuristic and biases scale (HBS) of change request artifacts (CRA)},
journal={Cluster Computing},
author={Madapuri, Rudra Kumar and Mahesh, P. C. Senthil},
year={2017},
pages={1 - 9},
issn={13867857},
abstract={Accurately calculating the impact of existing change requests is vital for estimating the probability of fault occurrence in future change requests. For a new request like bug fixing, effectiveness in current change requests is required. In a real-time scenario, bug trackers are deployed to save change requests and their associated information. The trackers and associated information are saved in CVS version control systems and these systems assist programmers to carry out multiple analytical functions and generating descriptions. In our earlier works, we devised the set of change request artifacts and also proposed novel statistical bipartite weighted graphical models to evaluate DFP degree of future change requests. With the motivation gained from this model, here we propose a novel strategy that estimates the DFP of the request by assessing the impact of a change request artifact towards fault-proneness that considers the correlation between code blocks as another factor, which is in addition to our earlier strategy. A novel heuristic and biases scale to evaluate the effectiveness of change request for DFP is devised here in this paper that titled as &ldquo;Defining a Heuristic and Biases Scale (HBS) of Change Request Artifacts (CRA)&rdquo;, in short HBS-CRA. The devised model makes use of information retrieval methods to identify the change request artifacts of the request. In addition, it also checks for DFP scope through HBS-CRA. The HBS-CRA is empirically assessed by applying on concurrent versioning and Change request logs of the production level maintenance project.<br/> &copy; 2017 Springer Science+Business Media, LLC, part of Springer Nature},
key={Information use},
keywords={Graphic methods;},
note={Artifacts;Change request;Fault proneness;Product metrics;Risk predictions;SDLC;Versioning systems;},
URL={http://dx.doi.org/10.1007/s10586-017-1424-0},
}


@inproceedings{20182605364200,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={2018 IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, MaLTeSQuE 2018 - Proceedings},
journal={2018 IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, MaLTeSQuE 2018 - Proceedings},
year={2018},
address={Campobasso, Italy},
abstract={The proceedings contain 9 papers. The topics discussed include: varying defect prediction approaches during project evolution: a preliminary investigation; the role of meta-learners in the adaptive selection of classifiers; machine learning-based run-time anomaly detection in software systems: an industrial evaluation; how high will it be? using machine learning models to predict branch coverage in automated testing; ensemble techniques for software change prediction: a preliminary investigation; co-evolution analysis of production and test code by learning association rules of changes; investigating type declaration mismatches in python; and user-perceived reusability estimation based on analysis of software repositories.<br/>},
}


@inproceedings{20122815237847,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={AODP: Refactoring code to provide advanced aspect-oriented modularization of design patterns},
journal={Proceedings of the ACM Symposium on Applied Computing},
author={Giunta, Rosario and Pappalardo, Giuseppe and Tramontana, Emiliano},
year={2012},
pages={1243 - 1250},
address={Trento, Italy},
abstract={Although solutions provided by design patterns are an invaluable resource for developers, some design patterns lead to placing code addressing design pattern concerns into the same class as application code. This weakens the modularity of an application because it makes classes more complex, more prone to changes and less reusable. In order to avoid the tangling of design pattern- and application-related code within classes, this paper proposes an approach for assisting the refactoring of an application that uses design patterns into an aspect-based version. This allows application classes, and aspects implementing design patterns, to stay independent of each other, thus greatly enhancing modularity. Developers intending to change the role of an application class need only update the code connecting it to the design pattern involved. &copy; 2012 ACM.<br/>},
key={Computer software reusability},
keywords={Codes (symbols);Modular construction;},
note={Application codes;Aspect-oriented;Design Patterns;Modularizations;Refactorings;Within class;},
URL={http://dx.doi.org/10.1145/2245276.2231971},
}


@inproceedings{20143918177201,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Assessment of subchannel code ASSERT-PV for prediction of post-dryout heat transfer in CANDU bundles},
journal={International Congress on Advances in Nuclear Power Plants, ICAPP 2014},
author={Cheng, Z. and Rao, Y.F. and Waddington, G.M.},
volume={2},
year={2014},
pages={1524 - 1531},
address={Charlotte, NC, United states},
abstract={Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadiar nuclear industry. The recently-released ASSERT-PV 3.2 provides enhanced models for improved predictions of subchannel flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against PDO tests performed during five full-size CANDU bundle experiments conducted in 1990s and in 2009 by Stem Laboratories (SL), using 28-, 37- and 43-element (CANFLEX) bundles. The SL experiments encompass the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for CANDU reactors. Codi predictions of maximum PDO fuel-sheath temperature were compared against measurements from the SL PDO tests to quantify the code?s prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. The sensitivity studies quantified the contribution of each PDO model change or enhancement to the improvement in PDO heat transfer prediction. Furthermore, the code convergence improvement was assessed against all selected SI tests as well. Overall, the assessment demonstrated significant improvement in prediction of PDO sheath temperature in horizontal fuel channels containing CANDU bundles.<br/>},
key={Heat transfer},
keywords={Codes (symbols);Forecasting;Heat flux;Nuclear energy;Nuclear fuels;Nuclear industry;Nuclear power plants;Nuclear reactors;Two phase flow;},
note={Atomic energy of canada limiteds;Critical heat flux(CHF);Flow distribution;Heat transfer predictions;Prediction accuracy;Pv applications;Sensitivity studies;Thermal hydraulics codes;},
}


@inproceedings{20183405705462,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Interpretable representation learning for healthcare via capturing disease progression through time},
journal={Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
author={Bai, Tian and Egleston, Brian L. and Zhang, Shanshan and Vucetic, Slobodan},
year={2018},
pages={43 - 51},
address={London, United kingdom},
abstract={Various deep learning models have recently been applied to predictive modeling of Electronic Health Records (EHR). In medical claims data, which is a particular type of EHR data, each patient is represented as a sequence of temporally ordered irregularly sampled visits to health providers, where each visit is recorded as an unordered set of medical codes specifying patient's diagnosis and treatment provided during the visit. Based on the observation that different patient conditions have different temporal progression patterns, in this paper we propose a novel interpretable deep learning model, called Timeline. The main novelty of Timeline is that it has a mechanism that learns time decay factors for every medical code. This allows the Timeline to learn that chronic conditions have a longer lasting impact on future visits than acute conditions. Timeline also has an attention mechanism that improves vector embeddings of visits. By analyzing the attention weights and disease progression functions of Timeline, it is possible to interpret the predictions and understand how risks of future visits change over time. We evaluated Timeline on two large-scale real world data sets. The specific task was to predict what is the primary diagnosis category for the next hospital visit given previous visits. Our results show that Timeline has higher accuracy than the state of the art deep learning models based on RNN. In addition, we demonstrate that time decay factors and attentions learned by Timeline are in accord with the medical knowledge and that Timeline can provide a useful insight into its predictions.<br/> &copy; 2018 Association for Computing Machinery.},
key={Deep learning},
keywords={Data mining;Diagnosis;Forecasting;Health care;Patient treatment;Records management;},
note={Attention mechanisms;Attention model;Chronic conditions;Disease progression;Electronic health record;Interpretable representation;Medical knowledge;Predictive modeling;},
URL={http://dx.doi.org/10.1145/3219819.3219904},
}


@article{20184606062564,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Calibration of non-linear effective stress code for seismic analysis of excess pore pressures and liquefaction in the free field},
journal={Soil Dynamics and Earthquake Engineering},
author={Dobry, R. and El-Sekelly, W. and Abdoun, T.},
volume={107},
year={2018},
pages={374 - 389},
issn={02677261},
abstract={The paper presents numerical predictions of excess pore pressure, liquefaction and settlement response of four centrifuge model tests of 6 m uniform deposits of saturated clean Ottawa sand, placed by dry pluviation and having a relative density ranging from 38% to 66%. The deposits were subjected to 1D uniform base shaking consisting of 10&ndash;15 cycles of peak acceleration ranging from 0.04 to 0.12 g. All predictions were conducted with the nonlinear effective stress numerical code Dmod2000. Significant effort was spent in calibrating Dmod2000 by matching the pore pressure and settlement measurements of the first shaking (S1) of a series of shakings conducted in centrifuge Experiment 3. This resulted in very good predictions of both pore pressures and settlement measured in this shaking S1. The exercise showed the importance for realistic simulations of having the correct soil compressibility and permeability. This calibrated version of Dmod2000 was used for a good pore pressure prediction of the preshaken deposit in the same Experiment 3 (S36), by modifying only one parameter in the undrained pore pressure model; and also well predicted pore pressure responses in Tests FFV3 and PFV1, without any change in the parameters of Dmod2000 except for use of the new input motions (Type B predictions). The experimental and numerical results showed that both cyclic shear stress/strains and upward water flow determine together the pore pressure buildup and liquefaction phenomena. The soil response is partially drained rather than undrained, and pore pressure dissipation does take place during shaking both before and after liquefaction occurs.<br/> &copy; 2018 Elsevier Ltd},
key={Pore pressure},
keywords={Centrifuges;Deposits;Flow of water;Forecasting;Shear flow;Shear stress;Soil liquefaction;},
note={Centrifuge experiments;Centrifuge model test;Excess pore pressure;Numerical predictions;Pore pressure dissipation;Pore pressure prediction;Settlement measurement;Soil compressibilities;},
URL={http://dx.doi.org/10.1016/j.soildyn.2018.01.029},
}


@article{20141817649058,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Extension and application of the reactor dynamics code DYN3D for Block-type High Temperature Reactors},
journal={Nuclear Engineering and Design},
author={Baier, Silvio and Fridman, Emil and Kliem, Soeren and Rohde, Ulrich},
volume={271},
year={2014},
pages={431 - 436},
issn={00295493},
abstract={The reactor code DYN3D was developed at the Helmholtz-Zentrum Dresden-Rossendorf to study steady state and transient behavior of Light Water Reactors. Concerning the neutronics part, the multigroup diffusion or SP3 transport equation based on nodal expansion methods is solved both for hexagonal and square fuel element geometry. To deal with Block-type High Temperature Reactor cores DYN3D was extended to a version DYN3D-HTR. A 3D heat conduction model was introduced to include 3D effects of heat transfer and heat conduction and the detailed structure of the fuel element. Homogenized neutronic cross sections were generated by applying a Monte Carlo approach with resolution of each individual TRISO fuel particle. Results of coupled steady state and transient calculations with 12 energy groups are presented. Transient case studies are control rod insertion, a change of the inlet coolant temperature and a change of the coolant gas mass flow rate. It is shown that DYN3D-HTR is an appropriate code system to simulate steady states and short time transients. Furthermore the necessity of the 3D heat conduction model is demonstrated. &copy; 2013 Elsevier B.V.<br/>},
key={High temperature reactors},
keywords={Codes (symbols);Coolants;Fuels;Heat conduction;Light water reactors;Nuclear fuel elements;},
note={Coolant temperature;Element geometry;Heat conduction models;Monte Carlo approach;Nodal expansion method;Reactor dynamics;Steady state and transients;Transport equation;},
URL={http://dx.doi.org/10.1016/j.nucengdes.2013.12.013},
}


@inproceedings{20181905173317,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Filtering of false positives from IR-based traceability links among software artifacts},
journal={2017 2nd International Conference for Convergence in Technology, I2CT 2017},
author={Jyoti and Chhabra, Jitender Kumar},
volume={2017-January},
year={2017},
pages={1111 - 1115},
address={Pune, India},
abstract={Correlation among software artifacts (also known as traceability links) of object oriented software plays a vital role in its maintenance. These traceability links are being commonly identified through Information Retrieval (IR) based techniques. But, it has been found that the resulting links from IR contain many false positives and some complementary approaches have been suggested for the purpose. Still, it usually requires manual verification of links which is neither desirable nor reliable. This paper suggests a new technique which can automatically filter out the false positives links (between requirement and source code) from IR and thus can help in reducing dependence as well as incorrectness of manual verification process. The proposed approach works on the basis of finding correlations among classes using either structural or co-changed dependency or both. A threshold is selected as a cut off on computed dependency values, to accept the presence of structural and co-changed dependency each. Now the traceability links are verified using these dependencies. If atleast one of the structural or co-change information validates the link obtained from IR approach, then that link is selected as candidate link, otherwise removed. Different thresholds have been experimented and comparison of results obtained from IR and the proposed approach is done. The results show that precision increases for all values of thresholds. Further analysis of results indicates that threshold in the range of 0.3 to 0.5 give better results. Hence, the proposed approach can be used as complementary to other Improved IR approaches to filter out false positives.<br/> &copy; 2017 IEEE.},
key={Information filtering},
keywords={Information retrieval;Object oriented programming;},
note={Change history;False positive;Object oriented software;Requirement traceabilitys;Software artifacts;Structural information;Traceability links;Verification process;},
URL={http://dx.doi.org/10.1109/I2CT.2017.8226300},
}


@article{20141817649068,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Development of an integrated fission product release and transport code for spatially resolved full-core calculations of V/HTRs},
journal={Nuclear Engineering and Design},
author={Xhonneux, Andre and Allelein, Hans-Josef},
volume={271},
year={2014},
pages={361 - 369},
issn={00295493},
abstract={The computer codes FRESCO-I, FRESCO-II, PANAMA and SPATRA developed at Forschungszentrum Ju&die;lich in Germany in the early 1980s are essential tools to predict the fission product release from spherical fuel elements and the TRISO fuel performance, respectively, under given normal or accidental conditions. These codes are able to calculate a conservative estimation of the source term, i.e. quantity and duration of radionuclide release. Recently, these codes have been reversed engineered, modernized (FORTRAN 95/2003) and combined to form a consistent code named STACY (Source Term Analysis Code System). STACY will later become a module of the V/HTR Code Package (HCP). In addition, further improvements have been implemented to enable more detailed calculations. For example the distinct temperature profile along the pebble radius is now taken into account and coated particle failure rates can be calculated under normal operating conditions. In addition, the absolute fission product release of an V/HTR pebble bed core can be calculated by using the newly developed burnup code Topological Nuclide Transformation (TNT) replacing the former rudimentary approach. As a new functionality, spatially resolved fission product release calculations for normal operating conditions as well as accident conditions can be performed. In case of a full-core calculation, a large number of individual pebbles which follow a random path through the reactor core can be simulated. The history of the individual pebble is recorded, too. Main input data such as spatially resolved neutron fluxes and fluid dynamics data are provided by the VSOP code. Capabilities of the FRESCO-I and SPATRA code which allow for the simulation of the redistribution of fission products within the primary circuit and the deposition of fission products on graphitic and metallic surfaces are also available in STACY. In this paper, details of the STACY model and first results for its application to the 200 MW(th) HTR-Module are presented.&copy; 2014 Published by Elsevier B.V.<br/>},
key={Codes (symbols)},
keywords={Failure analysis;Fission products;High temperature reactors;Reactor cores;},
note={Accident conditions;Metallic surface;Normal operating conditions;Radionuclide release;Source term analysis;Spatially resolved;Spherical fuel element;Temperature profiles;},
URL={http://dx.doi.org/10.1016/j.nucengdes.2013.11.063},
}


@inproceedings{20174504374418,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Problem and project-based learning in scripting lab},
journal={Proceedings - 2016 IEEE 4th International Conference on MOOCs, Innovation and Technology in Education, MITE 2016},
author={Giraddi, Shantala and Yaligar, Shilpa and Kavitha, H.S.},
year={2016},
pages={152 - 156},
address={Madurai, India},
abstract={Scripting language employ high-level constructs to interpret and execute one command at a time. In general scripting languages are easier to learn and faster to code than structured and compiled languages such as C and C++. Scripting languages have many important advantages over traditional programming languages. In future the usage of these languages is likely to increase. In this paper we discuss and report our experience in teaching scripting languages lab at the undergraduate level, 4th semester. Scripting language is an umbrella term used for languages like unix shell, TCL, perl, Java, python and LISP. Out of these, we have chosen UNIX shell programming and python for our curriculum. The authors report various pedagogical activities like multiple assignments, peer assessment within a group, self learning through e-resources and course project that were employed during the course. The course projects were specially designed so as to make students explore the vast number of python packages. The authors found that these activities definitely enhance the learning experience and there was a remarkable change in the learning level of the students as compared to previous years as evident in the grades obtained by the students.<br/> &copy; 2016 IEEE.},
key={C++ (programming language)},
keywords={Educational technology;Engineering research;LISP (programming language);Students;Teaching;UNIX;},
note={Assessment strategies;Pedagogy;Peer assessment;Python;Scripting languages;},
URL={http://dx.doi.org/10.1109/MITE.2016.18},
}


@inproceedings{20152500949651,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A novel technique for human face recognition using fractal code and bi-dimensional subspace},
journal={IFIP Advances in Information and Communication Technology},
author={Mohamed, Benouis and Kame, Benkkadour Mohamed and Redwan, Tlmesani and Mohamed, Senouci},
volume={456},
year={2015},
pages={31 - 42},
issn={18684238},
address={Saida, Algeria},
abstract={Face recognition is considered as one of the best biometric methods used for human identification and verification; this is because of its unique features that differ from one person to another, and its importance in the security field. This paper proposes an algorithm for face recognition and classification using a system based on WPD, fractal codes and two-dimensional subspace for feature extraction, and Combined Learning Vector Quantization and PNN Classifier as Neural Network approach for classification. This paper presents a new approach for extracted features and face recognition.Fractal codes which are determined by a fractal encoding method are used as feature in this system. Fractal image compression is a relatively recent technique based on the representation of an image by a contractive transform for which the fixed point is close to the original image. Each fractal code consists of five parameters such as corresponding domain coordinates for each range block. Brightness offset and an affine transformation. The proposed approach is tested on ORL and FEI face databases. Experimental results on this database demonstrated the effectiveness of the proposed approach for face recognition with high accuracy compared with previous methods.<br/> &copy; IFIP International Federation for Information Processing 2015.},
key={Face recognition},
keywords={Biometrics;Codes (symbols);Fractals;Image compression;Vectors;},
note={2DLDA;2DPCA;Affine transformations;Fractal codes;Fractal image compression;Human face recognition;Human identification;Two-dimensional subspaces;},
URL={http://dx.doi.org/10.1007/978-3-319-19578-0_3},
}


@inproceedings{20173504099147,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Bayesian learning based multiuser detection for M2M communications with time-varying user activities},
journal={IEEE International Conference on Communications},
author={Zhang, Xiaoxu and Liang, Ying-Chang and Fang, Jun},
year={2017},
issn={15503607},
address={Paris, France},
abstract={Machine-to-Machine (M2M) communication plays a significant role in supporting Internet of Thing (IoT). This paper is concerned about multiuser detection (MUD) for massive M2M supported by Low-Activity Code Division Multiple Access (LA-CDMA). In previous work, maximum likelihood (ML) and maximum a posterior probability (MAP) detectors have been developed for such system. The ML detector has exponential complexity, while the MAP detector requires perfect knowledge of user activity factor. In practice, the user activity factor may not be known and could change from time to time. To design MUD detectors addressing these problems, in this paper, we formulate multiple measurement vector (MMV) model for uplink LA-CDMA system with time-varying user activities. Since the transmitted signals have block sparse structure, we introduce the pattern coupled spare Bayesian learning (PCSBL) by using the neighbour coherence of each transmitted signal, which effectively solves the user activity factor unknown problem. Furthermore, we embed the generalized approximate message passing (GAMP) to PCSBL and develop a novel algorithm, called generalized approximate message passing pattern coupled sparse Bayesian learning (GAMP-PCSBL). The GAMP-PCSBL does not require activity factor either, and greatly reduces the computational complexity. Simulation results have shown that the proposed algorithms have superior recovery performance than the conventional algorithms. &copy; 2017 IEEE.},
URL={http://dx.doi.org/10.1109/ICC.2017.7997433},
}


@article{20142317783145,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={OFF, Open source Finite volume Fluid dynamics code: A free, high-order solver based on parallel, modular, object-oriented Fortran API},
journal={Computer Physics Communications},
author={Zaghi, S.},
volume={185},
number={7},
year={2014},
pages={2151 - 2194},
issn={00104655},
abstract={OFF, an open source (free software) code for performing fluid dynamics simulations, is presented. The aim of OFF is to solve, numerically, the unsteady (and steady) compressible Navier-Stokes equations of fluid dynamics by means of finite volume techniques: the research background is mainly focused on high-order (WENO) schemes for multi-fluids, multi-phase flows over complex geometries. To this purpose a highly modular, object-oriented application program interface (API) has been developed. In particular, the concepts of data encapsulation and inheritance available within Fortran language (from standard 2003) have been stressed in order to represent each fluid dynamics "entity" (e.g. the conservative variables of a finite volume, its geometry, etc...) by a single object so that a large variety of computational libraries can be easily (and efficiently) developed upon these objects. The main features of OFF can be summarized as follows: Programming LanguageOFF is written in standard (compliant) Fortran 2003; its design is highly modular in order to enhance simplicity of use and maintenance without compromising the efficiency; Parallel Frameworks Supported the development of OFF has been also targeted to maximize the computational efficiency: the code is designed to run on shared-memory multi-cores workstations and distributed-memory clusters of shared-memory nodes (supercomputers); the code's parallelization is based on Open Multiprocessing (OpenMP) and Message Passing Interface (MPI) paradigms; Usability, Maintenance and Enhancement in order to improve the usability, maintenance and enhancement of the code also the documentation has been carefully taken into account; the documentation is built upon comprehensive comments placed directly into the source files (no external documentation files needed): these comments are parsed by means of doxygen free software producing high quality html and latex documentation pages; the distributed versioning system referred as git has been adopted in order to facilitate the collaborative maintenance and improvement of the code; CopyrightsOFF is a free software that anyone can use, copy, distribute, study, change and improve under the GNU Public License version 3. The present paper is a manifesto of OFF code and presents the currently implemented features and ongoing developments. This work is focused on the computational techniques adopted and a detailed description of the main API characteristics is reported. OFF capabilities are demonstrated by means of one and two dimensional examples and a three dimensional real application.<br/>},
key={Object oriented programming},
keywords={Application programming interfaces (API);Application programs;Codes (symbols);Computational efficiency;Computational fluid dynamics;Computational geometry;Data encapsulation;Efficiency;FORTRAN (programming language);Maintenance;Memory architecture;Message passing;Multicore programming;Navier Stokes equations;Open source software;Open systems;Supercomputers;},
note={Compressible Navier-Stokes equations;Distributed memory clusters;Finite volume schemes;Fluid dynamics simulations;Object oriented application;OpenMP;Problem solvers;WENO;},
URL={http://dx.doi.org/10.1016/j.cpc.2014.04.005},
}


@article{20153101102570,
language={Portuguese},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Do historical metrics and developers communication aid to predict change couplings?},
journal={IEEE Latin America Transactions},
author={Wiese, I.S. and Kuroda, R.T. and Re, R. and Bulhoes, R.S. and Oliva, G.A. and Gerosa, M.A.},
volume={13},
number={6},
year={2015},
pages={1979 - 1988},
issn={15480992},
abstract={Developers have contributed to open-source projects by forking the code and submitting pull requests. Once a pull request is submitted, interested parties can review the set of changes, discuss potential modifications, and even push additional commits if necessary. Mining artifacts that were committed together during history of pull-requests makes it possible to infer change couplings among these artifacts. Supported by the Conway's Law, whom states that "organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations", we hypothesize that social network analysis (SNA) is able to identify strong and weak change dependencies. In this paper, we used statistical models relying on centrality, ego, and structural holes metrics computed from communication networks to predict co-changes among files included in pull requests submitted to the Ruby on Rails project. To the best of our knowledge, this is the first study to employ SNA metrics to predict change dependencies from Github projects<br/> &copy; 2015 IEEE.},
key={Forecasting},
keywords={Couplings;Open systems;Social networking (online);Telecommunication networks;},
note={Change couplings;Change dependencies;Communication aids;Communication structures;Conways law;Design systems;Open source projects;Structural holes;},
URL={http://dx.doi.org/10.1109/TLA.2015.7164225},
}


@inproceedings{20170503291855,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A proposal of three extensions in blank element selection algorithm for Java programming learning assistant system},
journal={2016 IEEE 5th Global Conference on Consumer Electronics, GCCE 2016},
author={Zaw, Khin Khin and Funabiki, Nobuo and Kuribayashi, Minoru},
year={2016},
address={Kyoto, Japan},
abstract={To assist Java programming educations, we have developed a Web-based Java Programming Learning Assistant System (JPLAS). JPLAS provides fill-in-blank problems to let students study Java grammar and basic programming skills by filling the blanked elements in a given Java code. To generate the feasible problems, we have proposed a blank element selection algorithm using the constraint graph to select as many blanks as possible such that they have grammatically correct and unique answers. In this paper, to further increase the number of blanks and control the difficulty of the generated problem, we extend this algorithm by 1) adding operators in conditional expressions for blank candidates, 2) improving the edge generation method in the constraint graph to increase the number of blanks, and 3) introducing two parameters to change the frequency of selecting blanks. To verify the effectiveness, we apply the extended algorithm to 55 Java codes for fundamental data structure or algorithms, and confirm that these extensions can increase the number of blanks and change the problem difficulty.<br/> &copy; 2016 IEEE.},
key={Java programming language},
keywords={Students;},
note={Conditional expressions;Constraint graph;Element selection;Generation method;Java programming;Problem difficulty;Programming skills;Two parameter;},
URL={http://dx.doi.org/10.1109/GCCE.2016.7800312},
}


@article{20162502515357,
language={Chinese},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Development of few group cross section calculation model for core nuclear design code CYCAS},
journal={Yuanzineng Kexue Jishu/Atomic Energy Science and Technology},
author={Yang, Wei-Yan and Tang, Chun-Tao and Bi, Guang-Wen and Yang, Bo},
volume={50},
number={5},
year={2016},
pages={859 - 863},
issn={10006931},
abstract={The few group cross section calculation model generates node homogeneous few group cross section for core 3D diffusion calculation, which is one of the key models of core calculation code. CYCAS is the new core 3D nuclear design code developed by Shanghai Nuclear Engineering Research &amp; Design Institute (SNERDI). A new model based on detail analysis of the factors affecting node cross section was developed for CYCAS. In the model, the energy spectrum correction method was used to process the second order effect introduced by energy spectrum change, and the micro-depletion correction method was utilized to treat depletion history effect. The numerical results of unit assembly and AP1000 core validate the high accuracy of the new model within CYCAS.<br/> &copy; 2016, Editorial Board of Atomic Energy Science and Technology. All right reserved.},
key={Pressurized water reactors},
keywords={Beams and girders;Codes (symbols);Electromagnetic wave attenuation;Spectroscopy;},
note={Calculation models;Correction method;CYCAS;Energy spectra;Nuclear design;},
URL={http://dx.doi.org/10.7538/yzk.2016.50.05.0859},
}


@inproceedings{20134416907563,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Insight into a method co-change pattern to identify highly coupled methods: An empirical study},
journal={IEEE International Conference on Program Comprehension},
author={Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.},
year={2013},
pages={103 - 112},
address={San Francisco, CA, United states},
abstract={In this paper, we describe an empirical study of a unique method co-change pattern that has the potential to pinpoint design deficiency in a software system. We automatically identify this pattern by inspecting the method co-change history using reasonable constraints on method association rules. We also investigate the effect of code clones on the method co-changes identified according to the pattern, because there is a common intuition that clone fragments from the same clone class often require corresponding changes to ensure they remain consistent with each other. According to our in-depth investigation on hundreds of revisions of seven open-source software systems considering three types of clones (Type 1, Type 2, Type 3), our identified pattern helps us detect methods that are logically coupled with multiple other methods and that exhibit a significantly higher modification frequency than other methods. We call the methods detected by the pattern MMCGs (Methods appearing in Multiple Commit Groups) considering the pattern semantic. MMCGs can be considered as the candidates for restructuring in order to minimize coupling as well as to reduce the change-proneness of a software system. According to our observation, code clones have a significant effect on method co-changes as well as on MMCGs. We believe that clone refactoring can help us minimize evolutionary coupling among methods. &copy; 2013 IEEE.<br/>},
key={Open systems},
keywords={Association rules;Cloning;Computer programming;Open source software;Semantics;},
note={Change patterns;Change proneness;Design deficiencies;Empirical studies;Life span;Method Genealogy;Open source software systems;Software systems;},
URL={http://dx.doi.org/10.1109/ICPC.2013.6613838},
}


@inproceedings{20152700997165,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A learning-based method for detecting defective classes in object-oriented systems},
journal={2015 IEEE 8th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2015 - Proceedings},
author={Biray, Cail and Buzluca, Feza},
year={2015},
address={Graz, Austria},
abstract={Code or design problems in software classes reduce understandability, flexibility and reusability of the system. Performing maintenance activities on defective components such as adding new features, adapting to the changes, finding bugs, and correcting errors, is hard and consumes a lot of time. Unless the design defects are corrected by a refactoring process these error-prone classes will most likely generate new errors after later modifications. Therefore, these classes will have a high error frequency (EF), which is defined as the ratio between the number of errors and modifications. Early estimate of error-prone classes helps developers to focus on defective modules, thus reduces testing time and maintenance costs. In this paper, we propose a learning-based decision tree model for detecting error-prone classes with structural design defects. The main novelty in our approach is that we consider EFs and change counts (ChC) of classes to construct a proper data set for the training of the model. We built our training set that includes design metrics of classes by analyzing numerous releases of real-world software products and considering EFs of classes to mark them as error-prone or non-error-prone. We evaluated our method using two long-standing software solutions of Ericsson Turkey. We shared and discussed our findings with the development teams. The results show that, our approach succeeds in finding error-prone classes and it can be used to decrease the testing and maintenance costs. &copy; 2015 IEEE.},
URL={http://dx.doi.org/10.1109/ICSTW.2015.7107477},
}


@inproceedings{20160701950067,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Introduction to the California historical building code},
journal={Improving the Seismic Performance of Existing Buildings and other Structures 2015 - Proceedings of the 2nd ATC and SEI Conference on Improving the Seismic Performance of Existing Buildings and Other Structures},
author={Gilmartin, U.M. and Dreyfuss, A.R.},
year={2015},
pages={208 - 215},
address={San Francisco, CA, United states},
abstract={California is unusual in that it has developed and adopted a building code specifically intended for historic preservation. The California Historical Building Code (CHBC) (CBSC 2013b) was developed nearly forty years ago in response to the problem that "restoration is frequently made difficult by the unnecessarily rigid interpretation of building...codes" (Winters 2003). Despite the passing of significant time, many engineers and architects are still completely unaware of the existence of the CHBC and its context in the suite of California codes. Even those aware of its existence are sometimes prone to misconceptions regarding its content and use. This paper will discuss the history of the CHBC and its purpose and intent. A summary of use of the CHBC will be presented, along with its application-and its differences with the regular code-for repair, additions, and alterations; change of use; and seismic upgrades. This is the first of two companion papers that address the CHBC. A complimentary paper provides examples of beneficial use of the CHBC and cautions regarding potential misuse.<br/> &copy; 2015 ASCE and ATC.},
key={Historic preservation},
keywords={Buildings;Codes (symbols);Seismic waves;Seismology;},
note={Beneficial use;California;Historical buildings;ITS applications;Seismic upgrade;},
}


@inproceedings{20130615994649,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Comparison of dynamic model selection with infinite HMM for statistical model change detection},
journal={2012 IEEE Information Theory Workshop, ITW 2012},
author={Sakurai, Eiichi and Yamanishi, Kenji},
year={2012},
pages={302 - 306},
address={Lausanne, Switzerland},
abstract={In this study, we address the issue of tracking changes in statistical models under the assumption that the statistical models used for generating data may change over time. This issue is of great importance for learning from non-stationary data. One of the promising approaches for resolving this issue is the use of the dynamic model selection (DMS) method, in which a model sequence is estimated on the basis of the minimum description length (MDL) principle. Another approach is the use of the infinite hidden Markov model (HMM), which is a non-parametric learning method for the case with an infinite number of states. In this study, we propose a few new variants of DMS and propose efficient algorithms to minimize the total code-length by using the sequential normalized maximum likelihood. We compare these algorithms with infinite HMM to investigate their statistical model change detection performance, and we empirically demonstrate that one of our variants of DMS significantly outperforms infinite HMM in terms of change-point detection accuracy. &copy; 2012 IEEE.<br/>},
key={Hidden Markov models},
keywords={Dynamic models;Information theory;Maximum likelihood;},
note={Change point detection;Dynamic model selections;Infinite numbers;Minimum description length principle;Non-parametric;Nonstationary data;Normalized maximum likelihood;Statistical modeling;},
URL={http://dx.doi.org/10.1109/ITW.2012.6404680},
}


@inproceedings{20142517843574,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Effect of previous stress history and vegetation on the coefficient of earth pressure at-rest, K0, in London clay},
journal={Numerical Methods in Geotechnical Engineering - Proceedings of the 8th European Conference on Numerical Methods in Geotechnical Engineering, NUMGE 2014},
author={Tsiampousi, A. and Vitsios, I. and Zdravkovic, L. and Potts, D.M.},
volume={1},
year={2014},
pages={209 - 214},
address={Delft, Netherlands},
abstract={Due to their previous stress history, highly overconsolidated clays, such as London clay, are characterised in their in-situ states by large values of the coefficient of earth pressure at-rest, K<inf>0</inf>, as confirmed by field and laboratory measurements. Numerical studies have focused in the past on the effect of previous stress history on K<inf>0</inf>, including pore water pressure variations due to changing the position of the phreatic surface. Nonetheless, under the combined effect of precipitation and vegetation, pore water pressures may change above the phreatic surface having only a small influence on its position.Therefore, additionally to previous stress history, soil-atmosphere interaction is expected to affect K<inf>0</inf>. This paper first presents a parametric study on the effect of previous stress history, carried out with the numerical code ICFEP, employing a kinematic hardening constitutive model. The effect of vegetation and precipitation is subsequently studied using sophisticated hydraulic boundary conditions. The numerical results demonstrate that soil-atmosphere interaction has a significant effect on the K<inf>0</inf>values even at depths considerably larger than the root depth. The effect of vegetation, however, seems to be erased by subsequent deposition and concurrent rise of the ground water table. &copy; 2014 Taylor &amp; Francis Group.<br/>},
key={Pressure effects},
keywords={Geotechnical engineering;Groundwater;Numerical methods;Pore pressure;Pressure distribution;Retaining walls;Vegetation;},
note={Coefficient of earth pressure at rest;Ground water table;Hydraulic boundaries;Kinematic hardening;Laboratory measurements;Numerical results;Overconsolidated clays;Pore-water pressures;},
}


@inproceedings{20150200414235,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Investigating the change-proneness of service patterns and antipatterns},
journal={Proceedings - IEEE 7th International Conference on Service-Oriented Computing and Applications, SOCA 2014},
author={Palma, Francis and An, Le and Khomh, Foutse and Moha, Naouel and Gueheneuc, Yann-Gael},
year={2014},
pages={1 - 8},
address={Matsue, Japan},
abstract={Like any other software systems, service-based systems (SBSs) evolve frequently to accommodate new user requirements. This evolution may degrade their design and implementation and may cause the introduction of common bad practice solutions - antipatterns - in opposition to patterns which are good solutions to common recurring design problems. We believe that the degradation of the design of SBSs does not only affect the clients of the SBSs but also the maintenance and evolution of the SBSs themselves. This paper presents the results of an empirical study that aimed to quantify the impact of service patterns and antipatterns on the maintenance and evolution of SBSs. We measure the maintenance effort of a service implementation in terms of the number of changes and the size of changes (i.e., Code churns) performed by developers to maintain and evolve the service, two effort metrics that have been widely used in software engineering studies. Using data collected from the evolutionary history of the SBS FraSCAti, we investigate if (1) services involved in patterns require less maintenance effort, (2) services detected as antipatterns require more maintenance effort than other services, and (3) if some particular service antipatterns are more change-prone than others. Results show that (1) services involved in patterns require less maintenance effort, but not at statistically significant level, (2) services detected as antipatterns require significantly more maintenance effort than non-antipattern services, and (3) services detected as God Component, Multi Service, and Service Chain antipatterns are more change-prone (i.e., Require more maintenance effort) than the services involved in other antipatterns. We also analysed the relation between object-oriented code smells and service patterns/antipatterns and found a significant difference in the proportion of code smells contained in the implementations of service patterns and antipatterns.<br/> &copy; 2014 IEEE.},
key={Computer software maintenance},
keywords={Codes (symbols);Distributed computer systems;Maintenance;Object oriented programming;Odors;Software engineering;},
note={Anti-patterns;Change proneness;Empirical Software Engineering;patterns;services;},
URL={http://dx.doi.org/10.1109/SOCA.2014.43},
}


@inproceedings{20161302174182,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Identifying wasted effort in the field via developer interaction data},
journal={2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings},
author={Balogh, Gergo and Antal, Gabor and Beszedes, Arpad and Vidacs, Laszlo and Gyimothy, Tibor and Vegh, Adam Zoltan},
year={2015},
pages={391 - 400},
address={Bremen, Germany},
abstract={During software projects, several parts of the source code are usually re-written due to imperfect solutions before the code is released. This wasted effort is of central interest to the project management to assure on-time delivery. Although the amount of thrown-away code can be measured from version control systems, stakeholders are more interested in productivity dynamics that reflect the constant change in a software project. In this paper we present a field study of measuring the productivity of a medium-sized J2EE project. We propose a productivity analysis method where productivity is expressed through dynamic profiles- the so-called Micro-Productivity Profiles (MPPs). They can be used to characterize various constituents of software projects such as components, phases and teams. We collected detailed traces of developers' actions using an Eclipse IDE plug-in for seven months of software development throughout two milestones. We present and evaluate profiles of two important axes of the development process: by milestone and by application layers. MPPs can be an aid to take project control actions and help in planning future projects. Based on the experiments, project stakeholders identified several points to improve the development process. It is also acknowledged, that profiles show additional information compared to a naive diff-based approach.<br/> &copy; 2015 IEEE.},
key={Software design},
keywords={Codes (symbols);Computer software;Computer software maintenance;Productivity;Project management;User interfaces;},
note={Application layers;Development process;Productivity analysis;Project stakeholders;Security;Software Measurement;Stakeholders;Version control system;},
URL={http://dx.doi.org/10.1109/ICSM.2015.7332490},
}


@inproceedings{20150700518602,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning from optimization: A case study with apache ant},
journal={Information and Software Technology},
author={De Oliveira Barros, Marcio and De Almeida Farzat, Fabio and Travassos, Guilherme Horta},
volume={57},
number={1},
year={2015},
pages={684 - 704},
issn={09505849},
abstract={Context: Software architecture degrades when changes violating the design-time architectural intents are imposed on the software throughout its life cycle. Such phenomenon is called architecture erosion. When changes are not controlled, erosion makes maintenance harder and negatively affects software evolution. Objective: To study the effects of architecture erosion on a large software project and determine whether search-based module clustering might reduce the conceptual distance between the current architecture and the design-time one. Method: To run an exploratory study with Apache Ant. First, we characterize Ant's evolution in terms of size, change dispersion, cohesion, and coupling metrics, highlighting the potential introduction of architecture and code-level problems that might affect the cost of changing the system. Then, we reorganize the distribution of Ant's classes using a heuristic search approach, intending to re-emerge its design-time architecture. Results: In characterizing the system, we observed that its original, simple design was lost due to maintenance and the addition of new features. In optimizing its architecture, we found that current models used to drive search-based software module clustering produce complex designs, which maximize the characteristics driving optimization while producing class distributions that would hardly be acceptable to developers maintaining Ant. Conclusion: The structural perspective promoted by the coupling and cohesion metrics precludes observing the adequate software module clustering from the perspective of software engineers when considering a large open source system. Our analysis adds evidence to the criticism of the dogma of driving design towards high cohesion and low coupling, at the same time observing the need for better models to drive design decisions. Apart from that, we see SBSE as a learning tool, allowing researchers to test SoftwareEngineering models in extreme situations that would not be easily found in software projects.<br/> &copy; 2014 Elsevier B.V. All rights reserved.},
key={Open systems},
keywords={Adhesion;Erosion;Heuristic algorithms;Life cycle;Modular robots;Open source software;Optimization;Software testing;},
note={Apache Ant;Architecture erosion;Class distributions;Experimental software engineering;Exploratory studies;Heuristic search;Open source system;Software modules;},
URL={http://dx.doi.org/10.1016/j.infsof.2014.07.015},
}


@inproceedings{20170803366614,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Why are commits being reverted? A comparative study of industrial and open source projects},
journal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
author={Shimagaki, Junji and Kamei, Yasutaka and McIntosh, Shane and Pursehouse, David and Ubayashi, Naoyasu},
year={2016},
pages={301 - 311},
address={Raleigh, NC, United states},
abstract={Software development is a cyclic process of integrating new features while introducing and fixing defects. During development, commits that modify source code files are uploaded to version control systems. Occasionally, these commits need to be reverted, i.e., the code changes need to be completely backed out of the software project. While one can often speculate about the purpose of reverted commits (e.g., the commit may have caused integration or build problems), little empirical evidence exists to substantiate such claims. The goal of this paper is to better understand why commits are reverted in large software systems. To that end, we quantitatively and qualitatively study two proprietary and four open source projects to measure: (1) the proportion of commits that are reverted, (2) the amount of time that commits that are eventually reverted linger within a codebase, and (3) the most frequent reasons why commits are reverted. Our results show that 1%-5% of the commits in the studied systems are reverted. Those commits that are eventually reverted linger within the studied codebases for 1-35 days (median). Furthermore, we identify 13 common reasons for reverting commits, and observe that the frequency of reverted commits of each reason varies broadly from project to project. A complementary qualitative analysis suggests that many reverted commits could have been avoided with better team communication and change awareness. Our findings made Sony Mobile's stakeholders aware that internally reverted commits can be reduced by paying more attention to their own changes. On the other hand, externally reverted commits could be minimized only if external stakeholders are involved to improve inter-company communication or requirements elicitation.<br/> &copy; 2016 IEEE.},
key={Open source software},
keywords={Computer software maintenance;Software design;},
note={Comparative studies;External stakeholders;Inter-company communication;Large software systems;Open source projects;Qualitative analysis;Requirements elicitation;Version control system;},
URL={http://dx.doi.org/10.1109/ICSME.2016.83},
}


@inproceedings{20153101087410,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Characterizing a new dimension of change in attending and responding to the substance of student thinking},
journal={Proceedings of International Conference of the Learning Sciences, ICLS},
author={Richards, Jennifer and Elby, Andrew and Gupta, Ayush},
volume={1},
number={January},
year={2014},
pages={286 - 293},
issn={18149316},
address={Boulder, CO, United states},
abstract={"Responsive teaching," in which teachers attend and respond to the substance of students' ideas, is central to facilitating student learning through engagement in authentic disciplinary practices. In characterizing teachers' progress toward greater responsiveness, researchers typically code teachers' attention as shifting toward the intellectual content (substance) of students' ideas and away from other foci such as students' correctness. These schemes, however, do not distinguish between different aspects of the substance of students' ideas. In this paper, we argue that a science teacher, Mr. S, demonstrates progress not by shifting toward greater attention to "substance," but rather by shifting in the facet of student thinking to which he primarily attends and responds. He shifts toward attending to causal stories (mechanistic explanations) and away from causal factors (potentially relevant variables). We argue that such shifts toward more sophisticated epistemic practices should be targets of professional development and of the assessment of responsive teaching.<br/> &copy; 2014 ISLS.},
key={Teaching},
keywords={Students;},
note={Intellectual content;New dimensions;Professional development;Science teachers;Student learning;Teachers';},
}


@article{20171003419417,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Use of ontology structure and Bayesian models to aid the crowdsourcing of ICD-11 sanctioning rules},
journal={Journal of Biomedical Informatics},
author={Lou, Yun and Tu, Samson W. and Nyulas, Csongor and Tudorache, Tania and Chalmers, Robert J.G. and Musen, Mark A.},
volume={68},
year={2017},
pages={20 - 34},
issn={15320464},
abstract={The International Classification of Diseases (ICD) is the de facto standard international classification for mortality reporting and for many epidemiological, clinical, and financial use cases. The next version of ICD, ICD-11, will be submitted for approval by the World Health Assembly in 2018. Unlike previous versions of ICD, where coders mostly select single codes from pre-enumerated disease and disorder codes, ICD-11 coding will allow extensive use of multiple codes to give more detailed disease descriptions. For example, &ldquo;severe malignant neoplasms of left breast&rdquo; may be coded using the combination of a &ldquo;stem code&rdquo; (e.g., code for malignant neoplasms of breast) with a variety of &ldquo;extension codes&rdquo; (e.g., codes for laterality and severity). The use of multiple codes (a process called post-coordination), while avoiding the pitfall of having to pre-enumerate vast number of possible disease and qualifier combinations, risks the creation of meaningless expressions that combine stem codes with inappropriate qualifiers. To prevent that from happening, &ldquo;sanctioning rules&rdquo; that define legal combinations are necessary. In this work, we developed a crowdsourcing method for obtaining sanctioning rules for the post-coordination of concepts in ICD-11. Our method utilized the hierarchical structures in the domain to improve the accuracy of the sanctioning rules and to lower the crowdsourcing cost. We used Bayesian networks to model crowd workers&rsquo; skills, the accuracy of their responses, and our confidence in the acquired sanctioning rules. We applied reinforcement learning to develop an agent that constantly adjusted the confidence cutoffs during the crowdsourcing process to maximize the overall quality of sanctioning rules under a fixed budget. Finally, we performed formative evaluations using a skin-disease branch of the draft ICD-11 and demonstrated that the crowd-sourced sanctioning rules replicated those defined by an expert dermatologist with high precision and recall. This work demonstrated that a crowdsourcing approach could offer a reasonably efficient method for generating a first draft of sanctioning rules that subject matter experts could verify and edit, thus relieving them of the tedium and cost of formulating the initial set of rules.<br/> &copy; 2017 Elsevier Inc.},
key={Bayesian networks},
keywords={Budget control;Codes (symbols);Crowdsourcing;Ontology;Reinforcement learning;Tumors;},
note={De facto standard;Disease and disorders;Formative evaluation;Hierarchical structures;International classification of disease;Post-coordination;Sanctioning rules;Subject matter experts;},
URL={http://dx.doi.org/10.1016/j.jbi.2017.02.004},
}


@inproceedings{20184105912855,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={MySQL extension automatic porting to PDO for PHP migration and security improvement},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Mondin, Fabio and Cortesi, Agostino},
volume={11127 LNCS},
year={2018},
pages={461 - 473},
issn={03029743},
address={Olomouc, Czech republic},
abstract={In software management, the upgrade of programming languages may introduce critical issues. This is the case of PHP, the fifth version of which is going towards the end of the support. The new release improves on different aspects, but removes the old deprecated MySQL extensions, and supports only the newer library of functions for the connection to the databases. The software systems already in place need to be renewed to be compliant with respect to the new language version. The conversion of the source code, to be safe against injection attacks, should involve also the transformation of the query code. The purpose of this work is the design of specific tool that automatically applies the required transformation yielding to a precise and efficient conversion procedure. The tool has been applied to different projects to provide evidence of its effectiveness.<br/> &copy; Springer Nature Switzerland AG 2018.},
key={Static analysis},
keywords={Codes (symbols);Industrial management;Information management;Information systems;Information use;Network security;},
note={Code conversion;Conversion procedures;Critical issues;Deprecated MySQL;Security improvement;Software management;Software systems;Specific tool;},
URL={http://dx.doi.org/10.1007/978-3-319-99954-8_38},
}


@inproceedings{20171703589559,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Exact Reconstruction from Insertions in Synchronization Codes},
journal={IEEE Transactions on Information Theory},
author={Sala, Frederic and Gabrys, Ryan and Schoeny, Clayton and Dolecek, Lara},
volume={63},
number={4},
year={2017},
pages={2428 - 2445},
issn={00189448},
abstract={This paper studies problems in data reconstruction, an important area with numerous applications. In particular, we examine the reconstruction of binary and nonbinary sequences from synchronization (insertion/deletion-correcting) codes. These sequences have been corrupted by a fixed number of symbol insertions (larger than the minimum edit distance of the code), yielding a number of distinct traces to be used for reconstruction. We wish to know the minimum number of traces needed for exact reconstruction. This is a general version of a problem tackled by Levenshtein for uncoded sequences. We introduce an exact formula for the maximum number of common supersequences shared by sequences at a certain edit distance, yielding an upper bound on the number of distinct traces necessary to guarantee exact reconstruction. Without specific knowledge of the code words, this upper bound is tight. We apply our results to the famous single deletion/insertion-correcting Varshamov-Tenengolts (VT) codes and show that a significant number of VT code word pairs achieve the worst case number of outputs needed for exact reconstruction. We also consider extensions to other channels, such as adversarial deletion and insertion/deletion channels and probabilistic channels.<br/> &copy; 1963-2012 IEEE.},
key={Codes (symbols)},
keywords={Synchronization;},
note={Edit distance;Exact reconstruction;Insertion/deletion channels;Insertions and deletions;Minimum edit distance;Nonbinary sequences;Sequence reconstruction;Synchronization codes;},
URL={http://dx.doi.org/10.1109/TIT.2017.2649493},
}


@inproceedings{20162302459172,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Segregating feature interfaces to support software product line maintenance},
journal={MODULARITY 2016 - Proceedings of the 15th International Conference on Modularity},
author={Cafeo, Bruno B.P. and Hunsen, Claus and Garcia, Alessandro and Apel, Sven and Lee, Jaejoon},
year={2016},
pages={1 - 12},
address={Malaga, Spain},
abstract={Although software product lines are widely used in practice, their maintenance is challenging. Features as units of behaviour can be heavily scattered across the source code of a product line, hindering modular reasoning. To alleviate this problem, feature interfaces aim at enhancing modular reasoning about features. However, considering all members of a feature interface is often cumbersome, especially due to the large number of members arising in practice. To address this problem, we present an approach to group members of a feature interface based on their mutual dependencies. We argue that often only a subset of all interface members is relevant to a maintenance task. Therefore, we propose a graph representation that is able to capture the collaboration between members and apply a clustering algorithm to it to group highly-related members and segregate non-related members. On a set of ten versions of a real-world product line, we evaluate the effectiveness of our approach, by comparing the two types of feature interfaces (segregated vs. original interfaces) with co-change information from the version-control system. We found a potential reduction of 62% of the interface members to be considered during maintenance.<br/> &copy; 2016 ACM.},
key={Computer software maintenance},
keywords={Clustering algorithms;Computer software;Software design;},
note={Feature dependencies;Graph representation;Maintenance tasks;Mutual dependencies;Potential reduction;Software Product Line;Software product line maintenances;Version control system;},
URL={http://dx.doi.org/10.1145/2889443.2889451},
}


@inproceedings{20160501876353,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={RCLinker: Automated Linking of Issue Reports and Commits Leveraging Rich Contextual Information},
journal={IEEE International Conference on Program Comprehension},
author={Le, Tien-Duy B. and Linares-Vasquez, Mario and Lo, David and Poshyvanyk, Denys},
volume={2015-August},
year={2015},
pages={36 - 47},
address={Florence, Italy},
abstract={Links between issue reports and their corresponding commits in version control systems are often missing. However, these links are important for measuring the quality of various parts of a software system, predicting defects, and many other tasks. A number of existing approaches have been designed to solve this problem by automatically linking bug reports to source code commits via comparison of textual information in commit messages with textual contents in the bug reports. Yet, the effectiveness of these techniques is oftentimes sub optimal when commit messages are empty or only contain minimum information, this particular problem makes the process of recovering trace ability links between commits and bug reports particularly challenging. In this work, we aim at improving the effectiveness of existing bug linking techniques by utilizing rich contextual information. We rely on a recently proposed tool, namely Change Scribe, which generates commit messages containing rich contextual information by using a number of code summarization techniques. Our approach then extracts features from these automatically generated commit messages and bug reports and inputs them into a classification technique that creates a discriminative model used to predict if a link exists between a commit message and a bug report. We compared our approach, coined as RCLinker (Rich Context Linker), to MLink, which is an existing state-of-the-art bug linking approach. Our experiment results on bug reports from 6 software projects show that RCLinker can outperform MLink in terms of F-measure by 138.66%.<br/> &copy; 2015 IEEE.},
key={Computer programming},
keywords={Classification (of information);Feature extraction;},
note={Automatically generated;ChangeScribe;Classification technique;Contextual information;Discriminative models;Recovering Missing Links;Textual information;Version control system;},
URL={http://dx.doi.org/10.1109/ICPC.2015.13},
}


@article{20160701954498,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A quality driven extension to the QVT-relations transformationlanguage},
journal={Computer Science - Research and Development},
author={Drago, Mauro Luigi and Ghezzi, Carlo and Mirandola, Raffaela},
volume={30},
number={1},
year={2015},
pages={1 - 20},
issn={18652034},
abstract={An emerging approach to software development is Model Driven Software Development(MDSD). It shifts the focus from source code to models, aims at cost reduction, riskmitigation, and eases the engineering of complex applications. System models can beused in the early development stages to verify certain relevant properties, such asperformance, before source code is available and problems become hard and costly tosolve. The present status of Model Driven Engineering (MDE) is still far from thisideal situation. A well-known problem is feedback provisioning, which arises whendifferent solutions for the same design problem exist. An approach for feedbackprovisioning automation leverages model transformations, which glue together modelsin an MDSD setting, encapsulate the design rationale, and promote knowledge reuseand solutions otherwise available only to experienced engineers. In this article wepresent QVTR<sup>2</sup>, our solution to the feedback problem.QVTR<sup>2</sup>&nbsp;is an extension of the QVT-Relations languagewith constructs to express design alternatives, their impact on non-functionalmetrics, and how to evaluate them and guide the engineers in the selection of themost appropriate solution. We demonstrate the effectiveness of our solution by usingthe QVTR<sup>2</sup>&nbsp;engine to perform a modified version of thestandard UML-to-RDBMS transformation in thecontext of a real e-commerce application, and by showing how we can guide anon-expert engineer in the selection of a solution that satisfies given performancerequirements.<br/> &copy; 2011, Springer-Verlag.},
key={Software design},
keywords={Cost engineering;Cost reduction;Engineers;},
note={Complex applications;Design alternatives;E-Commerce applications;Model transformation;Model-driven Engineering;Model-Driven Software Development;Quality prediction;Transformation languages;},
URL={http://dx.doi.org/10.1007/s00450-011-0202-0},
}


@inproceedings{20185106251671,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Detecting and predicting evolution in spreadsheets-a case study in an energy network company},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
author={Jansen, Bas and Hermans, Felienne and Tazelaar, Edwin},
year={2018},
pages={645 - 654},
address={Madrid, Spain},
abstract={The use of spreadsheets in industry is widespread and the information that they provide is often used for decisions. Research has shown that spreadsheets are error-prone, leading to the risk that decisions are made on incorrect information. Software Evolution is a well-researched topic and the results have proven to support developers in creating better software. Could this also be applied to spreadsheets? Unfortunately, the research on spreadsheet evolution is still limited. Therefore, the aim of this paper is to obtain a better understanding of how spreadsheets evolve over time and if the results of such a study provide similar benefits for spreadsheets as it does for source code. In this study, we cooperated with Alliander, a large energy network company in the Netherlands. We conducted two case studies on two different set of spreadsheets that both were already maintained for a period of three years. To have a better understanding of the spreadsheets itself and the context in which they evolved, we also interviewed the creators of the spreadsheets. We focus on the changes that are made over time in the formulas. Changes in these formulas change the behavior of the spreadsheet and could possibly introduce errors. To effectively analyze these changes we developed an algorithm that is able to detect and visualize these changes. Results indicate that studying the evolution of a spreadsheet helps to identify areas in the spreadsheet that are error-prone, likely to change or that could benefit from refactoring. Furthermore, by analyzing the frequency in which formulas are changed from version to version, it is possible to predict which formulas need to be changed when a new version of the spreadsheet is created.<br/> &copy; 2018 IEEE.},
key={Spreadsheets},
keywords={Computer programming;Computer software maintenance;Errors;},
note={Case-studies;End user programming;Energy networks;Error prones;Refactorings;Software Evolution;Source codes;Version;},
URL={http://dx.doi.org/10.1109/ICSME.2018.00074},
}


@article{20141217486041,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Parallel implementation of a combined moment expansion and spherical-multipole time-domain near-field to far-field transformation},
journal={IEEE Transactions on Magnetics},
author={Ramos, Glaucio L. and Rego, Cassio G. and Fonseca, Alexandre R.},
volume={50},
number={2},
year={2014},
issn={00189464},
abstract={A time-domain spherical-multipole near-to-far-field algorithm running in a parallelized code version is introduced in this paper. Such an approach is employed to obtain UWB antenna radiated fields and radiation patterns directly in time domain, which is more convenient to perform a unified characterization in time and frequency domains. We propose the use of the OpenMP, an application program interface that permits the code parallelization with almost no intervention. The results show that the proposed technique allows a code running 28 times faster when using a computer with 24 processors, each one with two threads, when compared with the sequential one. It also suggested a moment expansion technique to improve the accuracy and computational efficiency when using Gaussian pulse excitation when compared with the Fourier transform. &copy; 2014 IEEE.<br/>},
key={Time domain analysis},
keywords={Antennas;Application programming interfaces (API);Application programs;Codes (symbols);Computational efficiency;Directional patterns (antenna);Expansion;Spheres;Ultra-wideband (UWB);},
note={Application program interfaces;Gaussian-pulse excitation;Near field to far field transformations;OpenMP;Parallel processing;Spherical multipole;Unified characterization in time and frequency domains;UWB antenna;},
URL={http://dx.doi.org/10.1109/TMAG.2013.2280796},
}


@inproceedings{20184205951551,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Demo: Interactive robot transition repair},
journal={Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
author={Holtz, Jarrett and Guha, Arjun and Biswas, Joydeep},
volume={3},
year={2018},
pages={1818 - 1819},
issn={15488403},
address={Stockholm, Sweden},
abstract={Complex robot behaviors are often structured as state machines, where states encapsulate actions and a transition function switches between states. Since transitions depend on physical parameters, when the environment changes, a roboticist has to painstakingly readjust the parameters to work in the new environment. In this demo we present Interactive SMT-based Robot Transition Repair (SRTR): instead of manually adjusting parameters, we ask users to identify a few instances where the robot is in a wrong state and what the right state should be. A lightweight automated analysis of the transition function's source code then 1) identifies adjustable parameters, 2) converts the transition function into a system of logical constraints, and 3) formulates the constraints and user-supplied corrections as a MaxSMT problem that yields adjustments to parameter values. This demo uses a simulated RoboCup Small Size League platform, allows users to correct faulty behaviors, and then uses SRTR to adjust parameters automatically.<br/> &copy; 2018 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.},
key={Autonomous agents},
keywords={Multi agent systems;Repair;Robots;},
note={Adjustable parameters;Adjusting parameters;Environment change;Learning and adaptation;Logical constraints;Physical parameters;RoboCup Small Size League;Transition functions;},
}


@inproceedings{20153201154938,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Process mining multiple repositories for software defect resolution from control and organizational perspective},
journal={11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings},
author={Gupta, Monika and Sureka, Ashish and Padmanabhuni, Srinivas},
year={2014},
pages={122 - 131},
address={Hyderabad, India},
abstract={Issue reporting and resolution is a software engineering process supported by tools such as Issue Tracking System (ITS), Peer Code Review (PCR) system and Version Control System (VCS). Several open source software projects such as Google Chromium and Android follow process in which a defect or feature enhancement request is reported to an issue tracker followed by source-code change or patch review and patch commit using a version control system. We present an application of process mining three software repositories (ITS, PCR and VCS) from control flow and organizational perspective for effective process management. ITS, PCR and VCS are not explicitly linked so we implement regular expression based heuristics to integrate data from three repositories for Google Chromium project. We define activities such as bug reporting, bug fixing, bug verification, patch submission, patch review, and source code commit and create an event log of the bug resolution process. The extracted event log contains audit trail data such as caseID, timestamp, activity name and performer. We discover runtime process model for bug resolution process spanning three repositories using process mining tool, Disco, and conduct process performance and efficiency analysis. We identify bottlenecks, define and detect basic and composite anti-patterns. In addition to control flow analysis, we mine event log to perform organizational analysis and discover metrics such as handover of work, subcontracting, joint cases and joint activities.<br/> Copyright 2014 ACM.},
key={Open systems},
keywords={Application programs;Chromium;Codes (symbols);Computer software maintenance;Control systems;Data mining;Defects;Highway planning;Information management;Open source software;Social networking (online);Social sciences computing;Tracking (position);},
note={Empirical Software Engineering;Issue Tracking;Open source software projects;Organizational analysis;Organizational perspectives;Peer code review;Process mining;Software engineering process;},
URL={http://dx.doi.org/10.1145/2597073.2597081},
}


@article{20141817648362,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Synthesizing VHDL from activity models in UML 2},
journal={International Journal of Circuit Theory and Applications},
author={Balderas-Contreras, Tomas and Cumplido, Rene and Rodriguez, Gustavo},
volume={42},
number={5},
year={2014},
pages={542 - 550},
issn={00989886},
abstract={This document describes a synthesis technology that generates structural VHDL code from models describing the flow of data required to perform algorithms operating on bit-blocks. The models are built using restricted activity diagrams in the Unified Modeling Language version 2. The code generator is developed using Acceleo, a technology to implement transformations from models to text. The technology described in this paper exploits the principles of object orientation and model-driven engineering. The primary aim is to improve productivity and alleviate complexity during the design of digital hardware systems that implement demanding operations used by a wide variety of computing devices. The use of the technology is illustrated with the generation of VHDL code from models describing a block cipher algorithm. Copyright &copy; 2012 John Wiley &amp; Sons, Ltd. We explore the feasibility of transforming high-level models of block cipher algorithms, built using activity diagrams in the Unified Modeling Language version 2 (UML 2), to source code in VHDL. The lower level VHDL representation may be synthesized and implemented in a hardware platform like a FPGA. We describe how to adapt the UML 2 language to model block cipher algorithms accurately and the transformation tool that generates VHDL code from the UML 2 diagrams. This technology aims at improving the productivity of the designers. &copy; 2012 John Wiley &amp; Sons, Ltd.<br/>},
key={Unified Modeling Language},
keywords={Automatic programming;Codes (symbols);Computer hardware description languages;Digital devices;Productivity;Program compilers;Software design;Systems analysis;},
note={Code Generation;Domain specific modeling;Meta model;Model-driven Engineering;UML 2;},
URL={http://dx.doi.org/10.1002/cta.1874},
}


@inproceedings{20162902594969,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Programming arduino boards with the C/C++ interpreter CH},
journal={Proceedings of the ASME Design Engineering Technical Conference},
author={Turley, Curtis and Alessandra Montironi, Maria},
volume={9},
year={2015},
pages={Computers and Information in Engineering Division; Design Engineering Division - },
address={Boston, MA, United states},
abstract={This paper presents the ChArduino package which is designed to control the Atmel AVR microcontroller based Arduino boards through the C/C++ interpreter Ch. Traditionally, Arduino boards are programmed using the official Arduino IDE or lower-level AVR C libraries. These methods require specific cross-compilation tools to compile the code and upload it onto the board. Whenever a change is made to the source code, it needs to be recompiled and uploaded, making application development cumbersome, especially for beginners and as the size of the application grows. The approach presented in this paper is aimed at reducing the effort associated with code compilation, especially in classroom environments where microcontroller programming is first introduced. In fact, when using this method, code is executed in an interpreted manner and every function call is processed separately by the interpreter, thus compilation and uploading are not required to make changes effective. The ChArduino package consists of a library of functions running on a computer and a specialized firmware loaded onto the Arduino board. The firmware on the Arduino board is pre-compiled and the latest version is automatically uploaded at run time, if not already. At power-up, the firmware initializes the board and then waits for a command from the computer. The use of the C/C++ interpreter Ch also makes available line-by-line debugging, nu-merical analysis, and plotting capabilities. The supported communication protocols between the Arduino board and the computer are serial and Bluetooth. The application code written using this package is completely compatible with the entire spectrum of Arduino boards and can be ported to the Arduino IDE with minimal changes. The applications of the method described in this paper are general but apply especially to the K-12 education field in that the package creates a simple, user-friendly, environment for the absolute beginner to learn the basic principles of mechatronic systems including programming, microcontrollers, and electrical circuits. Lesson plans are being developed to use the ChArduino package in microcontroller introductory courses and the package is currently being introduced for preliminary testing in schools through the UC Davis C-STEM Center.<br/> &copy; Copyright 2015 by ASME.},
key={C++ (programming language)},
keywords={Codes (symbols);Computer systems programming;Controllers;Design;Embedded systems;Firmware;Integrodifferential equations;Microcontrollers;Network protocols;Program interpreters;Teaching;Uranium compounds;},
note={Application development;Atmel avr microcontrollers;C/C++ interpreter;Classroom environment;Electrical circuit;Introductory course;Mechatronic systems;Microcontroller programming;},
URL={http://dx.doi.org/10.1115/DETC2015-47837},
}


@article{20143017965901,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Symbolic crosschecking of data-parallel floating-point code},
journal={IEEE Transactions on Software Engineering},
author={Collingbourne, Peter and Cadar, Cristian and Kelly, Paul H.J.},
volume={40},
number={7},
year={2014},
pages={710 - 737},
issn={00985589},
abstract={We present a symbolic execution-based technique for cross-checking programs accelerated using SIMD or OpenCL against an unaccelerated version, as well as a technique for detecting data races in OpenCL programs. Our techniques are implemented in KLEE-CL, a tool based on the symbolic execution engine KLEE that supports symbolic reasoning on the equivalence between expressions involving both integer and floating-point operations. While the current generation of constraint solvers provide effective support for integer arithmetic, the situation is different for floating-point arithmetic, due to the complexity inherent in such computations. The key insight behind our approach is that floating-point values are only reliably equal if they are essentially built by the same operations. This allows us to use an algorithm based on symbolic expression matching augmented with canonicalisation rules to determine path equivalence. Under symbolic execution, we have to verify equivalence along every feasible control-flow path. We reduce the branching factor of this process by aggressively merging conditionals, if-converting branches into select operations via an aggressive phi-node folding transformation. To support the Intel Streaming SIMD Extension (SSE) instruction set, we lower SSE instructions to equivalent generic vector operations, which in turn are interpreted in terms of primitive integer and floating-point operations. To support OpenCL programs, we symbolically model the OpenCL environment using an OpenCL runtime library targeted to symbolic execution. We detect data races by keeping track of all memory accesses using a memory log, and reporting a race whenever we detect that two accesses conflict. By representing the memory log symbolically, we are also able to detect races associated with symbolically-indexed accesses of memory objects. We used KLEE-CL to prove the bounded equivalence between scalar and data-parallel versions of floating-point programs and find a number of issues in a variety of open source projects that use SSE and OpenCL, including mismatches between implementations, memory errors, race conditions and a compiler bug. &copy; 1976-2012 IEEE.<br/>},
key={Digital arithmetic},
keywords={Model checking;Program compilers;},
note={Data parallel;Floating points;OpenCL;SIMD;Symbolic execution;},
URL={http://dx.doi.org/10.1109/TSE.2013.2297120},
}


@inproceedings{20174004229737,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Investigating static analysis errors in student Java programs},
journal={ICER 2017 - Proceedings of the 2017 ACM Conference on International Computing Education Research},
author={Edwards, Stephen H. and Kandru, Nischel and Rajagopal, Mukund B.M.},
year={2017},
pages={65 - 73},
address={Tacoma, WA, United states},
abstract={Research on students learning to program has produced studies on both compile-Time errors (syntax errors) and run-Time errors (exceptions). Both of these types of errors are natural targets, since detection is built into the programming language. In this paper, we present an empirical investigation of static analysis errors present in syntactically correct code. Static analysis errors can be revealed by tools that examine a program's source code, but this error detection is typically not built into common programming languages and instead requires separate tools. Static analysis can be used to check formatting or commenting expectations, but it also can be used to identify problematic code or to find some kinds of conceptual or logic errors. We study nearly 10 million static analysis errors found in over 500 thousand program submissions made by students over a fivesemester period. The study includes data from four separate courses, including a non-majors introductory course as well as the CS1/CS2/CS3 sequence for CS majors. We examine the differences between the error rates of CS major and non-major beginners, and also examine how these patterns change over time as students progress through the CS major course sequence. Our investigation shows that while formatting and Javadoc issues are the most common, static checks that identify coding flaws that are likely to be errors are strongly correlated with producing correct programs, even when students eventually fix the problems. With experience, students produce fewer errors, but the errors that are most frequent are consistent between both computer science majors and non-majors, and across experience levels. These results can highlight student struggles or misunderstandings that have escaped past analyses focused on syntax or run-Time errors.<br/> &copy; 2017 ACM.},
key={Static analysis},
keywords={Codes (symbols);Coding errors;Computation theory;Computer software;Computer systems programming;Error detection;Java programming language;Object oriented programming;Polarization mode dispersion;Students;Syntactics;System program documentation;},
note={Checkstyle;Coding standards;Coding style;Formatting;Java;Web-CAT;},
URL={http://dx.doi.org/10.1145/3105726.3106182},
}


@inproceedings{20180604765575,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Improved query reformulation for concept location using CodeRank and document structures},
journal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
author={Rahman, Mohammad Masudur and Roy, Chanchal K.},
year={2017},
pages={428 - 439},
address={Urbana-Champaign, IL, United states},
abstract={During software maintenance, developers usually deal with a significant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique-ACER-that takes an initial query, identifies appropriate search terms from the source code using a novel term weight-CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with five closely related existing techniques in query reformulation not only validates our empirical findings but also demonstrates the superiority of our technique.<br/> &copy; 2017 IEEE.},
key={Quality control},
keywords={Learning systems;Location;Software engineering;},
note={CodeRank;Concept locations;query quality analysis;Query reformulation;Resampling;Term weighting;},
URL={http://dx.doi.org/10.1109/ASE.2017.8115655},
}


@inproceedings{20142717897698,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Towards suitable research paradigm for assessing the impact of free and open source software (foss)},
journal={Lecture Notes in Engineering and Computer Science},
author={Dehinbo, Kehinde O. and Dehinbo, Johnson O.},
volume={1},
year={2013},
pages={139 - 144},
issn={20780958},
address={San Francisco, CA, United states},
abstract={Free and Open Source Software (FOSS) allows users to use, change, and redistribute the source code. Recent changes in the software technologies landscape involve the introduction of FOSS which presents certain benefits and freedom in the use of software that demonstrate high potential towards achieving competitive advantage by institutions of higher learning. Higher institutions of learning stand to gain the benefits in teaching, learning and research in particular by adopting FOSS. Towards determining the possibility of such gains, research efforts can be conducted using interpretive and positivist approaches. This study proposes exploration using the two approaches in the form of case study and survey so that readers can make informed choice that could lead to the development of appropriate frameworks towards addressing the research objectives.<br/>},
key={Open source software},
keywords={Competition;Engineering education;Open systems;},
note={Competitive advantage;Free and open source softwares;Higher learning;Research efforts;Research objectives;Software technology;Technological acceptance model;Use of FOSS;},
}


@inproceedings{20182005207553,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={If memory serves: Towards designing and evaluating a game for teaching pointers to undergraduate students},
journal={ITiCSE-WGR 2017 - Proceedings of the 2017 ITiCSE Conference on Working Group Reports},
author={McGill, Monica M. and Johnson, Chris and Atlas, James and Bouchard, Durell and Messom, Chris and Pollock, Ian and Scott, Michael James},
volume={2018-January},
year={2018},
pages={25 - 46},
address={Bologna, Italy},
abstract={Games can serve as a valuable tool for enriching computer science education, since they can facilitate a number of conditions that can promote learning and instigate affective change. As part of the 22nd ACM Annual Conference on Innovation and Technology in Computer Science Education (ITiCSE 2017), the Working Group on Game Development for Computer Science Education convened to extend their prior work, a review of the literature and a review of over 120 educational games that support computing instruction. The Working Group builds off this earlier work to design and develop a prototype of a game grounded in specific learning objectives. They provide the source code for the game to the computing education community for further review, adaptation, and exploration. To aid this endeavor, the Working Group also chose to explore the research methods needed to establish validity, highlighting a need for more rigorous approaches to evaluate the effectiveness of the use of games in computer science education. This report provides two distinct contributions to the body of knowledge in games for computer science education.We present an experience report in the form of a case study describing the design and development of If Memory Serves, a game to support teaching pointers to undergraduate students. We then propose guidelines to validate its effectiveness rooted in theoretical approaches for evaluating learning in games and media.We include an invitation to the computer science education community to explore the game's potential in classrooms and report on its ability to achieve the stated learning outcomes.<br/> &copy; 2017 Association for Computing Machinery.},
key={Computer games},
keywords={Computers;Design;Education computing;Engineering education;Software design;Students;Teaching;},
note={Computer memory;Development;Educational;Games;Learning;Pointers;Serious;Validation framework;},
URL={http://dx.doi.org/10.1145/3174781.3174783},
}


@inproceedings{20183305681479,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Where's my code? Engineers navigating ethical issues on an uneven terrain},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
author={Rottmann, Cindy and Reeve, Doug and Sacks, Robin and Klassen, Mike},
volume={2018-June},
year={2018},
issn={21535965},
address={Salt Lake City, UT, United states},
abstract={Claims to professionalism among engineers are rooted in three key features: a specialized knowledge base, self-regulation, and a commitment to public service-[1-3] elements that have been historically codified into a set of ethical guidelines [1, 4, 5]. While these guidelines-Professional Codes of Ethics-may help engineers appreciate what not to do [4, 5], they are insufficiently specific to guide novice engineers through ethically ambiguous situations. As early 20<sup>th</sup> century artefacts, they also tend to reproduce structural inequities embedded in the history of the profession, and thus fail to reflect the experiences of historically underrepresented groups of engineers [6-14]. The Canadian Engineering Accreditation Board's (CEAB) pairing of ethics and equity [15] demands that we look beyond the codes to help our students navigate ethically ambiguous situations and patterns of privilege likely to arise in their professional lives. Unfortunately, there are several barriers to this process. Our critical analysis of career history interviews with 15 engineers committed to ethics and equity highlight three such barriers: 1) dominant narratives in engineering that make it difficult for social justice viewpoints to be acknowledged; 2) limited organizational influence on the part of junior engineers trying to challenge inequitable workplace practices; and 3) a fear that raising equity issues will result in personal attacks rather than positive change. Together, these three barriers-raised almost exclusively by female, racially under-represented, and LGBTQ identified engineers-illustrate the uneven terrain on which engineers navigate ethical issues.<br/> &copy; American Society for Engineering Education, 2018.},
}


@inproceedings{20173404059068,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Droidsentry: Efficient Code Integrity and Control Flow Verification on TrustZone Devices},
journal={Proceedings - 2017 21st International Conference on Control Systems and Computer, CSCS 2017},
author={Suciu, Darius-Andrei and Sion, Radu},
year={2017},
pages={156 - 158},
address={Bucharest, Romania},
abstract={The fast evolution of mobile devices has made them the center of attention for not only the research industry, but also malicious actors, as smartphones are used to store, transmit and process sensitive information. The diversity and number of typically installed applications create windows of opportunity for attackers. Attackers can use vulnerable applications to gain control over the device or change the behavior of applications relied on to manage user's finances or store their secret data. Thus, current mobile systems need application execution verification mechanisms. In consequence, we present a framework for current ARM mobile devices that can detect application control flow manipulation attempts by looking at the history of executed control flow altering instructions on the processor. This history examination provides enough information to implement the state-of-the-art fine-grained control policies, without additional binary instrumentation. Moreover, this framework is designed to work with existing hardware and have a minimal impact on performance.<br/> &copy; 2017 IEEE.},
key={Computer control systems},
keywords={Computer science;Computers;Control engineering;},
note={Application execution;Binary instrumentations;Control-flow integrities;Fine-grained control;Macro cells;Return-oriented programming;Sensitive informations;TrustZone;},
URL={http://dx.doi.org/10.1109/CSCS.2017.28},
}


@inproceedings{20141717601712,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Design Space Exploration based on multiobjective genetic algorithms and clustering-based high-level estimation},
journal={2013 23rd International Conference on Field Programmable Logic and Applications, FPL 2013 - Proceedings},
author={Martins, Luiz G. A. and Marques, Eduardo},
year={2013},
pages={Altera, Corp.; et al.; Microsemi, Corp.; Sigasi, nv; Solarflare Comm., Inc.; Xilinx, Inc. - },
address={Porto, Portugal},
abstract={A desirable characteristic in high-level synthesis (HLS) is fast search and analysis of implementation alternatives with low or none intervention. This process is known as Design Space Exploration (DSE) and it requires an efficient search method. The employment of intelligent techniques like evolutionary algorithms has been investigated as an alternative to DSE. They turn possible to reduce the search time through selection of higher potential regions of the solution space. We propose here the development of a DSE approach based on a multiobjective evolutionary algorithm (MOEA) and machine learning techniques. It must be employed to indicate the code transformations and architectural parameters adopted in design solution. Furthermore, DSE will use a high-level estimator model to evaluate candidate solutions. Such model must be able to provide a good estimation of energy consumption and execution time at early stages of design. &copy; 2013 IEEE.<br/>},
key={Clustering algorithms},
keywords={Computer circuits;Cosine transforms;Energy utilization;Genetic algorithms;High level synthesis;Learning systems;},
note={Architectural parameters;Code transformation;Design space exploration;High-level estimation;Intelligent techniques;Machine learning techniques;Multi objective evolutionary algorithms;Multi-objective genetic algorithm;},
URL={http://dx.doi.org/10.1109/FPL.2013.6645608},
}


@inproceedings{20121915000195,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using topic models to support software maintenance},
journal={Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
author={Grant, Scott and Cordy, James R. and Skillicorn, David B.},
year={2012},
pages={403 - 408},
issn={15345351},
address={Szeged, Hungary},
abstract={Our recent research has shown that the latent information found by commonly used topic models generally relates to the development history of a software system. While it is not always possible to associate these latent topics with human-oriented concepts, it is demonstrable that they identify historical maintenance relationships in source code. Specifically, when a developer makes a change to a software project, it is common for a significant part of that change to relate to a single latent topic. A significant conclusion can be drawn from this: latent topic models identify co-maintenance relationships with no supervision, and therefore topic models can be used to support the maintenance phase of software development. &copy; 2012 IEEE.<br/>},
key={Computer software maintenance},
keywords={Reengineering;Software design;},
note={Development history;Latent information;Latent topic model;Recent researches;Software project;Software systems;Source codes;Topic model;},
URL={http://dx.doi.org/10.1109/CSMR.2012.51},
}


@inproceedings{20180904837282,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={What if i Had No Smells?},
journal={International Symposium on Empirical Software Engineering and Measurement},
author={Falessi, Davide and Russo, Barbara and Mullen, Kathleen},
volume={2017-November},
year={2017},
pages={78 - 84},
issn={19493770},
address={Toronto, ON, Canada},
abstract={What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.<br/> &copy; 2017 IEEE.},
key={Odors},
keywords={Codes (symbols);Learning systems;Software engineering;},
note={Code smell;Industrial case study;Product changes;Software estimation;Technical debts;What-if scenarios;},
URL={http://dx.doi.org/10.1109/ESEM.2017.14},
}


@inproceedings{20162802585879,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Self-modification of policy and utility function in rational agents},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Everitt, Tom and Filan, Daniel and Daswani, Mayank and Hutter, Marcus},
volume={9782},
year={2016},
pages={1 - 11},
issn={03029743},
address={New York, NY, United states},
abstract={Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify &ndash; for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby &lsquo;escaping&rsquo; the control of their creators. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.<br/> &copy; Springer International Publishing Switzerland 2016.},
key={Rational functions},
keywords={Intelligent agents;Intelligent systems;Reinforcement learning;},
note={Rational agents;Self-modification;Source codes;Utility functions;Value functions;},
URL={http://dx.doi.org/10.1007/978-3-319-41649-6_1},
}


@inproceedings{20171203471172,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Efficient dynamic malware analysis based on network behavior using deep learning},
journal={2016 IEEE Global Communications Conference, GLOBECOM 2016 - Proceedings},
author={Shibahara, Toshiki and Yagi, Takeshi and Akiyama, Mitsuaki and Chiba, Daiki and Yada, Takeshi},
year={2016},
pages={AT and T; et al.; Huawei; Intel; National Instruments; Qualcomm - },
address={Washington, DC, United states},
abstract={Malware authors or attackers always try to evade detection methods to accomplish their mission. Such detection methods are broadly divided into three types: static feature, host-behavior, and network-behavior based. Static feature-based methods are evaded using packing techniques. Host- behavior-based methods also can be evaded using some code injection methods, such as API hook and dynamic link library hook. This arms race regarding static feature-based and host-behavior- based methods increases the importance of network-behavior-based methods. The necessity of communication between infected hosts and attackers makes it difficult to evade network-behavior- based methods. The effectiveness of such methods depends on how we collect a variety of communications by using malware samples. However, analyzing all new malware samples for a long period is infeasible. Therefore, we propose a method for determining whether dynamic analysis should be suspended based on network behavior to collect malware communications efficiently and exhaustively. The key idea behind our proposed method is focused on two characteristics of malware communication: the change in the communication purpose and the common latent function. These characteristics of malware communications resemble those of natural language from the viewpoint of data structure, and sophisticated analysis methods have been proposed in the field of natural language processing. For this reason, we applied the recursive neural network, which has recently exhibited high classification performance, to our proposed method. In the evaluation with 29,562 malware samples, our proposed method reduced 67.1% of analysis time while keeping the coverage of collected URLs to 97.9% of the method that continues full analyses.<br/> &copy; 2016 IEEE.},
key={Deep learning},
keywords={Computer crime;Malware;Natural language processing systems;Neural networks;},
note={Classification performance;Detection methods;Dynamic link library;Dynamic malware analysis;Natural languages;Network behaviors;Packing techniques;Recursive neural networks;},
URL={http://dx.doi.org/10.1109/GLOCOM.2016.7841778},
}


@inproceedings{20173204032152,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={From problem landscapes to language landscapes: Questions in genetic improvement},
journal={GECCO 2017 - Proceedings of the Genetic and Evolutionary Computation Conference Companion},
author={Cody-Kenny, Brendan and Fenton, Michael and O'Neill, Michael},
year={2017},
pages={1509 - 1510},
address={Berlin, Germany},
abstract={Managing and curating software is a time consuming process particularly as programming languages, libraries, and execution environments change. To support the engineering of software, we propose applying a GP-based continuous learning system to all "useful" software. We take the position that search-based itemization and analysis of all commonly used software is feasible, in large part, because the requirements that people place on software can be used to bound the search space to software which is of high practical use. By repeatedly reusing the information generated during the search process we hope to attain a higher-level, but also more rigorous, understanding of our engineering material-source code.<br/>},
key={Search engines},
keywords={Genetic programming;Software engineering;},
note={Continuous learning;Engineering materials;Execution environments;Genetic improvements;Learning;Practical use;Search;Search process;},
URL={http://dx.doi.org/10.1145/3067695.3082522},
}


@inproceedings{20162802587867,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning convolutional action primitives for fine-grained action recognition},
journal={Proceedings - IEEE International Conference on Robotics and Automation},
author={Lea, Colin and Vidal, Rene and Hager, Gregory D.},
volume={2016-June},
year={2016},
pages={1642 - 1649},
issn={10504729},
address={Stockholm, Sweden},
abstract={Fine-grained action recognition is important for many applications of human-robot interaction, automated skill assessment, and surveillance. The goal is to segment and classify all actions occurring in a time series sequence. While recent recognition methods have shown strong performance in robotics applications, they often require hand-crafted features, use large amounts of domain knowledge, or employ overly simplistic representations of how objects change throughout an action. In this paper we present the Latent Convolutional Skip Chain Conditional Random Field (LC-SC-CRF). This time series model learns a set of interpretable and composable action primitives from sensor data. We apply our model to cooking tasks using accelerometer data from the University of Dundee 50 Salads dataset and to robotic surgery training tasks using robot kinematic data from the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our performance on 50 Salads and JIGSAWS are 18.0% and 5.3% higher than the state of the art, respectively. This model performs well without requiring hand-crafted features or intricate domain knowledge. The code and features have been made public.<br/> &copy; 2016 IEEE.},
key={Robotic surgery},
keywords={Automation;Convolution;Human robot interaction;Personnel training;Random processes;Robotics;Time series;},
note={Accelerometer data;Action recognition;Conditional random field;Recognition methods;Robotics applications;Skill assessment;Surgery training;Time series modeling;},
URL={http://dx.doi.org/10.1109/ICRA.2016.7487305},
}


@inproceedings{20152801030260,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Neural Conditional Energy Models for Multi-label Classification},
journal={Proceedings - IEEE International Conference on Data Mining, ICDM},
author={Jing, How and Lin, Shou-De},
volume={2015-January},
number={January},
year={2015},
pages={240 - 249},
issn={15504786},
address={Shenzhen, China},
abstract={Multi-label classification (MLC) is a type of structured output prediction problems where a given instance can be associated to more than one labels at a time. From the probabilistic point of view, a model predicts a set of labels y given an input vector v by learning a conditional distribution p(y|v). This paper presents a powerful model called a Neural Conditional Energy Model (NCEM) to solve MLC. The model can be viewed as a hybrid deterministic-stochastic network of which we use a deterministic neural network to transform the input data, before contributing to the energy landscape of v, y, and a single stochastic hidden layer h. Non-linear transformation given by the neural network makes our model more expressive and more capable of capturing complex relations between input and output, and using deterministic neurons facilitates exact inference. We present an efficient learning algorithm that is simple to implement. We conduct extensive experiments on 15 real-world datasets from wide variety of domains with various evaluation metrics to confirm that NCEM is significantly superior to current state-of-the-art models most of the time based on pair-wise t-test at 5% significance level. The MATLAB source code to replicate our experiments are available at https://github.com/Kublai-Jing/NCEM<br/> &copy; 2014 IEEE.},
key={Stochastic models},
keywords={Classification (of information);Data mining;Learning algorithms;Linear transformations;Mathematical transformations;Metadata;Probability distributions;Stochastic systems;},
note={Conditional distribution;Hybrid deterministic;Multi label classification;Non-linear transformations;Probabilistic modeling;Real-world datasets;Stochastic networks;Structured output prediction;},
URL={http://dx.doi.org/10.1109/ICDM.2014.39},
}


@article{20185006242085,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Customizable Computing-- From Single Chip to Datacenters},
journal={Proceedings of the IEEE},
author={Cong, Jason and Fang, Zhenman and Huang, Muhuan and Wei, Peng and Wu, Di and Yu, Cody Hao},
year={2018},
issn={00189219},
abstract={Since its establishment in 2009, the Center for Domain-Specific Computing (CDSC) has focused on customizable computing. We believe that future computing systems will be customizable with extensive use of accelerators, as custom-designed accelerators often provide 10-100X performance/energy efficiency over the general-purpose processors. Such an accelerator-rich architecture presents a fundamental departure from the classical von Neumann architecture, which emphasizes efficient sharing of the executions of different instructions on a common pipeline, providing an elegant solution when the computing resource is scarce. In contrast, the accelerator-rich architecture features heterogeneity and customization for energy efficiency; this is better suited for energy-constrained designs where the silicon resource is abundant and spatial computing is favored--which has been the case with the end of Dennard scaling. Currently, customizable computing has garnered great interest; for example, this is evident by Intel's $17 billion acquisition of Altera in 2015 and Amazon's introduction of field-programmable gate-arrays (FPGAs) in its AWS public cloud. In this paper, we present an overview of the research programs and accomplishments of CDSC on customizable computing, from single chip to server node and to datacenters, with extensive use of composable accelerators and FPGAs. We highlight our successes in several application domains, such as medical imaging, machine learning, and computational genomics. In addition to architecture innovations, an equally important research dimension enables automation for customized computing. This includes automated compilation for combining source-code-level transformation for high-level synthesis with efficient parameterized architecture template generations, and efficient runtime support for scheduling and transparent resource management for integration of FPGAs for datacenter-scale acceleration with support to the existing programming interfaces, such as MapReduce, Hadoop, and Spark, for large-scale distributed computation. We will present the latest progress in these areas, and also discuss the challenges and opportunities ahead.<br/> IEEE},
key={Computer architecture},
keywords={Acceleration;Energy efficiency;Field programmable gate arrays (FPGA);General purpose computers;Green computing;High level synthesis;Learning systems;Medical imaging;Pipeline processing systems;Scheduling;},
note={Customizable;Distributed computations;Field programmable gate array (FPGAs);General purpose processors;Neumann architecture;Programming interface;Resource management;Template generation;},
URL={http://dx.doi.org/10.1109/JPROC.2018.2876372},
}


@article{20185006230045,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The MESSAGEix Integrated Assessment Model and the ix modeling platform (ixmp): An open framework for integrated and cross-cutting analysis of energy, climate, the environment, and sustainable development},
journal={Environmental Modelling and Software},
author={Huppmann, Daniel and Gidden, Matthew and Fricko, Oliver and Kolp, Peter and Orthofer, Clara and Pimmer, Michael and Kushin, Nikolay and Vinca, Adriano and Mastrucci, Alessio and Riahi, Keywan and Krey, Volker},
year={2019},
pages={143 - 156},
issn={13648152},
abstract={The MESSAGE Integrated Assessment Model (IAM) developed by IIASA has been a central tool of energy-environment-economy systems analysis in the global scientific and policy arena. It played a major role in the Assessment Reports of the Intergovernmental Panel on Climate Change (IPCC); it provided marker scenarios of the Representative Concentration Pathways (RCPs) and the Shared Socio-Economic Pathways (SSPs); and it underpinned the analysis of the Global Energy Assessment (GEA). Alas, to provide relevant analysis for current and future challenges, numerical models of human and earth systems need to support higher spatial and temporal resolution, facilitate integration of data sources and methodologies across disciplines, and become open and transparent regarding the underlying data, methods, and the scientific workflow. In this manuscript, we present the building blocks of a new framework for an integrated assessment modeling platform; the &ldquo;ecosystem&rdquo; comprises: i) an open-source GAMS implementation of the MESSAGE energy++ system model integrated with the MACRO economic model; ii) a Java/database back-end for version-controlled data management, iii) interfaces for the scientific programming languages Python &amp; R for efficient input data and results processing workflows; and iv) a web-browser-based user interface for model/scenario management and intuitive &ldquo;drag-and-drop&rdquo; visualization of results. The framework aims to facilitate the highest level of openness for scientific analysis, bridging the need for transparency with efficient data processing and powerful numerical solvers. The platform is geared towards easy integration of data sources and models across disciplines, spatial scales and temporal disaggregation levels. All tools apply best-practice in collaborative software development, and comprehensive documentation of all building blocks and scripts is generated directly from the GAMS equations and the Java/Python/R source code.<br/> &copy; 2018 Elsevier Ltd},
key={Information management},
keywords={Climate change;Climate models;Data integration;Data visualization;Data warehouses;Economics;Energy management systems;Groupware;Java programming language;Modeling languages;Numerical methods;Open source software;Open systems;Software design;Sustainable development;Systems analysis;User interfaces;},
note={Collaborative software development;Energy system optimizations;Integrated assessment;Integrated assessment models;Intergovernmental panel on climate changes;Open sources;Scenario management;Spatial and temporal resolutions;},
URL={http://dx.doi.org/10.1016/j.envsoft.2018.11.012},
}


@inproceedings{20185006250332,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={How do developers resolve merge conflicts? An investigation into the processes, tools, and improvements},
journal={ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
author={Brindescu, Caius},
year={2018},
pages={952 - 955},
address={Lake Buena Vista, FL, United states},
abstract={Most software development is done in teams. When more than one developer is modifying the source code, there is a change that their changes will conflict. When this happens, developers have to interrupt their workflow in order to resolve the merge conflict. This interruption can lead to frustration and lost productivity. This makes collaboration, and the problems associated with it, an important aspect of software development. Merge conflicts are some of the more difficult issues that arise when working in a team. We plan to bring in more information about the strategies developers use when resolving merge conflicts. We will gather information through in-situ observations and interviews of developers resolving conflicts when working on real development tasks, combined with analytical methods. The information obtained can then be used to improve the existing tools and make it easier for developers when working in a collaborative environment.<br/> &copy; 2018 Association for Computing Machinery.},
key={Software design},
keywords={Mergers and acquisitions;},
note={Analytical method;Collaborative environments;Development tasks;In-situ observations;Information foraging;Lost productivities;Source codes;Version control system;},
URL={http://dx.doi.org/10.1145/3236024.3275430},
}


@article{20152000846689,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Numerical simulation of single bubble condensation in subcooled flow using OpenFOAM},
journal={Progress in Nuclear Energy},
author={Zeng, Qingyun and Cai, Jiejin and Yin, Huaqiang and Yang, Xingtuan and Watanabe, Tadashi},
volume={83},
year={2015},
pages={336 - 346},
issn={01491970},
abstract={Abstract The single condensing bubble behavior in subcooled flow has been numerical investigated using the open source code OpenFOAM. A coupled Level Set (LS) and Volume of Fluid (VOF) method (CLSVOF) model with a phase change model for condensation was developed and implemented in the code. The simulated results were firstly compared with the experimental results, they were in great agreements, and thus the simulation model was validated. The validated numerical model was then used to analyze the condensing bubble deformation, bubble lifetime, bubble size history, condensate Nusselt number and other interesting parameters with different variables in subcooled flow. The numerical results indicated that the initial bubble size, subcooling of liquid and system pressure play an important role to influence the condensing bubble behaviors significantly and bubble will be pierced when the subcooling and initial diameter reach a certain value at the later condensing stage. The bubble diameter history and condensate Nusselt number were found in good agreement with the empirical correlation. The drag force coefficient was predicted well by introducing a reduced drag coefficient.<br/> &copy; 2015 Elsevier Ltd.},
key={Numerical methods},
keywords={Condensation;Drag;Numerical models;Nusselt number;Open source software;Open systems;},
note={Bubble behavior;Bubble deformation;CLSVOF method;Drag force coefficients;Empirical correlations;OpenFOAM;Phase change model;Volume of fluid method;},
URL={http://dx.doi.org/10.1016/j.pnucene.2015.04.011},
}


@inproceedings{20184806150878,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A case study of the effects of architecture debt on software evolution effort},
journal={Proceedings - 44th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2018},
author={Snipes, Will and Karlekar, Sunil L. and Mo, Ran},
year={2018},
pages={400 - 403},
address={Prague, Czech republic},
abstract={In large-scale software systems, the majority of defective files are architecturally connected, and the architecture connections usually exhibit design flaws, which are associated with higher change-proneness among files and higher maintenance costs. As software evolves with bug fixes, new features, or improvements, unresolved architecture design flaws can contribute to maintenance difficulties. The impact on effort due to architecture design flaws has been difficult to quantify and justify. In this paper, we conducted a case study where we identified flawed architecture relations and quantified their effects on maintenance activities. Using data from this project's source code and revision history, we identified file groups where files are architecturally connected and participated in flawed architecture designs, quantified the maintenance activities in the detected files, and assessed the penalty related to these files.<br/> &copy; 2018 IEEE.},
key={Computer software maintenance},
keywords={Application programs;Software architecture;},
note={Architecture designs;Change proneness;Exhibit design;Large-scale software systems;Maintenance activity;Maintenance cost;Software Evolution;Technical debts;},
URL={http://dx.doi.org/10.1109/SEAA.2018.00071},
}


@inproceedings{20170803379105,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Switching to Git: The Good, the Bad, and the Ugly},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
author={Just, Sascha and Herzig, Kim and Czerwonka, Jacek and Murphy, Brendan},
volume={0},
year={2016},
pages={400 - 411},
issn={10719458},
address={Ottawa, ON, United states},
abstract={Since its introduction 10 years ago, GIT has taken the world of version control systems (VCS) by storm. Its success is partly due to creating opportunities for new usage patterns that empower developers to work more efficiently. However, the resulting change in both user behavior and the way GIT stores changes impacts data mining and data analytics procedures [6], [13]. While some of these unique characteristics can be managed by adjusting mining and analytical techniques, others can lead to severe data loss and the inability to audit code changes, e.g. knowing the full history of changes of code related to security and privacy functionality. Thus, switching to GIT comes with challenges to established development process analytics. This paper is based on our experience in attempting to provide continuous process analysis for Microsoft product teams who switching to GIT as their primary VCS. We illustrate how GIT's concepts and usage patterns create a need for changing well-established data analytic processes. The goal of this paper is to raise awareness how certain GIT operations may damage or even destroy information about historical code changes necessary for continuous data development process analytics. To that end, we provide a list of common GIT usage patterns with a description of how these operations impact data mining applications. Finally, we provide examples of how one may counteract the effects of such destructive operations in the future. We further provide a new algorithm to detect integration paths that is specific to distributed version control systems like GIT, which allows us to reconstruct the information that is crucial to most development process analytics.<br/> &copy; 2016 IEEE.},
key={Process control},
keywords={Behavioral research;Codes (symbols);Control system analysis;Data mining;Information management;Software reliability;},
note={Continuous process;Destructive operations;Development process;Distributed version control systems;Mining software repositories;Process analysis;Security and privacy;Version control system;},
URL={http://dx.doi.org/10.1109/ISSRE.2016.38},
}


@inproceedings{20182305286686,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Model based rapid prototyping and evolution of web application},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Falzone, Emanuele and Bernaschina, Carlo},
volume={10845 LNCS},
year={2018},
pages={496 - 500},
issn={03029743},
address={Caceres, Spain},
abstract={We demonstrate a development work-flow for the co-evolution of model and code, based on IFMLEdit.org, an online tool for the rapid prototyping of web applications, and on common Version Control Systems. IFMLEdit.org exploits the Interaction Flow Modeling Language (IFML), an OMG standard for describing the user&rsquo;s interaction with the application by means of flows of information in reaction to user events. In the demo, attendees will be able to edit IFML specifications with IFMLEdit.org, generate the first version of the code of a web/mobile application from the model, improve the generated code with manually added details (e.g. styling), evolve the original IFML model introducing new requirements and re-generate the code of the updated version, in a way that fully preserves the manually coded details. The demonstrated approach solves the well-know problem of model driven forward engineering of breaking the automated development cycle when features that cannot be modelled are added manually to the generated code.<br/> &copy; Springer International Publishing AG, part of Springer Nature 2018.},
key={Software prototyping},
keywords={Codes (symbols);Modeling languages;Online systems;Rapid prototyping;},
note={Agile development;Code Generation;Development cycle;Forward engineerings;Model driven development;Model-based OPC;Version control system;WEB application;},
URL={http://dx.doi.org/10.1007/978-3-319-91662-0_43},
}


@article{20173904216699,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Satisfying rival forestry objectives in the komi republic: Effects of russian zoning policy change on wood production and riparian forest conservation},
journal={Canadian Journal of Forest Research},
author={Naumov, Vladimir and Angelstam, Per and Elbakidze, Marine},
volume={47},
number={10},
year={2017},
pages={1339 - 1349},
issn={00455067},
abstract={Spatial segregation of different forest landscape functions can accommodate rival forestry objectives more comprehensively than integrated approaches. Russia has a unique history of forest zoning separating production and environmental functions. However, the Russian Forest Code of 2006 increased the focus on wood production. We reviewed the history of zoning policy in Russia and assessed if the recent policy change affected logging rates and conservation of riparian forests. Using Russia&rsquo;s Komi Republic as a case study, we specifically assessed (i) if policy change led to increased logging near streams, (ii) if logging rates were different in headwaters vs. main rivers, and (iii) how logging changed among catchments with different accessibility to logging. Using a global open-access remote sensing dataset, we compared mean annual forest loss as a proxy of logging rates in 10 large forested catchments in the Komi Republic in one period with strict zoning policy (2000&ndash;2006) and one with moderate zoning policy (2007&ndash;2014). Harvesting rate was positively related to the distance from streams. On the other hand, it increased after the policy change in the buffer zone but decreased outside it. Forests were harvested more in headwater buffers than along larger rivers, and harvest in the catchments near industries was higher and increasing; remote catchments had low forest loss. We discuss the opportunity for adopting forest zoning policy in different governance contexts.<br/> &copy; 2017, Canadian Science Publishing. All rights reserved.},
key={Forestry},
keywords={Catchments;Conservation;Harvesting;Remote sensing;Runoff;Timber;Wood products;Zoning;},
note={Environmental functions;Forest policy;Forested catchments;Integrated approach;Komi Republic;Spatial planning;Spatial segregation;Sustainable forest management;},
URL={http://dx.doi.org/10.1139/cjfr-2016-0516},
}


@inproceedings{20161402203423,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An implementation of just-in-time fault-prone prediction technique using text classifier},
journal={Proceedings - International Computer Software and Applications Conference},
author={Mori, Keita and Mizuno, Osamu},
volume={3},
year={2015},
pages={609 - 612},
issn={07303157},
address={Taichung, Taiwan},
abstract={Since the fault prediction is an important technique to help allocating software maintenance effort, much research on fault prediction has been proposed so far. The goal of these studies is applying their prediction technique to actual software development. In this paper, we implemented a prototype fault-prone module prediction tool using a text-filtering based technique named 'Fault-Prone Filtering'. Our tool aims to show the result of fault prediction for each change (i.e., Commits) as a probability that a source code file to be faulty. The result is shown on a Web page and easy to track the histories of prediction. A case study performed on three open source projects shows that our tool could detect 90 percent of the actual fault modules (i.e., The recall of 0.9) with the accuracy of 0.67 and the precision of 0.63 on average.<br/> &copy; 2015 IEEE.},
key={Software design},
keywords={Application programs;Classification (of information);Computer software maintenance;Filtration;Forecasting;Learning systems;Open source software;Websites;},
note={Fault prediction;Fault-prone modules;Mining software repositories;Open source projects;Prediction techniques;Prediction tools;Software development support tools;SPAM filter;},
URL={http://dx.doi.org/10.1109/COMPSAC.2015.143},
}


@inproceedings{20151400714223,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Proposition of a three layers architecture for adaptable artificial intelligence},
journal={2014 Joint 7th International Conference on Soft Computing and Intelligent Systems, SCIS 2014 and 15th International Symposium on Advanced Intelligent Systems, ISIS 2014},
author={Benoit, Vallade and Alexandre, David and Nakashima, Tomoharu},
year={2014},
pages={247 - 252},
address={Kitakyushu, Japan},
abstract={This paper proposes a concept of artificial intelligence architecture which would be usable in various kind of problems. Artificial intelligences are used in many areas of computer science for decision making tasks. Traditionally each artificial intelligence is designed and programmed to be used within a particular software and for a specific purpose. However, this paper stands as the first step of research in progress whose final objective is to create an artificial intelligence adaptable to all kinds of problems without any change in its source code. The present proposition focuses on the architecture of that artificial intelligence and is introduced in the context of video games. This architecture, composed of three layers, would be re-usable for all types of game.<br/> &copy; 2014 IEEE.},
key={Human computer interaction},
keywords={Artificial intelligence;Decision making;Intelligent systems;Learning algorithms;Soft computing;Trees (mathematics);},
note={Monte-Carlo tree searches;Particle swarm algorithm;Salesman problem;Search Algorithms;Video game;},
URL={http://dx.doi.org/10.1109/SCIS-ISIS.2014.7044777},
}


@inproceedings{20164903095608,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An automation framework for configuration management to reduce manual intervention},
journal={ACM International Conference Proceeding Series},
author={Karale, Supriya V. and Kaushal, Vishal},
volume={12-13-August-2016},
year={2016},
address={Bikaner, India},
abstract={To support the increasing expectations of clients in getting quick bug resolutions and feature enhancements, many IT industries today use agile software development methodology. Under this methodology changes are made in the software on daily basis. In order to cope-up with these changes, Software Configuration Management (SCM) plays an important role. Configuration manager is responsible for maintaining and monitoring changes which are made in software over a period of time. Version control tools are used by the configuration manager to maintain the software. It allows developers to keep source code in repository and take copy from repository, whenever needed. This makes it necessary to integrate complete code in repository, compile it and create deliverable package. Configuration manager performs these activities, such as analyzing logs of daily compilation and package creation software that is to be delivered. In current scenario these tasks are performed manually. This paper proposes a framework to reduce manual intervention and automate above mentioned tasks of the configuration manager.<br/> &copy; 2016 ACM.},
key={Software design},
keywords={Managers;},
note={Agile Methodologies;Agile software development;Configuration management;Feature enhancement;Manual intervention;Monitoring change;Release management;Software configuration management;},
URL={http://dx.doi.org/10.1145/2979779.2979853},
}


@article{20183005602080,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={GreenScaler: training software energy models with automatic test generation},
journal={Empirical Software Engineering},
author={Chowdhury, Shaiful and Borle, Stephanie and Romansky, Stephen and Hindle, Abram},
year={2018},
issn={13823256},
abstract={Software energy consumption is a performance related non-functional requirement that complicates building software on mobile devices today. Energy hogging applications (apps) are a liability to both the end-user and software developer. Measuring software energy consumption is non-trivial, requiring both equipment and expertise, yet researchers have found that software energy consumption can be modelled. Prior works have hinted that with more energy measurement data we can make more accurate energy models. This data, however, was expensive to extract because it required energy measurement of running test cases (rare) or time consuming manually written tests. In this paper, we show that automatic random test generation with resource-utilization heuristics can be used successfully to build accurate software energy consumption models. Code coverage, although well-known as a heuristic for generating and selecting tests in traditional software testing, performs poorly at selecting energy hungry tests. We propose an accurate software energy model, GreenScaler, that is built on random tests with CPU-utilization as the test selection heuristic. GreenScaler not only accurately estimates energy consumption for randomly generated tests, but also for meaningful developer written tests. Also, the produced models are very accurate in detecting energy regressions between versions of the same app. This is directly helpful for the app developers who want to know if a change in the source code, for example, is harmful for the total energy consumption. We also show that developers can use GreenScaler to select the most energy efficient API when multiple APIs are available for solving the same problem. Researchers can also use our test generation methodology to further study how to build more accurate software energy models.<br/> &copy; 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
key={Software testing},
keywords={Application programs;Computer software selection and evaluation;Energy efficiency;Energy utilization;Learning systems;Mobile devices;},
note={Energy model;Energy optimization;Mining software repositories;Software energy consumption;Test generations;},
URL={http://dx.doi.org/10.1007/s10664-018-9640-7},
}


@inproceedings{20164803071665,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automatic performance testing using input-sensitive profiling},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
author={Luo, Qi},
volume={13-18-November-2016},
year={2016},
pages={1139 - 1141},
address={Seattle, WA, United states},
abstract={During performance testing, software engineers commonly perform application profiling to analyze an application's traces with different inputs to understand performance behaviors, such as time and space consumption. However, a non-Trivial application commonly has a large number of inputs, and it is mostly manual to identify the specific inputs leading to performance bottlenecks. Thus, it is challenge is to automate profiling and find these specific inputs. To solve these problems, we propose novel approaches, FOREPOST, GA-Prof and PerfImpact, which automatically profile applications for finding the specific combinations of inputs triggering performance bottlenecks, and further analyze the corresponding traces to identify problematic methods. Specially, our approaches work in two different types of real-world scenarios of performance testing: i) a single-version scenario, in which performance bottlenecks are detected in a single software release, and ii) a two-version scenario, in which code changes responsible for performance regressions are detected by considering two consecutive software releases.<br/> &copy; 2016 ACM.},
key={Software testing},
keywords={Application programs;Genetic algorithms;Learning algorithms;Learning systems;Object oriented programming;},
note={Application profiling;Change impact analysis;Input-sensitive profiling;Performance bottlenecks;Performance testing;Real-world scenario;Software release;Space consumption;},
URL={http://dx.doi.org/10.1145/2950290.2983975},
}


@inproceedings{20184406001098,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Towards a framework for stochastic performance optimizations in compilers and interpreters - An architecture overview},
journal={ACM International Conference Proceeding Series},
author={Krauss, Oliver},
year={2018},
pages={JVU Department of Computer Science; Labs; Linz AG; Oracle - },
address={Linz, Austria},
abstract={Modern compilers and interpreters provide code optimizations before and during run-time to stay competitive with alternative execution environments, thus moving required domain knowledge about the compilation process away from the developer and speeding up resulting software. These optimizations are often based on formal proof, or alternatively have recovery paths as backup. This publication proposes an architecture utilizing abstract syntax trees (ASTs) to optimize the runtime performance of code with stochastic - search based - machine learning techniques. From these AST modifying optimizations a pattern mining approach attempts to find transformation patterns which are applicable to a software language. The application of these patterns happens during the parsing process or the programs run-time. Future work consists of implementing and extending the presented architecture, with a considerable focus on the mining of transformation patterns.<br/> Copyright &copy; 2018 held by the owner/author(s). Publication rights licensed to ACM.},
key={Stochastic systems},
keywords={Application programs;Data mining;Learning systems;Optimization;Program compilers;Syntactics;Trees (mathematics);},
note={Abstract Syntax Trees;AST Transformation;Execution environments;Machine learning techniques;Pattern mining;Performance optimizations;Stochastic optimizations;Transformation patterns;},
URL={http://dx.doi.org/10.1145/3237009.3237024},
}


@article{20183305697549,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Foreground segmentation using convolutional neural networks for multiscale feature encoding},
journal={Pattern Recognition Letters},
author={Lim, Long Ang and Yalim Keles, Hacer},
volume={112},
year={2018},
pages={256 - 262},
issn={01678655},
abstract={Several methods have been proposed to solve moving objects segmentation problem accurately in different scenes. However, many of them lack the ability of handling various difficult scenarios such as illumination changes, background or camera motion, camouflage effect, shadow etc. To address these issues, we propose two robust encoder-decoder type neural networks that generate multi-scale feature encodings in different ways and can be trained end-to-end using only a few training samples. Using the same encoder-decoder configurations, in the first model, a triplet of encoders take the inputs in three scales to embed an image in a multi-scale feature space; in the second model, a Feature Pooling Module (FPM) is plugged on top of a single input encoder to extract multi-scale features in the middle layers. Both models use a transposed convolutional network in the decoder part to learn a mapping from feature space to image space. In order to evaluate our models, we entered the Change Detection 2014 Challenge (changedetection.net) and our models, namely FgSegNet_M and FgSegNet_S, outperformed all the existing state-of-the-art methods by an average F-Measure of 0.9770 and 0.9804, respectively. We also evaluate our models on SBI2015 and UCSD Background Subtraction datasets. Our source code is made publicly available at https://github.com/lim-anggun/FgSegNet.<br/> &copy; 2018 Elsevier B.V.},
key={Convolution},
keywords={Decoding;Deep learning;Encoding (symbols);Image recognition;Network coding;Neural networks;Security systems;},
note={Background subtraction;Convolutional neural network;Foreground segmentation;Pixel classification;Video surveillance;},
URL={http://dx.doi.org/10.1016/j.patrec.2018.08.002},
}


@article{20141217481125,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A new analytical solution for assessing climate change impacts on subsurface temperature},
journal={Hydrological Processes},
author={Kurylyk, Barret L. and Macquarrie, Kerry T.B.},
volume={28},
number={7},
year={2014},
pages={3161 - 3172},
issn={08856087},
abstract={Groundwater temperature is an important water quality parameter that affects species distributions in subsurface and surface environments. To investigate the response of subsurface temperature to atmospheric climate change, an analytical solution is derived for a one-dimensional, transient conduction-advection equation and verified with numerical methods using the finite element code SUTRA. The solution can be directly applied to forward model the impact of future climate change on subsurface temperature profiles or inversely applied to produce a surface temperature history from measured borehole profiles. The initial conditions are represented using superimposed linear and exponential functions, and the boundary condition is expressed as an exponential function. This solution expands on a classic solution in which the initial and boundary conditions were restricted to linear functions. The exponential functions allow more flexibility in matching climate model projections (boundary conditions) and measured temperature-depth profiles (initial conditions). For example, measured borehole temperature data from the Sendai Plain and Tokyo, Japan, were used to demonstrate the improved accuracy of the exponential function for replicating temperature-depth profiles. Also, the improved accuracy of the exponential boundary condition was demonstrated using air temperature anomaly data from the Intergovernmental Panel on Climate Change. These air temperature anomalies were then used to forward model the effect of surficial thermal perturbations in subsurface environments with significant groundwater flow. The simulation results indicate that recharge can accelerate shallow subsurface warming, whereas upward groundwater discharge can enhance deeper subsurface warming. Additionally, the simulation results demonstrate that future groundwater temperatures obtained from the proposed analytical solution can deviate significantly from those produced with the classic solution. &copy; 2013 John Wiley &amp; Sons, Ltd.<br/>},
key={Climate change},
keywords={Atmospheric temperature;Boundary conditions;Climate models;Exponential functions;Groundwater;Groundwater flow;Numerical methods;Water quality;},
note={Exponential boundary condition;Groundwater temperatures;Initial and boundary conditions;Intergovernmental panel on climate changes;Soil temperature;Subsurface warming;Temperature depth profiles;Thermal regimes;},
URL={http://dx.doi.org/10.1002/hyp.9861},
}


@article{20132516434441,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An evolutionary framework for cultural change: Selectionism versus communal exchange},
journal={Physics of Life Reviews},
author={Gabora, Liane},
volume={10},
number={2},
year={2013},
pages={117 - 145},
issn={15710645},
abstract={Dawkins' replicator-based conception of evolution has led to widespread mis-application of selectionism across the social sciences because it does not address the paradox that necessitated the theory of natural selection in the first place: how do organisms accumulate change when traits acquired over their lifetime are obliterated? This is addressed by von Neumann's concept of a self-replicating automaton (SRA). A SRA consists of a self-assembly code that is used in two distinct ways: (1) actively deciphered during development to construct a self-similar replicant, and (2) passively copied to the replicant to ensure that it can reproduce. Information that is acquired over a lifetime is not transmitted to offspring, whereas information that is inherited during copying is transmitted. In cultural evolution there is no mechanism for discarding acquired change. Acquired change can accumulate orders of magnitude faster than, and quickly overwhelm, inherited change due to differential replication of variants in response to selection. This prohibits a selectionist but not an evolutionary framework for culture and the creative processes that fuel it. The importance non-Darwinian processes in biological evolution is increasingly recognized. Recent work on the origin of life suggests that early life evolved through a non-Darwinian process referred to as communal exchange that does not involve a self-assembly code, and that natural selection emerged from this more haphazard, ancestral evolutionary process. It is proposed that communal exchange provides an evolutionary framework for culture that enables specification of cognitive features necessary for a (real or artificial) societies to evolve culture. This is supported by a computational model of cultural evolution and a conceptual network based program for documenting material cultural history, and it is consistent with high levels of human cooperation. &copy; 2013 Elsevier B.V.<br/>},
key={History},
keywords={Biology;Computation theory;Self assembly;},
note={Creativity;Cultural evolution;Meme;Natural selection;Replicator;Self-replicating automaton;},
URL={http://dx.doi.org/10.1016/j.plrev.2013.03.006},
}


@inproceedings{20183605768206,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Optimization and parallelization of the thermal-hydraulic subchannel code Cobra-En using parallel infrastructure jasmin},
journal={17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics, NURETH 2017},
author={Li, Kang and Liu, Na and Liu, Peng and Shi, Dunfu and Tian, Baolin},
volume={2017-September},
year={2017},
pages={Atomic Energy Society of Japan; Canadian Nuclear Society; JSME; Sociedad Nuclear Espanola (SNE); Sociedad Nuclear Mexicana (SNM) - },
address={Xi'an, Shaanxi, China},
abstract={COBRA-EN is a subchannel code for thermal-hydraulic transient and steady problems, selected to be the reactor core thermal hydraulic (TH) simulation tool. Upgraded by Basile et al., COBRA-EN (CEN) is able to solve three-equation mixture model and four-equation model using Pressure-Gradient iteration scheme or Newton-Raphson iteration scheme. However, in the successive versions of COBRA computer programs, the current serial version COBRA-EN can only simulate small pincell TH model due to the limitation of memory and efficient problems. In the present work, the Parallel version of COBRA-EN (PCEN) is developed based on JASMIN (J parallel Adaptive Structured Mesh applications INfrastructure). There are mainly four steps to achieve the parallelization of CEN. The first step is to standardize CEN, such as deleting the &lsquo;common&rsquo; module and &lsquo;implicit none&rsquo; usage. The preprocessing of the input cards is also abandoned and all the input parameters are transferred from JASMIN to the CEN subprogram. The initializing methods of flow field variables are changed to adapt the executing routines of JASMIN, too. Secondly, the memory is allocated explicitly. The original method using a big one-dimensional array to manage all variables is showed to be not suitable for parallel program. Thirdly, it is important to establish transformation subprograms from CEN&rsquo;s approximate unstructured mesh to JASMIN&rsquo;s structured mesh system and after the computing is accomplished, the results should be transformed in reverse for the next time step. The last step is applying the linear solver of JASMIN to the parallel version TH code to solve energy, moment and pressure-gradient or pressure-difference matrix for different numerical methods. To validate the Standardized version of serial COBRA-EN (SCEN), we compare the results of original CEN and SCEN using 45 rods model. The results show a little difference for flow variables such as coolant temperature and pressure drop distributions and less for fuel rod temperature distributions. The comparisons between SCEN and PCEN are currently made using pincell-resolved 9&times;9 and 4 assemblies model to validate data transferring and boundary conditions. It shows acceptable difference for coolant density, temperature and pressure drop distributions along the axial direction.<br/> &copy; 2016 Association for Computing Machinery Inc. All Rights Reserved.},
key={Two phase flow},
keywords={Codes (symbols);Coolants;Drops;Hydraulics;Iterative methods;Mesh generation;Nuclear reactors;Numerical methods;One dimensional;Pressure drop;Pressure gradient;},
note={COBRA-EN;JASMIN;Parallelizations;Pin-by-pin;Subchannels;},
}


@article{20150300433764,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The unified integral transforms (Unit) algorithm with total and partial transformation},
journal={Computational Thermal Sciences},
author={Cotta, Renato M. and Knupp, Diego C. and Naveira-Cotta, Carolina P. and Sphaier, Leandro A. and Quaresma, Joaon},
volume={6},
number={6},
year={2014},
pages={507 - 524},
issn={19402503},
abstract={The theory and algorithm behind the open-source mixed symbolic-numerical computational code named UNIT (unified integral transforms) are described. The UNIT code provides a computational environment for finding solutions of linear and nonlinear partial differential systems via integral transforms. The algorithm is based on the well-established analytical-numerical methodology known as the generalized integral transform technique (GITT), together with the mixed symbolic-numerical computational environment provided by the Mathematica system (version 7.0 and up). This paper is aimed at presenting a partial transformation scheme option in the solution of transient convective-diffusive problems, which allows the user to choose a space variable not to be integral transformed. This approach is shown to be useful in situations when one chooses to perform the integral transformation on those coordinates with predominant diffusion effects only, whereas the direction with predominant convection effects is handled numerically, together with the time variable, in the resulting transformed system of one-dimensional partial differential equations. Test cases are selected based on the nonlinear three-dimensional Burgers&rsquo; equation, with the establishment of reference results for specific numerical values of the governing parameters. Then the algorithm is illustrated in the solution of conjugated heat transfer in microchannels.<br/> &copy; 2014 by Begell House, Inc.},
key={Mathematical transformations},
keywords={Computation theory;Heat transfer;Integrodifferential equations;Microchannels;Nonlinear equations;One dimensional;Open systems;},
note={Conjugated heat transfer;Hybrid method;Integral transform;Microchannel flow;Symbolic computation;},
URL={http://dx.doi.org/10.1615/ComputThermalScien.2014008663},
}


@article{20184305983117,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={PRISM revisited: Declarative implementation of a probabilistic programming language using multi-prompt delimited control},
journal={International Journal of Approximate Reasoning},
author={Abdallah, Samer},
volume={103},
year={2018},
pages={364 - 382},
issn={0888613X},
abstract={PRISM is a probabilistic programming language based on Prolog, augmented with primitives to represent probabilistic choice. It is implemented using a combination of low level support from a modified version of B-Prolog, source level program transformation, and libraries for inference and learning implemented in C. More recently, developers working with functional programming languages have taken the approach of embedding probabilistic primitives into an existing language, with little or no modification to the host language, often by using delimited continuations. Captured continuations represent pieces of the probabilistic program which can be manipulated to achieve a great variety of computational effects useful for inference. In this paper, I will describe an approach based on delimited control operators recently introduced into SWI Prolog. These are used to create a system of nested effect handlers which together implement a core functionality of PRISM&mdash;the building of explanation graphs&mdash;entirely in Prolog and using an order of magnitude less code. Other declarative programming tools, such as constraint logic programming, are used to implement tools for inference, such as the inside-outside and EM algorithms, lazy best-first explanation search, and MCMC samplers. By embedding the functionality of PRISM into SWI Prolog, users gain access to its rich libraries and development environment. By expressing the functionality of PRISM in a small amount of pure, high-level Prolog, this implementation facilitates further experimentation with the mechanisms of probabilistic logic programming, including new probabilistic modelling features and inference algorithms, such as variational inference in models with real-valued variables.<br/> &copy; 2018 Elsevier Inc.},
key={PROLOG (programming language)},
keywords={Computer circuits;Functional programming;Inference engines;Learning systems;Libraries;Logic programming;Object oriented programming;Prisms;Probabilistic logics;},
note={Algebraic effects;Constraint Logic Programming;Declarative Programming;Delimited continuations;Probabilistic modelling;Probabilistic programming;Probabilistic programming language;Program transformations;},
URL={http://dx.doi.org/10.1016/j.ijar.2018.10.012},
}


@inproceedings{20144800259809,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Development of a discrete adjoint CFD code using algorithmic differentiation by operator overloading},
journal={OPT-i 2014 - 1st International Conference on Engineering and Applied Sciences Optimization, Proceedings},
author={Dastouri, Z. and Lotz, J. and Naumann, U.},
year={2014},
pages={2862 - 2876},
address={Kos Island, Greece},
abstract={Design optimization for fluid flow recently has drawn a great deal of attention in the industrial design area including aeronautic[1, 2], turbo-machinery[3] and automotive design[4, 5]. The optimization problem is subject to a large number of design variables that makes calculating the sensitivity derivatives by finite differences a costly procedure. Algorith- mic differentiation(AD) is a well known method to differentiate the computer program with the aim of obtaining the sensitivity of the objective with respect to design variables. There are two different AD approaches, operator overloading and source transformation. In this paper we describe a design framework for application of the algorithmic differentiation tool by operator overloading in Fortran dco/fortran<sup>1</sup>to CFD analysis solver called GPDE<sup>2</sup>. GPDE is an un- structured pressure-based steady Navier-Stokes code with finite volume spatial discretization, which is based on the SIMPLE pressure-correction scheme for the incompressible viscous flow computation. This approach yields a discrete tangent-linear and adjoint version of the CFD code. Moreover, we address typical implementation issues and complexities in the differentia- tion procedure by AD tool in the CFD code. The numerical results of a relevant test case by our overloading tool in forward and reverse mode are compared with their corresponding results of the previously differentiated code by source transformation that is generated by TAPENADE. We show that in terms of ease of implementation and ability to handle arbitrary functions, dco/- fortran provides the differentiated code with a greater flexibility and robustness in comparison with AD by source transformation. This research is aimed toward the application of AD tools by operator overloading on a legacy industrial incompressible flow solver that is in our context ESI? ACE+.<br/>},
key={Computational fluid dynamics},
keywords={Codes (symbols);FORTRAN (programming language);Incompressible flow;Industrial research;Machine design;Machinery;Navier Stokes equations;Product design;Sensitivity analysis;},
note={Algorithmic differentiations;Computational Fluid Dynamics codes;Dco/fortran;Discrete adjoint;Operator overloading;},
}


@inproceedings{20173404067877,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Parallelization of digital wavelet transformation of ECG signals},
journal={2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2017 - Proceedings},
author={Domazet, Ervin and Gusev, Marjan},
year={2017},
pages={318 - 323},
address={Opatija, Croatia},
abstract={The advances in electronics and ICT industry for biomedical use have initiated a lot of new possibilities. However, these IoT solutions face the big data challenge where data comes with a certain velocity and huge quantities. In this paper, we analyze a situation where wearable ECG sensors stream continuous data to the servers. A server needs to receive these streams from a lot of sensors and needs to star various digital signal processing techniques initiating huge processing demands. Our focus in this paper is on optimizing the sequential Wavelet Transform filter. Due to the highly dependent structure of the transformation procedure we propose several optimization techniques for efficient parallelization. We set a hypothesis that optimizing the DWT initialization and processing part can yield a faster code. In this paper, we have provided several experiments to test the validity of this hypothesis by using OpenMP for parallelization. Our analysis shows that proposed techniques can optimize the sequential version of the code.<br/> &copy; 2017 Croatian Society MIPRO.},
key={Big data},
keywords={Application programming interfaces (API);Digital signal processing;Electrocardiography;Microelectronics;Wavelet transforms;Wearable sensors;},
note={Continuous data;Digital signal processing techniques;Digital wavelets;Heart signal;OpenMP;Optimization techniques;Parallelizations;Wearable ecg sensors;},
URL={http://dx.doi.org/10.23919/MIPRO.2017.7973442},
}


@inproceedings{20134516941843,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={How much really changes? A case study of firefox version evolution using a clone detector},
journal={2013 7th International Workshop on Software Clones, IWSC 2013 - Proceedings},
author={Lavoie, Thierry and Merlo, Ettore},
year={2013},
pages={83 - 89},
address={San Francisco, CA, United states},
abstract={This paper focuses on the applicability of clone detectors for system evolution understanding. Specifically, it is a case study of Firefox for which the development release cycle changed from a slow release cycle to a fast release cycle two years ago. Since the transition of the release cycle, three times more versions of the software were deployed. To understand whether or not the changes between the newer versions are as significant as the changes in the older versions, we measured the similarity between consecutive versions.We analyzed 82MLOC of C/C++ code to compute the overall change distribution between all existing major versions of Firefox. The results indicate a significant decrease in the overall difference between many versions in the fast release cycle. We discuss the results and highlight how differently the versions have evolved in their respective release cycle. We also relate our results with other results assessing potential changes in the quality of Firefox. We conclude the paper by raising questions on the impact of a fast release cycle. &copy; 2013 IEEE.<br/>},
key={Cloning},
keywords={C++ (programming language);},
note={Clone detection;Firefox;Potential change;Release cycles;Slow release;Software Evolution;System evolution;},
URL={http://dx.doi.org/10.1109/IWSC.2013.6613048},
}


@inproceedings{20164302951100,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mobile multi-agent systems for the internet-of-things and clouds using the javascript agent machine platform and machine learning as a service},
journal={Proceedings - 2016 IEEE 4th International Conference on Future Internet of Things and Cloud, FiCloud 2016},
author={Bosse, Stefan},
year={2016},
pages={244 - 253},
address={Vienna, Austria},
abstract={The Internet-of-Things (IoT) gets real in today's life and is becoming part of pervasive and ubiquitous computing networks offering distributed and transparent services. A unified and common data processing and communication methodology is required to merge the IoT, sensor networks, and Cloud-based environments seamless, which can be fulfilled by the mobile agent-based computing paradigm, discussed in this work. Currently, portability, resource constraints, security, and scalability of Agent Processing Platforms (APP) are essential issues for the deployment of Multi-Agent Systems (MAS) in strong heterogeneous networks including the Internet, addressed in this work. To simplify the development and deployment of MAS it would be desirable to implement agents directly in JavaScript, which is a well known and public widespread used programming language, and JS VMs are available on all host platforms including WEB browsers. The novel proposed JS Agent Machine (JAM) is capable to execute AgentJS agents in a sandbox environment with full run-Time protection and Machine learning as a service. Agents can migrate between different JAM nodes seamless preserving their data and control state by using a on-The-fly code-To-Text transformation in an extended JSON+ format. A Distributed Organization System (DOS) layer provides JAM node connectivity and security in the Internet, completed by a Directory-Name Service offering an organizational graph structure. Agent authorization and platform security is ensured with capability-based access and different agent privilege levels.<br/> &copy; 2016 IEEE.},
key={Multi agent systems},
keywords={Agents;Cloud computing;Distributed computer systems;Embedded systems;Heterogeneous networks;High level languages;Internet of things;Learning systems;Metadata;Mobile agents;Network security;Sensor networks;Ubiquitous computing;Web browsers;},
note={Agent platform;Communication methodology;Distributed organizations;Internet of thing (IOT);Mobile multi-agent systems;Mobile-agent-based computing;Processing platform;Resource Constraint;},
URL={http://dx.doi.org/10.1109/FiCloud.2016.43},
}


@inproceedings{20142417804315,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Code-changeable encoded microparticles for multi-step bead-based assay},
journal={Proceedings of the 16th International Conference on Miniaturized Systems for Chemistry and Life Sciences, MicroTAS 2012},
author={Kwon, Taehong and Song, Younghoon and Lee, Daewon and Kim, Mira and Park, Tae-Joon and Kwon, Sunghoon},
year={2012},
pages={1858 - 1860},
address={Okinawa, Japan},
abstract={Recording the reaction history of microparticle-based combination assays is important. However, several existing encoding methods cannot change microparticle codes. In this paper, we present a new method of encoding microparticles that uses a photoluminescent material for multiple code writing. 2,2-Dimethoxy-2-phenylacetophenone (DMPA) is a commonly used photoinitiator for free-radical polymerization. DMPA exhibits photoluminescence when irradiated in the ultraviolet region. Photopolymerized microparticles that contain DMPA were generated and then graphically encoded by using the Optofluidic Maskless Lithography system [1]. Our encoding method has advantages such as high coding capacity and long-term durability aside from enabling repeated writing on microparticles. Our encoding method that uses the DMPA photoinitiator can be applicable to multi-step microparticle-based assays.<br/>},
key={Free radical polymerization},
keywords={Assays;Encoding (symbols);Free radicals;Lithography;Signal encoding;},
note={Bead-based;Code;Long term durability;Mask-less lithography;Micro particles;Photo-initiator;Photoluminescent materials;Ultraviolet region;},
}


@inproceedings{20172703879643,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Versioning for end-to-end machine learning pipelines},
journal={Proceedings of the 1st Workshop on Data Management for End-To-End Machine Learning, DEEM 2017 - In conjunction with the 2017 ACM SIGMOD/PODS Conference},
author={Van Der Weide, Tom and Papadopoulos, Dimitris and Smirnov, Oleg and Zielinski, Michal and Van Kasteren, Tim},
year={2017},
pages={ACM Special Interest Group on Management of Data (SIGMOD) - },
address={Chicago, IL, United states},
abstract={End-to-end machine learning pipelines that run in shared environments are challenging to implement. Production pipelines typically consist of multiple interdependent processing stages. Between stages, the intermediate results are persisted to reduce redundant computation and to improve robustness. Those results might come in the form of datasets for data processing pipelines or in the form of model coefficients in case of model training pipelines. Reusing persisted results improves efficiency but at the same time creates complicated dependencies. Every time one of the processing stages is changed, either due to code change or due to parameters change, it becomes difficult to fnd which datasets can be reused and which should be recomputed. In this paper we build upon previous work to produce derivations of datasets to ensure that multiple versions of a pipeline can run in parallel while minimizing the amount of redundant computations. Our extensions include partial derivations to simplify navigation and reuse, explicit support for schema changes of pipelines, and a central registry of running pipelines to coordinate upgrading pipelines between teams.<br/> &copy; 2017 Copyright held by the owner/author(s).},
key={Pipeline processing systems},
keywords={Artificial intelligence;Data handling;Information management;Learning systems;Pipelines;},
note={Data processing pipelines;Intermediate results;Model coefficient;Model training;Processing stage;Production pipelines;Redundant computation;Schema changes;},
URL={http://dx.doi.org/10.1145/3076246.3076248},
}


@article{20151600764050,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Code review analytics: Webkit as case study},
journal={IFIP Advances in Information and Communication Technology},
author={Gonzalez-Barahona, Jesus M. and Izquierdo-Cortazar, Daniel and Robles, Gregorio and Gallegos, Mario},
volume={427},
year={2014},
pages={1 - 10},
issn={18684238},
abstract={During the last years, most of the large free / open source software projects have included code review as an usual, or even mandatory practice for changes to their code. In many cases it is implemented as a process in which a developer proposing some change needs to ask for a review by another developer before it can enter the code base. Code reviews, therefore, become a critical process for the project, which could cause delays in contributions being accepted, and risk to become a bottleneck if not enough reviewers are available. In this paper we present a methodology designed to analyze the code review process, to determine its main characteristics and parameters, and to detect potential problems with it. We also present how we have applied this methodology to the WebKit project, learning about the main characteristics of how code review works in their case.<br/> &copy; IFIP International Federation for Information Processing 2014.},
key={Open systems},
keywords={Codes (symbols);Open source software;},
note={Code review;Free/open source softwares;Potential problems;},
}


@inproceedings{20151900830842,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Supporting code review by automatic detection of potentially buggy changes},
journal={Communications in Computer and Information Science},
author={Fejzer, Mikolaj and Wojtyna, Michal and Burzaska, Marta and Winiewski, Piotr and Stencel, Krzysztof},
volume={521},
year={2015},
pages={473 - 482},
issn={18650929},
abstract={Code reviews constitute an important activity in software quality assurance. Although they are essentially based on human expertise and scrupulosity, they can also be supported by automated tools. In this paper we present such a solution integrated with code review tools. It is based on a SVM classifier that indicates potentially buggy changes. We train such a classifier on the history of a project. In order to construct a training set, we assume that a change/commit is buggy if its modifications has been later altered by a bug-fix commit. We evaluated our approach on 77 selected projects taken from GitHub and achieved promising results. We also assessed the quality of the resulting classifier depending on the size of a project and the fraction of the history of a project that have been used to build the training set.<br/> &copy; Springer International Publishing Switzerland 2015.},
key={Codes (symbols)},
keywords={Classification (of information);Computer software selection and evaluation;Quality assurance;},
note={Bug detection;Code review;Gerrit;Github;Weka;},
URL={http://dx.doi.org/10.1007/978-3-319-18422-7_42},
}


@inproceedings{20154501492918,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Supporting code review by automatic detection of potentially buggy changes},
journal={Communications in Computer and Information Science},
author={Fejzer, Mikolaj and Wojtyna, Michal and Burzaska, Marta and Winiewski, Piotr and Stencel, Krzysztof},
volume={521},
year={2015},
pages={473 - 482},
issn={18650929},
address={Ustron, Poland},
abstract={Code reviews constitute an important activity in software quality assurance. Although they are essentially based on human expertise and scrupulosity, they can also be supported by automated tools. In this paper we present such a solution integrated with code review tools. It is based on a SVM classifier that indicates potentially buggy changes. We train such a classifier on the history of a project. In order to construct a training set, we assume that a change/commit is buggy if its modifications has been later altered by a bug-fix commit. We evaluated our approach on 77 selected projects taken from GitHub and achieved promising results. We also assessed the quality of the resulting classifier depending on the size of a project and the fraction of the history of a project that have been used to build the training set.<br/> &copy; Springer International Publishing Switzerland 2015.},
key={Codes (symbols)},
keywords={Classification (of information);Computer software selection and evaluation;Quality assurance;},
note={Bug detection;Code review;Gerrit;Github;Weka;},
URL={http://dx.doi.org/10.1007/978-3-319-18422-7_42},
}


@inproceedings{20122515139329,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning looping: From natural language to worked examples},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Sudol-DeLyser, Leigh Ann and Stehlik, Mark and Carver, Sharon},
volume={7315 LNCS},
year={2012},
pages={710 - 711},
issn={03029743},
address={Chania, Crete, Greece},
abstract={One important introductory concept in many CS courses is repetition (looping), the automated repeating of individual commands. In this work, we present results from a study of college undergraduates' naive conceptions of repetition, their difficulties with learning to construct valid repetition statements, and their abilities to apply what they have learned to new problem solving situations. Although computer programming is a new topic when high school or college students encounter it for the first time, students can draw upon their previous life experiences when solving problems. Those conceptions that align with CS topics [2,3] have been shown to be influenced by students' prior experiences. Alignment through analogies can be helpful [1] although where the scientific concept differs, common knowledge can hinder learning [4]. For many students, the topic of looping is their first encounter with nonlinearity in their programs. Until this point, each line of code is executed once, and then control moves to the next line of code. Such linearity makes reasoning about the programs straightforward. With the addition of looping, in the code you will need to evaluate a termination condition and then either repeat prior lines of code, or move to the next statement after the loop. While returning to a previous command or location is not unusual in everyday life and natural language, it is an important change in the way that novices see their code. &copy; 2012 Springer-Verlag.<br/>},
key={Students},
keywords={Codes (symbols);Computer aided instruction;Computer programming;Intelligent vehicle highway systems;Problem solving;Teaching;},
note={College students;College undergraduates;Common knowledge;Line of codes;Lines of code;Natural languages;Prior experience;Termination condition;},
URL={http://dx.doi.org/10.1007/978-3-642-30950-2_132},
}


@inproceedings{20140717336663,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Experience with digital game-based embodied learning: The road to create a framework for physically interactive digital games},
journal={7th European Conference on Games Based Learning, ECGBL 2013},
author={Busch, Carsten and Conrad, Florian and Meyer, Robert and Steinicke, Martin},
volume={1},
year={2013},
pages={72 - 79},
address={Porto, Portugal},
abstract={Over the past years, we have been researching various approaches to digital game-based learning in the field of change and innovation management. Broadening the range of possible applications while consolidating methodical underpinnings, we have subsequently narrowed down our findings into the description of three specific treatments. This paper focusses on one of the applied treatments, namely to make participants go through a game sequence or interact with a digitally enhanced setup (e.g. play-acting with motion capturing and real-time rendering of a virtual character) to engage learners in embodied and experience driven learning. We present our experience starting with commercial of the shelf physically interactive digital games, followed by two examples of self-made stand-alone Kinect games that have been developed for use in team and leadership trainings. The latter will be introduced describing their goals, the resulting game design as well as lessons learned. Starting from the experience with such settings in project "HELD" as well as applications of embodied digital learning and physically interactive game-based learning by others led us to the belief that there is a need for a framework that enables educational game and interaction designers to develop digital embodied settings without the need of (re)coding the Kinect management code as well as a number of other features relevant for education and training settings (e.g. control app, QR player identification and performance tracking). To further foster the easy development of physically interactive digital games and simulations, or digital aesthetic performances the framework integrates with the Unity game engine, thus enabling both rapid prototyping and quality games. First tests seem very promising with playable game prototypes developed in less than three days. To gather more feedback on real-life applications using digital embodied learning we plan to offer the introduced framework free of charge for non-profit applications.<br/>},
key={E-learning},
keywords={Computer games;Human computer interaction;Personnel training;},
note={Commercial of the shelves;Digital game-based learning;Education and training;Embodied learning;Innovation management;Leadership training;Physically interactive digital play;Real-life applications;},
}


@inproceedings{20141317513837,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Code analyser in CSTutor - A C# Intelligent Tutoring System},
journal={Proceedings of the 21st International Conference on Computers in Education, ICCE 2013},
author={Hartanto, Budi and Reye, Jim},
year={2013},
pages={157 - 159},
address={Bali, Indonesia},
abstract={This paper describes the process that is performed by CSTutor to analyse each student program. CSTutor is an Intelligent Tutoring System that supports the student's learning by doing. Built as an integrated part of Visual Studio 2010 or 2012, CSTutor can give assistance to a student writing programs in Visual Studio from the earliest stage. The analysis process starts by capturing the student's program from the Visual Studio Editor. The program is then parsed and simplified into facts in a knowledge base. This knowledge base also contains rules, actions, constraints, and a goal to be achieved. The goal can be decomposed into several sub-goals to give a finer detail of feedback to the student. So that it can be used as a practical supplement to classroom instruction, CSTutor provides a number of exercises that can be tried by the students. Further, the number of exercises can be increased without having to change CSTutor's program code. The teacher just needs to add the description of the exercise, the constraints, and the goal that should be achieved in the new exercise. The evaluation of CSTutor is in progress and it is expected that CSTutor will help students learn programming to an improved degree.<br/>},
key={Students},
keywords={Cesium;Computer aided instruction;Education computing;Knowledge based systems;Studios;Teaching;},
note={Analysis process;Classroom instruction;Intelligent tutoring system;Knowledge base;Learning by doing;Program analyser;Student writing;Visual studios;},
}


@inproceedings{20181304947332,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Active convolution: Learning the shape of convolution for image classification},
journal={Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
author={Jeon, Yunho and Kim, Junmo},
volume={2017-January},
year={2017},
pages={1846 - 1854},
address={Honolulu, HI, United states},
abstract={In recent years, deep learning has achieved great success in many computer vision applications. Convolutional neural networks (CNNs) have lately emerged as a major approach to image classification. Most research on CNNs thus far has focused on developing architectures such as the Inception and residual networks. The convolution layer is the core of the CNN, but few studies have addressed the convolution unit itself. In this paper, we introduce a convolution unit called the active convolution unit (ACU). A new convolution has no fixed shape, because of which we can define any form of convolution. Its shape can be learned through backpropagation during training. Our proposed unit has a few advantages. First, the ACU is a generalization of convolution; it can define not only all conventional convolutions, but also convolutions with fractional pixel coordinates. We can freely change the shape of the convolution, which provides greater freedom to form CNN structures. Second, the shape of the convolution is learned while training and there is no need to tune it by hand. Third, the ACU can learn better than a conventional unit, where we obtained the improvement simply by changing the conventional convolution to an ACU. We tested our proposed method on plain and residual networks, and the results showed significant improvement using our method on various datasets and architectures in comparison with the baseline. Code is available at https://github.com/jyh2986/Active-Convolution.<br/> &copy; 2017 IEEE.},
key={Convolution},
keywords={Computer vision;Deep learning;Image classification;Network architecture;Neural networks;},
note={Computer vision applications;Convolutional neural network;Fractional pixel;},
URL={http://dx.doi.org/10.1109/CVPR.2017.200},
}


@inproceedings{20180204626856,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={CUDA multiclass change detection for remote sensing hyperspectral images using extended morphological profiles},
journal={Proceedings of the 2017 IEEE 9th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications, IDAACS 2017},
author={Lopez-Fandino, Javier and Heras, Dora B. and Arguello, Francisco and Duro, Richard J.},
volume={1},
year={2017},
pages={404 - 409},
address={Bucharest, Romania},
abstract={The need for information of the Earth's surface is growing as it is the base for applications such as monitoring the land uses or performing environmental studies, for example. In this context the effective change detection (CD) among multitemporal datasets is a key process that must produce accurate results obtained by computationally efficient algorithms. Most of the CD methods are focused on binary detection (presence or absence of changes) or in the clustering of the different detected types of changes. In this paper, a CUDA scheme to perform pixel-based multiclass CD for hyperspectral datasets is introduced. The scheme combines multiclass CD with binary CD to obtain an accurate multiclass change map. The combination with the binary map contributes to reducing the execution time of the CUDA code. The binary CD is based on performing the difference among images based on Euclidean and Spectral Angle Mapper (SAM) distances and a later thresholding by Otsu's algorithm to detect the changed pixels. The multiclass CD begins with the fusion of the multitemporal data following with feature extraction by Principal Component Analysis (PCA) and incorporating spatial features by means of an Extended Morphological Profile (EMP). The resulting dataset is filtered using the binary CD map and classified pixel by pixel by the supervised algorithms Extreme Learning Machine (ELM) and Support Vector Machine (SVM). The scheme was validated in a non-synthetic multitemporal hyperspectral dataset.<br/> &copy; 2017 IEEE.},
key={Principal component analysis},
keywords={Data acquisition;Graphics processing unit;Image segmentation;Land use;Pixels;Remote sensing;Spectroscopy;Support vector machines;},
note={Change detection;Computationally efficient;CUDA;Environmental studies;Extended morphological profiles;Extreme learning machine;Spectral angle mappers;Supervised algorithm;},
URL={http://dx.doi.org/10.1109/IDAACS.2017.8095113},
}


@article{20173704145532,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Predicting change consistency in a clone group},
journal={Journal of Systems and Software},
author={Zhang, Fanlong and Khoo, Siau-cheng and Su, Xiaohong},
volume={134},
year={2017},
pages={105 - 119},
issn={01641212},
abstract={Code cloning has been accepted as one of the general code reuse methods in software development, thanks to the increasing demand in rapid software production. The introduction of clone groups and clone genealogies enable software developers to be aware of the presence of and changes to clones as a collective group; they also allow developers to understand how clone groups evolve throughout software life cycle. Due to similarity in codes within a clone group, a change in one piece of the code may require developers to make consistent change to other clones in the group. Failure in making such consistent change to a clone group when necessary is commonly known as &ldquo;clone consistency-defect&rdquo;, which can adversely impact software reusability. In this work, we propose an approach to predict the need for making consistent change in clones within a clone group at the time when changes have been made to one of its clones. We build a variant of clone genealogies to collect all consistent/inconsistent changes to clone groups, and extract three attribute sets from clone groups as input for predicting the need for consistent clone change. These three attribute sets are code attributes, context attributes and evolution attributes respectively. Together, they provide a holistic view about clone changes. We conduct experiments on four open source projects. Our experiments show that our approach has reasonable precision and recall in predicting whether a clone group requires (or is free of) consistent change. This holistic approach can aid developers in maintaining clone changes, and avoid potential consistency-defect, which can improve software quality and reusability.<br/> &copy; 2017 Elsevier Inc.},
key={Cloning},
keywords={Bayesian networks;Codes (symbols);Computer software reusability;Computer software selection and evaluation;Defects;Forecasting;History;Life cycle;Open source software;Reusability;Software design;},
note={Clone attributes;Code clone;Consistency requirements;Open source projects;Precision and recall;Software developer;Software life cycles;Software production;},
URL={http://dx.doi.org/10.1016/j.jss.2017.08.045},
}


@article{20150800560610,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Multi up-gradation software reliability growth model with learning effect and severity of faults using SDE},
journal={International Journal of Systems Assurance Engineering and Management},
author={Singh, Jagvinder and Singh, Ompal and Kapur, P.K.},
volume={6},
number={1},
year={2015},
pages={18 - 25},
issn={09756809},
abstract={In recent years, the dependence on a computer system has become large in our social life. Therefore, it becomes more important for software developers to produce highly reliable software systems. Due to timely demand and competitive nature of the market of software product, firms are frequently launching their upgraded versions of the base software. Many software reliability growth model have been developed by software developers and managers in tracking and measuring the growth of reliability. As the size of software system is large and the number of faults detected during the testing phase becomes large, so the change of the number of faults that are detected and removed through each debugging becomes sufficiently small compared with the initial fault content at the beginning of the testing phase. In such a situation, we can model the software fault detection process as a stochastic process with continuous-state space. In this paper, we derive a stochastic differential equation of (Formula presented.) type based multi-up-gradation model with severity of faults and effect of learning. Moreover, we discuss the identification of the faults left in the software when it is in operational phase during the testing of the new code i.e. developed while adding new features to the existing software. We examine the case where there exists two types of faults in the software; simple and hard and during testing the simple faults are removed by exponential rate whereas hard faults are removed by Yamada with learning effect function. Results are supplemented by a numerical example.<br/> &copy; 2014, The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.},
key={Software reliability},
keywords={Differential equations;Fault detection;Program debugging;Random processes;Software testing;Stochastic models;Stochastic systems;},
note={Continuous State Space;Effect of learning;Software developer;Software fault detection;Software Reliability Growth Modeling;Software reliability growth models;Stochastic differential equations;Up gradations;},
URL={http://dx.doi.org/10.1007/s13198-014-0238-1},
}


@inproceedings{20170803379101,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Predicting Consistent Clone Change},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
author={Zhang, Fanlong and Khoo, Siau-Cheng and Su, Xiaohong},
volume={0},
year={2016},
pages={353 - 364},
issn={10719458},
address={Ottawa, ON, United states},
abstract={Code clones, being an inevitable by-product of rapid software development, can impact software quality. The introduction of code clone groups and clone genealogies enable software developers to be aware of the presence of and changes to clones as a collective group, they also allow developers to understand how clone groups evolve throughout software life cycle. Due to similarity in codes within a clone group, a change in one piece of the code may require developers to make changes to other clones in the group. Failure in making consistent change to a clone group when necessary is commonly known as 'clone consistency-defect', which can adversely impact software reliability. We propose an approach to predict clone consistency-requirement at the time when changes have been made to a clone group. Our predictor is a Bayesian network implemented in WEKA. We build a variant of clone genealogies to collect all consistent/inconsistent changes to clone groups, and extract three sets of attributes from clone groups as input for predicting consistent clone change. These three sets are: code attributes, context attributes and evolution attributes. We conduct experiments on three open source projects. These experiments show that our approach has high precision and recall in predicting clone consistency-requirement. This holistic approach can aid developers in maintaining code clone changes, and avoid potential clone consistency-defect, which can improve the software quality and reliability.<br/> &copy; 2016 IEEE.},
key={Software reliability},
keywords={Bayesian networks;Cloning;Codes (symbols);Computer software selection and evaluation;Defects;Forecasting;History;Life cycle;Open source software;Software design;},
note={Clone attributes;Code clone;Consistency requirements;Context attributes;Open source projects;Software developer;Software life cycles;Software Quality;},
URL={http://dx.doi.org/10.1109/ISSRE.2016.11},
}


@inproceedings{20183105639921,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Formal description and verification of a text-based model differencing and merging method},
journal={MODELSWARD 2018 - Proceedings of the 6th International Conference on Model-Driven Engineering and Software Development},
author={Somogyi, Ferenc A. and Asztalos, Mark},
volume={2018-January},
year={2018},
pages={657 - 667},
address={Funchal, Madeira, Portugal},
abstract={Version control is an integral part of teamwork in software development. Differencing and merging key artifacts (i.e. source code) is a key feature in version control systems. The concept of version control can also be applied to model-driven methodologies. The models are usually differenced and merged in their graph-based form. However, if supported, we can also use the textual representation of the models during this process. Text-based model differencing and merging methods have some useful use cases, like supporting the persistence of the model, or having a fallback plan should the differencing algorithm fail. Using the textual notation to display and edit models is relatively rare, as the visual (graph-based) representation of the model is more common. However, many believe that using them both would be the ideal solution. In this paper, we present the formal description of a text-based model differencing and merging method from previous work. We also verify our algorithm based on this formal description. The focus of the verification is the soundness and completeness of the method. The long term goal of our research is to develop a modeling environment-independent algorithm. This could be used in version control systems that support textual representations.<br/> Copyright &copy; 2018 by SCITEPRESS &ndash; Science and Technology Publications, Lda. All rights reserved.},
key={Software design},
keywords={Algorithms;Control systems;Graphic methods;Information management;Merging;Verification;},
note={Differencing algorithm;Formal Description;Model-driven methodology;Modeling environments;Soundness and completeness;Textual representation;Version control;Version control system;},
}


@inproceedings{20122815225918,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using graph-based program characterization for predictive modeling},
journal={Proceedings - International Symposium on Code Generation and Optimization, CGO 2012},
author={Park, Eunjung and Cavazos, John and Alvarez, Marco A.},
year={2012},
pages={196 - 206},
address={San Jose, CA, United states},
abstract={Using machine learning has proven effective at choosing the right set of optimizations for a particular program. For machine learning techniques to be most effective, compiler writers have to develop expressive means of characterizing the program being optimized. The current state-of-the-art techniques for characterizing programs include using a fixed-length feature vector of either source code features extracted during compile time or performance counters collected when running the program. For the problem of identifying optimizations to apply, models constructed using performance counter characterizations of a program have been shown to outperform models constructed using source code features. However, collecting performance counters requires running the program multiple times, and this "dynamic" method of characterizing programs can be specific to inputs of the program. It would be preferable to have a method of characterizing programs that is as expressive as performance counter features, but that is "static" like source code features and therefore does not require running the program. In this paper, we introduce a novel way of characterizing programs using a graph-based characterization, which uses the program's intermediate representation and an adapted learning algorithm to predict good optimization sequences. To evaluate different characterization techniques, we focus on loop-intensive programs and construct prediction models that drive polyhedral optimizations, such as auto-parallelism and various loop transformation. We show that our graph-based characterization technique outperforms three current state-of-the-art characterization techniques found in the literature. By using the sequences predicted to be the best by our graph-based model, we achieved up to 73% of the speedup achievable in our search space for a particular platform, whereas we could only achieve up to 59% by other state-of-the-art techniques we evaluated. Copyright &copy; 2012 ACM.<br/>},
key={Program compilers},
keywords={Artificial intelligence;Characterization;Codes (symbols);Computer programming languages;Graphic methods;Image retrieval;Iterative methods;Learning algorithms;Learning systems;Support vector machines;},
note={Characterization techniques;Compiler optimizations;Graph-based;Intermediate representations;Iterative compilation;Machine learning techniques;Polyhedral optimizations;State-of-the-art techniques;},
URL={http://dx.doi.org/10.1145/2259016.2259042},
}


@article{20183405725457,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Minnesota sustainable building guidelines: History, effectiveness and path for the future},
journal={Journal of Green Building},
author={Graves, Richard and Smith, Patrick},
volume={13},
number={2},
year={2018},
pages={163 - 180},
issn={15526100},
abstract={INTRODUCTION The Minnesota Sustainable Building Guidelines is a progressive sustainability program for state funded buildings which serves as a model for sustainability in Minnesota buildings. The program was created by the State of Minnesota in 2001 and developed by a team led by the Center for Sustainable Building Research (CSBR) at the University of Minnesota. Unlike other green building programs, it focuses on measured performance improvements, using a list of required metrics instead of a menu of potential options. The program is structured to provide a feedback loop to the building design, construction and operations industry in the state. Elements of the program are used through all phases of the development of state-funded buildings in Minnesota from pre-design through design, and construction and for ten years of operations. It is continually updated and improved in collaboration with state agencies and industry stakeholders and could serve as a model for localized green building programs.<br/> &copy; 2018, College Publishing. All rights reserved.},
key={Architectural design},
keywords={Climate change;Construction;Emission control;Intelligent buildings;Sustainable development;},
note={Actual performance;Code;Emission reduction;Green buildings;Net zero;Rating system;Renewable;},
URL={http://dx.doi.org/10.3992/1943-4618.13.2.163},
}


@inproceedings{20163502757990,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Helping mobile software code reviewers: A study of bug repair and refactoring patterns},
journal={Proceedings - International Conference on Mobile Software Engineering and Systems, MOBILESoft 2016},
author={Chen, Zhiyuan},
year={2016},
pages={34 - 35},
address={Austin, TX, United states},
abstract={Mobile Developers commonly spend a significant amount of time and effort on conducting code reviews on newly introduced and domain-specific practices, such as platform-specific feature addition, quality of service anti-pattern refactorings, and battery-related bug fixes. To address these problems, we conducted a large empirical study over the software change history of 318 open source projects and investigated platform-dependent code changes from open source projects. Our analysis focuses on what types of changes mobile application developers typically make and how they perceive, recall, and communicate changed and affected code. Our study required the development of an automated strategy to examine open source repositories and categorize platform-related refactoring edits, bug repairs, and API updates, mining 1,961,990 commit changes. Our findings call for the need to develop a new recommendation system aimed at efficiently identifying required changes such as bug fixes and refactorings during mobile application code reviews.<br/> &copy; 2016 Copyright held by the owner/author(s).},
key={Open systems},
keywords={Codes (symbols);Mobile computing;Open source software;Quality of service;},
note={Anti-patterns;Domain specific;Empirical studies;Mobile applications;Mobile softwares;Open source projects;Open source repositories;Software change;},
URL={http://dx.doi.org/10.1145/2897073.2897130},
}


@inproceedings{20125215831127,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Multi-layered approach for recovering links between bug reports and fixes},
journal={Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012},
author={Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Hoan Anh and Nguyen, Tien N.},
year={2012},
pages={Assoc. Comput. Mach., Spec. Interest; Group Softw. Eng. (ACM SIGSOFT) - },
address={Cary, NC, United states},
abstract={The links between the bug reports in an issue-tracking system and the corresponding fixing changes in a version repository are not often recorded by developers. Such linking information is crucial for research in mining software repositories in measuring software defects and maintenance efforts. However, the state-of-the-art bug-to-fix link recovery approaches still rely much on textual matching between bug reports and commit/change logs and cannot handle well the cases where their contents are not textually similar. This paper introduces MLink, a multi-layered approach that takes into account not only textual features but also source code features of the changed code corresponding to the commit logs. It is also capable of learning the association relations between the terms in bug reports and the names of entities/components in the changed source code of the commits from the established bug-to-fix links, and uses them for link recovery between the reports and commits that do not share much similar texts. Our empirical evaluation on real-world projects shows that MLink can improve the state-of-the-art bug-to-fix link recovery methods by 11 - 18%, 13 - 17%, and 8 - 17% in F-score, recall, and precision, respectively. &copy; 2012 ACM.<br/>},
key={Program debugging},
keywords={Codes (symbols);Costs;Recovery;Software engineering;},
note={bug-to-fix links;bugs;Empirical evaluations;fixes;Maintenance efforts;Mining software repositories;Multi-layered approach;Real world projects;},
URL={http://dx.doi.org/10.1145/2393596.2393671},
}


@article{20172903957519,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Axially deformed solution of the SkyrmeHartreeFockBogolyubov equations using the transformed harmonic oscillator basis (III) HFBTHO (v3.00): A new version of the program},
journal={Computer Physics Communications},
author={Perez, R. Navarro and Schunck, N. and Lasseri, R.-D. and Zhang, C. and Sarich, J.},
volume={220},
year={2017},
pages={363 - 375},
issn={00104655},
abstract={We describe the new version 3.00 of the code HFBTHO that solves the nuclear Hartree&ndash;Fock (HF) or Hartree&ndash;Fock&ndash;Bogolyubov (HFB) problem by using the cylindrical transformed deformed harmonic oscillator basis. In the new version, we have implemented the following features: (i) the full Gogny force in both particle&ndash;hole and particle&ndash;particle channels, (ii) the calculation of the nuclear collective inertia at the perturbative cranking approximation, (iii) the calculation of fission fragment charge, mass and deformations based on the determination of the neck, (iv) the regularization of zero-range pairing forces, (v) the calculation of localization functions, (vi) a MPI interface for large-scale mass table calculations. PROGRAM SUMMARY Program title:HFBTHO v3.00 Program Files doi: http://dx.doi.org/10.17632/c5g2f92by3.1 Licensing provisions: GPL v3 Programming language: FORTRAN-95 Journal reference of previous version: M.V. Stoitsov, N. Schunck, M. Kortelainen, N. Michel, H. Nam, E. Olsen, J. Sarich, and S. Wild, Comput. Phys. Commun. 184 (2013). Does the new version supersede the previous one: Yes Summary of revisions: 1. the Gogny force in both particle&ndash;hole and particle&ndash;particle channels was implemented; 2. the nuclear collective inertia at the perturbative cranking approximation was implemented; 3. fission fragment charge, mass and deformations were implemented based on the determination of the position of the neck between nascent fragments; 4. the regularization method of zero-range pairing forces was implemented; 5. the localization functions of the HFB solution were implemented; 6. a MPI interface for large-scale mass table calculations was implemented. Nature of problem:HFBTHO is a physics computer code that is used to model the structure of the nucleus. It is an implementation of the energy density functional (EDF) approach to atomic nuclei, where the energy of the nucleus is obtained by integration over space of some phenomenological energy density, which is itself a functional of the neutron and proton intrinsic densities. In the present version of HFBTHO, the energy density derives either from the zero-range Skyrme or the finite-range Gogny effective two-body interaction between nucleons. Nuclear super-fluidity is treated at the Hartree&ndash;Fock&ndash;Bogolyubov (HFB) approximation. Constraints on the nuclear shape allows probing the potential energy surface of the nucleus as needed e.g., for the description of shape isomers or fission. The implementation of a local scale transformation of the single-particle basis in which the HFB solutions are expanded provide a tool to properly compute the structure of weakly-bound nuclei. Solution method: The program uses the axial Transformed Harmonic Oscillator (THO) single-particle basis to expand quasiparticle wave functions. It iteratively diagonalizes the Hartree&ndash;Fock&ndash;Bogolyubov Hamiltonian based on generalized Skyrme-like energy densities and zero-range pairing interactions or the finite-range Gogny force until a self-consistent solution is found. A previous version of the program was presented in M.V. Stoitsov, N. Schunck, M. Kortelainen, N. Michel, H. Nam, E. Olsen, J. Sarich, and S. Wild, Comput. Phys. Commun. 184 (2013) 1592&ndash;1604 with much of the formalism presented in the original paper M.V. Stoitsov, J. Dobaczewski, W. Nazarewicz, P. Ring, Comput. Phys. Commun. 167 (2005) 43&ndash;63. Additional comments: The user must have access to (i) the LAPACK subroutines DSYEEVR, DSYEVD, DSYTRF and DSYTRI, and their dependencies, which compute eigenvalues and eigenfunctions of real symmetric matrices, (ii) the LAPACK subroutines DGETRI and DGETRF, which invert arbitrary real matrices, and (iii) the BLAS routines DCOPY, DSCAL, DGEMM and DGEMV for double-precision linear algebra (or provide another set of subroutines that can perform such tasks). The BLAS and LAPACK subroutines can be obtained from the Netlib Repository at the University of Tennessee, Knoxville: http://netlib2.cs.utk.edu/.<br/> &copy; 2017 Elsevier B.V.},
key={Density functional theory},
keywords={Carrier concentration;Deformation;Eigenvalues and eigenfunctions;Fission reactions;FORTRAN (programming language);Harmonic analysis;Hartree approximation;Isomers;Iterative methods;Matrix algebra;Oscillators (mechanical);Potential energy;Problem oriented languages;Quantum chemistry;Wave functions;},
note={Bogoliubov theory;Collective inertia;Energy density functional theory;Gogny force;Harmonic oscillators;Pairing regularization;Skyrme-forces;},
URL={http://dx.doi.org/10.1016/j.cpc.2017.06.022},
}


@article{20132616444717,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={POINCARE CODE: A package of open-source implements for normalization and computer algebra reduction near equilibria of coupled ordinary differential equations},
journal={Computer Physics Communications},
author={Mikram, J. and Zinoun, F. and El Abdllaoui, A.},
volume={184},
number={9},
year={2013},
pages={2204 - 2213},
issn={00104655},
abstract={The Poincare&acute; code is a Maple project package that aims to gather significant computer algebra normal form (and subsequent reduction) methods for handling nonlinear ordinary differential equations. As a first version, a set of fourteen easy-to-use Maple commands is introduced for symbolic creation of (improved variants of Poincare&acute;'s) normal forms as well as their associated normalizing transformations. The software is the implementation by the authors of carefully studied and followed up selected normal form procedures from the literature, including some authors' contributions to the subject. As can be seen, joint-normal-form programs involving Lie-point symmetries are of special interest and are published in CPC Program Library for the first time, Hamiltonian variants being also very useful as they lead to encouraging results when applied, for example, to models from computational physics like He&acute;non-Heiles. &copy; 2013 Elsevier B.V. All rights reserved.<br/>},
key={Ordinary differential equations},
keywords={Algebra;Nonlinear equations;Open source software;Open systems;},
note={Computational physics;Computer algebra;Lie point symmetries;Lie transforms;Nonlinear ordinary differential equation;Normal form;Normalizing transformation;Subsequent reduction;},
URL={http://dx.doi.org/10.1016/j.cpc.2013.04.003},
}


@article{20144800248635,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Data hiding based on overlapped pixels using hamming code},
journal={Multimedia Tools and Applications},
author={Kim, Cheonshik and Yang, Ching-Nung},
volume={75},
number={23},
year={2016},
pages={15651 - 15663},
issn={13807501},
abstract={Most data hiding schemes change the least significant bits to conceal messages in the cover images. Matrix encoding scheme is a well known scheme in this field. The matrix encoding proposed by Crandall can be used in steganographic data hiding methods. Hamming codes are kinds of cover codes. &ldquo;Hamming + 1&rdquo; proposed by Zhang et al. is an improved version of matrix encoding steganography. The embedding efficiency of &ldquo;Hamming + 1&rdquo; is very high for data hiding, but the embedding rate is low. Our proposed &ldquo;Hamming + 3&rdquo; scheme has a slightly reduced embedding efficiency, but improve the embedding rate and image quality. &ldquo;Hamming + 3&rdquo; is applied to overlapped blocks, which are composed of 2<sup>k</sup>+3 pixels, where k=3. We therefore propose verifying the embedding rate during the embedding and extracting phases. Experimental results show that the reconstructed secret messages are the same as the original secret message, and the proposed scheme exhibits a good embedding rate compared to those of previous schemes.<br/> &copy; 2014, Springer Science+Business Media New York.},
key={Matrix algebra},
keywords={Block codes;Efficiency;Encoding (symbols);Image enhancement;Pixels;Signal encoding;Steganography;},
note={Data hiding;Embedding and extracting;Embedding efficiency;Embedding rates;Hamming code;Least significant bits;Matrix encoding;Secret messages;},
URL={http://dx.doi.org/10.1007/s11042-014-2355-x},
}


@inproceedings{20181505002873,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Industrial experience with the migration of legacy models using a DSL},
journal={ACM International Conference Proceeding Series},
author={Schuts, Mathijs and Hooman, Jozef and Tielemans, Paul},
year={2018},
pages={Engineering and Physical Sciences Research Council (EPSRC); Heriot-Watt University - },
address={Vienna, Austria},
abstract={Software departments of companies that exist for several decades often have to deal with legacy models. Important business assets have been modelled with tools that are no longer preferred within the company. Manually remodelling these models with a new tool would be too costly. In this paper, we describe an approach to migrate from Rhapsody models to models of another tool. To perform the migration, we created a Domain Specific Language (DSL) that accepts Rhapsody models as instances. A generator of this DSL can then produces model instances for the new tool. To get confidence in the transformation in a pragmatic way, we applied a combination of model learning and equivalence checking. Learning has been applied to both the source code generated by Rhapsody and the code generated by the new tool. The resulting models are compared using equivalence checking.<br/> &copy; 2018 Association for Computing Machinery.},
key={Digital subscriber lines},
keywords={Codes (symbols);Graphical user interfaces;Problem oriented languages;},
note={Domain specific language (DSL);Domain specific languages;Equivalence checking;Industrial experience;Legacy;Model based development;Model learning;Model transformation;},
URL={http://dx.doi.org/10.1145/3183895.3183897},
}


@inproceedings{20161502236596,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Visualizing Time-based Weighted Coupling Using Particle Swarm Optimization to Aid Program Comprehension},
journal={Procedia Computer Science},
author={Hendrawan, Rully Agus and Maruyama, Katsuhisa},
volume={72},
year={2015},
pages={597 - 604},
issn={18770509},
address={Shenzhen, China},
abstract={By knowing software coupling, developers can get better view of the software quality and improve their productivity in development and maintenance. This paper presents a method to visualize coupling network that are often very complex, using heuristic approach based on particle swarming optimization. Each node is placed randomly and assigned with initial speed. Node that are coupled together will be attracted each other and trying to get closer until they reach a particular distance. This distance is determined from the coupling value of two nodes. A closely related nodes will move closer until reaching a short distance. On each iteration, node position is dynamically updated based on attraction and repulsive force around them. Thus, gradually forming a near best solution of logical coupling graph. The coupling values are measured by mining the association rule from changes history. A software development project sometimes can be very active, updates happen within minutes. Sometimes it becomes slow with weekly or even monthly updates. Time-based weighted analysis method was used to accommodate these time sensitive situations. A co-change in a short duration will be weighted more than co-changes that happen in longer duration.<br/> &copy; 2015 The Authors.},
key={Heuristic methods},
keywords={Computer programming;Computer software selection and evaluation;Information systems;Information use;Iterative methods;Particle swarm optimization (PSO);Software design;},
note={Association mining;Heuristic approach;Program comprehension;Repulsive forces;Software coupling;Software development projects;Software Quality;Source code visualizations;},
URL={http://dx.doi.org/10.1016/j.procs.2015.12.168},
}


@article{20143600048504,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Heuristic expansion of feature mappings in evolving program families},
journal={Software - Practice and Experience},
author={Nunes, Camila and Garcia, Alessandro and Lucena, Carlos and Lee, Jaejoon},
volume={44},
number={11},
year={2014},
pages={1315 - 1349},
issn={00380644},
abstract={Establishing explicit mappings between features and their implementation elements in code is one of the critical factors to maintain and evolve software systems successfully. This is especially important when developers have to evolve program families, which have evolved from one single core system to similar but different systems to accommodate various requirements from customers. Many techniques and tools have emerged to assist developers in the feature mapping activity. However, existing techniques and tools for feature mapping are limited as they operate on a single program version individually. Additionally, existing approaches are limited to recover features on demand, that is, developers have to run the tools for each family member version individually. In this paper, we propose a cohesive suite of five mapping heuristics addressing those two limitations. These heuristics explore the evolution history of the family members in order to expand feature mappings in evolving program families. The expansion refers to the action of automatically generating the feature mappings for each family member version by systematically considering its previous change history. The mapping expansion starts from seed mappings and continually tracks the features of the program family, thus eliminating the need of on demand algorithms. Additionally, we present the MapHist tool that provides support to the application of the proposed heuristics. We evaluate the accuracy of our heuristics through two evolving program families from our industrial partners.<br/> Copyright &copy; 2013 John Wiley & Sons, Ltd.},
key={Mapping},
keywords={Expansion;Heuristic programming;},
note={Evolution history;Experimental evaluation;Feature mapping;Heuristics;Industrial partners;Program family;Software systems;Techniques and tools;},
URL={http://dx.doi.org/10.1002/spe.2200},
}


@article{20172903952894,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Code Reviewing in the Trenches: Challenges and Best Practices},
journal={IEEE Software},
author={MacLeod, Laura and Greiler, Michaela and Storey, Margaret-Anne and Bird, Christian and Czerwonka, Jacek},
volume={35},
number={4},
year={2018},
pages={34 - 42},
issn={07407459},
abstract={Code review has been widely adopted by and adapted to open source and industrial projects. Code review practices have undergone extensive research, with most studies relying on trace data from tool reviews, sometimes augmented by surveys and interviews. Several recent industrial research studies, along with blog posts and white papers, have revealed additional insights on code reviewing 'from the trenches.' Unfortunately, the lessons learned about code reviewing are widely dispersed and poorly summarized by the existing literature. In particular, practitioners wishing to adopt or reflect on an existing or new code review process might have difficulty determining what challenges to expect and which best practices to adopt for their development context. Building on the existing literature, this article adds insights from a recent large-scale study of Microsoft developers to summarize the challenges that code-change authors and reviewers face, suggest best code-reviewing practices, and discuss tradeoffs that practitioners should consider. This article is part of a theme issue on Process Improvement.<br/> &copy; 2018 IEEE.},
key={Open systems},
keywords={Codes (symbols);Economic and social effects;Engineering education;Industrial research;Object recognition;Open source software;Program debugging;Societies and institutions;Software testing;Tools;},
note={Best practices;Context;Interviews;Learning technology;Peer reviewing;Social technologies;Software/software engineering;Stakeholders;Walkthroughs;},
URL={http://dx.doi.org/10.1109/MS.2017.265100500},
}


@article{20143117999476,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Electron number distribution functions from molecular wavefunctions. Version 2},
journal={Computer Physics Communications},
author={Francisco, E. and Martin Pendas, A.},
volume={185},
number={10},
year={2014},
pages={2663 - 2682},
issn={00104655},
abstract={We present in this article a new and considerably faster version of the edf Fortran 77/90 code that replaces the old one (Francisco et al., 2008). In the new version, given an N-electron molecule and an exhaustive, fuzzy, or orbital-based partition of the physical space<sup>R3</sup>into m domains, the probabilities p(S) of all possible distributions S={<sup>n1</sup>,<sup>n2</sup>,&mellip;,<sup>nm</sup>} of the N electrons (<sup>n1</sup>+<sup>n2</sup>+&mellip;+<sup>nm</sup>=N) into m real space domains are computed. The set {p(S)} defines the electron number distribution function (EDF) of the molecule for this specific space partition. The molecule may be described by either a single- or a multi-determinant wavefunction &Psi;(1,N). Both spin-resolved and spin-unresolved EDFs are determined. Isopycnic orbital localizations of the natural molecular orbitals (MOs) can be optionally performed to make the use of the core approximation possible. This explicitly eliminates from the calculation those MOs strongly that are localized over one of the m domains, considerably speeding up the process. An optional approximation consisting of assuming that localized MOs are orthogonal to each other in all the domains is shown to give reasonably accurate results and further accelerates the calculation. The new edf code does also allows for the computation of a single probability p(<sup>n1</sup>,<sup>n2</sup>,&mellip;,<sup>nm</sup>) instead of the full EDF. Finally, this new version computes multiple-domain covariances of electron populations, particularly relevant for chemical bonding theory. Program summary/new version program summary Program title: edf Catalogue identifier: AEAJ-v2-0 Program summary URL:http://cpc.cs. qub.ac.uk/summaries/AEAJ-v2-0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 16847 No. of bytes in distributed program, including test data, etc.: 206274 Distribution format: tar.gz Programming language: Fortran 77/90. Computer: 2.80 GHz Intel Pentium IV CPU. Operating system: GNU/Linux. RAM: Dynamic Classification: 2.7. External routines: mkl Does the new version supersede the previous version?: Yes Catalogue identifier of the previous version: AEAJ-v1-0 Journal reference of the previous version: Comput. Phys. Comm. 178 (2008) 621 Nature of problem: Given an N-electron molecule described by a single- or multi-determinant wavefunction &Psi;(1,N), and a partition of the physical space<sup>R3</sup>into m domains<sup>&Omega;1</sup>,<sup>&Omega;2</sup>,&mellip;,<sup>&Omega;m</sup>, edf computes the probabilities p(S) of having exactly<sup>n1</sup>,<sup>n2</sup>,&mellip;, and<sup>nm</sup>electrons in<sup>&Omega;1</sup>,<sup>&Omega;2</sup>,&mellip;, and<sup>&Omega;m</sup>, respectively, for all possible distributions S={<sup>n1</sup>,<sup>n2</sup>,&mellip;,<sup>nm</sup>}, being<sup>n1</sup>,<sup>n2</sup>,&mellip;, and<sup>nm</sup>integer numbers. Solution method: Given a wavefunction &Psi;(1,N)=&sum;rM<sup>cr</sup><sup>&Psi;r</sup>(1,N), where the<sup>&Psi;r</sup>'s are Slater determinants<sup>&Psi;r</sup>=det[X1r,&mellip;,XNr], and calling (Skrs)ij the overlap integral within the domain<sup>&Omega;k</sup>between Xir and Xjs, edf finds all the p(S)'s using a three-step procedure: For each (r,s) pair, solve the linear system &sum;{n&sigma;i} t1n1&sigma;t2n2&sigma;&mellip;tmnm&sigma;prs({ni&sigma;})=det[&sum;k=1m<sup>tk</sup>Skrs] in the unknowns prs(ni&sigma;), where &sigma;=(&alpha;, &beta;), {n&sigma;i}(i=1,2,&mellip;,m) are the integer electronic populations with spin &sigma; of the domains<sup>&Omega;1</sup>,<sup>&Omega;2</sup>, &mellip;,<sup>&Omega;m</sup>,<sup>tm</sup>=1, and<sup>t1</sup>,&mellip;,tm-<inf>1</inf>are arbitrary real numbers,Compute the spin-resolved probabilities p({<sup>n&alpha;</sup>;<sup>n&beta;</sup>})=p({<sup>ip</sup>})=&sum;r,sM<sup>crcsprs</sup>({ni&alpha;})<sup>prs</sup>({ni&beta;}), andobtain the p(S)'s by adding up the p({<sup>ip</sup>})'s with ni&alpha;+ni&beta;=<sup>ni</sup>Reasons for new version: Dynamic memory allocation instead of static memory allocation is used throughout. Further partitions of the 3D space have been added. Thanks to the change in the algorithm used to solve the problem, the new version is 1-2 orders of magnitude faster than the previous one and can deal with molecules having a greater number of electrons. Approximate calculations as well as exact ones are possible in the new version by making use of the core-valence separability. Summary of revisions: Most data structures are stored in dynamic memory. The basic algorithm has been changed to ensure a much faster computation of the probabilities p(S). Algorithms to obtain the latter in an approximate manner, as well as using different partitions of the 3D space have been included. Restrictions: The number of {ni&sigma;} sets in Eq. 1, i.e. the dimension of the linear system to be solved, grows very fast with m and N. This dimension is much smaller than in the previous version of edf but, even so, this restricts the applicability of the method to relatively small systems, unless some drastic approximations are used (excluding, for instance, a large part of the electrons of the system from the calculation). Running time: 0.016 and 0.004 s for the test examples 1 and 2, respectively. However, running times are very variable depending on the molecule, the type of the wavefunction (single- or multi-determinant), the number of fragments (m). etc. References: [1] E. Francisco, A. Marti&acute;n Penda&acute;s, and M.A. Blanco. J. Chem. Phys. 126, 094102 (2007). [2] A. Marti&acute;n Penda&acute;s, E. Francisco, and M.A. Blanco. J. Chem. Phys. 127, 144103 (2007). [3] E. Francisco, A. Marti&acute;n Penda&acute;s, and M.A. Blanco. Computer Physics Commun. 178, 621-634 (2008). [4] A. Marti&acute;n Penda&acute;s, E. Francisco, and M.A. Blanco. Faraday Discuss. 135, 423-438 (2007). [5] A. Marti&acute;n Penda&acute;s, E. Francisco, M.A. Blanco, and C. Gatti. Chemistry: A European Journal. 13, 9362-9371 (2007). [6] A. Marti&acute;n Penda&acute;s, E. Francisco, and M.A. Blanco. Phys. Chem. Chem. Phys. 9, 1087-1092 (2007). [7] E. Francisco, A. Marti&acute;n Penda&acute;s, and M.A. Blanco. J. Chem. Phys. 131, 124125 (2009). [8] E. Francisco, A. Marti&acute;n Penda&acute;s, and M.A. Blanco. Theor. Chemistry Accounts. 128, 433 (2011). [9] E. Francisco, A. Marti&acute;n Penda&acute;s, A. Costales, and M.A. Garci&acute;a-Revilla. Comput. Theor. Chem. 975, 2-8 (2011). [10] M.A. Garci&acute;a-Revilla, E. Francisco, A. Marti&acute;n Penda&acute;s, J.M. Recio, M. Bartolomei, M.I. Herna&acute;ndez, J. Campos-Marti&acute;nez, E. Carmona-Novillo, and R. Herna&acute;ndez-Lamoneda. J. Chem. Theory Comput. 9, 2179-2188 (2013). &copy; 2014 Elsevier B.V. All rights reserved.<br/>},
key={Probability distributions},
keywords={Chemical bonds;Computation theory;Distribution functions;Dynamics;Electronic properties;Electrons;FORTRAN (programming language);Linear systems;Memory architecture;Molecular orbitals;Molecules;Open source software;Problem oriented languages;Quantum computers;Random access storage;Software testing;Storage allocation (computer);Wave functions;},
note={Approximate calculations;Catalogue identifiers;Chemical bonding theory;Distributed program;Dynamic classification;Dynamic memory allocation;Molecular wave functions;Quantum Theory of Atoms in Molecules;},
URL={http://dx.doi.org/10.1016/j.cpc.2014.05.009},
}


@article{20143218043367,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A new measure of code complexity during software evolution: 'A case study'},
journal={International Journal of Multimedia and Ubiquitous Engineering},
author={Singh, Vinay and Bhattacherjee, Vandana},
volume={9},
number={7},
year={2014},
pages={403 - 414},
issn={19750080},
abstract={This paper first computes the Complexity increment by taking four complexity metrics WMC (CK), CMC (Li), CC (BS) and CCC (S&amp;B). The maintainability index of the successive version has been computed at the system level. The tracking of the number of classes added and deleted has also been obtained for the archaeology of successive versions. The understandability and the maintainability of software are then mapped with the trends of complexity increment, change in number of classes added and deleted and the Maintainability index. The complexity increments between successive versions give an indication towards the maturity level of software. These metrics are empirically evaluated with 38 versions of JFree Chart and nine versions of three live project data at the system level. &copy; 2014 SERSC.},
key={Maintainability},
keywords={Artificial intelligence;Engineering;Industrial engineering;Multimedia systems;},
note={Code complexity;Complexity metric;Complexity metrics;Maturity;Maturity levels;Number of class;Software Evolution;Understandability;},
URL={http://dx.doi.org/10.14257/ijmue.2014.9.7.34},
}


@article{20124415619742,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning better inspection optimization policies},
journal={International Journal of Software Engineering and Knowledge Engineering},
author={Lumpe, Markus and Vasa, Rajesh and Menzies, Tim and Rush, Rebecca and Turhan, Burak},
volume={22},
number={5},
year={2012},
pages={621 - 644},
issn={02181940},
abstract={Recent research has shown the value of social metrics for defect prediction. Yet many repositories lack the information required for a social analysis. So, what other means exist to infer how developers interact around their code? One option is static code metrics that have already demonstrated their usefulness in analyzing change in evolving software systems. But do they also help in defect prediction? To address this question we selected a set of static code metrics to determine what classes are most "active" (i.e., the classes where the developers spend much time interacting with each other's design and implementation decisions) in 33 open-source Java systems that lack details about individual developers. In particular, we assessed the merit of these activity-centric measures in the context of "inspection optimization" a technique that allows for reading the fewest lines of code in order to find the most defects. For the task of inspection optimization these activity measures perform as well as (usually, within 4%) a theoretical upper bound on the performance of any set of measures. As a result, we argue that activity-centric static code metrics are an excellent predictor for defects. &copy; 2012 World Scientific Publishing Company.<br/>},
key={Open systems},
keywords={Codes (symbols);Data mining;Defects;Forecasting;Inspection;Open source software;},
note={Defect prediction;Design and implementations;Inspection optimization;Recent researches;Social analysis;Software systems;Static code metrics;Static measures;},
URL={http://dx.doi.org/10.1142/S0218194012500179},
}


@article{20182805527878,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Model Change Detection With the MDL Principle},
journal={IEEE Transactions on Information Theory},
author={Yamanishi, Kenji and Fukushima, Shintaro},
volume={64},
number={9},
year={2018},
pages={6115 - 6126},
issn={00189448},
abstract={We are concerned with the issue of detecting model changes in probability distributions. We specifically consider the strategies based on the minimum description length (MDL) principle. We theoretically analyze their basic performance from the two aspects: data compression and hypothesis testing. From the view of data compression, we derive a new bound on the minimax regret for model changes. Here, the mini-max regret is defined as the minimum of the worst-case code-length relative to the least normalized maximum likelihood code-length over all model changes. From the view of hypothesis testing, we reduce the model change detection into a simple hypothesis testing problem. We thereby derive upper bounds on error probabilities for the MDL-based model change test. The error probabilities are valid for finite sample size and are related to the information-theoretic complexity as well as the discrepancy measure of the hypotheses to be tested.<br/> &copy; 2018 IEEE.},
key={Data compression},
keywords={Analytical models;Data structures;Errors;Hidden Markov models;Information theory;Learning systems;Maximum likelihood;Sampling;Testing;},
note={Change detection;Error probabilities;Hypothesis testing;MDL principle;Prediction algorithms;},
URL={http://dx.doi.org/10.1109/TIT.2018.2852747},
}


@article{20150500476435,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Framework support for the efficient implementation of multi-version algorithms},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Dias, Ricardo J. and Vale, Tiago M. and Lourenco, Joao M.},
volume={8913},
year={2015},
pages={166 - 191},
issn={03029743},
abstract={Software Transactional Memory algorithms associate metadata with the memory locations accessed during a transaction&rsquo;s lifetime. This metadata may be stored in an external table and accessed by way of a function that maps the address of each memory location with the table entry that keeps its metadata (this is the out-place or external scheme); or alternatively may be stored adjacent to the associated memory cell by wrapping them together (the in-place scheme). In transactional memory multi-version algorithms, several versions of the same memory location may exist. The efficient implementation of these algorithms requires a one-to-one correspondence between each memory location and its list of past versions, which is stored as metadata. In this chapter we address the matter of the efficient implementation of multi-version algorithms in Java by proposing and evaluating a novel in-place metadata scheme for the Deuce framework. This new scheme is based in Java Bytecode transformation techniques and its use requires no changes to the application code. Experimentation indicates that multi-versioning STM algorithms implemented using our new in-place scheme are in average 6&times; faster than when implemented with the out-place scheme.<br/> &copy; Springer International Publishing Switzerland 2015.},
key={Metadata},
keywords={Java programming language;Location;Storage allocation (computer);},
note={Application codes;Associated memory;Efficient implementation;Java byte codes;Memory locations;Multi-versioning;Software transactional memory;Transactional memory;},
}


@inproceedings{20164302945540,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={SSPARED: Saliency and sparse code analysis for rare event detection in video},
journal={2016 IEEE 12th Image, Video, and Multidimensional Signal Processing Workshop, IVMSP 2016},
author={Sarkar, Rituparna and Vaccari, Andrea and Acton, Scott T.},
year={2016},
address={Bordeaux, France},
abstract={The problem of detecting rare and unusual events in video is critical to the analysis of large video datasets. Such events are identified as those occurrences within a sequence that cause a significant change in the scene. We propose to determine the significance of a frame, while preserving its compact representation, by introducing a saliency-driven dictionary learning technique. The derived sparse codes are then leveraged, together with the Kullback-Leibler divergence, in the design of a histogram-based metric that we use to evaluate the scene changes between consecutive frames. Our method, SSPARED, is compared with two state of the art methods for anomaly detection and shows significant improvement in detecting abnormal incidents and reduced false alarm generation.<br/> &copy; 2016 IEEE.},
key={Video signal processing},
keywords={Image processing;},
note={Anomaly detection;Compact representation;Dictionary learning;Event detection;Event detection in video;Kullback Leibler divergence;saliency;Sparse representation;},
URL={http://dx.doi.org/10.1109/IVMSPW.2016.7528218},
}


@inproceedings{20151500729196,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Pseudo labels for imbalanced multi-label learning},
journal={DSAA 2014 - Proceedings of the 2014 IEEE International Conference on Data Science and Advanced Analytics},
author={Zeng, Wenrong and Chen, Xuewen and Cheng, Hong},
year={2014},
pages={25 - 31},
address={Shanghai, China},
abstract={The classification with instances which can be tagged with any of the 2<sup>L</sup>possible subsets from the predefined L labels is called multi-label classification. Multi-label classification is commonly applied in domains, such as multimedia, text, web and biological data analysis. The main challenge lying in multi-label classification is the dilemma of optimising label correlations over exponentially large label powerset and the ignorance of label correlations using binary relevance strategy (1-vs-all heuristic). The classification with label powerset usually encounters with highly skewed data distribution, called imbalanced problem. While binary relevance strategy reduces the problem from exponential to linear, it totally neglects the label correlations. In this artical, we propose a novel strategy of introducing Balanced Pseudo-Labels (BPL) which build more robust classifiers for imbalanced multi-label classification, which embeds imbalanced data in the problems innately. By incorporating the new balanced labels we aim to increase the average distances among the distinct label vectors. In this way, we also code the label correlation implicitly in the algorithm. Another advantage of the proposed method is that it can combined with any classifier and it is proportional to linear label transformation. In the experiment, we choose five multi-label benchmark data sets and compare our algorithm with the most state-of-art algorithms. Our algorithm outperforms them in standard multi-label evaluation in most scenarios.<br/> &copy; 2014 IEEE.},
key={Classification (of information)},
keywords={Linear transformations;Mathematical transformations;Text processing;},
note={Average Distance;Binary relevances;Biological data;Imbalanced data;Label correlations;Multi label classification;Multi-label learning;Novel strategies;},
URL={http://dx.doi.org/10.1109/DSAA.2014.7058047},
}


@inproceedings{20173404067527,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Apollo: Reusable Models for Fast, Dynamic Tuning of Input-Dependent Code},
journal={Proceedings - 2017 IEEE 31st International Parallel and Distributed Processing Symposium, IPDPS 2017},
author={Beckingsale, David and Pearce, Olga and Laguna, Ignacio and Gamblin, Todd},
year={2017},
pages={307 - 316},
address={Orlando, FL, United states},
abstract={Increasing architectural diversity makes performance portability extremely important for parallel simulation codes. Emerging on-node parallelization frameworks such as Kokkos and RAJA decouple the work done in kernels from the parallelization mechanism, allowing for a single source kernel to be tuned for different architectures at compile time. However, computational demands in production applications change at runtime, and performance depends both on the architecture and the input problem, and tuning a kernel for one set of inputs may not improve its performance on another. The statically optimized versions need to be chosen dynamically to obtain the best performance. Existing auto-tuning approaches can handle slowly evolving applications effectively, but are too slow to tune highly input-dependent kernels. We developed Apollo, an auto-tuning extension for RAJA that uses pre-trained, reusable models to tune input-dependent code at runtime. Apollo is designed for highly dynamic applications; it generates sufficiently low-overhead code to tune parameters each time a kernel runs, making fast decisions. We apply Apollo to two hydrodynamics benchmarks and to a production multi-physics code, and show that it can achieve speedups from 1.2x to 4.8x.<br/> &copy; 2017 IEEE.},
key={Codes (symbols)},
keywords={Architecture;Dynamics;Learning systems;Memory architecture;},
note={Autotuning;Computational demands;Different architectures;Dynamic applications;Parallel simulations;performance;Performance portability;Programming models;},
URL={http://dx.doi.org/10.1109/IPDPS.2017.38},
}


@inproceedings{20162402486031,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Rock physics modeling for waterflood simulation: A case history from the burgan field, Kuwait},
journal={3rd EAGE Workshop on Rock Physics: From Rocks to Basin - Applying Rock Physics in Prospect Evaluation and Reservoir Characterization},
author={Edwards, K. and Ebrahim, M. and Qassim, F. and Al-Asfour, S.},
year={2015},
pages={127 - 131},
address={Istanbul, Turkey},
abstract={We show with some simple code, a wide range of possible scenarios can be simulated and visualized. Based on our simulations, we conclude that a water flood in the Burgan and/or Wara formations should be visible as an amplitude change on 4D seismic with a water flood thickness of at least 15 feet.<br/>},
key={Rocks},
keywords={Floods;Oil well flooding;Reservoirs (water);},
note={4D seismic;Amplitude changes;Case history;Rock physics model;Water flood;Waterflood simulations;},
}


@article{20165103154201,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Introduction to SWAT+, A Completely Restructured Version of the Soil and Water Assessment Tool},
journal={Journal of the American Water Resources Association},
author={Bieger, Katrin and Arnold, Jeffrey G. and Rathjens, Hendrik and White, Michael J. and Bosch, David D. and Allen, Peter M. and Volk, Martin and Srinivasan, Raghavan},
volume={53},
number={1},
year={2017},
pages={115 - 130},
issn={1093474X},
abstract={SWAT+ is a completely restructured version of the Soil and Water Assessment Tool (SWAT) that was developed to face present and future challenges in water resources modeling and management and to meet the needs of the worldwide user community. It is expected to improve code development and maintenance; support data availability, analysis, and visualization; and enhance the model's capabilities in terms of the spatial representation of elements and processes within watersheds. The most important change is the implementation of landscape units and flow and pollutant routing across the landscape. Also, SWAT+ offers more flexibility than SWAT in defining management schedules, routing constituents, and connecting managed flow systems to the natural stream network. To test the basic hydrologic function of SWAT+, it was applied to the Little River Experimental Watershed (Georgia) without enhanced overland routing and compared with previous models. SWAT+ gave similar results and inaccuracies as these models did for streamflow and water balance. Taking full advantage of the new capabilities of SWAT+ regarding watershed discretization and landscape and river interactions is expected to improve simulations in future studies. While many capabilities of SWAT have already been enhanced in SWAT+ and new capabilities have been added, the model will continue to evolve in response to advancements in scientific knowledge and the demands of the growing worldwide user community. Editor's note: This paper is part of the featured series on SWAT Applications for Emerging Hydrologic and Water Quality Challenges. See the February 2017 issue for the introduction and background to the series.<br/> &copy; 2016 American Water Resources Association},
key={Rivers},
keywords={Computational methods;Data visualization;Soil conservation;Water conservation;Water management;Water quality;Watersheds;},
note={Rivers/streams;simulation;Soil and water assessment tool;Soil and Water assessment tools;Spatial representations;Water resources modeling;Watershed discretization;Watershed management;},
URL={http://dx.doi.org/10.1111/1752-1688.12482},
}


@article{20121414921927,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Verification and validation of EnergyPlus phase change material model for opaque wall assemblies},
journal={Building and Environment},
author={Tabares-Velasco, Paulo Cesar and Christensen, Craig and Bianchi, Marcus},
volume={54},
year={2012},
pages={186 - 196},
issn={03601323},
abstract={Phase change materials (PCMs) represent a technology that may reduce peak loads and HVAC energy consumption in buildings. A few building energy simulation programs have the capability to simulate PCMs, but their accuracy has not been completely tested. This study shows the procedure used to verify and validate the PCM model in EnergyPlus using a similar approach as dictated by ASHRAE Standard 140, which consists of analytical verification, comparative testing, and empirical validation. This process was valuable, as two bugs were identified and fixed in the PCM model, and version 7.1 of EnergyPlus will have a validated PCM model. Preliminary results using whole-building energy analysis show that careful analysis should be done when designing PCMs in homes, as their thermal performance depends on several variables such as PCM properties and location in the building envelope. &copy; 2012 Elsevier Ltd.},
key={Phase change materials},
keywords={Computer simulation;Energy storage;Energy utilization;Pulse code modulation;Solar buildings;},
note={Building energy simulations;Building envelope;Building envelopes;Comparative testing;Empirical validation;Energy analysis;EnergyPlus;In-buildings;Opaque walls;PCM model;Peak load;Phase Change Material (PCM);Several variables;Thermal Performance;Validation;Verification and validation;},
URL={http://dx.doi.org/10.1016/j.buildenv.2012.02.019},
}


@inproceedings{20183205669427,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An ensemble-rich multi-aspect approach for robust style change detection: Notebook for PAN at CLEF-2018},
journal={CEUR Workshop Proceedings},
author={Zlatkova, Dimitrina and Kopev, Daniel and Mitov, Kristiyan and Atanasov, Atanas and Hardalov, Momchil and Koychev, Ivan and Nakov, Preslav},
volume={2125},
year={2018},
issn={16130073},
address={Avignon, France},
abstract={We describe the winning system for the PAN@CLEF 2018 task on Style Change Detection. Given a document, the goal is to determine whether it contains style change. We present our supervised approach, which combines a TF.IDF representation of the documents with features specifically engineered for the task and which makes predictions using an ensemble of diverse models including SVM, Random Forest, AdaBoost, MLP and LightGBM. We further perform comparative analysis on the performance of the models on three different datasets, two of which we have developed for the task. Moreover, we release our code in order to enable further research.<br/>},
key={Natural language processing systems},
keywords={Adaptive boosting;Decision trees;Deep learning;},
note={Gradient boosting;Multi-authorship;Stacking ensemble;Style change;Stylometry;},
}


@inproceedings{20172603847315,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Face detection method based on histogram of sparse code in tree deformable model},
journal={Proceedings - International Conference on Machine Learning and Cybernetics},
author={Zhang, Qi and Zhou, Li-Fang and Li, Wei-Sheng and Ricanek, Karl and Li, Xin-Yi},
volume={2},
year={2017},
pages={996 - 1002},
issn={2160133X},
address={Jeju Island, Korea, Republic of},
abstract={Face detection is a challenging research area and crucial step of face detection system. Because of the factors of rotation, pose change, and complicated background, false faces also can be found in detection results. This paper puts forward a new approach based on the landmark localization to detect face image which includes various pose variation. Furthermore, the proposed histogram of sparse code-based method is very effective and it can capture global elastic and multi-view deformation which can be optimized easily. The proposed method achieved higher effectiveness and efficiency in comparison with the existing face detection methods on different data sets.<br/> &copy; 2016 IEEE.},
key={Face recognition},
keywords={Artificial intelligence;Codes (symbols);Graphic methods;Learning systems;},
note={Deformable modeling;Deformable part models;Effectiveness and efficiencies;Face detection methods;Face detection system;Landmark localization;Pose variation;Sparse codes;},
URL={http://dx.doi.org/10.1109/ICMLC.2016.7873015},
}


@inproceedings{20154001326511,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={How the evolution of emerging collaborations relates to code changes: An empirical study},
journal={22nd International Conference on Program Comprehension, ICPC 2014 - Proceedings},
author={Panichella, Sebastiano and Canfora, Gerardo and Di Penta, Massimiliano and Oliveto, Rocco},
year={2014},
pages={177 - 188},
address={Hyderabad, India},
abstract={Developers contributing to open source projects spontaneously group into " emerging" teams, reected by messages ex- changed over mailing lists, issue trackers and other commu- nication means. Previous studies suggested that such teams somewhat mirror the software modularity. This paper empirically investigates how, when a project evolves, emerging teams re-organize themselves|e.g., by splitting or merging. We relate the evolution of teams to the files they change, to investigate whether teams split to work on cohesive groups of files. Results of this study|conducted on the evolution history of four open source projects, namely Apache httpd, Eclipse JDT, Netbeans, and Samba|provide indications of what happens in the project when teams reorganize. Specifically, we found that emerging team splits imply working on more cohesive groups of files and emerging team merges imply working on groups of files that are cohesive from structural perspective. Such indications serve to better under- stand the evolution of software projects. More important, the observation of how emerging teams change can serve to suggest software remodularization actions.<br/> Copyright &copy; 2014 ACM.},
key={Open source software},
keywords={Computer programming;Open systems;},
note={Developers' communications;Emerging collaboration;Empirical studies;Evolution history;Mining software repositories;Open source projects;Software modularity;Software project;},
URL={http://dx.doi.org/10.1145/2597008.2597145},
}


@article{20183205675715,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A brief history of parameterized matching problems},
journal={Discrete Applied Mathematics},
author={Mendivelso, Juan and Thankachan, Sharma V. and Pinzon, Yoan},
year={2018},
issn={0166218X},
abstract={Parameterized pattern matching is a string searching variant that was initially defined to detect duplicate code but later proved to support several other applications. In particular, two equal-length strings X andY are a parameterized-match if there exists a bijective function g for which every text symbol in X is equal to g(Y). Baker was the first researcher to have addressed this problem (Baker, 1993) and, since then, many others have followed her work. She did, indeed, open up a wide field of extensive research. Over the years, many variants and extensions that have been pursued include: parameterized matching under edit and Hamming distances, parameterized multi-pattern matching, two dimensional parameterized matching, structural matching, function matching, and the very recent developments in succinct and streaming models. This accelerated research could only be justified by the usefulness of its practical applications such as in software maintenance, image processing and bioinformatics to name some. Even though the problem was posed about 25 years ago, research on parameterized matching is still very active. Its extensive study over the years and its current relevance motivate us to review the most notorious contributions as road map for current and future research.<br/> &copy; 2018 Elsevier B.V.},
key={Parameterization},
keywords={Application programs;Computer software maintenance;Hamming distance;Image processing;Pattern matching;},
note={Bijective functions;Function matching;Multi-pattern matching;Parameterized matching;Streaming model;String matching;String-searching;Structural matching;},
URL={http://dx.doi.org/10.1016/j.dam.2018.07.017},
}


@inproceedings{20183405719830,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Do developers update third-party libraries in mobile apps?},
journal={Proceedings - International Conference on Software Engineering},
author={Salza, Pasquale and Palomba, Fabio and Di Nucci, Dario and D'Uva, Cosmo and De Lucia, Andrea and Ferrucci, Filomena},
year={2018},
pages={255 - 265},
issn={02705257},
address={Gothenburg, Sweden},
abstract={One of the most common strategies to develop new software is to take advantage of existing source code, which is available in comprehensive packages called third-party libraries. As for all software systems, even these libraries change to offer new functionalities and fix bugs or security issues. The way the changes are propagated has been studied by researchers, interested in understanding their impact on the non-functional attributes of the systems source code. While the research community mainly focused on the change propagation phenomenon in the context of traditional applications, only little is known regarding the mobile context. In this paper, we aim at bridging this gap by conducting an empirical study on the evolution history of 291 mobile apps, by investigating (i) whether mobile developers actually update third-party libraries, (ii) which are the categories of libraries with respect to the developers' proneness to update their apps, (iii) what are the common patterns followed by developers when updating a software library, and (iv) whether high-and low-rated apps present peculiar update patterns. The results of the study showed that mobile developers rarely update their apps with respect to the used libraries, and when they do, they mainly tend to update the libraries related to the Graphical User Interface, with the aim of keeping the mobile apps updated with the latest design tendencies. In some cases developers ignore updates because of a poor awareness of the benefits, or a too high cost/benefit ratio. Finally, high-and low-rated apps present strong differences.<br/> &copy; 2018 ACM.},
key={Program debugging},
keywords={Application programming interfaces (API);Graphical user interfaces;Libraries;Software engineering;},
note={API usage;Change propagation;Empirical studies;Evolution history;Mining software repositories;Research communities;Software libraries;Third parties;},
URL={http://dx.doi.org/10.1145/3196321.3196341},
}


@inproceedings{20134216854496,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Extracting artifact lifecycle models from metadata history},
journal={2013 1st International Workshop on Data Analysis Patterns in Software Engineering, DAPSE 2013 - Proceedings},
author={Baysal, Olga and Kononenko, Oleksii and Holmes, Reid and Godfrey, Michael W.},
year={2013},
pages={17 - 19},
address={San Francisco, CA, United states},
abstract={Software developers and managers make decisions based on the understanding they have of their software systems. This understanding is both built up experientially and through investigating various software development artifacts. While artifacts can be investigated individually, being able to summarize characteristics about a set of development artifacts can be useful. In this paper we propose lifecycle models as an effective way to gain an understanding of certain development artifacts. Lifecycle models capture the dynamic nature of how various development artifacts change over time in a graphical form that can be easily understood and communicated. Lifecycle models enables reasoning of the underlying processes and dynamics of the artifacts being analyzed. In this paper we describe how lifecycle models can be generated and demonstrate how they can be applied to the code review process of a development project. &copy; 2013 IEEE.<br/>},
key={Software design},
keywords={Data handling;Information analysis;Life cycle;},
note={Code review;Development project;Dynamic nature;Graphical forms;Life cycle model;Software developer;Software systems;},
URL={http://dx.doi.org/10.1109/DAPSE.2013.6603803},
}


@inproceedings{20160801960313,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Displaying people with old addresses on a map},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Zhang, Gang and Murakami, Harumi},
volume={9460},
year={2015},
pages={381 - 386},
issn={03029743},
address={Brisbane, QLD, Australia},
abstract={This paper proposes a method of converting old addresses to current addresses for geocoding, with the aim of displaying on a map people who have such old addresses. Existing geocoding services often fail to handle old addresses since the names of towns, cities, or prefectures can be different from those of current addresses. To solve this geocoding problem, we focus on postal codes, extracting them from Web search result snippets using the query &ldquo;prefecture name AND important place name AND postal code.&rdquo; The frequency of postal codes and the edit distance between the old address and the addresses obtained using the postal codes are used to judge the most suitable postal code and thus the corresponding current address. The effectiveness of the proposed method is evaluated in an experiment using a relative dataset. A prototype system was implemented in which users could display people using their birthdate and birthplace addresses on a map chronologically with an associated history chart.<br/> &copy; Springer International Publishing Switzerland 2015.},
key={Codes (symbols)},
keywords={Information retrieval;Infrared devices;},
note={Edit distance;Geo coding;Old address;People searches;Postal code;Prototype system;Web search results;},
URL={http://dx.doi.org/10.1007/978-3-319-28940-3_30},
}


@article{20163002650241,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Collective personalized change classification with multiobjective search},
journal={IEEE Transactions on Reliability},
author={Xia, Xin and Lo, David and Wang, Xinyu and Yang, Xiaohu},
volume={65},
number={4},
year={2016},
pages={1810 - 1829},
issn={00189529},
abstract={Many change classification techniques have been proposed to identify defect-prone changes. These techniques consider all developers' historical change data to build a global prediction model. In practice, since developers have their own coding preferences and behavioral patterns, which causes different defect patterns, a separate change classification model for each developer can help to improve performance. Jiang, Tan, and Kim refer to this problem as personalized change classification, and they propose PCC+ to solve this problem. A software project has a number of developers; for a developer, building a prediction model not only based on his/her change data, but also on other relevant developers' change data can further improve the performance of change classification. In this paper, we propose a more accurate technique named collective personalized change classification (CPCC), which leverages a multiobjective genetic algorithm. For a project, CPCC first builds a personalized prediction model for each developer based on his/her historical data. Next, for each developer, CPCC combines these models by assigning different weights to these models with the purpose of maximizing two objective functions (i.e., F1-scores and cost effectiveness). To further improve the prediction accuracy, we propose CPCC+ by combining CPCC with PCC proposed by Jiang, Tan, and Kim To evaluate the benefits of CPCC+ and CPCC, we perform experiments on six large software projects from different communities: Eclipse JDT, Jackrabbit, Linux kernel, Lucene, PostgreSQL, and Xorg. The experiment results show that CPCC+ can discover up to 245 more bugs than PCC+ (468 versus 223 for PostgreSQL) if developers inspect the top 20% lines of code that are predicted buggy. In addition, CPCC+ can achieve F1-scores of 0.60-0.75, which are statistically significantly higher than those of PCC+ on all of the six projects.<br/> &copy; 1963-2012 IEEE.},
key={Cost effectiveness},
keywords={Computer operating systems;Defects;Forecasting;Genetic algorithms;Learning systems;},
note={Classification models;Classification technique;developer;Improve performance;Multi-objective genetic algorithm;Multiobjective search;personalized change classification (PCC);Prediction accuracy;},
URL={http://dx.doi.org/10.1109/TR.2016.2588139},
}


@article{20154801617706,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Experimental characterisation of sub-cooling in hydrated salt phase change materials},
journal={Applied Thermal Engineering},
author={Taylor, Robert A. and Tsafnat, Naomi and Washer, Alex},
volume={93},
year={2016},
pages={935 - 938},
issn={13594311},
abstract={Phase change materials (PCMs) allow storage of large amounts of energy within a narrow temperature range via their latent heat. This is useful for applications where the outside environment swings above and below the nominal temperature range, enabling the design of passively regulated thermal systems. This short communication presents an experimental characterisation of two proprietary hydrated calcium chloride-based salt materials designed for maintaining temperatures of 25-30 &deg;C for building/enclosure temperature stability. It was found that materials' thermal performance is critically influenced by their rate of cooling. Using a T-history method, the experiments revealed that these salts undergo high specific enthalpy changes across a broad temperature range (e.g. up to 1 MJ/kg, which is 5-10 times their latent heat), but that up to 10 &deg;C of sub-cooling and long nucleation times are possible, depending on their rate of cooling. This communication reveals that careful operation is needed to ensure that these materials achieve control within the desired temperature range.<br/> &copy; 2015 Elsevier Ltd.},
key={Phase change materials},
keywords={Calcium chloride;Characterization;Cooling;Freezing;Hydration;Latent heat;Nucleation;Pulse code modulation;Salts;},
note={Broad temperature ranges;Hydrated salt;Maintaining temperatures;Narrow temperature ranges;Phase Change;Subcoolings;Temperature stability;Thermal Performance;},
URL={http://dx.doi.org/10.1016/j.applthermaleng.2015.10.032},
}


@inproceedings{20162702569499,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mining API expertise profiles with Partial Program Analysis},
journal={ACM International Conference Proceeding Series},
author={Mani, Senthil and Padhye, Rohan and Sinha, Vibha Singhal},
volume={18-20-February-2016},
year={2016},
pages={109 - 118},
address={Goa, India},
abstract={A developer's API usage expertise can be estimated by analyzing source code that they have checked-in to a software repository. In prior work we proposed a system for creating a social network of developers centered around the APIs they use in order to recommend people and projects they might be interested in. The implementation of such a system requires analyzing code from repositories of large numbers of projects that use different build systems. Hence, one challenge is to determine the APIs referenced in code in these repositories without relying on the ability to resolve every project's external dependencies. In this paper, we consider a technique called Partial Program Analysis for resolving type bindings in Java source code in the absence of third-party library binaries. Another important design decision concerns the approach of associating such API references with the developers who authored them such as walking entire change history or use blame information. We evaluate these different design options on 4 open-source Java projects and found that both Partial Program Analysis and blame-based approach provide precision greater than 80%. However, use of blame as opposed to complete program history leads to significant recall loss, in most cases greater than 40%.<br/> &copy; 2016 ACM.},
key={Java programming language},
keywords={Application programming interfaces (API);Codes (symbols);Open source software;},
note={API usage expertise;Blame;Build systems;Change history;Design decisions;Java source codes;Program analysis;Software repositories;},
URL={http://dx.doi.org/10.1145/2856636.2856646},
}


@inproceedings{20150600493028,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Spatio-temporal analysis of GPS tracks of CODE RED: MOBILE an experimental mobile scenario and location based training exercise},
journal={CEUR Workshop Proceedings},
author={Quinn, P.B. and Cartwright, W.E.},
volume={1328},
year={2012},
issn={16130073},
address={Melbourne, VIC, Australia},
abstract={As part of an ongoing research project, geovisualisations of bushfires were delivered at GPS-determined locations to volunteer fire fighters from the Country Fire Authority's Macedon Ranges Group. The participants skill level ranged from basic wildfire firefighter trained through to captain of brigade. The location-based scenario training exercise is called CODE RED: MOBILE. Using information from the geovisualisations about a virtual bushfire at Hanging Rock, participants selected which houses would likely burn down after a wind change. They were free to take any path to reach the virtual houses, indicated by markers on the screen of an iPad New. They were asked to go to the virtual house location to observe the real landscape and to estimate where the virtual fire would go. Most participants took about an hour to complete the exercise. A GPS device kept track of where they went. A Fractal D score was assigned to participant's tracks using Vilis O. Nams' software: Fractal 5.20.0. Spatio-temporal analysis of the GPS tracks using ArcMap 10 and Geotime 5.3 found that participants undertook the exercise by following unusual tracks. Preliminary results showed that some of these participants, not following test procedure instructions closely, had sometimes undertaken more direct tracks, shown by low Fractal D scores. However, they were able to choose the correct houses assigned to visit. This type of analysis can assist in improving the design of mobile, location based exercises. It can also provide an additional means of assessing and improving firefighter performance. This paper will outline the background behind the exercise, specify the type of information that was sought and provides details of the results obtained through analysis.<br/>},
key={Location},
keywords={Fire extinguishers;Fractals;Heating;Houses;Personnel training;Testing;},
note={Bushfires;Firefighters;Heatmaps;Mobile Learning;Scenario-based training;Spatiotemporal analysis;},
}


@article{20154901646074,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Round-Robin Test of Paraffin Phase-Change Material},
journal={International Journal of Thermophysics},
author={Vidi, S. and Mehling, H. and Hemberger, F. and Haussmann, Th. and Laube, A.},
volume={36},
number={10-11},
year={2015},
pages={2518 - 2522},
issn={0195928X},
abstract={A round-robin test between three institutes was performed on a paraffin phase-change material (PCM) in the context of the German quality association for phase-change materials. The aim of the quality association is to define quality and test specifications for PCMs and to award certificates for successfully tested materials. To ensure the reproducibility and comparability of the measurements performed at different institutes using different measuring methods, a round-robin test was performed. The sample was unknown. The four methods used by the three participating institutes in the round-robin test were differential scanning calorimetry, Calvet calorimetry and three-layer calorimetry. Additionally, T-history measurements were made. The aim of the measurements was the determination of the enthalpy as a function of temperature. The results achieved following defined test specifications are in excellent agreement.<br/> &copy; 2014, Springer Science+Business Media New York.},
key={Phase change materials},
keywords={Calorimeters;Calorimetry;Differential scanning calorimetry;Enthalpy;Paraffins;Pulse code modulation;Routers;Specifications;Superconducting tapes;Testing;},
note={Calvet calorimetry;Measuring method;Paraffin phase-change material;Reproducibilities;Round Robin test;Test specifications;Three-layer;},
URL={http://dx.doi.org/10.1007/s10765-014-1754-6},
}


@inproceedings{20152500951159,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A slice-based estimation approach for maintenance effort},
journal={Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014},
author={Alomari, Hakam W. and Collard, Michael L. and Maletic, Jonathan I.},
year={2014},
pages={81 - 90},
address={Victoria, BC, Canada},
abstract={Program slicing is used as a basis for an approach to estimate maintenance effort. A case study of the GNU Linux kernel with over 900 versions spanning 17 years of history is presented. For each version a system dictionary is built using a lightweight slicing approach and encodes the forward decomposition static slice profiles for all variables in all the files in the system. Changes to the system are then modeled at the behavioral level using the difference between the system dictionaries of two versions. The three different granularities of slice (i.e., line, function, and file) are analyzed. We use a direct extension of srcML to represent computed change information. The retrieved information reflects the fact that additional knowledge of the differences can be automatically derived to help maintainers understand code changes. We consider the hypotheses: (1) The structured format helps create traceability links between the changes and other software artifacts. (2) This model is predictive of maintenance effort. The results demonstrate that the approach accurately predicts effort in a scalable manner.<br/> &copy; 2014 IEEE.},
key={Computer software maintenance},
keywords={Linux;Open source software;},
note={Additional knowledge;Different granularities;Effort Estimation;Estimation approaches;Maintenance efforts;Program slicing;Software artifacts;Software metrics;},
URL={http://dx.doi.org/10.1109/ICSME.2014.30},
}


@inproceedings{20163902840775,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Developer recommendation with awareness of accuracy and cost},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
author={Liu, Jin and Tian, Yiqiuzi and Hong, Liang and Xu, Zhou},
volume={2016-January},
year={2016},
pages={213 - 218},
issn={23259000},
address={Redwood City, CA, United states},
abstract={As the scale and complexity of software products increase, software maintenance on bug resolution has become a challenging work. In the process of software implementation, developers often use bug reports, source code and change history to help solve bugs. However, hundreds of bug reports are being submitted every day. It is time-consuming and effortless for developers to review all the bug reports. To facilitate the assignment of bug reports, existing developer recommendation systems typically recommend the developer who has the fullest potential. However, bug reports are highly varied; time that the developers may spend fixing them is also important. To address the problem of developer recommendation, we propose a developer recommendation system with awareness of accuracy and cost (DRAC). This recommendation system is based on modern portfolio theory by striking a balance between accuracy and cost (time). We evaluate our approach with experiments on data collected from Bugzilla1.<br/> Copyright &copy; 2016 by KSI Research Inc. and Knowledge Systems Institute Graduate School.},
key={Program debugging},
keywords={Costs;Knowledge engineering;Recommender systems;Software engineering;},
note={Bug triage;Change history;Developer recommendations;Modern portfolio theories;Portfolio theories;Software implementation;Software products;Source codes;},
URL={http://dx.doi.org/10.18293/SEKE2016-125},
}


@inproceedings{20162802578624,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Development of mobile serious game for self-assessment as base for a game-editor for teachers},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Herrler, Andreas and Grubert, Simon and Kajzer, Marko and Behrens, Sadie and Klamma, Ralf},
volume={9599},
year={2016},
pages={71 - 79},
issn={03029743},
address={Rome, Italy},
abstract={Self-assessment is an important tool to improve student performance. A good serious game (SG) would be a very strong self-assessment tool because of the additional motivational aspects. However, self-assessment tools have to fit the learning matters of a course by 100%, what general SG seldom do. The goal of this work is to build a simple SG editor each teacher can adjust to the learning goals of his course. Here we present the development of a game-editor for teachers for a puzzle-style game. By this editor teachers can edit their own course based game without sophisticated computer knowledge. The game mechanics meet the requirements of mobile- and micro-learning strategies. Furthermore, the game implements learning analytics for students as well as for teachers. The game engine and the editor are both based on standard Web technologies. The source code is maintained as an open source project to lower the barriers for further uptake.<br/> &copy; Springer International Publishing Switzerland 2016.},
key={Serious games},
keywords={Open source software;Open systems;Teaching;},
note={Computer knowledge;Learning analytics;Micro-learning;Open source projects;Self assessment;Self-assessment tools;Student performance;Web technologies;},
URL={http://dx.doi.org/10.1007/978-3-319-40216-1_8},
}


@inproceedings{20161602266675,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An ensemble SVM model for the accurate prediction of non-canonical MicroRNA targets},
journal={BCB 2015 - 6th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
author={Ghoshal, Asish and Grama, Ananth and Bagchi, Saurabh and Chaterji, Somali},
year={2015},
pages={403 - 412},
address={Atlanta, GA, United states},
abstract={Background MicroRNAs are small non-coding endogenous RNAs that are responsible for post-transcriptional regulation of genes. Given that large numbers of human genes are targeted by microRNAs, understanding the precise mechanism of microRNA action and accurately mapping their targets is of paramount importance; this will uncover the role of microRNAs in development, difierentiation, and disease pathogenesis. However, the current state-of-the-art computational methods for microRNA target prediction suffer from high false-positive rates to be useful in practice. Results In this paper, we develop a suite of models for microRNA target prediction, under the banner Avishkar, that have superior prediction performance over the state-of-the-art protocols. Specifically, our final model developed in this paper achieves an average true positive rate of more than 75%, when keeping the false positive rate of 20%, for non-canonical microRNA target sites in humans. This is an improvement of over 150% in the true positive rate for non-canonical sites, over the best competitive protocol. We are able to achieve such superior performance by representing the thermodynamic and sequence profiles of microRNA-mRNA interaction as curves, coming up with a novel metric of seed enrichment to model seed matches as well as all possible non-canonical matches, and learning an ensemble of microRNA family-specific non-linear SVM classifiers. We provide an easy-to-use system, built on top of Apache Spark, for large-scale interactive analysis and prediction of microRNA targets. All operations in our system, namely candidate set generation, feature generation and transformation, training, prediction and computing performance metrics are fully distributed and are scalable. Availability All source code and sample data is available at https://bitbucket.org/cellsandmachines/avishkar. We also provide scalable implementations of kernel SVM using Apache Spark, which can be used to solve large-scale non-linear binary classification problems at https: //bitbucket.org/cellsandmachines/kernelsvmspark.<br/> Copyright 2015 ACM.},
key={RNA},
keywords={Bioinformatics;Distributed computer systems;Forecasting;Genes;HTTP;Learning systems;Nucleic acids;},
note={Distributed machine learning;Kernel SVM;Large-scale;MicroRNAs;mRNA;Non-canonical matches;Target prediction;},
URL={http://dx.doi.org/10.1145/2808719.2808761},
}


@inproceedings{20184806133623,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Visualizations of evolving graphical models in the context of model review},
journal={Proceedings - 21st ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2018},
author={Zoubek, Florian and Langer, Philip and Mayerhofer, Tanja},
year={2018},
pages={381 - 391},
address={Copenhagen, Denmark},
abstract={Code reviewing is well recognized as a valuable software engineering practice for improving software quality. Today a large variety of tools exist that support code reviewing and are widely adopted in open source and commercial software projects. They commonly support developers in manually inspecting code changes, providing feedback on and discussing these code changes, as well as tracking the review history. As source code is usually text-based, code reviewing tools also only support text-based artifacts. Hence, code changes are visualized textually and review comments are attached to text passages. This renders them unsuitable for reviewing graphical models, which are visualized graphically in diagrams instead of textually and hence require graphical change visualizations as well as annotation capabilities on the diagram level. Consequently, developers currently have to switch back and forth between code reviewing tools and comparison tools for graphical models to relate reviewer comments to model changes. Furthermore, adding and discussing reviewer comments on the diagram level is simply not possible. To improve this situation, we propose a set of coordinated visualizations of reviewing-relevant information for graphical models including model changes, diagram changes, review comments, and review history. The proposed visualizations have been implemented in a prototype tool called Mervin supporting the reviewing of graphical UML models developed with Eclipse Papyrus. Using this prototype, the proposed visualizations have been evaluated in a user study concerning effectiveness. The evaluation results show that the proposed visualizations can improve the review process of graphical models in terms of issue detection.<br/> &copy; 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
key={Open systems},
keywords={Codes (symbols);Computer software selection and evaluation;FORTH (programming language);Open source software;Unified Modeling Language;Visualization;},
note={Commercial software;Evaluation results;GraphicaL model;Model reviews;Prototype tools;Review process;Software engineering practices;Software Quality;},
URL={http://dx.doi.org/10.1145/3239372.3239403},
}


@inproceedings{20131316141036,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Answering software evolution questions: An empirical evaluation},
journal={Information and Software Technology},
author={Hattori, Lile and D'Ambros, Marco and Lanza, Michele and Lungu, Mircea},
volume={55},
number={4},
year={2013},
pages={755 - 775},
issn={09505849},
abstract={Context: Developers often need to find answers to questions regarding the evolution of a system when working on its code base. While their information needs require data analysis pertaining to different repository types, the source code repository has a pivotal role for program comprehension tasks. However, the coarse-grained nature of the data stored by commit-based software configuration management systems often makes it challenging for a developer to search for an answer. Objective: We present Replay, an Eclipse plug-in that allows developers to explore the change history of a system by capturing the changes at a finer granularity level than commits, and by replaying the past changes chronologically inside the integrated development environment, with the source code at hand. Method: We conducted a controlled experiment to empirically assess whether Replay outperforms a baseline (SVN client in Eclipse) on helping developers to answer common questions related to software evolution. Results: The experiment shows that Replay leads to a decrease in completion time with respect to a set of software evolution comprehension tasks. Conclusion: We conclude that there are benefits in using Replay over the state of the practice tools for answering questions that require fine-grained change information and those related to recent changes. &copy; 2012 Elsevier B.V. All rights reserved.<br/>},
key={Information management},
keywords={Codes (symbols);Search engines;},
note={Controlled experiment;Empirical evaluations;Mining software repositories;Software change;Software Evolution;},
URL={http://dx.doi.org/10.1016/j.infsof.2012.09.001},
}


@inproceedings{20162902611743,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Evolution of XSD documents and their variability during project life cycle: A preliminary study},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={De Almeida, Diego Benincasa Fernandes Cavalcanti and Guerra, Eduardo Martins},
volume={9789},
year={2016},
pages={392 - 406},
issn={03029743},
address={Beijing, China},
abstract={During a software system life cycle, project modifications occur for different reasons. Regarding web services, communication contracts modifications are equally common, which induces the need for adaptation in every system node. To help reduce the contracts changing impact over software source code, it is necessary to understand how these contract changes occur. This paper presents a preliminary study on the evaluation of the change history of different open-source projects that defines XSD documents, specifying metrics for such files, extracting them by software repository mining and analyzing their evolution during the project life cycle. Based on the results, and considering that Web Service Definition Language (WSDL) contracts use XSD, a deeper study focused on web services projects only is further proposed to assess what exactly is changed at each contract revision, possibly revealing changing tendencies to support easy-to-adapt web service development.<br/> &copy; Springer International Publishing Switzerland 2016.},
key={Web services},
keywords={Contracts;Life cycle;Open source software;Open systems;Websites;XML;},
note={Change history;Open source projects;Project life cycle;Service development;Software repository mining;Software source codes;Software systems;Web service definition languages;},
URL={http://dx.doi.org/10.1007/978-3-319-42089-9_28},
}


@article{20172603843790,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The road code: encouraging more efficient driving practices in New Zealand},
journal={Energy Efficiency},
author={Scott, Michelle Grace and Lawson, Rob},
volume={11},
number={7},
year={2018},
pages={1617 - 1626},
issn={1570646X},
abstract={Road transport contributes a significant amount towards New Zealand&rsquo;s carbon emissions, mostly from light vehicles. These emissions could be partly reduced by an increase in more efficient driving practices, and reductions of 10&ndash;20% of fuel are possible without increasing trip times significantly. This study was conducted to understand whether people knew how to drive efficiently, whether they actually ever drove in an efficient manner and what ways there could be to influence people to drive more efficiently. Focus groups were conducted across New Zealand in urban and rural areas with groups of students, young professionals, parents and older people in order to cover different lifestyles and environments. These focus groups covered a wide range of topics including knowledge and practices of efficient driving, learning to drive, infrastructure and aspirations. Our results show that most people reported knowing the things they could do to be more fuel-efficient, however, despite this knowledge, they very rarely engaged in these practices. When they did consider fuel efficiency, it was almost always linked to saving fuel costs and environmental aspects were not considered. There is a clear lack of connection between carbon emissions and driving when people are in their cars. Better messages could be presented to drivers linking their driving practices to carbon emissions and therefore climate change. There are a range of other options where more efficient practices and choices could be encouraged depending on context, the driver and their way of life.<br/> &copy; 2017, The Author(s).},
key={Fuels},
keywords={Carbon;Climate change;Energy efficiency;Roads and streets;},
note={Behaviour;Carbon emissions;Efficient driving;Environmental aspects;Fuel efficiency;Transport;Urban and rural areas;Young professionals;},
URL={http://dx.doi.org/10.1007/s12053-017-9538-z},
}


@inproceedings{20173504080867,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A Clairvoyant Approach to Evaluating Software (In)Security},
journal={Proceedings of the Workshop on Hot Topics in Operating Systems - HOTOS},
author={Jain, Bhushan and Tsai, Chia-Che and Porter, Donald E.},
volume={Part F129307},
year={2017},
pages={62 - 68},
address={Whistler, BC, Canada},
abstract={Nearly all modern software has security flaws - -either known or unknown by the users. However, metrics for evaluating software security (or lack thereof) are noisy at best. Common evaluation methods include counting the past vulnerabilities of the program, or comparing the size of the Trusted Computing Base (TCB), measured in lines of code (LoC) or binary size. Other than deleting large swaths of code from project, it is difficult to assess whether a code change decreased the likelihood of a future security vulnerability. Developers need a practical, constructive way of evaluating security. This position paper argues that we actually have all the tools needed to design a better, empirical method of security evaluation. We discuss related work that estimates the severity and vulnerability of certain attack vectors based on code properties that can be determined via static analysis. This paper proposes a grand, unified model that can predict the risk and severity of vulnerabilities in a program. Our prediction model uses machine learning to correlate these code features of open-source applications with the history of vulnerabilities reported in the CVE (Common Vulnerabilities and Exposures) database. Based on this model, one can incorporate an analysis into the standard development cycle that predicts whether the code is becoming more or less prone to vulnerabilities.<br/> &copy; 2017 ACM.},
key={Static analysis},
keywords={Codes (symbols);Learning systems;Open source software;Open systems;},
note={Common vulnerabilities and exposures;Evaluating software;Evaluation methods;Open source application;Security evaluation;Security vulnerabilities;Standard development;Trusted computing base;},
URL={http://dx.doi.org/10.1145/3102980.3102991},
}


@inproceedings{20152100878238,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Extracting dependencies from software changes: An industry experience report},
journal={Proceedings - 2014 Joint Conference of the International Workshop on Software Measurement, IWSM 2014 and the International Conference on Software Process and Product Measurement, Mensura 2014},
author={Wetzlmaier, Thomas and Klammer, Claus and Ramler, Rudolf},
year={2014},
pages={163 - 168},
address={Rotterdam, Netherlands},
abstract={Retrieving and analyzing information from software repositories and detecting dependencies are important tasks supporting software evolution. Dependency information is used for change impact analysis, defect prediction as well as cohesion and coupling measurement. In this paper we report our experience from extracting dependency information from the change history of a commercial software system. We analyzed the software system's evolution of about six years, from the start of development to the transition to product releases and maintenance. Analyzing the co-evolution of software artifacts allows detecting logical dependencies between system parts implemented with heterogeneous technologies as well as between different types of development artifacts such as source code, data models or documentation. However, the quality of the extracted dependencies relies on established development practices and conformance to a defined change process. In this paper we indicate resulting limitations and recommend further processing and filtering steps to prepare the dependency data for subsequent analysis and measurement activities.<br/> &copy; 2014 IEEE.},
key={Computer software},
keywords={Measurement;},
note={Change history;Change impact analysis;Cohesion and couplings;Commercial software systems;Dependency analysis;Dependency informations;Heterogeneous technology;Mining software repositories;},
URL={http://dx.doi.org/10.1109/IWSM.Mensura.2014.12},
}


@inproceedings{20142817914776,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Active files as a measure of software maintainability},
journal={36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings},
author={Schulte, Lukas and Sajnani, Hitesh and Czerwonka, Jacek},
year={2014},
pages={34 - 43},
address={Hyderabad, India},
abstract={In this paper, we explore the set of source files which are changed unusually often. We define these files as active files. Although discovery of active files relies only on version history and defect classification, the simple concept of active files can deliver key insights into software development activities. Active files can help focus code reviews, implement targeted testing, show areas for potential merge conflicts and identify areas that are central for program comprehension. In an empirical study of six large software systems within Microsoft ranging from products to services, we found that active files constitute only between 2-8% of the total system size, contribute 20-40% of system file changes, and are responsible for 60-90% of all defects. Not only this, but we establish that the majority, 65-95%, of the active files are architectural hub files which change due to feature addition as opposed to fixing defects. Copyright &copy; 2014 ACM.<br/>},
key={Software testing},
keywords={Defects;Maintainability;Risks;Software design;},
note={Active file;Defect classification;Large software systems;Program comprehension;Software maintainability;Software metrics;Source files;Technical debts;},
URL={http://dx.doi.org/10.1145/2591062.2591176},
}


@inproceedings{20174704429697,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using LSTMs to model the java programming language},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Boldt, Brendon},
volume={10614 LNCS},
year={2017},
pages={268 - 275},
issn={03029743},
address={Alghero, Italy},
abstract={Recurrent neural networks (RNNs), specifically long-short term memory networks (LSTMs), can model natural language effectively. This research investigates the ability for these same LSTMs to perform next &ldquo;word&rdquo; prediction on the Java programming language. Java source code from four different repositories undergoes a transformation that preserves the logical structure of the source code and removes the code&rsquo;s various specificities such as variable names and literal values. Such datasets and an additional English language corpus are used to train and test standard LSTMs&rsquo; ability to predict the next element in a sequence. Results suggest that LSTMs can effectively model Java code achieving perplexities under 22 and accuracies above 0.47, which is an improvement over LSTM&rsquo;s performance on the English language which demonstrated a perplexity of 85 and an accuracy of 0.27. This research can have applicability in other areas such as syntactic template suggestion and automated bug patching.<br/> &copy; Springer International Publishing AG 2017.},
key={Java programming language},
keywords={Codes (symbols);Learning systems;Long short-term memory;},
note={English languages;Java source codes;Logical structure;Natural languages;Recurrent neural network (RNNs);Short term memory;Source codes;Test standards;},
URL={http://dx.doi.org/10.1007/978-3-319-68612-7_31},
}


@inproceedings{20161702279426,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Coarse grain parallelization of deep neural networks},
journal={Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP},
author={Tallada, Marc Gonzalez},
volume={12-16-March-2016},
year={2016},
pages={Association for Computing Machinery (ACM) SIGPLAN - },
address={Barcelona, Spain},
abstract={Deep neural networks (DNN) have recently achieved extraordinary results in domains like computer vision and speech recognition. An essential element for this success has been the introduction of high performance computing (HPC) techniques in the critical step of training the neural network. This paper describes the implementation and analysis of a network-agnostic and convergence-invariant coarse-grain parallelization of the DNN training algorithm. The coarse-grain parallelization is achieved through the exploitation of the batch-level parallelism. This strategy is independent from the support of specialized and optimized libraries. Therefore, the optimization is immediately available for accelerating the DNN training. The proposal is compatible with multi-GPU execution without altering the algorithm convergence rate. The parallelization has been implemented in Caffe, a state-of-the-art DNN framework. The paper describes the code transformations for the parallelization and we also identify the limiting performance factors of the approach. We show competitive performance results for two state-of-the-art computer vision datasets, MNIST and CIFAR-10. In particular, on a 16-core Xeon E5-2667v2 at 3.30GHz we observe speedups of 8&times; over the sequential execution, at similar performance levels of those obtained by the GPU optimized Caffe version in a NVIDIA K40 GPU.<br/> &copy; 2016 ACM.},
key={Deep neural networks},
keywords={Application programming interfaces (API);Computer vision;Cosine transforms;Deep learning;Neural networks;Optimization;Parallel programming;Speech recognition;Stochastic systems;},
note={Algorithm convergence;Code transformation;Competitive performance;High performance computing (HPC);Limiting performance;OpenMP;Sequential execution;Stochastic gradient descent;},
URL={http://dx.doi.org/10.1145/2851141.2851158},
}


@inproceedings{20133616708580,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Designing linear algebra algorithms by transformation: Mechanizing the expert developer},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Marker, Bryan and Poulson, Jack and Batory, Don and Van De Geijn, Robert},
volume={7851 LNCS},
year={2013},
pages={362 - 378},
issn={03029743},
address={Kobe, Japan},
abstract={To implement dense linear algebra algorithms for distributed-memory computers, an expert applies knowledge of the domain, the target architecture, and how to parallelize common operations. This is often a rote process that becomes tedious for a large collection of algorithms. We have developed a way to encode this expert knowledge such that it can be applied by a system to generate mechanically the same (and sometimes better) highly-optimized code that an expert creates by hand. This paper illustrates how we have encoded a subset of this knowledge and how our system applies it and searches a space of generated implementations automatically. &copy; 2013 Springer-Verlag.<br/>},
key={Linear transformations},
keywords={Learning algorithms;Linear algebra;Mathematical transformations;Memory architecture;},
note={Common operations;Dense linear algebra;Distributed-memory computers;Expert knowledge;Linear algebra algorithms;Target architectures;},
URL={http://dx.doi.org/10.1007/978-3-642-38718-0_34},
}


@inproceedings{20183205656116,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Gabor Convolutional Networks},
journal={Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
author={Luan, Shangzhen and Zhang, Baochang and Zhou, Siyue and Chen, Chen and Han, Jungong and Yang, Wankou and Liu, Jianzhuang},
volume={2018-January},
year={2018},
pages={1254 - 1262},
address={Lake Tahoe, NV, United states},
abstract={Steerable properties dominate the design of traditional filters, e.g., Gabor filters, and endow features the capability of dealing with spatial transformations. However, such excellent properties have not been well explored in the popular deep convolutional neural networks (DCNNs). In this paper, we propose a new deep model, termed Gabor Convolutional Networks (GCNs or Gabor CNNs), which incorporates Gabor filters into DCNNs to enhance the resistance of deep learned features to the orientation and scale changes. By only manipulating the basic element of DCNNs based on Gabor filters, i.e., the convolution operator, GCNs can be easily implemented and are compatible with any popular deep learning architecture. Experimental results demonstrate the super capability of our algorithm in recognizing objects, where the scale and rotation changes occur frequently. The proposed GCNs have much fewer learnable network parameters, and thus is easier to train with an endtoend pipeline. The source code will be here 1.<br/> &copy; 2018 IEEE.},
key={Gabor filters},
keywords={Computer vision;Convolution;Deep neural networks;Neural networks;},
note={Convolution operators;Convolutional networks;Deep convolutional neural networks;Learning architectures;Network parameters;Scale and rotation;Spatial transformation;Traditional filter;},
URL={http://dx.doi.org/10.1109/WACV.2018.00142},
}


@inproceedings{20143218020830,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An efficient approach for providing rationale of method change for object oriented programming},
journal={2014 International Conference on Informatics, Electronics and Vision, ICIEV 2014},
author={Ami, Amit Seal and Islam, Md. Shariful},
year={2014},
address={Dhaka, Bangladesh},
abstract={Software engineering requires modification of code during development and maintenance phase. During modification, a difficult task is to understand rationale of code changed by others. Present Integrated Development Environments (IDEs) attempt to help this by providing features integrated with different types of repositories. However, these features still consume developers' time as he has to switch from editor to another window for this purpose. Moreover, these features focus on elements available in present version of code, thus increasing the difficulty of finding rationale of an element removed or modified earlier. Leveraging different sources for providing information through code completion menus has been shown to be valuable, even when compared to standalone counterparts offering similar functionalities in literature. Literature also shows that it is one of the most used features for consuming information within IDE. Based on that, we prepare an Eclipse plug-in and a framework that allows providing reason of code change, at method granularity, across versions through a new code completion menu in IDE. These allow a software engineer to gain insight about rationale of removed or modified methods which are otherwise not available in present version of code. Professional software engineers participated in our empirical evaluation process and we observed that more than 80% participants considered this to be a useful approach for saving time and effort to understand rationale of method change. &copy; 2014 IEEE.<br/>},
key={Object oriented programming},
keywords={Codes (symbols);Integrodifferential equations;Software engineering;},
note={Code changes;Code completions;Empirical evaluations;Gain insight;Integrated development environment;Modified methods;Plug-ins;Professional software engineers;},
URL={http://dx.doi.org/10.1109/ICIEV.2014.6850772},
}


@article{20182005202395,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Gabor Convolutional Networks},
journal={IEEE Transactions on Image Processing},
author={Luan, Shangzhen and Chen, Chen and Zhang, Baochang and Han, Jungong and Liu, Jianzhuang},
volume={27},
number={9},
year={2018},
pages={4357 - 4366},
issn={10577149},
abstract={In steerable filters, a filter of arbitrary orientation can be generated by a linear combination of a set of 'basis filters.' Steerable properties dominate the design of the traditional filters, e.g., Gabor filters and endow features the capability of handling spatial transformations. However, such properties have not yet been well explored in the deep convolutional neural networks (DCNNs). In this paper, we develop a new deep model, namely, Gabor convolutional networks (GCNs or Gabor CNNs), with Gabor filters incorporated into DCNNs such that the robustness of learned features against the orientation and scale changes can be reinforced. By manipulating the basic element of DCNNs, i.e., the convolution operator, based on Gabor filters, GCNs can be easily implemented and are readily compatible with any popular deep learning architecture. We carry out extensive experiments to demonstrate the promising performance of our GCNs framework, and the results show its superiority in recognizing objects, especially when the scale and rotation changes take place frequently. Moreover, the proposed GCNs have much fewer network parameters to be learned and can effectively reduce the training complexity of the network, leading to a more compact deep learning model while still maintaining a high feature representation capacity. The source code can be found at https://github.com/bczhangbczhang.<br/> &copy; 1992-2012 IEEE.},
key={Gabor filters},
keywords={Convolution;Crystal orientation;Deep neural networks;Feature extraction;Modulation;Neural networks;Personnel training;Robustness (control systems);},
note={Convolutional networks;Convolutional neural network;Deep convolutional neural networks;Feature representation;Gabor CNNs;Kernel;Learning architectures;Spatial transformation;},
URL={http://dx.doi.org/10.1109/TIP.2018.2835143},
}


@inproceedings{20165103135988,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Bypassing system callsbased intrusion detection systems},
journal={Concurrency Computation},
author={Rosenberg, Ishai and Gudes, Ehud},
volume={29},
number={16},
year={2017},
issn={15320626},
abstract={Machine learning augments today's intrusion detection system (IDS) capability to cope with unknown malware. However, if an attacker gains partial knowledge about the IDS' classifier, he can create a modified version of his malware, which can evade detection. In this article we present an IDS on the basis of various classifiers using system calls, executed by the inspected code as features. We then present a camouflage algorithm that is used to modify malicious code to be classified as benign, while preserving the code's functionality, for decision tree and random forest classifiers. We also present transformations to the classifier's input, to prevent this camouflage - and a modified camouflage algorithm that overcomes those transformations. Our research shows that it is not enough to provide a decision tree based classifier with a large training set to counter malware. One must also be aware of the possibility that the classifier would be fooled by a camouflage algorithm, and try to counter such an attempt with techniques such as input transformation or training set updates.<br/> Copyright &copy; 2016 John Wiley & Sons, Ltd.},
key={Intrusion detection},
keywords={Artificial intelligence;Classification (of information);Codes (symbols);Computer crime;Data mining;Decision trees;Learning systems;Malware;},
note={Behavior analysis;Input transformation;Intrusion Detection Systems;Malicious codes;Malware detection;Partial knowledge;Random forest classifier;Training sets;},
URL={http://dx.doi.org/10.1002/cpe.4023},
}


@inproceedings{20164102884168,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Evading system-calls based intrusion detection systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Rosenberg, Ishai and Gudes, Ehud},
volume={9955 LNCS},
year={2016},
pages={200 - 216},
issn={03029743},
address={Taipei, Taiwan},
abstract={Machine-learning augments today&rsquo;s IDS capability to cope with unknown malware. However, if an attacker gains partial knowledge about the IDS&rsquo;s classifier, he can create a modified version of his malware, which can evade detection. In this article we present an IDS based on various classifiers using system calls executed by the inspected code as features. We then present a camouflage algorithm that is used to modify malicious code to be classified as benign, while preserving the code&rsquo;s functionality, for decision tree and random forest classifiers. We also present transformations to the classifier&rsquo;s input, to prevent this camouflage - and a modified camouflage algorithm that overcomes those transformations. Our research shows that it is not enough to provide a decision tree based classifier with a large training set to counter malware. One must also be aware of the possibility that the classifier would be fooled by a camouflage algorithm, and try to counter such an attempt with techniques such as input transformation or training set updates.<br/> &copy; Springer International Publishing AG 2016.},
key={Intrusion detection},
keywords={Artificial intelligence;Classification (of information);Codes (symbols);Computer crime;Data mining;Decision trees;Learning systems;Malware;Network security;},
note={Behavior analysis;Input transformation;Intrusion Detection Systems;Malicious codes;Malware detection;Partial knowledge;Random forest classifier;Training sets;},
URL={http://dx.doi.org/10.1007/978-3-319-46298-1_14},
}


@inproceedings{20164102895063,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Attacking and defending dynamic analysis system-calls based IDS},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Rosenberg, Ishai and Gudes, Ehud},
volume={9895 LNCS},
year={2016},
pages={103 - 119},
issn={03029743},
address={Heraklion, Crete, Greece},
abstract={Machine-learning augments today&rsquo;s IDS capability to cope with unknown malware. However, if an attacker gains partial knowledge about the IDS&rsquo;s classifier, he can create a modified version of his malware, which can evade detection. In this article we present an IDS based on various classifiers using system calls executed by the inspected code as features. We then present a camouflage algorithm that is used to modify malicious code to be classified as benign, while preserving the code&rsquo;s functionality, for decision tree and random forest classifiers. We also present transformations to the classifier&rsquo;s input, to prevent this camouflage - and a modified camouflage algorithm that overcomes those transformations. Our research shows that it is not enough to provide a decision tree based classifier with a large training set to counter malware. One must also be aware of the possibility that the classifier would be fooled by a camouflage algorithm, and try to counter such an attempt with techniques such as input transformation or training set updates.<br/> &copy; IFIP International Federation for Information Processing 2016.},
key={Malware},
keywords={Artificial intelligence;Classification (of information);Codes (symbols);Computer crime;Data mining;Decision trees;Learning systems;Network security;},
note={Analysis system;Behavior analysis;Input transformation;Malicious codes;Malware detection;Partial knowledge;Random forest classifier;Training sets;},
URL={http://dx.doi.org/10.1007/978-3-319-45931-8_7},
}


@inproceedings{20130615993974,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Measuring software library stability through historical version analysis},
journal={IEEE International Conference on Software Maintenance, ICSM},
author={Raemaekers, Steven and Van Deursen, Arie and Visser, Joost},
year={2012},
pages={378 - 387},
address={Riva del Garda, Trento, Italy},
abstract={Backward compatibility is a major concern for any library developer. In this paper, we evaluate how stable a set of frequently used third-party libraries is in terms of method removals, implementation change, the ratio of change in old methods to change in new ones and the percentage of new methods in each snapshot. We provide a motivating example of a commercial company which demonstrates several issues associated with the usage of third-party libraries. To obtain dependencies from software systems we developed a framework which extracts dependencies from Maven build files and which analyzes system and library code. We propose four metrics which provide different insights in the implementation and interface stability of a library. The usage frequency of library methods is utilized as a weight in the final metric and is obtained from a dataset of more than 2300 snapshots of 140 industrial Java systems. We finally describe three scenarios and an example of the application of our metrics. &copy; 2012 IEEE.<br/>},
key={Computer software reusability},
keywords={Application programming interfaces (API);Computer software maintenance;Libraries;},
note={API Usage;Backward compatibility;Implementation changes;Interface stabilities;Library developers;Software libraries;Software systems;Third parties;},
URL={http://dx.doi.org/10.1109/ICSM.2012.6405296},
}


@article{20184506052282,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An aggregated coupling measure for the analysis of object-oriented software systems},
journal={Journal of Systems and Software},
author={Czibula, Istvan Gergely and Czibula, Gabriela and Miholca, Diana-Lucia and Onet-Marian, Zsuzsanna},
volume={148},
year={2019},
pages={1 - 20},
issn={01641212},
abstract={Coupling is a fundamental property of software systems which is strongly connected with the quality of software design and has high impact on program understanding. The coupling between software components influences software maintenance and evolution as well. In order to ease the maintenance and evolution processes it is essential to estimate the impact of changes made in the software system, coupling indicating such a possible impact. This paper introduces a new aggregated coupling measurement which captures both the structural and the conceptual characteristics of coupling between the software components. The proposed measure combines the textual information contained in the source code with the structural relationships between software components. We conduct several experiments which underline that the proposed aggregated coupling measure reveals new characteristics of coupling and is also effective for change impact analysis.<br/> &copy; 2018 Elsevier Inc.},
key={Object oriented programming},
keywords={Computer software;Computer software maintenance;Software design;Unsupervised learning;},
note={Change impact analysis;Coupling measure;Fundamental properties;Object-oriented software systems;Program understanding;Software maintenance and evolution;Structural coupling;Structural relationship;},
URL={http://dx.doi.org/10.1016/j.jss.2018.10.052},
}


@inproceedings{20173404067416,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning syntactic program transformations from examples},
journal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017},
author={Rolim, Reudismam and Soares, Gustavo and D'Antoni, Loris and Polozov, Oleksandr and Gulwani, Sumit and Gheyi, Rohit and Suzuki, Ryo and Hartmann, Bjorn},
year={2017},
pages={404 - 415},
address={Buenos Aires, Argentina},
abstract={Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-The-Art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.<br/> &copy; 2017 IEEE.},
key={Open source software},
keywords={Codes (symbols);Computer systems programming;Costs;Digital subscriber lines;Problem oriented languages;Students;},
note={Computer science students;Open source projects;Program synthesis;Program transformations;Programming assignments;Programming by Example;Refactorings;Tutoring system;},
URL={http://dx.doi.org/10.1109/ICSE.2017.44},
}


@inproceedings{20134316890526,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automation of Upgrade Process for Enterprise Resource Planning Systems},
journal={Communications in Computer and Information Science},
author={Laukaitis, Algirdas},
volume={403},
year={2013},
pages={70 - 81},
issn={18650929},
address={Kaunas, Lithuania},
abstract={This paper presents a framework for semi-automatic process of enterprise resource planning (ERP) system upgrade. We suggest to change currently accepted practice of manual upgrade process when domain expert-programmer works through all localizations and transforms them manually to the new version of ERP system. The core idea for this framework is to induce the software code transformation patterns from completed upgrade projects and then to refine these patterns by using knowledge of ERP upgrade expert. These patterns lets us to increase productivity of upgrade process by improving automatic code alignment and annotation and by providing code transformation to the new version of ERP system. The price for these improvements is a requirement for upgrade expert to move from traditional 4/GL ERP programming language to stochastic meta-programming language which is used to describe code alignment and code transformation patterns. &copy; Springer-Verlag Berlin Heidelberg 2013.<br/>},
key={Enterprise resource planning},
keywords={Alignment;Automatic programming;Codes (symbols);Computer programming languages;Cosine transforms;Knowledge representation;Resource allocation;Stochastic systems;Systems analysis;},
note={Automatic code generations;Automatic codes;Code transformation;Enterprise resource planning (ERP) systems;Enterprise resource planning systems;ERP system;Rules induction;Semi-automatics;},
URL={http://dx.doi.org/10.1007/978-3-642-41947-8_7},
}


@inproceedings{20154801614828,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Efficient code generation for automatic parallelization and optimization},
journal={Proceedings - 2nd International Symposium on Parallel and Distributed Computing, ISPDC 2003},
author={Bastoul, Cedric},
year={2015},
pages={23 - 30},
address={Ljubljana, Slovenia},
abstract={Graphical Design Tool for Parallel ProgramsSupercompilers look for the best execution order of the statement instances in the most compute intensive kernels. It has been extensively shown that the polyhedral model provides convenient abstractions to find and perform the useful program transformations. Nevertheless, the current polyhedral code generation algorithms lack for flexibility by adressing mainly unimodular or at least invertible transformation functions. Moreover, their complexity is challenging for large problems (with many statements). In this paper, we discuss a general transformation framework able to deal with non-unimodular, non-invertible functions. A completed and improved version of one of the best algorithms known so far is presented to actually perform the code generation. Experimental evidence proves the ability of our framework to handle real-life problems.<br/>},
key={Automatic programming},
keywords={Codes (symbols);Distributed computer systems;},
note={Automatic Parallelization;Code generation algorithm;Experimental evidence;Graphical designs;Polyhedral modeling;Program transformations;Real-life problems;Transformation functions;},
URL={http://dx.doi.org/10.1109/ISPDC.2003.1267639},
}


@inproceedings{20184406016803,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Annotation-free and one-shot learning for instance segmentation of homogeneous object clusters},
journal={IJCAI International Joint Conference on Artificial Intelligence},
author={Wu, Zheng and Chang, Ruiheng and Ma, Jiaxu and Lu, Cewu and Tang, Chi Keung},
volume={2018-July},
year={2018},
pages={1036 - 1042},
issn={10450823},
address={Stockholm, Sweden},
abstract={We propose a novel approach for instance segmentation given an image of homogeneous object cluster (HOC). Our learning approach is one-shot because a single video of an object instance is captured and it requires no human annotation. Our intuition is that images of homogeneous objects can be effectively synthesized based on structure and illumination priors derived from real images. A novel solver is proposed that iteratively maximizes our structured likelihood to generate realistic images of HOC. Illumination transformation scheme is applied to make the real and synthetic images share the same illumination condition. Extensive experiments and comparisons are performed to verify our method. We build a dataset consisting of pixel-level annotated images of HOC. The dataset and code will be released.<br/> &copy; 2018 International Joint Conferences on Artificial Intelligence. All right reserved.},
key={Image segmentation},
keywords={Artificial intelligence;Fuzzy clustering;Iterative methods;},
note={Homogeneous objects;Human annotations;Illumination conditions;Learning approach;One-shot learning;Realistic images;Synthetic images;Transformation scheme;},
}


@inproceedings{20163802810687,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Querying sequential software engineering data},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
author={Sun, Chengnian and Zhang, Haidong and Lou, Jian-Guang and Zhang, Hongyu and Wang, Qiang and Zhang, Dongmei and Khoo, Siau-Cheng},
volume={16-21-November-2014},
year={2014},
pages={700 - 710},
address={Hong Kong, China},
abstract={We propose a pattern-based approach to effectively and efficiently analyzing sequential software engineering (SE) data. Different from other types of SE data, sequential SE data preserves unique temporal properties, which cannot be easily analyzed without much programming effort. In order to facilitate the analysis of sequential SE data, we design a sequential pattern query language (SPQL), which specifies the temporal properties based on regular expressions, and is enhanced with variables and statements to store and manipulate matching states. We also propose a query engine to effectively process the SPQL queries. We have applied our approach to analyze two types of SE data, namely bug report history and source code change history. We experiment with 181,213 Eclipse bug reports and 323,989 code revisions of Android. SPQL enables us to explore interesting temporal properties underneath these sequential data with a few lines of query code and low matching overhead. The analysis results can help better understand a software process and identify process violations.<br/> Copyright 2014 ACM.},
key={Software engineering},
keywords={Codes (symbols);Computer programming languages;Pattern matching;Query languages;},
note={Mining software repositories;Pattern-based approaches;Query;Regular expressions;Sequential data;Sequential patterns;Sequential softwares;Source code changes;},
URL={http://dx.doi.org/10.1145/2635868.2635902},
}


@article{20183205668781,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Data change analysis based on function call path},
journal={International Journal of Computers and Applications},
author={Yong, Cao and Yongmin, Mu and Meie, Shen},
volume={40},
number={3},
year={2018},
pages={1 - 10},
issn={1206212X},
abstract={In the software life cycle, software version changes may make the normal function of the original emerge questions, so the analysis of the affected domain data flow is of great significance for the localization of software defects. In this paper, the analysis method is based on the analysis of the regression test of the data change. First of all obtain the data changed point from comparing source code and the changed code, then static analyze the changed code to get the statements related to the change, and obtain the data flow path where the change data include the statement block tree according to the data change analysis and syntax analysis. Finally, the data change path is generated by combining the function call path, and then obtain the data change influence domain. The experiment results show that based on the modification of a function call trace data analysis is correct and ensure the integrity of the test, and this method is helpful to determine more test cases, improves the function of the test coverage, and provides help for developers to quickly deal with the software defects in the regression test.<br/> &copy; 2018, &copy; 2018 Informa UK Limited, trading as Taylor & Francis Group.},
key={Trees (mathematics)},
keywords={Codes (symbols);Data flow analysis;Data transfer;Defects;Life cycle;Regression analysis;Software testing;Syntactics;Testing;},
note={Data flow;Function call paths;Function call traces;Influence domains;Regression tests;Software defects;Software life cycles;Software versions;},
URL={http://dx.doi.org/10.1080/1206212X.2017.1413625},
}


@inproceedings{20184005905906,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Impact of gamification on code review process - An experimental study},
journal={ACM International Conference Proceeding Series},
author={Khandelwal, Shivam and Sripada, Sai Krishna and Raghu Reddy, Y.},
year={2017},
pages={122 - 126},
address={Jaipur, India},
abstract={Researchers have supported the idea of gamification to enhance students&rsquo; interest in activities like code reviews, change management, knowledge management, issue tracking, etc. which might otherwise be repetitive and monotonous. We performed an experimental study consisting of nearly 180+ participants to measure the impact of gamification on code review process using 5 different code review tools, including one gamified code review instance from our extensible architectural framework. We assess the impact of gamification based on the code smells and bugs identified in a gamified and non-gamified environment as per code inspection report. Further, measurement and comparison of the quantity and usefulness of code review comments was done using machine learning techniques.<br/> &copy; 2017 ACM.},
key={Codes (symbols)},
keywords={Classification (of information);Knowledge management;Learning systems;Software engineering;Text processing;},
note={Architectural frameworks;Code review;Evaluation;Gamification;Text analysis;},
URL={http://dx.doi.org/10.1145/3021460.3021474},
}


@inproceedings{20134616983873,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Information transformation and automated reasoning for automated compliance checking in construction},
journal={Computing in Civil Engineering - Proceedings of the 2013 ASCE International Workshop on Computing in Civil Engineering},
author={Zhang, J. and El-Gohary, N.M.},
year={2013},
pages={701 - 708},
address={Los Angeles, CA, United states},
abstract={This paper presents a new approach for automated compliance checking in the construction domain. The approach utilizes semantic modeling, semantic Natural Language Processing (NLP) techniques (including text classification and information extraction), and logic reasoning to facilitate automated textual regulatory document analysis and processing for extracting requirements from these documents and formalizing these requirements in a computer-processable format. The approach involves developing a set of algorithms and combining them into one computational platform: (1) semantic machine-learning-based algorithms for text classification (TC); (2) hybrid syntactic-semantic rule-based algorithms for information extraction (IE); (3) semantic rule-based algorithms for information transformation (ITr); and (4) logic-based algorithms for compliance reasoning (CR). This paper focuses on presenting our algorithms for ITr. A semantic, logic-based representation for construction regulatory requirements is described. Semantic mapping rules and conflict resolution rules for transforming the extracted information into the representation are discussed. Our combined TC, IE and ITr algorithms were tested in extracting and formalizing quantitative requirements in the 2006 International Building Code, achieving 96% and 92% precision and recall, respectively. &copy; 2013 American Society of Civil Engineers.<br/>},
key={Compliance control},
keywords={Automation;Classification (of information);Computation theory;Computer circuits;Data mining;Information management;Information retrieval;Information retrieval systems;Learning algorithms;Learning systems;Modeling languages;Natural language processing systems;Semantics;Text processing;},
note={Automated compliance checking;Computational platforms;Information transformations;International Building Code;Logic-based representation;Precision and recall;Regulatory documents;Regulatory requirements;},
}


@book{20174404349088,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Machine learning using R},
journal={Machine Learning Using R},
author={Ramasubramanian, Karthik and Singh, Abhishek},
year={2016},
pages={1 - 566},
abstract={Examine the latest technological advancements in building a scalable machine learning model with Big Data using R. This book shows you how to work with a machine learning algorithm and use it to build a ML model from raw data. All practical demonstrations will be explored in R, a powerful programming language and software environment for statistical computing and graphics. The various packages and methods available in R will be used to explain the topics. For every machine learning algorithm covered in this book, a 3-D approach of theory, case-study and practice will be given. And where appropriate, the mathematics will be explained through visualization in R. All the images are available in color and hi-res as part of the code download. This new paradigm of teaching machine learning will bring about a radical change in perception for many of those who think this subject is difficult to learn. Though theory sometimes looks difficult, especially when there is heavy mathematics involved, the seamless flow from the theoretical aspects to example-driven learning provided in this book makes it easy for someone to connect the dots.. What You&rsquo;ll Learn Use the model building process flow Apply theoretical aspects of machine learning Review industry-based cae studies Understand ML algorithms using R Build machine learning models using Apache Hadoop and Spark Who This Book is For Data scientists, data science professionals and researchers in academia who want to understand the nuances of machine learning approaches/algorithms along with ways to see them in practice using R. The book will also benefit the readers who want to understand the technology behind implementing a scalable machine learning model using Apache Hadoop, Hive, Pig and Spark.<br/> &copy; 2017 Karthik Ramasubramanian and Abhishek Singh.},
key={Learning algorithms},
keywords={Artificial intelligence;Big data;Computation theory;Computer software;Learning systems;Reviews;},
note={Building process;Machine learning approaches;Machine learning models;Scalable machine learning;Software environments;Statistical computing;Technological advancement;Theoretical aspects;},
URL={http://dx.doi.org/10.1007/978-1-4842-2334-5},
}


@inproceedings{20181104897817,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Deep auto-encoder based on supervised learning for damaged face reconstruction},
journal={Communications in Computer and Information Science},
author={Rui, Ting and Zhang, Sai and Zou, Junhua and Zhou, You and Tang, Jian},
volume={819},
year={2018},
pages={290 - 299},
issn={18650929},
address={Qingdao, China},
abstract={Based on the reconstruction idea of auto-encoder (AE) and image reconstruction, we present a new idea that the classical auto-encoder can be polished up by supervised learning. We also present a novel supervised deep learning framework for damaged face reconstruction after analyzing the deep model structure. The proposed model is unlike the classical auto-encoder which is unsupervised learning. In this paper, the deep supervised auto-encoder model is illustrated, which has a set of &ldquo;progressive&rdquo; and &ldquo;interrelated&rdquo; learning strategies by multiple groups of supervised single-layer AE. In this structure, we define a Deep Supervised Network with the supervised auto-encoder which is trained to extract characteristic features from damaged images and reconstruct the corresponding similar facial images, and it improves the ability to express the feature code. Extensive experiment on AR database demonstrates that the proposed method can significantly improve the smoothness of the damaged face reconstruction under enormous illumination, expression change. Experiments show that the proposed method has good contribution and adaptability to the damaged face reconstruction.<br/> &copy; Springer Nature Singapore Pte Ltd. 2018.},
key={Deep learning},
keywords={Image enhancement;Image reconstruction;Signal encoding;Supervised learning;},
note={Auto encoders;Deep structure;Face reconstruction;Feature codes;Learning frameworks;Learning strategy;Multiple-group;Supervised network;},
URL={http://dx.doi.org/10.1007/978-981-10-8530-7_28},
}


@inproceedings{20153201154929,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Prediction and ranking of co-change candidates for clones},
journal={11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings},
author={Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.},
year={2014},
pages={32 - 41},
address={Hyderabad, India},
abstract={Code clones are identical or similar code fragments scattered in a code-base. A group of code fragments that are similar to one another form a clone group. Clones in a particular group often need to be changed together (i.e., co-changed) consistently. However, all clones in a group might not require consistent changes, because some clone fragments might evolve independently. Thus, while changing a particular clone fragment, it is important for a programmer to know which other clone fragments in the same group should be consistently cochanged with that particular clone fragment. In this research work, we empirically investigate whether we can automatically predict and rank these other clone fragments (i.e., the co-change candidates) from a clone group while making changes to a particular clone fragment in this group. For prediction and ranking we automatically retrieve and infer evolutionary coupling among clones by mining the past clone evolution history. Our experimental result on six subject systems written in two different programming languages (C, and Java) considering both exact and near-miss clones implies that we can automatically predict and rank co-change candidates for clones by analyzing evolutionary coupling. Our ranking mechanism can help programmers pinpoint the likely co-change candidates while changing a particular clone fragment and thus, can help us to better manage software clones.<br/> Copyright 2014 ACM.},
key={Cloning},
keywords={C (programming language);Codes (symbols);Forecasting;},
note={Change frequencies;Code clone;Code fragments;Evolution history;Near-misses;Ranking;Ranking mechanisms;Software clones;},
URL={http://dx.doi.org/10.1145/2597073.2597104},
}


@article{20161702284201,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Detecting, Tracing, and Monitoring Architectural Tactics in Code},
journal={IEEE Transactions on Software Engineering},
author={Mirakhorli, Mehdi and Cleland-Huang, Jane},
volume={42},
number={3},
year={2016},
pages={206 - 221},
issn={00985589},
abstract={Software architectures are often constructed through a series of design decisions. In particular, architectural tactics are selected to satisfy specific quality concerns such as reliability, performance, and security. However, the knowledge of these tactical decisions is often lost, resulting in a gradual degradation of architectural quality as developers modify the code without fully understanding the underlying architectural decisions. In this paper we present a machine learning approach for discovering and visualizing architectural tactics in code, mapping these code segments to tactic traceability patterns, and monitoring sensitive areas of the code for modification events in order to provide users with up-to-date information about underlying architectural concerns. Our approach utilizes a customized classifier which is trained using code extracted from fifty performance-centric and safety-critical open source software systems. Its performance is compared against seven off-the-shelf classifiers. In a controlled experiment all classifiers performed well; however our tactic detector outperformed the other classifiers when used within the larger context of the Hadoop Distributed File System. We further demonstrate the viability of our approach for using the automatically detected tactics to generate viable and informative messages in a simulation of maintenance events mined from Hadoop's change management system.<br/> &copy; 2015 IEEE.},
key={Open systems},
keywords={Architecture;Codes (symbols);File organization;Learning systems;Open source software;Safety engineering;Software architecture;},
note={Architectural decision;Controlled experiment;Hadoop distributed file systems;Machine learning approaches;Open source software systems;tactics;traceability;Traceability information;},
URL={http://dx.doi.org/10.1109/TSE.2015.2479217},
}


@inproceedings{20183905872947,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Deep Reinforcement Learning for Surgical Gesture Segmentation and Classification},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Liu, Daochang and Jiang, Tingting},
volume={11073 LNCS},
year={2018},
pages={247 - 255},
issn={03029743},
address={Granada, Spain},
abstract={Recognition of surgical gesture is crucial for surgical skill assessment and efficient surgery training. Prior works on this task are based on either variant graphical models such as HMMs and CRFs, or deep learning models such as Recurrent Neural Networks and Temporal Convolutional Networks. Most of the current approaches usually suffer from over-segmentation and therefore low segment-level edit scores. In contrast, we present an essentially different methodology by modeling the task as a sequential decision-making process. An intelligent agent is trained using reinforcement learning with hierarchical features from a deep model. Temporal consistency is integrated into our action design and reward mechanism to reduce over-segmentation errors. Experiments on JIGSAWS dataset demonstrate that the proposed method performs better than state-of-the-art methods in terms of the edit score and on par in frame-wise accuracy. Our code will be released later.<br/> &copy; 2018, Springer Nature Switzerland AG.},
key={Deep learning},
keywords={Decision making;Image segmentation;Medical computing;Medical imaging;Recurrent neural networks;Reinforcement learning;Time series analysis;Transplantation (surgical);},
note={Convolutional networks;Hierarchical features;Over segmentation;Sequential decision making;State-of-the-art methods;Surgery training;Surgical gestures;Temporal consistency;},
URL={http://dx.doi.org/10.1007/978-3-030-00937-3_29},
}


@article{20173504105902,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Surface water mapping by deep learning},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
author={Isikdogan, Furkan and Bovik, Alan C. and Passalacqua, Paola},
volume={10},
number={11},
year={2017},
pages={4909 - 4918},
issn={19391404},
abstract={Mapping of surface water is useful in a variety of remote sensing applications, such as estimating the availability of water, measuring its change in time, and predicting droughts and floods. Using the imagery acquired by currently active Landsat missions, a surface water map can be generated from any selected region as often as every 8 days. Traditional Landsat water indices require carefully selected threshold values that vary depending on the region being imaged and on the atmospheric conditions. They also suffer from many false positives, arising mainly from snow and ice, and from terrain and cloud shadows being mistaken for water. Systems that produce high-quality water maps usually rely on ancillary data and complex rule-based expert systems to overcome these problems. Here, we instead adopt a data-driven, deep-learning-based approach to surface water mapping. We propose a fully convolutional neural network that is trained to segment water on Landsat imagery. Our proposed model, named DeepWaterMap, learns the characteristics of water bodies from data drawn from across the globe. The trained model separates water from land, snow, ice, clouds, and shadows using only Landsat bands as input. Our code and trained models are publicly available at http://live.ece.utexas.edu/research/deepwatermap/.<br/> &copy; 2008-2012 IEEE.},
key={Surface waters},
keywords={Computer vision;Convolution;Deep learning;Expert systems;Learning algorithms;Learning systems;Mapping;Neural networks;Remote sensing;Snow;},
note={Atmospheric conditions;Convolutional neural network;High quality water;LANDSAT;Landsat imagery;Learning-based approach;Remote sensing applications;Rule based expert systems;},
URL={http://dx.doi.org/10.1109/JSTARS.2017.2735443},
}


@inproceedings{20171403541903,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Towards a blended learning using mobile devices, podcasts and QR codes in Algeria},
journal={Communication, Management and Information Technology - Proceedings of the International Conference on Communication, Management and Information Technology, ICCMIT 2016},
author={Ghizlene, Soulimane and Belkacem, Kouninef and Mohamed, Djelti},
year={2017},
pages={159 - 166},
address={Cosenza, Italy},
abstract={Higher education in Algeria has witnessed significant reforms in its educational system with a growing number of students from year to year due to its young population and a dynamic transition in the integration of Information and Communication Technologies (ICTs). Algeria is gradually advancing in terms of telecoms and Internet. The fixed network is difficult to access. With mobile operators, Algeria is distinguished through the use of Mobile. In Algeria the mobile penetration rate stands at over 111% and 21% with 3G. Internet access remains inaccessible to our students. But almost all students have access to mobile technology. Indeed, learners now want to learn &ldquo;on the move&rdquo;. They are nomadic learners who learn in faculty, restaurant, library, before sleeping, around a coffee. But they also learn in communities (social networks) i.e they exchange them with unprecedented ease of information solutions to the problems and mutually explain what the professor said. With this Internet generation, we have to change our methods of teaching and learning to think fast and efficiently, with a minimum of organizational, logistical and above all loss of time. Mobile technology is increasingly being used to support blended learning. The satisfactory results of our previous research show that the use of mobile technology could enhance accessibility and communication in a blended learning course. We discuss the emerging trends that allow more involved learners, such as e-learning, m-learning, blended learning, the use of podcasts and using QR codes in Learning Management System (LMS) at the National Institute of Telecommunications and ICT (INTTIC). QR Code is still relatively new and still in its infancy in education. The use of QR codes in INTTIC-LMS can be placed in the context of mobile learning.<br/> &copy; 2017 Taylor & Francis Group, London.},
key={Engineering education},
keywords={Codes (symbols);E-learning;Students;Teaching;Telecommunication equipment;},
note={Blended learning;Learning management system;Mobile Learning;MOODLE platform;Podcasting;QR codes;},
}


@inproceedings{20161902351455,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Experience report: Evaluating the effectiveness of decision trees for detecting code smells},
journal={2015 IEEE 26th International Symposium on Software Reliability Engineering, ISSRE 2015},
author={Amorim, Lucas and Costa, Evandro and Antunes, Nuno and Fonseca, Baldoino and Ribeiro, Marcio},
year={2015},
pages={261 - 269},
address={Gaithersbury, MD, United states},
abstract={Developers continuously maintain software systems to adapt to new requirements and to fix bugs. Due to the complexity of maintenance tasks and the time-to-market, developers make poor implementation choices, also known as code smells. Studies indicate that code smells hinder comprehensibility, and possibly increase change- and fault-proneness. Therefore, they must be identified to enable the application of corrections. The challenge is that the inaccurate definitions of code smells make developers disagree whether a piece of code is a smell or not, consequently, making difficult creation of a universal detection solution able to recognize smells in different software projects. Several works have been proposed to identify code smells but they still report inaccurate results and use techniques that do not present to developers a comprehensive explanation how these results have been obtained. In this experimental report we study the effectiveness of the Decision Tree algorithm to recognize code smells. For this, it was applied in a dataset containing 4 open source projects and the results were compared with the manual oracle, with existing detection approaches and with other machine learning algorithms. The results showed that the approach was able to effectively learn rules for the detection of the code smells studied. The results were even better when genetic algorithms are used to pre-select the metrics to use.<br/> &copy; 2015 IEEE.},
key={Software reliability},
keywords={Codes (symbols);Computer software selection and evaluation;Data mining;Decision trees;Genetic algorithms;Learning algorithms;Learning systems;Odors;Open source software;Program debugging;Trees (mathematics);},
note={Code smell;Decision-tree algorithm;Detection approach;Experience report;Maintenance tasks;Open source projects;Software Quality;Universal detections;},
URL={http://dx.doi.org/10.1109/ISSRE.2015.7381819},
}


@inproceedings{20174204273909,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Fatiguing STDP: Learning from spike-timing codes in the presence of rate codes},
journal={Proceedings of the International Joint Conference on Neural Networks},
author={Moraitis, Timoleon and Sebastian, Abu and Boybat, Irem and Le Gallo, Manuel and Tuma, Tomas and Eleftheriou, Evangelos},
volume={2017-May},
year={2017},
pages={1823 - 1830},
address={Anchorage, AK, United states},
abstract={Spiking neural networks (SNNs) could play a key role in unsupervised machine learning applications, by virtue of strengths related to learning from the fine temporal structure of event-based signals. However, some spike-timing-related strengths of SNNs are hindered by the sensitivity of spike-timing-dependent plasticity (STDP) rules to input spike rates, as fine temporal correlations may be obstructed by coarser correlations between firing rates. In this article, we propose a spike-timing-dependent learning rule that allows a neuron to learn from the temporally-coded information despite the presence of rate codes. Our long-term plasticity rule makes use of short-term synaptic fatigue dynamics. We show analytically that, in contrast to conventional STDP rules, our fatiguing STDP (FSTDP) helps learn the temporal code, and we derive the necessary conditions to optimize the learning process. We showcase the effectiveness of FSTDP in learning spike-timing correlations among processes of different rates in synthetic data. Finally, we use FSTDP to detect correlations in real-world weather data from the United States in an experimental realization of the algorithm that uses a neuro-morphic hardware platform comprising phase-change memristive devices. Taken together, our analyses and demonstrations suggest that FSTDP paves the way for the exploitation of the spike-based strengths of SNNs in real-world applications.<br/> &copy; 2017 IEEE.},
key={Neurons},
keywords={Codes (symbols);Learning systems;Memristors;Neural networks;Timing circuits;},
note={Experimental realizations;Hardware platform;Learning process;Spike timing dependent plasticities;Spiking neural networks;Temporal correlations;Temporal structures;Unsupervised machine learning;},
URL={http://dx.doi.org/10.1109/IJCNN.2017.7966072},
}


@inproceedings{20160501880020,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Wikipedia miner engine: A re-usable e-learning service based on a virtual MVC design pattern},
journal={Frontiers in Artificial Intelligence and Applications},
author={Cortez, Ruth and Vazhenin, Alexander and Brine, John},
volume={246},
year={2012},
pages={165 - 179},
issn={09226389},
abstract={E-Learning platforms are evolving from monolithic applications with a rigid structure that did not allowed for the exchange of tools or components to applications incorporating service orientation concepts as well as facilitating the dynamic discovery and assembling of e-learning services. Accordingly, the usage of support materials to provide additional guidance to students facilitates the comprehension of learning tasks. Wikipedia is one of the richest sources of human knowledge, encompassing a vast range of topics of all kinds of information, and content, which is in constant change due to its collaborative dynamic nature. The Wikipedia Miner provides a code that can parse a given document identifying main topics and link them to corresponding articles or short definitions from the Wikipedia content. In this paper, we discuss the realization of a reusable Wikipedia Miner service for the e-Learning Computational Cloud (eLC2) Platform designed with the J2EE technology and Service-Oriented (V-MVC) model excluding a direct link between the Model and the View. This allows enhancing the Controller as a middleware, removing the dependency and acting as a single point of contact. In the V-MVC design pattern, the Controller is modeled by the compound design pattern of the Enterprise Service Bus (ESB) supporting higher privacy of the business logic and higher re-usability Architecture standards. The eLC2 is also based on an original Virtual Model-View-Controller of application components. In this framework, Wikipedia Miner services were prototyped as an Application Engine that wraps the logic of the Wikipedia Miner API in order to re-use it for different types of applications. Particularly, we are focusing on two applications in order to demonstrate the usability of the proposed approach. The first application is the WikiGloss tool, which is based on a glossing approach to help learners of English-as-second-language with an extensive reading task. The second application is an Intelligent Hints service for a Task Management Environment which provides explanatory links from relevant Wikipedia articles related to topics of the e-Learning task. This allows re-use of the same problems in different task type modes such as lectures, exercises, and quizzes. &copy; 2012 The authors and IOS Press. All rights reserved.<br/>},
key={Service oriented architecture (SOA)},
keywords={Application programming interfaces (API);Computation theory;Computer circuits;Computer software reusability;Controllers;E-learning;Engines;Information services;Memory architecture;Middleware;Miners;Telecommunication services;Web services;},
note={Application components;Design Patterns;E-learning platforms;E-learning services;English as second languages;Enterprise service bus;Service orientation;Wikipedia articles;},
URL={http://dx.doi.org/10.3233/978-1-61499-125-0-165},
}


@inproceedings{20174404329581,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Supervised Machine Learning to Predict Follow-Up among Adjuvant Endocrine Therapy Patients},
journal={Proceedings - 2017 IEEE International Conference on Healthcare Informatics, ICHI 2017},
author={Harrell, Morgan and Levy, Mia and Fabbri, Daniel},
year={2017},
pages={490 - 495},
address={Park City, UT, United states},
abstract={Long-term adjuvant endocrine therapy patients often fail to follow-up with their care providers for the recommended duration of time. We used electronic health record data, tumor registry records, and appointment logs to predict follow-up for an adjuvant endocrine therapy patient cohort. Learning predictors for follow-up may facilitate interventions that improve follow-up rates, and ultimately improve patient care in the adjuvant endocrine therapy patient population.We selected 1455 adjuvant endocrine therapy patients at Vanderbilt University Medical Center, and modeled them as a matrix of medical-related, appointment-related, and demographic related features derived from EHR data. We built and optimized a random forest classifier and neural network to differentiate between patients that follow-up, or fail to follow-up, with their care provider for at least five years. We measured follow-up three different ways: thought appointments with any care providers, appointments with an oncologist, and adjuvant endocrine therapy medication records. Classifiers make predictions at the start of adjuvant endocrine therapy, and additionally use temporal subsets of data to learn the change in accuracy as patient data accrues.Our best model is a random forest classifier combining medical-related, appointment-related, and demographic-related features to achieve an AUC of 0.74. The most predictive features for follow-up in our random forest model are total medication counts, patient age, and median income for zip code. We suggest that reliable prediction for follow-up may be correlated with amount of care received at VUMC (i.e., VUMC primary care).This study achieved moderately accurate prediction for followup in adjuvant endocrine therapy patients from electronic health record data. Predicting follow-up can facilitate interventions for improving follow-up rates and improve patient care for adjuvant endocrine therapy cohorts. This study demonstrates the ability to find opportunities for patient care improvement from EHR data.<br/> &copy; 2017 IEEE.},
key={Patient treatment},
keywords={Artificial intelligence;Decision trees;Forecasting;Hospital data processing;Learning systems;Population statistics;Records management;Supervised learning;},
note={Accurate prediction;Breast Cancer;Electronic health record;Endocrine therapy;Random forest classifier;Random forest modeling;Supervised machine learning;Vanderbilt University;},
URL={http://dx.doi.org/10.1109/ICHI.2017.46},
}


@inproceedings{20135017079245,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={How to study programming on mobile touch devices - Interactive Python code exercises},
journal={ACM International Conference Proceeding Series},
author={Ihantola, Petri and Helminen, Juha and Karavirta, Ville},
year={2013},
pages={51 - 58},
address={Koli, Finland},
abstract={Scaffolded learning tasks where programs are constructed from predefined code fragments by dragging and dropping them (i.e. Parsons problems) are well suited to mobile touch devices, but quite limited in their applicability. They do not adequately cater for different approaches to constructing a program. After studying solutions to automatically assessed programming exercises, we found out that many different solutions are composed of a relatively small set of mutually similar code lines. Thus, they can be constructed by using the drag-and-drop approach if only it was possible to edit some small parts of the predefined fragments. Based on this, we have designed and implemented a new exercise type for mobile devices that builds on Parsons problems and falls somewhere between their strict scaffolding and full-blown coding exercises. In these exercises, we can gradually fade the scaffolding and allow programs to be constructed more freely so as not to restrict thinking and limit creativity too much while still making sure we are able to deploy them to small-screen mobile devices. In addition to the new concept and the related implementation, we discuss other possibilities of how programming could be practiced on mobile devices. &copy; 2013 ACM.<br/>},
key={Codes (symbols)},
keywords={High level languages;Mathematical programming;Scaffolds;Teaching;},
note={learning;M-Learning;Mobile Learning;Mobile touch devices;parsons problem;Parsons puzzles;Python;},
URL={http://dx.doi.org/10.1145/2526968.2526974},
}


@article{20183405721999,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The Sparse Polyhedral Framework: Composing Compiler-Generated Inspector-Executor Code},
journal={Proceedings of the IEEE},
author={Strout, Michelle Mills and Hall, Mary and Olschanowsky, Catherine},
volume={106},
number={11},
year={2018},
pages={1921 - 1934},
issn={00189219},
abstract={Irregular applications such as big graph analysis, material simulations, molecular dynamics simulations, and finite element analysis have performance problems due to their use of sparse data structures. Inspector-executor strategies improve sparse computation performance through parallelization and data locality optimizations. An inspector reschedules and reorders data at runtime, and an executor is a transformed version of the original computation that uses the newly reorganized schedules and data structures. Inspector-executor transformations are commonly written in a domain-specific or even application-specific fashion. Significant progress has been made in incorporating such inspector-executor transformations into existing compiler transformation frameworks, thus enabling their use with compile-time transformations. However, composing inspector-executor transformations in a general way has only been done in the context of the Sparse Polyhedral Framework (SPF). Though SPF enables the general composition of such transformations, the resulting inspector and executor performance suffers due to missed specialization opportunities. This paper reviews the history and current state of the art for inspector-executor strategies and reviews how the SPF enables the composition of inspector-executor transformations. Further, it describes a research vision to combine this generality in SPF with specialization to achieve composable and high performance inspectors and executors, producing a powerful compiler framework for sparse matrix computations.<br/> &copy; 1963-2012 IEEE.},
key={Matrix algebra},
keywords={Computer control systems;Data structures;Libraries;Molecular dynamics;Optimization;Program compilers;},
note={Arrays;Indexes;Intermediate representations;Irregular computations;Parallelizations;Runtimes;Sparse matrices;},
URL={http://dx.doi.org/10.1109/JPROC.2018.2857721},
}


@inbook{20164102890554,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Good change and bad change: An analysis perspective on software evolution},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Lindvall, Mikael and Becker, Martin and Tenev, Vasil and Duszynski, Slawomir and Hinchey, Mike},
volume={9960 LNCS},
year={2016},
pages={90 - 112},
issn={03029743},
abstract={Software does change, and should change. Traditional industrial software systems often evolve over long periods of time with each new version forming a discreet milestone, while some new software systems involve constant adaptation to situations in the environment and therefore evolve continually. While necessary, software change can also be devastating, making the system difficult to change and maintain further. We believe that one promising way to manage and control change is to view an evolving system as a software product line where each version of the software is a product. Key to any successful software product line approach is a software architecture that supports variability management. Tools that can identify commonalities and differences among various releases are essential in collecting and managing the information on changed, added and deleted components. Equally important are tools that allow the architect to analyse the current status of the product line as well as its products from various perspectives, and to be able to detect and remove architectural violations that threaten the variability points and built-in flexibility. In this paper, we describe our current research on defining such a process and supporting tools for software evolution management based on product line concepts and apply it in a case study to a software testbed called TSAFE. We describe how we reverse engineer the actual architecture from the source code and how we develop new target architectures based on the reverse engineered one and the expected changes. We then described how we analyse the actual change across different implementations and visualize where the change actually occurred. We then describe how we determine if a particular implementation match the target architecture. The conclusion is that we have found that both these analysis techniques are particularly useful for analysing software evolution and complement each other.<br/> &copy; Springer International Publishing AG 2016.},
key={Computer software},
keywords={Artificial intelligence;Computer science;Computers;},
note={Analysis techniques;Industrial software;Software Evolution;Software Product Line;Software systems;Software testbed;Target architectures;Variability management;},
URL={http://dx.doi.org/10.1007/978-3-319-46508-1_6},
}


@article{20140817350746,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Improving automatic centralization by version separation},
journal={IPSJ Online Transactions},
author={Ma, Lei and Artho, Cyrille and Sato, Hiroyuki},
volume={7},
number={1},
year={2014},
pages={1 - 13},
issn={18826660},
abstract={With today's importance of distributed applications, their verification and analysis are still challenging. They involve large combinational states, interactive network communications between peers, and concurrency. Although there are some dynamic analysis tools for analyzing the runtime behavior of a single-process application, they do not provide methods to analyze distributed applications as a whole, where multiple processes run simultaneously. Centralization is a general solution which transforms multi-process applications into a single-process one that can be directly analyzed by existing tools. In this paper, we improve the accuracy of centralization. Moreover, we extend it as a general framework for analyzing distributed applications with multiple versions. First, we formalize the version conflict problem and present a simple solution, and further propose an optimized solution to resolving class version conflicts during centralization. Our techniques enable sharing common code whenever possible while keeping the version space of each component application separate. Centralization issues like startup semantics and static field transformation are improved and discussed. We implement and apply our centralization tool to some network benchmarks. Experiments, where existing tools are used on the centralized application, prove the usefulness of our automatic centralization tool, showing that centralization enables these tools to analyze distributed applications with multiple versions. &copy; 2014 Information Processing Society of Japan.<br/>},
key={Application programs},
keywords={Dynamic analysis;Model checking;Semantics;},
note={Component application;Distributed applications;Dynamic analysis tools;Network communications;Optimized solutions;Software model checking;Verification and analysis;Version conflict;},
URL={http://dx.doi.org/10.2197/ipsjtrans.7.1},
}


@inproceedings{20180204623142,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Towards a Severity and Activity based Assessment of Code Smells},
journal={Procedia Computer Science},
author={Husien, Harris Kristanto and Harun, Muhammad Firdaus and Lichter, Horst},
volume={116},
year={2017},
pages={460 - 467},
issn={18770509},
address={Bali, Indonesia},
abstract={Code smells are the structural weaknesses which reside in a software system. They evolve negatively over time reducing the system quality i.e., maintainability, understandability etc. Therefore, they should be detected and prioritized based on criticality in order to be refactored. Most of the existing approaches are based on severity score, but little works have been done to include the information from changes history. Thus, we introduce a Harmfulness Model that integrates both information: severity and changes history (i.e., code smells activity). This study characterizes a god class activity based on its severity and change frequency of the JHotDraw open source system. The result indicates that there are two main activities of god class that can be assessed as active and passive smells. In fact, an active god class can be differentiated as strong, stable, and ameliorate smells while a passive god class has one type called dormant. Besides that, from severity and activity information, the model can compute the harmfulness score and also indicate the degree of harmfulness level. The harmfulness level may be useful to improve change likelihood estimation and refactoring candidates prioritization.<br/> &copy; 2017 The Authors. Published by Elsevier B.V.},
key={Open systems},
keywords={Artificial intelligence;Codes (symbols);Odors;Open source software;},
note={Activity informations;Change frequencies;Change-likelihood;Code smell;Evolution;Likelihood estimation;Open source system;Understandability;},
URL={http://dx.doi.org/10.1016/j.procs.2017.10.040},
}


@article{20160801972585,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Improving automatic centralization by version separation},
journal={IPSJ Online Transactions},
author={Ma, Lei and Artho, Cyrille and Sato, Hiroyuki},
volume={7},
number={2014},
year={2014},
pages={1 - 13},
issn={18826660},
abstract={With today's importance of distributed applications, their verification and analysis are still challenging. They involve large combinational states, interactive network communications between peers, and concurrency. Although there are some dynamic analysis tools for analyzing the runtime behavior of a single-process application, they do not provide methods to analyze distributed applications as a whole, where multiple processes run simultaneously. Centralization is a general solution which transforms multi-process applications into a single-process one that can be directly analyzed by existing tools. In this paper, we improve the accuracy of centralization. Moreover, we extend it as a general framework for analyzing distributed applications with multiple versions. First, we formalize the version conflict problem and present a simple solution, and further propose an optimized solution to resolving class version conflicts during centralization. Our techniques enable sharing common code whenever possible while keeping the version space of each component application separate. Centralization issues like startup semantics and static field transformation are improved and discussed. We implement and apply our centralization tool to some network benchmarks. Experiments, where existing tools are used on the centralized application, prove the usefulness of our automatic centralization tool, showing that centralization enables these tools to analyze distributed applications with multiple versions.<br/> &copy; 2014 Information Processing Society of Japan.},
key={Application programs},
keywords={Dynamic analysis;Model checking;Semantics;},
note={Component application;Distributed applications;Dynamic analysis tools;Network communications;Optimized solutions;Software model checking;Verification and analysis;Version conflict;},
URL={http://dx.doi.org/10.2197/ipsjtrans.7.1},
}


@inproceedings{20185106254767,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Which method-stereotype changes are indicators of code smells?},
journal={Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018},
author={Decker, Michael J. and Newman, Christian D. and Dragan, Natalia and Collard, Michael L. and Maletic, Jonathan I. and Kraft, Nicholas A.},
year={2018},
pages={82 - 91},
address={Madrid, Spain},
abstract={A study of how method roles evolve during the lifetime of a software system is presented. Evolution is examined by analyzing when the stereotype of a method changes. Stereotypes provide a high-level categorization of a method's behavior and role, and also provide insight into how a method interacts with its environment and carries out tasks. The study covers 50 open-source systems and 6 closed-source systems. Results show that method behavior with respect to stereotype is highly stable and constant over time. Overall, out of all the history examined, only about 10% of changes to methods result in a change in their stereotype. Examples of methods that change stereotype are further examined. A select number of these types of changes are indicators of code smells.<br/> &copy; 2018 IEEE.},
key={Open systems},
keywords={Codes (symbols);Odors;Open source software;},
note={Code smell;Empirical;Method stereotypes;Software change;Software Evolution;},
URL={http://dx.doi.org/10.1109/SCAM.2018.00017},
}


@article{20121114845267,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using machine learning to improve automatic vectorization},
journal={Transactions on Architecture and Code Optimization},
author={Stock, Kevin and Pouchet, Louis-Noel and Sadayappan, P.},
volume={8},
number={4},
year={2012},
issn={15443566},
abstract={Automatic vectorization is critical to enhancing performance of compute-intensive programs on modern processors. However, there is much room for improvement over the auto-vectorization capabilities of current production compilers through careful vector-code synthesis that utilizes a variety of loop transformations (e.g., unroll-and-jam, interchange, etc.). As the set of transformations considered is increased, the selection of the most effective combination of transformations becomes a significant challenge: Currently used cost models in vectorizing compilers are often unable to identify the best choices. In this paper, we address this problem using machine learning models to predict the performance of SIMD codes. In contrast to existing approaches that have used highlevel features of the program, we develop machine learning models based on features extracted from the generated assembly code. The models are trained offline on a number of benchmarks and used at compile-time to discriminate between numerous possible vectorized variants generated from the input code. We demonstrate the effectiveness of the machine learning model by using it to guide automatic vectorization on a variety of tensor contraction kernels, with improvements ranging from 2&times; to 8&times; over Intel ICC's auto-vectorized code. We also evaluate the effectiveness of the model on a number of stencil computations and show good improvement over auto-vectorized code. &copy; 2012 ACM.<br/>},
key={Learning systems},
keywords={Artificial intelligence;Codes (symbols);Program compilers;},
note={Automatic vectorization;Current production;High-level features;Loop transformation;Machine learning models;Modern processors;Stencil computations;Tensor contraction;},
URL={http://dx.doi.org/10.1145/2086696.2086729},
}


@inproceedings{20134416923370,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Lase: Locating and applying systematic edits by learning from examples},
journal={Proceedings - International Conference on Software Engineering},
author={Meng, Na and Kim, Miryung and McKinley, Kathryn S.},
year={2013},
pages={502 - 511},
issn={02705257},
address={San Francisco, CA, United states},
abstract={Adding features and fixing bugs often require systematic edits that make similar, but not identical, changes to many code locations. Finding all the relevant locations and making the correct edits is a tedious and error-prone process for developers. This paper addresses both problems using edit scripts learned from multiple examples. We design and implement a tool called Lase that (1) creates a context-aware edit script from two or more examples, and uses the script to (2) automatically identify edit locations and to (3) transform the code. We evaluate Lase on an oracle test suite of systematic edits from Eclipse JDT and SWT. Lase finds edit locations with 99% precision and 89% recall, and transforms them with 91% accuracy. We also evaluate Lase on 37 example systematic edits from other open source programs and find Lase is accurate and effective. Furthermore, we confirmed with developers that Lase found edit locations which they missed. Our novel algorithm that learns from multiple examples is critical to achieving high precision and recall; edit scripts created from only one example produce too many false positives, false negatives, or both. Our results indicate that Lase should help developers in automating systematic editing. Whereas most prior work either suggests edit locations or performs simple edits, Lase is the first to do both for nontrivial program edits. &copy; 2013 IEEE.<br/>},
key={Open systems},
keywords={Location;Open source software;},
note={Design and implements;Error-prone process;False negatives;High-precision;Learning from examples;Non-trivial programs;Novel algorithm;Open source projects;},
URL={http://dx.doi.org/10.1109/ICSE.2013.6606596},
}


@book{20172703871015,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Machine Learning: A Bayesian and Optimization Perspective},
journal={Machine Learning: A Bayesian and Optimization Perspective},
author={Theodoridis, Sergios},
year={2015},
pages={1 - 1050},
abstract={This tutorial text gives a unifying perspective on machine learning by covering both probabilistic and deterministic approaches -which are based on optimization techniques - together with the Bayesian inference approach, whose essence lies in the use of a hierarchy of probabilistic models. The book presents the major machine learning methods as they have been developed in different disciplines, such as statistics, statistical and adaptive signal processing and computer science. Focusing on the physical reasoning behind the mathematics, all the various methods and techniques are explained in depth, supported by examples and problems, giving an invaluable resource to the student and researcher for understanding and applying machine learning concepts. The book builds carefully from the basic classical methods to the most recent trends, with chapters written to be as self-contained as possible, making the text suitable for different courses: pattern recognition, statistical/adaptive signal processing, statistical/Bayesian learning, as well as short courses on sparse modeling, deep learning, and probabilistic graphical models. &bull; All major classical techniques: Mean/Least-Squares regression and filtering, Kalman filtering, stochastic approximation and online learning, Bayesian classification, decision trees, logistic regression and boosting methods. &bull; The latest trends: Sparsity, convex analysis and optimization, online distributed algorithms, learning in RKH spaces, Bayesian inference, graphical and hidden Markov models, particle filtering, deep learning, dictionary learning and latent variables modeling. &bull; Case studies - protein folding prediction, optical character recognition, text authorship identification, fMRI data analysis, change point detection, hyperspectral image unmixing, target localization, channel equalization and echo cancellation, show how the theory can be applied. &bull; MATLAB code for all the main algorithms are available on an accompanying website, enabling the reader to experiment with the code.<br/> &copy; 2015 Elsevier Ltd. All rights reserved.},
key={Deep learning},
keywords={Adaptive optics;Artificial intelligence;Bayesian networks;Decision trees;Echo suppression;Hidden Markov models;Inference engines;Kalman filters;MATLAB;Optical character recognition;Radar target recognition;Spectroscopy;Stochastic systems;Teaching;Trees (mathematics);},
note={Adaptive signal processing;Bayesian classification;Change point detection;Machine learning methods;Optimization techniques;Probabilistic graphical models;Protein folding predictions;Stochastic approximations;},
URL={http://dx.doi.org/10.1016/C2013-0-19102-7},
}


@inproceedings{20155301739536,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={When and why your code starts to smell bad},
journal={Proceedings - International Conference on Software Engineering},
author={Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and Olivetox, Rocco and Di Penta, Massimiliano and De Lucia, Andrea and Poshyvanyk, Denys},
volume={1},
year={2015},
pages={403 - 414},
issn={02705257},
address={Florence, Italy},
abstract={In past and recent years, the issues related to managing technical debt received significant attention by researchers from both industry and academia. There are several factors that contribute to technical debt. One of these is represented by code bad smells, i.e., symptoms of poor design and implementation choices. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced. To fill this gap, we conducted a large empirical study over the change history of 200 open source projects from different software ecosystems and investigated when bad smells are introduced by developers, and the circumstances and reasons behind their introduction. Our study required the development of a strategy to identify smellintroducing commits, the mining of over 0.5M commits, and the manual analysis of 9,164 of them (i.e., those identified as smellintroducing). Our findings mostly contradict common wisdom stating that smells are being introduced during evolutionary tasks. In the light of our results, we also call for the need to develop a new generation of recommendation systems aimed at properly planning smell refactoring activities.<br/> &copy; 2015 IEEE.},
key={Open systems},
keywords={Codes (symbols);Odors;Open source software;},
note={Anecdotal evidences;Change history;Design and implementations;Empirical studies;Manual analysis;Open source projects;Software ecosystems;Technical debts;},
URL={http://dx.doi.org/10.1109/ICSE.2015.59},
}


@article{20162202450021,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Numerical predictions for the thermal history, microstructure and hardness distributions at the HAZ during welding of low alloy steels},
journal={Materials Research},
author={Xavier, Carlos Roberto and Delgado, Horacio Guimaraes and De Castro, Jose Adilson and Ferreira, Alexandre Furtado},
volume={19},
number={3},
year={2016},
pages={520 - 533},
issn={15161439},
abstract={A phenomenological model to predict the multiphase diffusional decomposition of the austenite in low-alloy hypoeutectoid steels was adapted for welding conditions. The kinetics of phase transformations coupled with the heat transfer phenomena was numerically implemented using the Finite Volume Method (FVM) in a computational code. The model was applied to simulate the welding of a commercial type of low-alloy hypoeutectoid steel, making it possible to track the phase formations and to predict the volume fractions of ferrite, pearlite and bainite at the heat-affected zone (HAZ). The volume fraction of martensite was calculated using a novel kinetic model based on the optimization of the well-known Koistinen-Marburger model. Results were confronted with the predictions provided by the continuous cooling transformation (CCT) diagram for the investigated steel, allowing the use of the proposed methodology for the microstructure and hardness predictions at the HAZ of low-alloy hypoeutectoid steels.<br/>},
key={Alloy steel},
keywords={Bainitic transformations;Finite volume method;Forecasting;Hardness;Heat affected zone;Heat transfer;High strength steel;Microstructure;Pearlitic transformations;Steel metallurgy;Volume fraction;Welding;},
note={Continuous cooling transformation;Diffusional decomposition;Hardness distribution;Kinetics of phase transformation;Numerical predictions;Phenomenological modeling;Thermal history;Transfer phenomenon;},
URL={http://dx.doi.org/10.1590/1980-5373-MR-2015-0068},
}


@article{20162302472708,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A Literature Review of Research in Bug Resolution: Tasks, Challenges and Future Directions},
journal={Computer Journal},
author={Zhang, Tao and Jiang, He and Luo, Xiapu and Chan, Alvin T.S.},
volume={59},
number={5},
year={2016},
pages={741 - 773},
issn={00104620},
abstract={Due to the increasing scale and complexity of software products, software maintenance especially on bug resolution has become a challenging task. Generally in large-scale software programs, developers depend on software artifacts (e.g., bug report, source code and change history) in bug repositories to complete the bug resolution task. However, a mountain of submitted bug reports every day increase the developers' workload. Therefore, 'How to effectively resolve software defects by utilizing software artifacts?' becomes a research hotspot in software maintenance. Considerable studies have been done on bug resolution by using multi-techniques, which cover data mining, machine learning and natural language processing. In this paper, we present a literature survey on tasks, challenges and future directions of bug resolution in software maintenance process. Our investigation concerns the most important phases in bug resolution, including bug understanding, bug triage and bug fixing. Moreover, we present the advantages and disadvantages of each study. Finally, based on the investigation and comparison results, we propose the future research directions of bug resolution.<br/> &copy; 2015 The British Computer Society.},
key={Computer software maintenance},
keywords={Data mining;Learning algorithms;Learning systems;Natural language processing systems;Program debugging;},
note={Bug reports;bug triage;bug understanding;Bug-fixing;Future research directions;Large-scale software projects;Literature reviews;Software maintenance process;},
URL={http://dx.doi.org/10.1093/comjnl/bxv114},
}


@inproceedings{20140617271276,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning in the limit: A mutational and adaptive approach},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Inojosa Da Silva Filho, Reginaldo and De Azevedo Da Rocha, Ricardo Luis and Gracini Guiraldelli, Ricardo Henrique},
volume={7070 LNAI},
year={2013},
pages={106 - 118},
issn={03029743},
address={Melbourne, VIC, Australia},
abstract={The purpose of this work is to show the strong connection between learning in the limit and the second-order adaptive automaton. The connection is established using the mutating programs approach, in which any hypothesis can be used to start a learning process, and produces a correct final model following a step-by-step transformation of that hypothesis by a second-order adaptive automaton. Second-order adaptive automaton learner will be proved to acts as a learning in the limit one. &copy; 2013 Springer-Verlag Berlin Heidelberg.<br/>},
key={Learning systems},
keywords={Artificial intelligence;Automata theory;},
note={Adaptive approach;Adaptive automata;Adaptive devices;Code mutation;Inductive inference;Learning models;Learning process;Model following;},
URL={http://dx.doi.org/10.1007/978-3-642-44958-1-8},
}


@article{20153201158408,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Experiential Learning of Robotics Fundamentals Based on a Case Study of Robot-Assisted Stereotactic Neurosurgery},
journal={IEEE Transactions on Education},
author={Faria, Carlos and Vale, Carolina and Machado, Toni and Erlhagen, Wolfram and Rito, Manuel and Monteiro, Sergio and Bicho, Estela},
volume={59},
number={2},
year={2016},
pages={119 - 128},
issn={00189359},
abstract={Robotics has been playing an important role in modern surgery, especially in procedures that require extreme precision, such as neurosurgery. This paper addresses the challenge of teaching robotics to undergraduate engineering students, through an experiential learning project of robotics fundamentals based on a case study of robot-assisted stereotactic neurosurgery. The project was integrated into the curriculum of a Biomedical Engineering and Electrical and Computer Engineering program, but can also be integrated in related courses. First, students are given a presentation on the planning and execution of a stereotactic neurosurgery procedure, with special attention being paid to the concepts involved, namely spatial transformations, kinematics, and trajectory planning. Students are then taught to use a robotics simulation tool for robot-assisted stereotactic neurosurgery. They are shown how this can be used as a specialized control application, providing direct feedback on the robot's motion in a neurosurgery scenario. They are then required to select a robotic manipulator, and to develop and implement its control code to make it perform as a robot assistant in this surgical procedure. Project efficacy was evaluated through student self-report data (with dedicated anonymous surveys) and through the impact on academic and pedagogical results (by means of statistical inference). The results of the student surveys show that the robotics simulator for stereotactic neurosurgery is well suited to its role as an experiential learning tool since it enhances the understanding and application of several robotics concepts in an appealing manner. The positive impact of the project learning experience is supported by a comparison to earlier years of student grades, pass rates, and feedback from an institutional survey.<br/> &copy; 2015 IEEE.},
key={Robotic surgery},
keywords={Biomedical engineering;Engineering education;Feedback;Flexible manipulators;Kinematics;Neurosurgery;Robot programming;Robotics;Robots;Students;Surgical equipment;Surveys;Teaching;},
note={Experiential learning;Robot kinematics;Robotic manipulators;Spatial transformation;Trajectory Planning;},
URL={http://dx.doi.org/10.1109/TE.2015.2456176},
}


@article{20154501525834,
language={Chinese},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Error-correcting output codes based on feature space transformation},
journal={Kongzhi yu Juece/Control and Decision},
author={Lei, Lei and Wang, Xiao-Dan and Luo, Xi and Song, Ya-Fei and Xue, Ai-Jun},
volume={30},
number={9},
year={2015},
pages={1597 - 1602},
issn={10010920},
abstract={The independency between each dichotomizer trained by coding matrix's bi-partition is the key to using errorcorrecting output codes (ECOC) to solve multiclass problems. Therefore, an error-correcting output codes method based on feature space transformation (FST) is proposed. Inspired by the ensemble learning theory, a third feature space dimension is introduced into the coding matrix. Then, different subspaces are obtained by feature space transformation based on different positive and negative subclasses, so that the diversity between different binary classifiers are promoted to make the classification performance better. The experiment results based on UCI datasets show that the codes based on FST are better than the original codes. Besides, the proposed method can be applied to any kind of coding matrix, and provides new thought to large dataset for its quick training time and simplicity.<br/> &copy;, 2015, Northeast University. All right reserved.},
key={Codes (symbols)},
keywords={Errors;Natural language processing systems;},
note={Binary classifiers;Classification performance;Ensemble learning;Error correcting output code;Feature space;Independence of dchotomizer;Large dataset;Multi-class problems;},
URL={http://dx.doi.org/10.13195/j.kzyjc.2014.0843},
}


@inproceedings{20174704443953,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automatic calculation of process metrics and their bug prediction capabilities},
journal={Acta Cybernetica},
author={Gyimesi, Peter},
volume={23},
number={2},
year={2017},
pages={537 - 559},
issn={0324721X},
abstract={Identifying fault-prone code parts is useful for the developers to help re-duce the time required for locating bugs. It is usually done by characterizing the already known bugs with certain kinds of metrics and building a predictive model from the data. For the characterization of bugs, software product and process metrics are the most popular ones. The calculation of product metrics is supported by many free and commercial software products. However, tools that are capable of computing process metrics are quite rare. In this study, we present a method of computing software process metrics in a graph database. We describe the schema of the database created and we present a way to readily get the process metrics from it. With this technique, process metrics can be calculated at the file, class and method levels. We used GitHub as the source of the change history and we selected 5 open-source Java projects for processing. To retrieve positional information about the classes and methods, we used SourceMeter, a static source code analyzer tool. We used Neo4j as the graph database engine, and its query language-cypher-to get the process metrics. We published the tools we created as open-source projects on GitHub. To demonstrate the utility of our tools, we selected 25 release versions of the 5 Java projects and calculated the process metrics for all of the source code elements (files, classes and methods) in these versions. Using our previous published bug database, we built bug databases for the selected projects that contain the computed process metrics and the corresponding bug numbers for files and classes. (We published these databases as an online appendix.) Then we applied 13 machine learning algorithms on the database we created to find out if it is feasible for bug prediction purposes. We achieved F-measure values on average of around 0.7 at the class level, and slightly better values of between 0.7 and 0.75 at the file level. The best performing algorithm was the RandomForest method for both cases.<br/>},
key={Java programming language},
keywords={Codes (symbols);Forecasting;Graph Databases;Learning algorithms;Learning systems;Open source software;Program debugging;Query languages;Query processing;},
note={Automatic calculations;Bug predictions;Commercial software products;Computing software;Open source projects;Positional information;Predictive modeling;Process metrics;},
URL={http://dx.doi.org/10.14232/actacyb.23.2.2017.7},
}


@inproceedings{20180904837280,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An Empirical Examination of the Relationship between Code Smells and Merge Conflicts},
journal={International Symposium on Empirical Software Engineering and Measurement},
author={Ahmed, Iftekhar and Brindescu, Caius and Mannan, Umme Ayda and Jensen, Carlos and Sarma, Anita},
volume={2017-November},
year={2017},
pages={58 - 67},
issn={19493770},
address={Toronto, ON, Canada},
abstract={Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both 'smelly' and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.<br/> &copy; 2017 IEEE.},
key={Codes (symbols)},
keywords={Learning systems;Mergers and acquisitions;Odors;Semantics;Software design;Trees (mathematics);},
note={Abstract Syntax Trees;Code smell;Conflict Resolution;Day-to-day operations;Development workflow;Empirical analysis;Empirical examination;Undesirable effects;},
URL={http://dx.doi.org/10.1109/ESEM.2017.12},
}


@article{20135117096747,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Porting existing radiation code for GPU acceleration},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
author={Coleman, Daniel M. and Feldman, Daniel R.},
volume={6},
number={6},
year={2013},
pages={2486 - 2491},
issn={19391404},
abstract={Graphics processing units (GPUs) have proven very robust architectures for performing intensive scientific calculations, resulting in speedups as high as several hundred times. In this paper, the GPU acceleration of a radiation code for use in creating simulated satellite observations of predicted climate change scenarios is explored, particularly the prospect of porting an already existing and widely used radiation transport code to a GPU version that fully exploits the parallel nature of GPUs. The porting process is attempted with a simple radiation code, revealing that this process centers on creating many copies of variables and inlining function/subroutine calls. A resulting speedup of about 25x is reached. This is less than the speedup achieved from a radiation code built for CUDA from scratch, but it was achieved with an already existing radiation code using the PGI Accelerator to automatically generate CUDA kernels, and this demonstrates a possible strategy to speed up other existing models like MODTRAN and LBLRTM. &copy; 2008-2012 IEEE.<br/>},
key={Graphics processing unit},
keywords={Climate change;Codes (symbols);Computer graphics;Computer graphics equipment;Orbits;Program processors;Radiation;Satellite communication systems;},
note={Atmospheric model;Climate change scenarios;GPU accelerations;Hyperspectral sensors;Low earth orbit satellites;Radiation transport codes;Satellite observations;Scientific calculations;},
URL={http://dx.doi.org/10.1109/JSTARS.2013.2247379},
}


@inproceedings{20171903652225,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Robust Traffic Vehicle Lane Change Maneuver Recognition},
journal={SAE Technical Papers},
author={Sun, Hao and Deng, Weiwen and Su, Chen and Wu, Jian},
volume={2017-March},
number={March},
year={2017},
pages={ADIENT; et al.; HITACHI; Mentor Automotive; OMRON; Southwest Research Institute (SwRI) - },
issn={01487191},
address={Detroit, MI, United states},
abstract={The ability to recognize traffic vehicles' lane change maneuver lays the foundation for predicting their long-term trajectories in real-time, which is a key component for Advanced Driver Assistance Systems (ADAS) and autonomous automobiles. Learning-based approach is powerful and efficient, such approach has been used to solve maneuver recognition problems of the ego vehicles on conventional researches. However, since the parameters and driving states of the traffic vehicles are hardly observed by exteroceptive sensors, the performance of traditional methods cannot be guaranteed. In this paper, a novel approach using multi-class probability estimates and Bayesian inference model is proposed for traffic vehicle lane change maneuver recognition. The multi-class recognition problem is first decomposed into three binary problems under error correcting output codes (ECOC) framework. With probability estimates from the three binary classifiers, multi-class probability estimates are obtained through paired team comparisons. A sequence of the multi-class probability estimates are then fed into the Bayesian inference model. The Bayesian inference model views the input as sample of a random variable, and the output of the Bayesian inference model is used for the final recognition. A data set which is collected from a real-time driving simulation platform is used for the training of the binary classifiers. Typical samples are used to evaluate the performance of the proposed approach. The experimental results have demonstrated the improvement of robustness when using the proposed approach, and the approach is able to recognize lane change maneuver of the traffic vehicle with an average prediction horizon of 1.51 seconds.<br/> Copyright &copy; 2017 SAE International.},
key={Vehicles},
keywords={Advanced driver assistance systems;Automobile drivers;Bayesian networks;Classification (of information);Inference engines;Probability;Problem solving;Real time systems;},
note={Bayesian inference model;Error correcting output code;Exteroceptive sensor;Lane change maneuvers;Learning-based approach;Long-term trajectories;Maneuver recognition;Probability estimate;},
URL={http://dx.doi.org/10.4271/2017-01-0110},
}


@inproceedings{20173804183719,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={VuRLE: Automatic vulnerability detection and repair by learning from examples},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Ma, Siqi and Thung, Ferdian and Lo, David and Sun, Cong and Deng, Robert H.},
volume={10493 LNCS},
year={2017},
pages={229 - 246},
issn={03029743},
address={Oslo, Norway},
abstract={Vulnerability becomes a major threat to the security of many systems. Attackers can steal private information and perform harmful actions by exploiting unpatched vulnerabilities. Vulnerabilities often remain undetected for a long time as they may not affect typical systems&rsquo; functionalities. Furthermore, it is often difficult for a developer to fix a vulnerability correctly if he/she is not a security expert. To assist developers to deal with multiple types of vulnerabilities, we propose a new tool, called VuRLE, for automatic detection and repair of vulnerabilities. VuRLE (1) learns transformative edits and their contexts (i.e., code characterizing edit locations) from examples of vulnerable codes and their corresponding repaired codes; (2) clusters similar transformative edits; (3) extracts edit patterns and context patterns to create several repair templates for each cluster. VuRLE uses the context patterns to detect vulnerabilities, and customizes the corresponding edit patterns to repair them. We evaluate VuRLE on 279 vulnerabilities from 48 real-world applications. Under 10-fold cross validation, we compare VuRLE with another automatic repair tool, LASE. Our experiment shows that VuRLE successfully detects 183 out of 279 vulnerabilities, and repairs 101 of them, while LASE can only detect 58 vulnerabilities and repair 21 of them.<br/> &copy; 2017, Springer International Publishing AG.},
key={Repair},
keywords={Codes (symbols);Security of data;Security systems;},
note={10-fold cross-validation;Automatic Detection;Context patterns;Learning from examples;Private information;Security experts;Template generation;Vulnerability detection;},
URL={http://dx.doi.org/10.1007/978-3-319-66399-9_13},
}


@inproceedings{20125015803139,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Heat storage in direct-contact heat exchanger with phase change material},
journal={Applied Thermal Engineering},
author={Nomura, Takahiro and Tsubota, Masakatsu and Oya, Teppei and Okinaka, Noriyuki and Akiyama, Tomohiro},
volume={50},
number={1},
year={2013},
pages={26 - 34},
issn={13594311},
abstract={This paper describes the development and performance of a direct-contact heat exchanger using erythritol (melting point: 391 K) as a phase change material (PCM) and a heat transfer oil (HTO) for accelerating heat storage. A vertical cylinder with 200-mm inner diameter and 1000-mm height was used as the heat storage unit (HSU). A nozzle facing vertically downward was placed at the bottom of the HSU. We examined the effects of flowrate and inlet temperature of the HTO using three characteristic parameters of heat storage - difference between inlet and outlet HTO temperatures, temperature effectiveness, and heat storage rate. The temperature history of latent heat storage (LHS) showed three stages: sensible heat of solid PCM, latent heat of PCM, and sensible heat of liquid PCM. Further, the operating mechanism of the DCHEX was proposed to explain the results. The average heat storage rate during LHS was proportional to the increase in flowrate and inlet temperature of HTO. Thus, latent heat can be rapidly stored under large HTO flowrate and high inlet temperature in the DCHEX. &copy; 2012 Elsevier Ltd. All rights reserved.<br/>},
key={Storage (materials)},
keywords={Cylinders (shapes);Heat exchangers;Heat storage;Heat transfer;Latent heat;Phase change materials;Pulse code modulation;Thermal energy;},
note={Direct contact;Direct contact heat exchangers;Heat exchange;Heat storage units;Operating mechanism;Temperature effectiveness;Temperature history;Vertical cylinders;},
URL={http://dx.doi.org/10.1016/j.applthermaleng.2012.04.062},
}


@inproceedings{20150900591779,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Evolution of model clones in simulink},
journal={CEUR Workshop Proceedings},
author={Stephan, Matthew and Alalfi, Manar H. and Cordy, James R. and Stevenson, Andrew},
volume={1090},
year={2013},
pages={40 - 49},
issn={16130073},
address={Miami, FL, United states},
abstract={A growing and important area of Model-Based Development (MBD) is model evolution. Despite this, very little research on the evolution of Simulink models has been conducted. This is in contrast to the notable amount of research on UML models, which differ significantly from Simulink. Code clones and their evolution across system versions have been used to learn about source-code evolution. We postulate that the same idea can be applied to model clones and model evolution. In this paper, we explore this notion and apply it to Simulink models. We detect model clones in successive versions of MBD projects and, with a new tool, track the evolution of model clones with respect to their containing clone classes. When there is a change in classification of a model-clone, we investigate what specifically evolved in the model to cause this classification change.<br/> &copy; 2013 for the individual papers by the papers' authors.},
key={Cloning},
keywords={Belt drives;Unified Modeling Language;},
note={Clone detection;Code clone;Model based development;Model evolution;Simulink;Simulink models;Source codes;System version;},
}


@inproceedings{20121214876250,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Identifying hotspots in a program for data parallel architecture: An early experience},
journal={Proceedings of the 5th India Software Engineering Conference, ISEC'12},
author={Sarkar, Santonu and Maltouf, Mageri Filali},
year={2012},
pages={131 - 137},
address={Kanpur, India},
abstract={In applications that rely on data intensive computation, one can gain significant performance if the source code is suitably transformed for parallel hardware. A common approach is to identify the loops inside the program that consume a significant amount of time, that we call hotspots. One of the impending business need here is to quickly identify such loops for further transformation. However, the exact identification of such hotspots requires an elaborate runtime analysis. When we deal with a third party business application, only a partial version of the source code is available, with limited test inputs, which hinders a correct runtime analysis. Therefore, we resort to static analysis of source code to get a conservative loop iteration count. In this paper we describe our approach to analyze a source code to find hotspots. Our approach is based on estimating the iteration count of a loop using the polytope model for volume computation. This is then combined with the cyclomatic complexity measurement of the loop body. Both these metrics together provides an approximate idea of hotspots in a program and serves as a code transformation clue to programmers. We have run our tool on Rodinia benchmark applications and found encouraging results. &copy; 2012 ACM.<br/>},
key={Static analysis},
keywords={Benchmarking;Codes (symbols);Computer programming languages;Cosine transforms;Graphics processing unit;Parallel architectures;Program compilers;Software engineering;},
note={Benchmark applications;Business applications;Code transformation;Cyclomatic complexity;Data-intensive computation;Data-parallel architectures;Loop analysis;Polytope models;},
URL={http://dx.doi.org/10.1145/2134254.2134277},
}


@inproceedings{20180304660616,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A characterization study of repeated bug fixes},
journal={Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017},
author={Yue, Ruru and Meng, Na and Wang, Qianxiang},
year={2017},
pages={422 - 432},
address={Shanghai, China},
abstract={Programmers always fix bugs when maintaining software. Previous studies showed that developers apply repeated bug fixes-similar or identical code changes-to multiple locations. Based on the observation, researchers built tools to identify code locations in need of similar changes, or to suggest similar bug fixes to multiple code fragments. However, some fundamental research questions, such as what are the characteristics of repeated bug fixes, are still unexplored. In this paper, we present a comprehensive empirical study with 341,856 bug fixes from 3 open source projects to investigate repeated fixes in terms of their frequency, edit locations, and semantic meanings. Specifically, we sampled bug reports and retrieved the corresponding fixing patches in version history. Then we chopped patches into smaller fixes (edit fragments). Among all the fixes related to a bug, we identified repeated fixes using clone detection, and put a fix and its repeated ones into one repeated-fix group. With these groups, we characterized the edit locations, and investigated the common bug patterns as well as common fixes. Our study on Eclipse JDT, Mozilla Firefox, and LibreOffice shows that (1) 15-20% of bugs involved repeated fixes; (2) 73-92% of repeated-fix groups were applied purely to code clones; and (3) 39% of manually examined groups focused on bugs relevant to additions or deletions of whole if-structures. These results deepened our understanding of repeated fixes. They enabled us to assess the effectiveness of existing tools, and will further provide insights for future research directions in automatic software maintenance and program repair.<br/> &copy; 2017 IEEE.},
key={Program debugging},
keywords={Cloning;Codes (symbols);Computer software maintenance;Costs;Location;Open source software;Repair;Semantics;},
note={Characterization studies;Clone detection;Empirical studies;Fundamental research;Future research directions;Mozilla firefox;Multiple codes;Open source projects;},
URL={http://dx.doi.org/10.1109/ICSME.2017.16},
}


@inproceedings{20154001326577,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Variable provenance in software systems},
journal={4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014 - Proceedings},
author={Chittimalli, Pavan Kumar and Naik, Ravindra},
year={2014},
pages={9 - 13},
address={Hyderabad, India},
abstract={Data Provenance is defined as lineage or history of the given dataitem. Knowing the source of the data or the transformation of the data-source to compute the given data is critical for analyzing the quality of the data. Many transformations of data are done in software (source code). We introduce and define the concept of Variable Provenance for source code. We argue that determining the origin(s) of the data held by a variable and the history of modifications of the variable can provide critical information along many dimensions about what happens in the source code. We use understanding of source code and creating business rules from source code as use-cases to illustrate our view-point. To compute the variable provenance, we combine program slicing techniques and operational rules associated with mathematical operators in computations to propagate the annotations. We predict that the solution to the problem of variable provenance can lead to many use-cases in the software engineering community, effective discovery of processes and business rules from existing systems, and powerful development, debugging and evolution techniques for the software industry.<br/> &copy; 2014 ACM.},
key={Program debugging},
keywords={Codes (symbols);Computer programming languages;Database systems;Mathematical operators;Metadata;Recommender systems;Software engineering;},
note={Business rules;Engineering community;Existing systems;Program analysis;Program comprehension;Provenance;Software industry;Software systems;},
URL={http://dx.doi.org/10.1145/2593822.2593826},
}


@inproceedings{20161302174177,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Who should review this change?: Putting text and file location analyses together for more accurate recommendations},
journal={2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings},
author={Xia, Xin and Lo, David and Wang, Xinyu and Yang, Xiaohu},
year={2015},
pages={261 - 270},
address={Bremen, Germany},
abstract={Software code review is a process of developers inspecting new code changes made by others, to evaluate their quality and identify and fix defects, before integrating them to the main branch of a version control system. Modern Code Review (MCR), a lightweight and tool-based variant of conventional code review, is widely adopted in both open source and proprietary software projects. One challenge that impacts MCR is the assignment of appropriate developers to review a code change. Considering that there could be hundreds of potential code reviewers in a software project, picking suitable reviewers is not a straightforward task. A prior study by Thongtanunam et al. showed that the difficulty in selecting suitable reviewers may delay the review process by an average of 12 days. In this paper, to address the challenge of assigning suitable reviewers to changes, we propose a hybrid and incremental approach Tie which utilizes the advantages of both Text mIning and a filE location-based approach. To do this, Tie integrates an incremental text mining model which analyzes the textual contents in a review request, and a similarity model which measures the similarity of changed file paths and reviewed file paths. We perform a large-scale experiment on four open source projects, namely Android, OpenStack, QT, and LibreOffice, containing a total of 42,045 reviews. The experimental results show that on average Tie can achieve top-1, top-5, and top-10 accuracies, and Mean Reciprocal Rank (MRR) of 0.52, 0.79, 0.85, and 0.64 for the four projects, which improves the state-of-the-art approach RevFinder, proposed by Thongtanunam et al., by 61%, 23%, 8%, and 37%, respectively.<br/> &copy; 2015 IEEE.},
key={Open systems},
keywords={Codes (symbols);Computer software maintenance;Data mining;Open source software;Quality control;Recommender systems;},
note={Code review;Incremental text mining;Large scale experiments;Mean reciprocal ranks;Path similarities;State-of-the-art approach;Text mining;Version control system;},
URL={http://dx.doi.org/10.1109/ICSM.2015.7332472},
}


@inproceedings{20173404060187,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Prioritization of smelly classes: A two phase approach (Reducing refactoring efforts)},
journal={3rd IEEE International Conference on "Computational Intelligence and Communication Technology", CICT 2017},
author={Rani, Anshul and Chhabra, Jitender Kumar},
year={2017},
address={Ghaziabad, India},
abstract={Frequent changes in an object-oriented software system often result into a poor-quality and less maintainable design and the symptoms (known as Code Smells) causing that degradation, need to be corrected for which refactoring is one of the possible solutions. It is not feasible to refactor/ restructure each and every smelly class due to various constraints such as time and cost. Hence it is desirable to make an efficient approach of refactoring. Proposed scheme aims to save time (and cost) of refactoring by carrying out selective refactoring for high priority smelly classes. Prioritization is proposed to be done according to interaction level of each class with other classes. The proposed methodology works in two phases; first phase detects smelly classes using structural information of source code and second phase mines change history to prioritize smelly classes. This prioritization is used to carry out refactoring of more severe classes. This process helps in reducing efforts of refactoring and at the same time may result in avoiding refactoring chains. The proposed technique has been evaluated over a software consisting of 49 classes and results have been validated. The results clearly indicate that the proposed approach performs better and can be very useful for software maintainers in effective and efficient refactoring.<br/> &copy; 2017 IEEE.},
key={Object oriented programming},
keywords={Artificial intelligence;Codes (symbols);Odors;},
note={Change history;Code smell;interaction among classes;Interaction levels;Object oriented software;Object-oriented software systems;Refactorings;Structural information;},
URL={http://dx.doi.org/10.1109/CIACT.2017.7977311},
}


@inproceedings{20130615993978,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={When would this bug get reported?},
journal={IEEE International Conference on Software Maintenance, ICSM},
author={Thung, Ferdian and Lo, David and Jiang, Lingxiao and Lucia, L. and Rahman, Foyzur and Devanbu, Premkumar T.},
year={2012},
pages={420 - 429},
address={Riva del Garda, Trento, Italy},
abstract={Not all bugs in software would be experienced and reported by end users right away: Some bugs manifest themselves quickly and may be reported by users a few days after they get into the code base; others manifest many months or even years later, and may only be experienced and reported by a small number of users. We refer to the period of time between the time when a bug is introduced into code and the time when it is reported by a user as bug reporting latency. Knowledge of bug reporting latencies has an implication on prioritization of bug fixing activities-bugs with low reporting latencies may be fixed earlier than those with high latencies to shift debugging resources towards bugs highly concerning users. To investigate bug reporting latencies, we analyze bugs from three Java software systems: AspectJ, Rhino, and Lucene. We extract bug reporting data from their version control repositories and bug tracking systems, identify bug locations based on bug fixes, and back-trace bug introducing time based on change histories of the buggy code. Also, we remove non-essential changes, and most importantly, recover root causes of bugs from their treatments/fixes. We then calculate the bug reporting latencies, and find that bugs have diverse reporting latencies. Based on the calculated reporting latencies and features we extract from bugs, we build classification models that can predict whether a bug would be reported early (within 30 days) or later, which may be helpful for prioritizing bug fixing activities. Our evaluation on the three software systems shows that our bug reporting latency prediction models could achieve an AUC (Area Under the Receiving Operating Characteristics Curve) of 70.869%. &copy; 2012 IEEE.<br/>},
key={Program debugging},
keywords={Codes (symbols);Computer software;Computer software maintenance;},
note={Bug tracking system;Change history;Classification models;Latency predictions;Prioritization;Receiving operating characteristics;Software systems;Version control;},
URL={http://dx.doi.org/10.1109/ICSM.2012.6405302},
}


@inbook{20182705433686,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Phase Change Memory},
journal={Emerging Nanoelectronic Devices},
author={Jeyasingh, Rakesh and Ahn, Ethan C. and Burc Eryilmaz, S. and Fong, Scott and Philip Wong, H.-S.},
volume={9781118447741},
year={2015},
pages={78 - 109},
abstract={In the late 1960s, Stanford Ovshinsky's (1922-2012) discovery of switching and phase change phenomena in chalcogenide materials seeded new possibilities in data storageunderstanding/application. Concomitantly, innovation in the materials and solid-state memory device research has led to phase change memory (PCM) as one of the potential candidates for future nonvolatile memory technology. PCM has the potential to combine DRAM-like features such as bit alteration, fast read and write, and good endurance and flash-like features such as nonvolatility using a simple device structure. Thus, introduction of PCM in the memory hierarchy would enable a seamless and versatile data exchange between the processor and storage. In this chapter, we explore important PCM materials and device learning in recent years, with a focus on how fundamental physics interact with device properties and the device scaling potential of PCM. We cover basic device operation, properties of the phase change material, potential for scaling to nanoscale dimensions, high-density memory structures, and promising applications of PCM.<br/> &copy; 2015 John Wiley and Sons Ltd.},
key={Phase change memory},
keywords={Chalcogenides;Dynamic random access storage;Electronic data interchange;Flash memory;Nonvolatile storage;Phase change materials;Pulse code modulation;},
note={Electronic synapse;Multilevel cell;Neuromorphic computing;Non-volatile memory;Phase Change;Storage-class memory;},
URL={http://dx.doi.org/10.1002/9781118958254.ch05},
}


@inproceedings{20162502510125,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={DataLab: A version data management and analytics system},
journal={Proceedings - 2nd International Workshop on BIG Data Software Engineering, BIGDSE 2016},
author={Zhang, Yang and Xu, Fangzhou and Frise, Erwin and Wu, Siqi and Yu, Bin and Xu, Wei},
year={2016},
pages={12 - 18},
address={Austin, TX, United states},
abstract={One challenge in big data analytics is the lack of tools to manage the complex interactions among code, data and parameters, especially in the common situation where all these factors can change a lot. We present our preliminary experience with DataLab, a system we build to manage the big data workflow. DataLab improves big data analytical workflow in several novel ways. 1) DataLab manages the revision of both code and data in a coherent system, and includes a distributed code execution engine to run users' code; 2) DataLab keeps track of all the data analytics results in a data work flow graph, and is able to compare the code / results between any two versions, making it easier for users to intuitively see the results of their code change; 3) DataLab provides an efficient data management system to separate data from their metadata, allowing efficient preprocessing filters; and 4) DataLab provides a common API so people can build different applications on top of it. We also present our experience of applying a DataLab prototype in a real bioinformatics application.<br/> &copy; 2016 ACM.},
key={Big data},
keywords={Codes (symbols);Flow graphs;Information management;Software engineering;},
note={Analytics systems;Big Data Analytics;Bioinformatics applications;Code changes;Coherent system;Data analytics;Data management system;Distributed codes;},
URL={http://dx.doi.org/10.1145/2896825.2896830},
}


@article{20184406025582,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A model-driven framework to enhance the consistency of logical integrity constraints: Introducing integrity regression testing},
journal={Software - Practice and Experience},
author={Nooraei Abadeh, Maryam and Ajoudanian, Shohreh},
year={2018},
issn={00380644},
abstract={Although the importance of models continuously grows in software development, common development approaches are less able to integrate the automatic management of model integrity into the development process. These critically important constraints may ensure the coherence of models in the evolution process to prevent manipulations that could violate defined constraints on a model. This paper proposes an integrity framework in the context of model-driven architecture to achieve sufficient structural code coverage at a higher program representation level than machine code. Our framework offers to propagate the modifications from a platform-independent specification to the corresponding test template model while keeping the consistency and integrity constraints after system evolution. To examine the efficiency of the proposed framework, a quantitative analysis plan is evaluated based on two experimental case studies. In addition, we propose coverage criteria for integrity regression testing (IRT), derived from logic coverage criteria that apply different conceptual levels of testing for the formulation of integrity requirements. The defined criteria for IRT reduce the inherent complexity and cost of verifying complex design changes in regression testing while keeping the fault detection capability with respect to the changes. The framework aims to keep pace with IRT in a formal way. The framework can solve a number of restricted outlooks in model integrity and some limiting factors of incremental maintenance and retesting. The framework satisfies several valuable quality attributes in software testing, such as safety percentage, precision, abstract fault detection performance measurable coverage level, and generality.<br/> &copy; 2018 John Wiley & Sons, Ltd.},
key={Software testing},
keywords={Computer circuits;Fault detection;Regression analysis;Safety testing;Software architecture;Software design;},
note={Change history;consistency;integrity rule;logic coverage;Model transformation;Model-driven testing;Regression testing;},
URL={http://dx.doi.org/10.1002/spe.2654},
}


@inproceedings{20173104013074,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={RefDiff: Detecting Refactorings in Version Histories},
journal={IEEE International Working Conference on Mining Software Repositories},
author={Silva, Danilo and Valente, Marco Tulio},
volume={0},
year={2017},
pages={269 - 279},
issn={21601852},
address={Buenos Aires, Argentina},
abstract={Refactoring is a well-known technique that is widely adopted by software engineers to improve the design and enable the evolution of a system. Knowing which refactoring operations were applied in a code change is a valuable information to understand software evolution, adapt software components, merge code changes, and other applications. In this paper, we present RefDiff, an automated approach that identifies refactorings performed between two code revisions in a git repository. RefDiff employs a combination of heuristics based on static analysis and code similarity to detect 13 well-known refactoring types. In an evaluation using an oracle of 448 known refactoring operations, distributed across seven Java projects, our approach achieved precision of 100% and recall of 88%. Moreover, our evaluation suggests that RefDiff has superior precision and recall than existing state-of-The-Art approaches.<br/> &copy; 2017 IEEE.},
key={Static analysis},
keywords={Application programs;Codes (symbols);},
note={Automated approach;Code similarities;Precision and recall;Refactorings;Software component;Software Evolution;Software repositories;State-of-the-art approach;},
URL={http://dx.doi.org/10.1109/MSR.2017.14},
}


@inproceedings{20140217192335,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mining software repositories for accurate authorship},
journal={IEEE International Conference on Software Maintenance, ICSM},
author={Meng, Xiaozhu and Miller, Barton P. and Williams, William R. and Bernat, Andrew R.},
year={2013},
pages={250 - 259},
address={Eindhoven, Netherlands},
abstract={Code authorship information is important for analyzing software quality, performing software forensics, and improving software maintenance. However, current tools assume that the last developer to change a line of code is its author regardless of all earlier changes. This approximation loses important information. We present two new line-level authorship models to overcome this limitation. We first define the repository graph as a graph abstraction for a code repository, in which nodes are the commits and edges represent the development dependencies. Then for each line of code, structural authorship is defined as a sub graph of the repository graph recording all commits that changed the line and the development dependencies between the commits, weighted authorship is defined as a vector of author contribution weights derived from the structural authorship of the line and based on a code change measure between commits, for example, best edit distance. We have implemented our two authorship models as a new git built-in tool git-author. We evaluated git-author in an empirical study and a comparison study. In the empirical study, we ran git-author on five open source projects and found that git-author can recover more information than a current tool (git-blame) for about 10% of lines. In the comparison study, we used git-author to build a line-level model for bug prediction. We compared our line-level model with an existing file-level model. The results show that our line-level model performs consistently better than the file-level model when evaluated on our data sets produced from the Apache HTTP server project. &copy; 2013 IEEE.<br/>},
key={Computer software maintenance},
keywords={Codes (symbols);Computer software selection and evaluation;Open source software;Quality control;},
note={Apache http servers;Author contribution;Bug predictions;Mining software repositories;Open source projects;Software forensics;Software Quality;Version control system;},
URL={http://dx.doi.org/10.1109/ICSM.2013.36},
}


@inproceedings{20175104550405,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automatic detection of significant updates in regulatory documents},
journal={Frontiers in Artificial Intelligence and Applications},
author={Asooja, Kartik and Foghlu, Oscar O. and Domhnaill, Breiffni O. and Marchin, George and McGrath, Sean},
volume={302},
year={2017},
pages={165 - 169},
issn={09226389},
address={Luxembourg, Luxembourg},
abstract={Regulations and legislations are regularly updated, which significantly burdens up the lawyers and compliance officers with a firehose of changes. However, not all changes are significant, and only a percentage of them are of legal importance. This percentage can certainly vary in different types of regulations. This paper focuses on automatic detection or ranking of meaningful legal changes, and presents a preliminary approach based on machine learning for the same, in the domain of Internal Revenue Code (IRC) related regulatory documents. Such system would provide the users with a means to quickly identify significant legal changes.<br/> &copy; 2017 The authors and IOS Press.},
key={Laws and legislation},
keywords={Artificial intelligence;Information systems;Information use;Learning systems;Taxation;},
note={Automatic Detection;Change detection;Internal Revenue Code;Preliminary approach;Regulation;Regulatory change;Regulatory documents;Version;},
URL={http://dx.doi.org/10.3233/978-1-61499-838-9-165},
}


@article{20184105938439,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An empirical study of software change classification with imbalance data-handling methods},
journal={Software - Practice and Experience},
author={Zhu, Xiaoyan and Niu, Binbin and Whitehead, E. James and Sun, Zhongbin},
volume={48},
number={11},
year={2018},
pages={1968 - 1999},
issn={00380644},
abstract={Bug prediction in software code changes can help developers to find out and fix bugs immediately when they are introduced, thus to improve the effectiveness and validity of bug fixing. In data mining, this problem can be regarded as a change classification task. However, one of its key characteristics, ie, class-imbalance, holds back the performance of standard classification methods. In this paper, we consider a quantity of imbalance data-handling methods and extract a more comprehensive groups of change features, aiming to achieve better change classification performance. Two different types of imbalance data-handling methods, namely, resampling and ensemble learning methods, are employed. Especially, we explore the performance of their combination. To compare the performance of different imbalance data-handling methods, an experiment with 10 open source projects is conducted. Four classification methods, including J48, Na&iuml;ve Bayes, SMO, and Random Forest, are used as standard classifiers and as the base classifiers, respectively. Moreover, contribution of different groups of change features are evaluated. Experimental results show that imbalance data-handling methods can improve the performance of change classification and the combination methods, which take advantage of both ensemble learning and resampling, perform better than using ensemble learning methods or resampling methods individually. Of the studied imbalance data-handling methods, the combination of Bagging and random undersampling with J48 as the base classifier yields out better prediction results than those achieved by other methods. Additionally, of the collected change features, text vector features accounts for a larger proportion than others.<br/> &copy; 2018 John Wiley & Sons, Ltd.},
key={Classification (of information)},
keywords={Data handling;Data mining;Decision trees;Forecasting;Learning systems;Open source software;Program debugging;},
note={Bug predictions;Classification methods;Classification performance;Ensemble learning;Imbalance datum;Open source projects;Random under samplings;Resampling;},
URL={http://dx.doi.org/10.1002/spe.2606},
}


@article{20133116555688,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Finite element implementation of multi-pass fillet weld with phase changes},
journal={Manufacturing Technology},
author={Novak, Pavol and Meko, Jozef and Zmindak, Milan},
volume={13},
number={1},
year={2013},
pages={79 - 85},
issn={12132489},
abstract={First, in this paper, a brief review of theoretical aspects of weld simulation and residual stress modelling using the finite element method (FEM) is presented. Thermo-elastic-plastic formulations using a von Mises yield criterion with nonlinear isotropic hardening has been employed. Residual stresses obtained from the analysis have been shown. The commercial FEM code ANSYS and a user created code were used for uncoupled thermalmechanical analysis. Second, the aim of this paper is to compare ANSYS capabilities extended by authors to model weld phenomena versus well known SYSWELD code. Element birth and death FEM technique was used to simulate the weld metal added to base metal due the welding process and to reset plastic history for molten portion of material. Goldak's double ellipsoid heat source was used to model welding heat source. The Leblond's model was used to simulate ferritic and bainitic phase transformations and Koistinen - Marburger model was used to simulate martensitic transformation. &copy; 2013 Published by Manufacturing Technology.<br/>},
key={Finite element method},
keywords={Bainitic transformations;Codes (symbols);Elastoplasticity;Hardening;Martensitic transformations;Residual stresses;Welding;Welds;},
note={Bainitic phase transformation;Finite element implementation;Metallurgical transformation;Nonlinear isotropic hardening;Phase Change;Thermal mechanical analysis;Von Mises yield criterion;Welding heat sources;},
}


@inproceedings{20174904489214,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Gated factored 3-way RBM for image transformation},
journal={2016 13th International Computer Conference on Wavelet Active Media Technology and Information Processing, ICCWAMTIP 2017},
author={Xia, Lei and Yadav, Amit and Duobiao, Ning},
year={2016},
pages={150 - 153},
address={Chengdu, China},
abstract={The Factored 3-way Restricted Boltzmann Machine has encoded the image transformation successfully. But when utilize the code to unknown image, the result was much affected by the feature of training samples. Based on the model, we separated the transformation feature out of the hidden representation and designed a new probabilistic model with gate for learning distributed representations of image transformations. Inference in the model consists extracting the transformation, find the mapping code, training filters to fit for the affine or more general transformations. We also provide experimental results to validate the performance of our model to a various tasks.<br/> &copy; 2016 IEEE.},
key={Codes (symbols)},
keywords={Computer science;Computers;},
note={Distributed representation;Factored;Image transformations;Probabilistic modeling;Restricted boltzmann machine;Training sample;Transformation coding;},
URL={http://dx.doi.org/10.1109/ICCWAMTIP.2016.8079826},
}


@inproceedings{20151300689699,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Quantum cost realization of new reversible gates with transformation based synthesis technique},
journal={2015 International Conference on VLSI Systems, Architecture, Technology and Applications, VLSI-SATA 2015},
author={Jayashree, H.V. and Agrawal, V.K. and Shishir Bharadwaj, N.},
year={2015},
address={Bengaluru, India},
abstract={Reversible computing appears to be promising due its applications in emerging technologies. To compute any reversible function it is necessary to build the system with reversible gates. Simplified version of transformation technique [3,5] to synthesize new reversible gates with Fredkin and Toffoli gates network is presented in this paper. Basic and bidirectional transformation algorithms with an example are illustrated, which uncovers every step of the algorithm. The same example is used for both the algorithms to give clarity on the difference in the efficiency of the algorithms. Simple pseudo code is also presented to illustrate the steps of the algorithm. The best quantum cost obtained is listed in this paper.<br/> &copy; 2015 IEEE.},
key={Logic gates},
keywords={Synthesis (chemical);VLSI circuits;},
note={Bidirectional transformation;Emerging technologies;Fredkin;Reversible Computing;Reversible gates;Toffoli;Transformation;Transformation techniques;},
URL={http://dx.doi.org/10.1109/VLSI-SATA.2015.7050491},
}


@article{20124515644501,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Revised calculation of four-particle harmonic-oscillator transformation brackets matrix},
journal={Computer Physics Communications},
author={Mickevicius, S. and Germanas, D. and Kalinauskas, R.K.},
volume={184},
number={2},
year={2013},
pages={409 - 413},
issn={00104655},
abstract={In this article we present a new, considerably enhanced and more rapid method for calculation of the matrix of four-particle harmonic-oscillator transformation brackets (4HOB). The new method is an improved version of 4HOB matrix calculations which facilitates the matrix calculation by finding the eigenvectors of the 4HOB matrix explicitly. Using this idea the new Fortran code for fast and 4HOB matrix calculation is presented. The calculation time decreases more than a few hundred times for large matrices. As many problems of nuclear and hadron physics structure are modeled on the harmonic oscillator (HO) basis our presented method can be useful for large-scale nuclear structure and many-particle identical fermion systems calculations. &copy; 2012 Elsevier B.V. All rights reserved.<br/>},
key={Linear transformations},
keywords={Fasteners;Harmonic analysis;Matrix algebra;Oscillators (mechanical);},
note={Algebraic method;Calculation time;Harmonic oscillators;Mathematical methods in physics;Matrix calculations;Nuclear shell model;Nuclear structure;Transformation brackets;},
URL={http://dx.doi.org/10.1016/j.cpc.2012.09.021},
}


@inproceedings{20155101693597,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Scalable graph hashing with feature transformation},
journal={IJCAI International Joint Conference on Artificial Intelligence},
author={Jiang, Qing-Yuan and Li, Wu-Jun},
volume={2015-January},
year={2015},
pages={2248 - 2254},
issn={10450823},
address={Buenos Aires, Argentina},
abstract={Hashing has been widely used for approximate nearest neighbor (ANN) search in big data applications because of its low storage cost and fast retrieval speed. The goal of hashing is to map the data points from the original space into a binary-code space where the similarity (neighborhood structure) in the original space is preserved. By directly exploiting the similarity to guide the hashing code learning procedure, graph hashing has attracted much attention. However, most existing graph hashing methods cannot achieve satisfactory performance in real applications due to the high complexity for graph modeling. In this paper, we propose a novel method, called scalable graph hashing with feature transformation (SGH), for large-scale graph hashing. Through feature transformation, we can effectively approximate the whole graph without explicitly computing the similarity graph matrix, based on which a sequential learning method is proposed to learn the hash functions in a bit-wise manner. Experiments on two datasets with one million data points show that our SGH method can outperform the state-of-the-art methods in terms of both accuracy and scalability.<br/>},
key={Big data},
keywords={Artificial intelligence;Digital storage;Hash functions;Linear transformations;Nearest neighbor search;},
note={Approximate nearest neighbors (ANN);Feature transformations;Learning procedures;Neighborhood structure;Real applications;Search in big datum;Sequential learning;State-of-the-art methods;},
}


@inproceedings{20140617266607,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Attributing authorship of revisioned content},
journal={WWW 2013 - Proceedings of the 22nd International Conference on World Wide Web},
author={De Alfaro, Luca and Shavlovsky, Michael},
year={2013},
pages={343 - 353},
address={Rio de Janeiro, Brazil},
abstract={A considerable portion of web content, from wikis to collaboratively edited documents, to code posted online, is revisioned. We consider the problem of attributing authorship to such revisioned content, and we develop scalable attribution algorithms that can be applied to very large bodies of revisioned content, such as the English Wikipedia. Since content can be deleted, only to be later re-inserted, we introduce a notion of authorship that requires comparing each new revision with the entire set of past revisions. For each portion of content in the newest revision, we search the entire history for content matches that are statistically unlikely to occur spontaneously, thus denoting common origin. We use these matches to compute the earliest possible attribution of each word (or each token) of the new content. We show that this \earliest plausible attribution" can be computed efficiently via compact summaries of the past revision history. This leads to an algorithm that runs in time proportional to the sum of the size of the most recent revision, and the total amount of change (edit work) in the revision history. This amount of change is typically much smaller than the total size of all past revisions. The resulting algorithm can scale to very large repositories of revisioned content, as we show via experimental data over the English Wikipedia Copyright is held by the International World Wide Web Conference Committee (IW3C2).<br/>},
key={Websites},
keywords={Hardware;World Wide Web;},
note={Authorship;Revisioned content;Web content;Wikipedia;},
}


@article{20174204270087,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Enhancing developer recommendation with supplementary information via mining historical commits},
journal={Journal of Systems and Software},
author={Sun, Xiaobing and Yang, Hui and Xia, Xin and Li, Bin},
volume={134},
year={2017},
pages={355 - 368},
issn={01641212},
abstract={Given a software issue request, one important activity is to recommend suitable developers to resolve it. A number of approaches have been proposed on developer recommendation. These developer recommendation techniques tend to recommend experienced developers, i.e., the more experienced a developer is, the more possible he/she is recommended. However, if the experienced developers are hectic, the junior developers may be employed to finish the incoming issue. But they may have difficulty in these tasks for lack of development experience. In this article, we propose an approach, EDR_SI, to enhance developer recommendation by considering their expertise and developing habits. Furthermore, EDR_SI also provides the personalized supplementary information for developers to use, such as personalized source code files, developer network and source-code change history. An empirical study on five open source subjects is conducted to evaluate the effectiveness of EDR_SI. In our study, EDR_SI is also compared with the state-of-art developer recommendation techniques, iMacPro, Location and ABA-Time-tf-idf, to evaluate the effectiveness of developer recommendation, and the results show that EDR_SI can not only improve the accuracy of developer recommendation, but also effectively provide useful supplementary information for them to use when they implement the incoming issue requests.<br/> &copy; 2017 Elsevier Inc.},
key={Information use},
keywords={Open source software;},
note={Bug assignment;Commit repository;Developer recommendations;Personalized recommendation;Supplementary information;Topic Modeling;},
URL={http://dx.doi.org/10.1016/j.jss.2017.09.021},
}


@inproceedings{20181104906716,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Code reusability in cloud based ERP solutions},
journal={INES 2017 - IEEE 21st International Conference on Intelligent Engineering Systems, Proceedings},
author={Orosz, Istvan and Orosz, Tamas},
volume={2017-January},
year={2017},
pages={193 - 198},
address={Larnaca, Cyprus},
abstract={Cloud based technology created a new software abstraction layer above the implementation layers in and therefore changed the way how Enterprise Resource Planning (ERP) systems are developed and implemented over the hardware abstraction layers. The traditional release-by-release update methodology governed by main version change (from pre-alpha to gold release) was changed to a continuous release management. Within the cloud based Software as a Service (SaaS) model, the core business logic is implied above the physical implementation layer. This scenario can predict that the software product can have a longer lifetime, because it is segregated from the always changing physical implementation layer. As the sudden change of technology is present in nowadays IT architecture, the presence of this new abstraction layer seems logical, because the basic business processes are not changing this rapidly. The SaaS type life cycle management means that the heavily technology independent part are not describing the business processes anymore. Previous lifecycle implementations from the assessment phase to the post go-live and support phase dealt the business logic as one entity with its implementation. That means, that the question of code reusability has a different role as in the standard on premise model. This paper introduces a new method of encapsulating and identifying the software parts, which can be later reused in a cloud SaaS environment.<br/> &copy;2017 IEEE},
key={Software as a service (SaaS)},
keywords={Abstracting;Codes (symbols);Computer circuits;Enterprise resource planning;Life cycle;Reusability;},
note={Code reusability;Enterprise resource planning (ERP) systems;Hardware Abstraction Layers;Life-cycle management;Release management;SaaS;Software abstractions;Technology independent;},
URL={http://dx.doi.org/10.1109/INES.2017.8118554},
}


@article{20170303260633,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Discriminative transfer subspace learning via low-rank and sparse representation},
journal={IEEE Transactions on Image Processing},
author={Xu, Yong and Fang, Xiaozhao and Wu, Jian and Li, Xuelong and Zhang, David},
volume={25},
number={2},
year={2016},
pages={850 - 863},
issn={10577149},
abstract={In this paper, we address the problem of unsupervised domain transfer learning in which no labels are available in the target domain. We use a transformation matrix to transfer both the source and target data to a common subspace, where each target sample can be represented by a combination of source samples such that the samples from different domains can be well interlaced. In this way, the discrepancy of the source and target domains is reduced. By imposing joint low-rank and sparse constraints on the reconstruction coefficient matrix, the global and local structures of data can be preserved. To enlarge the margins between different classes as much as possible and provide more freedom to diminish the discrepancy, a flexible linear classifier (projection) is obtained by learning a non-negative label relaxation matrix that allows the strict binary label matrix to relax into a slack variable matrix. Our method can avoid a potentially negative transfer by using a sparse matrix to model the noise and, thus, is more robust to different types of noise. We formulate our problem as a constrained low-rankness and sparsity minimization problem and solve it by the inexact augmented Lagrange multiplier method. Extensive experiments on various visual domain adaptation tasks show the superiority of the proposed method over the state-of-the art methods. The MATLAB code of our method will be publicly available at http://www.yongxu.org/lunwen.html.<br/> &copy; 2015 IEEE.},
key={Transfer matrix method},
keywords={Knowledge management;Lagrange multipliers;Linear transformations;MATLAB;Metadata;Problem solving;},
note={Knowledge transfer;Low-rank and sparse constraints;Source domain;Subspace learning;Target domain;},
URL={http://dx.doi.org/10.1109/TIP.2015.2510498},
}


@inproceedings{20182605364195,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Ensemble techniques for software change prediction: A preliminary investigation},
journal={2018 IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, MaLTeSQuE 2018 - Proceedings},
author={Catolino, Gemma and Ferrucci, Filomena},
year={2018},
pages={25 - 30},
address={Campobasso, Italy},
abstract={Predicting the classes more likely to change in the future helps developers to focus on the more critical parts of a software system, with the aim of preventively improving its maintainability. The research community has devoted a lot of effort in the definition of change prediction models, i.e., models exploiting a machine learning classifier to relate a set of independent variables to the change-proneness of classes. Besides the good performances of such models, key results of previous studies highlight how classifiers tend to perform similarly even though they are able to correctly predict the change-proneness of different code elements, possibly indicating the presence of some complementarity among them. In this paper, we aim at analyzing the extent to which ensemble methodologies, i.e., machine learning techniques able to combine multiple classifiers, can improve the performances of change-prediction models. Specifically, we empirically compared the performances of three ensemble techniques (i.e., Boosting, Random Forest, and Bagging) with those of standard machine learning classifiers (i.e., Logistic Regression and Naive Bayes). The study was conducted on eight open source systems and the results showed how ensemble techniques, in some cases, perform better than standard machine learning approaches, even if the differences among them is small. This requires the need of further research aimed at devising effective methodologies to ensemble different classifiers.<br/> &copy; 2018 IEEE.},
key={Learning systems},
keywords={Adaptive boosting;Artificial intelligence;Computer software selection and evaluation;Decision trees;Forecasting;Open source software;Open systems;Quality control;},
note={Change prediction;Empirical studies;Ensemble techniques;Independent variables;Logistic regressions;Machine learning techniques;Research communities;Software change prediction;},
URL={http://dx.doi.org/10.1109/MALTESQUE.2018.8368455},
}


@inproceedings{20183205667126,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A Support Vector Machine Based Approach for Code Smell Detection},
journal={Proceedings - 2017 International Conference on Machine Learning and Data Science, MLDS 2017},
author={Kaur, Amandeep and Jain, Sushma and Goel, Shivani},
volume={2018-January},
year={2018},
pages={9 - 14},
address={Noida, India},
abstract={Code smells may be introduced in software due to market rivalry, work pressure deadline, improper functioning, skills or inexperience of software developers. Code smells indicate problems in design or code which makes software hard to change and maintain. Detecting code smells could reduce the effort of developers, resources and cost of the software. Many researchers have proposed different techniques like DETEX for detecting code smells which have limited precision and recall. To overcome these limitations, a new technique named as SVMCSD has been proposed for the detection of code smells, based on support vector machine learning technique. Four code smells are specified namely God Class, Feature Envy, Data Class and Long Method and the proposed technique is validated on two open source systems namely ArgoUML and Xerces. The accuracy of SVMCSD is found to be better than DETEX in terms of two metrics, precision and recall, when applied on a subset of a system. While considering the entire system, SVMCSD detect more occurrences of code smells than DETEX.<br/> &copy; 2017 IEEE.},
key={Open systems},
keywords={Artificial intelligence;Codes (symbols);Computer software maintenance;Odors;Open source software;Support vector machines;},
note={Anti-patterns;Code smell;Entire system;Open source system;Precision and recall;Software developer;Support vector;Work pressures;},
URL={http://dx.doi.org/10.1109/MLDS.2017.8},
}


@inproceedings{20183205678057,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A replication study: Just-in-time defect prediction with ensemble learning},
journal={Proceedings - International Conference on Software Engineering},
author={Young, Steven and Abdou, Tamer and Bener, Ayse},
volume={Part F137725},
year={2018},
pages={42 - 47},
issn={02705257},
address={Gothenburg, Sweden},
abstract={Just-in-time defect prediction, which is also known as change-level defect prediction, can be used to efficiently allocate resources and manage project schedules in the software testing and debugging process. Just-in-time defect prediction can reduce the amount of code to review and simplify the assignment of developers to bug fixes. This paper reports a replicated experiment and an extension comparing the prediction of defect-prone changes using traditional machine learning techniques and ensemble learning. Using datasets from six open source projects, namely Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL we replicate the original approach to verify the results of the original experiment and use them as a basis for comparison for alternatives in the approach. Our results from the replicated experiment are consistent with the original. The original approach uses a combination of data preprocessing and a two-layer ensemble of decision trees. The first layer uses bagging to form multiple random forests. The second layer stacks the forests together with equal weights. Generalizing the approach to allow the use of any arbitrary set of classifiers in the ensemble, optimizing the weights of the classifiers, and allowing additional layers, we apply a new deep ensemble approach, called deep super learner, to test the depth of the original study. The deep super learner achieves statistically significantly better results than the original approach on five of the six projects in predicting defects as measured by F<inf>1</inf> score.<br/> &copy; 2018 ACM.},
key={Deep learning},
keywords={Artificial intelligence;Decision trees;Defects;Forecasting;Just in time production;Open source software;Program debugging;Software testing;},
note={Data preprocessing;Defect prediction;Ensemble approaches;Machine learning techniques;Open source projects;Project schedules;Replicated experiment;Software Testing and Debugging;},
URL={http://dx.doi.org/10.1145/3194104.3194110},
}


@article{20154801631048,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Modeling of unstable behaviors of shape memory alloys during localization and propagation of phase transformation using a gradient-enhanced model},
journal={Journal of Intelligent Material Systems and Structures},
author={Badnava, Hojat and Kadkhodaei, Mahmoud and Mashayekhi, Mohammad},
volume={26},
number={18},
year={2015},
pages={2531 - 2546},
issn={1045389X},
abstract={In this article, a multi-dimensional gradient-enhanced constitutive model of shape memory alloys is developed. The model is developed to capture unstable behaviors of shape memory alloys during both the forward and reverse phase transformations. Also, influence of loading history on the start of the phase transformation during both forward and reverse transformations is considered in the model by introducing new transformation limits and phase fraction formulations. The model is implemented in a finite element code, and using a numerical framework, effects of loading, boundary condition, inhomogeneous deformations, imperfection, and geometry on the unstable behaviors of different shape memory alloy samples during nucleation and phase transformation are investigated. The obtained results are compared with the available experimental and numerical results in the literature. The obtained results show that the gradient enhancement removes pathological localization effects which would typically result from the softening influence. In addition, the numerical study proves that the mode can capture the basic features of phase transformation front patterns and their evolution during transformations.<br/> &copy; SAGE Publications.},
key={Shape memory effect},
keywords={Alloying;Finite element method;Optimization;Phase transitions;},
note={Finite element codes;Inhomogeneous deformation;Localization effect;Multi dimensional;Reverse phase transformation;Reverse Transformation;Shape memory;Unstable behavior;},
URL={http://dx.doi.org/10.1177/1045389X15604407},
}


@inproceedings{20143918177392,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The learning curves in Open-Source Software (OSS) development network},
journal={ACM International Conference Proceeding Series},
author={Kim, Youngsoo and Jiang, Lingxiao},
year={2014},
pages={41 - 48},
address={Philadelphia, PA, United states},
abstract={We examine the learning curves of individual software developers in Open-Source Software (OSS) Development. We collected the dataset of multi-year code change histories from the repositories for five open source software projects involving more than 100 developers. We build and estimate regression models to assess individual developers' learning progress (in reducing the likelihood they may make a bug). Our estimation results show that developer's coding experience does not decrease bug ratios while cumulative bug-fixing experience leads to learning progress. The results may have implications and provoke future research on project management about allocating resources on tasks that add new code versus tasks that debug and fix existing code. We also find that different developers indeed make different kinds of bug patterns, supporting personalized bug prediction in OSS network. We found the moderating effects of bug types on learning progress. Developers exhibit learning effects for some simple bug types (e.g., wrong literals) or bug types with many instances (e.g., wrong if conditionals). &copy; 2014 ACM.<br/>},
key={Open source software},
keywords={Codes (symbols);Electronic commerce;Open systems;Project management;Regression analysis;},
note={Estimation results;Learning curves;Learning effects;Learning progress;Moderating effect;Open source software projects;Regression model;Software developer;},
URL={http://dx.doi.org/10.1145/2617848.2617857},
}


@article{20171403538341,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={TLEL: A two-layer ensemble learning approach for just-in-time defect prediction},
journal={Information and Software Technology},
author={Yang, Xinli and Lo, David and Xia, Xin and Sun, Jianling},
volume={87},
year={2017},
pages={206 - 220},
issn={09505849},
abstract={Context Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time [1]. Objective Ensemble learning becomes a hot topic in recent years. There have been several studies about applying ensemble learning to defect prediction [2&ndash;5]. Traditional ensemble learning approaches only have one layer, i.e., they use ensemble learning once. There are few studies that leverages ensemble learning twice or more. To bridge this research gap, we try to hybridize various ensemble learning methods to see if it will improve the performance of just-in-time defect prediction. In particular, we focus on one way to do this by hybridizing bagging and stacking together and leave other possibly hybridization strategies for future work. Method In this paper, we propose a two-layer ensemble learning approach TLEL which leverages decision tree and ensemble learning to improve the performance of just-in-time defect prediction. In the inner layer, we combine decision tree and bagging to build a Random Forest model. In the outer layer, we use random under-sampling to train many different Random Forest models and use stacking to ensemble them once more. Results To evaluate the performance of TLEL, we use two metrics, i.e., cost effectiveness and F1-score. We perform experiments on the datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. Also, we compare our approach with three baselines, i.e., Deeper, the approach proposed by us [6], DNC, the approach proposed by Wang et&nbsp;al. [2], and MKEL, the approach proposed by Wang et&nbsp;al. [3]. The experimental results show that on average across the six datasets, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code, as compared with about 50% for the baselines. In addition, the F1-scores TLEL can achieve are substantially and statistically significantly higher than those of three baselines across the six datasets. Conclusion TLEL can achieve a substantial and statistically significant improvement over the state-of-the-art methods, i.e., Deeper, DNC and MKEL. Moreover, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code.<br/> &copy; 2017 Elsevier B.V.},
key={Defects},
keywords={Computer software selection and evaluation;Cost effectiveness;Costs;Decision trees;Forecasting;Just in time production;Open source software;},
note={Defect prediction;Development process;Ensemble learning;Ensemble learning approach;Open source projects;Random forest modeling;Random under samplings;State-of-the-art methods;},
URL={http://dx.doi.org/10.1016/j.infsof.2017.03.007},
}


@inproceedings{20133616687546,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={It's alive! Continuous feedback in UI programming},
journal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
author={Burckhardt, Sebastian and De Halleux, Peli and Moskal, Michal and Fahndrich, Manuel and McDirmid, Sean and Tillmann, Nikolai and Kato, Jun},
year={2013},
pages={95 - 104},
address={Seattle, WA, United states},
abstract={Live programming allows programmers to edit the code of a running program and immediately see the effect of the code changes. This tightening of the traditional edit-compile-run cycle reduces the cognitive gap between program code and execution, improving the learning experience of beginning programmers while boosting the productivity of seasoned ones. Unfortunately, live programming is difficult to realize in practice as imperative languages lack well-defined abstraction boundaries that make live programming responsive or its feedback comprehensible. This paper enables live programming for user interface programming by cleanly separating the rendering and non-rendering aspects of a UI program, allowing the display to be refreshed on a code change without restarting the program. A type and effect system formalizes this separation and provides an evaluation model that incorporates the code update step. By putting live programming on a more formal footing, we hope to enable critical and technical discussion of live programming systems. Copyright &copy; 2013 ACM.<br/>},
key={Codes (symbols)},
keywords={Computer programming languages;Graphical user interfaces;Rendering (computer graphics);},
note={Code changes;Cognitive gap;Evaluation modeling;Imperative languages;Learning experiences;Program code;Programming system;Type and effect systems;},
URL={http://dx.doi.org/10.1145/2462156.2462170},
}


@inproceedings{20132916516256,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={It's alive! continuous feedback in ui programming},
journal={ACM SIGPLAN Notices},
author={Burckhardt, Sebastian and Fahndrich, Manuel and De Halleux, Peli and McDirmid, Sean and Moskal, Michal and Tillmann, Nikolai and Kato, Jun},
volume={48},
number={6},
year={2013},
pages={95 - 104},
issn={15232867},
abstract={Live programming allows programmers to edit the code of a running program and immediately see the effect of the code changes. This tightening of the traditional edit-compile-run cycle reduces the cognitive gap between program code and execution, improving the learning experience of beginning programmers while boosting the productivity of seasoned ones. Unfortunately, live programming is difficult to realize in practice as imperative languages lack welldefined abstraction boundaries that make live programming responsive or its feedback comprehensible. This paper enables live programming for user interface programming by cleanly separating the rendering and non-rendering aspects of a UI program, allowing the display to be refreshed on a code change without restarting the program. A type and effect system formalizes this separation and provides an evaluation model that incorporates the code update step. By putting live programming on a more formal footing, we hope to enable critical and technical discussion of live programming systems.<br/>},
key={Codes (symbols)},
keywords={Graphical user interfaces;Rendering (computer graphics);},
note={Code changes;Cognitive gap;Evaluation modeling;Imperative languages;Learning experiences;Program code;Programming system;Type and effect systems;},
URL={http://dx.doi.org/10.1145/2499370.2462170},
}


@inproceedings{20123115287350,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Dispersion of changes in cloned and non-cloned code},
journal={2012 6th International Workshop on Software Clones, IWSC 2012 - Proceedings},
author={Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.},
year={2012},
pages={29 - 35},
address={Zurich, Switzerland},
abstract={Currently, the impacts of clones in software maintenance activities are being investigated by different researchers in different ways. Comparative stability analysis of cloned and non-cloned regions of a subject system is a well-known way of measuring the impacts where the hypothesis is that, the more a region is stable the less it is harmful for maintenance. Each of the existing stability measurement methods lacks to address one important characteristic, dispersion, of the changes happening in the cloned and non-cloned regions of software systems. Change dispersion of a particular region quantifies the extent to which the changes are scattered over that region. The intuition is that, more dispersed changes require more efforts to be spent in the maintenance phase. Measurement of Dispersion requires the extraction of method genealogies. In this paper, we have measured the dispersions of changes in cloned and non-cloned regions of several subject systems using a concurrent and robust framework for method genealogy extraction. We implemented the framework on Actor Architecture platform which facilitates coarse grained parallellism with asynchronous message passing capabilities. Our experimental results on 12 open-source subject systems written in three different programming languages (Java, C and C#) using two clone detection tools suggest that, the changes in cloned regions are more dispersed than the changes in non-cloned regions. Also, Type-3 clones exhibit more dispersion as compared to the Type-1 and Type-2 clones. The subject systems written in Java and C show higher dispersions as well as increased maintenance efforts as compared to the subject systems written in C#. &copy; 2012 IEEE.<br/>},
key={Cloning},
keywords={Dispersion (waves);Dispersions;Extraction;History;Java programming language;Maintenance;Message passing;Open source software;Open systems;System stability;},
note={Architecture platforms;Average Last Change Date;Changeability;Concurrent Framework;Maintenance efforts;Modification Frequency;Software maintenance activity;Stability measurements;},
URL={http://dx.doi.org/10.1109/IWSC.2012.6227863},
}


@article{20160902036539,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Dynamic background estimation and complementary learning for pixel-wise foreground/background segmentation},
journal={Pattern Recognition},
author={Ge, Weifeng and Guo, Zhenhua and Dong, Yuhan and Chen, Youbin},
volume={59},
year={2016},
pages={112 - 125},
issn={00313203},
abstract={Change and motion detection plays a basic and guiding role in surveillance video analysis. Since most outdoor surveillance videos are taken in native and complex environments, these &ldquo;static&rdquo; backgrounds change in some unknown patterns, which make perfect foreground extraction very difficult. This paper presents two universal modifications for pixel-wise foreground/background segmentation: dynamic background estimation and complementary learning. These two modifications are embedded in three classic background subtraction algorithms: probability based background subtraction (Gaussian mixture model, GMM), sample based background subtraction (visual background extractor, ViBe) and code words based background subtraction (code book, CB). Experiments on several popular public datasets prove the effectiveness and real-time performance of the proposed method. Both GMM and CB with the proposed modifications have better performance than the original versions. Especially, ViBe with the modifications outperforms some state-of-art algorithms presented on the CHANGEDETECTION website.<br/> &copy; 2016 Elsevier Ltd},
key={Security systems},
keywords={Gaussian distribution;Image segmentation;Pixels;},
note={Background subtraction;Background subtraction algorithms;Complementary learning;Dynamic background;Foreground/background segmentation;Gaussian Mixture Model;Real time performance;ViBe;},
URL={http://dx.doi.org/10.1016/j.patcog.2016.01.031},
}


@article{20151100645680,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Proposal for e-learning system with expandable and developable functions},
journal={International Journal of Innovative Computing, Information and Control},
author={Mabuchi, Hiroshi and Satoh, Toshiki},
volume={11},
number={2},
year={2015},
pages={743 - 757},
issn={13494198},
abstract={Although e-learning systems are used by a wide number of learners and teachers, it is not necessarily true that all the functions they need are provided. In this study, we will discuss how to develop an e-learning system which will allow learners and teachers to create system functions and then share them by incorporating these functions into the system. E-learning systems are implemented through web applications. We will explain the reason why it is difficult to create these web applications and discuss a specification program which abstractly describes a common gateway interface (CGI) program, HTML and database access the elements of a web application. This specification program is described by an ET program, one based on the theory of equivalent transformation (ET) models, and expresses the web application&rsquo;s parallelism through the ET program framework. We will also explain how to incorporate a specification program into an e-learning system by transforming the program into an actual web application. Compared to conventional methods in which CGI programs, HTML and database access are directly scripted, the method, proposed in this study can clearly describe with a lower amount of code and, without requiring background, knowledge.<br/> &copy; 2015 ISSN 1349-4198.},
key={Learning systems},
keywords={Application programs;Database systems;E-learning;HTML;Specifications;Teaching;},
note={Common gateway interface;Conventional methods;Database access;Equivalent transformations;System functions;WEB application;},
}


@inproceedings{20164302940954,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The role of learning in construction technology transfer: A 'SCOT' perspective},
journal={Proceedings of the 32nd Annual ARCOM Conference, ARCOM 2016},
author={Oti-Sarpong, Kwadwo and Leiringer, Roine},
year={2016},
pages={699 - 708},
address={Manchester, United kingdom},
abstract={Technology transfer (TT) has been given increasing importance since the formulation of the international code of conduct for technology transfer by the UNCTAD in 1985, and has become a preferred medium to bridge development gaps between developed and developing countries. Concomitantly, international joint ventures (IJVs) have been put forward as vehicles for change in the belief that contractors in developing countries can position themselves to receive technology from their developed counterparts. So far, TT has been studied through a variety of theoretical lenses. However, predominantly, the perspectives taken have assumed a linear process, viewing technology merely as an object, and effectively disregarding the multiple interactions involved in TT. In this paper, we argue that such perspectives only provide partial explanations of what construction technology entails, and how it is transferred between organisations. A counter-argument is put forward to view TT as a process of socio-technical interactions that is reliant on learning. Adopting the theoretical lens of the Social Construction of Technology (SCOT), we show how the SCOT framework allows for examining the socio-technical interactions between human actors and construction technology in TT. Specifically, we use the SCOT constructs of 'interpretative flexibility' and, 'closure and stabilisation' to reveal how learning is an integral process within the socio-technical interactions, which plays a critical role in TT between contractors in IJVs. Conclusions are drawn, highlighting the importance of studying TT as a system of socio-technical interactions on a construction project, in order to understand how learning plays a role in the process.<br/>},
key={Engineering education},
keywords={Contractors;Developing countries;Project management;Technology transfer;},
note={Construction projects;Construction technologies;International codes;International joint ventures;Interpretative flexibility;Learning;Multiple interactions;Social construction of technology;},
}


@inproceedings{20182705402429,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={CASPER: An efficient approach to detect anomalous code execution from unintended electronic device emissions},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
author={Agrawal, Hira and Chen, Ray and Hollingsworth, Jeffrey K. and Hung, Christine and Izmailov, Rauf and Koshy, John and Liberti, Joe and Mesterharm, Chris and Morman, Josh and Panagos, Thimios and Pucci, Marc and Sebuktekin, Iil and Alexander, Scott and Tsang, Simon},
volume={10630},
year={2018},
pages={The Society of Photo-Optical Instrumentation Engineers (SPIE) - },
issn={0277786X},
address={Orlando, FL, United states},
abstract={The CASPER system offers a lightweight, multi-disciplinary approach to detect the execution of anomalous code by monitoring the unintended electronic device emissions. Using commodity hardware and a combination of novel signal processing, machine learning, and program analysis techniques, we have demonstrated the ability to detect unknown code running on a device placed 12" from the CASPER system by analyzing the devices RF emissions. Our innovations for the sensors subsystem include multi-antenna processing algorithms which allow us to extend range and extract signal features in the presence of background noise and interference encountered in realistic training and monitoring environments. In addition, robust feature estimation methods have been developed that allow detection of device operating conditions in the presence of varying clock frequency and other aspects that may change from device to device or from training to monitoring. Furthermore, a band-scan technique has been implemented to automatically identify suitable frequency bands for monitoring based on a set of metrics including received power, expected spectral feature content (based on loop length and clock frequency), kurtosis, and mode clustering. CASPER also includes an auto-labeling feature that is used to discover the signal processing features that provide the greatest information for detection without human intervention. The system additionally includes a framework for anomaly detection engines, currently populated with three engines based on n-grams, statistical frequency, and control flow. As we will describe, the combination of these engines reduces the ways in which an attacker can adapt in an attempt to hide from CASPER. We will describe the CASPER concept, components and technologies used, a summary of results to-date, and plans for further development. CASPER is an ongoing research project funded under the DARPA LADS program.<br/> &copy; 2018 SPIE.},
key={Feature extraction},
keywords={Antennas;Clocks;Codes (symbols);Electron devices;Engines;Frequency estimation;Learning systems;Signal processing;Thermoelectric equipment;},
note={Commodity hardware;Feature estimation;Human intervention;Monitoring environment;Multi-disciplinary approach;Operating condition;Statistical frequency;Suitable frequency bands;},
URL={http://dx.doi.org/10.1117/12.2500234},
}


@inproceedings{20143918177202,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Assessment of subchannel code ASSERT-PV for prediction of critical heat flux in CANDU bundles},
journal={International Congress on Advances in Nuclear Power Plants, ICAPP 2014},
author={Rao, Y.F. and Cheng, Z. and Waddington, G.M.},
volume={2},
year={2014},
pages={1532 - 1539},
address={Charlotte, NC, United states},
abstract={Atomic Energy of Canada Limited (AECL) has developed the subchannel thermalhydraulics code ASSERT-PV for the Canadian nuclear industry. The recently-released ASSERT-PV 3.2 provides enhanced models for improved predictions of flow distribution, critical heat flux (CHF), and post-dryout (PDO) heat transfer in horizontal CANDU fuel channels. This paper presents results of an assessment of the new code version against five full-scale CANDU bundle experiments conducted in 1990s and in 2009 by Stern Laboratories (SL), using 28-, 37- and 43-element (CANFLEX) bundles. A total of 15 CHF test series with varying pressure-tube creep and/or bearing-pad height were analyzed. The SL experiments encompass the bundle geometries and range of flow conditions for the intended ASSERT-PV applications for CANDU reactors. Code predictions of channel dryout power and axial and radial CHF locations were compared against measurements from the SL CHF tests to quantify the code prediction accuracy. The prediction statistics using the recommended model set of ASSERT-PV 3.2 were compared to those from previous code versions. Furthermore, the sensitivity studies evaluated the contribution of each CHF model change or enhancement to the improvement in CHF prediction. Overall, the assessment demonstrated significant improvement in prediction of channel dryout power and axial and radial CHF locations in horizontal fuel channels containing CANDU bundles.<br/>},
key={Heat flux},
keywords={Codes (symbols);Forecasting;Heat transfer;Nuclear energy;Nuclear fuels;Nuclear industry;Nuclear power plants;Nuclear reactors;Two phase flow;},
note={Atomic energy of canada limiteds;Canadian nuclear industry;Code predictions;Critical heat flux(CHF);Flow distribution;Sensitivity studies;Subchannel code;Thermal hydraulics codes;},
}


@inproceedings{20171503559587,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Does cloned code increase maintenance effort?},
journal={IWSC 2017 - 11th IEEE International Workshop on Software Clones, co-located with SANER 2017},
author={Mondal, Manishankar and Roy, Chanchal K. and Schneider, Kevin A.},
year={2017},
pages={Alpen-Adria Universitat; IEEE; IEEE Computer Society; Technical Council on Software Engineering (TCSE) - },
address={Klagenfurt, Austria},
abstract={In-spite of a number of in-depth investigations regarding the impact of clones in the maintenance phase there is no concrete answer to the long lived research question, Does the presence of code clones increase maintenance effort?. Existing studies have measured different change related metrics for cloned and non-cloned regions, however, no study calculates the maintenance effort spent for these code regions. In this paper, we perform an in-depth empirical study in order to compare the maintenance efforts required for cloned and non-cloned code. For the purpose of our study we implement a prototype tool which is capable of estimating the effort spent by a developer for changing a particular method. It can also predict effort that might need to be spent for making some changes to a particular method. Our estimation and prediction involve automatic extraction and analysis of the entire evolution history of a candidate software system. We applied our tool on hundreds of revisions of six open source subject systems written in three different programming languages for calculating the efforts spent for cloned and non-cloned code. According to our experimental results: (i) cloned code requires more effort in the maintenance phase than non-cloned code, and (ii) Type 2 and Type 3 clones require more effort compared to the efforts required by Type 1 clones. According to our findings, we should prioritize Type 2 and Type 3 clones when making clone management decisions.<br/> &copy; 2017 IEEE.},
key={Cloning},
keywords={Codes (symbols);Maintenance;Open source software;Open systems;},
note={Automatic extraction;Clone management;Empirical studies;Estimation and predictions;Evolution history;Maintenance efforts;Research questions;Software systems;},
URL={http://dx.doi.org/10.1109/IWSC.2017.7880507},
}


@article{20121915006482,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Detecting heap-spraying code injection attacks in malicious web pages using runtime execution},
journal={IEICE Transactions on Communications},
author={Choi, Younghan and Kim, Hyoungchun and Lee, Donghoon},
volume={E95-B},
number={5},
year={2012},
pages={1711 - 1721},
issn={09168516},
abstract={The growing use of web services is increasing web browser attacks exponentially. Most attacks use a technique called heap spraying because of its high success rate. Heap spraying executes a malicious code without indicating the exact address of the code by copying it into many heap objects. For this reason, the attack has a high potential to succeed if only the vulnerability is exploited. Thus, attackers have recently begun using this technique because it is easy to use JavaScript to allocate the heap memory area. This paper proposes a novel technique that detects heap spraying attacks by executing a heap object in a real environment, irrespective of the version and patch status of the web browser. This runtime execution is used to detect various forms of heap spraying attacks, such as encoding and polymorphism. Heap objects are executed after being filtered on the basis of patterns of heap spraying attacks in order to reduce the overhead of the runtime execution. Patterns of heap spraying attacks are based on analysis of how an web browser accesses benign web sites. The heap objects are executed forcibly by changing the instruction register into the address of them after being loaded into memory. Thus, we can execute the malicious code without having to consider the version and patch status of the browser. An object is considered to contain a malicious code if the execution reaches a call instruction and then the instruction accesses the API of system libraries, such as kernel32.dll and ws 32.dll. To change registers and monitor execution flow, we used a debugger engine. A prototype, named HERAD(HEap spRAying Detector), is implemented and evaluated. In experiments, HERAD detects various forms of exploit code that an emulation cannot detect, and some heap spraying attacks that NOZZLE cannot detect. Although it has an execution overhead, HERAD produces a low number of false alarms. The processing time of several minutes is negligible because our research focuses on detecting heap spraying. This research can be applied to existing systems that collect malicious codes, such as Honeypot. Copyright &copy; 2012 The Institute of Electronics, Information and Communication Engineers.<br/>},
key={Malware},
keywords={Codes (symbols);Spray nozzles;Web browsers;Web services;Websites;},
note={Code injection attacks;Heap spraying;Instruction register;Malicious web pages;Malware detection;Number of false alarms;Real environments;Run-time execution;},
URL={http://dx.doi.org/10.1587/transcom.E95.B.1711},
}


@inproceedings{20183605764975,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Wiretap Polar Codes in Encryption Schemes Based on Learning with Errors Problem},
journal={IEEE International Symposium on Information Theory - Proceedings},
author={Rajagopalan, Aswin and Thangaraj, Andrew and Agrawal, Shweta},
volume={2018-June},
year={2018},
pages={1146 - 1150},
issn={21578095},
address={Vail, CO, United states},
abstract={The Learning with Errors (LWE) problem has been extensively studied in cryptography due to its strong hardness guarantees, efficiency and expressiveness in constructing advanced cryptographic primitives. In this work, we show that using polar codes in conjunction with LWE-based encryption yields several advantages. To begin, we demonstrate the obvious improvements in the efficiency or rate of information transmission in the LWE-based scheme by leveraging polar coding (with no change in the cryptographic security guarantee). Next, we integrate wiretap polar coding with LWE-based encryption to ensure provable semantic security over a wiretap channel in addition to cryptographic security based on the hardness of LWE. To the best of our knowledge this is the first wiretap code to have cryptographic security guarantees as well. Finally, we study the security of the private key used in LWE-based encryption with wiretap polar coding, and propose a key refresh method using random bits used in wiretap coding. Under a known-plaintext attack, we show that non-vanishing information-theoretic secrecy can be achieved for the key. We believe our approach is at least as interesting as our final results: our work combines cryptography and coding theory in a novel 'non blackbox-way' which may be relevant to other scenarios as well.<br/> &copy; 2018 IEEE.},
key={Cryptography},
keywords={Codes (symbols);Computer programming;Efficiency;Hardness;Semantics;},
note={Cryptographic primitives;Cryptographic security;Encryption schemes;Information transmission;Information-theoretic secrecies;Known-plaintext attacks;Learning with Errors;Learning with errors problems;},
URL={http://dx.doi.org/10.1109/ISIT.2018.8437896},
}


@inproceedings{20171203486039,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A change guide method based on developers' interaction and past recommendation},
journal={Proceedings of the 13th IASTED International Conference on Parallel and Distributed Computing and Networks, PDCN 2016},
author={Yamamori, Akihiro and Kobayashi, Takashi},
year={2016},
pages={281 - 288},
address={Innsbruck, Austria},
abstract={In this paper, we propose a change guide method based on the past developers' activity that consists of read and write access records of artifacts. In our proposed method, we calculate candidates of next change recommendation considering the history of its recommendations. We define "cumulative likelihood" to enable the method to recommend the appropriate candidates when a change propagates more than one code elements. A case study using interaction history logs from 15 participants showed the improvement of the accuracy of the method-level change recommendation.<br/>},
key={Distributed computer systems},
keywords={Computer networks;Computer software maintenance;Hardware;Information systems;},
note={Change guide;Interaction history;Level change;Software repository mining;},
URL={http://dx.doi.org/10.2316/P.2016.835-012},
}


@inproceedings{20185106252814,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automatic clone recommendation for refactoring based on the present and the past},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
author={Yue, Ruru and Gao, Zhe and Meng, Na and Xiong, Yingfei and Wang, Xiaoyin and Morgenthaler, J. David},
year={2018},
pages={115 - 126},
address={Madrid, Spain},
abstract={When many clones are detected in software programs, not all clones are equally important to developers. To help developers refactor code and improve software quality, various tools were built to recommend clone-removal refactorings based on the past and the present information, such as the cohesion degree of individual clones or the co-evolution relations of clone peers. The existence of these tools inspired us to build an approach that considers as many factors as possible to more accurately recommend clones. This paper introduces CREC, a learning-based approach that recommends clones by extracting features from the current status and past history of software projects. Given a set of software repositories, CREC first automatically extracts the clone groups historically refactored (R-clones) and those not refactored (NR-clones) to construct the training set. CREC extracts 34 features to characterize the content and evolution behaviors of individual clones, as well as the spatial, syntactical, and co-change relations of clone peers. With these features, CREC trains a classifier that recommends clones for refactoring. We designed the largest feature set thus far for clone recommendation, and performed an evaluation on six large projects. The results show that our approach suggested refactorings with 83% and 76% F-scores in the within-project and cross-project settings. CREC significantly outperforms a state-of-The-Art similar approach on our data set, with the latter one achieving 70% and 50% F-scores. We also compared the effectiveness of different factors and different learning algorithms.<br/> &copy; 2018 IEEE.},
key={Cloning},
keywords={Computer software maintenance;Computer software selection and evaluation;Learning algorithms;Learning systems;},
note={Evolution behavior;Extracting features;Learning-based approach;Refactorings;Software project;Software Quality;Software repositories;State of the art;},
URL={http://dx.doi.org/10.1109/ICSME.2018.00021},
}


@inproceedings{20163702795617,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Exact sequence reconstruction for insertion-correcting codes},
journal={IEEE International Symposium on Information Theory - Proceedings},
author={Sala, Frederic and Gabrys, Ryan and Schoeny, Clayton and Mazooji, Kayvon and Dolecek, Lara},
volume={2016-August},
year={2016},
pages={615 - 619},
issn={21578095},
address={Barcelona, Spain},
abstract={We study the problem of perfectly reconstructing sequences from traces. The sequences are codewords from a deletion/insertion-correcting code and the traces are the result of corruption by a fixed number of symbol insertions (larger than the minimum edit distance of the code.) This is the general version of a problem tackled by Levenshtein for uncoded sequences. We introduce an exact formula for the maximum number of common supersequences shared by sequences at a certain edit distance, yielding a tight upper bound on the number of distinct traces necessary to guarantee exact reconstruction. We apply our results to the famous single deletion/insertion-correcting Varshamov-Tenengolts (VT) codes and show that a significant number of VT codeword pairs achieve the worst-case number of outputs needed for exact reconstruction.<br/> &copy; 2016 IEEE.},
key={Codes (symbols)},
keywords={Image reconstruction;Information theory;},
note={Combinatorics;Correcting codes;Exact reconstruction;General version;Insertions and deletions;Minimum edit distance;Sequence reconstruction;Symbol insertion;},
URL={http://dx.doi.org/10.1109/ISIT.2016.7541372},
}


@article{20180304646709,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={When and Why Your Code Starts to Smell Bad (and Whether the Smells Go Away)},
journal={IEEE Transactions on Software Engineering},
author={Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and Oliveto, Rocco and Penta, Massimiliano Di and De Lucia, Andrea and Poshyvanyk, Denys},
volume={43},
number={11},
year={2017},
pages={1063 - 1088},
issn={00985589},
abstract={Technical debt is a metaphor introduced by Cunningham to indicate 'not quite right code which we postpone making it right'. One noticeable symptom of technical debt is represented by code smells, defined as symptoms of poor design and implementation choices. Previous studies showed the negative impact of code smells on the comprehensibility and maintainability of code. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced, what is their survivability, and how they are removed by developers. To empirically corroborate such anecdotal evidence, we conducted a large empirical study over the change history of 200 open source projects. This study required the development of a strategy to identify smell-introducing commits, the mining of over half a million of commits, and the manual analysis and classification of over 10K of them. Our findings mostly contradict common wisdom, showing that most of the smell instances are introduced when an artifact is created and not as a result of its evolution. At the same time, 80 percent of smells survive in the system. Also, among the 20 percent of removed instances, only 9 percent are removed as a direct consequence of refactoring operations.<br/> &copy; 1976-2012 IEEE.},
key={Codes (symbols)},
keywords={Odors;Open source software;},
note={Anecdotal evidences;Code smell;Design and implementations;Empirical studies;Manual analysis;Mining software repositories;Open source projects;Technical debts;},
URL={http://dx.doi.org/10.1109/TSE.2017.2653105},
}


@inproceedings{20172903947649,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automated Systolic Array Architecture Synthesis for High Throughput CNN Inference on FPGAs},
journal={Proceedings - Design Automation Conference},
author={Wei, Xuechao and Yu, Cody Hao and Zhang, Peng and Chen, Youxiang and Wang, Yuxin and Hu, Han and Liang, Yun and Cong, Jason},
volume={Part 128280},
year={2017},
pages={ACM Special Interest Group on Design Automation (SIGDA); ACM Special Interest Group on Embedded Systems (SIGBED); Electronic Design Automation Consortium (EDAC); IEEE-CEDA - },
issn={0738100X},
address={Austin, TX, United states},
abstract={Convolutional neural networks (CNNs) have been widely applied in many deep learning applications. In recent years, the FPGA implementation for CNNs has attracted much attention because of its high performance and energy efficiency. However, existing implementations have difficulty to fully leverage the computation power of the latest FPGAs. In this paper we implement CNN on an FPGA using a systolic array architecture, which can achieve high clock frequency under high resource utilization. We provide an analytical model for performance and resource utilization and develop an automatic design space exploration framework, as well as source-to-source code transformation from a C program to a CNN implementation using systolic array. The experimental results show that our framework is able to generate the accelerator for real-life CNN models, achieving up to 461 GFlops for floating point data type and 1.2 Tops for 8-16 bit fixed point.<br/> &copy; 2017 ACM.},
key={Systolic arrays},
keywords={Automation;C (programming language);Computer aided design;Cosine transforms;Deep learning;Digital arithmetic;Energy efficiency;Field programmable gate arrays (FPGA);Network architecture;Neural networks;Systems analysis;},
note={Automatic design space explorations;Computation power;Convolutional neural network;Floating-point data;FPGA implementations;Resource utilizations;Source code transformation;Systolic array architecture;},
URL={http://dx.doi.org/10.1145/3061639.3062207},
}


@inproceedings{20123315332439,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Recommending relevant code artifacts for change requests using multiple predictors},
journal={2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings},
author={Denninger, Oliver},
year={2012},
pages={78 - 79},
address={Zurich, Switzerland},
abstract={Finding code artifacts affected by a given change request is a time-consuming process in large software systems. Various approaches have been proposed to automate this activity, e.g., based on information retrieval. The performance of a particular prediction approach often highly depends on attributes like coding style or writing style of change request. Thus, we propose to use multiple prediction approaches in combination with machine learning. First experiments show that machine learning is well suitable to weight different prediction approaches for individual software projects and hence improve prediction performance. &copy; 2012 IEEE.<br/>},
key={Recommender systems},
keywords={Artificial intelligence;Codes (symbols);Computer software maintenance;Forecasting;Learning systems;Software engineering;},
note={Large software systems;Prediction performance;Software project;Writing style;},
URL={http://dx.doi.org/10.1109/RSSE.2012.6233416},
}


@article{20170803382798,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Towards an understanding of change types in bug fixing code},
journal={Information and Software Technology},
author={Zhao, Yangyang and Leung, Hareton and Yang, Yibiao and Zhou, Yuming and Xu, Baowen},
volume={86},
year={2017},
issn={09505849},
abstract={Context: As developing high quality software becomes increasingly challenging because of the explosive growth of scale and complexity, bugs become inevitable in software systems. The knowledge of bugs will naturally guide software development and hence improve software quality. As changes in bug fixing code provide essential insights into the original bugs, analyzing change types is an intuitive and effective way to understand the characteristics of bugs. Objective: In this work, we conduct a thorough empirical study to investigate the characteristics of change types in bug fixing code. Method: We first propose a new change classification scheme with 5 change types and 9 change subtypes. We then develop an automatic classification tool CTforC to categorize changes. To gain deeper insights into change types, we perform our empirical study based on three questions from three perspectives, i.e. across project, across domain and across version. Results: Based on 17 versions of 11 systems with thousands of faulty functions, we find that: (1) across project: the frequencies of change subtypes are significantly similar across most studied projects; interface related code changes are the most frequent bug-fixing changes (74.6% on average); most of faulty functions (65.2% on average) in studied projects are finally fixed by only one or two change subtypes; function call statements are likely to be changed together with assignment statements or branch statements; (2) across domain: the frequencies of change subtypes share similar trends across studied domains; changes on function call, assignment, and branch statements are often the three most frequent changes in studied domains; and (3) across version: change subtypes occur with similar frequencies across studied versions, and the most common subtype pairs tend to be same. Conclusion: Our experimental results improve the understanding of changes in bug fixing code and hence the understanding of the characteristics of bugs.<br/> &copy; 2017 Elsevier B.V.},
key={Program debugging},
keywords={Codes (symbols);Computer software selection and evaluation;Software design;},
note={Bug-fixing;Change;Empirical studies;Software Quality;Understanding;},
URL={http://dx.doi.org/10.1016/j.infsof.2017.02.003},
}


@article{20171203476273,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Towards an understanding of change types in bug fixing code},
journal={Information and Software Technology},
author={Zhao, Yangyang and Leung, Hareton and Yang, Yibiao and Zhou, Yuming and Xu, Baowen},
volume={86},
year={2017},
pages={37 - 53},
issn={09505849},
abstract={Context As developing high quality software becomes increasingly challenging because of the explosive growth of scale and complexity, bugs become inevitable in software systems. The knowledge of bugs will naturally guide software development and hence improve software quality. As changes in bug fixing code provide essential insights into the original bugs, analyzing change types is an intuitive and effective way to understand the characteristics of bugs. Objective In this work, we conduct a thorough empirical study to investigate the characteristics of change types in bug fixing code. Method We first propose a new change classification scheme with 5 change types and 9 change subtypes. We then develop an automatic classification tool CTforC to categorize changes. To gain deeper insights into change types, we perform our empirical study based on three questions from three perspectives, i.e. across project, across domain and across version. Results Based on 17 versions of 11 systems with thousands of faulty functions, we find that: (1) across project: the frequencies of change subtypes are significantly similar across most studied projects; interface related code changes are the most frequent bug-fixing changes (74.6% on average); most of faulty functions (65.2% on average) in studied projects are finally fixed by only one or two change subtypes; function call statements are likely to be changed together with assignment statements or branch statements; (2) across domain: the frequencies of change subtypes share similar trends across studied domains; changes on function call, assignment, and branch statements are often the three most frequent changes in studied domains; and (3) across version: change subtypes occur with similar frequencies across studied versions, and the most common subtype pairs tend to be same. Conclusion Our experimental results improve the understanding of changes in bug fixing code and hence the understanding of the characteristics of bugs.<br/> &copy; 2017 Elsevier B.V.},
key={Program debugging},
keywords={Codes (symbols);Computer software selection and evaluation;Software design;},
note={Bug-fixing;Change;Empirical studies;Software Quality;Understanding;},
URL={http://dx.doi.org/10.1016/j.infsof.2017.02.003},
}


@inproceedings{20121915000167,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Impact analysis using Static Execute after in WebKit},
journal={Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
author={Jasz, Judit and Schrettner, Lajos and Beszedes, Arpad and Osztrogonac, Csaba and Gyimothy, Tibor},
year={2012},
pages={95 - 104},
issn={15345351},
address={Szeged, Hungary},
abstract={Insufficient propagation of changes causes the majority of regression errors in heavily evolving software systems. Impact analysis of a particular change can help identify those parts of the system that also need to be investigated and potentially propagate the change. A static code analysis technique called Static Execute After can be used to automatically infer such impact sets. The method is safe and comparable in precision to more detailed analyses. At the same time it is significantly more efficient, hence we could apply it to different large industrial systems, including the open source WebKit project. We overview the benefits of the method, its existing implementations, and present our experiences in adapting the method to such a complex project. Finally, using this particular analysis on the WebKit project, we verify whether applying the method we can actually predict the required change propagation and hence reduce regression errors. We report on the properties of the resulting impact sets computed for the change history, and their relationship to the actual fixes required. We looked at actual defects provided by the regression test suite along with their fixes taken from the version control repository, and compared these fixes to the predicted impact sets computed at the changes that caused the failing tests. The results show that the method is applicable for the analysis of the system, and that the impact sets can predict the required changes in a fair amount of cases, but that there are still open issues for the improvement of the method. &copy; 2012 IEEE.<br/>},
key={Open systems},
keywords={Computer software maintenance;Open source software;Program debugging;Reengineering;Regression analysis;Software testing;},
note={Change impact analysis;Change propagation;Industrial systems;Propagation of changes;Regression testing;Source code analysis;Static code analysis;Static Execute After;},
URL={http://dx.doi.org/10.1109/CSMR.2012.20},
}


@inproceedings{20183805823063,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An extensive analysis of efficient bug prediction configurations},
journal={ACM International Conference Proceeding Series},
author={Osman, Haidar and Ghafari, Mohammad and Nierstrasz, Oscar and Lungu, Mircea},
year={2017},
pages={107 - 115},
address={Toronto, ON, Canada},
abstract={Background: Bug prediction helps developers steer maintenance activities towards the buggy parts of a software. There are many design aspects to a bug predictor, each of which has several options, i.e., software metrics, machine learning model, and response variable. Aims: These design decisions should be judiciously made because an improper choice in any of them might lead to wrong, misleading, or even useless results. We argue that bug prediction con?gurations are intertwined and thus need to be evaluated in their entirety, in contrast to the common practice in the ?eld where each aspect is investigated in isolation. Method: We use a cost-aware evaluation scheme to evaluate 60 di?erent bug prediction con?guration combinations on ?ve open source Java projects. Results:We ?nd out that the best choices for building a cost-e?ective bug predictor are change metrics mixed with source code metrics as independent variables, Random Forest as the machine learning model, and the number of bugs as the response variable. Combining these con?guration options results in the most e?cient bug predictor across all subject systems. Conclusions: We demonstrate a strong evidence for the interplay among bug prediction con?gurations and provide concrete guidelines for researchers and practitioners on how to build and evaluate e?cient bug predictors.<br/> &copy; 2017 Association for Computing Machinery. All rights reserved.},
key={Open source software},
keywords={Artificial intelligence;Decision trees;Forecasting;Learning systems;Predictive analytics;},
note={Bug predictions;Effort-aware evaluation;Evaluation scheme;Independent variables;Machine learning models;Maintenance activity;Software metrics;Source code metrics;},
URL={http://dx.doi.org/10.1145/3127005.3127017},
}


@inproceedings{20184406004239,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Visualizing a Tangled Change for Supporting Its Decomposition and Commit Construction},
journal={Proceedings - International Computer Software and Applications Conference},
author={Sothornprapakorn, Sarocha and Hayashi, Shinpei and Saeki, Motoshi},
volume={1},
year={2018},
pages={74 - 79},
issn={07303157},
address={Tokyo, Japan},
abstract={Developers often save multiple kinds of source code edits into a commit in a version control system, producing a tangled change, which is difficult to understand and revert. However, its separation using an existing sequence-based change representation is tough. We propose a new visualization technique to show the details of a tangled change and align its component edits in a tree structure for expressing multiple groups of changes. Our technique is combined with utilizing refactoring detection and change relevance calculation techniques for constructing the structural tree. Our combination allows us to divide the change into several associations. We have implemented a tool and conducted a controlled experiment with industrial developers to confirm its usefulness and efficiency. Results show that by using our tool with tree visualization, the subjects could understand and decompose tangled changes easier, faster, and higher accuracy than the baseline file list visualization.<br/> &copy; 2018 IEEE.},
key={Application programs},
keywords={Flow visualization;Forestry;Trees (mathematics);Visualization;},
note={Calculation techniques;Controlled experiment;Refactorings;Tangled change;Tree structures;Tree visualization;Version control system;Visualization technique;},
URL={http://dx.doi.org/10.1109/COMPSAC.2018.00018},
}


@inproceedings{20180204634050,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning binary code features for UAV target tracking},
journal={2017 3rd IEEE International Conference on Control Science and Systems Engineering, ICCSSE 2017},
author={Xiao, Qiao and Zhang, Qinyu and Wu, Xi and Han, Xiao and Li, Ronghua},
year={2017},
pages={65 - 68},
address={Beijing, China},
abstract={During target tracking, in order to obtain a higher tracking accuracy, the region we would like to track should have a good feature expression. Furthermore, we need to extract multilevel and complex features to deal with problems which are usually encountered during UAV tracking, such as the target deformation, scale change and occlusion. However, such features make tracker more complex which would seriously affect the real-time tracking. Considering the above problems, we take the advantage of random forest for features selection, and then transform the features to binary code, which can not only reduce redundancy but speed up the tracker. In order to further improve the accuracy of UAV tracking, we utilize structured SVM for online learning to distinguish object from background. In addition, we apply the scale pyramid to achieve the scale invariance of tracker, which help to obtain a more precise position of the object. We have verified the effectiveness and robustness of our method on the classical UAV object tracking dataset UAV123.<br/> &copy; 2017 IEEE.},
key={Aircraft detection},
keywords={Binary codes;Clutter (information theory);Decision trees;Systems engineering;Unmanned aerial vehicles (UAV);},
note={Feature expression;Features selection;Precise position;Random forests;Real time tracking;Scale invariance;Structured SVM;Tracking accuracy;},
URL={http://dx.doi.org/10.1109/CCSSE.2017.8087896},
}


@inproceedings{20140617284426,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mining system specific rules from change patterns},
journal={Proceedings - Working Conference on Reverse Engineering, WCRE},
author={Hora, Andre and Anquetil, Nicolas and Ducasse, Stephane and Valente, Marco Tulio},
year={2013},
pages={331 - 340},
issn={10951350},
address={Koblenz, Germany},
abstract={A significant percentage of warnings reported by tools to detect coding standard violations are false positives. Thus, there are some works dedicated to provide better rules by mining them from source code history, analyzing bug-fixes or changes between system releases. However, software evolves over time, and during development not only bugs are fixed, but also features are added, and code is refactored. In such cases, changes must be consistently applied in source code to avoid maintenance problems. In this paper, we propose to extract system specific rules by mining systematic changes over source code history, i.e., not just from bug-fixes or system releases, to ensure that changes are consistently applied over source code. We focus on structural changes done to support API modification or evolution with the goal of providing better rules to developers. Also, rules are mined from predefined rule patterns that ensure their quality. In order to assess the precision of such specific rules to detect real violations, we compare them with generic rules provided by tools to detect coding standard violations on four real world systems covering two programming languages. The results show that specific rules are more precise in identifying real violations in source code than generic ones, and thus can complement them. &copy; 2013 IEEE.<br/>},
key={Program debugging},
keywords={Codes (symbols);Computer programming languages;Computer systems programming;Reverse engineering;},
note={Change patterns;Coding standards;False positive;Maintenance Problem;Mining systems;Real-world system;System specific;Systematic changes;},
URL={http://dx.doi.org/10.1109/WCRE.2013.6671308},
}


@inproceedings{20181504988053,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Micro-versioning tool to support experimentation in exploratory programming},
journal={Conference on Human Factors in Computing Systems - Proceedings},
author={Mikami, Hiroaki and Sakamoto, Daisuke and Igarashi, Takeo},
volume={2017-May},
year={2017},
pages={6208 - 6219},
address={Denver, CO, United states},
abstract={Experimentation plays an essential role in exploratory programming, and programmers apply version control operations when switching the part of the source code back to the past state during experimentation. However, these operations, which we refer to as micro-versioning, are not well supported in current programming environments. We first examined previous studies to clarify the requirements for a micro-versioning tool. We then developed a micro-versioning tool that displays visual cues representing possible micro-versioning operations in a textual code editor. Our tool includes a history model that generates meaningful candidates by combining a regional undo model and tree-structured undo model. The history model uses code executions as a delimiter to segment text edit operations into meaning groups. A user study involving programmers indicated that our tool satisfies the above-mentioned requirements and that it is useful for exploratory programming. Copyright is held by the owner/author(s). Publication rights licensed to ACM.<br/> &copy; 2017 ACM.},
key={Computer programming},
keywords={Codes (symbols);Human engineering;Information management;},
note={Code execution;Current programming;Develpment environment;Source codes;Tree-structured;Version control;Version control system;Versioning;},
URL={http://dx.doi.org/10.1145/3025453.3025597},
}


@inproceedings{20130615994705,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An MDL-based change-detection algorithm with its applications to learning piecewise stationary memoryless sources},
journal={2012 IEEE Information Theory Workshop, ITW 2012},
author={Kanazawa, Hiroki and Yamanishi, Kenji},
year={2012},
pages={557 - 561},
address={Lausanne, Switzerland},
abstract={Kleinberg has proposed an algorithm for detecting bursts from a data sequence, which has turned out to be effective in the scenario of data mining, such as topic detection, change-detection. In this paper we extend Kleinberg's algorithm in an information-theoretic fashion to obtain a new class of algorithms and apply it into learning of piecewise stationary memoryless sources (PSMSs). The keys of the proposed algorithm are; 1) the parameter space is discretized so that discretization scale depends on the Fisher information, and 2) the optimal path over the discretized parameter space is efficiently computed using the dynamic programming method so that the sum of the data and parameter description lengths is minimized on the basis of the MDL principle. We prove that an upper bound on the total code-length for the proposed algorithm asymptotically matches Merhav's lower bound. &copy; 2012 IEEE.<br/>},
key={Learning algorithms},
keywords={Data mining;Dynamic programming;Fisher information matrix;Information theory;Signal detection;},
note={Change detection;Change detection algorithms;Dynamic programming methods;Fisher information;ITS applications;Memoryless source;Parameter spaces;Piecewise stationaries;},
URL={http://dx.doi.org/10.1109/ITW.2012.6404736},
}


@inproceedings{20163102671260,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Performance search engine driven by prior knowledge of optimization},
journal={ARRAY 2015 - Proceedings of the 2nd ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming, co-located with PLDI 2015},
author={Kim, Youngsung and erny, Pavol and Dennis, John},
year={2015},
pages={25 - 30},
address={Portland, OR, United states},
abstract={For scientific array-based programs, optimization for a particular target platform is a hard problem. There are many optimization techniques such as (semantics-preserving) source code transformations, compiler directives, environment variables, and compiler flags that influence performance. Moreover, the performance impact of (combinations of) these factors is unpredictable. This paper focuses on providing a platform for automatically searching through search space consisting of such optimization techniques. We provide (i) a search-space description language, which enables the user to describe optimization options to be used; (ii) search engine that enables testing the performance impact of optimization options by executing optimized programs and checking their results; and (iii) an interface for implementing various search algorithms. We evaluate our platform by using two simple search algorithms - a random search and a casetree search that heuristically learns from the already examined parts of the search space. We show that such algorithms are easily implementable in our platform, and we empirically find that the framework can be used to find useful optimized algorithms.<br/> &copy; 2015 ACM.},
key={Program compilers},
keywords={Cosine transforms;Learning algorithms;Libraries;Natural sciences computing;Optimization;Search engines;Semantics;},
note={Automated optimization;Code Generation;Compiler directives;Optimization techniques;Optimized algorithms;Performance;Simple search algorithm;Source code transformation;},
URL={http://dx.doi.org/10.1145/2774959.2774963},
}


@inproceedings{20123415352800,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automatic software architecture recovery: A machine learning approach},
journal={IEEE International Conference on Program Comprehension},
author={Sajnani, Hitesh},
year={2012},
pages={265 - 268},
address={Passau, Germany},
abstract={Automatically recovering functional architecture of the software can facilitate the developer's understanding of how the system works. In legacy systems, original source code is often the only available source of information about the system and it is very time consuming to understand source code. Current architecture recovery techniques either require heavy human intervention or fail to recover quality components. To alleviate these shortcomings, we propose use of machine learning techniques which use structural, runtime behavioral, domain, textual and contextual (e.g. code authorship, line co-change) features. These techniques will allow us to experiment with a large number of features of the software artifacts without having to establish a priori our own insights about what is important and what is not important. We believe this is a promising approach that may finally start to produce usable solutions to this elusive problem. &copy; 2012 IEEE.<br/>},
key={Learning systems},
keywords={Artificial intelligence;Codes (symbols);Computer programming;Legacy systems;Recovery;},
note={Architecture recovery;Functional architecture;Human intervention;Machine learning approaches;Machine learning techniques;Quality components;Software architecture recovery;Software artifacts;},
}


@inproceedings{20141817656955,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Detecting asynchrony and dephase change patterns by mining software repositories},
journal={Journal of software: Evolution and Process},
author={Jaafar, Fehmi and Gueheneuc, Yann-Gael and Hamel, Sylvie and Antoniol, Giuliano},
volume={26},
number={1},
year={2014},
pages={77 - 106},
issn={20477481},
abstract={Software maintenance accounts for the largest part of the costs of any program. During maintenance activities, developers implement changes (sometimes simultaneously) on artifacts in order to fix bugs and to implement new requirements. To reduce this part of the costs, previous work proposed approaches to identify the artifacts of programs that change together. These approaches analyze historical data, mined from version control systems, and report change patterns, which lead at the causes, consequences, and actors of the changes to source code files. They also introduce so-called change patterns that describe some typical change dependencies among files. In this paper, we introduce two novel change patterns: the asynchrony change pattern, corresponding to macro co-changes (MC), that is, of files that co-change within a large time interval (change periods) and the dephase change pattern, corresponding to dephase macro co-changes (DC), that is, MC that always happens with the same shifts in time. We present our approach, that we named Macocha, to identify these two change patterns in large programs. We use the k-nearest neighbor algorithm to group changes into change periods. We also use the Hamming distance to detect approximate occurrences of MC and DC. We apply Macocha and compare its performance in terms of precision and recall with UMLDiff (file stability) and association rules (co-changing files) on seven systems: ArgoUML, FreeBSD, JFreeChart, Openser, SIP, XalanC, and XercesC developed with three different languages (C, C++, and Java). These systems have a size ranging from 532 to 1693 files, and during the study period, they have undergone 1555 to 23,944 change commits. We use external information and static analysis to validate (approximate) MC and DC found by Macocha. Through our case study, we show the existence and usefulness of these novel change patterns to ease software maintenance and, potentially, reduce related costs. Copyright &copy; 2013 John Wiley &amp; Sons, Ltd.<br/>},
key={C++ (programming language)},
keywords={Computer software maintenance;Convergence of numerical methods;Cost reduction;Hamming distance;Macros;Nearest neighbor search;Pattern recognition;Static analysis;},
note={Bit vector;Change patterns;Change period;External informations;K nearest neighbor algorithm;Maintenance activity;Mining software repositories;Version control system;},
URL={http://dx.doi.org/10.1002/smr.1635},
}


@article{20184406021865,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Deep Semantic Feature Learning for Software Defect Prediction},
journal={IEEE Transactions on Software Engineering},
author={Wang, Song and Liu, Taiyue and Nam, Jaechang and Tan, Lin},
year={2018},
issn={00985589},
abstract={Software defect prediction, which predicts defective code regions, can assist developers in finding bugs and prioritizing their testing efforts. Traditional defect prediction features often fail to capture the semantic differences between different programs. This degrades the performance of the prediction models built on these traditional features. Thus, the capability to capture the semantics in programs is required to build accurate prediction models. To bridge the gap between semantics and defect prediction features, we propose leveraging a powerful representation-learning algorithm, deep learning, to learn the semantic representations of programs automatically from source code files and code changes. Specifically, we leverage a deep belief network (DBN) to automatically learn semantic features using token vectors extracted from the programs&amp;#x0027; abstract syntax trees (AST) (for file-level defect prediction models) and source code changes (for change-level defect prediction models). We examine the effectiveness of our approach on two file-level defect prediction tasks (i.e., file-level within-project defect prediction and file-level cross-project defect prediction) and two change-level defect prediction tasks (i.e., change-level within-project defect prediction and change-level cross-project defect prediction). Our experimental results indicate that the DBN-based semantic features can significantly improve the examined defect prediction tasks. Specifically, the improvements of semantic features against existing traditional features (in F1) range from 2.1 to 41.9 percentage points for file-level within-project defect prediction, from 1.5 to 13.4 percentage points for file-level cross-project defect prediction, from 1.0 to 8.6 percentage points for change-level within-project defect prediction, and from 0.6 to 9.9 percentage points for change-level cross-project defect prediction.<br/> IEEE},
key={Deep learning},
keywords={Codes (symbols);Data structures;Defects;Feature extraction;Forecasting;Job analysis;Learning algorithms;Program debugging;Quality assurance;Semantics;Software testing;Trees (mathematics);},
note={Computer bugs;Defect prediction;Predictive models;Semantic features;Task analysis;},
URL={http://dx.doi.org/10.1109/TSE.2018.2877612},
}


@inproceedings{20144600189221,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Embrace your issues: Compassing the software engineering landscape using bug reports},
journal={ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
author={Borg, Markus},
year={2014},
pages={891 - 894},
address={Vasteras, Sweden},
abstract={Software developers in large projects work in complex information landscapes, and staying on top of all relevant software artifacts is challenging. As software systems often evolve for years, a high number of issue reports is typically managed during the lifetime of a system. Efficient management of incoming issue requires successful navigation of the information landscape. In our work, we address two important work tasks involved in issue management: Issue Assignment (IA) and Change Impact Analysis (CIA). IA is the early task of allocating an issue report to a development team. CIA deals with identifying how source code changes affect the software system, a fundamental activity in safetycritical development. Our solution approach is to support navigation, both among development teams and software artifacts, based on information available in historical issue reports. We present how we apply techniques from machine learning and information retrieval to develop recommendation systems. Finally, we report intermediate results from two controlled experiments and an industrial case study.<br/> &copy; 2014 ACM.},
key={Search engines},
keywords={Air navigation;Artificial intelligence;Information management;Information retrieval;Learning systems;Object oriented programming;Recommender systems;},
note={Change impact analysis;Complex information;Controlled experiment;Efficient managements;Industrial case study;Intermediate results;Issue managements;Source code changes;},
URL={http://dx.doi.org/10.1145/2642937.2653469},
}


@inproceedings{20183205664681,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Privacy preservation in interaction history on integrated development environments},
journal={2018 IEEE 1st International Workshop on Mining and Analyzing Interaction Histories, MAINT 2018 - Proceedings},
author={Omori, Takayuki},
volume={2018-January},
year={2018},
pages={7 - 11},
address={Campobasso, Italy},
abstract={The interaction history in a software development environment allows us to analyze how developers change source code and how they use tools on the integrated development environment. Sharing the interaction history with tool providers increases the chances that developers obtain better tools. However, the interaction history sometimes contains privacy-sensitive information, which is an obstacle in collecting and using the interaction history. As an attempt to tackle this issue, this paper proposes a technique to replace sensitive text in a recorded interaction history. This paper describes the proposed technique, its current implementation, the results of a preliminary survey on how potential privacy-sensitive information exists in recorded interaction histories, and how privacy issues in sharing interaction histories can be ameliorated.<br/> &copy; 2018 IEEE.},
key={Software design},
note={Integrated development environment;Interaction history;Privacy issue;Privacy preservation;Sensitive informations;Software development environment;Source codes;Tool providers;},
URL={http://dx.doi.org/10.1109/MAINT.2018.8323088},
}


@inproceedings{20132216386688,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Optimizing generated aspect-oriented assertion checking code for JML using program transformations: An empirical study},
journal={Science of Computer Programming},
author={Rebelo, Henrique and Lima, Ricardo and Leavens, Gary T. and Cornelio, Marcio and Mota, Alexandre and Oliveira, Cesar},
volume={78},
number={8},
year={2013},
pages={1137 - 1156},
issn={01676423},
abstract={The AspectJ JML compiler (ajmlc) explores aspect-oriented programming (AOP) mechanisms to implement JML specifications, such as pre- and postconditions, and enforce them during runtime. This compiler was created to improve source-code modularity. Some experiments were conducted to evaluate the performance of the code generated through ajmlc. Results demonstrated that the strategy of adopting AOP to implement JML specifications is very promising. However, there is still a need for optimization of the generated code's bytecode size and running time. This paper presents a catalog of transformations which represent the optimizations implemented in the new optimized version of the ajmlc compiler. We employ such transformations to reduce the bytecode size and running time of the code generated through the ajmlc compiler. Aiming at demonstrating the impact of such transformation on the code quality, we conduct an empirical study using four applications in optimized and non-optimized versions generated by ajmlc. We show that our AOP transformations provide a significant improvement, regarding bytecode size and running time. &copy; 2012 Elsevier B.V. All rights reserved.<br/>},
key={Aspect oriented programming},
keywords={Codes (symbols);Program compilers;Specifications;},
note={Aspect-oriented;Aspect-Oriented Programming (AOP);Assertion-checking;Code quality;Empirical studies;Program transformations;Running time;Source codes;},
URL={http://dx.doi.org/10.1016/j.scico.2012.09.003},
}


@inproceedings{20130615993950,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Configuration selection using code change impact analysis for regression testing},
journal={IEEE International Conference on Software Maintenance, ICSM},
author={Qu, Xiao and Acharya, Mithun and Robinson, Brian},
year={2012},
pages={129 - 138},
address={Riva del Garda, Trento, Italy},
abstract={Configurable systems that let users customize system behaviors are becoming increasingly prevalent. Testing a configurable system with all possible configurations is very expensive and often impractical. For a single version of a configurable system, sampling approaches exist that select a subset of configurations from the full configuration space for testing. However, when a configurable system changes and evolves, existing approaches for regression testing select all configurations that are used to test the old versions for testing the new version. As demonstrated in our experiments, this retest-all approach for regression testing configurable systems turns out to be highly redundant. To address this redundancy, we propose a configuration selection approach for regression testing. Formally, given two versions of a configurable system, S (old) and S' (new), and given a set of configurations C<inf>S</inf>for testing S, our approach selects a subset C<inf>S'</inf>of C<inf>S</inf>for regression testing S'. Our study results on two open source systems and a large industrial system show that, compared to the retest-all approach, our approach discards 15% to 60% of configurations as redundant. Our approach also saves 20% to 55% of the regression testing time, while retaining the same fault detection capability and code coverage of the retest-all approach. &copy; 2012 IEEE.<br/>},
key={Software testing},
keywords={Computer software maintenance;Fault detection;Open source software;Open systems;Regression analysis;},
note={Change impact analysis;Configurable systems;Configuration Selection;Regression testing;Static program;},
URL={http://dx.doi.org/10.1109/ICSM.2012.6405263},
}


@article{20153701273368,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automatic detection of system-specific conventions unknown to developers},
journal={Journal of Systems and Software},
author={Hora, Andre and Anquetil, Nicolas and Etien, Anne and Ducasse, Stephane and Valente, Marco Tulio},
volume={109},
year={2015},
pages={192 - 204},
issn={01641212},
abstract={In Apache Ant, a convention to improve maintenance was introduced in 2004 stating a new way to close files instead of the Java generic InputStream.close(). Yet, six years after its introduction, this convention was still not generally known to the developers. Two existing solutions could help in these cases. First, one can deprecate entities, but, in our example, one can hardly deprecate Java's method. Second, one can create a system-specific rule to be automatically enforced. In a preceding publication, we showed that system-specific rules are more likely to be noticed by developers than generic ones. However, in practice, developers rarely create specific rules. We therefore propose to free the developers from the need to create rules by automatically detecting such conventions from source code repositories. This is done by mining the change history of the system to discover similar changes being applied over several revisions. The proposed approach is applied to a real-world system, and the extracted rules are validated with the help of experts. The results show that many rules are in fact relevant for the experts.<br/> &copy; 2015 ElsevierInc.Allrightsreserved.},
key={Java programming language},
keywords={Hardware;Software engineering;},
note={Automatic Detection;Change history;Java generics;Mining software repositories;Real-world system;Software Evolution;Source code repositories;System specific;},
URL={http://dx.doi.org/10.1016/j.jss.2015.08.007},
}


@inproceedings{20164803060124,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Predicting Software Maintenance Effort by Mining Software Project Reports Using Inter-Version Validation},
journal={International Journal of Reliability, Quality and Safety Engineering},
author={Jindal, Rajni and Malhotra, Ruchika and Jain, Abha},
volume={23},
number={6},
year={2016},
issn={02185393},
abstract={Changes in the software are unavoidable due to an ever changing dynamic and active environment wherein expectations and requirements of the users tend to change rapidly. As a result, software needs to upgrade itself from its previous version to the next version in order to meet expectations of the user. The upgradation of the software is in terms of total number of Lines of Code (LOC) that might have been inserted, deleted or modified in moving from one version of software to the next. These changes are maintained in the change reports which constitute of the defect ID and defect description. Defect description describes the cause of defect which might have occurred in the previous version of the software due to which either new LOC needs to be inserted or existing LOC need to be deleted or modified. A lot of effort is required to correct the defects identified in software at the maintenance phase i.e., when software is delivered at the customers end. Thus, in this paper, we intend to predict maintenance effort by analyzing the defect reports using text mining techniques and thereafter developing the prediction models using suitable machine learning algorithms viz. Multi-Layer Perceptron (MLP), Radial-Basis Function (RBF) network and Decision Tree (DT). We have considered the changes between three successive versions of 'MMS' application package of Android operating system and have performed inter-version validation where the model predicted using the version 'v' is validated on the subsequent version i.e., 'v+1'. The performance of the model was evaluated using Receiver Operating Characteristics (ROC) analysis. The results indicated that the model predicted on 'MMS' 4.0 version using MLP algorithm has shown good results when validated on 'MMS' 4.1 version. On the other hand, the performance of RBF and DT algorithms has been consistently average in predicting the maintenance effort.<br/> &copy; 2016 World Scientific Publishing Company.},
key={Computer software maintenance},
keywords={Data mining;Decision trees;Defects;Forecasting;Learning algorithms;Learning systems;Radial basis function networks;},
note={Active environments;Defect reports;Maintenance efforts;Multi layer perceptron;Receiver operating characteristics;Receiver operating characteristics analysis;Text mining;Text mining techniques;},
URL={http://dx.doi.org/10.1142/S021853931640009X},
}


@inproceedings{20182105238180,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An automated tool for collection of code attributes for cross project defect prediction},
journal={Proceedings - 2017 2nd International Conference on Man and Machine Interfacing, MAMI 2017},
author={Malhotra, Ruchika and Bansal, Bhavyaa and Jain, Chitranshi and Punia, Ekta},
volume={2018-March},
year={2018},
pages={1 - 6},
address={Bhubaneswar, India},
abstract={This paper presents a tool that automates the process of data collection for defect or change analysis. Prediction of defects in early phases has become crucial to reduce the efforts and costs incurred due to defects. This tool extracts the information from Git Version Control System of open source projects. Two consecutive versions of one single project have been used by the tool to obtain results. The tool generates a matrix containing code churn (added lines, deleted lines, modified lines, total LoC), complexity, pre-release bugs and post-release bugs of each file of source code. The obtained software metrics can be used to measure the development process of a software and therefore in analysis and prediction purposes.<br/> &copy; 2017 IEEE.},
key={Program debugging},
keywords={Codes (symbols);Defects;Forecasting;Open source software;Open systems;},
note={code churn;Defect prediction;Open sources;post release bugs;prerelease bugs;},
URL={http://dx.doi.org/10.1109/MAMI.2017.8307864},
}


@article{20181104890951,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Empirical analysis of change metrics for software fault prediction},
journal={Computers and Electrical Engineering},
author={Choudhary, Garvit Rajesh and Kumar, Sandeep and Kumar, Kuldeep and Mishra, Alok and Catal, Cagatay},
volume={67},
year={2018},
pages={15 - 24},
issn={00457906},
abstract={A quality assurance activity, known as software fault prediction, can reduce development costs and improve software quality. The objective of this study is to investigate change metrics in conjunction with code metrics to improve the performance of fault prediction models. Experimental studies are performed on different versions of Eclipse projects and change metrics are extracted from the GIT repositories. In addition to the existing change metrics, several new change metrics are defined and collected from the Eclipse project repository. Machine learning algorithms are applied in conjunction with the change and source code metrics to build fault prediction models. The classification model with new change metrics performs better than the models using existing change metrics. In this work, the experimental results demonstrate that change metrics have a positive impact on the performance of fault prediction models, and high-performance models can be built with several change metrics.<br/> &copy; 2018 Elsevier Ltd},
key={Forecasting},
keywords={Computer software selection and evaluation;Learning algorithms;Learning systems;Quality assurance;},
note={Change logs;Defect prediction;Eclipse;Metrics;Software fault prediction;Software Quality;},
URL={http://dx.doi.org/10.1016/j.compeleceng.2018.02.043},
}


@inproceedings{20183505753403,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A program slicing-based Bayesian network model for change impact analysis},
journal={Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security, QRS 2018},
author={Ufuktepe, Ekincan and Tuglular, Tugkan},
year={2018},
pages={490 - 499},
address={Lisbon, Portugal},
abstract={Change impact analysis plays an important role in identifying potential affected areas that are caused by changes that are made in a software. Most of the existing change impact analysis techniques are based on architectural design and change history. However, source code-based change impact analysis studies are very few and they have shown higher precision in their results. In this study, a static method-granularity level change impact analysis, that uses program slicing and Bayesian Network technique has been proposed. The technique proposes a directed graph model that also represents the call dependencies between methods. In this study, an open source Java project with 8999 to 9445 lines of code and from 505 to 528 methods have been analyzed through 32 commits it went. Recall and f-measure metrics have been used for evaluation of the precision of the proposed method, where each software commit has been analyzed separately.<br/> &copy; 2018 IEEE.},
key={Open source software},
keywords={Bayesian networks;Computer programming;Computer software selection and evaluation;Directed graphs;Open systems;Software reliability;},
note={Bayesian network models;Change history;Change impact analysis;Directed graph models;Granularity levels;Network techniques;Program analysis;Program slicing;},
URL={http://dx.doi.org/10.1109/QRS.2018.00062},
}


@inproceedings{20160301803258,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Computational intelligence from AI to BI to NI},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
author={Werbos, Paul J.},
volume={9496},
year={2015},
pages={The Society of Photo-Optical Instrumentation Engineers (SPIE) - },
issn={0277786X},
address={Baltimore, MD, United states},
abstract={This paper gives highlights of the history of the neural network field, stressing the fundamental ideas which have been in play. Early neural network research was motivated mainly by the goals of artificial intelligence (AI) and of functional neuroscience (biological intelligence, BI), but the field almost died due to frustrations articulated in the famous book Perceptrons by Minsky and Papert. When I found a way to overcome the difficulties by 1974, the community mindset was very resistant to change; it was not until 1987/1988 that the field was reborn in a spectacular way, leading to the organized communities now in place. Even then, it took many more years to establish crossdisciplinary research in the types of mathematical neural networks needed to really understand the kind of intelligence we see in the brain, and to address the most demanding engineering applications. Only through a new (albeit short-lived) funding initiative, funding crossdisciplinary teams of systems engineers and neuroscientists, were we able to fund the critical empirical demonstrations which put our old basic principle of "deep learning" firmly on the map in computer science. Progress has rightly been inhibited at times by legitimate concerns about the "Terminator threat" and other possible abuses of technology. This year, at SPIE, in the quantum computing track, we outline the next stage ahead of us in breaking out of the box, again and again, and rising to fundamental challenges and opportunities still ahead of us.<br/> &copy; 2015 COPYRIGHT SPIE.},
key={Neural networks},
keywords={Artificial intelligence;Backpropagation;Biological systems;Compressed sensing;Deep learning;History;Neurology;Quantum computers;},
note={Basic principles;Computational neuroscience;Cross-disciplinary research;Cross-disciplinary teams;Engineering applications;Mathematical neural networks;Neural code;Quantum Computing;},
URL={http://dx.doi.org/10.1117/12.2191520},
}


@article{20142117745945,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automating the maintenance of nonfunctional system properties using demonstration-based model transformation},
journal={Journal of software: Evolution and Process},
author={Sun, Yu and Gray, Jeff and Delamare, Romain and Baudry, Benoit and White, Jules},
volume={25},
number={12},
year={2013},
pages={1335 - 1356},
issn={20477481},
abstract={Domain-Specific Modeling Languages (DSMLs) are playing an increasingly significant role in software development. By raising the level of abstraction using notations that are representative of a specific domain, DSMLs allow the core essence of a problem to be separated from irrelevant accidental complexities, which are typically found at the implementation level in source code. In addition to modeling the functional aspects of a system, a number of nonfunctional properties (e.g., quality of service constraints and timing requirements) also need to be integrated into models in order to reach a complete specification of a system. This is particularly true for domains that have distributed real time and embedded needs. Given a base model with functional components, maintaining the nonfunctional properties that crosscut the base model has become an essential modeling task when using DSMLs. The task of maintaining nonfunctional properties in DSMLs is traditionally supported by manual model editing or by using model transformation languages. However, these approaches are challenging to use for those unfamiliar with the specific details of a modeling transformation language and the underlying metamodel of the domain, which presents a7 steep learning curve for many users. This paper presents a demonstration-based approach to automate the maintenance of nonfunctional properties in DSMLs. Instead of writing model transformation rules explicitly, users demonstrate how to apply the nonfunctional properties by directly editing the concrete model instances and simulating a single case of the maintenance process. By recording a user's operations, an inference engine analyzes the user's intention and generates generic model transformation patterns automatically, which can be refined by users and then reused to automate the same evolution and maintenance task in other models. Using this approach, users are able to automate the maintenance tasks without learning a complex model transformation language. In addition, because the demonstration is performed on model instances, users are isolated from the underlying abstract metamodel definitions. Our demonstration-based approach has been applied to several scenarios, such as auto scaling and model layout. The specific contribution in this paper is the application of the demonstration-based approach to capture crosscutting concerns representative of aspects at the modeling level. Several examples are presented across multiple modeling languages to demonstrate the benefits of our approach. &copy; 2013 John Wiley &amp; Sons, Ltd.<br/>},
key={Modeling languages},
keywords={Demonstrations;Embedded systems;High level languages;Maintenance;Quality of service;Software design;Specification languages;},
note={Cross-cutting concerns;Distributed real-time and embedded;Domain specific modeling languages;Functional components;Model transformation;Model transformation languages;Non functional properties;Quality of Service constraints;},
URL={http://dx.doi.org/10.1002/smr.1606},
}


@article{20162102407828,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automatic CPU/GPU Generation of Multi-versioned OpenCL Kernels for C++ Scientific Applications},
journal={International Journal of Parallel Programming},
author={Sotomayor, Rafael and Sanchez, Luis Miguel and Garcia Blas, Javier and Fernandez, Javier and Garcia, J. Daniel},
volume={45},
number={2},
year={2017},
pages={262 - 282},
issn={08857458},
abstract={Parallelism has become one of the most extended paradigms used to improve performance. However, it forces software developers to adapt applications and coding mechanisms to exploit the available computing devices. Legacy source code needs to be re-written to take advantage of multi- core and many-core computing devices. Writing parallel applications in a traditional way is hard, expensive, and time consuming. Furthermore, there is often more than one possible transformation or optimization that can be applied to a single piece of legacy code. Therefore many parallel versions of the same original sequential code need to be considered. In this paper, we describe an automatic parallel source code generation workflow (REWORK) for parallel heterogeneous platforms. REWORK automatically identifies promising kernels on legacy C++ source code and generates multiple specific versions of kernels for improving C++ applications, selecting the most adequate version based on both static source code and target platform characteristics.<br/> &copy; 2016, Springer Science+Business Media New York.},
key={C++ (programming language)},
keywords={Application programs;Automatic programming;Cesium;Codes (symbols);},
note={Code Generation;Heterogeneous platforms;Multi core and many cores;Multi-versioning;OpenCL;Parallel application;Scientific applications;Source code generation;},
URL={http://dx.doi.org/10.1007/s10766-016-0425-6},
}


@inproceedings{20152500951184,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Recommending clones for refactoring using design, context, and history},
journal={Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014},
author={Wang, Wei and Godfrey, Michael W.},
year={2014},
pages={331 - 340},
address={Victoria, BC, Canada},
abstract={Developers know that copy-pasting code (aka code cloning) is often a convenient shortcut to achieving a design goal, albeit one that carries risks to the code quality over time. However, deciding which, if any, clones should be eliminated within an existing system is a daunting task. Fixing a clone usually means performing an invasive refactoring, and not all clones may be worth the effort, cost, and risk that such a change entails. Furthermore, sometimes cloning fulfils a useful design role, and should not be refactored at al. And clone detection tools often return very large result sets, making it hard to choose which clones should be investigated and possibly removed. In this paper, we propose an automated approach to recommend clones for refactoring by training a decision tree-based classifier. We analyze more than 600 clone instances in three medium-to large-sized open source projects, and we collect features that are associated with the source code, the context, and the history of clone instances. Our approach achieves a precision of around 80% in recommending clone refactoring instances for each target system, and similarly good precision is achieved in cross-project evaluation. By recommending which clones are appropriate for refactoring, our approach allows for better resource allocation for refactoring itself after obtaining clone detection results, and can thus lead to improved clone management in practice.<br/> &copy; 2014 IEEE.},
key={Cloning},
keywords={Chemical detection;Codes (symbols);Decision trees;Open source software;Open systems;Project management;},
note={Automated approach;Clone detection;Clone management;Code re-factoring;Existing systems;Open source projects;Project evaluation;Software clones;},
URL={http://dx.doi.org/10.1109/ICSME.2014.55},
}


@inproceedings{20165203187594,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Exploiting tree structures for classifying programs by functionalities},
journal={Proceedings - 2016 8th International Conference on Knowledge and Systems Engineering, KSE 2016},
author={Phan, Viet Anh and Chau, Ngoc Phuong and Nguyen, Minh Le},
year={2016},
pages={85 - 90},
address={Hanoi, Viet nam},
abstract={Analyzing source code to solve software engineering problems such as fault prediction, cost, and effort estimation always receives attention of researchers as well as companies. The traditional approaches are based on machine learning, and software metrics obtained by computing standard measures of software projects. However, these methods have faced many challenges due to limitations of using software metrics which were not enough to capture the complexity of programs. The aim of this paper is to apply several natural language processing techniques, which deal with software engineering problems by exploring information of programs' abstract syntax trees (ASTs) instead of software metrics. To speed up computational time, we propose a pruning tree technique to eliminate redundant branches of ASTs. In addition, the k-Nearest Neighbor (kNN) algorithm was adopted to compare with other methods whereby the distance between programs is measured by using the tree edit distance (TED) and the Levenshtein distance. These algorithms are evaluated based on the performance of solving 104-label program classification problem. The experiments show that due to the use of appropriate data structures although kNN is a simple machine learning algorithm, the classifiers achieve the promising results.<br/> &copy; 2016 IEEE.},
key={Learning algorithms},
keywords={Cost engineering;Learning systems;Machinery;Natural language processing systems;Nearest neighbor search;Object oriented programming;Problem solving;Software engineering;Systems engineering;Trees (mathematics);XML;},
note={Abstract Syntax Trees;Computational time;Effort Estimation;K nearest neighbor algorithm;Levenshtein distance;Program classifications;Traditional approaches;Tree edit distance;},
URL={http://dx.doi.org/10.1109/KSE.2016.7758034},
}


@inproceedings{20123915458962,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A modified version of Rijndael algorithm implemented to analyze the cyphertexts correlation for switched S-Boxes},
journal={2012 9th International Conference on Communications, COMM 2012 - Conference Proceedings},
author={Cretu, Marian and Apostol, Cristian-Gabriel},
year={2012},
pages={331 - 334},
address={Bucharest, Romania},
abstract={There are more than eleven years since Rijndael algorithm was declared the winner of the NIST contest for the new AES election. All this time the original algorithm was analyzed and attacked by cryptanalysts and hackers in order to find its vulnerabilities. The modified version of Rijndael we analyze in this paper randomly changes the accessing order of S-Boxes implemented in the source code of the original algorithm, due to affine transformation and inverse matrix properties. The goal is to obtain two different cyphertexts, keeping the plaintext and the secret key. For this to be possible, a PRNG designed by Gorge Marsaglia was implemented in the software solution. &copy; 2012 IEEE.<br/>},
key={Cryptography},
keywords={Correlation methods;Inverse problems;Linear transformations;Personal computing;Random number generation;},
note={Affine transformations;histogram;Inverse matrix;Original algorithms;PRNG;Rijndael;Rijndael algorithm;Software solution;},
URL={http://dx.doi.org/10.1109/ICComm.2012.6262548},
}


@inproceedings{20134416906822,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using co-change histories to improve bug localization performance},
journal={SNPD 2013 - 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing},
author={Tantithamthavorn, Chakkrit and Ihara, Akinori and Matsumoto, Ken-Ichi},
year={2013},
pages={543 - 548},
address={Honolulu, HI, United states},
abstract={A large open source software (OSS) project receives many bug reports on a daily basis. Bug localization techniques automatically pinpoint source code fragments that are relevant to a bug report, thus enabling faster correction. Even though many bug localization methods have been introduced, their performance is still not efficient. In this research, we improved on existing bug localization methods by taking into account co-change histories. We conducted experiments on two OSS datasets, the Eclipse SWT 3.1 project and the Android ZXing project. We validated our approach by evaluating effectiveness compared to the state-of-the-art approach Bug Locator. In the Eclipse SWT 3.1 project, our approach reliably identified source code that should be fixed for a bug in 72.46% of the total bugs, while Bug Locator identified only 51.02%. In the Android ZXing project, our approach identified 85.71%, while Bug Locator identified 60%. &copy; 2013 IEEE.<br/>},
key={Open source software},
keywords={Android (operating system);Artificial intelligence;Computer software maintenance;Information retrieval;Open systems;},
note={Bug localizations;Bug reports;Change history;Open source software projects;Source codes;State-of-the-art approach;},
URL={http://dx.doi.org/10.1109/SNPD.2013.92},
}


@inproceedings{20175104563808,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Discovering maintainability changes in large software systems*},
journal={ACM International Conference Proceeding Series},
author={Molnar, Arthur and Motogna, Simona},
volume={Part F131936},
year={2017},
pages={88 - 93},
address={Gothenburg, Sweden},
abstract={In this paper we propose an approach to automatically discover meaningful changes to maintainability of applications developed using object oriented programming languages. Our approach consists of an algorithm that employs the values of several class-level software metrics that can be easily obtained using open source software. Based on these values, a score that illustrates the maintainability change between two versions of the system is calculated. We present relevant related work, together with the state of research regarding the link between software metrics and maintainability for object oriented systems. In order to validate the approach, we undertake a case study that covers the entire development history of the jEdit open source text editor. We consider 41 version pairs that are assessed for changes to maintainability. First, a manual tool assisted examination of the source code was performed, followed by calculating the Maintainability Index for each application version. In the last step, we apply the proposed approach and compare the findings with those of the manual examination as well as those obtained using the Maintainability Index. In the final section, we present the identified issues and propose future work to further fine tune the approach.<br/> &copy; 2017 Association for Computing Machinery.},
key={Object oriented programming},
keywords={Maintainability;Open source software;Open systems;},
note={Development history;Large software systems;Manual examination;Object oriented metrics;Object-oriented system;Open source texts;Software metrics;State of research;},
URL={http://dx.doi.org/10.1145/3143434.3143447},
}


@article{20180204632296,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A detection framework for semantic code clones and obfuscated code},
journal={Expert Systems with Applications},
author={Sheneamer, Abdullah and Roy, Swarup and Kalita, Jugal},
volume={97},
year={2018},
pages={405 - 420},
issn={09574174},
abstract={Code obfuscation is a staple tool in malware creation where code fragments are altered substantially to make them appear different from the original, while keeping the semantics unaffected. A majority of the obfuscated code detection methods use program structure as a signature for detection of unknown codes. They usually ignore the most important feature, which is the semantics of the code, to match two code fragments or programs for obfuscation. Obfuscated code detection is a special case of the semantic code clone detection task. We propose a detection framework for detecting both code obfuscation and clone using machine learning. We use features extracted from Java bytecode dependency graphs (BDG), program dependency graphs (PDG) and abstract syntax trees (AST). BDGs and PDGs are two representations of the semantics or meaning of a Java program. ASTs capture the structural aspects of a program. We use several publicly available code clone and obfuscated code datasets to validate the effectiveness of our framework. We use different assessment parameters to evaluate the detection quality of our proposed model. Experimental results are excellent when compared with contemporary obfuscated code and code clone detectors. Interestingly, we achieve 100% success in detecting obfuscated code based on recall, precision, and F1-Score. When we compare our method with other methods for all of obfuscations types, viz, contraction, expansion, loop transformation and renaming, our model appears to be the winner. In case of clone detection our model achieve very high detection accuracy in comparison to other similar detectors.<br/> &copy; 2017 Elsevier Ltd},
key={Codes (symbols)},
keywords={Artificial intelligence;Cloning;Java programming language;Learning systems;Program processors;Semantics;Trees (mathematics);},
note={Abstract Syntax Trees;Code obfuscation;Dependency graphs;Detection framework;Important features;Loop transformation;Program dependency graphs;Semantic codes;},
URL={http://dx.doi.org/10.1016/j.eswa.2017.12.040},
}


@inproceedings{20173204021260,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Developing prediction models to assist software developers and support managers},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Sharma, Meera and Tondon, Abhishek},
volume={10408 LNCS},
year={2017},
pages={548 - 560},
issn={03029743},
address={Trieste, Italy},
abstract={A huge amount of historical information about the evolution of a software project is available in software repositories, namely bug repositories, source control repositories, archived communications, deployment logs, and code repositories. By mining the evolutionary history of a software, we have designed prediction models to assist software developers by predicting bug attributes like priority, severity, assignee and fix time. We have evaluated the uncertainty in the software in terms of entropy arises due to source code changes done in files of the software to fix different issues. To support software managers, we have designed prediction models to predict potential values of entropy and different issues, namely bugs, improvements in existing features (IMPs) and new features (NFs) over a long run. In this research work, we have developed mathematical models to assist software managers and developers in bug triag-ing, bug fixing and different software maintenance related tasks. Our work has been validated on issue and code change data of several open source projects, namely Eclipse, Open office, Mozilla and Apache.<br/> &copy; Springer International Publishing AG 2017.},
key={Open systems},
keywords={Codes (symbols);Entropy;Forecasting;Managers;Open source software;Program debugging;},
note={Evolutionary history;Historical information;Open source projects;Software developer;Software managers;Software project;Software repositories;Source code changes;},
URL={http://dx.doi.org/10.1007/978-3-319-62404-4_41},
}


@inproceedings{20155301739520,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Supporting selective undo in a code editor},
journal={Proceedings - International Conference on Software Engineering},
author={Yoon, Young Seok and Myers, Brad A.},
volume={1},
year={2015},
pages={223 - 233},
issn={02705257},
address={Florence, Italy},
abstract={Programmers often need to revert some code to an earlier state, or restore a block of code that was deleted a while ago. However, support for this backtracking in modern programming environments is limited. Many of the backtracking tasks can be accomplished by having a selective undo feature in code editors, but this has major challenges: there can be conflicts among edit operations, and it is difficult to provide usable interfaces for selective undo. In this paper, we present AZURITE, an Eclipse plug-in that allows programmers to selectively undo finegrained code changes made in the code editor. With AZURITE, programmers can easily perform backtracking tasks, even when the desired code is not in the undo stack or a version control system. AZURITE also provides novel user interfaces specifically designed for selective undo, which were iteratively improved through user feedback gathered from actual users in a preliminary field trial. A formal lab study showed that programmers can successfully use AZURITE, and were twice as fast as when limited to conventional features.<br/> &copy; 2015 IEEE.},
key={User interfaces},
keywords={Codes (symbols);Computer programming;Software engineering;},
note={Backtracking;Code changes;Code editors;Field trial;Programming environment;Selective undo;User feedback;Version control system;},
URL={http://dx.doi.org/10.1109/ICSE.2015.43},
}


@inproceedings{20173003992429,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Diagnosing machine learning pipelines with fine-grained lineage},
journal={HPDC 2017 - Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing},
author={Zhang, Zhao and Sparks, Evan R. and Franklin, Michael J.},
year={2017},
pages={143 - 153},
address={Washington, DC, United states},
abstract={We present the Hippo system to enable the diagnosis of distributed machine learning (ML) pipelines by leveraging fine-grained data lineage. Hippo exposes a concise yet powerful API, derived from primitive lineage types, to capture fine-grained data lineage for each data transformation. It records the input datasets, the output datasets and the cell-level mapping between them. It also collects sufficient information that is needed to reproduce the computation. Hippo efficiently enables common ML diagnosis operations such as code debugging, result analysis, data anomaly removal, and computation replay. By exploiting the metadata separation and high-order function encoding strategies, we observe an O (10<sup>3</sup>)x total improvement in lineage storage efficiency vs. the baseline of cell-wise mapping recording while maintaining the lineage integrity. Hippo can answer the real use case lineage queries within a few seconds, which is low enough to enable interactive diagnosis of ML pipelines.<br/> &copy; 2017 Association for Computing Machinery.},
key={Learning systems},
keywords={Artificial intelligence;Digital storage;Distributed computer systems;Mapping;Metadata;Pipelines;Plasma diagnostics;Program diagnostics;Query processing;},
note={Code debugging;Data anomalies;Data transformation;Distributed machine learning;Encoding strategy;Fine grained;Result analysis;Storage efficiency;},
URL={http://dx.doi.org/10.1145/3078597.3078603},
}


@article{20152300916011,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An empirical evaluation of supervised learning approaches in assigning diagnosis codes to electronic medical records},
journal={Artificial Intelligence in Medicine},
author={Kavuluru, Ramakanth and Rios, Anthony and Lu, Yuan},
volume={65},
number={2},
year={2015},
pages={155 - 166},
issn={09333657},
abstract={Background: Diagnosis codes are assigned to medical records in healthcare facilities by trained coders by reviewing all physician authored documents associated with a patient's visit. This is a necessary and complex task involving coders adhering to coding guidelines and coding all assignable codes. With the popularity of electronic medical records (EMRs), computational approaches to code assignment have been proposed in the recent years. However, most efforts have focused on single and often short clinical narratives, while realistic scenarios warrant full EMR level analysis for code assignment. Objective: We evaluate supervised learning approaches to automatically assign international classification of diseases (ninth revision) - clinical modification (ICD-9-CM) codes to EMRs by experimenting with a large realistic EMR dataset. The overall goal is to identify methods that offer superior performance in this task when considering such datasets. Methods: We use a dataset of 71,463 EMRs corresponding to in-patient visits with discharge date falling in a two year period (2011-2012) from the University of Kentucky (UKY) Medical Center. We curate a smaller subset of this dataset and also use a third gold standard dataset of radiology reports. We conduct experiments using different problem transformation approaches with feature and data selection components and employing suitable label calibration and ranking methods with novel features involving code co-occurrence frequencies and latent code associations. Results: Over all codes with at least 50 training examples we obtain a micro F-score of 0.48. On the set of codes that occur at least in 1% of the two year dataset, we achieve a micro F-score of 0.54. For the smaller radiology report dataset, the classifier chaining approach yields best results. For the smaller subset of the UKY dataset, feature selection, data selection, and label calibration offer best performance. Conclusions: We show that datasets at different scale (size of the EMRs, number of distinct codes) and with different characteristics warrant different learning approaches. For shorter narratives pertaining to a particular medical subdomain (e.g., radiology, pathology), classifier chaining is ideal given the codes are highly related with each other. For realistic in-patient full EMRs, feature and data selection methods offer high performance for smaller datasets. However, for large EMR datasets, we observe that the binary relevance approach with learning-to-rank based code reranking offers the best performance. Regardless of the training dataset size, for general EMRs, label calibration to select the optimal number of labels is an indispensable final step.<br/> &copy; 2015 Elsevier B.V.},
key={Classification (of information)},
keywords={Calibration;Codes (symbols);Computer aided diagnosis;Data reduction;E-learning;Feature extraction;Medical computing;Metadata;Radiation;Radiology;Supervised learning;Text processing;},
note={Code assignments;Electronic medical record;Electronic medical records (EMRs);International classification of disease;Learning to rank;Multi-label text classification;Problem transformations;Supervised learning approaches;},
URL={http://dx.doi.org/10.1016/j.artmed.2015.04.007},
}


@inproceedings{20173104015104,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={COPE: Vision for a change-oriented programming environment},
journal={Proceedings - International Conference on Software Engineering},
author={Dig, Danny and Johnson, Ralph and Marinov, Darko and Bailey, Brian and Batory, Don},
volume={0},
year={2016},
pages={773 - 776},
issn={02705257},
address={Austin, TX, United states},
abstract={Software engineering involves a lot of change as code artifacts are not only created once but maintained over time. In the last 25 years, major paradigms of program development have arisen - agile development with refactorings, software product lines, moving sequential code to multicore or cloud, etc. Each is centered on particular kinds of change; their conceptual foundations rely on transformations that (semi-) automate these changes. We are exploring how transformations can be placed at the center of software development in future IDEs, and when such a view can provide benefits over the traditional view. COPE, a Change-Oriented Programming Environment, looks at 5 activities: (1) analyze what changes programmers typically make and how they perceive, recall, and communicate changes, (2) automate transformations to make it easier to apply and script changes, (3) develop tools that compose and manipulate transformations to make it easier to reuse them, (4) integrate transformations with version control to provide better ways for archiving and understanding changes, and (5) develop tools that infer higher-level transformations from lower-level changes. Characterizing software development in terms of transformations is an essential step to take software engineering from manual development to (semi-) automated development of software.<br/> &copy; 2016 ACM.},
key={Software design},
keywords={Multicore programming;},
note={Agile development;Conceptual foundations;Level transformation;Oriented programming environment;Program development;Refactorings;Software Product Line;Version control;},
URL={http://dx.doi.org/10.1145/2889160.2889208},
}


@inproceedings{20180304663029,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Paper: Togpu: Automatic source transformation from C++ to CUDA using Clang/LLVM},
journal={IS and T International Symposium on Electronic Imaging Science and Technology},
author={Marangoni, Matthew and Wischgoll, Thomas},
year={2016},
pages={Kitware - },
issn={24701173},
address={San Francisco, CA, United states},
abstract={Parallel processing using GPUS provides substantial increases in algorithm performance across many disciplines. As a result serial algorithms are commonly translated to parallel algorithms written in CUDA or OpenCL. To perform this translation a user must first overcome various barriers to entry. These obstacles change depending on the user but in general may include learning to program using the chosen API, understanding the intricacies of parallel processing and optimization, and other issues such as the upkeep of two sets of code. Such barriers are experienced by both experts and novices alike. Leveraging the unique source to source transformation tools provided by Clang/LLVM we have created a tool to generate CUDA from C++. Such transformations reduce obstacles experienced in developing GPU software and can increase efficiency and revision speed regardless of experience. This manuscript details a new open source, cross platform tool, togpu, which performs source to source transformations from C++ to CUDA. We present experimentation results using common image processing algorithms. The tool lowers entrance barriers while preserving a singular code base and readability. Enhancing the GPU developer workflow through providing core tooling affords users immediate benefits-And facilitates further developments -To improve high performance, parallel computing.<br/> &copy; 2016 Society for Imaging Science and Technology.},
key={C++ (programming language)},
keywords={Constraint theory;Data visualization;Global system for mobile communications;Graphics processing unit;Image processing;Open source software;},
note={Algorithm performance;Entrance barrier;Further development;Image processing algorithm;Parallel processing;Serial algorithms;Source transformation;Source-to-source transformations;},
}


@inproceedings{20124515638758,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Error-correcting output codes as a transformation from multi-class to multi-label prediction},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Furnkranz, Johannes and Park, Sang-Hyeun},
volume={7569 LNAI},
year={2012},
pages={254 - 267},
issn={03029743},
address={Lyon, France},
abstract={In this paper, we reinterpret error-correcting output codes (ECOCs) as a framework for converting multi-class classification problems into multi-label prediction problems. Different well-known multi-label learning approaches can be mapped upon particular ways of dealing with the original multi-class problem. For example, the label powerset approach obviously constitutes the inverse transformation from multi-label back to multi-class, whereas binary relevance learning may be viewed as the conventional way of dealing with ECOCs, in which each classifier is learned independently of the others. Consequently, we evaluate whether alternative choices for solving the multi-label problem may result in improved performance. This question is interesting because it is not clear whether approaches that do not treat the bits of the code words independently have sufficient error-correcting properties. Our results indicate that a slight but consistent advantage can be obtained with the use of multi-label methods, in particular when longer codes are employed. &copy; 2012 Springer-Verlag Berlin Heidelberg.<br/>},
key={Inverse problems},
keywords={Codes (symbols);Errors;Learning systems;},
note={Binary relevances;Error correcting output code;Error-correcting;Inverse transformations;Multi-class problems;Multi-label learning;Multi-label problems;Multiclass classification problems;},
URL={http://dx.doi.org/10.1007/978-3-642-33492-4_21},
}


@article{20121114866136,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Martensitic-Austenitic phase transformation of Ni-Ti SMAs: Thermal properties},
journal={Intermetallics},
author={Zanotti, C. and Giuliani, P. and Chrysanthou, A.},
volume={24},
year={2012},
pages={106 - 114},
issn={09669795},
abstract={The main scope of this work concerns the definition of the thermal conductivity temperature dependence of fully dense NiTi SMAs in the temperature range where the Martensitic-Austenitic phase transformation occurs. The methodology used to evaluate this thermal property is based on an experimental-numerical approach that requires the definition of the heat capacity temperature dependence and the knowledge of the latent heat of transformation. The experimental work is based on the capability to heat the cylindrical NiTi samples uniformly on one side and to impose a variety of initial heating rates ranging from 0.1 to 5 K/s. Laser radiant energy was used as the heating source and the temperature history of the top and bottom NiTi sample surfaces were recorded using thermocouples. The numerical code considered the sample as a solid with a constant density and with thermal properties that were dependent on temperature. The heat capacity and latent heat of transformation were defined on the basis of the thermal analysis data, while the convection heat exchange coefficient was estimated from knowledge of the experimental configuration, lateral sample surface and the temperature field of the gas surrounding the sample. The results indicated that the thermal conductivity generally increased with temperature, but a minimum in the temperature range defining the Martensitic-Austenitic transformation has been pointed out. The higher thermal conductivity value of the Austenite phase is correlated with its electronic structure. &copy; 2012 Elsevier Ltd. All rights reserved.},
key={Thermal conductivity},
keywords={Austenite;Austenitic transformations;Electronic structure;Hybrid materials;Intermetallics;Latent heat;Martensitic transformations;Metadata;Specific heat;Temperature distribution;Thermoanalysis;Thermocouples;Thermodynamic properties;},
note={Austenite phase;Constant density;Convection heat exchange coefficients;Heat of transformation;Heating source;Intermetallics, miscellaneous;Numerical code;Radiant energy;Sample surface;Temperature dependence;Temperature history;Temperature range;},
URL={http://dx.doi.org/10.1016/j.intermet.2012.01.026},
}


@inproceedings{20140717317065,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A study of repetitiveness of code changes in software evolution},
journal={2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings},
author={Nguyen, Hoan Anh and Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Tien N. and Rajan, Hridesh},
year={2013},
pages={180 - 190},
address={Palo Alto, CA, United states},
abstract={In this paper, we present a large-scale study of repetitiveness of code changes in software evolution. We collected a large data set of 2,841 Java projects, with 1.7 billion source lines of code (SLOC) at the latest revisions, 1.8 million code change revisions (0.4 million fixes), 6.2 million changed files, and 2.5 billion changed SLOCs. A change is considered repeated within or cross-project if it matches another change having occurred in the history of the project or another project, respectively. We report the following important findings. First, repetitiveness of changes could be as high as 70-100% at small sizes and decreases exponentially as size increases. Second, repetitiveness is higher and more stable in the cross-project setting than in the within-project one. Third, fixing changes repeat similarly to general changes. Importantly, learning code changes and recommending them in software evolution is beneficial with accuracy for top-1 recommendation of over 30% and top-3 of nearly 35%. Repeated fixing changes could also be useful for automatic program repair. &copy; 2013 IEEE.<br/>},
key={Codes (symbols)},
keywords={Automatic programming;Software engineering;},
note={Automatic programs;Code changes;Large datasets;Large-scale studies;Software Evolution;Source lines of codes;},
URL={http://dx.doi.org/10.1109/ASE.2013.6693078},
}


@article{20114914577206,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Effects of physics change in Monte Carlo code on electron pencil beam dose distributions},
journal={Radiation Physics and Chemistry},
author={Toutaoui, Abdelkader and Khelassi-Toutaoui, Nadia and Brahimi, Zakia and Chami, Ahmed Chafik},
volume={81},
number={1},
year={2012},
pages={1 - 8},
issn={0969806X},
abstract={Pencil beam algorithms used in computerized electron beam dose planning are usually described using the small angle multiple scattering theory. Alternatively, the pencil beams can be generated by Monte Carlo simulation of electron transport. In a previous work, the 4th version of the Electron Gamma Shower (EGS) Monte Carlo code was used to obtain dose distributions from monoenergetic electron pencil beam, with incident energy between 1. MeV and 50. MeV, interacting at the surface of a large cylindrical homogeneous water phantom. In 2000, a new version of this Monte Carlo code has been made available by the National Research Council of Canada (NRC), which includes various improvements in its electron-transport algorithms. In the present work, we were interested to see if the new physics in this version produces pencil beam dose distributions very different from those calculated with oldest one. The purpose of this study is to quantify as well as to understand these differences. We have compared a series of pencil beam dose distributions scored in cylindrical geometry, for electron energies between 1. MeV and 50. MeV calculated with two versions of the Electron Gamma Shower Monte Carlo Code. Data calculated and compared include isodose distributions, radial dose distributions and fractions of energy deposition. Our results for radial dose distributions show agreement within 10% between doses calculated by the two codes for voxels closer to the pencil beam central axis, while the differences are up to 30% for longer distances. For fractions of energy deposition, the results of the EGS4 are in good agreement (within 2%) with those calculated by EGSnrc at shallow depths for all energies, whereas a slightly worse agreement (15%) is observed at deeper distances. These differences may be mainly attributed to the different multiple scattering for electron transport adopted in these two codes and the inclusion of spin effect, which produces an increase of the effective range of electrons. &copy; 2011 Elsevier Ltd.<br/>},
key={Monte Carlo methods},
keywords={Codes (symbols);Cylinders (shapes);Deposition;Electron beams;Electron energy levels;Electron transport properties;Intelligent systems;Multiple scattering;Nuclear instrumentation;},
note={Cylindrical geometry;Electron pencil beam;Isodose distribution;Monoenergetic electrons;Multiple-scattering theory;National Research Council of Canada;Pencil beam;Pencil beam algorithms;},
URL={http://dx.doi.org/10.1016/j.radphyschem.2011.08.009},
}


@inproceedings{20140717303551,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Predicting defects using change genealogies},
journal={2013 IEEE 24th International Symposium on Software Reliability Engineering, ISSRE 2013},
author={Herzig, Kim and Just, Sascha and Rau, Andreas and Zeller, Andreas},
year={2013},
pages={118 - 127},
address={Pasadena, CA, United states},
abstract={When analyzing version histories, researchers traditionally focused on single events: e.g. the change that causes a bug, the fix that resolves an issue. Sometimes however, there are indirect effects that count: Changing a module may lead to plenty of follow-up modifications in other places, making the initial change having an impact on those later changes. To this end, we group changes into change genealogies, graphs of changes reflecting their mutual dependencies and influences and develop new metrics to capture the spatial and temporal influence of changes. In this paper, we show that change genealogies offer good classification models when identifying defective source files: With a median precision of 73% and a median recall of 76%, change genealogy defect prediction models not only show better classification accuracies as models based on code complexity, but can also outperform classification models based on code dependency network metrics. &copy; 2013 IEEE.<br/>},
key={Software reliability},
keywords={Data mining;Defects;History;Software engineering;},
note={Classification accuracy;Classification models;Code complexity;Defect prediction models;Dependency networks;Indirect effects;Mutual dependencies;Predictive models;},
URL={http://dx.doi.org/10.1109/ISSRE.2013.6698911},
}


@inproceedings{20153201117835,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An online course and teacher resource for residential building codes and above code construction methods},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
author={Shealy, Tripp and Kiesling, Audra Ann and Smail, Timothy R.},
volume={122nd ASEE Annual Conference and Exposition: Making Value for Society},
number={122nd ASEE Annual Conference and Exposition: Making Value for Society},
year={2015},
issn={21535965},
address={Seattle, WA, United states},
abstract={Community destruction and loss of life due to residential building code violations still occur too frequently and increasing code enforcement is often not possible due to lack of funds and resources. Teaching the International Residential Code (IRC) to college-level construction students is another way to encourage greater code compliance and enhance community resilience. In a national curriculum review of construction management, architecture, and civil engineering programs (2-year, 4-year, and graduate degrees, 950 in total), only seven percent provide courses with IRC related learning outcomes. A follow-up national survey to construction, architecture and civil engineering faculty suggests the barriers to teach codes are the lack of available resources and low cognitive student learning perceived in teaching about the IRC. In response to these findings an online course was developed. Students learn how codes will influence their professional careers, identifying the difference between prescriptive and performance based codes and communicating how codes relate to the performance of a structure. Student learning outcomes are created through multiple active learning methods. For example, house plans are distributed to students, and in a problem-based approach, students "red line" drawings to meet the IRC. In a case-based module, students identify solutions to grey-water systems that do not meet current local codes. Course modules were developed with an advisory committee including building code officials, architects, construction managers, disaster mitigation experts, and academic faculty. Advisory members anonymously submitted feedback for each module. Feedback was compiled, discussed and course content edited. This review-discuss-edit process was repeated until a final version was agreed upon with the advisory committee. The course and content is a free resource for educators. Over thirty modules, house plans and videos of industry professionals are embedded within. Modules can be delivered in a semester long course but can also stand-alone. Course link: canvas.instructure.com/courses/780681. &copy; American Society for Engineering Education, 2015.},
}


@inproceedings{20180904837321,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mining Version Control System for Automatically Generating Commit Comment},
journal={International Symposium on Empirical Software Engineering and Measurement},
author={Huang, Yuan and Zheng, Qiaoyang and Chen, Xiangping and Xiong, Yingfei and Liu, Zhiyong and Luo, Xiaonan},
volume={2017-November},
year={2017},
pages={414 - 423},
issn={19493770},
address={Toronto, ON, Canada},
abstract={Commit comments increasingly receive attention as an important complementary component in code change comprehension. To address the comment scarcity issue, a variety of automatic approaches for commit comment generation have been intensively proposed. However, most of these approaches mechanically outline a superficial level summary of the changed software entities, the change intent behind the code changes is lost (e.g., the existing approaches cannot generate such comment: 'fixing null pointer exception'). Considering the comments written by developers often describe the intent behind the code change, we propose a method to automatically generate commit comment by reusing the existing comments in version control system. Specifically, for an input commit, we apply syntax, semantic, pre-syntax, and pre-semantic similarities to discover the similar commits from half a million commits, and recommend the reusable comments to the input commit from the ones of the similar commits. We evaluate our approach on 7 projects. The results show that 9.1% of the generated comments are good, 27.7% of the generated comments need minor fix, and 63.2% are bad, and we also analyze the reasons that make a comment available or unavailable.<br/> &copy; 2017 IEEE.},
key={Codes (symbols)},
keywords={Control systems;Information management;Semantics;Software engineering;Syntactics;},
note={Automatic approaches;Code changes;Code semantics;Code Syntax Similarity;Commit Comment Generation;Semantic similarity;Software entities;Version control system;},
URL={http://dx.doi.org/10.1109/ESEM.2017.56},
}


@inproceedings{20161102102447,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={What change history tells us about thread synchronization},
journal={2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings},
author={Gu, Rui and Jin, Guoliang and Song, Linhai and Zhu, Linjie and Lu, Shan},
year={2015},
pages={426 - 438},
address={Bergamo, Italy},
abstract={Multi-threaded programs are pervasive, yet difficult to write. Missing proper synchronization leads to correctness bugs and over synchronization leads to performance problems. To improve the correctness and efficiency of multi-threaded software, we need a better understanding of synchronization challenges faced by real-world developers. This paper studies the code repositories of open-source multi-threaded software projects to obtain a broad and indepth view of how developers handle synchronizations. We first examine how critical sections are changed when software evolves by checking over 250,000 revisions of four representative open-source software projects. The findings help us answer questions like how often synchronization is an afterthought for developers; whether it is difficult for developers to decide critical section boundaries and lock variables; and what are real-world over-synchronization problems. We then conduct case studies to better understand (1) how critical sections are changed to solve performance problems (i.e. over-synchronization issues) and (2) how software changes lead to synchronization-related correctness problems (i.e. concurrency bugs). This in-depth study shows that tool support is needed to help developers tackle over-synchronization problems; it also shows that concurrency bug avoidance, detection, and testing can be improved through better awareness of code revision history.<br/> &copy; 2015 ACM.},
key={Open source software},
keywords={Concurrency control;Locks (fasteners);Open systems;Problem solving;Program debugging;Synchronization;},
note={Concurrency bugs;Empirical studies;Multithreaded softwares;Performance bugs;Repository mining;},
URL={http://dx.doi.org/10.1145/2786805.2786815},
}


@inproceedings{20183405719693,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Classifying code comments in Java mobile applications},
journal={Proceedings - International Conference on Software Engineering},
author={Pascarella, Luca},
year={2018},
pages={39 - 40},
issn={02705257},
address={Gothenburg, Sweden},
abstract={Developers adopt code comments for different reasons such as document source codes or change program flows. Due to a variety of use scenarios, code comments may impact on readability and maintainability. In this study, we investigate how developers of 5 open-source mobile applications use code comments to document their projects. Additionally, we evaluate the performance of two machine learning models to automatically classify code comments. Initial results show marginal differences between desktop and mobile applications.<br/> &copy; 2018 ACM.},
key={Open systems},
keywords={Codes (symbols);Java programming language;Learning systems;Mobile computing;Open source software;},
note={Android;Change programs;code comments;Mining software repositories;Mobile applications;Open sources;Source codes;Two machines;},
URL={http://dx.doi.org/10.1145/3197231.3198444},
}


@article{20183405718137,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Emerging topics in mining software repositories: Machine learning in software repositories and datasets},
journal={Progress in Artificial Intelligence},
author={Guemes-Pena, Diego and Lopez-Nozal, Carlos and Marticorena-Sanchez, Raul and Maudes-Raedo, Jesus},
volume={7},
number={3},
year={2018},
pages={237 - 247},
issn={21926352},
abstract={A software process is a set of related activities that culminates in the production of a software package: specification, design, implementation, testing, evolution into new versions, and maintenance. There are also other supporting activities such as configuration and change management, quality assurance, project management, evaluation of user experience, etc. Software repositories are infrastructures to support all these activities. They can be composed with several systems that include code change management, bug tracking, code review, build system, release binaries, wikis, forums, etc. This position paper on mining software repositories presents a review and a discussion of research in this field over the past decade. We also identify applied machine learning strategies, current working topics, and future challenges for the improvement of company decision-making systems. Machine learning is defined as the process of discovering patterns in data. It can be applied to software repositories, since every change is recorded as data. Companies can then use these patterns as the basis for their decision-making systems and for knowledge discovery.<br/> &copy; 2018, Springer-Verlag GmbH Germany, part of Springer Nature.},
key={Computer software selection and evaluation},
keywords={Artificial intelligence;Data mining;Decision making;Learning systems;Project management;Quality assurance;Software engineering;Software testing;},
note={Applied machine learning;Change management;Decision-making systems;Evaluation of users;Future challenges;Mining software repositories;Software process;Software repositories;},
URL={http://dx.doi.org/10.1007/s13748-018-0147-7},
}


@article{20183805837730,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Yet Another Intelligent Code-Generating System: A Flexible and Low-Cost Solution},
journal={Journal of Computer Science and Technology},
author={Filho, Joao Fabricio and Rodriguez, Luis Gustavo Araujo and da Silva, Anderson Faustino},
volume={33},
number={5},
year={2018},
pages={940 - 965},
issn={10009000},
abstract={Modern compilers apply various code transformation algorithms to improve the quality of the target code. However, a complex problem is to determine which transformation algorithms must be utilized. This is difficult because of three reasons: a number of transformation algorithms, various combination possibilities, and several configuration possibilities. Over the last few years, various intelligent systems were presented in the literature. The goal of these systems is to search for transformation algorithms and thus apply them to a certain program. This paper proposes a flexible, low-cost and intelligent system capable of identifying transformation algorithms for an input program, considering the program&rsquo;s specific features. This system is flexible for parameterization selection and has a low-computational cost. In addition, it has the capability to maximize the exploration of available computational resources. The system was implemented under the Low Level Virtual Machine infrastructure and the results indicate that it is capable of exceeding, up to 21.36%, performance reached by other systems. In addition, it achieved an average improvement of up to 17.72% over the most aggressive compiler optimization level of the Low Level Virtual Machine infrastructure.<br/> &copy; 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
key={Program compilers},
keywords={Codes (symbols);Cosine transforms;Costs;Intelligent systems;Iterative methods;Knowledge representation;Learning systems;Network security;Virtual machine;},
note={Code transformation;compiler;Compiler optimizations;Computational costs;Computational resources;Iterative compilation;Low level virtual machines;Transformation algorithm;},
URL={http://dx.doi.org/10.1007/s11390-018-1867-7},
}


@inproceedings{20144400141824,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Supporting streams of changes during branch integration},
journal={Science of Computer Programming},
author={Uquillas Gomez, Veronica and Ducasse, Stephane and Kellens, Andy},
volume={96},
number={P1},
year={2014},
pages={84 - 106},
issn={01676423},
abstract={When developing large applications, integrators face the problem of integrating changes between branches or forks. While version control systems provide support for merging changes, this support is mostly text-based, and does not take the program entities into account. Furthermore, there exists no support for assessing which other changes a particular change depends on have to be integrated. Consequently, integrators are left to perform a manual and tedious comparison of the changes within the sequence of their branch and to successfully integrate them. In this paper, we present an approach that analyzes changes within a sequence of changes (stream of changes): such analysis identifies and characterizes dependencies between the changes. The approach identifies changes as autonomous, only used by others, only using other changes, or both. Such a characterization aims at easing the integrator's work. In addition, the approach supports important queries that an integrator otherwise has to perform manually. We applied the approach to a stream of changes representing 5 years of development work on an open-source project and report our experiences.<br/> &copy; 2014 Elsevier B.V. All rights reserved.},
key={Open source software},
keywords={Computer programming;Merging;Software engineering;},
note={Branch;Change dependencies;Open source projects;Source code changes;Stream of changes;Version control system;},
URL={http://dx.doi.org/10.1016/j.scico.2014.07.012},
}


@inproceedings{20173904214909,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Revisiting context-based code smells prioritization: On supporting referred context},
journal={ACM International Conference Proceeding Series},
author={Sae-Lim, Natthawute and Hayashi, Shinpei and Saeki, Motoshi},
volume={Part F129907},
year={2017},
address={Cologne, Germany},
abstract={Because numerous code smells are revealed by code smell detectors, many attempts have been undertaken to mitigate related problems by prioritizing and filtering code smells. We earlier proposed a technique to prioritize code smells by leveraging the context of the developers, i.e., the modules that the developers plan to implement. Our empirical studies revealed that the results of code smells prioritized using our technique are useful to support developers' implementation on the modules they intend to change. Nonetheless, in software change processes, developers often navigate through many modules and refer to them before making actual changes. Such modules are important when considering the developers' context. Therefore, it is essential to ascertain whether our technique can also support developers on modules to which they are going to refer to make changes. We conducted an empirical study of an open source project adopting tools for recording developers' interaction history. Our results demonstrate that the code smells prioritized using our approach can also be used to support developers for modules to which developers are going to refer, irrespective of the need for modification.<br/> &copy; 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
key={Codes (symbols)},
keywords={Odors;Open source software;},
note={Code smell;Empirical studies;Impact analysis;Interaction history;Issue Tracking;Open source projects;Prioritization;Software change;},
URL={http://dx.doi.org/10.1145/3120459.3120463},
}


@inproceedings{20134516949282,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Collaborative bug triaging using textual similarities and change set analysis},
journal={2013 6th International Workshop on Cooperative and Human Aspects of Software Engineering, CHASE 2013 - Proceedings},
author={Kevic, Katja and Muller, Sebastian C. and Fritz, Thomas and Gall, Harald C.},
year={2013},
pages={17 - 24},
address={San Francisco, CA, United states},
abstract={Bug triaging assigns a bug report, which is also known as a work item, an issue, a task or simply a bug, to the most appropriate software developer for fixing or implementing it. However, this task is tedious, time-consuming and error-prone if not supported by effective means. Current techniques either use information retrieval and machine learning to find the most similar bugs already fixed and recommend expert developers, or they analyze change information stemming from source code to propose expert bug solvers. Neither technique combines textual similarity with change set analysis and thereby exploits the potential of the interlinking between bug reports and change sets. In this paper, we present our approach to identify potential experts by identifying similar bug reports and analyzing the associated change sets. Studies have shown that effective bug triaging is done collaboratively in a meeting, as it requires the coordination of multiple individuals, the understanding of the project context and the understanding of the specific work practices. Therefore, we implemented our approach on a multi-touch table to allow multiple stakeholders to interact simultaneously in the bug triaging and to foster their collaboration. In the current stage of our experiments we have experienced that the expert recommendations are more specific and useful when the rationale behind the expert selection is also presented to the users. &copy; 2013 IEEE.<br/>},
key={Program debugging},
keywords={Learning systems;Software engineering;},
note={bug triaging;collaboration;Expert recommendations;Multi-touch;Multi-touch tables;Multiple stakeholders;Software developer;Textual similarities;},
URL={http://dx.doi.org/10.1109/CHASE.2013.6614727},
}


@inbook{20172803909807,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A web-based system for error correction questions in programming exercise},
journal={Artificial Intelligence Technologies and the Evolution of Web 3.0},
author={Hachisu, Yoshinari and Yoshida, Atsushi},
year={2015},
pages={124 - 143},
abstract={In this chapter, the authors propose a system for generating error correction questions of programs, which are suitable for developing debugging skills in programming education. The system generates HTML files for answering questions and CGI programs for checking answers. They are deployed on a Web server, and learners read and answer questions on Web browsers. It provides an intuitive user interface; a learner can edit codes in place at the text. To make programs including errors for learning debugging, the authors analyze types of errors and define the processes of error injection as code transformation patterns. If learners can edit any codes freely, it is difficult to check all possible answers. Instead, the authors adopt a strategy to restrict editable points and possible answers from the educational view. A demonstration of error correction questions of the C language is available at http://ecq.tebasaki.jp/.<br/> &copy; 2015 by IGI Global. All rights reserved.},
key={C (programming language)},
keywords={Codes (symbols);Cosine transforms;Error correction;Program debugging;User interfaces;Web browsers;},
note={Code transformation;Error injection;HTML files;Intuitive user interface;Programming education;Programming exercise;Web servers;Web-based system;},
URL={http://dx.doi.org/10.4018/978-1-4666-8147-7.ch006},
}


@inproceedings{20183105627563,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Assessing change proneness at the architecture level: An empirical validation},
journal={Proceedings - 2017 24th Asia-Pacific Software Engineering Conference Workshops, APSECW 2017},
author={Arvanitou, Elvira Maria and Ampatzoglou, Apostolos and Tzouvalidis, Konstantinos and Chatzigeorgiou, Alexander and Avgeriou, Paris and Deligiannis, Ignatios},
volume={2018-January},
year={2018},
pages={98 - 105},
address={Nanjing, China},
abstract={Change proneness is a characteristic of software artifacts that represents their probability to change in future. Change proneness can be assessed at different levels of granularity, ranging from classes to modules. Although change proneness can be successfully assessed at the source code level (i.e., methods and classes), it remains rather unexplored for architectures. Additionally, the methods that have been introduced at the source code level are not directly transferable to the architecture level. In this paper, we propose and empirically validate a method for assessing the change proneness of architectural modules. Assessing change proneness at the level of architectural modules requires information from two sources: (a) the history of changes in the module, as a proxy of how frequently the module itself undergoes changes; and (b) the dependencies with other modules that affect the probability of a change being propagated from one module to the other. To validate the proposed approach, we performed a case study on five open-source projects. Specifically, we compared the accuracy of the proposed approach to the use of software package metrics as assessors of modules change proneness, based on the 1061-1998 IEEE Standard. The results suggest that compared to examined metrics, the proposed method is a better assessor of change proneness. Therefore, we believe that the method and accompanying tool can effectively aid architects during software maintenance and evolution.<br/> &copy; 2017 IEEE.},
key={Open source software},
keywords={IEEE Standards;},
note={Architectural metrics;Change proneness;Empirical;Empirical validation;Open source projects;Software artifacts;Software maintenance and evolution;Source codes;},
URL={http://dx.doi.org/10.1109/APSECW.2017.21},
}


@inproceedings{20183905859992,
language={Turkish},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A new approach making it possible to change software behavior dynamically},
title={Yazılım Davranışlarının Dinamik Olarak Değiştirilebilmesini Sağlayan Yeni Bir Yaklaşım},
journal={CEUR Workshop Proceedings},
author={Gerede, Cada Evren},
volume={2201},
year={2018},
issn={16130073},
address={Istanbul, Turkey},
abstract={Nowadays software provided over web and mobile platforms can be frequently updated. W i t h updates the behavior of software changes. Certain behaviors can cause undesired damages. Therefore, the correc&not; tion of incorrect behavior that reaches customers is done via new up&not; dates. Fixing software's source code, testing these new changes, getting peer approval through code reviews, building a new version of the soft&not; ware and deploying it to the production environment can take hours. As a result, being able to correct any changes that may severely affect a lot of customers in the matter of seconds is an important need for software developers to preserve developers' prestige and customer satisfaction. I n this study, we propose an approach that enables developers to change software's behavior at run time and describe an implementation realiz&not; ing this approach. W i t h our approach, software updates can be deployed incrementally without making any code changes or they can be reverted completely within seconds. I n addition, our approach makes it possible to try out experimental features on small subsets of users in production environments.<br/>},
key={Software testing},
keywords={Codes (symbols);Customer satisfaction;Sales;},
note={Junction;Mobile platform;Production environments;Software behavior;Software change;Software deployment;Software developer;Software updates;},
}


@inproceedings{20171803619433,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={MaLTeSQuE 2017 - IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with SANER 2017},
journal={MaLTeSQuE 2017 - IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with SANER 2017},
year={2017},
pages={Alpen-Adria Universitat; IEEE; IEEE Computer Society; Technical Council on Software Engineering (TCSE) - },
address={Klagenfurt, Austria},
abstract={The proceedings contain 6 papers. The topics discussed include: using source code metrics to predict change-prone web services: a case-study on eBay services; investigating code smell co-occurrences using association rule learning: a replicated study; using machine learning to design a flexible LOC counter; machine learning for finding bugs: an initial report; automatic feature selection by regularization to improve bug prediction accuracy; and hyperparameter optimization to improve bug prediction accuracy.},
}


@article{20163502741221,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={CAVE-CL: An OpenCL version of the package for detection and quantitative analysis of internal cavities in a system of overlapping balls: Application to proteins},
journal={Computer Physics Communications},
author={Bua, Jan and Bua, Jan and Hayryan, Shura and Hu, Chin-Kun and Wu, Ming-Chya},
volume={190},
year={2015},
pages={224 - 227},
issn={00104655},
abstract={Here we present the revised and newly rewritten version of our earlier published CAVE package (Bu&scaron;a et&nbsp;al., 2010) which was originally written in FORTRAN. The package has been rewritten in C language, the algorithm has been parallelized and implemented using OpenCL. This makes the program convenient to run on platforms with Graphical Processing Units (GPUs). Improvements include also some modifications/optimizations of the original algorithm. A considerable improvement in the performance of the code has been achieved. A new tool called input_structure has been added which helps the user to make the data input and conversion more easier and universal. New version program summary Program Title: CAVE-CL, CAVE C Catalogue identifier: AEHC_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/aehc_v2_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 32646 No. of bytes in distributed program, including test data, etc.: 444248 Distribution format: tar.gz Programming language: C, C++, OpenCL. Computer: PC with GPU. Operating system: OpenCL compatible systems. Has the code been vectorized or parallelized?: Parallelized using GPUs. A revised serial version (non GPU) is included in the package as well. Keywords: Proteins, Solvent accessible area, Excluded volume, Cavities, Analytic method, Stereographic projection, GPGPU, OpenCL. PACS: 82.20.Wt, 02.60.Cb, 02.70.Ns. Classification: 16.1. Catalogue identifier of previous version: AEHC_v1_0. Journal reference of previous version: Comput. Phys. Commun. 181 (2010) 2116. Does the new version supersede the previous version?: Yes Nature of problem: Molecular structure analysis. Solution method: Analytical method, which uses the stereographic transformation for exact detection of internal cavities in the system of overlapping balls and numerical algorithm for calculation of the volume and the surface area of cavities. Reasons for the new version: This work is in line with our global efforts to modernize the protein structure related algorithms and software packages developed in our research group during last several years [1&ndash;8]. These tools are keeping to receive considerable attention from researches and they have been used in solving many interesting research problems [9,10]. Among many others, one important application has been found by the members of our team [11]. Therefore, we think that there is a demand to revise and modernize these tools and to make them more efficient. Here we follow the approach used earlier in [8] to develop a new version of the CAVE package [7]. The original CAVE package was written in FORTRAN language. One of the reasons for the new version is to rewrite it in C in order to make it more friendly to the young researchers who are not familiar with FORTRAN. Another, a more important reason, is to use the possibilities of the contemporary hardware (for example, the modern graphical cards) to improve the performance of the package. We also want to allow the user to avoid the re-compiling of the program for every molecule during multiple calculations of the array of molecules. For this purpose we are providing the possibility to use general pdb files as an input. After compiling one time, the program can receive any number of input files successively. Also, we found it necessary to go through the algorithm and to optimize, where it is possible, the memory usage and to make the algorithm more efficient. Summary of revisions: 1. Memory usage and language. The whole code has been ported into C and the static arrays have been replaced with dynamic memory allocation. This allows to load and handle the proteins of arbitrary size.2. Changes in the algorithm. Like in [8], the original method of North Pole test and molecule rotation [4] has been changed. The details of implementation and the benefits from this change are properly described in [8] and we find it not necessary to repeat it here.3. New tool. A module called input_structure which takes as an input a protein structure file in the format compatible with Protein Data Bank (pdb) [12] has been adopted from [8]. Using external tool allows users to create their own mappings of atoms and radii without re-compiling the module input_structure itself or the CAVE. It is the user's responsibility to assign proper radii to each type of atoms. One can use any of the published standard sets of radii (see for example, [13&ndash;17]). Alternatively, the user can assign his own values for radii immediately in the module input_structure. The radii are assigned in a special file with extension pds (see the documentation) which consists of lines like this: ATOM CA ALA 2.0 which is read as &ldquo;the C<inf>&alpha;</inf>atom of Alanine has radius 2.0 &#8491;&rdquo;.4. Some computational tricks. In several parts of the program square roots were replaced by second powers and calls of sin and cos functions were replaced by calls to sincos allowing for further speed-up (in comparison to original FORTRAN version). The typical value of the relative error between results obtained by original (FORTRAN), C, and OpenCL versions was between 10<sup>&minus;8</sup>and 10<sup>&minus;10</sup>and it never exceeded 10<sup>&minus;5</sup>. Small differences in results can be due to the implementation of compiler and specially in case of OpenCL also in the implementation of arithmetic by the GPU vendor. [Table presented]5. OpenCL implementation and testing results. OpenCL [18] is an open standard for parallel programming in heterogeneous systems. It is becoming increasingly popular and has proved to be an efficient tool for computations in different fields (see, for example, the most recent [19,20] and the references therein). Table&nbsp;1 shows the speedup of the C and OpenCL implementations of CAVE as compared to the FORTRAN version. We compare both results obtained using free GNU FORTRAN (g77) and commercial (and faster) ifort. Speedup is calculated as a ratio between the original time obtained by FORTRAN and C or OpenCL version of program. Times of execution are measured in seconds. [Figure presented] One could expect greater speed-ups but the problem is that not the whole algorithm could be parallelized. Only about 1/3 of the whole program was parallelized and the effect of this is visible for the proteins with 2000 atoms and more if the calculation time of FORTRAN version is higher than approximately 10 s. The rest of the code is sequential and its parallelization will require entirely new algorithm which might be the future work. Fig.&nbsp;1 shows the speed-up as a function of number of neighbors. This clearly indicates, that the effect of parallelization is stronger for proteins with many neighbors. This is also the reason, why the effect is not so strong for proteins with 0 testing sphere radius. Most of the cavities in such case are enclosed only in few (around 4&ndash;8) spheres, while in the case of 1.2 testing sphere radius we have easily 35 or more enclosing spheres. In global, we can see that C version is a good choice for general proteins (and testing sphere radius of 0), OpenCL is proper for larger proteins and larger computational times. 0 in the name of protein means that no probe radius has been added to the atomic radii. In other cases 1.2 &#8491; was added to all atomic radii. All results were obtained on computer with Intel Core 2 Duo E8500 CPU running at 3.16&nbsp;GHz with 4&nbsp;GB RAM and GPU NVIDIA GTX470 and computer with Intel Xeon X5450 CPU running at 3.00&nbsp;GHz with 32&nbsp;GB RAM and dedicated NVIDIA C1060 GPU card. When considering which GPU to use, it is important to watch its double precision performance. Consumer oriented GPUs have usually intentionally decreased double precision performance and because of that results can be similar even if newer generation of GPUs is used. For instance in 2010 the performance in double precision of NVIDIA GPUs (except for highly specialized GPUs for scientific computing) was 1/8 of the performance in single precision. Nowadays (2014) this ratio is 1/24, meaning that GPUs from 2010 are as fast as current GPUs (except for special editions of GPUs or dedicated cards).<br/> &copy; 2015 Elsevier B.V.},
key={C++ (programming language)},
keywords={Amino acids;Atoms;Bioinformatics;Caves;Codes (symbols);Computer graphics equipment;Data handling;Distributed computer systems;FORTRAN (programming language);Graphics processing unit;Molecules;Numerical methods;Open source software;Parallel programming;Problem oriented languages;Program compilers;Proteins;Random access storage;Software testing;Spheres;Storage allocation (computer);},
note={Catalogue identifiers;Cavity;Dynamic memory allocation;Graphical processing unit (GPUs);Heterogeneous systems;Numerical algorithms;OpenCL;Stereographic projection;},
URL={http://dx.doi.org/10.1016/j.cpc.2014.12.017},
}


@article{20174904493538,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Aggregating Association Rules to Improve Change Recommendation},
journal={Empirical Software Engineering},
author={Rolfsnes, Thomas and Moonen, Leon and Alesio, Stefano Di and Behjati, Razieh and Binkley, Dave},
volume={23},
number={2},
year={2018},
pages={987 - 1035},
issn={13823256},
abstract={As the complexity of software systems grows, it becomes increasingly difficult for developers to be aware of all the dependencies that exist between artifacts (e.g., files or methods) of a system. Change recommendation has been proposed as a technique to overcome this problem, as it suggests to a developer relevant source-code artifacts related to her changes. Association rule mining has shown promise in deriving such recommendations by uncovering relevant patterns in the system&rsquo;s change history. The strength of the mined association rules is captured using a variety of interestingness measures. However, state-of-the-art recommendation engines typically use only the rule with the highest interestingness value when more than one rule applies. In contrast, we argue that when multiple rules apply, this indicates collective evidence, and aggregating those rules (and their evidence) will lead to more accurate change recommendation. To investigate this hypothesis we conduct a large empirical study of 15 open source software systems and two systems from our industry partners. We evaluate association rule aggregation using four variants of the change history for each system studied, enabling us to compare two different levels of granularity in two different scenarios. Furthermore, we study 40 interestingness measures using the rules produced by two different mining algorithms. The results show that (1) between 13 and 90% of change recommendations can be improved by rule aggregation, (2) rule aggregation almost always improves change recommendation for both algorithms and all measures, and (3) fine-grained histories benefit more from rule aggregation.<br/> &copy; 2017, Springer Science+Business Media, LLC.},
key={Open systems},
keywords={Association rules;Data mining;Open source software;},
note={Change impact analysis;Change recommendations;Empirical studies;Interestingness;Interestingness measures;Mining algorithms;Open source software systems;Relevant patterns;},
URL={http://dx.doi.org/10.1007/s10664-017-9560-y},
}


@inproceedings{20173504097106,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Requirements Traceability Through Information Retrieval Using Dynamic Integration of Structural and Co-change Coupling},
journal={Communications in Computer and Information Science},
author={Jyoti and Chhabra, Jitender Kumar},
volume={712},
year={2017},
pages={107 - 118},
issn={18650929},
address={Jalandhar, India},
abstract={Requirement Traceability (RT) links correlate requirements to their corresponding source code and helps in better requirement understanding, reusability and other software maintenance activities. Since a major portion of software artifacts is in the form of text, for finding these links Information Retrieval (IR) techniques based on textual similarity are widely adopted for Requirement Traceability. But it is hard to find RT links when artifacts have less textual description. So, for finding these links indirectly non-textual techniques like structural information based, co-change history based, ownership based are used with IR. However, if the results of IR contain false positives, the combined approach may increase them further. So, instead of directly combining, this paper proposes an automatic technique for RT by first improving the IR approach and then combining it with the non-textual based techniques. Also, we present a new non-textual based technique based on weighted integration of structural coupling and change history based coupling of classes for retrieving indirect links. The results show that our proposed approach performs better than the existing methods which use coupling information complementary to IR.<br/> &copy; 2017, Springer Nature Singapore Pte Ltd.},
key={Information use},
keywords={Computer software reusability;Information retrieval;Integration;Reusability;},
note={Change couplings;Requirement traceabilitys;Requirements traceability;Software maintenance activity;Structural coupling;Structural information;Textual description;Textual similarities;},
URL={http://dx.doi.org/10.1007/978-981-10-5780-9_10},
}


@article{20172803929337,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={HMETS-A simple and efficient hydrology model for teaching hydrological modelling, flow forecasting and climate change impacts},
journal={International Journal of Engineering Education},
author={Martel, Jean-Luc and Demeester, Kenjy and Brissette, Francois and Poulin, Annie and Arsenault, Richard},
volume={33},
number={4},
year={2017},
pages={1307 - 1316},
issn={0949149X},
abstract={Hydrological models are commonly used to forecast streamflow and for climate change impact studies. There is a wide range of hydrology models using lumped conceptual approaches all the way to more physically based distributed algorithms. Most of these models come with a steep learning curve before they can be used efficiently by the end user, and they can be tricky to calibrate appropriately. Only a small number of hydrology models can be considered easy to set up and use, and even fewer provide their source code for easy modification to be tailored to individual needs. These drawbacks make it difficult to use these models in educational applications. The goal of this paper is to introduce a very simple, yet efficient, lumped-conceptual hydrological model designed to address the above problems. The MATLAB-based HMETS hydrological model is simple and can be easily and quickly set up on a new watershed, including automatic calibration using state of the art optimization algorithms. Despite its simplicity, the model has proved to perform well against two other lumped-conceptual hydrological models over 320 watersheds. HMETS obtained a median Nash-Sutcliffe Efficiency of 0.72 in validation, compared to 0.64 for MOHYSE (similar structure) and 0.77 for HSAMI (more complex structure). The model's source code is freely available and includes an optional simplified user interface. A climate change impacts simulation tool using the constant scaling downscaling method is also incorporated to the interface. HMETS has been tested in the Construction Engineering Final-Year Project for a group of 60 undergraduate students.<br/> &copy; 2017 TEMPUS Publications.},
key={Climate change},
keywords={Climate models;Hydrology;MATLAB;Rain;Students;User interfaces;},
note={Climate change impact;Conceptual model;Hydrological modeling;Lumped modeling;Rainfall-runoff modeling;},
}


@inproceedings{20173003980993,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A method for assessing class change proneness},
journal={ACM International Conference Proceeding Series},
author={Arvanitou, Elvira-Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Avgeriou, Paris},
volume={Part F128635},
year={2017},
pages={186 - 195},
address={Karlskrona, Sweden},
abstract={Change proneness is a quality characteristic of software artifacts that represents their probability to change in the future due to: (a) evolving requirements, (b) bug fixing, or (c) ripple effects. In the literature, change proneness has been associated with many negative consequences along software evolution. For example, artifacts that are change-prone tend to produce more defects, and accumulate more technical debt. Therefore, identifying and monitoring modules of the system that are change-prone is of paramount importance. Assessing change proneness requires information from two sources: (a) the history of changes in the artifact as a proxy of how frequently the artifact itself is changing, and (b) the source code structure that affects the probability of a change being propagated among artifacts. In this paper, we propose a method for assessing the change proneness of classes based on the two aforementioned information sources. To validate the proposed approach, we performed a case study on five open-source projects. Specifically, we compared the accuracy of the proposed approach to the use of other software metrics and change history to assess change proneness, based on the 1061-1998 IEEE Standard on Software Measurement. The results of the case study suggest that the proposed method is the most accurate and reliable assessor of change proneness. The high accuracy of the method suggests that the method and accompanying tool can effectively aid practitioners during software maintenance and evolution.<br/> &copy; 2017 Association for Computing Machinery ACM.},
key={C (programming language)},
keywords={Computer software selection and evaluation;IEEE Standards;Open source software;},
note={Change proneness;Information sources;Metrics;Open source projects;Quality characteristic;Software maintenance and evolution;Software Measurement;Software Quality;},
URL={http://dx.doi.org/10.1145/3084226.3084239},
}


@inproceedings{20161602263122,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Tool for detecting standardwise differences in C++ legacy code},
journal={2015 IEEE 13th International Scientific Conference on Informatics, INFORMATICS 2015 - Proceedings},
author={Brunner, Tibor and Pataki, Norbert and Porkolab, Zoltan},
year={2015},
pages={57 - 62},
address={Poprad, Slovakia},
abstract={Programming languages are continuously evolving as the experiences are accumulated, developers face new problems and other requirements such as increasing support for multi-threading is emerging. These changes are reflected in new language standards and compiler versions. Although these changes are carefully planned to keep reverse compatibility with previous versions to keep the syntax and the semantics of earlier written code, sometimes languages break this rule. In case of silent semantic changes, when the earlier written code is recompiled with the new version, this is especially harmful. The new C++11 standard introduced major changes in the core language. This changes are widely believed to be reverse compatible, i.e. a simple recompilation of earlier written code will keep the old semantics. Recently we found examples that the backward compatibility between language versions is broken. The previously written code compiled with a new C++ compiler may change the program behaviour without any compiler diagnostic message. In a large code base such issues are very hard to catch by manual inspection, therefore some automatic tool support is required for this purpose. In this paper we propose a tool support to detect such backward incompatibilities in C++. The basic idea is to parse the source code using different standards, and then compare the abstract syntax trees. We implemented a proof of concept prototype tool to demonstrate our idea based on the LLVM/Clang compiler infrastructure.<br/> &copy; 2015 IEEE.},
key={C++ (programming language)},
keywords={Codes (symbols);Program compilers;Program diagnostics;Semantics;Syntactics;Trees (mathematics);},
note={Abstract Syntax Trees;Automatic tools;Backward compatibility;Diagnostic messages;Language standards;Manual inspection;Multi-threading;Proof of concept;},
URL={http://dx.doi.org/10.1109/Informatics.2015.7377808},
}


@inproceedings{20184906172021,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Implication of N400 and P600 waves in the Linguistic Code Change in Monolinguals and Bilinguals},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
author={Achanccaray, David and Astucuri, Jhonatan and Hayashibe, Mitsuhiro and Pirca, Jairo and Espinoza, Vilma},
volume={2018-July},
year={2018},
pages={2032 - 2035},
issn={1557170X},
address={Honolulu, HI, United states},
abstract={There is evidence of the importance of N400 and P600 waves in linguistic processes, theses brain waves are related to syntax. This work proposes to evaluate learning process through the analysis of responses generated when formulation of word is requested, an artificial grammar test (AGT) is developed and N400 and P600 peaks are taken as indicators of performance; and two different groups of subjects took the AGT, 5 monolinguals and 5 bilinguals.The AGT is composed by 30 hybrids, each hybrid defines rules to formulate words; then if this word accomplished the rules, it is considered as grammatical. The N400 and P600 waves are computed by each word letter, and the mean for all 30 hybrids is compared between both two groups by electrode.Greater amplitudes for N400 and P600 peaks was found for monolinguals in comparison with bilinguals.<br/> &copy; 2018 IEEE.},
URL={http://dx.doi.org/10.1109/EMBC.2018.8512651},
}


@article{20174204270641,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Wearable tech for halloween-The gemma MO's embedded python lets you change your code on the fly [Resources-Tools]},
journal={IEEE Spectrum},
author={Cass, Stephen},
volume={54},
number={10},
year={2017},
pages={15 - 16},
issn={00189235},
abstract={Halloween is approaching, and with it a global parade of costumes. So I thought this would be the perfect time to try out a new wearable microcontroller from Adafruit Industries: The Gemma M0. Adafruit has been putting out wearable microcontrollers for several years. These differ from conventional controllers, such as the Arduino Uno, in that the wearables are typically more compact and use pads with large through holes for input and output, instead of pins. These holes make it easy to sew boards to fabric or tie conductive thread to the pads. What makes the Gemma M0 particularly interesting is that it runs CircuitPython, Adafruit's modified version of the Python language designed for embedded devices. (At this point, I should note that Limor Fried, the founder of Adafruit, is a member of IEEE Spectrum's editorial advisory board, but she played no role in the origination of this article.<br/> &copy; 2017 IEEE.},
key={Wearable computers},
keywords={Controllers;High level languages;Microcontrollers;},
note={Advisory boards;Conductive threads;Conventional controllers;Embedded device;Input and outputs;On the flies;PYTHON language;Through hole;},
URL={http://dx.doi.org/10.1109/MSPEC.2017.8048828},
}


@inproceedings{20162702569517,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={ACM International Conference Proceeding Series},
journal={ACM International Conference Proceeding Series},
volume={18-20-February-2016},
year={2016},
pages={BITS Pilani Goa; Infosys; Persistent; Siemens; TCS - },
address={Goa, India},
abstract={The proceedings contain 26 papers. The topics discussed include: cognitive and contextual enterprise mobile computing; cognitive and contextual enterprise mobile computing; automatically detecting the up-to-date status of to-do comments in Java programs; energy and performance prediction of CUDA applications using dynamic regression models; SymTest : a framework for symbolic testing of embedded software; process edification for traceability in evolving architectures; architecting an extensible framework for gamifying software engineering concepts; divergence aware automated partitioning of OpenCL workloads; JDQL: a framework for java static analysis; explicating the relationships among subsystems; system maps: integrating knowledge system models; LogOpt: static feature extraction from source code for automated catch block logging prediction; lean transformation: adapting to the change, factors for success and lessons learnt during the journey; formal verification of avionics self adaptive software: a case study; learning's from developing a domain specific engineering environment for control systems; an approach for collaborative quality assessment of software; and integrating values into mobile software engineering.},
}


@article{20174604394625,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={ChangeLocator: locate crash-inducing changes based on crash reports},
journal={Empirical Software Engineering},
author={Wu, Rongxin and Wen, Ming and Cheung, Shing-Chi and Zhang, Hongyu},
volume={23},
number={5},
year={2018},
pages={2866 - 2900},
issn={13823256},
abstract={Software crashes are severe manifestations of software bugs. Debugging crashing bugs is tedious and time-consuming. Understanding software changes that induce a crashing bug can provide useful contextual information for bug fixing and is highly demanded by developers. Locating the bug inducing changes is also useful for automatic program repair, since it narrows down the root causes and reduces the search space of bug fix location. However, currently there are no systematic studies on locating the software changes to a source code repository that induce a crashing bug reflected by a bucket of crash reports. To tackle this problem, we first conducted an empirical study on characterizing the bug inducing changes for crashing bugs (denoted as crash-inducing changes). We also propose ChangeLocator, a method to automatically locate crash-inducing changes for a given bucket of crash reports. We base our approach on a learning model that uses features originated from our empirical study and train the model using the data from the historical fixed crashes. We evaluated ChangeLocator with six release versions of Netbeans project. The results show that it can locate the crash-inducing changes for 44.7%, 68.5%, and 74.5% of the bugs by examining only top 1, 5 and 10 changes in the recommended list, respectively. It significantly outperforms the existing state-of-the-art approach.<br/> &copy; 2017, Springer Science+Business Media, LLC.},
key={Program debugging},
keywords={Location;},
note={Automatic programs;Bug localizations;Contextual information;Crash stack;Crash-inducing change;Empirical studies;Source code repositories;State-of-the-art approach;},
URL={http://dx.doi.org/10.1007/s10664-017-9567-4},
}


@inproceedings{20174004237425,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An empirical comparison of the development history of CloudStack and Eucalyptus},
journal={ACM International Conference Proceeding Series},
author={Zerouali, Ahmed and Mens, Tom},
volume={Part F130526},
year={2017},
pages={116 - 121},
address={Rabat, Morocco},
abstract={Open source cloud computing solutions, such as CloudStack and Eucalyptus, have become increasingly popular in recent years. Despite this popularity, a better understanding of the factors influencing user adoption is still under active research. For example, increased project agility may lead to solutions that remain competitive in a rapidly evolving market, while keeping the software quality under control. Like any software system that is subject to frequent evolution, cloud computing solutions are subject to errors and quality problems, which may affect user experience and require frequent bug fixes. While prior comparisons of cloud platforms have focused most often on their provided services and functionalities, the current paper provides an empirical comparison of CloudStack and Eucalyptus, focusing on quality-related software development aspects.More specifically, we study the change history of the source code and its unit tests, as well as the history of bugs in the Jira issue tracker.We found that CloudStack has a high and more rapidly increasing test coverage than Eucalyptus. CloudStack contributors are more likely to participate in development and testing. We also observed differences between both projects pertaining to the bug life cycle and bug fixing time.<br/> &copy; 2017 Association for Computing Machinery.},
key={Open systems},
keywords={Cloud computing;Computer software selection and evaluation;Life cycle;Open source software;Platform as a Service (PaaS);Program debugging;Quality control;Software design;Software testing;},
note={Bug reports;Empirical analysis;Open sources;Software analysis;Unit testing;},
URL={http://dx.doi.org/10.1145/3128128.3128146},
}


@inbook{20172803910043,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An introduction to recommendation systems in software engineering},
journal={Recommendation Systems in Software Engineering},
author={Robillard, Martin P. and Walker, Robert J.},
year={2014},
pages={1 - 11},
abstract={Software engineering is a knowledge-intensive activity that presents many information navigation challenges. Information spaces in software engineering include the source code and change history of the software, discussion lists and forums, issue databases, component technologies and their learning resources, and the development environment. The technical nature, size, and dynamicity of these information spaces motivate the development of a special class of applications to support developers: recommendation systems in software engineering (RSSEs), which are software applications that provide information items estimated to be valuable for a software engineering task in a given context. In this introduction, we review the characteristics of information spaces in software engineering, describe the unique aspects of RSSEs, present an overview of the issues and considerations involved in creating, evaluating, and using RSSEs, and present a general outlook on the current state of research and development in the field of recommendation systems for highly technical domains.<br/> &copy; Springer-Verlag Berlin Heidelberg 2014.},
key={Application programs},
keywords={Engineering education;Recommender systems;},
note={Component technologies;Development environment;Information items;Information spaces;Knowledge intensives;Learning resource;Software applications;State of research;},
URL={http://dx.doi.org/10.1007/978-3-642-45135-5_1},
}


@inproceedings{20160501882720,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Supporting "what-if" in touch-screen web applications},
journal={PROMOTO 2015 - Proceedings of the 3rd International Workshop on Programming for Mobile and Touch},
author={Simonyi, Peter and Wilson, Jeff and Brown, Judith M. and Biddle, Robert},
year={2015},
pages={13 - 20},
address={Pittsburgh, PA, United states},
abstract={Surface computing encourages exploratory interaction, and many applications are designed to work this way. In essence, the fluid interaction causes the user to ask "What if?" We suggest this requires support for recording the history of such explorations and allowing reversion to earlier states. There are currently a variety of related mechanisms, but they are either underpowered for the sort of interaction history management we suggest is needed, or are restricted to very specific domains. We present a prototype implementation of an interaction history manager: Ra is a JavaScript library for supporting this exploration and version tracking in web applications. We illustrate the interface for end users seen in augmenting simple web applications; we describe the underlying technical architecture, which uses ES6 Proxy objects to maintain access to the application's model; and we present the API, which allows an existing application to include Ra with minimal code change.<br/> &copy; 2015 ACM.},
key={Application programming interfaces (API)},
keywords={Human computer interaction;Touch screens;},
note={Interaction history;Touch interfaces;Undo;Version control;WEB application;},
URL={http://dx.doi.org/10.1145/2824823.2824826},
}


@inproceedings{20180804826778,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The design and implementation of OpenMP 4.5 and OpenACC backends for the RAJA C++ performance portability layer},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Killian, William and Scogland, Tom and Kunen, Adam and Cavazos, John},
volume={10732 LNCS},
year={2018},
pages={63 - 82},
issn={03029743},
address={Denver, CO, United states},
abstract={Portability abstraction layers such as RAJA enable users to quickly change how a loop nest is executed with minimal modifications to high-level source code. Directive-based programming models such as OpenMP and OpenACC provide easy-to-use annotations on for-loops and regions which change the execution pattern of user code. Directive-based language backends for RAJA have previously been limited to few options due to multiplicative clauses creating version explosion. In this work, we introduce an updated implementation of two directive-based backends which helps mitigate the aforementioned version explosion problem by leveraging the C++ type system and template meta-programming concepts. We implement partial OpenMP 4.5 and OpenACC backends for the RAJA portability layer which can apply loop transformations and specify how loops should be executed. We evaluate our approach by analyzing compilation and runtime overhead for both backends using PGI 17.7 and IBM clang (OpenMP 4.5) on a collection of computation kernels.<br/> &copy; Springer International Publishing AG, part of Springer Nature 2018.},
key={C++ (programming language)},
keywords={Abstracting;Application programming interfaces (API);Codes (symbols);},
note={Abstraction layer;Code Generation;Design and implementations;Explosion problems;Loop transformation;Performance portability;Programming models;Template meta programming;},
URL={http://dx.doi.org/10.1007/978-3-319-74896-2_4},
}


@inproceedings{20141617597892,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mining frequent bug-fix code changes},
journal={2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014 - Proceedings},
author={Osman, Haidar and Lungu, Mircea and Nierstrasz, Oscar},
year={2014},
pages={343 - 347},
address={Antwerp, Belgium},
abstract={Detecting bugs as early as possible plays an important role in ensuring software quality before shipping. We argue that mining previous bug fixes can produce good knowledge about why bugs happen and how they are fixed. In this paper, we mine the change history of 717 open source projects to extract bug-fix patterns. We also manually inspect many of the bugs we found to get insights into the contexts and reasons behind those bugs. For instance, we found out that missing null checks and missing initializations are very recurrent and we believe that they can be automatically detected and fixed. &copy; 2014 IEEE.<br/>},
key={Program debugging},
keywords={Computer software maintenance;Computer software selection and evaluation;Open source software;Reengineering;Reverse engineering;},
note={Bug fixes;Change history;Code changes;Detecting bugs;Open source projects;Software Quality;},
URL={http://dx.doi.org/10.1109/CSMR-WCRE.2014.6747191},
}


@article{20162302473886,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Thermophysical properties of phase change emulsions prepared by D-phase emulsification},
journal={Energy Conversion and Management},
author={Morimoto, Takashi and Togashi, Kenichi and Kumano, Hiroyuki and Hong, Hiki},
volume={122},
year={2016},
pages={215 - 222},
issn={01968904},
abstract={A phase change emulsion (PCE) is a mixture of fine particles of a phase change material (PCM) and an aqueous surfactant solution. PCEs are attracting attention as thermal media. They provide high-density thermal energy storage by utilizing the latent heat of the PCM, and high transportability because of their high fluidity. In this study, PCEs were prepared by D-phase emulsification, and their properties were evaluated. Two paraffin PCMs, n-hexadecane and n-octadecane, were dispersed inside the PCEs, and their mass fraction was varied. The PCEs exhibited similar particle size distributions, regardless of the type of dispersed PCM or its mass fraction. The viscosity of the PCE increased with increasing PCM mass fraction, in agreement with theoretical values. The latent heat of fusion and specific heat of the PCEs were evaluated using a temperature history method. Pump consumption rates were calculated from these results, and are compared with that of water.<br/> &copy; 2016 Elsevier Ltd.},
key={Emulsification},
keywords={Heat storage;Latent heat;Paraffins;Particle size;Phase change materials;Pulse code modulation;Specific heat;Viscosity;},
note={Aqueous surfactant solutions;Fine particles;Latent heat of fusion;Mass fraction;Phase Change;Pump consumption;Temperature history;Theoretical values;},
URL={http://dx.doi.org/10.1016/j.enconman.2016.05.065},
}


@article{20132816478279,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Clone evolution: A systematic review},
journal={Journal of software: Evolution and Process},
author={Pate, Jeremy R. and Tairas, Robert and Kraft, Nicholas A.},
volume={25},
number={3},
year={2013},
pages={261 - 283},
issn={20477481},
abstract={Detection of code clones - similar or identical source code fragments - is of concern both to researchers and to practitioners. An analysis of the clone detection results for a single source code version provides a developer with information about a discrete state in the evolution of the software system. However, tracing clones across multiple source code versions permits a clone analysis to consider a temporal dimension. Such an analysis of clone evolution can be used to uncover the patterns and characteristics exhibited by clones as they evolve within a system. Developers can use the results of this analysis to understand the clones more completely, which may help them to manage the clones more effectively. Thus, studies of clone evolution serve a key role in understanding and addressing issues of cloning in software. In this paper, we present a systematic review of the literature on clone evolution. In particular, we present a detailed analysis of 30 relevant papers that we identified in accordance with our review protocol. The review results were organized to address three research questions. Through our answers to these questions, we present the methods that researchers have used to study clone evolution, the patterns that researchers have found evolving clones to exhibit, and the evidence that researchers have established regarding the extent of inconsistent change undergone by clones during software evolution. Overall, the review results indicate that whereas researchers have conducted several empirical studies of clone evolution, there are contradictions among the reported findings, particularly regarding the lifetimes of clone lineages and the consistency with which clones are changed during software evolution. We identify human-based empirical studies and classification of clone evolution patterns as two areas that are in particular need of further work. Copyright &copy; 2011 John Wiley &amp;Sons, Ltd.<br/>},
key={Cloning},
keywords={Codes (symbols);Computer programming languages;},
note={Code clone;Empirical studies;Evolution patterns;Research questions;Software Evolution;Systematic literature review;Systematic Review;Temporal dimensions;},
URL={http://dx.doi.org/10.1002/smr.579},
}


@inproceedings{20164502984614,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={How do centralized and distributed version control systems impact software changes?},
journal={Proceedings - International Conference on Software Engineering},
author={Brindescu, Caius and Codoban, Mihai and Shmarkatiuk, Sergii and Dig, Danny},
volume={0},
number={1},
year={2014},
pages={322 - 333},
issn={02705257},
address={Hyderabad, India},
abstract={Distributed Version Control Systems (DVCS) have seen an increase in popularity relative to traditional Centralized Version Control Systems (CVCS). Yet we know little on whether developers are benefitting from the extra power of DVCS. Without such knowledge, researchers, developers, tool builders, and team managers are in the danger of making wrong assumptions. In this paper we present the first in-depth, large scale empirical study that looks at the influence of DVCS on the practice of splitting, grouping, and committing changes. We recruited 820 participants for a survey that sheds light into the practice of using DVCS. We also analyzed 409M lines of code changed by 358300 commits, made by 5890 developers, in 132 repositories containing a total of 73M LOC. Using this data, we uncovered some interesting facts. For example, (i) commits made in distributed repositories were 32% smaller than the centralized ones, (ii) developers split commits more often in DVCS, and (iii) DVCS commits are more likely to have references to issue tracking labels.<br/> &copy; 2014 ACM.},
key={Information management},
keywords={Control systems;Human resource management;Software engineering;Surveys;},
note={Distributed repositories;Distributed version control systems;Distributed version controls;Empirical studies;Issue Tracking;Software change;Version control;Version control system;},
URL={http://dx.doi.org/10.1145/2568225.2568322},
}


@article{20153001066812,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Identifying and understanding header file hotspots in C/C++ build processes},
journal={Automated Software Engineering},
author={McIntosh, Shane and Adams, Bram and Nagappan, Meiyappan and Hassan, Ahmed E.},
volume={23},
number={4},
year={2016},
pages={619 - 647},
issn={09288910},
abstract={Software developers rely on a fast build system to incrementally compile their source code changes and produce modified deliverables for testing and deployment. Header files, which tend to trigger slow rebuild processes, are most problematic if they also change frequently during the development process, and hence, need to be rebuilt often. In this paper, we propose an approach that analyzes the build dependency graph (i.e., the data structure used to determine the minimal list of commands that must be executed when a source code file is modified), and the change history of a software system to pinpoint header file hotspots&mdash;header files that change frequently and trigger long rebuild processes. Through a case study on the GLib, PostgreSQL, Qt, and Ruby systems, we show that our approach identifies header file hotspots that, if improved, will provide greater improvement to the total future build cost of a system than just focusing on the files that trigger the slowest rebuild processes, change the most frequently, or are used the most throughout the codebase. Furthermore, regression models built using architectural and code properties of source files can explain 32&ndash;57 % of these hotspots, identifying subsystems that are particularly hotspot-prone and would benefit the most from architectural refinement.<br/> &copy; 2015, Springer Science+Business Media New York.},
key={C++ (programming language)},
keywords={Codes (symbols);Regression analysis;Software testing;},
note={Build systems;Dependency graphs;Development process;Mining software repositories;Performance analysis;Software developer;Software systems;Source code changes;},
URL={http://dx.doi.org/10.1007/s10515-015-0183-5},
}


@inproceedings{20171103437797,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={In-Situ visualisation of fractional code ownership over time},
journal={ACM International Conference Proceeding Series},
author={Moller, Christoph and Reina, Guido and Ertl, Thomas},
year={2015},
pages={13 - 20},
address={Tokyo, Japan},
abstract={The term code ownership is used in software engineering to describe who authored a certain piece of software. Code ownership is commonly determined by investigating the data from version control systems, defining the owner as the person who contributed the most lines to a file, module, etc. Existing visualisation for ownership usually relies on perline annotations from the version control system, thus only conveying the information who changed a line the last time, potentially adding some visual cue about how old the respective change is. This, however, can be misleading, because any change of even a single character changes ownership. In this paper, we propose a visualisation that accounts for fractional ownership changes over time on a per-character basis. Our technique incorporates visual cues to convey that typical definitions of ownership have an inherent uncertainty and provides details on the cause of this uncertainty on demand. For our definition of code ownership being a lowlevel one, we propose implementing the visualisation as an in-situ visualisation in the code editor of modern development environments. We will show examples of the efficacy of our approach and discuss its advantages and disadvantages compared to conventional line-based ownership.<br/> &copy; 2015 ACM.},
key={Codes (symbols)},
keywords={Control systems;Information management;Software engineering;Visual communication;Visualization;},
note={Code editors;Code Ownership;Conventional line;Fractional ownerships;Modern development;Software visualisation;Uncertainty on demands;Version control system;},
URL={http://dx.doi.org/10.1145/2801040.2801055},
}


@inproceedings{20174504363114,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Vectorization-aware loop optimization with user-defined code transformations},
journal={Proceedings - IEEE International Conference on Cluster Computing, ICCC},
author={Takizawa, Hiroyuki and Reimann, Thorsten and Komatsu, Kazuhiko and Soga, Takashi and Egawa, Ryusuke and Musa, Akihiro and Kobayashi, Hiroaki},
volume={2017-September},
year={2017},
pages={685 - 692},
issn={15525244},
address={Honolulu, HI, United states},
abstract={The cost of maintaining an application code would significantly increase if the application code is branched into multiple versions, each of which is optimized for a different architecture. In this work, default and vector versions of a realworld application code are refactored to be a single version, and the differences between the versions are expressed as userdefined code transformations. As a result, application developers can maintain only the single version, and transform it to its vector version just before the compilation. Although code optimizations for a vector processor are sometimes different from those for other processors, application developers can enjoy the performance of the vector processor without increasing the code complexity. Evaluation results demonstrate that vectorizationaware loop optimization for a vector processor can be expressed as user-defined code transformation rules, and thereby significantly improve the performance of a vector processor without major code modifications.<br/> &copy; 2017 IEEE.},
key={Codes (symbols)},
keywords={Array processing;Cluster computing;Computer architecture;Cosine transforms;Vectors;},
note={Application developers;Code modifications;Code transformation;Different architectures;Evaluation results;Loop optimizations;User-defined code transformaiton;Xevolver;},
URL={http://dx.doi.org/10.1109/CLUSTER.2017.102},
}


@inproceedings{20180204642264,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Wikipedia Vandal Early Detection: From User Behavior to User Embedding},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Yuan, Shuhan and Zheng, Panpan and Wu, Xintao and Xiang, Yang},
volume={10534 LNAI},
year={2017},
pages={832 - 846},
issn={03029743},
address={Skopje, Macedonia},
abstract={Wikipedia is the largest online encyclopedia that allows anyone to edit articles. In this paper, we propose the use of deep learning to detect vandals based on their edit history. In particular, we develop a multi-source long-short term memory network (M-LSTM) to model user behaviors by using a variety of user edit aspects as inputs, including the history of edit reversion information, edit page titles and categories. With M-LSTM, we can encode each user into a low dimensional real vector, called user embedding. Meanwhile, as a sequential model, M-LSTM updates the user embedding each time after the user commits a new edit. Thus, we can predict whether a user is benign or vandal dynamically based on the up-to-date user embedding. Furthermore, those user embeddings are crucial to discover collaborative vandals. Code and data related to this chapter are available at: https://bitbucket.org/bookcold/vandal_detection.<br/> &copy; 2017, Springer International Publishing AG.},
key={Behavioral research},
keywords={Deep learning;Long short-term memory;},
note={Embeddings;Low dimensional;Multi-Sources;Online encyclopedia;Real vector;Sequential model;Short term memory;User behaviors;},
URL={http://dx.doi.org/10.1007/978-3-319-71249-9_50},
}


@inproceedings{20151800813961,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Untangling fine-grained code changes},
journal={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
author={Dias, Martin and Bacchelli, Alberto and Gousios, Georgios and Cassou, Damien and Ducasse, Stephane},
year={2015},
pages={341 - 350},
address={Montreal, QC, Canada},
abstract={After working for some time, developers commit their code changes to a version control system. When doing so, they often bundle unrelated changes (e.g., bug fix and refactoring) in a single commit, thus creating a so-called tangled commit. Sharing tangled commits is problematic because it makes review, reversion, and integration of these commits harder and historical analyses of the project less reliable. Researchers have worked at untangling existing commits, i.e., finding which part of a commit relates to which task. In this paper, we contribute to this line of work in two ways: (1) A publicly available dataset of untangled code changes, created with the help of two developers who accurately split their code changes into self contained tasks over a period of four months; (2) a novel approach, EpiceaUntangler, to help developers share untangled commits (aka. atomic commits) by using fine-grained code change information. EpiceaUntangler is based and tested on the publicly available dataset, and further evaluated by deploying it to 7 developers, who used it for 2 weeks. We recorded a median success rate of 91% and average one of 75%, in automatically creating clusters of untangled fine-grained code changes.<br/> &copy; 2015 IEEE.},
key={Codes (symbols)},
note={Bug fixes;Code changes;Fine grained;Historical analysis;Refactorings;Two ways;Version control system;},
URL={http://dx.doi.org/10.1109/SANER.2015.7081844},
}


@article{20151700779429,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={CLOCK-DWF: A write-history-aware page replacement algorithm for hybrid PCM and DRAM memory architectures},
journal={IEEE Transactions on Computers},
author={Lee, Soyoon and Bahn, Hyokyung and Noh, Sam H.},
volume={63},
number={9},
year={2014},
pages={2187 - 2200},
issn={00189340},
abstract={Phase change memory (PCM) has emerged as one of the most promising technologies to incorporate into the memory hierarchy of future computer systems. However, PCM has two critical weaknesses to substitute DRAM memory in its entirety. First, the number of write operations allowed to each PCM cell is limited. Second, write access time of PCM is about 6-10 times slower than that of DRAM. To cope with this situation, hybrid memory architectures that use a small amount of DRAM together with PCM have been suggested. In this paper, we present a new memory management technique for hybrid PCM and DRAM memory architecture that efficiently hides the slow write performance of PCM. Specifically, we aim to estimate future write references accurately and then absorb frequent memory writes into DRAM. To do this, we analyze the characteristics of memory write references and find two noticeable phenomena. First, using write history alone performs better than using both read and write history in estimating future write references. Second, the frequency characteristic is a better estimator than temporal locality in predicting future memory writes. Based on these two observations, we present a new page replacement algorithm called CLOCK-DWF (CLOCK with Dirty bits and Write Frequency) that significantly reduces the number of write operations that occur on PCM and also increases the lifespan of PCM memory.<br/> &copy; 2013 IEEE.},
key={Memory architecture},
keywords={Clocks;Dynamic random access storage;Estimation;Frequency estimation;Phase change memory;Pulse code modulation;},
note={Frequency characteristic;Hybrid memory;Hybrid memory architectures;Memory management techniques;Page replacement;Page replacement algorithms;Phase change memory (pcm);Write references;},
URL={http://dx.doi.org/10.1109/TC.2013.98},
}


@inproceedings{20173104013079,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An Empirical Analysis of the Docker Container Ecosystem on GitHub},
journal={IEEE International Working Conference on Mining Software Repositories},
author={Cito, Jurgen and Schermann, Gerald and Wittern, John Erik and Leitner, Philipp and Zumberi, Sali and Gall, Harald C.},
volume={0},
year={2017},
pages={323 - 333},
issn={21601852},
address={Buenos Aires, Argentina},
abstract={Docker allows packaging an application with its dependencies into a standardized, self-contained unit (a so-called container), which can be used for software development and to run the application on any system. Dockerfiles are declarative definitions of an environment that aim to enable reproducible builds of the container. They can often be found in source code repositories and enable the hosted software to come to life in its execution environment. We conduct an exploratory empirical study with the goal of characterizing the Docker ecosystem, prevalent quality issues, and the evolution of Dockerfiles. We base our study on a data set of over 70000 Dockerfiles, and contrast this general population with samplings that contain the Top-100 and Top-1000 most popular Docker-using projects. We find that most quality issues (28.6%) arise from missing version pinning (i.e., specifying a concrete version for dependencies). Further, we were not able to build 34% of Dockerfiles from a representative sample of 560 projects. Integrating quality checks, e.g., to issue version pinning warnings, into the container build process could result into more reproducible builds. The most popular projects change more often than the rest of the Docker population, with 5.81 revisions per year and 5 lines of code changed on average. Most changes deal with dependencies, that are currently stored in a rather unstructured manner. We propose to introduce an abstraction that, for instance, could deal with the intricacies of different package managers and could improve migration to more light-weight images.<br/> &copy; 2017 IEEE.},
key={Application programs},
keywords={Containers;Ecosystems;Image enhancement;Population statistics;Software design;},
note={Docker;Empirical analysis;Empirical Software Engineering;Execution environments;General population;GitHub;Representative sample;Source code repositories;},
URL={http://dx.doi.org/10.1109/MSR.2017.67},
}


@inproceedings{20134416923413,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation},
journal={Proceedings - International Conference on Software Engineering},
author={Balachandran, Vipin},
year={2013},
pages={931 - 940},
issn={02705257},
address={San Francisco, CA, United states},
abstract={Peer code review is a cost-effective software defect detection technique. Tool assisted code review is a form of peer code review, which can improve both quality and quantity of reviews. However, there is a significant amount of human effort involved even in tool based code reviews. Using static analysis tools, it is possible to reduce the human effort by automating the checks for coding standard violations and common defect patterns. Towards this goal, we propose a tool called Review Bot for the integration of automatic static analysis with the code review process. Review Bot uses output of multiple static analysis tools to publish reviews automatically. Through a user study, we show that integrating static analysis tools with code review process can improve the quality of code review. The developer feedback for a subset of comments from automatic reviews shows that the developers agree to fix 93% of all the automatically generated comments. There is only 14.71% of all the accepted comments which need improvements in terms of priority, comment message, etc. Another problem with tool assisted code review is the assignment of appropriate reviewers. Review Bot solves this problem by generating reviewer recommendations based on change history of source code lines. Our experimental results show that the recommendation accuracy is in the range of 60%-92%, which is significantly better than a comparable method based on file change history. &copy; 2013 IEEE.<br/>},
key={Static analysis},
keywords={Codes (symbols);Cost effectiveness;Defects;Quality control;Software engineering;},
note={Automatic static analysis;Automatically generated;Coding standards;Defect patterns;Peer code review;Recommendation accuracy;Software defects;Source code lines;},
URL={http://dx.doi.org/10.1109/ICSE.2013.6606642},
}


@inproceedings{20182605364196,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Co-evolution analysis of production and test code by learning association rules of changes},
journal={2018 IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, MaLTeSQuE 2018 - Proceedings},
author={Vidacs, Laszlo and Pinzger, Martin},
year={2018},
pages={31 - 36},
address={Campobasso, Italy},
abstract={Many modern software systems come with automated tests. While these tests help to maintain code quality by providing early feedback after modifications, they also need to be maintained. In this paper, we replicate a recent pattern mining experiment to find patterns on how production and test code co-evolve over time. Understanding co-evolution patterns may directly affect the quality of tests and thus the quality of the whole system. The analysis takes into account fine grained changes in both types of code. Since the full list of fine grained changes cannot be perceived, association rules are learned from the history to extract co-change patterns. We analyzed the occurrence of 6 patterns throughout almost 2500 versions of a Java system and found that patterns are present, but supported by weaker links than in previously reported. Hence we experimented with weighting methods and investigated the composition of commits.<br/> &copy; 2018 IEEE.},
key={Quality control},
keywords={Artificial intelligence;Association rules;Codes (symbols);Computer software selection and evaluation;Learning algorithms;Learning systems;Software testing;Testing;},
note={Automated test;Change analysis;Change patterns;Co-evolution;Fine-grained changes;Software Evolution;Software systems;Weighting methods;},
URL={http://dx.doi.org/10.1109/MALTESQUE.2018.8368456},
}


@article{20154301431276,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Co-changing code volume prediction through association rule mining and linear regression model},
journal={Expert Systems with Applications},
author={Lee, Shin-Jie and Lo, Li Hsiang and Chen, Yu-Cheng and Shen, Shi-Min},
volume={45},
year={2016},
pages={185 - 194},
issn={09574174},
abstract={Code smells are symptoms in the source code that indicate possible deeper problems and may serve as drivers for code refactoring. Although effort has been made on identifying divergent changes and shotgun surgeries, little emphasis has been put on predicting the volume of co-changing code that appears in the code smells. More specifically, when a software developer intends to perform a particular modification task on a method, a predicted volume of code that will potentially be co-changed with the method could be considered as significant information for estimating the modification effort. In this paper, we propose an approach to predicting volume of co-changing code affected by a method to be modified. The approach has the following key features: co-changing methods can be identified for detecting divergent changes and shotgun surgeries based on association rules mined from change histories; and volume of co-changing code affected by a method to be modified can be predicted through a derived fitted regression line with t-test based on the co-changing methods identification results. The experimental results show that the success rate of co-changing methods identification is 82% with a suggested threshold, and the numbers of correct identifications would not be influenced by the increasing number of commits as a project continuously evolves. Additionally, the mean absolute error of co-changing code volume predictions is 133 lines of code which is 95.3% less than the one of a naive approach.<br/> &copy; 2015 Elsevier Ltd. All rights reserved.},
key={Codes (symbols)},
keywords={Association rules;Forecasting;Linear regression;Odors;Surgery;},
note={Change history;Code re-factoring;Linear regression models;Mean absolute error;Modification effort;Regression lines;Software developer;Volume predictions;},
URL={http://dx.doi.org/10.1016/j.eswa.2015.09.023},
}


@article{20152100868501,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mining version histories for detecting code smells},
journal={IEEE Transactions on Software Engineering},
author={Palomba, Fabio and Bavota, Gabriele and Di Penta, Massimiliano and Oliveto, Rocco and Poshyvanyk, Denys and De Lucia, Andrea},
volume={41},
number={5},
year={2015},
pages={462 - 489},
issn={00985589},
abstract={Code smells are symptoms of poor design and implementation choices that may hinder code comprehension, and possibly increase change- and fault-proneness. While most of the detection techniques just rely on structural information, many code smells are intrinsically characterized by how code elements change over time. In this paper, we propose H istorical Information for Smell deTection (HIST), an approach exploiting change history information to detect instances of five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy. We evaluate HIST in two empirical studies. The first, conducted on 20 open source projects, aimed at assessing the accuracy of HIST in detecting instances of the code smells mentioned above. The results indicate that the precision of HIST ranges between 72 and 86 percent, and its recall ranges between 58 and 100 percent. Also, results of the first study indicate that HIST is able to identify code smells that cannot be identified by competitive approaches solely based on code analysis of a single system's snapshot. Then, we conducted a second study aimed at investigating to what extent the code smells detected by HIST (and by competitive code analysis techniques) reflect developers' perception of poor design and implementation choices. We involved 12 developers of four open source projects that recognized more than 75 percent of the code smell instances identified by HIST as actual design/implementation problems.<br/> &copy; 2014 IEEE.},
key={Open systems},
keywords={Codes (symbols);Odors;Open source software;},
note={Code comprehension;Code smell;Design and implementations;Detection techniques;Empirical studies;Mining software repositories;Open source projects;Structural information;},
URL={http://dx.doi.org/10.1109/TSE.2014.2372760},
}


@inproceedings{20180704794563,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={How Preprocessor Annotations (Do Not) Affect Maintainability: A Case Study on Change-Proneness},
journal={GPCE 2017 - Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences, co-located with SPLASH 2017},
author={Fenske, Wolfram and Schulze, Sandro and Saake, Gunter},
year={2018},
pages={77 - 90},
address={Vancouver, BC, Canada},
abstract={Preprocessor annotations (e. g., #ifdef in C) enable the development of similar, but distinct software variants from a common code base. One particularly popular preprocessor is the C preprocessor, cpp. But the cpp is also widely criticized for impeding software maintenance by making code hard to understand and change. Yet, evidence to support this criticism is scarce. In this paper, we investigate the relation between cpp usage and maintenance effort, which we approximate with the frequency and extent of source code changes. To this end, we mined the version control repositories of eight open-source systems written in C. For each system, we measured if and how individual functions use cpp annotations and how they were changed. We found that functions containing cpp annotations are generally changed more frequently and more profoundly than other functions. However, when accounting for function size, the differences disappear or are greatly diminished. In summary, with respect to the frequency and extent of changes, our findings do not support the criticism of the cpp regarding maintainability.<br/> &copy; 2017 Association for Computing Machinery.},
key={C (programming language)},
keywords={Codes (symbols);Maintainability;Maintenance;Open source software;Open systems;Program processors;},
note={Annotations;Change proneness;Maintenance efforts;Open source system;Software variants;Source code changes;Variability;Version control;},
URL={http://dx.doi.org/10.1145/3136040.3136059},
}


@inproceedings{20185106252824,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={How do multiple pull requests change the same code: A study of competing pull requests in github},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
author={Zhang, Xin and Chen, Yang and Gu, Yongfeng and Zou, Weiqin and Xie, Xiaoyuan and Jia, Xiangyang and Xuan, Jifeng},
year={2018},
pages={228 - 239},
address={Madrid, Spain},
abstract={GitHub is a widely used collaborative platform for global software development. A pull request plays an important role in bridging code changes with version controlling. Developers can freely and parallelly submit pull requests to base branches and wait for the merge of their contributions. However, several developers may submit pull requests to edit the same lines of code; such pull requests result in a latent collaborative conflict. We refer such pull requests that tend to change the same lines and remain open during an overlapping time period to as competing pull requests. In this paper, we conduct a study on 9,476 competing pull requests from 60 Java repositories in GitHub. The data are collected by mining pull requests that are submitted in 2017 from top Java projects with the most forks. We explore how multiple pull requests change the same code via answering four research questions, including the distribution of competing pull requests, the involved developers, the changed lines of code, and the impact on pull request integration. Our study shows that there indeed exist competing pull requests in GitHub: in 45 out of 60 repositories, over 31% of pull requests belong to competing pull requests; 20 repositories have more than 100 groups of competing pull requests, each of which is submitted by over five developers; 42 repositories have over 10% of competing pull requests with over 10 same lines of code. Meanwhile, we observe that attributes of competing pull requests do not have strong impacts on pull request integration, comparing with other types of pull requests. Our study provides a preliminary analysis for further research that aims to detect and eliminate conflicts among competing pull requests.<br/> &copy; 2018 IEEE.},
key={Codes (symbols)},
keywords={Computer software maintenance;Java programming language;Software design;},
note={Collaborative development;Collaborative platform;GitHub;Global software development;Lines of code;Preliminary analysis;Pull requests;Research questions;},
URL={http://dx.doi.org/10.1109/ICSME.2018.00032},
}


@inproceedings{20170303264852,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A supervised learning model for identifying inactive VMs in private cloud data centers},
journal={Proceedings of the Industry Track of the 17th ACM/IFIP/USENIX Middleware Conference, Middleware Industry 2016},
author={Kim, In Kee and Zeng, Sai and Young, Christopher and Hwang, Jinho and Humphrey, Marty},
year={2016},
pages={Assoc. for Computing Machinery (ACM); IFIP; The Advanced Computing System Association (USENIX) - },
address={Trento, Italy},
abstract={A private cloud has become an essential computing infrastructure for many enterprises. However, according to a recent study, 30% of VMs in data centers are not being used for any productive work. These "inactive" (or "zombie") VMs can arise from faulty VM management code within the cloud infrastructure but are usually the result of human neglect. Inactive VMs can hurt the performance of productive VMs, can distort internal cost management, and in the extreme can result in the cloud infrastructure being unable to allocate resources for new VMs. Correctly assessing the productivity of a VM can be challenging: e.g., is a VM that has low CPU utilization being used to slowly edit source code or is it an inactive VM that happens to be performing routine maintenance (e.g., virus-scan and software updates)? To address this problem, we develop a supervised learning model that leverages primitive information (e.g., running process, login history, network connections) of VMs periodically collected by a lightweight data collection framework. This model employs a linear support vector machine (SVM) approach that reflects single VM behavior as well as coordinated VM behaviors. We evaluated the identification accuracy of this model with a real-world dataset within IBM of more than 750 VMs. Results show that our model has a 20% higher accuracy (90%) than state-of-the-art approaches. An accurate model is an important first step to enable private cloud infrastructures to achieve better resource management through such actions as suspending or dynamically downsizing inactive VMs.<br/> &copy; 2016 ACM.},
key={Supervised learning},
keywords={Cloud computing;Middleware;Support vector machines;Viruses;},
note={Cloud infrastructures;Computing infrastructures;Data center management;Garbage collection;Identification accuracy;Identifying inactive VMs;Linear Support Vector Machines;State-of-the-art approach;},
URL={http://dx.doi.org/10.1145/3007646.3007654},
}


@inproceedings{20160902002627,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Progress report on version aware LibreOffice},
journal={ACM International Conference Proceeding Series},
author={Pandey, Meenu and Munson, Ethan V. and Thao, Cheng},
volume={16-September-2014},
year={2014},
address={Fort Collins, CO, United states},
abstract={In an earlier paper at DocEng 2013, we reported on our efforts to make LibreOffice Writer documents be \version aware". Version aware documents use a namespace pro-tected preamble to include a complete version history within the saved document file, plus unique identifier attributes on the document content elements in order to support efficient differencing and merging of versions. A particular challenge in this effort has been to ensure that the unique identifiers on the elements would be preserved through a complete load-edit-save cycle. This is challenging because content element data passes through three representations in its lifetime. At load time, XML is read to create ImportContext objects, which are then used to generate internal data structures used during editing. At save time, the internal data structures are converted to ExportContext objects, from which XML is generated for the saved file. The internal data structures are drawn from a small forest of inheritance hierarchies, where each hierarchy has a slightly different construction-destruction protocol and thus each one requires a different solution to preserving the unique identifiers. Working with a particular snapshot of the C++ imple-mentation of LibreOffice Writer, we have reached a point where unique identifiers are preserved on nearly all content elements used in Writer documents. Unfortunately, there is no support for versioning of document style elements at this time. Support for version awareness has added about 3000 lines of code to a code base of slightly more than one million lines. The changes affect 128 files out of a total of 3354, organized in three large modules. We believe that these numbers show that adding full support for version aware-ness would have only a modest affect on the implementation of an office software suite. However, the fairly large number of files affected shows that version awareness resembles a non-functional requirement, since its support is not isolated in a small set of files.<br/> Copyright &copy; 2014 ACM.},
key={C++ (programming language)},
keywords={Data structures;Digital storage;Visualization;XML;},
note={Document contents;Inheritance hierarchies;Non-functional requirements;Office softwares;Progress report;Unique identifiers;User collaborations;Version aware;},
URL={http://dx.doi.org/10.1145/2723147.2723151},
}


@inproceedings{20152500950944,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Querying the history of software projects using QWALKEKO},
journal={Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014},
author={Stevens, Reinout and Roover, Coen De},
year={2014},
pages={585 - 588},
address={Victoria, BC, Canada},
abstract={We present the QwalKeko meta-programming library for Clojure that enables querying the history of versioned software projects in a declarative manner. Unique to this library is its support for regular path expressions within history queries. Regular path expressions are akin to regular expressions, except that they match a sequence of successive snapshots of a software project along which user-specified logic conditions must hold. Such logic conditions can concern the source code within a snapshot, versioning information associated with the snapshot, as well as patterns of source code changes with respect to other snapshots. We have successfully used the resulting multi-faceted queries to detect refactorings in project histories. In this paper, we discuss how applicative logic meta-programming enabled combining the heterogenous components of QwalKeko into a uniform whole. We focus on the applicative logic interface to a new implementation of a well-known change distilling algorithm. We use the problem of detecting and categorizing changes made to Selenium-based test scripts for illustration purposes.<br/> &copy; 2014 IEEE.},
key={Computer circuits},
keywords={Object oriented programming;},
note={Declarative Programming;Logic meta programming;Meta Programming;Mining software repositories;program querying;Regular expressions;Software project;Source code changes;},
URL={http://dx.doi.org/10.1109/ICSME.2014.101},
}


@inproceedings{20180204641283,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Examining studies on learning management systems in SSCI database: A content analysis study},
journal={Procedia Computer Science},
author={Soykan, Fatih and Simek, Burak},
volume={120},
year={2017},
pages={871 - 876},
issn={18770509},
address={Budapest, Hungary},
abstract={Distance education systems have become more prevalent over time and their use continue to increase. Learning Management Systems (LMS) is the main component for distance education to be carried out effectively and distantly. In this stuy, it is aimed to examine current studies on LMS published between 2010 and 2014 and provide an instructive source for researchers. The obtained data were analyzed and interpreted based on certain criteria. According to the results, LMS studies are mostly published in Computer &amp; Science Journal, however there is a decline in the number of studies in 2014. Results also showed that researchers mostly conducted studies with university students and they examined the systems either developed by themselves or by an institution. It was revealed that Moodle is mostly used as open source code systems and WebCT (Blackboard) is mostly used as commercial. It can be indicated that seeking for new LMS by researchers would change based on the requirements and technological advances in the further research.<br/> &copy; 2018 The Authors. Published by Elsevier B.V.},
key={Open systems},
keywords={Computation theory;Distance education;Open source software;Soft computing;},
note={Content analysis;Distance education systems;Learning management system;Open sources;Open-source code;Science journals;Technological advances;University students;},
URL={http://dx.doi.org/10.1016/j.procs.2017.11.320},
}


@article{20181605030676,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The TDHF code Sky3D version 1.1},
journal={Computer Physics Communications},
author={Schuetrumpf, B. and Reinhard, P.-G. and Stevenson, P.D. and Umar, A.S. and Maruhn, J.A.},
volume={229},
year={2018},
pages={211 - 213},
issn={00104655},
abstract={The nuclear mean-field model based on Skyrme forces or related density functionals has found widespread application to the description of nuclear ground states, collective vibrational excitations, and heavy-ion collisions. The code Sky3D solves the static or dynamic equations on a three-dimensional Cartesian mesh with isolated or periodic boundary conditions and no further symmetry assumptions. Pairing can be included in the BCS approximation for the static case. The code is implemented with a view to allow easy modifications for including additional physics or special analysis of the results. New version program summary: Program title: Sky3D Program Files doi: http://dx.doi.org/10.17632/vzbrzvyrn4.1 Licensing provisions: GPLv3 Programming language: Fortran 90. The OpenMP version requires a relatively recent compiler; it was found to work using gfortran 4.6.2 or later and the Intel compiler version 12 or later. Journal reference of previous version: J. A. Maruhn, P.-G. Reinhard, P. D. Stevenson, and A. S. Umar, &ldquo;The TDHF Code Sky3D&rdquo; Comp. Phys. Comm. 185, 2195 (2014). Does the new version supersede the previous version?: Yes. Nature of problem: The time-dependent Hartree&ndash;Fock equations can be used to simulate nuclear vibrations and collisions between nuclei for low energies. This code implements the equations based on a Skyrme energy functional and also allows the determination of the ground-state structure of nuclei through the static version of the equations. For the case of vibrations the principal aim is to calculate the excitation spectra by Fourier-analyzing the time dependence of suitable observables. In collisions, the formation of a neck between nuclei, the dissipation of energy from collective motion, processes like charge transfer and the approach to fusion are of principal interest. Solution method: The nucleonic wave function spinors are represented on a three-dimensional Cartesian mesh with no further symmetry restrictions. The boundary conditions are always periodic for the wave functions, while the Coulomb potential can also be calculated for an isolated charge distribution. All spatial derivatives are evaluated using the finite Fourier transform method. The code solves the static Hartree&ndash;Fock equations with a damped gradient iteration method and the time-dependent Hartree&ndash;Fock equations with an expansion of the time-development operator. Any number of initial nuclei can be placed into the mesh with arbitrary positions and initial velocities. Reasons for the new version: A few bugs were fixed and a number of enhancements added concerning faster convergence, better stability, and more sophisticated analysis of some results. Summary of revisions: The following is a brief summary. A more complete documentation can be found as update.pdf in the Documentation subdirectory. New documentation: It was decided to switch the documentation to using the Doxygen system (available from www.doxygen.org), which can generate the documentation in a variety of formats. To generate the documentation, go into the Doc-doxygen subdirectory and execute make html, make latex, or make all to produce the corresponding version or both of them. The documentation inserted into the source files accounts for most of the formal changes in them. The general documentation is also updated and present as &ldquo;Documentation.pdf&rdquo;. Bug fixes: 1. In the force database forces.data two digits were interchanged in the definition of SLy4d, leading to wrong results for that force.2. If a restart is done for a two-body collision, the code changed the number of fragments to nof =1. The restart is then initialized like a single-nucleus case with nof =1. But two-body analysis was activated only for nof =1 such that it was absent after restart.3. In the time-dependent mode, the wave functions were only save at intervals of mprint and mrest, respectively. If a calculation stops because of reaching the final distance or fulfilling the convergence criterion, this may lead to a loss of information, so that now both are done also in this event before the job finishes.4. The external field parameters were calculated directly from the input in getin_external. Since this is called before the fragment initialization is done, coefficients depending on proton or neutron number will not be calculated correctly. For this reason, the calculation of these coefficients is separated into a new routine init_external, which is called directly before the dynamic calculation starts.Array allocation: It turned out that having the working arrays as automatic variables could cause problems, as they are allocated on the stack and the proper stack size must be calculated. Therefore in all cases where a larger array is concerned, it is now changed to ALLOCATABLE and allocated and deallocated as necessary. Elimination of &ldquo;guru&rdquo; mode of FFTW3 While the guru mode as defined in the FFTW3 package (see fftw.org) offers an elegant formulation of complicated multidimensional transforms, it is not implemented in some support libraries like the Intel<sup>&reg;</sup>MKL. There is not much loss in speed when this is replaced by standard transforms with some explicit loops added where necessary. This affects the wave function transforms in the y and z direction. Enhancement of the makefile In the previous version there were several versions of the makefile, which had to be edited by hand to use different compilers. This was reformulated using a more flexible file with various targets predefined. Thus to generate the executable code, it is sufficient to execute &ldquo;make target&rdquo; in the Code subdirectory, where target is one of the following: &bull; seq : simple sequential version with the gfortran compiler.&bull; ifort, ifort_seq : sequential version using the Intel compiler.&bull; omp and ifort_omp produce the OpenMP version for the gfortran and Intel compiler, respectively.&bull; mpi : MPI version, which uses the compiler mpif90.&bull; mpi-omp : MPI version also using OpenMP on each node.&bull; debug, seq_debug, omp_debug, mpi_debug : enable debugging mode for these cases. The first three use the gfortran compiler.&bull; clean : removes the generated object and module files.&bull; clean-exec : same as clean but removes the executable files as well. The generated executable files are called sky3d.seq, sky3d.ifort.seq, sky3d.mpi, sky3d.omp, sky3d.ifort.omp, and sky3d.mpi-omp, which should be self-explanatory. Thus several versions may be kept in the code directory, but a make clean should be done before producing a new version to make sure the object and module files are correct. Skyrme-force compatibility for static restart: the code normally checks that the Skyrme forces for all the input wave functions agree. It may be useful, however, to initialize a static calculation from results for a different Skyrme force. Therefore the consistency check was eliminated for the static case. Acceleration of the static calculations: The basic parameters for the static iterations are (see Eq. 12 of the original paper) x<inf>0</inf>(variable x0dmp), which determines the size of the gradient step, and E<inf>0</inf>(variable e0dmp) for the energy damping. These were read in and never changed throughout the calculation, except possibly through a restart. This can cause slow convergence, so that a method was developed to change x0dmp during the iterations. The value from the input is now regarded as the minimum allowed one and saved in x0dmpmin. At the start of the iterations, however, x0dmp is multiplied by 3 to attempt a faster convergence. The change in x0dmp is then implemented by comparing the HF energy ehf and the fluctuations efluct1 and efluct2 to the previous values saved in the variables ehfprev, efluct1prev, and efluct2prev. If the energy decreases or one of the fluctuations decreases by a factor of less than 1&minus;10<sup>&minus;5</sup>, x0dmp is increased by a factor 1.005 to further speed up convergence. If none of these conditions holds, it is assumed that the step was too large and x0dmp is reduced by a factor 0.8, but is never allowed to fall below x0dmpmin. This whole process is turned on only if the input variable tvaryx_0 in the namelist &ldquo;static&rdquo; is.TRUE. The default value is.FALSE. A speedup up to a factor of 3 has been observed. External field expectation value This value, which is printed in the file external.res, was calculated from the spatial field including the (time-independent) amplitude amplq0. The temporal Fourier transform then becomes quadratic in the amplitude, as the fluctuations in the density also grow linearly in amplq0 (provided the perturbation is not strong enough to take it into the nonlinear regime). This may be confusing and we therefore divided the expectation value by this factor. Note that if the external field is composed of a mixture of different multipoles (not coded presently), an overall scale factor should instead be used. Enhanced two-body analysis: the analysis of the final two-body quantities after breakup included directly in the code was very simplified and actually it was superfluous to do this so frequently. This is replaced by a much more thorough analysis, including determination of the internal angular momenta of the fragments and of a quite accurate Coulomb energy. It is done only when the final separation is reached, while a simple determination of whether the fragments have separated and, if so, what their distance is, is performed every time step. Diagonalization In the original program the diagonalization of the Hamiltonian in the subroutine diagstep was carried out employing an eigenvalue decomposition using the LAPACK routine ZHBEVD which is optimized for banded matrices. This routine is replaced in the update by the routine ZHEEVD which is optimized for general hermitian matrices. This change should result in a moderate speed up for very large calculations. Furthermore the array unitary, previously a nstmax&times;nstmax array has been reduced to a nlin&times;nlin array, where nlin is the number of wave functions of either neutrons or protons. This array is now used as input and output for ZHEEVD. New formulation of the spin&ndash;orbit term: The action of the spin&ndash;orbit term has been corrected to comply with a strictly variational form. Starting from the spin&ndash;orbit energy E<inf>ls</inf>=t<inf>ls</inf>&int;d<sup>3</sup>rJ&rarr;&sdot;&nabla;&rho;,we obtain by variation with respect to the s.p. wavefunction &psi;<sup>&lowast;</sup>the spin&ndash;orbit term in the mean field in the symmetrized form h&circ;<inf>ls</inf>&psi;=[Formula presented]W&rarr;&sdot;(&sigma;&rarr;&times;&nabla;)&psi;+&sigma;&rarr;&sdot;(&nabla;&times;(W&rarr;&psi;))where W&rarr;=t<inf>ls</inf>&nabla;&rho;. In the previous version of the code, this term was simplified by applying the product rule for the &nabla; operator yielding [Formula presented]W&rarr;&sdot;(&sigma;&rarr;&times;&nabla;)&psi;+&sigma;&rarr;&sdot;(&nabla;&times;(W&rarr;&psi;))=iW&rarr;&sdot;(&sigma;&rarr;&times;&nabla;)&psi;.Closer inspection reveals that the product rule is not perfectly fulfilled if the &nabla; operator is evaluated with finite Fourier transformation as inevitably done in the grid representation of the code. It turned out that this slight mismatch can accumulate to instabilities in TDHF runs over long times. Thus the variationally correct form (2) has been implemented now, although it leads to slightly longer running times. Supplementary material: Extensive documentation and a number of utility programs to analyze the results and prepare them for graphics output using the Silo library (http://wci.llnl.gov/simulation/computer-codes/silo) for use in VisIT [1] or Paraview (https://www.paraview.org). The code can serve as a template for interfacing to other database or graphics systems. External routines/libraries: LAPACK, FFTW3. Restrictions: The reliability of the mean-field approximation is limited by the absence of hard nucleon&ndash;nucleon collisions. This limits the scope of applications to collision energies about a few MeV per nucleon above the Coulomb barrier and to relatively short interaction times. Similarly, some of the missing time-odd terms in the implementation of the Skyrme interaction may restrict the applications to even&ndash;even nuclei. Unusual features: The possibility of periodic boundary conditions and the highly flexible initialization make the code also suitable for astrophysical nuclear-matter applications. Acknowledgments This work was supported by DOE under contract numbers DE-SC0013847, DE-NA0002847, DE-SC0013365, and DE-SC0008511, by BMBF under contract number 05P15RDFN1, and by UK STFC under grant number ST/P005314/1. References [1] H. Childs, E. Brugger, B. Whitlock, J. Meredith, S. Ahern, D. Pugmire, K. Biagas, M. Miller, C. Harrison, G. H. Weber, H. Krishnan, T. Fogal, A. Sanderson, C. Garth, E. W. Bethel, D. Camp, O. R&uuml;bel, M. Durant, J. M. Favre, P. Navr&aacute;til, VisIt: An End-User Tool For Visualizing and Analyzing Very Large Data, in: High Performance Visualization&ndash;Enabling Extreme-Scale Scientific Insight, 2012, pp. 357&ndash;372.<br/> &copy; 2018 Elsevier B.V.},
key={Codes (symbols)},
keywords={Application programming interfaces (API);Boundary conditions;Charge transfer;Colliding beam accelerators;Data visualization;Density functional theory;Eigenvalues and eigenfunctions;Electric fields;Excited states;FORTRAN (programming language);Fourier transforms;Ground state;Heavy ions;Ion sources;Iterative methods;Matrix algebra;Mean field theory;Mesh generation;Multiprocessing systems;Problem oriented languages;Program compilers;Subroutines;Utility programs;Vibration analysis;Wave functions;},
note={Eigenvalue decomposition;Energy functionals;Finite Fourier transform;Giant resonances;Heavy ion collision;Mean field approximation;Performance visualization;Periodic boundary conditions;},
URL={http://dx.doi.org/10.1016/j.cpc.2018.03.012},
}


@inproceedings{20162502517019,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Bug localization based on code change histories and bug reports},
journal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
author={Youm, Klaus Changsun and Ahn, June and Kim, Jeongho and Lee, Eunseok},
volume={2016-May},
year={2016},
pages={190 - 197},
issn={15301362},
address={New Delhi, India},
abstract={A bug report is mainly used to find a fault location in software maintenance. It contains several fields such as summary, description, status and version. The description field includes detail scenario and stack traces if exceptional messages are presented. Recently researchers have proposed several approaches for automatic bug localization by using information retrieval and data mining. We propose BLIA, a statically integrated analysis approach of IR-based bug localization by utilizing texts and stack traces in bug reports, structured information of source files, and source code change histories. We performed experiments on three open source projects, namely AspectJ, SWT and ZXing. Compared with prior tools, our experiment results showed that BLIA outperforms the existing tools in terms of mean average precision. Our approach on average improved the metric of BugLocator, BLUiR, BRTracer and AmaLgam by 34%, 23%, 17% and 8%, respectively.<br/> &copy; 2015 IEEE.},
key={Data mining},
keywords={Codes (symbols);Information retrieval;Open source software;},
note={Bug localizations;Bug reports;Code changes;Fault localization;Stack traces;},
URL={http://dx.doi.org/10.1109/APSEC.2015.23},
}


@inproceedings{20163002625565,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Source transformation of C++ Codes for compatibility with operator overloading},
journal={Procedia Computer Science},
author={Huck, Alexander and Utke, Jean and Bischof, Christian},
volume={80},
year={2016},
pages={1485 - 1496},
issn={18770509},
address={San Diego, CA, United states},
abstract={In C++, new features and semantics can be added to an existing software package without sweeping code changes by introducing a user-defined type using operator overloading. This approach is used, for example, to add capabilities such as algorithmic differentiation. However, the introduction of operator overloading can cause a multitude of compilation errors. In a previous paper, we identified code constructs that cause a violation of the C++ language standard after a type change, and a tool called OO-Lint based on the Clang compiler that identifies these code constructs with lint-like messages. In this paper, we present an extension of this work that automatically transforms such problematic code constructs in order to make an existing code base compatible with a semantic augmentation through operator overloading. We applied our tool to the CFD software OpenFOAM and detected and transformed 23 instances of problematic code constructs in 160,000 lines of code. A significant amount of these root causes are included up to 425 times in other files causing a tremendous compiler error amplification. In addition, we show the significance of our work with a case study of the evolution of the ice flow modeling software ISSM, comparing a recent version which was manually type changed with a legacy version. The recent version shows no signs of problematic code constructs. In contrast, our tool detected and transformed a remarkable amount of issues in the legacy version that previously had to be manually located and fixed.<br/> &copy; The Authors. Published by Elsevier B.V.},
key={C++ (programming language)},
keywords={Cesium;Codes (symbols);Computational fluid dynamics;Program compilers;Semantics;Static analysis;},
note={Algorithmic differentiations;Code constructs;Error amplification;Operator overloading;Semantic augmentations;Source transformation;Type change;User-defined types;},
URL={http://dx.doi.org/10.1016/j.procs.2016.05.470},
}


@article{20182605368067,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={What are the effects of history length and age on mining software change impact?},
journal={Empirical Software Engineering},
author={Moonen, Leon and Rolfsnes, Thomas and Binkley, Dave and Di Alesio, Stefano},
volume={23},
number={4},
year={2018},
pages={2362 - 2397},
issn={13823256},
abstract={The goal of Software Change Impact Analysis is to identify artifacts (typically source-code files or individual methods therein) potentially affected by a change. Recently, there has been increased interest in mining software change impact based on evolutionary coupling. A particularly promising approach uses association rule mining to uncover potentially affected artifacts from patterns in the system&rsquo;s change history. Two main considerations when using this approach are the history length, the number of transactions from the change history used to identify the impact of a change, and history age, the number of transactions that have occurred since patterns were last mined from the history. Although history length and age can significantly affect the quality of mining results, few guidelines exist on how to best select appropriate values for these two parameters. In this paper, we empirically investigate the effects of history length and age on the quality of change impact analysis using mined evolutionary coupling. Specifically, we report on a series of systematic experiments using three state-of-the-art mining algorithms that involve the change histories of two large industrial systems and 17 large open source systems. In these experiments, we vary the length and age of the history used to mine software change impact, and assess how this affects precision and applicability. Results from the study are used to derive practical guidelines for choosing history length and age when applying association rule mining to conduct software change impact analysis.<br/> &copy; 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
key={Open systems},
keywords={Association rules;Data mining;Open source software;Quality control;},
note={Change impact analysis;Industrial systems;Mining algorithms;Open source system;Parameter-tuning;Practical guidelines;Software change;Systematic experiment;},
URL={http://dx.doi.org/10.1007/s10664-017-9588-z},
}


@inproceedings{20123415364461,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Can we predict types of code changes? An empirical analysis},
journal={IEEE International Working Conference on Mining Software Repositories},
author={Giger, Emanuel and Pinzger, Martin and Gall, Harald C.},
year={2012},
pages={217 - 226},
issn={21601852},
address={Zurich, Switzerland},
abstract={There exist many approaches that help in pointing developers to the change-prone parts of a software system. Although beneficial, they mostly fall short in providing details of these changes. Fine-grained source code changes (SCC) capture such detailed code changes and their semantics on the statement level. These SCC can be condition changes, interface modifications, inserts or deletions of methods and attributes, or other kinds of statement changes. In this paper, we explore prediction models for whether a source file will be affected by a certain type of SCC. These predictions are computed on the static source code dependency graph and use social network centrality measures and object-oriented metrics. For that, we use change data of the Eclipse platform and the Azureus 3 project. The results show that Neural Network models can predict categories of SCC types. Furthermore, our models can output a list of the potentially change-prone files ranked according to their change-proneness, overall and per change type category. &copy; 2012 IEEE.<br/>},
key={Object oriented programming},
keywords={Codes (symbols);Computer software maintenance;Computer software selection and evaluation;Forecasting;Learning systems;Semantics;},
note={Dependency graphs;Empirical analysis;Fine-grained source code changes;Interface modification;Neural network model;Object oriented metrics;Software Quality;Use social networks;},
URL={http://dx.doi.org/10.1109/MSR.2012.6224284},
}


@inproceedings{20170503306415,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Exploring the effects of history length and age on mining software change impact},
journal={Proceedings - 2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation, SCAM 2016},
author={Moonen, Leon and Alesio, Stefano Di and Rolfsnes, Thomas and Binkley, Dave W.},
year={2016},
pages={207 - 216},
address={Raleigh, NC, United states},
abstract={The goal of Software Change Impact Analysis is to identify artifacts (typically source-code files) potentially affected by a change. Recently, there is an increased interest in mining software change impact based on evolutionary coupling. A particularly promising approach uses association rule mining to uncover potentially affected artifacts from patterns in the system's change history. Two main considerations when using this approach are the history length, the number of transactions from the change history used to identify the impact of a change, and history age, the number of transactions that have occurred since patterns were last mined from the history. Although history length and age can significantly affect the quality of mining results, few guidelines exist on how to best select appropriate values for these two parameters. In this paper, we empirically investigate the effects of history length and age on the quality of change impact analysis using mined evolutionary couplings. Specifically, we report on a series of systematic experiments involving the change histories of two large industrial systems and 17 large open source systems. In these experiments, we vary the length and age of the history used to mine software change impact, and assess how this affects precision and applicability. Results from the study are used to derive practical guidelines for choosing history length and age when applying association rule mining to conduct software change impact analysis.<br/> &copy; 2016 IEEE.},
key={Quality control},
keywords={Association rules;Codes (symbols);Computer programming languages;Open source software;Open systems;},
note={Change impact analysis;Industrial systems;Mining software;Open source system;Parameter-tuning;Practical guidelines;Software change;Systematic experiment;},
URL={http://dx.doi.org/10.1109/SCAM.2016.9},
}


@inproceedings{20171803630200,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Lost comments support program comprehension},
journal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
author={Omori, Takayuki},
year={2017},
pages={567 - 568},
address={Klagenfurt, Austria},
abstract={Source code comments are valuable to keep developers' explanations of code fragments. Proper comments help code readers understand the source code quickly and precisely. However, developers sometimes delete valuable comments since they do not know about the readers' knowledge and think the written comments are redundant. This paper describes a study of lost comments based on edit operation histories of source code. The experimental result shows that developers sometimes delete comments although their associated code fragments are not changed. Lost comments contain valuable descriptions that can be utilized as new data sources to support program comprehension.<br/> &copy; 2017 IEEE.},
key={Codes (symbols)},
keywords={Computer programming languages;Reengineering;},
note={Code fragments;Code readers;Data-sources;Operation history;Source code comments;Source codes;Support programs;},
URL={http://dx.doi.org/10.1109/SANER.2017.7884680},
}


@inproceedings{20144600189188,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Needfeed: Taming change notifications by modeling code relevance},
journal={ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
author={Padhye, Rohan and Mani, Senthil and Sinha, Vibha Singhal},
year={2014},
pages={665 - 675},
address={Vasteras, Sweden},
abstract={Most software development tools allow developers to subscribe to notifications about code checked-in by their team members in order to review changes to artifacts that they are responsible for. However, past user studies have indicated that this mechanism is counter-productive, as developers spend a significant amount of effort sifting through such feeds looking for items that are relevant to them. We present NeedFeed, a system that models code relevance by mining a project's software repository and highlights changes that a developer may need to review. We evaluate several techniques to model code relevance, from a naive TOUCH-based approach to generic HISTORY-based classifiers using temporal code metrics at file and method-level granularities, which are then improved by building developer-specific models using TEXT-based features from commit messages. NeedFeed reduces notification clutter by more than 90%, on average, with the best strategy giving an average precision and recall of more than 75%.<br/> &copy; 2014 ACM.},
key={Software design},
keywords={Codes (symbols);Groupware;},
note={Change notification;Collaborative software development;Mining software repositories;Precision and recall;Software development tools;Software repositories;Text-based features;Version control;},
URL={http://dx.doi.org/10.1145/2642937.2642985},
}


@inproceedings{20161302174308,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Will This Bug-Fixing Change Break Regression Testing?},
journal={International Symposium on Empirical Software Engineering and Measurement},
author={Tang, Xinye and Wang, Song and Mao, Ke},
volume={2015-November},
year={2015},
pages={215 - 224},
issn={19493770},
address={Beijing, China},
abstract={Context: Software source code is frequently changed for fixing revealed bugs. These bug-fixing changes might introduce unintended system behaviors, which are inconsistent with scenarios of existing regression test cases, and consequently break regression testing. For validating the quality of changes, regression testing is a required process before submitting changes during the development of software projects. Our pilot study shows that 48.7% bug-fixing changes might break regression testing at first run, which means developers have to run regression testing at least a couple of times for 48.7% changes. Such process can be tedious and time consuming. Thus, before running regression test suite, finding these changes and corresponding regression test cases could be helpful for developers to quickly fix these changes and improve the efficiency of regression testing. Goal: This paper proposes bug- fixing change impact prediction (BFCP), for predicting whether a bug-fixing change will break regression testing or not before running regression test cases, by mining software change histories. Method: Our approach employs the machine learning algorithms and static call graph analysis technique. Given a bug-fixing change, BFCP first predicts whether it will break existing regression test cases; second, if the change is predicted to break regression test cases, BFCP can further identify the might-be-broken test cases. Results: Results of experiments on 552 real bug-fixing changes from four large open source projects show that BFCP could achieve prediction precision up to 83.3% recall up to 92.3% and F-score up to 81.4%. For identifying the might-be-broken test cases, BFCP could achieve 100% recall.<br/> &copy; 2015 IEEE.},
key={Software testing},
keywords={Forecasting;Learning algorithms;Learning systems;Object oriented programming;Open source software;Program debugging;Regression analysis;},
note={Open source projects;Prediction precision;Regression testing;Regression tests;Software project;Software source codes;Source code changes;Static program analysis;},
URL={http://dx.doi.org/10.1109/ESEM.2015.7321218},
}


@inproceedings{20122015014502,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Enhancing flexibility and portability of Execution Preserving Language Transformation using Meta programming},
journal={2012 International Conference on Power, Signals, Controls and Computation, EPSCICON 2012},
author={Beevi, S. Nadera and Prasad, D. Chitra and Chandra, S. S. Vinod},
year={2012},
address={Thrissur, Kerala, India},
abstract={This paper describes flexibility and effectiveness of Execution Preserving Language Transformation (EPLT) using a meta framework. Program transformation is visualized as transforming the program written in legacy code to a more contemporary environment. Pure program transformation systems translate source code to target code preserving the functionality of legacy systems. Augmented versions of existing languages can be developed by combining good properties of two languages. In this work a meta framework is developed from C++ and Java language. The growing popularity of Java language forces the programmer to implement data structures and algorithms of other languages in Java. This meta framework enhances the conversion of unsafe source code written in C++ and Java to safe byte code. It provides a transformational scheme which unifies the syntax and semantics of existing languages and reduce the learning curves. &copy; 2012 IEEE.<br/>},
key={C++ (programming language)},
keywords={Codes (symbols);Java programming language;Legacy systems;Power control;Semantics;},
note={Contemporary environments;Java;Learning curves;Meta Programming;Meta-frameworks;Program transformation systems;Program transformations;yacc;},
URL={http://dx.doi.org/10.1109/EPSCICON.2012.6175252},
}


@inproceedings{20124515635801,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The atomic change set of Java programming language},
journal={ICCSE 2012 - Proceedings of 2012 7th International Conference on Computer Science and Education},
author={You, Liang and Lu, Yansheng},
year={2012},
pages={1090 - 1092},
address={Melbourne, VIC, Australia},
abstract={Software configuration management computes the diff between the old version and new version of the program and represents the diff as added, deleted and updated text lines. The source code change represents the change type of code revisions in fine-grained levels such as class or field but rather to added, deleted and updated text lines. Atomic change is the minimal source code change. An atomic change can not be decomposed into other atomic changes. This paper defines the atomic change and the atomic change set of Java programming language. It analyzes properties of the atomic change set of Java programming language. It also emphasizes the differences of the binary compatibility and source compatibility of atomic changes of Java programming language. Finally, it gives a method for computing all source compatible atomic changes of Java programming language. &copy; 2012 IEEE.<br/>},
key={Java programming language},
keywords={Ada (programming language);Atoms;Codes (symbols);Education computing;},
note={Atomic change sets;Atomic changes;Binary compatibilities;Change types;Fine grained;Software configuration management;Source code changes;Source compatibilities;},
URL={http://dx.doi.org/10.1109/ICCSE.2012.6295253},
}


@article{20181204932095,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Early prediction of merged code changes to prioritize reviewing tasks},
journal={Empirical Software Engineering},
author={Fan, Yuanrui and Xia, Xin and Lo, David and Li, Shanping},
volume={23},
number={6},
year={2018},
pages={3346 - 3393},
issn={13823256},
abstract={Modern Code Review (MCR) has been widely used by open source and proprietary software projects. Inspecting code changes consumes reviewers much time and effort since they need to comprehend patches, and many reviewers are often assigned to review many code changes. Note that a code change might be eventually abandoned, which causes waste of time and effort. Thus, a tool that predicts early on whether a code change will be merged can help developers prioritize changes to inspect, accomplish more things given tight schedule, and not waste reviewing effort on low quality changes. In this paper, motivated by the above needs, we build a merged code change prediction tool. Our approach first extracts 34 features from code changes, which are grouped into 5 dimensions: code, file history, owner experience, collaboration network, and text. And then we leverage machine learning techniques such as random forest to build a prediction model. To evaluate the performance of our approach, we conduct experiments on three open source projects (i.e., Eclipse, LibreOffice, and OpenStack), containing a total of 166,215 code changes. Across three datasets, our approach statistically significantly improves random guess classifiers and two prediction models proposed by Jeong et al. (2009) and Gousios et al. (2014) in terms of several evaluation metrics. Besides, we also study the important features which distinguish merged code changes from abandoned ones.<br/> &copy; 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
key={Open systems},
keywords={Classification (of information);Codes (symbols);Decision trees;Forecasting;Learning systems;Mergers and acquisitions;Open source software;},
note={Code review;Collaboration network;Evaluation metrics;Features;Machine learning techniques;Open source projects;Predictive modeling;Proprietary software;},
URL={http://dx.doi.org/10.1007/s10664-018-9602-0},
}


@inproceedings{20171703609420,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Towards automatic learning of heuristics for mechanical transformations of procedural code},
journal={Electronic Proceedings in Theoretical Computer Science, EPTCS},
author={Vigueras, Guillermo and Carro, Manuel and Tamarit, Salvador and Marino, Julio},
volume={237},
year={2017},
pages={52 - 67},
issn={20752180},
address={Salamanca, Spain},
abstract={The current trends in next-generation exascale systems go towards integrating a wide range of specialized (co-)processors into traditional supercomputers. Due to the efficiency of heterogeneous systems in terms of Watts and FLOPS per surface unit, opening the access of heterogeneous platforms to a wider range of users is an important problem to be tackled. However, heterogeneous platforms limit the portability of the applications and increase development complexity due to the programming skills required. Program transformation can help make programming heterogeneous systems easier by defining a step-wise transformation process that translates a given initial code into a semantically equivalent final code, but adapted to a specific platform. Program transformation systems require the definition of efficient transformation strategies to tackle the combinatorial problem that emerges due to the large set of transformations applicable at each step of the process. In this paper we propose a machine learning-based approach to learn heuristics to define program transformation strategies. Our approach proposes a novel combination of reinforcement learning and classification methods to efficiently tackle the problems inherent to this type of systems. Preliminary results demonstrate the suitability of this approach.<br/> &copy; G. Vigueras, M. Carro, S. Tamarit & J. Mari&ntilde;o.},
key={Codes (symbols)},
keywords={Artificial intelligence;Learning algorithms;Learning systems;Reinforcement learning;Supercomputers;},
note={Classification methods;Combinatorial problem;Heterogeneous platforms;Heterogeneous systems;Program transformation systems;Program transformations;Programming skills;Transformation process;},
URL={http://dx.doi.org/10.4204/EPTCS.237.4},
}


@article{20182405310280,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Query expansion based on statistical learning from code changes},
journal={Software - Practice and Experience},
author={Huang, Qing and Yang, Yangrui and Zhan, Xue and Wan, Hongyan and Wu, Guoqing},
volume={48},
number={7},
year={2018},
pages={1333 - 1351},
issn={00380644},
abstract={Thesaurus-based, code-related, and software-specific query expansion techniques are the main contributions in free-form query search. However, these techniques still could not put the most relevant query result in the first position because they lack the ability to infer the expansion words that represent the user needs based on a given query. In this paper, we discover that code changes can imply what users want and propose a novel query expansion technique with code changes (QECC). It exploits (changes, contexts) pairs from changed methods. On the basis of statistical learning from pairs, it can infer code changes for a given query. In this way, it expands a query with code changes and recommends the query results that meet actual needs perfectly. In addition, we implement InstaRec to perform QECC and evaluate it with 195 039 change commits from GitHub and our code tracker. The results show that QECC can improve the precision of 3 code search algorithms (ie, IR, Portfolio, and VF) by up to 52% to 62% and outperform the state-of-the-art query expansion techniques (ie, query expansion based on crowd knowledge and CodeHow) by 13% to 16% when the top 1 result is inspected.<br/> Copyright &copy; 2018 John Wiley & Sons, Ltd.},
key={Codes (symbols)},
keywords={Computer software reusability;Information retrieval;},
note={Code changes;Code search;Free form queries;Query expansion;Query expansion techniques;Relevant query;State of the art;Statistical learning;},
URL={http://dx.doi.org/10.1002/spe.2574},
}


@inproceedings{20160601908475,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Landfill: An open dataset of code smells with public evaluation},
journal={IEEE International Working Conference on Mining Software Repositories},
author={Palomba, Fabio and Di Nucci, Dario and Tufano, Michele and Bavota, Gabriele and Oliveto, Rocco and Poshyvanyk, Denys and De Lucia, Andrea},
volume={2015-August},
year={2015},
pages={482 - 485},
issn={21601852},
address={Florence, Italy},
abstract={Code smells are symptoms of poor design and implementation choices that may hinder code comprehension and possibly increase change- and fault-proneness of source code. Several techniques have been proposed in the literature for detecting code smells. These techniques are generally evaluated by comparing their accuracy on a set of detected candidate code smells against a manually-produced oracle. Unfortunately, such comprehensive sets of annotated code smells are not available in the literature with only few exceptions. In this paper we contribute (i) a dataset of 243 instances of five types of code smells identified from 20 open source software projects, (ii) a systematic procedure for validating code smell datasets, (iii) LANDFILL, a Web-based platform for sharing code smell datasets, and (iv) a set of APIs for programmatically accessing LANDFILL's contents. Anyone can contribute to Landfill by (i) improving existing datasets (e.g., Adding missing instances of code smells, flagging possibly incorrectly classified instances), and (ii) sharing and posting new datasets. Landfill is available at www.sesa.unisa.it/landfill/, while the video demonstrating its features in action is available at http://www.sesa.unisa.it/tools/landfill.jsp.<br/> &copy; 2015 IEEE.},
key={Open systems},
keywords={Anthropomorphic robots;Buildings;Codes (symbols);History;Land fill;Odors;Open source software;},
note={Androids;Code comprehension;Design and implementations;Humanoid robot;Manuals;Open source software projects;Software systems;Web based platform;},
URL={http://dx.doi.org/10.1109/MSR.2015.69},
}


@inproceedings{20140217192353,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automatically extracting instances of code change patterns with AST analysis},
journal={IEEE International Conference on Software Maintenance, ICSM},
author={Martinez, Matias and Duchien, Laurence and Monperrus, Martin},
year={2013},
pages={388 - 391},
address={Eindhoven, Netherlands},
abstract={A code change pattern represents a kind of recurrent modification in software. For instance, a known code change pattern consists of the change of the conditional expression of an if statement. Previous work has identified different change patterns. Complementary to the identification and definition of change patterns, the automatic extraction of pattern instances is essential to measure their empirical importance. For example, it enables one to count and compare the number of conditional expression changes in the history of different projects. In this paper we present a novel approach for search patterns instances from software history. Our technique is based on the analysis of Abstract Syntax Trees (AST) files within a given commit. We validate our approach by counting instances of 18 change patterns in 6 open-source Java projects. &copy; 2013 IEEE.<br/>},
key={Open source software},
keywords={Codes (symbols);Computer software maintenance;Trees (mathematics);},
note={Abstract Syntax Trees;Automatic extraction;Change patterns;Code changes;Conditional expressions;Open sources;Search patterns;Software history;},
URL={http://dx.doi.org/10.1109/ICSM.2013.54},
}


@inproceedings{20154201398051,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Evaluating QR code case studies using a mobile learning framework},
journal={Proceedings of the 10th International Conference on Mobile Learning 2014, ML 2014},
author={Rikala, Jenni},
year={2014},
pages={199 - 206},
address={Madrid, Spain},
abstract={The aim of this study was to evaluate the feasibility of Quick Response (QR) codes and mobile devices in the context of Finnish basic education. The feasibility was analyzed through a mobile learning framework, which includes the core characteristics of mobile learning. The study is part of a larger research where the aim is to develop a theoretical framework for mobile learning and mobile learning practices. QR codes were chosen in particular because teachers were interested in seeing how this relatively easy and versatile technology could be utilized in their classrooms. QR code implementations blended in and enriched the traditional teaching methods and classroom learning in a motivating and meaningful way. The core characteristics of mobile leaning were realized comparatively well. However, the study also indicated that the factors that should be added to the framework are the school curriculum, ICT integration strategies, teacher competencies, and technological, social and cultural change.<br/> &copy; 2014 IADIS.},
key={E-learning},
keywords={Codes (symbols);Curricula;Teaching;},
note={Classroom learning;Cultural changes;ICT integrations;Mobile Learning;Pedagogical practices;Quick response code;Theoretical framework;Traditional teachings;},
}


@inproceedings{20161202130352,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Post release versions based code change quality metrics},
journal={ACM International Conference Proceeding Series},
author={Sharma, Meera and Kumari, Madhu and Singh, V.B.},
volume={10-13-August-2015},
year={2015},
pages={235 - 243},
address={Aluva, Kochi, Kerala, India},
abstract={Software Metric is a quantitative measure of the degree to which a system, component or process possesses a given attribute. Bug fixing, new features (NFs) introduction and feature improvements (IMPs) are the key factors in deciding the next version of software. For fixing an issue (bug/new feature/feature improvement), a lot of changes have to be incorporated into the source code of the software. These code changes need to be understood by software engineers and managers when performing their daily development and maintenance tasks. In this paper, we have proposed four new metrics namely code change quality, code change density, file change quality and file change density to understand the quality of code changes across the different versions of five open source software products, namely Avro, Pig, Hive, jUDDI and Whirr of Apache project. Results show that all the products get better code change quality over a period of time. We have also observed that all the five products follow the similar code change trend.<br/> &copy; 2015 ACM.},
key={Open systems},
keywords={Codes (symbols);Entropy;Open source software;},
note={Feature improvement;Maintenance tasks;New feature;Quality metrics;Quantitative measures;Software metrices;Software repositories;Source codes;},
URL={http://dx.doi.org/10.1145/2791405.2791466},
}


@inproceedings{20170703353492,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Research of speech amplitude distribution based on hadamard transformation},
journal={Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
author={Tu, Jingxue and Xu, Jingyun and Zhao, Xiaoqun},
volume={183},
year={2017},
pages={267 - 273},
issn={18678211},
address={Shanghai, China},
abstract={In view of PCM (Pulse Code Modulation) of speech signal, this paper puts forward a method of speech processing based on hadamard matrix transformation to change the amplitude distribution of speech signal, which can reduce the standard deviation of speech signal. Experiments show that the hadamard matrix transformation algorithm can obviously reduce the amplitude range of speech signal. Speech signal standard deviation is reduced by 20% after the transformation. At the same time, speech quality after decoding is not decreased according to listening experimenter. The algorithm reduces amplitude range and standard deviation of the speech signal, which can code the speech signal with less bits, and compression efficiency can be further improved.<br/> &copy; ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2017.},
key={Audio signal processing},
keywords={Artificial intelligence;Hadamard matrices;Hadamard transforms;Learning systems;Linear transformations;Speech;Speech communication;Speech processing;Statistics;},
note={Amplitude distributions;Compression efficiency;Hadamard;Hadamard transformation;Matrix transformation;Speech quality;Speech signals;Standard deviation;},
URL={http://dx.doi.org/10.1007/978-3-319-52730-7_27},
}


@inproceedings{20151200653045,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The ekeko/X program transformation tool},
journal={Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014},
author={De Roover, Coen and Inoue, Katsuro},
year={2014},
pages={53 - 58},
address={Victoria, BC, Canada},
abstract={Developers often need to perform repetitive changes to source code. For instance, to repair several instances of a bug or to update all clients of a library to a newer version. Manually performing such changes is laborious and error-prone. Program transformation tools enable automating changes, but specifying changes as a program transformation requires significant expertise. Code templates are often touted as a remedy, yet have never been endorsed wholeheartedly. Their use is mostly limited to expressing the syntactic characteristics of the intended change subjects. Less familiar means have to be resorted to for expressing their structural, control flow, and data flow characteristics. In this tool paper, we introduce a decidedly template-driven program transformation tool called Ekeko/X. Its specifications feature templates for specifying all of the aforementioned characteristics of its subjects. To this end, developers can associate different directives with individual components of a template. Each matching directive imposes particular constraints on the matches for the component it is associated with. Rewriting directives, on the other hand, determine how each match should be changed. We develop Ekeko/X from the ground up, starting from its applicative logic meta-programming foundation. We highlight the key choices in this implementation and demonstrate its use through two example program transformations.<br/> &copy; 2014 IEEE.},
key={Codes (symbols)},
note={code templates;Control flows;Feature template;Individual components;Logic meta programming;Program transformations;Refactorings;Template-driven;},
URL={http://dx.doi.org/10.1109/SCAM.2014.32},
}


@inproceedings{20142117735125,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A game-based learning approach to road safety: The code of everand},
journal={Conference on Human Factors in Computing Systems - Proceedings},
author={Dunwell, Ian and De Freitas, Sara and Petridis, Panagiotis and Hendrix, Maurice and Arnab, Sylvester and Lameras, Petros and Stewart, Craig},
year={2014},
pages={3389 - 3398},
address={Toronto, ON, Canada},
abstract={Game and gamification elements are increasingly seeing use as part of interface designs for applications seeking to engage and retain users whilst transferring information. This paper presents an evaluation of a game-based approach seeking to improve the road safety behaviour amongst children aged 9-15 within the UK, made available outside of a classroom context as an online, browser-based, free-to-play game. The paper reports on data for 99,683 players over 315,882 discrete logins, supplemented by results from a nationally-representative survey of children at UK schools (n=1,108), an incentivized survey of the player-base (n=1,028), and qualitative data obtained through a series of one-to-one interviews aged 9-14 (n=28). Analysis demonstrates the reach of the game to its target demographic, with 88.13% of players within the UK. A 3.94 male/female ratio was observed amongst players surveyed, with an age distribution across the target range of 9-15. Noting mean and median playtimes of 93 and 31 minutes (n=99,683), it is suggested such an approach to user engagement and retention can surpass typical contact times obtained through other forms of web-based content. The size of the player-base attracted to the game and players' qualitative feedback demonstrates the potential for serious games deployed on a national scale.<br/>},
key={Serious games},
keywords={Accident prevention;Binary alloys;E-learning;Human computer interaction;Human engineering;Motor transportation;Roads and streets;Surveys;Teaching;},
note={Attitudinal change;Game-Based;Game-based approaches;Game-based Learning;Gamification;Interface designs;Qualitative feedback;Road safety;},
URL={http://dx.doi.org/10.1145/2556288.2557281},
}


@inproceedings{20180404675527,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning Feature Representations from Change Dependency Graphs for Defect Prediction},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
author={Loyola, Pablo and Matsuo, Yutaka},
volume={2017-October},
year={2017},
pages={361 - 372},
issn={10719458},
address={Toulouse, France},
abstract={Given the heterogeneity of the data that can be extracted from the software development process, defect prediction techniques have focused on associating different sources of data with the introduction of faulty code, usually relying on handcrafted features. While these efforts have generated considerable progress over the years, little attention has been given to the fact that the performance of any predictive model depends heavily on the representation of the data used, and that different representations can lead to different results. We consider this a relevant problem, as it could be affecting directly the efforts towards generating safer software systems. Therefore, we propose to study the impact of the representation of the data in defect prediction models. To this end, we focus on the use of developer activity data, from which we structure dependency graphs. Then, instead of manually generating features, such as network metrics, we propose two models inspired by recent advances in representation learning which are able to automatically generate feature representations from graph data. These new representations are compared against manually crafted features for defect prediction in real world software projects. Our results show that automatically learned features are competitive, reaching increments in prediction performance up to 13%.<br/> &copy; 2017 IEEE.},
key={Software reliability},
keywords={Defects;Forecasting;Learning systems;Software design;},
note={Change dependencies;Defect prediction;Defect prediction models;Developer activities;Feature representation;Prediction performance;Representation learning;Software development process;},
URL={http://dx.doi.org/10.1109/ISSRE.2017.30},
}


@article{20185106280915,
language={Spanish},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={NESC Handbook, Premier Edition - Spanish version - A Discussion of the National Electrical Safety Code(R) plus COMENTARIOS en ESPAÑOL},
journal={NESC Handbook, Premier Edition Spanish version - A Discussion of the National Electrical Safety Code(R) + COMENTARIOS EN ESPA&Ntilde;OL},
year={2018},
pages={1 - 826},
abstract={Scope: The 2017 NESC&reg; Handbook, Premier Edition - Spanish version, is essential in Spanish speaking markets and assist in areas that need well-defined regulations related to electrical safety for power, energy transmission and distribution, and communications industries. Important to power and energy engineers, as well as to communications providers, such as internet, phone et al. The handbook includes text directly from Code in English which provides users an easy reference back to the code, ruling-by-ruling. It gives users insight into what lies behind the NESC's rules and how to apply them. The Handbook was developed for use at many levels in the electric and communication industries, including those involved in system design, construction, maintenance, inspection, standards development and worker training. The Handbook also discusses how the NESC Committee has developed the rules in the Code and responded to change proposals during the past 100 years. This allows users to understand how questions they may have were dealt with in the past. The Premier Edition includes: These are key points from the 2017 Handbook Edition: &bull; Revising the purpose rule to include only the safeguarding of persons and utility facilities and clarifying the application rules. &bull; Deleting unused definitions and adding definitions for communication and supply space. &bull; Revising the substation impenetrable fence requirements. &bull; Adding an exception to exempt underground cable grounding requirements from the 4 grounds in each mile rule under certain conditions. &bull; Revising and reorganizing the guy insulator placement rules along with eliminating the voltage transfer requirements associated with them. &bull; Requiring a 40 vertical clearance from communication cables in the communication space if a luminaire is not effectively grounded. &bull; Deleting the conductance requirement for underground insulating jacketed grounded neutral supply cables and revising the grounding and bonding rules for supply and communication cables in random separation installations. &bull; Revising and reorganizing the Grades of Construction Table 242-1 that will now include service drops. &bull; Revising the strength rules to require that all conductors be considered for damage due to Aeolian vibration. &bull; Revising the rules in Part 4 to align with changes made to 29 CFR by the Occupational Safety and Health Administration (OSHA).<br/> &copy; 2018 IEEE.},
key={Electric power system control},
keywords={Accident prevention;Cables;Codes (symbols);Construction equipment;Electric control equipment;Electric lines;Electric power transmission;Electric utilities;Occupational risks;Telecommunication cables;Underground cables;Underground equipment;Voltage control;},
note={Communication lines;Communications industry;Communications systems;Electric supply;Electric supply systems;Electrical safety;High voltage safety;National Electric Code;Occupational safety;Power station;Safety work;Spanish;Underground power cables;},
URL={http://dx.doi.org/10.1109/IEEESTD.2018.8574031},
versions={1},
status={Active - Approved},
standardID={NESC HBK-2018},
}


@inproceedings{20184906168040,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Exploring Open Source for Machine Learning Problem on Diabetic Retinopathy},
journal={Advances in Intelligent Systems and Computing},
author={Kumari, Archana and Choudhury, Tanupriya and Chitra Rajagopal, P.},
volume={841},
year={2019},
pages={565 - 574},
issn={21945357},
address={Jaipur, India},
abstract={Open-source operating system, as well as its packages, is more powerful and secure than the proprietary sources. In the proprietary source, software source code is not easily available because it is secret; by contrast in the open-source operating system source code is easily available, so any programmer can change the code and implement their ideas and modify it because of its openness. Also, one major advantage is that we do not need to spend a huge amount of money on the software. So, in this paper, we used open-source software for coding purposes and looked at the data available on the UCI machine learning repository on the diabetic retinopathy.<br/> &copy; 2019, Springer Nature Singapore Pte Ltd.},
key={Open source software},
keywords={Artificial intelligence;Codes (symbols);Computer programming;Decision trees;Eye protection;Learning systems;Neural networks;Open systems;},
note={Boosted decision trees;Diabetic retinopathy;Machine learning problem;Open source operating systems;Open sources;Software source codes;Source codes;UCI machine learning repository;},
URL={http://dx.doi.org/10.1007/978-981-13-2285-3_67},
}


@article{20184506046072,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={DeepBugs: A learning approach to name-based bug detection},
journal={Proceedings of the ACM on Programming Languages},
author={Pradel, Michael and Sen, Koushik},
volume={2},
number={OOPSLA},
year={2018},
issn={24751421},
abstract={Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents DeepBugs, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 JavaScript files yields bug detectors that have a high accuracy (between 89% and 95%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68% true positive rate) in real-world code.<br/> &copy; 2018 Copyright held by the owner/author(s).},
key={Codes (symbols)},
keywords={Cosine transforms;High level languages;Learning systems;Program debugging;Semantics;},
note={Binary classification problems;Bug detection;Code transformation;Javascript;Natural languages;Program analysis;Semantic representation;True positive rates;},
URL={http://dx.doi.org/10.1145/3276517},
}


@inproceedings{20184706087415,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An empirical investigation into learning bug-fixing patches in the wild via neural machine translation},
journal={ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
author={Tufano, Michele and Penta, Massimiliano Di and Watson, Cody and White, Martin and Bavota, Gabriele and Poshyvanyk, Denys},
year={2018},
pages={832 - 837},
address={Montpellier, France},
abstract={Millions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. We mine millions of bug-fixes from the change histories of GitHub repositories to extract meaningful examples of such bug-fixes. Then, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. Our model is able to fix hundreds of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9% of the cases.<br/> &copy; 2018 Association for Computing Machinery.},
key={Open systems},
keywords={Codes (symbols);Computational linguistics;Computer aided language translation;Open source software;Program debugging;Software design;},
note={Bug fixes;Change history;Development history;Empirical investigation;Empirical studies;Encoder-decoder;Machine translations;Open source projects;},
URL={http://dx.doi.org/10.1145/3238147.3240732},
}


@inproceedings{20135017064804,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The impact of tangled code changes},
journal={IEEE International Working Conference on Mining Software Repositories},
author={Herzig, Kim and Zeller, Andreas},
year={2013},
pages={121 - 130},
issn={21601852},
address={San Francisco, CA, United states},
abstract={When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source JAVA projects, we found up to 15% of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6% of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes. &copy; 2013 IEEE.<br/>},
key={Open source software},
keywords={Codes (symbols);},
note={Bias;Data quality;Mining software repositories;Noise;Tangled code;},
URL={http://dx.doi.org/10.1109/MSR.2013.6624018},
}


@inproceedings{20164502985287,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mining fine-grained code changes to detect unknown change patterns},
journal={Proceedings - International Conference on Software Engineering},
author={Negara, Stas and Codoban, Mihai and Dig, Danny and Johnson, Ralph E.},
volume={0},
number={1},
year={2014},
pages={803 - 813},
issn={02705257},
address={Hyderabad, India},
abstract={Identifying repetitive code changes benefits developers, tool builders, and researchers. Tool builders can automate the popular code changes, thus improving the productivity of developers. Researchers can better understand the practice of code evolution, advancing existing code assistance tools and benefiting developers even further. Unfortunately, existing research either predominantly uses coarse-grained Version Control System (VCS) snapshots as the primary source of code evolution data or considers only a small subset of program transformations of a single kind - refactorings. We present the first approach that identifies previously unknown frequent code change patterns from a fine-grained sequence of code changes. Our novel algorithm effectively handles challenges that distinguish continuous code change pattern mining from the existing data mining techniques. We evaluated our algorithm on 1,520 hours of code development collected from 23 developers, and showed that it is effective, useful, and scales to large amounts of data. We analyzed some of the mined code change patterns and discovered ten popular kinds of high-level program transformations. More than half of our 420 survey participants acknowledged that eight out of ten transformations are relevant to their programming activities.<br/> &copy; 2014 ACM.},
key={Codes (symbols)},
keywords={Data mining;Metadata;Software engineering;Surveys;},
note={Code changes;Code development;High-level program;Large amounts of data;Novel algorithm;Program transformations;Programming activities;Version control system;},
URL={http://dx.doi.org/10.1145/2568225.2568317},
}


@inproceedings{20135017064805,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A dataset from change history to support evaluation of software maintenance tasks},
journal={IEEE International Working Conference on Mining Software Repositories},
author={Dit, Bogdan and Holtzhauer, Andrew and Poshyvanyk, Denys and Kagdi, Huzefa},
year={2013},
pages={131 - 134},
issn={21601852},
address={San Francisco, CA, United states},
abstract={Approaches that support software maintenance need to be evaluated and compared against existing ones, in order to demonstrate their usefulness in practice. However, oftentimes the lack of well-established sets of benchmarks leads to situations where these approaches are evaluated using different datasets, which results in biased comparisons. In this data paper we describe and make publicly available a set of benchmarks from six Java applications, which can be used in the evaluation of various software engineering (SE) tasks, such as feature location and impact analysis. These datasets consist of textual description of change requests, the locations in the source code where they were implemented, and execution traces. Four of the benchmarks were already used in several SE research papers, and two of them are new. In addition, we describe in detail the methodology used for generating these benchmarks and provide a suite of tools in order to encourage other researchers to validate our datasets and generate new benchmarks for other subject software systems. Our online appendix: http://www.cs.wm.edu/semeru/data/msr13/ &copy; 2013 IEEE.<br/>},
key={Computer software maintenance},
keywords={Application programs;Benchmarking;Location;},
note={Datasets;Feature location;Impact analysis;Java applications;Research papers;Software systems;Software-maintenance tasks;Textual description;},
URL={http://dx.doi.org/10.1109/MSR.2013.6624019},
}


@inproceedings{20180304659602,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Impact of Version Management for Transactional Memories on Phase-Change Memories},
journal={Proceedings - 29th International Symposium on Computer Architecture and High Performance Computing Workshops, SBAC-PADW 2017},
author={Teixeira, Felipe L. and Pilla, Mauricio L. and Bois, Andre R. Du and Mosse, Daniel},
year={2017},
pages={91 - 96},
address={Campinas, Brazil},
abstract={Two of the major issues in current computer systems are energy consumption and how to explore concurrent systems in a correct and efficient way. Solutions for these hazards may be sought both in hardware and in software. Phase-Change Memory (PCM) is a memory technology intended to replace DRAMs (Dynamic Random Access Memories) as the main memory, providing reduced static power consumption. Their main problem is related to write operations that are slow and wear their material. Transactional Memories are synchronization methods developed to reduce the limitations of lock-based synchronization. Their main advantages are related to being high-level and allowing composition and reuse of code, besides the absence of deadlocks. The objective of this study is to analyze the impact of different versioning managers (VMs) for transactional memories in PCMs. The lazy versioning/lazy acquisition scheme for version management presented the lowest wear on the PCM in 3 of 7 benchmarks analyzed, and results similar to the alternative versioning for the other 4~benchmarks. These results are related to the number of aborts of VMs, where this VM presents a much smaller number of aborts than the others, up to 39 times less aborts in the experiment with the benchmark Kmeans with 64 threads.<br/> &copy; 2017 IEEE.},
key={Dynamic random access storage},
keywords={Energy utilization;Green computing;Memory architecture;Phase change memory;Static random access storage;Storage allocation (computer);},
note={Dynamic random access memory;Lock-based synchronization;Memory hierarchy;Phase change memory (pcm);Software transactional memory;Static power consumption;Synchronization method;Transactional memory;},
URL={http://dx.doi.org/10.1109/SBAC-PADW.2017.24},
}


@article{20164502995638,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A fully-neoclassical finite-orbit-width version of the CQL3D Fokker-Planck code},
journal={Plasma Physics and Controlled Fusion},
author={Petrov, Yu.V. and Harvey, R.W.},
volume={58},
number={11},
year={2016},
issn={07413335},
abstract={The time-dependent bounce-averaged CQL3D flux-conservative finite-difference Fokker-Planck equation (FPE) solver has been upgraded to include finite-orbit-width (FOW) capabilities which are necessary for an accurate description of neoclassical transport, losses to the walls, and transfer of particles, momentum, and heat to the scrape-off layer. The FOW modifications are implemented in the formulation of the neutral beam source, collision operator, RF quasilinear diffusion operator, and in synthetic particle diagnostics. The collisional neoclassical radial transport appears naturally in the FOW version due to the orbit-averaging of local collision coefficients coupled with transformation coefficients from local (R, Z) coordinates along each guiding-center orbit to the corresponding midplane computational coordinates, where the FPE is solved. In a similar way, the local quasilinear RF diffusion terms give rise to additional radial transport of orbits. We note that the neoclassical results are obtained for 'full' orbits, not dependent on a common small orbit-width approximation. Results of validation tests for the FOW version are also presented.<br/> &copy; 2016 IOP Publishing Ltd.},
key={Fokker Planck equation},
keywords={Finite difference method;},
note={bounce-averaged;Collision coefficients;finite-orbit-width;Fokker Planck;Neoclassical transport;Particle diagnostics;Quasilinear diffusion;Transformation coefficients;},
URL={http://dx.doi.org/10.1088/0741-3335/58/11/115001},
}


@article{20143600049566,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={SMPLearner: learning to predict software maintainability},
journal={Automated Software Engineering},
author={Zhang, Wei and Huang, LiGuo and Ng, Vincent and Ge, Jidong},
volume={22},
number={1},
year={2014},
pages={111 - 141},
issn={09288910},
abstract={Accurate and practical software maintainability prediction enables organizations to effectively manage their maintenance resources and guide maintenance-related decision making. This paper presents SMPLearner, an automated learning-based approach to train maintainability predictors by harvesting the actual average maintenance effort computed from the code change history as well as employing a much richer set of 44 four-level hierarchical code metrics collected by static code analysis tools. We systematically evaluated SMPLearner on 150 observations partitioned from releases of eight large scale open source software systems. Our evaluation showed that SMPLearner not only outperformed the traditional 4-metric MI model but also the recent learning-based maintainability predictors constructed based on single Class-level metrics, demonstrating that single Class-level metrics were not sufficient for maintainability prediction.<br/> &copy; 2014, Springer Science+Business Media New York.},
key={Maintainability},
keywords={Codes (symbols);Computer software maintenance;Decision making;Forecasting;Learning algorithms;Learning systems;Object oriented programming;Open source software;Open systems;},
note={Automated learning;Code metrics;Maintenance efforts;Maintenance resources;Open source software systems;Software maintainability;Software metrices;Static code analysis tools;},
URL={http://dx.doi.org/10.1007/s10515-014-0161-3},
}


@inproceedings{20151200653061,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Explaining why methods change together},
journal={Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014},
author={Lozano, Angela and Noguera, Carlos and Jonckers, Viviane},
year={2014},
pages={185 - 194},
address={Victoria, BC, Canada},
abstract={By analyzing historical information from Source Code Management systems, previous research has observed that certain methods tend to change together consistently. Co-change has been identified as a good predictor of the entities that are likely to be affected by a change, which ones might be missing modifications, and which ones might change in the future. However, existing co-change analysis provides no insight on why methods consistently co-change. Being able to identify the rationale that explains co-changes could allow to document and enforce design knowledge. This paper proposes an automatic approach to derive the reason behind a co-change. We define the reason of a (set) of co-changes as a set of properties common to the elements that co-change. We consider two kinds of properties: structural properties which indicate explicit dependencies, and semantic properties which reveal implicit dependencies. Then we attempt to identify the reasons behind single commits, as well as the reasons behind co-changes that repeatedly affect the same set of methods. These sets of methods are identified by clustering methods that tend to be modified in the same commit-transactions. We perform our analysis over the history of two open-source systems, analyzing nearly 19.000 methods and over 3700 commits. We show that it is possible to automatically extract explanations for co-changes, that the quality of such explanations improves when structural and semantic properties are taken into account, and when the methods analyzed co-change recurrently.<br/> &copy; 2014 IEEE.},
key={Open systems},
keywords={Information management;Open source software;Semantics;},
note={Automatic approaches;Clustering methods;Empirical Software Engineering;Historical information;Open source system;Program comprehension;Semantic properties;Source code managements;},
URL={http://dx.doi.org/10.1109/SCAM.2014.27},
}


@article{20122715217319,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Assigning change requests to software developers},
journal={Journal of software: Evolution and Process},
author={Kagdi, Huzefa and Gethers, Malcom and Poshyvanyk, Denys and Hammad, Maen},
volume={24},
number={1},
year={2012},
pages={3 - 33},
issn={20477481},
abstract={The paper presents an approach to recommend a ranked list of expert developers to assist in the implementation of software change requests (e.g., bug reports and feature requests). An Information Retrieval (IR)-based concept location technique is first used to locate source code entities, e.g., files and classes, relevant to a given textual description of a change request. The previous commits from version control repositories of these entities are then mined for expert developers. The role of the IR method in selectively reducing the mining space is different from previous approaches that textually index past change requests and/or commits. The approach is evaluated on change requests from three open-source systems: ArgoUML, Eclipse, and KOffice, across a range of accuracy criteria. The results show that the overall accuracies of the correctly recommended developers are between 47 and 96% for bug reports, and between 43 and 60% for feature requests. Moreover, comparison results with two other recommendation alternatives show that the presented approach outperforms them with a substantial margin. Project leads or developers can use this approach in maintenance tasks immediately after the receipt of a change request in a free-form text. &copy; 2011 John Wiley &amp; Sons, Ltd.<br/>},
key={Program debugging},
keywords={Information retrieval;Open source software;Open systems;},
note={Developer recommendations;Feature location;Mining software repositories;Open source system;Software developer;Software evolution and maintenances;Source code entities;Textual description;},
URL={http://dx.doi.org/10.1002/smr.530},
}


@article{20170803371055,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={HOTB update: Parallel code for calculation of three- and four-particle harmonic oscillator transformation brackets and their matrices using OpenMP},
journal={Computer Physics Communications},
author={Germanas, D. and Stepys, A. and Mickeviius, S. and Kalinauskas, R.K.},
volume={215},
year={2017},
pages={259 - 264},
issn={00104655},
abstract={This is a new version of the HOTB code designed to calculate three and four particle harmonic oscillator (HO) transformation brackets and their matrices. The new version uses the OpenMP parallel communication standard for calculations of harmonic oscillator transformation brackets. A package of Fortran code is presented. Calculation time of large matrices, orthogonality conditions and array of coefficients can be significantly reduced using effective parallel code. Other functionalities of the original code (for example calculation of single harmonic oscillator brackets) have not been modified. New version program summary Program Title: HOTB_OpenMP Program Files doi:http://dx.doi.org/10.17632/ddjxc7rkpr.1 Licensing provisions: GPLv3 Programming language: Fortran 90 Journal reference of previous version: &nbsp;[2], [3], [4] Does the new version supersede the previous version?: Yes Reasons for the new version: Calculation of huge amount of HOB's, using single processor, takes much time. Speed-up is needed. The new program version allows to perform calculations on multiple processors using shared memory OpenMP API that reduces time needed for calculations and is easily implemented by end user. Nature of problem: Often solving nuclear structure and other problems huge amount of HOB's is needed. Calculation of matrices, orthogonality conditions and arrays of three-particle harmonic oscillator brackets (3HOB) and four-particle harmonic oscillator brackets (4HOB) using single processor for high values of harmonic oscillator quanta e takes much time. Solution method: Using OpenMP parallelization standard for the three and four-particle harmonics oscillator brackets 3HOB and 4HOB, based on methods, is presented in&nbsp;[2], [3], [4]. Summary of revisions: Subroutines for calculation arrays of HOB's, their matrices and orthogonality conditions are rewritten to use OpenMP parallel programming standard. 1. Additional features of new code HOTB_OpenMP: (1) Calculation time. As the code of previous version of program was redone using parallelism paradigma, it is now possible to reduce the calculation time of transformation matrices significantly, depending on the number of processors (cores), as the dimensions of matrices are growing very rapidly according to the energy and momenta values.2. Modifications or corrections to previous versions: Updated program HOTB_OpenMP is written in the FORTRAN 90 language, according to formulas described in&nbsp;[1], [2], [3], [4]. There is one file: HOTB_OpenMP.f90. Detailed descriptions of internal parameters used by subroutines are represented in&nbsp;[1] for 3HOB,&nbsp;[2] for 4HOB,&nbsp;[3] for 4HOB for special cases of the parameters and&nbsp;[4] for 4HOB for matrix M method and also are located in files read.me and appendix.pdf. File read.me also contains description of example input and output files. Main computational subroutines using OpenMP: (1) subroutineall_3HOB_OpenMPdimens bus_3HOB array_3HOB. Performs parallel calculations of all 3HOB brackets for given e (total harmonic oscillator energy). Other input parameters: dimens&mdash;dimension of the problem, bus_3HOB&mdash;array of quantum numbers. The return value is array_3HOB&mdash;array of 3HOB brackets.(2) subroutineall_4HOB_OpenMPdimens bus_4HOB array_4HOB. Performs parallel calculations of all 4HOB brackets for given e (total HO energy). Other input parameters: dimens&mdash;dimension of the problem, bus_4HOB&mdash;array of quantum numbers. The return value is array_4HOB&mdash;array of 4HOB brackets.(3) subroutineorthogonality_3HOB_OpenMPpaklaida nk dimens bus_3HOB. Performs parallel calculations of orthogonality of 3HOB matrix using OpenMP for given e (total HO energy). Other input parameters: dimens&mdash;dimension of the problem, bus_3HOB&mdash;array of quantum numbers. The return values are paklaida&mdash;error of the problem and nk&mdash;amount of coefficients (equation (43) in&nbsp;[1]).(4) subroutineorthogonality_4HOB_OpenMPpaklaida nk dimens bus_4HOB. Performs parallel calculations of orthogonality of 4HOB matrix using OpenMP for given e (total HO energy). Other input parameters: dimens&mdash;dimension of the problem, bus_4HOB&mdash;array of quantum numbers. The return values are paklaida&mdash;error of the problem and nk&mdash;amount of coefficients (equation (12) in&nbsp;[2]).(5) subroutinematrix_3HOB_OpenMPl dimens bus_3HOB matr_3HOB. Performs parallel calculations of 3HOB matrix using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens&mdash;dimension of the problem, bus_3HOB&mdash;array of quantum numbers. The return value is matr_3HOB&mdash;matrix of 3HOB coefficients.(6) subroutinematrix_4HOB_OpenMPl dimens bus_4HOB matr_4HOB. Performs parallel calculations of 4HOB matrix using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens&mdash;dimension of the problem, bus_4HOB&mdash;array of quantum numbers. The return value is matr_4HOB&mdash;matrix of 4HOB coefficients.(7) subroutinematrix_4HOB_d_0_OpenMPl dimens bus_4HOB matr_4HOB. Performs parallel calculations of 4HOB matrix with parameter d&nbsp;=&nbsp;0 using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens&mdash;dimension of the problem, bus_4HOB&mdash;array of quantum numbers. The return value is matr_4HOB&mdash;matrix of 4HOB coefficients.(8) subroutinematrix_4HOB_d_infinity_OpenMPl dimens bus_4HOB matr_4HOB. Performs parallel calculations of 4HOB matrix with parameter d&nbsp;=&nbsp;infinity using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens&mdash;dimension of the problem, bus_4HOB&mdash;array of quantum numbers. The return value is matr_4HOB&mdash;matrix of 4HOB coefficients.(9) subroutinematrix_4HOB_d1_0_OpenMPl dimens bus_4HOB matr_4HOB. Performs parallel calculations of 4HOB matrix with parameter d1&nbsp;=&nbsp;0 using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens&mdash;dimension of the problem, bus_4HOB&mdash;array of quantum numbers. The return value is matr_4HOB&mdash;matrix of 4HOB coefficients.(10) subroutinematrix_M_OpenMPl dimens bus_4HOB matr_M. Performs parallel calculations of M matrix using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens&mdash;dimension of the problem, bus_4HOB&mdash;array of quantum numbers. The return value is matr_4HOB&mdash;matrix of 4HOB coefficients.(11) subroutinematrix_4HOB_M_OpenMPl dimens bus_4HOB matr_4HOB_M. Performs parallel calculations of 4HOB matrix using M technique and OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens&mdash;dimension of the problem, bus_4HOB&mdash;array of quantum numbers. The return value is matr_4HOB&mdash;matrix of 4HOB coefficients.3. Additional non-parallel subroutines, needed main computational subroutines to run: (1) subroutineall_3HOB_dimensione dimens. Outputs number of all 3HOB brackets for given e (total HO energy). The return value is dimens&mdash;dimension of the problem.(2) subroutinearray_quantum_numbers_all_3HOBe dimens bus_3HOB. Fills array of quantum numbers for all 3HOB brackets for given e (total HO energy). Other input parameters: dimens&mdash;dimension of the problem. The return value is bus_3HOB&mdash;array of quantum numbers.(3) subroutineall_4HOB_dimensione dimens. Outputs number of all 4HOB brackets for given e (total HO energy). The return value is dimens&mdash;dimension of the problem.(4) subroutinearray_quantum_numbers_all_4HOBe dimens bus_4HOB. Fills array of quantum numbers for all 4HOB brackets for given e (total HO energy). Other input parameters: dimens&mdash;dimension of the problem. The return value is bus_4HOB&mdash;array of quantum numbers.(5) subroutineort_3HOB_dimensione dimens. Calculates number of 3HOB coefficients for orthogonality test for given e (total HO energy). The return value is dimens&mdash;dimension of the problem.(6) subroutinearray_quantum_numbers_ort_3HOBe dimens bus_3HOB. Fills array of quantum numbers for 3HOB orthogonality test for given e (total HO energy). Other input parameters: dimens&mdash;dimension of the problem. The return value is bus_3HOB&mdash;array of quantum numbers.(7) subroutineort_4HOB_dimensione dimens. Calculates number of 4HOB coefficients for orthogonality test for given e (total HO energy). The return value is dimens&mdash;dimension of the problem.(8) subroutinearray_quantum_numbers_ort_4HOBe dimens bus_4HOB. Fills array of quantum numbers for 4HOB orthogonality test for given e (total HO energy). Other input parameters: dimens&mdash;dimension of the problem. The return value is bus_4HOB&mdash;array of quantum numbers.(9) subroutinematrix_3HOB_dimensione l dimens. Calculates dimension of 3HOB matrix for given e (total HO energy) and l (total angular momentum). The return value is dimens&mdash;dimension of the problem.(10) subroutinearray_quantum_numbers_matrix_3HOBe l dimens bus_3HOB. Fills array of quantum numbers for matrix 3HOB for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens&mdash;dimension of the problem. The return value is bus_3HOB&mdash;array of quantum numbers.(11) subroutinematrix_4HOB_dimensione l dimens. Calculates dimension of 4HOB matrix for given e (total HO energy) and l (total angular momentum). The return value is dimens&mdash;dimension of the problem.(12) subroutinearray_quantum_numbers_matrix_4HOBe l dimens bus_4HOB. Fills array of quantum numbers for matrix 4HOB for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens&mdash;dimension of the problem. The return value is bus_4HOB&mdash;array of quantum numbers.(13) subroutinehob_output_to_filearray_xHOB dimens name bus_xHOB n e. Writes HOB's array array_xHOB to file. Other input parameters: dimens&mdash;dimension of the problem,&nbsp; bus_xHOB&mdash;array of quantum numbers, n&mdash;parameter, that equals 9 in case of 3HOB and equals 17 in case of 4HOB, e&mdash;harmonic oscillator quanta. The return value is text file that name is value of the variable name.(14) subroutineort_output_to_filepaklaida dimens nk name n e. Writes orthogonality test results to file. Input parameters: paklaida&mdash;error of the problem, dimens&mdash;dimension of the problem, nk&mdash;amount of coefficients, n&mdash;parameter, that equals 1 in case of 3HOB and equals 2 in case of 4HOB, e&mdash;harmonic oscillator quanta. The return value is text file that name is value of the variable name.(15) subroutinematrix_output_to_filematrix dimens name bus_xHOB n e l. Writes HOB's matrix matrix to file. Other input parameters: dimens&mdash;dimension of the problem, bus_xHOB&mdash;array of quantum numbers, n&mdash;parameter, that equals 4 in case of 3HOB and equals 8 in case of 4HOB, e&mdash;harmonic oscillator quanta, l&mdash;total angular momentum. The return value is text file that name is value of the variable name.4. Additional subroutines for user conveniency: (1) subroutinesingle_3HOB Computes single 3HOB for given set of quantum numbers.(2) subroutinesingle_4HOB Computes single 4HOB for given set of quantum numbers.(3) subroutinesingle_4HOB_d_0 Computes single 4HOB for given set of quantum numbers, case d&nbsp;=&nbsp;0.(4) subroutinesingle_4HOB_d_infinity Computes single 4HOB for given set of quantum numbers, case d&nbsp;=&nbsp;infinity.(5) subroutinesingle_4HOB_d1_0 Computes single 4HOB for given set of quantum numbers, case d1&nbsp;=&nbsp;0.For illustrational purposes we have made some calculations for all HOB's, their orthogonality conditions and matrices. Calculations were done on 12 cores 96 GB RAM computer. Calculation time is OpenMP wall-time function omp_get_wtime. Results are presented in Tables&nbsp;1&ndash;6. Additional comments including Restrictions and Unusual features: Calculations can be done up to harmonic oscillator (HO) energy quanta e&nbsp;=&nbsp;28 for double precision (presented here) version. Acknowledgments Computations were performed using resources at the High Performance Computing Center &ldquo;HPC Saultekis&rdquo; in Vilnius University's Faculty of Physics. [1] G.P. Kamuntavi&#269;ius, R.K. Kalinauskas, B.R. Barrett, S. Mickevi&#269;ius, D. Germanas, The general harmonic-oscillator brackets: compact expression, symmetries, sums and Fortran code, Nucl. Phys. A 695(2001) 191-201.[2] D. Germanas, R.K. Kalinauskas, S. Mickevi&#269;ius, Calculation of four-particle harmonic-oscillator transformation brackets, Computer Physics Communications 181(2010) 420-425.[3] S. Mickevi&#269;ius, E. Brazauskas, D. Germanas, R.K. Kalinauskas, The four-particle harmonic-oscillator brackets: Compact expressions and updated Fortran program, Computer Physics Communications 182(2011) 1377-1381.[4] S. Mickevi&#269;ius, D. Germanas, R.K. Kalinauskas, Revised calculation of four-particle harmonic-oscillator transformation brackets matrix, Computer Physics Communications 184(2013) 409-413.<br/> &copy; 2017 Elsevier B.V.},
key={Matrix algebra},
keywords={Angular momentum;Application programming interfaces (API);Codes (symbols);Errors;Fasteners;FORTRAN (programming language);Harmonic analysis;Linear transformations;Oscillators (mechanical);Parallel processing systems;Parallel programming;Problem oriented languages;Problem solving;Program processors;Quantum theory;Random access storage;Subroutines;Testing;},
note={Algebraic method;High performance computing;Mathematical methods in physics;Nuclear shell model;OpenMP-parallelization;Orthogonality conditions;Transformation brackets;Transformation matrices;},
URL={http://dx.doi.org/10.1016/j.cpc.2017.01.028},
}


@article{20150500461428,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Enhancing frequency based change proneness prediction method using artificial bee colony algorithm},
journal={Advances in Intelligent Systems and Computing},
author={Godara, Deepa and Singh, Rakesh Kumar},
volume={320},
year={2015},
pages={535 - 543},
issn={21945357},
abstract={In the field of software engineering, during the development of Object Oriented (OO) software, the knowledge of the classes which are more prone to changes in software is an important problem that arises nowadays. In order to solve this problem, several methods were introduced by predicting the changes in the software earlier. But those methods are not facilitating very good prediction result. This research work proposes a novel approach for predicting changes in software. Our proposed probabilistic approach uses the behavioral dependency generated from UML diagrams, as well as other code metrics such as time and trace events generated from source code. These measures combined with frequency of method calls and popularity can be used in automated manner to predict a change prone class. Thus all these five features (time, trace events, behavioral dependency, frequency and popularity) are obtained from our proposed work. Then, these features are given as the input to the ID3 (Interactive Dichotomizer version 3) decision tree algorithm for effectively classifying the classes, whether it predicts the change proneness or not. If a class is classified into prediction of change prone class, then the value of change proneness is also obtained by our work.<br/> &copy; Springer International Publishing Switzerland 2015.},
key={Object oriented programming},
keywords={Data mining;Decision trees;Engineering research;Evolutionary algorithms;Forecasting;Optimization;Software engineering;},
note={Artificial bee colony algorithms;Change proneness;Code metrics;Decision-tree algorithm;Object oriented software;Prediction methods;Probabilistic approaches;UML diagrams;},
URL={http://dx.doi.org/10.1007/978-3-319-11218-3_48},
}


@article{20143618140610,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={HOTB: High precision parallel code for calculation of four-particle harmonic oscillator transformation brackets},
journal={Computer Physics Communications},
author={Stepys, A. and Mickevicius, S. and Germanas, D. and Kalinauskas, R.K.},
volume={185},
number={11},
year={2014},
pages={3062 - 3064},
issn={00104655},
abstract={This new version of the HOTB program for calculation of the three and four particle harmonic oscillator transformation brackets provides some enhancements and corrections to the earlier version (Germanas et al., 2010) [1]. In particular, new version allows calculations of harmonic oscillator transformation brackets be performed in parallel using MPI parallel communication standard. Moreover, higher precision of intermediate calculations using GNU Quadruple Precision and arbitrary precision library FMLib [2] is done. A package of Fortran code is presented. Calculation time of large matrices can be significantly reduced using effective parallel code. Use of Higher Precision methods in intermediate calculations increases the stability of algorithms and extends the validity of used algorithms for larger input values. &copy; 2014 Elsevier B.V. All rights reserved.<br/>},
key={Codes (symbols)},
keywords={Algebra;Fasteners;Harmonic analysis;Oscillators (mechanical);},
note={Algebraic method;Arbitrary precision;Harmonic oscillators;Mathematical methods in physics;Nuclear shell model;Parallel communication;Quadruple precision;Transformation brackets;},
URL={http://dx.doi.org/10.1016/j.cpc.2014.06.025},
}


@inproceedings{20170803366607,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Recommending code changes for automatic backporting of linux device drivers},
journal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
author={Thung, Ferdian and Le, Xuan-Bach D. and Lo, David and Lawall, Julia},
year={2016},
pages={222 - 232},
address={Raleigh, NC, United states},
abstract={Device drivers are essential components of any operating system (OS). They specify the communication protocol that allows the OS to interact with a device. However, drivers for new devices are usually created for a specific OS version. These drivers often need to be backported to the older versions to allow use of the new device. Backporting is often done manually, and is tedious and error prone. To alleviate this burden on developers, we propose an automatic recommendation system to guide the selection of backporting changes. Our approach analyzes the version history for cues to recommend candidate changes. We have performed an experiment on 100 Linux driver files and have shown that we can give a recommendation containing the correct backport for 68 of the drivers. For these 68 cases, 73.5%, 85.3%, and 88.2% of the correct recommendations are located in the Top-1, Top-2, and Top-5 positions of the recommendation lists respectively. The successful cases cover various kinds of changes including change of record access, deletion of function argument, change of a function name, change of constant, and change of if condition. Manual investigation of failed cases highlights limitations of our approach, including inability to infer complex changes, and unavailability of relevant cues in version history.<br/> &copy; 2016 IEEE.},
key={Recommender systems},
keywords={Computer software maintenance;Linux;},
note={Backporting;Code changes;Device Driver;Error prones;Linux drivers;New devices;},
URL={http://dx.doi.org/10.1109/ICSME.2016.71},
}


@inproceedings{20162502509859,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A learning algorithm for change impact prediction},
journal={Proceedings - 5th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2016},
author={Musco, Vincenzo and Carette, Antonin and Monperrus, Martin and Preux, Philippe},
year={2016},
pages={8 - 14},
address={Austin, TX, United states},
abstract={Change impact analysis (CIA) consists in predicting the impact of a code change in a software application. In this paper, the artifacts that are considered for CIA are methods of object-oriented software; the change under study is a change in the code of the method, the impact is the test methods that fail because of the change that has been performed. We propose LCIP, a learning algorithm that learns from past impacts to predict future impacts. To evaluate LCIP, we consider Java software applications that are strongly tested. We simulate 6000 changes and their actual impact through code mutations, as done in mutation testing. We find that LCIP can predict the impact with a precision of 74%, a recall of 85%, corresponding to a F-score of 64%. This shows that taking a learning perspective on change impact analysis let us achieve good precision and recall in change impact analysis.<br/> &copy; 2016 Copyright held by the owner/author(s).},
key={Learning algorithms},
keywords={Application programs;Artificial intelligence;Codes (symbols);Forecasting;Object oriented programming;Software testing;},
note={Change impact analysis;Change impacts;Code mutation;Java software;Mutation testing;Object oriented software;Precision and recall;Software applications;},
URL={http://dx.doi.org/10.1145/2896995.2896996},
}


@inproceedings{20123115288106,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Integrated impact analysis for managing software changes},
journal={Proceedings - International Conference on Software Engineering},
author={Gethers, Malcom and Dit, Bogdan and Kagdi, Huzefa and Poshyvanyk, Denys},
year={2012},
pages={430 - 440},
issn={02705257},
address={Zurich, Switzerland},
abstract={The paper presents an adaptive approach to perform impact analysis from a given change request to source code. Given a textual change request (e.g., a bug report), a single snapshot (release) of source code, indexed using Latent Semantic Indexing, is used to estimate the impact set. Should additional contextual information be available, the approach configures the best-fit combination to produce an improved impact set. Contextual information includes the execution trace and an initial source code entity verified for change. Combinations of information retrieval, dynamic analysis, and data mining of past source code commits are considered. The research hypothesis is that these combinations help counter the precision or recall deficit of individual techniques and improve the overall accuracy. The tandem operation of the three techniques sets it apart from other related solutions. Automation along with the effective utilization of two key sources of developer knowledge, which are often overlooked in impact analysis at the change request level, is achieved. To validate our approach, we conducted an empirical evaluation on four open source software systems. A benchmark consisting of a number of maintenance issues, such as feature requests and bug fixes, and their associated source code changes was established by manual examination of these systems and their change history. Our results indicate that there are combinations formed from the augmented developer contextual information that show statistically significant improvement over standalone approaches. &copy; 2012 IEEE.<br/>},
key={Open source software},
keywords={Codes (symbols);Computer programming languages;Data mining;Open systems;Semantics;},
note={Contextual information;Empirical evaluations;Latent Semantic Indexing;Manual examination;Open source software systems;Overall accuracies;Source code changes;Source code entities;},
URL={http://dx.doi.org/10.1109/ICSE.2012.6227172},
}


@inproceedings{20163902831984,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Software Defect Prediction Using Semi-Supervised Learning with Change Burst Information},
journal={Proceedings - International Computer Software and Applications Conference},
author={He, Qing and Shen, Beijun and Chen, Yuting},
volume={1},
year={2016},
pages={113 - 122},
issn={07303157},
address={Atlanta, GA, United states},
abstract={Software defect prediction is an important software quality assurance technique. It utilizes historical project data and previously discovered defects to predict potential defects. However, most of existing methods assume that large amounts of labeled historical data are available for prediction, while in the early stage of the life cycle, projects may lack the data needed for building such predictors. In addition, most of existing techniques use static code metrics as predictors, while they omit change information that may introduce risks into software development. In this paper, we take these two issues into consideration, and propose a semi-supervised based defect prediction approach - extRF. extRF extends the classical supervised Random Forest algorithm by self-training paradigm. It also employs change burst information for improving accuracy of software defect prediction. We also conduct an experiment to evaluate extRF against three other supervised machine learners (i.e. Logistic Regression, Na&iuml;ve Bayes, Random Forest) and compare the effectiveness of code metrics, change burst metrics, and a combination of them. Experimental results show that extRF trained with a small size of labeled dataset achieves comparable performance to some supervised learning approaches trained with a larger size of labeled dataset. When only 2% of Eclipse 2.0 data are used for training, extRF can achieve F-measure about 0:562, approximate to that of LR (a supervised learning approach) at labeled sampling rate of 50%. Besides, change burst metrics outperform code metrics in that F-measure rises to a peak value of 0:75 for Eclipse 3.0 and JDT.Core.<br/> &copy; 2016 IEEE.},
key={Supervised learning},
keywords={Application programs;Codes (symbols);Computer software selection and evaluation;Decision trees;Defects;Forecasting;Life cycle;Quality assurance;Software design;},
note={Change Metrics;Defect prediction;Logistic regressions;Random forest algorithm;Semi- supervised learning;Software defect prediction;Software quality assurance;Supervised learning approaches;},
URL={http://dx.doi.org/10.1109/COMPSAC.2016.193},
}


@inproceedings{20124315594818,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Characterizing the roles of classes and their fault-proneness through change metrics},
journal={International Symposium on Empirical Software Engineering and Measurement},
author={Steff, Maximilian and Russo, Barbara},
year={2012},
pages={59 - 68},
issn={19493770},
address={Lund, Sweden},
abstract={Many approaches to determine the fault-proneness of code artifacts rely on historical data of and about these artifacts. These data include the code and how it was changed over time, and information about the changes from version control systems. Each of these can be considered at different levels of granularity. The level of granularity can substantially influence the estimated fault-proneness of a code artifact. Typically, the level of detail oscillates between releases and commits on the one hand, and single lines of code and whole files on the other hand. Not every information may be readily available or feasible to collect at every level, though, nor does more detail necessarily improve the results. Our approach is based on time series of changes in method-level dependencies and churn on a commit-to-commit basis for two systems, Spring and Eclipse. We identify sets of classes with distinct properties of the time series of their change histories. We differentiate between classes based on temporal patterns of change. Based on this differentiation, we show that our measure of structural change in concert with its complement, churn, effectively indicates fault-proneness in classes. We also use windows on time series to select sets of commits and show that changes over short amounts of time do effectively indicate the fault-proneness of classes. Copyright 2012 ACM.<br/>},
key={Codes (symbols)},
keywords={Software architecture;Software engineering;Time series;},
note={Change history;Fault proneness;Historical data;Level of detail;Lines of code;Product metrics;Temporal pattern;Version control system;},
URL={http://dx.doi.org/10.1145/2372251.2372261},
}


@inproceedings{20123415364448,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={MINCE: Mining change history of Android project},
journal={IEEE International Working Conference on Mining Software Repositories},
author={Sinha, Vibha Singhal and Mani, Senthil and Gupta, Monika},
year={2012},
pages={132 - 135},
issn={21601852},
address={Zurich, Switzerland},
abstract={An analysis of commit history of Android reveals that Android has a code base of 550K files, where on an average each file has been modified 8.7 times. 41% of files have been modified at-least once. In terms of contributors, it has an overall contributor community of 1563, with 58.5% of them having made &gt; 5 commits. Moreover, the contributor community shows high churn levels, with only 13 of contributors continuing from 2005 to 2011. In terms of industry participation, Google &amp; Android account for 22% of developers. Intel and RedHat account for 2% of contributors each and IBM, Oracle, TI, SGI account for another 1% each. Android code can be classified into 5 sub-projects: kernel, platform, device, tools and toolchain. In this paper, we profile each of these sub-projects in terms of change volumes, contributor and industry participation. We further picked specific framework topics such as UI, security, whose understanding is required from perspective of developing apps over Android, and present some insights on community participation around the same. &copy; 2012 IEEE.<br/>},
key={Android (operating system)},
note={Change history;Change volumes;Commit history;Community participation;Industry participations;},
URL={http://dx.doi.org/10.1109/MSR.2012.6224271},
}


@article{20152000854371,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Improving multi-objective code-smells correction using development history},
journal={Journal of Systems and Software},
author={Ouni, Ali and Kessentini, Marouane and Sahraoui, Houari and Inoue, Katsuro and Hamdi, Mohamed Salah},
volume={105},
year={2015},
pages={18 - 39},
issn={01641212},
abstract={One of the widely used techniques to improve the quality of software systems is refactoring. Software refactoring improves the internal structure of the system while preserving its external behavior. These two concerns drive the existing approaches to refactoring automation. However, recent studies demonstrated that these concerns are not enough to produce correct and consistent refactoring solutions. In addition to quality improvement and behavior preservation, studies consider, among others, construct semantics preservation and minimization of changes. From another perspective, development history was proven as a powerful source of knowledge in many maintenance tasks. Still, development history is not widely explored in the context of automated software refactoring. In this paper, we use the development history collected from existing software projects to propose new refactoring solutions taking into account context similarity with situations seen in the past. We propose a multi-objective optimization-based approach to find good refactoring sequences that (1) minimize the number of code-smells, and (2) maximize the use of development history while (3) preserving the construct semantics. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between these three objectives. We evaluate our approach using a benchmark composed of five medium and large-size open-source systems and four types of code-smells (Blob, spaghetti code, functional decomposition, and data class). Our experimental results show the effectiveness of our approach, compared to three different state-of-the-art approaches, with more than 85% of code-smells fixed and 86% of suggested refactorings semantically coherent when the change history is used.<br/>},
key={Open systems},
keywords={Codes (symbols);Digital storage;Economic and social effects;Genetic algorithms;Multiobjective optimization;Odors;Open source software;Semantics;},
note={Code smell;Functional decomposition;Non dominated sorting genetic algorithm (NSGA II);Quality of softwares;Refactorings;Search-based software engineering;Software refactoring;State-of-the-art approach;},
URL={http://dx.doi.org/10.1016/j.jss.2015.03.040},
}


@inproceedings{20152100879327,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An empirical study of the relation between strong change coupling and defects using history and social metrics in the apache aries project},
journal={IFIP Advances in Information and Communication Technology},
author={Wiese, Igor Scaliante and Kuroda, Rodrigo Takashi and Re, Reginaldo and Oliva, Gustavo Ansaldi and Gerosa, Marco Aurelio},
volume={451},
year={2015},
pages={3 - 12},
issn={18684238},
address={Florence, Italy},
abstract={Change coupling is an implicit relationship observed when artifacts change together during software evolution. The literature leverages change coupling analysis for several purposes. For example, researchers discovered that change coupling is associated with software defects and reveals relationships between software artifacts that cannot be found by scanning code or documentation. In this paper, we empirically investigate the strongest change couplings from the Apache Aries project to characterize and identify their impact in software development. We used historical and social metrics collected from commits and issue reports to build classification models to identify strong change couplings. Historical metrics were used because change coupling is a phenomenon associated with recurrent co-changes found in the software history. In turn, social metrics were used because developers often interact with each other in issue trackers to accomplish the tasks. Our classification models showed high accuracy, with 70-99% F-measure and 88-99% AUC. Using the same set of metrics, we also predicted the number of future defects for the artifacts involved in strong change couplings. More specifically, we were able to predict 45.7% of defects where these strong change couplings reoccurred in the post-release. These findings suggest that developers and projects managers should detect and monitor strong change couplings, because they can be associated with defects and tend to happen again in the subsequent release.<br/> &copy; IFIP International Federation for Information Processing 2015.},
key={Open systems},
keywords={Couplings;Defects;Open source software;Software design;},
note={Change couplings;Classification models;Empirical studies;Implicit relationships;Software artifacts;Software defects;Software Evolution;Software history;},
URL={http://dx.doi.org/10.1007/978-3-319-17837-0_1},
}


@inproceedings{20161302174284,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Code Bad Smell Detection through Evolutionary Data Mining},
journal={International Symposium on Empirical Software Engineering and Measurement},
author={Fu, Shizhe and Shen, Beijun},
volume={2015-November},
year={2015},
pages={41 - 49},
issn={19493770},
address={Beijing, China},
abstract={The existence of code bad smell has a severe impact on the software quality. Numerous researches show that ignoring code bad smells can lead to failure of a software system. Thus, the detection of bad smells has drawn the attention of many researchers and practitioners. Quite a few approaches have been proposed to detect code bad smells. Most approaches are solely based on structural information extracted from source code. However, we have observed that some code bad smells have the evolutionary property, and thus propose a novel approach to detect three code bad smells by mining software evolutionary data: duplicated code, shotgun surgery, and divergent change. It exploits association rules mined from change history of software systems, upon which we define heuristic algorithms to detect the three bad smells. The experimental results on five open source projects demonstrate that the proposed approach achieves higher precision, recall and F-measure.<br/> &copy; 2015 IEEE.},
key={Open source software},
keywords={Codes (symbols);Computer software selection and evaluation;Data mining;Heuristic algorithms;Odors;},
note={Bad smells;Evolutionary data minings;Evolutionary history;Mining software;Open source projects;Software Quality;Software systems;Structural information;},
URL={http://dx.doi.org/10.1109/ESEM.2015.7321194},
}


@inproceedings{20185106254772,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The case for adaptive change recommendation},
journal={Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018},
author={Pugh, Sydney and Binkley, David and Moonen, Leon},
year={2018},
pages={129 - 138},
address={Madrid, Spain},
abstract={As the complexity of a software system grows, it becomes increasingly difficult for developers to be aware of all the dependencies that exist between artifacts (e.g., files or methods) of the system. Change impact analysis helps to overcome this problem, as it recommends to a developer relevant source-code artifacts related to her current changes. Association rule mining has shown promise in determining change impact by uncovering relevant patterns in the system's change history. State-of-the-art change impact mining algorithms typically make use of a change history of tens of thousands of transactions. For efficiency, targeted association rule mining focuses on only those transactions potentially relevant to answering a particular query. However, even targeted algorithms must consider the complete set of relevant transactions in the history. This paper presents ATARI, a new adaptive approach to association rule mining that considers a dynamic selection of the relevant transactions. It can be viewed as a further constrained version of targeted association rule mining, in which as few as a single transaction might be considered when determining change impact. Our investigation of adaptive change impact mining empirically studies seven algorithm variants. We show that adaptive algorithms are viable, can be just as applicable as the start-of-the-art complete-history algorithms, and even outperform them for certain queries. However, more important than the direct comparison, our investigation lays necessary groundwork for the future study of adaptive techniques and their application to challenges such as the on-the-fly style of impact analysis that is needed at the GitHub-scale.<br/> &copy; 2018 IEEE.},
key={Data mining},
keywords={Adaptive algorithms;Association rules;Codes (symbols);Computer programming languages;},
note={Adaptive approach;Adaptive technique;Change impact analysis;Dynamic selection;Mining algorithms;Relevant patterns;Software systems;State of the art;},
URL={http://dx.doi.org/10.1109/SCAM.2018.00022},
}


@inproceedings{20182605356678,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Transform-data-by-example (TDE): Extensible data transformation in excel},
journal={Proceedings of the ACM SIGMOD International Conference on Management of Data},
author={He, Yeye and Ganjam, Kris and Lee, Kukjin and Wang, Yue and Narasayya, Vivek and Chaudhuri, Surajit and Chu, Xu and Zheng, Yudian},
year={2018},
pages={1785 - 1788},
issn={07308078},
address={Houston, TX, United states},
abstract={Business analysts and data scientists today increasingly need to clean, standardize and transform diverse data sets, such as name, address, date time, phone number, etc., before they can perform analysis. These ad-hoc transformation problems are typically solved by one-off scripts, which is both difficult and time-consuming. Our observation is that these domain-specific transformation problems have long been solved by developers with code libraries, which are often shared in places like GitHub. We thus develop an extensible data transformation system called Transform-Data-by-Example (TDE) that can leverage rich transformation logic in source code, DLLs, web services and mapping tables, so that end-users only need to provide a few (typically 3) input/output examples, and TDE can synthesize desired programs using relevant transformation logic from these sources. The beta version of TDE was released in Office Store for Excel.<br/> &copy; 2018 Association for Computing Machinery.},
key={Metadata},
keywords={Web services;},
note={Business analysts;Code libraries;Data transformation;Diverse data sets;Domain specific;Input/output;Phone number;Source codes;},
URL={http://dx.doi.org/10.1145/3183713.3193539},
}


@inproceedings{20173404067391,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Machine Learning-Based Detection of Open Source License Exceptions},
journal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017},
author={Vendome, Christopher and Linares-Vasquez, Mario and Bavota, Gabriele and Di Penta, Massimiliano and German, Daniel},
year={2017},
pages={118 - 129},
address={Buenos Aires, Argentina},
abstract={From a legal perspective, software licenses govern the redistribution, reuse, and modification of software as both source and binary code. Free and Open Source Software (FOSS) licenses vary in the degree to which they are permissive or restrictive in allowing redistribution or modification under licenses different from the original one(s). In certain cases, developers may modify the license by appending to it an exception to specifically allow reuse or modification under a particular condition. These exceptions are an important factor to consider for license compliance analysis since they modify the standard (and widely understood) terms of the original license. In this work, we first perform a large-scale empirical study on the change history of over 51K FOSS systems aimed at quantitatively investigating the prevalence of known license exceptions and identifying new ones. Subsequently, we performed a study on the detection of license exceptions by relying on machine learning. We evaluated the license exception classification with four different supervised learners and sensitivity analysis. Finally, we present a categorization of license exceptions and explain their implications.<br/> &copy; 2017 IEEE.},
key={Copyrights},
keywords={Artificial intelligence;Classifiers;Computer software reusability;Learning systems;Open source software;Open systems;Regulatory compliance;Sensitivity analysis;},
note={Change history;Empirical studies;Free and open source softwares;License compliances;On-machines;Open source license;Particular condition;Software license;},
URL={http://dx.doi.org/10.1109/ICSE.2017.19},
}


@inproceedings{20165303206297,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={TrueGrid: Code the table, tabulate the data},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Hermans, Felienne and van der Storm, Tijs},
volume={9946 LNCS},
year={2016},
pages={388 - 393},
issn={03029743},
address={Vienna, Austria},
abstract={Spreadsheet systems are live programming environments. Both the data and the code are right in front you, and if you edit either of them, the effects are immediately visible. Unfortunately, spreadsheets lack mechanisms for abstraction, such as classes, function definitions etc. Programming languages excel at abstraction, but most mainstream languages or integrated development environments (IDEs) do not support the interactive, live feedback loop of spreadsheets. As a result, exploring and testing of code is cumbersome and indirect. In this paper we propose a method to bring both worlds closer together, by juxtaposing ordinary code and spreadsheet-like grids in the IDE, called TrueGrid. Using TrueGrid spreadsheet cells can be programmed with a fully featured programming language. Spreadsheet users then may enjoy benefits of source code, including added abstractions, syntax highlighting, version control, etc. On the other hand, programmers may leverage the grid for interactive exploring and testing of code. We illustrate these benefits using a prototype implementation of True- Grid that runs in the browser and uses Javascript as a programming language.<br/> &copy; Springer International Publishing AG 2016.},
key={Codes (symbols)},
keywords={Abstracting;Application programs;Computation theory;Computer programming languages;Formal methods;Spreadsheets;Verification;},
note={Feed-back loop;Function definitions;Integrated development environment;Javascript;Programming environment;Prototype implementations;Source codes;Version control;},
URL={http://dx.doi.org/10.1007/978-3-319-50230-4_29},
}


@article{20122315090722,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The effects of thermistor linearization techniques on the T-history characterization of phase change materials},
journal={Applied Thermal Engineering},
author={Stankovic, Stanislava B. and Kyriacou, Panayiotis A.},
volume={44},
year={2012},
pages={78 - 84},
issn={13594311},
abstract={Phase Change Materials (PCMs) are increasingly being used in the area of energy sustainability. Thermal characterization is a prerequisite for any reliable utilization of these materials. Current characterization methods including the well-known T-history method depend on accurate temperature measurements. This paper investigates the impact of different thermistor linearization techniques on the temperature uncertainty in the T-history characterization of PCMs. Thermistor sensors and two linearization techniques were evaluated in terms of achievable temperature accuracy through consideration of both, non-linearity and self-heating errors. T-history measurements of RT21 (RUBITHERM<sup>&reg;</sup>GmbH) PCM were performed. Temperature measurement results on the RT21 sample suggest that the Serial-Parallel Resistor (SPR)<sup>1</sup>linearization technique gives better uncertainty (less than &plusmn;0.1&deg;C) in comparison with the Wheatstone Bridge (WB)<sup>1</sup>technique (up to &plusmn;1.5&deg;C). These results may considerably influence the usability of latent heat storage density of PCMs in the certain temperature range. They could also provide a solid base for the development of a T-history measuring device. &copy; 2012 Elsevier Ltd. All rights reserved.<br/>},
key={Phase change materials},
keywords={Characterization;Heat storage;Linearization;Pulse code modulation;Temperature measurement;Thermistors;Uncertainty analysis;},
note={Characterization methods;Energy sustainability;Heat density;Linearization technique;T-history method;Temperature accuracies;Temperature uncertainties;Thermal characterization;},
URL={http://dx.doi.org/10.1016/j.applthermaleng.2012.03.032},
}


@article{20132616454943,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Improved measurement technique for the characterization of organic and inorganic phase change materials using the T-history method},
journal={Applied Energy},
author={Stankovic, Stanislava B. and Kyriacou, Panayiotis A.},
volume={109},
year={2013},
pages={433 - 440},
issn={03062619},
abstract={In the past decade, the interest in phase change materials (PCMs) has grown significantly due to their ability to store large amounts of thermal energy in relatively small temperature intervals. Accurate knowledge of thermo-physical properties is a prerequisite for any reliable utilization of these materials. The T-history method is widely used for the investigation of PCM. This paper presents an improved measurement technique for the characterization of PCM using the T-history method. The suggested improvements include the arrangements made in three different prospects: the experimental setup, data processing and data representation. T-history measurements of organic RT21 and inorganic SP22 A17 (RUBITHERM&reg; GmbH) PCM were performed. The applied arrangements resulted in the temperature accuracy of &plusmn;0.3. &deg;C and the reduction of uncertainty associated with heat stored/released between the cooling and heating measurements. The obtained results showed some important aspects of the T-history PCM investigation and could provide more effective design and development process of the thermal energy storage systems based on the investigated materials. &copy; 2013 Elsevier Ltd.<br/>},
key={Phase change materials},
keywords={Characterization;Data handling;Digital storage;Heat storage;Pulse code modulation;Thermal energy;Uncertainty analysis;},
note={Heat density;Measurement techniques;Subcoolings;T-history method;Temperature accuracies;Temperature uncertainties;Thermal energy storage systems;Thermo-physical property;},
URL={http://dx.doi.org/10.1016/j.apenergy.2013.01.079},
}


@inproceedings{20173104002703,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using code change types in an analogy-based classifier for short-Term defect prediction},
journal={ACM International Conference Proceeding Series},
author={Tasse, Josee},
volume={Part F128822},
year={2013},
address={Baltimore, MD, United states},
abstract={Current approaches for defect prediction usually analyze files (or modules) and their development as work is done on a given release, to predict post-release defects. What is missing is an approach for predicting bugs to be detected in a more short-Term interval, even within the development of a particular version. In this paper, we propose a defect predictor that looks into change bursts in a given file, analyzing the number of changes and their types, and then predict whether the file is likely to have a bug found within the next 3 months after that change burst. An analogy-based classifier is used for this task: The prediction is made based on comparisons with similar change bursts that occurred in other files. New metrics are described to capture the change type of a file (e.g., small local change, massive change all in one place, multiple changes scattered throughout the file).<br/>},
key={Forecasting},
keywords={Defects;Software engineering;},
note={Change burst;Change types;Code changes;Defect prediction;Multiple changes;Short term;Short term prediction;},
URL={http://dx.doi.org/10.1145/2499393.2499399},
}


@article{20162102410805,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mapping Bug Reports to Relevant Files: A Ranking Model, a Fine-Grained Benchmark, and Feature Evaluation},
journal={IEEE Transactions on Software Engineering},
author={Ye, Xin and Bunescu, Razvan and Liu, Chang},
volume={42},
number={4},
year={2016},
pages={379 - 402},
issn={00985589},
abstract={When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and improve productivity. This paper introduces an adaptive ranking approach that leverages project knowledge through functional decomposition of source code, API descriptions of library components, the bug-fixing history, the code change history, and the file dependency graph. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluate the ranking system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the learning-to-rank approach outperforms three recent state-of-the-art methods. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70 percent of the bug reports in the Eclipse Platform and Tomcat projects.<br/> &copy; 2015 IEEE.},
key={Program debugging},
keywords={Codes (symbols);Computer software maintenance;Open source software;Open systems;Productivity;},
note={Adaptive rankings;Bug reports;Dependency graphs;Feature evaluation;Functional decomposition;Learning to rank;Library components;Ranking system;},
URL={http://dx.doi.org/10.1109/TSE.2015.2479232},
}


@inproceedings{20162502513453,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mining performance regression inducing code changes in evolving software},
journal={Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},
author={Luo, Qi and Poshyvanyk, Denys and Grechanik, Mark},
year={2016},
pages={25 - 36},
address={Austin, TX, United states},
abstract={During software evolution, the source code of a system frequently changes due to bug fixes or new feature requests. Some of these changes may accidentally degrade performance of a newly released software version. A notable problem of regression testing is how to find problematic changes (out of a large number of committed changes) that may be responsible for performance regressions under certain test inputs. We propose a novel recommendation system, coined as PerfImpact, for automatically identifying code changes that may potentially be responsible for performance regressions using a combination of search-based input profiling and change impact analysis techniques. PerfImpact independently sends the same input values to two releases of the application under test, and uses a genetic algorithm to mine execution traces and explore a large space of input value combinations to find specific inputs that take longer time to execute in a new release. Since these input values are likely to expose performance regressions, PerfImpact automatically mines the corresponding execution traces to evaluate the impact of each code change on the performance and ranks the changes based on their estimated contribution to performance regressions. We implemented PerfImpact and evaluated it on different releases of two open-source web applications. The results demonstrate that PerfImpact effectively detects input value combinations to expose performance regressions and mines the code changes are likely to be responsible for these performance regressions.<br/> &copy; 2016 ACM.},
key={Regression analysis},
keywords={Codes (symbols);Genetic algorithms;Open source software;Software testing;},
note={Application under tests;Change impact analysis;Execution trace;Feature requests;Identifying code;Regression testing;Software Evolution;Software versions;},
URL={http://dx.doi.org/10.1145/2901739.2901765},
}


@article{20151400714818,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Spectral history model in DYN3D: Verification against coupled Monte-Carlo thermal-hydraulic code BGCore},
journal={Annals of Nuclear Energy},
author={Bilodid, Y. and Kotlyar, D. and Margulis, M. and Fridman, E. and Shwageraus, E.},
volume={81},
year={2015},
pages={34 - 40},
issn={03064549},
abstract={This research focuses on the verification of a recently developed methodology accounting for spectral history effects in 3D full core nodal simulations. The traditional deterministic core simulation procedure includes two stages: (1) generation of homogenized macroscopic cross section sets and (2) application of these sets to obtain a full 3D core solution with nodal codes. The standard approach adopts the branch methodology in which the branches represent all expected combinations of operational conditions as a function of burnup (main branch). The main branch is produced for constant, usually averaged, operating conditions (e.g. coolant density). As a result, the spectral history effects that associated with coolant density variation are not taken into account properly. Number of methods to solve this problem (such as micro-depletion and spectral indexes) were developed and implemented in modern nodal codes. Recently, we proposed a new and robust method to account for history effects. The methodology was implemented in DYN3D and involves modification of the few-group cross section sets. The method utilizes the local Pu-239 concentration as an indicator of spectral history. The method was verified for PWR and VVER applications. However, the spectrum variation in BWR core is more pronounced due to the stronger coolant density change. The purpose of the current work is investigating the applicability of the method to BWR analysis. The proposed methodology was verified against recently developed BGCore system, which couples Monte Carlo neutron transport with depletion and thermal-hydraulic solvers and thus capable of providing a reference solution for 3D simulations. The results clearly show that neglecting the spectral history effects leads to a very large deviation (e.g. 1700 pcm in multiplication factor) from the reference solution. Application of the Pu-correction method results in a very good agreement between DYN3D and BGCore on the order of 200 pcm in k<inf>inf</inf>.<br/> &copy; 2015 Elsevier Ltd. All rights reserved.},
key={Monte Carlo methods},
keywords={Boiling water reactors;Codes (symbols);Coolants;Pressurized water reactors;Reactor cores;},
note={BGCore;Carlo;Coupled;DYN3D;History effects;Monte;},
URL={http://dx.doi.org/10.1016/j.anucene.2015.03.030},
}


@inproceedings{20133316602358,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Implicit theories of programming aptitude as a barrier to learning to code: Are they distinct from intelligence?},
journal={Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE},
author={Scott, Michael James and Ghinea, Gheorghita},
year={2013},
pages={347 - },
issn={1942647X},
address={Canterbury, United kingdom},
abstract={Contemporary psychology has shown that self-theories can have a profound influence on affect and behavior. Entity-theorists, believing their traits are fixed, adopt maladaptive learning strategies in the face of difficulty. In contrast, incremental-theorists, believing their qualities can change, adopt mastery-orientated strategies. However, can this concept be domain-specific? This poster presentation challenges the notion of a single dominant mindset. People can nurture a variety of beliefs about different traits, so in the minds of learners, programming aptitude may not be the same as intelligence. The results from a confirmatory factor analysis of 94 responses to an undergraduate programming experience survey indicate that beliefs towards aptitude are empirically distinct from those towards intelligence, suggesting that alternate self-traits should be considered when extending self-theories into specific domains.<br/>},
key={Programming theory},
keywords={Education computing;Engineering education;Engineering research;Factor analysis;Mathematical programming;},
note={Aptitude;Dweck;Entity;Implicit theories;Incremental;Intelligence;Learning;Self-theories;},
URL={http://dx.doi.org/10.1145/2462476.2462515},
}


@inproceedings{20172803900929,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An unsupervised distance learning framework for multimedia retrieval},
journal={ICMR 2017 - Proceedings of the 2017 ACM International Conference on Multimedia Retrieval},
author={Valem, Lucas Pascotti and Pedronette, Daniel Carlos Guimaraes},
year={2017},
pages={107 - 111},
address={Bucharest, Romania},
abstract={Due to the increasing availability of image and multimedia collections, unsupervised post-processing methods, which are capable of improving the effectiveness of retrieval results without the need of user intervention, have become indispensable. This paper presents the Unsupervised Distance Learning Framework (UDLF), a software which enables an easy use and evaluation of unsupervised learning methods. The framework defines a broad model, allowing the implementation of different unsupervised methods and supporting diverse file formats for input and output. Seven different unsupervised methods are initially available in the framework. Executions and experiments can be easily defined by setting a configuration file. The framework also includes the evaluation of the retrieval results exporting visual output results, computing effectiveness and efficiency measures. The source-code is public available, such that anyone can freely access, use, change, and share the software under the terms of the GPLv2 license.<br/> &copy; 2017 Association for Computing Machinery.},
key={Distance education},
keywords={Content based retrieval;Image enhancement;Unsupervised learning;},
note={Content based image retrieval;Effectiveness and efficiencies;Multimedia collections;Multimedia Retrieval;Postprocessing methods;Rank aggregation;Re-ranking;Unsupervised learning method;},
URL={http://dx.doi.org/10.1145/3078971.3079017},
}


@article{20152600980427,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Efficient image-aware version control systems using GPU},
journal={Software - Practice and Experience},
author={da Silva Junior, Jose Ricardo and Clua, Esteban and Murta, Leonardo},
volume={46},
number={8},
year={2016},
pages={1011 - 1033},
issn={00380644},
abstract={Version control is considered to be a vital component for supporting professional software development. While it has been widely used for textual artifacts, such as source code or documentation, little attention has been given to binary artifacts. This omission can place huge restrictions on projects in the game and media industries as they contain large amounts of binary data, such as images, videos, three-dimensional models, and animations, along with their source code. For these kinds of artifacts, existing strategies such as storing the file as a whole for each revision or saving conventional binary deltas consume significant storage space with duplicate data and, even worse, do not provide any understandable information on which modifications were made. As a response to this problem, this paper introduces a change-set model infrastructure to support version control of image artifacts using a specialized data structure. Additionally, our approach can deal with the maintenance of duplicate nearly identical images through a merge operation. Because of the amount of data that has to be processed, we designed our solution based on a parallel architecture, which permits a massively parallel approach to version control. The paper also compares our approach with some popular open-source version control systems, showing their repository growth in relation to ours as well as the time required to process image artifacts. Finally, we demonstrate that our architecture requires less storage space and runs much faster than current methods. Copyright &copy; 2015 John Wiley &amp; Sons, Ltd.<br/> Copyright &copy; 2015 John Wiley & Sons, Ltd.},
key={Open systems},
keywords={Animation;Control systems;Copyrights;Digital storage;Graphics processing unit;Information management;Open source software;Parallel architectures;Software design;Three dimensional computer graphics;},
note={CUDA;image;Massively parallels;Merge operations;Professional software;Three-dimensional model;Version control;Version control system;},
URL={http://dx.doi.org/10.1002/spe.2340},
}


@inproceedings{20124715695528,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Modern version control: Creating an efficient development ecosystem},
journal={SIGUCCS'12 - ACM Proceedings of the SIGUCCS Annual Conference},
author={Bertino, Nic},
year={2012},
pages={219 - 222},
address={Memphis, TN, United states},
abstract={In 2011, Santa Clara University School of Law Technology and Academic Computing (LTAC) identified that its version control system could greatly benefit from the use of modern source control management software. Source code for high value projects such as the Santa Clara Law website, were previously held in a Subversion (SVN) repository in a client-server model, providing version control and redundancy. Because of the resource footprint associated with SVN, only projects with high importance could be setup with version control. As more web-based applications were introduced, the need for a more efficient revision control system arose. Git, a highly efficient decentralized version control system (DVCS), was selected after evaluating similar technologies. This change transformed the entire development process, making the development cycle more streamlined and with greater flexibility. In the early use of Git, LTAC also discovered its use as a deployment tool, increasing redundancy on servers and reducing overhead usually associated with revision control. It also serves as the vital link between LTAC's issue tracking system, Redmine, and the development team. The introduction of Redmine has helped LTAC monitor website issues, manage projects, and continually review changes to the code base. LTAC has created a development ecosystem that provides redundancy and accountability using open source products that carry no cost. Git has significant performance gains over SVN, making its integration and use less frustrating and distracting for developers. Redmine gives developers and customers the opportunity to organize, track, and resolve issues. The flexibility of the technology used means that any project, from a content management system to a one-off script, can benefit from source control without large costs or long deployment times. Copyright &copy; 2012 ACM.<br/>},
key={Information management},
keywords={Control systems;Ecosystems;Open source software;Redundancy;Websites;},
note={Automated deployment;Issue Tracking;Redmine;Source control;Version control;},
URL={http://dx.doi.org/10.1145/2382456.2382510},
}


@inproceedings{20163902846078,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The employment of online self-learning coding course to enhance logical reasoning ability for fifth and sixth grader students},
journal={2016 International Conference on Applied System Innovation, IEEE ICASI 2016},
author={Yang, Kai-Ming and Tsai, Chih-Heng and Lee, Lai-Chung and Chen, Chun-Ting},
year={2016},
pages={et al.; Fuzhou University; IEEE of Electrical and Electronics Engineers (IEEE) Tainan Section; Kun Shan University; National Formosa University; Taiwanese Institute of Knowledge Innovation (TIKI) - },
address={4-3-1 Mashiki, Ginowan City Okinawa, Japan},
abstract={This paper was aimed to investigate the influence of online coding courses on logical reasoning ability towards fifth and sixth graders. Students used "Code.org online programming courses" about three months. Through "Raven's Standard Progressive Matrices" before and after testing to understand the change of students' logical reasoning ability in the experiment. And based on "Technology Acceptance Model", researcher edit "Code.org Learning Attitude Questionnaire" to explore the attitude of students towards the courses and the factors of influencing students learning online courses.<br/> &copy; 2016 IEEE.},
key={E-learning},
keywords={Ability testing;Computer programming;Students;},
note={Learning attitudes;Logical reasoning;On-line programming;Online course;Self-learning;Students learning;Technology acceptance model;},
URL={http://dx.doi.org/10.1109/ICASI.2016.7539890},
}


@inproceedings{20155201710175,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Projectional editing of variational software},
journal={ACM SIGPLAN Notices},
author={Walkingshaw, Eric and Ostermann, Klaus},
volume={50},
number={3},
year={2015},
pages={29 - 38},
issn={15232867},
abstract={Editing the source code of variational software is complicated by the presence of variation annotations, such as #ifdef statements, and by code that is only included in some configurations. When editing some configurations and not others, it would be easier to edit a simplified version of the source code that includes only the configurations we currently care about. In this paper, we present a projectional editing model for variational software. Using our approach, a programmer can partially configure a variational program, edit this simplified view of the code, and then automatically update the original, fully variational source code. The model is based on an isolation principle where edits affect only the variants that are visible in the view. We show that this principle has several nice properties that are suggested by related work on bidirectional transformations.<br/> &copy; 2014 ACM 978-1-4503-3161-6/14/09&mellip;$15.00.},
key={Codes (symbols)},
keywords={Computer programming languages;},
note={Bidirectional transformation;Projectional editing;Software Product Line;Variation;View update problems;},
URL={http://dx.doi.org/10.1145/2658761.2658766},
}


@inproceedings{20153401195106,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Projectional editing of variational software},
journal={13th International Conference on Generative Programming: Concepts and Experiences, GPCE 2014 - Proceedings},
author={Walkingshaw, Eric and Ostermann, Klaus},
year={2014},
pages={29 - 38},
address={Vasteras, Sweden},
abstract={Editing the source code of variational software is complicated by the presence of variation annotations, such as #ifdef statements, and by code that is only included in some configurations. When editing some configurations and not others, it would be easier to edit a simplified version of the source code that includes only the configurations we currently care about. In this paper, we present a projectional editing model for variational software. Using our approach, a programmer can partially configure a variational program, edit this simplified view of the code, and then automatically update the original, fully variational source code. The model is based on an isolation principle where edits affect only the variants that are visible in the view. We show that this principle has several nice properties that are suggested by related work on bidirectional transformations. Copyright is held by the author/owner(s). Publication rights licensed to ACM.<br/>},
key={Codes (symbols)},
keywords={Computer programming languages;},
note={Bidirectional transformation;Projectional editing;Software Product Line;Variation;View update problems;},
URL={http://dx.doi.org/10.1145/2658761.2658766},
}


@inproceedings{20123615405963,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using version control system to construct ownership architecture documentations},
journal={Lecture Notes in Electrical Engineering},
author={Huang, Po-Han and Yeh, Dowming and Lee, Wen-Tin},
volume={156 LNEE},
number={VOL. 1},
year={2013},
pages={41 - 46},
issn={18761100},
address={Bali, Indonesia},
abstract={Ownership architecture was usually constructed by investigating the comments at the top of source files. That is, to associate developer names with source files is to examine the comments manually. If such documentation can be produced automatically, it will be more immediate to indicate the status of the project. This research focus on the logs in the version control system. The data within version control logs is in a regular form and information can be retrieved quickly. The importance of developers can also be estimated by the number of own files and frequency of making a change. In order to understand the system architecture, the directory structure of source code can be used to identify function components of the system essentially. The source files in a directory implement the same function component, and the owners of these source files can be considered a team. Using the documents, researcher can know the ownership architecture and more information about the status of the project. &copy; 2013 Springer-Verlag GmbH.<br/>},
key={Computer architecture},
keywords={Control systems;Information management;},
note={Directory structure;Function components;Research focus;Source codes;Source files;System architectures;Version control;Version control system;},
URL={http://dx.doi.org/10.1007/978-3-642-28807-4_7},
}


@article{20173604130117,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow},
journal={IEEE Transactions on Visualization and Computer Graphics},
author={Wongsuphasawat, Kanit and Smilkov, Daniel and Wexler, James and Wilson, Jimbo and Mane, Dandelion and Fritz, Doug and Krishnan, Dilip and Viegas, Fernanda B. and Wattenberg, Martin},
volume={24},
number={1},
year={2018},
pages={1 - 12},
issn={10772626},
abstract={We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.<br/> &copy; 1995-2012 IEEE.},
key={Data flow analysis},
keywords={Artificial intelligence;Data flow graphs;Deep learning;Graph theory;Neural networks;},
note={Cluster expansion;Clustered graph;Graph Transformation;Graph visualization;Hierarchical structures;Interactive diagrams;Machine intelligence;Modular composition;},
URL={http://dx.doi.org/10.1109/TVCG.2017.2744878},
}


@inbook{20172803910058,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Recommending program transformations: Automating repetitive software changes},
journal={Recommendation Systems in Software Engineering},
author={Kim, Miryung and Meng, Na},
year={2014},
pages={421 - 453},
abstract={Adding features and fixing bugs in software often require systematic edits which are similar but not identical changes to multiple code locations. Finding all relevant locations and making the correct edits is a tedious and error-prone process. This chapter presents several state-of-the art approaches to recommending program transformation in order to automate repetitive software changes. First, it discusses programming-by-demonstration (PBD) approaches that automate repetitive tasks by inferring a generalized action script from a user&rsquo;s recorded actions. Second, it presents edit location suggestion approaches that only recommend candidate edit locations but do not apply necessary code transformations. Finally, it describes program transformation approaches that take code examples or version histories as input, automatically identify candidate edit locations, and apply context awareness, customization program transformations to generate a new program version. In particular, this chapter describes two concrete example-based program transformation approaches in detail, Sydit and Lase. These two approaches are selected for an in-depth discussion, because they handle the issue of both recommending change locations and applying transformations, and they are specifically designed to update programs as opposed to regular text documents. The chapter is then concluded with open issues and challenges of recommending program transformations.<br/> &copy; Springer-Verlag Berlin Heidelberg 2014.},
key={Program debugging},
keywords={Codes (symbols);Cosine transforms;Location;},
note={Code transformation;Context- awareness;Error-prone process;Issues and challenges;Program transformation approach;Program transformations;Programming by demonstration;State-of-the-art approach;},
URL={http://dx.doi.org/10.1007/978-3-642-45135-5_16},
}


@inproceedings{20171203477102,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Understanding systematic and collaborative code changes by mining evolutionary trajectory patterns},
journal={Journal of Software: Evolution and Process},
author={Jiang, Qingtao and Peng, Xin and Wang, Hai and Xing, Zhenchang and Zhao, Wenyun},
volume={29},
number={3},
year={2017},
issn={20477481},
abstract={The life cycle of a large-scale software system can undergo many releases. Each release often involves hundreds or thousands of revisions committed by many developers over time. Many code changes are made in a systematic and collaborative way. However, such systematic and collaborative code changes are often undocumented and hidden in the evolution history of a software system. It is desirable to recover commonalities and associations among dispersed code changes in the evolutionary trajectory of a software system. In this paper, we present Summarizing Evolutionary Trajectory by Grouping and Aggregation (SETGA), an approach to summarizing historical commit records as trajectory patterns by grouping and aggregating relevant code changes committed over time. The SETGA extracts change operations from a series of commit records from version control systems. It then groups extracted change operations by their common properties from different dimensions such as change operation types, developers, and change locations. After that, SETGA aggregates relevant change operation groups by mining various associations among them. We implement SETGA and conduct an empirical study with 3 open-source systems. We investigate underlying evolution rules and problems that can be revealed by the identified patterns and analyze the evolution of trajectory patterns in different periods. The results show that SETGA can identify various types of trajectory patterns that are useful for software evolution management and quality assurance.<br/> Copyright &copy; 2017 John Wiley & Sons, Ltd.},
key={Open systems},
keywords={Codes (symbols);Control systems;Information management;Life cycle;Mining;Open source software;Quality assurance;Trajectories;},
note={Code changes;Collaborative codes;evolution;Large-scale software systems;pattern;Software Evolution;Trajectory pattern;Version control system;},
URL={http://dx.doi.org/10.1002/smr.1840},
}


@inproceedings{20161602247609,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={InTime: A machine learning approach for efficient selection of FPGA CAD tool parameters},
journal={FPGA 2015 - 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
author={Kapre, Nachiket and Ng, Harnhua and Teo, Kirvy and Naude, Jaco},
year={2015},
pages={23 - 26},
address={Monterey, CA, United states},
abstract={FPGA CAD tool parameters controlling synthesis optimizations, place and route effort, mapping criteria along with user-supplied physical constraints can affect timing results of the circuit by as much as 70% without any change in original source code. A correct selection of these parameters across a diverse set of benchmarks with varying characteristics and design goals is challenging. The sheer number of parameters and option values that can be selected is large (thousands of combinations for modern CAD tools) with often conflicting interactions. In this paper, we present InTime, a machinelearning approach supported by a cloud-based (or clusterbased) compilation infrastructure for automating the selection of these parameters effectively to minimize timing costs. InTime builds a database of results from a series of preliminary runs based on canned configurations of CAD options. It then learns from these runs to predict the next series of CAD tool options to improve timing results. Towards the end, we rely on a limited degree of statistical sampling of certain options like placer and synthesis seeds to further tighten results. Using our approach, we show 70% reduction in final timing results across industrial benchmark problems for the Altera CAD flow. This is 30% better than vendor-supplied design space exploration tools that attempts a similar optimization using canned heuristics.<br/> &copy; Copyright ACM.},
key={Computer aided logic design},
keywords={Artificial intelligence;Computer aided design;Field programmable gate arrays (FPGA);Integrated circuit design;Learning systems;Logic gates;Logic Synthesis;Optimization;Sampling;Tools;},
note={Bench-mark problems;Cluster-based;Design-space exploration tool;Machine learning approaches;Physical constraints;Place and route;Statistical sampling;Synthesis optimization;},
URL={http://dx.doi.org/10.1145/2684746.2689081},
}


@article{20172903945824,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={How deep learning extracts and learns leaf features for plant classification},
journal={Pattern Recognition},
author={Lee, Sue Han and Chan, Chee Seng and Mayo, Simon Joseph and Remagnino, Paolo},
volume={71},
year={2017},
pages={1 - 13},
issn={00313203},
abstract={Plant identification systems developed by computer vision researchers have helped botanists to recognize and identify unknown plant species more rapidly. Hitherto, numerous studies have focused on procedures or algorithms that maximize the use of leaf databases for plant predictive modeling, but this results in leaf features which are liable to change with different leaf data and feature extraction techniques. In this paper, we learn useful leaf features directly from the raw representations of input data using Convolutional Neural Networks (CNN), and gain intuition of the chosen features based on a Deconvolutional Network (DN) approach. We report somewhat unexpected results: (1) different orders of venation are the best representative features compared to those of outline shape, and (2) we observe multi-level representation in leaf data, demonstrating the hierarchical transformation of features from lower-level to higher-level abstraction, corresponding to species classes. We show that these findings fit with the hierarchical botanical definitions of leaf characters. Through these findings, we gained insights into the design of new hybrid feature extraction models which are able to further improve the discriminative power of plant classification systems. The source code and models are available at: https://github.com/cs-chan/Deep-Plant.<br/> &copy; 2017 Elsevier Ltd},
key={Deep learning},
keywords={Extraction;Feature extraction;Metadata;Neural networks;},
note={Convolutional Neural Networks (CNN);Discriminative power;Feature extraction techniques;Higher-level abstraction;Hybrid-feature extraction;Plant classification;Plant identification systems;Plant recognition;},
URL={http://dx.doi.org/10.1016/j.patcog.2017.05.015},
}


@inproceedings{20185006250404,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={What makes a code change easier to review: An empirical investigation on code change reviewability},
journal={ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
author={Ram, Achyudh and Sawant, Anand Ashok and Castelluccio, Marco and Bacchelli, Alberto},
year={2018},
pages={201 - 212},
address={Lake Buena Vista, FL, United states},
abstract={Peer code review is a practice widely adopted in software projects to improve the quality of code. In current code review practices, code changes are manually inspected by developers other than the author before these changes are integrated into a project or put into production. We conducted a study to obtain an empirical understanding of what makes a code change easier to review. To this end, we surveyed published academic literature and sources from gray literature (e.g., blogs and white papers), we interviewed ten professional developers, and we designed and deployed a reviewability evaluation tool that professional developers used to rate the reviewability of 98 changes.We find that reviewability is defined through several factors, such as the change description, size, and coherent commit history. We provide recommendations for practitioners and researchers.<br/> &copy; 2018 Association for Computing Machinery.},
key={Codes (symbols)},
keywords={Software engineering;},
note={Academic literature;Code quality;Code review;Empirical investigation;Evaluation tool;Peer code review;pull request;Software project;},
URL={http://dx.doi.org/10.1145/3236024.3236080},
}


@inproceedings{20151800813943,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Summarizing Evolutionary Trajectory by Grouping and Aggregating relevant code changes},
journal={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
author={Jiang, Qingtao and Peng, Xin and Wang, Hai and Xing, Zhenchang and Zhao, Wenyun},
year={2015},
pages={361 - 370},
address={Montreal, QC, Canada},
abstract={The lifecycle of a large-scale software system can undergo many releases. Each release often involves hundreds or thousands of revisions committed by many developers over time. Many code changes are made in a systematic and collaborative way. However, such systematic and collaborative code changes are often undocumented and hidden in the evolution history of a software system. It is desirable to recover commonalities and associations among dispersed code changes in the evolutionary trajectory of a software system. In this paper, we present SETGA (Summarizing Evolutionary Trajectory by Grouping and Aggregation), an approach to summarizing historical commit records as trajectory patterns by grouping and aggregating relevant code changes committed over time. SETGA extracts change operations from a series of commit records from version control systems. It then groups extracted change operations by their common properties from different dimensions such as change operation types, developers and change locations. After that, SETGA aggregates relevant change operation groups by mining various associations among them. The proposed approach has been implemented and applied to three open-source systems. The results show that SETGA can identify various types of trajectory patterns that are useful for software evolution management and quality assurance.<br/> &copy; 2015 IEEE.},
key={Open systems},
keywords={Codes (symbols);Control systems;Information management;Mining;Open source software;Quality assurance;Trajectories;},
note={Code changes;Collaborative codes;Evolution;Large-scale software systems;Open source system;Pattern;Trajectory pattern;Version control system;},
URL={http://dx.doi.org/10.1109/SANER.2015.7081846},
}


@inproceedings{20184906177197,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={What Strokes to Modify in the Painting? Code Changes Prediction for Object-Oriented Software},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Zhang, Dinan and Chen, Shizhan and He, Qiang and Feng, Zhiyong and Huang, Keman},
volume={11293 LNCS},
year={2018},
pages={103 - 119},
issn={03029743},
address={Shenzheng, China},
abstract={Software systems shall evolve to fulfill users&rsquo; increasingly various and sophisticated needs. As they become larger and more complex, the corresponding testing and maintenance have become a practical research challenge. In this paper, we employ an approach that can identify the change-proneness in the source code of new object-oriented software releases and predict the corresponding change sizes. We first define two metrics, namely Class Change Metric and Change Size Metric, to describe the features and sizes of code changes. A new software release may be based on several previous releases. Thus, we employ an Entropy Weight Method to calculate the best window size for determining the number of previous releases to use in the prediction of change-proneness in the new release. Based on a series of change evolution matrices, a code change prediction approach is proposed based on the Gauss Process Regression (GPR) algorithm. Experiments are conducted on 17 software systems collected from GitHub to evaluate our prediction approach. The results show that our approach outperforms three existing state-of-the-art approaches with significantly higher prediction accuracy.<br/> &copy; 2018, Springer Nature Switzerland AG.},
key={Object oriented programming},
keywords={Codes (symbols);Forecasting;Software testing;},
note={Change history;Object oriented software;Software Evolution;Software metrics;Source codes;},
URL={http://dx.doi.org/10.1007/978-3-030-04272-1_7},
}


@inproceedings{20153201154930,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Incremental origin analysis of source code files},
journal={11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings},
author={Steidl, Daniela and Hummel, Benjamin and Juergens, Elmar},
year={2014},
pages={42 - 51},
address={Hyderabad, India},
abstract={The history of software systems tracked by version control systems is often incomplete because many file movements are not recorded. However, static code analyses that mine the file history, such as change frequency or code churn, produce precise results only if the complete history of a source code file is available. In this paper, we show that up to 38.9% of the files in open source systems have an incomplete history, and we propose an incremental, commit-based approach to reconstruct the history based on clone information and name similarity. With this approach, the history of a file can be reconstructed across repository boundaries and thus provides accurate information for any source code analysis. We evaluate the approach in terms of correctness, completeness, performance, and relevance with a case study among seven open source systems and a developer survey.<br/> Copyright 2014 ACM.},
key={Open systems},
keywords={Cloning;Codes (symbols);Computer programming languages;Open source software;},
note={Change frequencies;Clone detection;Open source system;Origin analysis;Software Evolution;Source code analysis;Static code analysis;Version control system;},
URL={http://dx.doi.org/10.1145/2597073.2597111},
}


@inproceedings{20171803630356,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Enriching in-IDE process information with fine-grained source code history},
journal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
author={Proksch, Sebastian and Nadi, Sarah and Amann, Sven and Mezini, Mira},
year={2017},
pages={250 - 260},
address={Klagenfurt, Austria},
abstract={Current studies on software development either focus on the change history of source code from version-control systems or on an analysis of simplistic in-IDE events without context information. Each of these approaches contains valuable information that is unavailable in the other case. Our work proposes enriched event streams, a solution that combines the best of both worlds and provides a holistic view on the software development process. Enriched event streams not only capture developer activities in the IDE, but also specialized context information, such as source-code snapshots for change events. To enable the storage of such code snapshots in an analyzable format, we introduce a new intermediate representation called Simplified Syntax Trees (SSTs) and build CA&squ;RET, a platform that offers reusable components to conveniently work with enriched event streams. We implement FEEDBAG++, an instrumentation for Visual Studio that collects enriched event streams with code snapshots in the form of SSTs. We share a dataset of enriched event streams captured from 58 users and representing 915 days of work. Additionally, to demonstrate usefulness, we present three research applications that have already made use of CA&squ;RET and FEEDBAG++.<br/> &copy; 2017 IEEE.},
key={Software design},
keywords={Codes (symbols);Computer programming languages;Control system analysis;Integrodifferential equations;Reengineering;Semantics;Trees (mathematics);},
note={Context information;Developer activities;Intermediate representations;Process information;Research applications;Reusable components;Software development process;Version control system;},
URL={http://dx.doi.org/10.1109/SANER.2017.7884626},
}


@inproceedings{20143118014611,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The impact of version control operations on the quality change of the source code},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author={Farago, Csaba and Hegedus, Peter and Ferenc, Rudolf},
volume={8583 LNCS},
number={PART 5},
year={2014},
pages={353 - 369},
issn={03029743},
address={Guimaraes, Portugal},
abstract={The number of software systems under development and maintenance is rapidly increasing. The quality of a system's source code tends to decrease during its lifetime which is a problem because maintaining low quality code consumes a big portion of the available efforts. In this research we investigated one aspect of code change, the version control commit operations (add, update, delete). We studied the impact of these operations on the maintainability of the code. We calculated the ISO/IEC 9126 quality attributes for thousands of revisions of an industrial and three open-source software systems. We also collected the cardinality of each version control operation type for every investigated revision. Based on these data, we identified that operation Add has a rather positive, while operation Update has a rather negative effect on the quality. On the other hand, for operation Delete we could not find a clear connection to quality change. &copy; 2014 Springer International Publishing.<br/>},
key={Quality control},
keywords={Codes (symbols);Computer programming languages;Information dissemination;Information management;ISO Standards;Maintainability;Open source software;Open systems;},
note={ISO/IEC 9126;Open source software systems;Quality attributes;Quality change;Software maintainability;Software systems;Source codes;Version control;},
URL={http://dx.doi.org/10.1007/978-3-319-09156-3_26},
}


@inproceedings{20170803366604,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={An empirical study on the characteristics of python fine-grained source code change types},
journal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
author={Lin, Wei and Chen, Zhifei and Ma, Wanwangying and Chen, Lin and Xu, Lei and Xu, Baowen},
year={2016},
pages={188 - 199},
address={Raleigh, NC, United states},
abstract={Software has been changing during its whole life cycle. Therefore, identification of source code changes becomes a key issue in software evolution analysis. However, few current change analysis research focus on dynamic language software. In this paper, we pay attention to the fine-grained source code changes of Python software. We implement an automatic tool named PyCT to extract 77 kinds of fine-grained source code change types from commit history information. We conduct an empirical study on ten popular Python projects from five domains, with 132294 commits, to investigate the characteristics of dynamic software source code changes. Analyzing the source code changes in four aspects, we distill 11 findings, which are summarized into two insights on software evolution: change prediction and fault code fix. In addition, we provide direct evidence on how developers use and change dynamic features. Our results provide useful guidance and insights for improving the understanding of source code evolution of dynamic language software.<br/> &copy; 2016 IEEE.},
key={Computer software},
keywords={Codes (symbols);Computer software maintenance;High level languages;Life cycle;},
note={Characteristics of dynamics;Fine-grained changes;Fine-grained source code changes;Identification of sources;Python;Software Evolution;Software evolution analysis;Source code changes;},
URL={http://dx.doi.org/10.1109/ICSME.2016.25},
}


@inproceedings{20150300431026,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A token oriented measurement method of source code similarity},
journal={Applied Mechanics and Materials},
author={Zhu, Hong Mei and Zhang, Liang and Sun, Wei and Sun, Yong Xiang},
volume={668-669},
year={2014},
pages={899 - 902},
issn={16609336},
address={Zhuhai, China},
abstract={In order to help teachers to identify plagiarism in student assignment submissions among students&rsquo; Source code quickly and accurately, this paper discusses a measurement method of Source code similarity. In the proposed algorithm, firstly, both of token oriented edit distance (TD) and token oriented length of longest common subsequence (TLCSLen) is calculated; secondly, considering the TD and TLCSLen, a similarity calculation formula is given to measure similarity of Source code; Thirdly, a dynamic and variable similarity threshold is set to determine whether there is plagiarism between Source codes, which ensure a relatively reasonable judgment of plagiarism. This method has been applied to the university's programming course work online submission system and online examination system. Practical application results show that this method can identify similar Source code timely, effectively and accurately. &copy; (2014) Trans Tech Publications, Switzerland.},
key={Computer programming languages},
keywords={Codes (symbols);E-learning;Intellectual property;Social networking (online);Teaching;},
note={Edit distance;Longest common subsequences;On-line examinations;Similarity;Similarity calculation;Similarity threshold;Source code similarities;Source codes;},
URL={http://dx.doi.org/10.4028/www.scientific.net/AMM.668-669.899},
}


@inproceedings{20184005905893,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Empirical analysis on effectiveness of source code metrics for predicting change-proneness},
journal={ACM International Conference Proceeding Series},
author={Kumar, Lov and Rath, Santanu Kumar and Sureka, Ashish},
year={2017},
pages={4 - 14},
address={Jaipur, India},
abstract={Change-prone classes or modules are defined as software components in the source code which are likely to change in the future. Change-proneness prediction are useful to the maintenance team as they can optimize and focus their testing resources on the modules which have a higher likelihood of change. The quality of change-proneness prediction model can be best assessed by the use of software metrics that are considered to design the prediction model. In this work, 62 software metrics with four metrics dimensions, including 7 size metrics, 18 cohesion metrics, 20 coupling metrics, and 17 inheritance metrics are considered to develop a model for predicting change-proneness modules. Since the performance of the change-proneness model depends on the source code metrics, they are used as input of the change-proneness model. We also considered five different types of feature selection techniques to remove irrelevant feature and select best set of features. The effectiveness of these set of source code metrics are evaluated using eight different machine learning algorithms and two ensemble techniques. Experimental results demonstrates that the model developed by considering selected set of source code metrics by feature selection technique as input achieves better results as compared to considering all source code metrics. The experimental results also ravel that the change-proneness model developed by using coupling metrics achieved better performance as compared other dimension metrics such as size metrics, cohesion metrics, and inheritance metrics.<br/> &copy; 2017 ACM.},
key={Feature extraction},
keywords={Codes (symbols);Computer programming languages;Forecasting;Learning algorithms;Learning systems;Object oriented programming;Software engineering;},
note={Change proneness;Classification technique;Ensemble techniques;Feature selection techniques;Source code metrics;},
URL={http://dx.doi.org/10.1145/3021460.3021461},
}


@inproceedings{20174404328758,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automatic Matching Release Notes and Source Code by Generating Summary for Software Change},
journal={Proceedings - 2016 International Conference on Digital Home, ICDH 2016},
author={Huang, Yuan and Liu, Zhiyong and Chen, Xiangping and Luo, Xiaonan},
year={2016},
pages={104 - 109},
address={Guangzhou, Guangdong, China},
abstract={To quickly locate the source code that maps to a specific change described in change history, establishing traceability links between release notes and source code is a necessary task. Current works on the traceability link recovery can be used to find out source code changes which are of higher textual similarities with the release note. However, these approaches rely on consistency of the text used in artifacts at various abstraction levels, and the completeness of text descriptions. In this paper, we propose to leverage source code change information for improving the accuracy of release note to source code traceability recovery tasks. In order to reduce the complexity of link recovery, our approach first performs change impact analysis to cluster the source code changes for the same purpose as a virtual class. After that, our approach employs a natural language generation algorithm to generate readable summary sentence for each virtual class. The traceability links are built between release notes and clusters of program entities by computing the linguistic similarity of sentences. We conduct case studies on 26 releases of 3 popular softwares to evaluate the approach, and the results indicate that our proposed method can improve the accuracy of traceability link recovery compared to other IR-based techniques.<br/> &copy; 2016 IEEE.},
key={Computer programming languages},
keywords={Codes (symbols);Digital devices;Natural language processing systems;Recovery;Semantics;},
note={Change impact analysis;Linguistic similarities;Natural language generation;Semantic similarity;Source code changes;Summary generation;Textual similarities;Traceability links;},
URL={http://dx.doi.org/10.1109/ICDH.2016.031},
}


@article{20132916505967,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Predicting bugs in source code changes with incremental learning method},
journal={Journal of Software},
author={Yuan, Zi and Yu, Lili and Liu, Chao and Zhang, Linghua},
volume={8},
number={7},
year={2013},
pages={1620 - 1633},
issn={1796217X},
abstract={Software is constructed by a series of changes and each change has the risk to introduce bugs. Predicting the existence of bugs in source code changes could help developers detect and fix bugs immediately upon the completion of a change, which accelerates the bug fixing process and save the limited time and human resources effectively. However, because of altering nature in the underlying bug generation process, the concept used to depict the bugintroducing patterns is drifting, which makes it difficult to predict latent bugs of source code changes accurately, especially in the long-term prediction scenario. In order to deal with this problem, a feature-based incremental learning framework is proposed. It is comprised of three components: (1) an incremental discretization method, which is used to transform the quantitive features in the corpus incrementally, (2) an incremental feature selection method, which is always keeping a subset with the most informative features, and (3) an incremental classification algorithm, which updates the classifier dynamically and considers the current best subset of features during prediction. This proposed approach is evaluated on three famous open source systems, Eclipse, Mozilla and jedit. The results show that our approach performs better than the non-incremental method in dealing with concept drift, with the consideration of keeping the value of both precision and recall stable at a suitable level over time. We also implement a prototype with this learning framework and apply it to a real software development scenario. &copy; 2013 ACADEMY PUBLISHER.<br/>},
key={Program debugging},
keywords={Codes (symbols);Computer programming languages;Discrete event simulation;Forecasting;Open source software;Open systems;Software design;Software engineering;Software prototyping;},
note={Bug predictions;Classification algorithm;Concept drifts;Discretization method;Feature selection methods;Incremental learning;Long-term prediction;Source code changes;},
URL={http://dx.doi.org/10.4304/jsw.8.7.1620-1633},
}


@inproceedings{20171803619427,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using source code metrics to predict change-prone web services: A case-study on ebay services},
journal={MaLTeSQuE 2017 - IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with SANER 2017},
author={Kumar, Lov and Rath, Santanu Kumar and Sureka, Ashish},
year={2017},
pages={1 - 7},
address={Klagenfurt, Austria},
abstract={Predicting change-prone object-oriented software using source code metrics is an area that has attracted several researchers attention. However, predicting change-prone web services in terms of changes in the WSDL (Web Service Description Language) Interface using source code metrics implementing the services is a relatively unexplored area. We conduct a case-study on change proneness prediction on an experimental dataset consisting of several versions of eBay web services wherein we compute the churn between different versions of the WSDL interfaces using the WSDLDiff Tool. We compute 21 source code metrics using Chidamber and Kemerer Java Metrics (CKJM) extended tool serving as predictors and apply Least Squares Support Vector Machines (LSSVM) based technique to develop a change proneness estimator. Our experimental results demonstrates that a predictive model developed using all 21 metrics and linear kernel yields the best results.<br/> &copy; 2017 IEEE.},
key={Web services},
keywords={Artificial intelligence;Codes (symbols);Computer programming languages;Computer software selection and evaluation;Forecasting;Learning algorithms;Object oriented programming;Quality control;Support vector machines;Websites;},
note={Change proneness;Least squares support vector machines;Linear kernel;Object oriented software;Predictive modeling;Source code metrics;Web service description language;WSDL interfaces;},
URL={http://dx.doi.org/10.1109/MALTESQUE.2017.7882009},
}


@inproceedings{20183405719983,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={CROP: Linking code reviews to source code changes},
journal={Proceedings - International Conference on Software Engineering},
author={Paixao, Matheus and Krinke, Jens and Han, Donggyun and Harman, Mark},
year={2018},
pages={46 - 49},
issn={02705257},
address={Gothenburg, Sweden},
abstract={Code review has been widely adopted by both industrial and open source software development communities. Research in code review is highly dependant on real-world data, and although existing researchers have attempted to provide code review datasets, there is still no dataset that links code reviews with complete versions of the system's code base mainly because reviewed versions are not kept in the system's version control repository. Thus, we present CROP, the Code Review Open Platform, the first curated code review repository that links review data with isolated complete versions (snapshots) of the source code at the time of review. CROP currently provides data for 8 software systems, 48,975 reviews and 112,617 patches, including versions of the systems that are inaccessible in the systems' original repositories. Moreover, CROP is extensible, and it will be continuously curated and extended.<br/> &copy; 2018 ACM.},
key={Open systems},
keywords={Codes (symbols);Crops;Open source software;Software design;},
note={Code review;Open platforms;platform;repository;Software change;Software systems;Source code changes;Version control;},
URL={http://dx.doi.org/10.1145/3196398.3196466},
}


@article{20114914575007,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Ring: A unifying meta-model and infrastructure for Smalltalk source code analysis tools},
journal={Computer Languages, Systems and Structures},
author={Uquillas Gomez, Veronica and Ducasse, Stephane and Dhondt, Theo},
volume={38},
number={1},
year={2012},
pages={44 - 60},
issn={14778424},
abstract={Source code management systems record different versions of code. Tool support can then compute deltas between versions. To ease version history analysis we need adequate models to represent source code entities. Now naturally the questions of their definition, the abstractions they use, and the APIs of such models are raised, especially in the context of a reflective system which already offers a model of its own structure. We believe that this problem is due to the lack of a powerful code meta-model as well as an infrastructure. In Smalltalk, often several source code meta-models coexist: the Smalltalk reflective API coexists with the one of the Refactoring engine or distributed versioning system such as Monticello or Store. While having specific meta-models is an adequate engineered solution, it multiplies meta-models and it requires more maintenance efforts (e.g., duplication of tests, transformation between models), and more importantly hinders navigation tool reuse when meta-models do not offer polymorphic APIs. As a first step to provide an infrastructure to support history analysis, this article presents Ring, a unifying source code meta-model that can be used to support several activities and proposes a unified and layered approach to be the foundation for building an infrastructure for version and stream of change analyses. We re-implemented three tools based on Ring to show that it can be used as the underlying meta-model for remote and off-image browsing, scoping refactoring, and visualizing and analyzing changes. As a future work and based on Ring we will build a new generation of history analysis tools. &copy; 2011 Elsevier Ltd. All rights reserved.<br/>},
key={Codes (symbols)},
keywords={Computer programming languages;},
note={Meta model;Monticello;Refactorings;Smalltalk;Versioning;},
URL={http://dx.doi.org/10.1016/j.cl.2011.11.001},
}


@article{20160501864382,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Connection between version control operations and quality change of the source code},
journal={Acta Cybernetica},
author={Farago, Csaba and Hegedus, Peter and Vegh, adam Zoltan and Ferenc, Rudolf},
volume={21},
number={4},
year={2014},
pages={585 - 607},
issn={0324721X},
abstract={Software erosion is a well-known phenomena, meaning that software quality is continuously decreasing due to the ever-ongoing modifications in the source code. In this research work we investigated this phenomena by studying the impact of version control commit operations (add, update, delete) on the quality of the code. We calculated the ISO/IEC 9126 quality attributes for thousands of revisions of an industrial and three open-source software systems with the help of the Columbus Quality Model. We also collected the cardinality of each version control operation type for every investigated revision. We performed Chisquared tests on contingency tables with rows of quality change and columns of version control operation commit types. We compared the results with random data as well. We identified that the relationship between the version control operations and quality change is quite strong. Great maintainability improvements are mostly caused by commits containing Add operation. Commits containing file updates only tend to have a negative impact on the quality. Deletions have a weak connection with quality, and we could not formulate a general statement.<br/>},
key={Open source software},
keywords={Codes (symbols);Computer control;Computer programming languages;Computer software selection and evaluation;Erosion;Information management;ISO Standards;Maintainability;Open systems;},
note={Chi-Squared test;Contingency table;ISO/IEC 9126;Open source software systems;Quality attributes;Quality modeling;Software maintainability;Source codes;},
URL={http://dx.doi.org/10.14232/actacyb.21.4.2014.4},
}


@inproceedings{20130615993981,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Triaging incoming change requests: Bug or commit history, or code authorship?},
journal={IEEE International Conference on Software Maintenance, ICSM},
author={Linares-Vasquez, Mario and Hossen, Kamal and Dang, Hoang and Kagdi, Huzefa and Gethers, Malcom and Poshyvanyk, Denys},
year={2012},
pages={451 - 460},
address={Riva del Garda, Trento, Italy},
abstract={There is a tremendous wealth of code authorship information available in source code. Motivated with the presence of this information, in a number of open source projects, an approach to recommend expert developers to assist with a software change request (e.g., a bug fixes or feature) is presented. It employs a combination of an information retrieval technique and processing of the source code authorship information. The relevant source code files to the textual description of a change request are first located. The authors listed in the header comments in these files are then analyzed to arrive at a ranked list of the most suitable developers. The approach fundamentally differs from its previously reported counterparts, as it does not require software repository mining. Neither does it require training from past bugs/issues, which is often done with sophisticated techniques such as machine learning, nor mining of source code repositories, i.e., commits. An empirical study to evaluate the effectiveness of the approach on three open source systems, ArgoUML, JEdit, and MuCommander, is reported. Our approach is compared with two representative approaches: 1) using machine learning on past bug reports, and 2) based on commit logs. The presented approach is found to provide recommendation accuracies that are equivalent or better than the two compared approaches. These findings are encouraging, as it opens up a promising and orthogonal possibility of recommending developers without the need of any historical change information. &copy; 2012 IEEE.<br/>},
key={Open systems},
keywords={Artificial intelligence;Codes (symbols);Computer programming languages;Computer software maintenance;Information retrieval;Learning systems;Open source software;},
note={change request;code authorship;Developer recommendations;Open source projects;Recommendation accuracy;Software repository mining;Source code repositories;triaging;},
URL={http://dx.doi.org/10.1109/ICSM.2012.6405306},
}


@inproceedings{20153201154962,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A dictionary to translate change tasks to source code},
journal={11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings},
author={Kevic, Katja and Fritz, Thomas},
year={2014},
pages={320 - 323},
address={Hyderabad, India},
abstract={At the beginning of a change task, software developers spend a substantial amount of their time searching and navigating to locate relevant parts in the source code. Current approaches to support developers in this initial code search predominantly use information retrieval techniques that leverage the similarity between task descriptions and the identifiers of code elements to recommend relevant elements. However, the vocabulary or language used in source code often differs from the one used for describing change tasks, especially since the people developing the code are not the same as the ones reporting bugs or defining new features to be implemented. In our work, we investigate the creation of a dictionary that maps the different vocabularies using information from change sets and interaction histories stored with previously completed tasks. In an empirical analysis on four open source projects, our approach substantially improved upon the results of traditional information retrieval techniques for recommending relevant code elements.<br/>},
key={Open systems},
keywords={Codes (symbols);Computer programming languages;Glossaries;Information retrieval;Information use;Location;Open source software;},
note={Change tasks;Code search;Empirical analysis;Interaction history;Open source projects;Software developer;Source codes;Task description;},
URL={http://dx.doi.org/10.1145/2597073.2597095},
}


@inproceedings{20183205659967,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Reconciling the past and the present: An empirical study on the application of source code transformations to automatically rejuvenate Java programs},
journal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
author={Dantas, Reno and Carvalho, Antonio and Marcilio, Diego and Fantin, Luisa and Silva, Uriel and Lucas, Walter and Bonifacio, Rodrigo},
volume={2018-March},
year={2018},
pages={497 - 501},
address={Campobasso, Italy},
abstract={Software systems change frequently over time, either due to new business requirements or technology pressures. Programming languages evolve in a similar constant fashion, though when a language release introduces new programming constructs, older constructs and idioms might become obsolete. The coexistence between newer and older constructs leads to several problems, such as increased maintenance efforts and steeper learning curve for developers. In this paper we present a RASCAL Java transformation library that evolves legacy systems to use more recent programming language constructs (such as multi-catch and lambda expressions). In order to understand how relevant automatic software rejuvenation is, we submitted 2462 transformations to 40 open source projects via the GitHub pull request mechanism. Initial results show that simple transformations, for instance the introduction of the diamond operator, are more likely to be accepted than transformations that change the code substantially, such as refactoring enhanced for loops to the newer functional style.<br/> &copy; 2018 IEEE.},
key={Java programming language},
keywords={Application programs;Cosine transforms;Legacy systems;Open source software;Open systems;Reengineering;},
note={Business requirement;Empirical studies;Learning curves;Maintenance efforts;Open source projects;Software rejuvenation;Software systems;Source code transformation;},
URL={http://dx.doi.org/10.1109/SANER.2018.8330247},
}


@inproceedings{20130115856656,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Locating source code to be fixed based on initial bug reports - A case study on the eclipse project},
journal={Proceedings - 2012 4th International Workshop on Empirical Software Engineering in Practice, IWESEP 2012},
author={Bangcharoensap, Phiradet and Ihara, Akinori and Kamei, Yasutaka and Matsumoto, Ken-Ichi},
year={2012},
pages={10 - 15},
address={Osaka, Japan},
abstract={In most software development, a Bug Tracking System is used to improve software quality. Based on bug reports managed by the bug tracking system, triagers who assign a bug to fixers and fixers need to pinpoint buggy files that should be fixed. However if triagers do not know the details of the buggy file, it is difficult to select an appropriate fixer. If fixers can identify the buggy files, they can fix the bug in a short time. In this paper, we propose a method to quickly locate the buggy file in a source code repository using 3 approaches, text mining, code mining, and change history mining to rank files that may be causing bugs. (1) The text mining approach ranks files based on the textual similarity between a bug report and source code. (2) The code mining approach ranks files based on prediction of the fault-prone module using source code product metrics. (3) The change history mining approach ranks files based on prediction of the fault-prone module using change process metrics. Using Eclipse platform project data, our proposed model gains around 20% in TOP1 prediction. This result means that the buggy files are ranked first in 20% of bug reports. Furthermore, bug reports that consist of a short description and many specific words easily identify and locate the buggy file. &copy; 2012 IEEE.<br/>},
key={Codes (symbols)},
keywords={Computer programming languages;Computer software selection and evaluation;Data mining;Forecasting;Software design;Tracking (position);},
note={Bug localizations;Bug tracking system;Change history;Fault-prone modules;Software Quality;Source code repositories;Text mining;Textual similarities;},
URL={http://dx.doi.org/10.1109/IWESEP.2012.14},
}


@inproceedings{20185106252838,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Linking source code to untangled change intents},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
author={Liu, Xiaoyu and Huang, Liguo and Liu, Chuanyi and Ng, Vincent},
year={2018},
pages={393 - 403},
address={Madrid, Spain},
abstract={Previous work [13] suggests that tangled changes (i.e., different change intents aggregated in one single commit message) could complicate tracing to different change tasks when developers manage software changes. Identifying links from changed source code to untangled change intents could help developers solve this problem. Manually identifying such links requires lots of experience and review efforts, however. Unfortunately, there is no automatic method that provides this capability. In this paper, we propose AutoCILink, which automatically identifies code to untangled change intent links with a pattern-based link identification system (AutoCILink-P) and a supervised learning-based link classification system (AutoCILink-ML). Evaluation results demonstrate the effectiveness of both systems: The pattern-based AutoCILink-P and the supervised learning-based AutoCILink-ML achieve average accuracy of 74.6% and 81.2%, respectively.<br/> &copy; 2018 IEEE.},
key={Computer software maintenance},
keywords={Codes (symbols);Learning systems;Supervised learning;},
note={Automatic method;Change tasks;Classification system;Code changes;Commit;Evaluation results;Pattern-based;Software change;},
URL={http://dx.doi.org/10.1109/ICSME.2018.00047},
}


@inproceedings{20153001068614,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Bug prediction for fine-grained source code changes},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
author={Yuan, Zi and Yu, Lili and Liu, Chao},
volume={2013-January},
number={January},
year={2013},
pages={381 - 387},
issn={23259000},
address={Boston, MA, United states},
abstract={Software is constructed by a series of changes and each change has a risk of introducing bugs. Building bug prediction models for software changes can help developers know the existence of bugs immediately upon the completion of the change, which allows them to allocate more resources of testing and inspecting on the current risky changes, and to find and fix the introduced bugs timely. In this paper, we present a bug prediction model for fine-grained source code changes based on machine learning method, which takes a fine-grained source code change as a learning instance and a series of properties of the fine-grained change as features. This model has two desirable qualities: 1) Compared with previous research work that building bug prediction models for software changes at the file level or commit level (including one or more files), this model can predict bugs for changes at the statement level, which increases the granularity of prediction and thus reduces manual inspection efforts for developers. 2) This model can help developers or managers gain better knowledge on key factors of bug injection and provide guidance for software change of high quality. From the experiments on 8 famous open source projects, we observe that when using Random Forest as the classifier, the model proposed in this paper achieves the best performance, which can predict bugs for fine-grained source code changes with 78% precision, 71% recall, and 75% F-measure on average. Furthermore, among all the four feature groups (i.e. where, what, who, and when) defined in this paper, where is most influential, which has the strongest discriminative power in predicting bugs.<br/> Copyright &copy; 2013 by Knowledge Systems Institute Graduate School.},
key={Open source software},
keywords={Codes (symbols);Computer programming languages;Decision trees;Engineering research;Forecasting;Knowledge engineering;Learning systems;Open systems;Program debugging;Software testing;},
note={Bug predictions;Code metrics;Discriminative power;Fine-grained changes;Fine-grained source code changes;Manual inspection;Open source projects;Provide guidances;},
}


@inproceedings{20140217175989,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Chronos: Visualizing slices of source-code history},
journal={2013 1st IEEE Working Conference on Software Visualization - Proceedings of VISSOFT 2013},
author={Servant, Francisco and Jones, James A.},
year={2013},
address={Eindhoven, Netherlands},
abstract={In this paper, we present CHRONOS - a tool that enables the querying, exploration, and discovery of historical change events to source code. Unlike traditional Revision-Control-System tools, CHRONOS allows queries across any subset of the code, down to the line-level, which can potentially be contiguous or disparate, even among multiple files. In addition, CHRONOS provides change history across all historical versions (i.e., it is not limited to a pairwise "diff"). The tool implements a zoom-able user interface as a visualization of the history of the queried code to provide both a high-level view of the changes, which supports pattern recognition and discovery, and a low-level view that supports semantic comprehension for tasks such as reverse engineering and identifying design rationale. In this paper, we describe use cases in which CHRONOS may be helpful, provide a motivating example to demonstrate the benefits brought by CHRONOS, and describe its visualization in detail. &copy; 2013 IEEE.<br/>},
key={Codes (symbols)},
keywords={Pattern recognition;Reverse engineering;Semantics;User interfaces;Visualization;},
note={Change history;Design rationale;Historical changes;Revision control systems;Semantic comprehension;Source codes;},
URL={http://dx.doi.org/10.1109/VISSOFT.2013.6650547},
}


@inproceedings{20182505333135,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Automatic algorithm recognition of source-code using machine learning},
journal={Proceedings - 16th IEEE International Conference on Machine Learning and Applications, ICMLA 2017},
author={Shalaby, Maged and Mehrez, Tarek and El Mougy, Amr and Abdulnasser, Khalid and Al-Safty, Aysha},
volume={2017-December},
year={2018},
pages={170 - 177},
address={Cancun, Mexico},
abstract={As codebases for software projects get larger, reaching ranges of millions of lines of code, the need for computeraided program comprehension grows. We define one of the tasks of program comprehension to be algorithm recognition: Given a piece of source-code from a file, identify the algorithm this code is implementing, such as brute-force or dynamic programming. Most research in this area is making use of pattern matching, which involves much human effort and is of questionable accuracy when the structure and semantics of programs change. Thus, this paper proposes to let go of defined patterns, and make use of simpler features, such as counts of variables and counts of different constructs to recognize algorithms. We then feed these features to a classification algorithm to predict the class or type of algorithm used in this source code. We show through experimental results that our proposed method achieves a good improvement over baseline.<br/> &copy; 2017 IEEE.},
key={Learning algorithms},
keywords={Artificial intelligence;Codes (symbols);Computer programming languages;Dynamic programming;Learning systems;Pattern matching;Semantics;},
note={Algorithm recognition;Automatic algorithms;Classification algorithm;Computer-aided;Feature recognition;Lines of code;Program comprehension;Software project;},
URL={http://dx.doi.org/10.1109/ICMLA.2017.00033},
}


@inproceedings{20140217192377,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={LHDiff: Tracking source code lines to support software maintenance activities},
journal={IEEE International Conference on Software Maintenance, ICSM},
author={Asaduzzaman, Muhammad and Roy, Chanchal K. and Schneider, Kevin A. and Penta, Massimiliano Di},
year={2013},
pages={484 - 487},
address={Eindhoven, Netherlands},
abstract={Tracking lines across versions of a file is a necessary step for solving a number of problems during software development and maintenance. Examples include, but are not limited to, locating bug-inducing changes, tracking code fragments or vulnerable instructions across versions, co-change analysis, merging file versions, reviewing source code changes, and software evolution analysis. In this tool demonstration, we present a language-independent line-level location tracker, named LHDiff, that can be used to track lines and analyze changes in various kinds of software artifacts, ranging from source code to arbitrary text files. The tool can effectively detect changed or moved lines across versions of a file, has the ability to detect line splits, and can easily be integrated with existing version control systems. It overcomes the limitations of existing language-independent techniques and is even comparable to tools that are language dependent. In addition to describing the tool, we also describe its effectiveness in analyzing source code artifacts. &copy; 2013 IEEE.<br/>},
key={Computer software maintenance},
keywords={Codes (symbols);Computer programming languages;Software design;},
note={Differencing tools;Language independents;Line tracking;Software development and maintenances;Software evolution analysis;Software maintenance activity;Source code changes;Version control system;},
URL={http://dx.doi.org/10.1109/ICSM.2013.78},
}


@article{20181004854165,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Comparative study between two approaches using edit operations and code differences to detect past refactorings},
journal={IEICE Transactions on Information and Systems},
author={Omori, Takayuki and Maruyama, Katsuhisa},
volume={E101D},
number={3},
year={2018},
pages={644 - 658},
issn={09168532},
abstract={Understanding which refactoring transformations were performed is in demand in modern software constructions. Traditionally, many researchers have been tackling understanding code changes with history data derived from version control systems. In those studies, problems of the traditional approach are pointed out, such as entanglement of multiple changes. To alleviate the problems, operation histories on IDEs' code editors are available as a new source of software evolution data nowadays. By replaying such histories, we can investigate past code changes in a fine-grained level. However, the prior studies did not provide enough evidence of their effectiveness for detecting refactoring transformations. This paper describes an experiment in which participants detect refactoring transformations performed by other participants after investigating the code changes with an operation-replay tool and diff tools. The results show that both approaches have their respective factors that pose misunderstanding and overlooking of refactoring transformations. Two negative factors on divided operations and generated compound operations were observed in the operation-based approach, whereas all the negative factors resulted from three problems on tangling, shadowing, and out-of-order of code changes in the difference-based approach. This paper also shows seven concrete examples of participants' mistakes in both approaches. These findings give us hints for improving existing tools for understanding code changes and detecting refactoring transformations.<br/> &copy; 2018 The Institute of Electronics, Information and Communication Engineers.},
key={Codes (symbols)},
keywords={Information science;Software engineering;},
note={Code changes;Comparative studies;Compound operation;Refactorings;Software construction;Software Evolution;Traditional approaches;Version control system;},
URL={http://dx.doi.org/10.1587/transinf.2017EDP7160},
}


@inproceedings{20171203461256,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Predicting bug inducing source code change patterns},
journal={ICOSST 2016 - 2016 International Conference on Open Source Systems and Technologies, Proceedings},
author={Khan, Ayazuddin and Ahsan, Syed Nadeem},
year={2016},
pages={29 - 35},
address={Lahore, Pakistan},
abstract={A change in source code without the prior analysis of its impact may generate one or more defects. Fixing of such defects consumes maintenance time which ultimately increases the cost of software maintenance. Therefore, in the recent years, several research works have been done to develop techniques for the automatic impact analysis of changes in source code. In this paper, we propose to use Frequent Pattern Mining (FPM) technique of machine learning for the automatic impact analysis of those changes in source code which may induce bugs. Therefore, to find patterns associated with some specific types of software changes, we applied FPM's algorithms' Apriori and Predictive Apriori on the stored data of software changes of the following three Open-Source Software (OSS) projects: Mozilla, GNOME, and Eclipse. We grouped the data of software changes into two major categories: changes to meet bug fixing requirements and changes to meet requirements other than bug fixing. In the case of bug fixing requirements, we predict source files which are frequently changed together to fix any one of the following four types of bugs related to: memory (MEMORY), variables locking (LOCK), system (SYSTEM) and graphical user interface (UI). Our experimental results predict several interesting software change patterns which may induce bugs. The obtained bug inducing patterns have high confidence and accuracy value i.e., more than 90%.<br/> &copy; 2016 IEEE.},
key={Open source software},
keywords={Codes (symbols);Computer programming languages;Data mining;Defects;Forecasting;Graphical user interfaces;Learning systems;Locks (fasteners);Open systems;Program debugging;},
note={Frequent pattern mining;Impact analysis;Software change;Specific Types of Bugs;Transaction data;},
URL={http://dx.doi.org/10.1109/ICOSST.2016.7838573},
}


@inproceedings{20151200653070,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={On automatically generating commit messages via summarization of source code changes},
journal={Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014},
author={Cortes-Coy, Luis Fernando and Linares-Vasquez, Mario and Aponte, Jairo and Poshyvanyk, Denys},
year={2014},
pages={275 - 284},
address={Victoria, BC, Canada},
abstract={Although version control systems allow developers to describe and explain the rationale behind code changes in commit messages, the state of practice indicates that most of the time such commit messages are either very short or even empty. In fact, in a recent study of 23K+ Java projects it has been found that only 10% of the messages are descriptive and over 66% of those messages contained fewer words as compared to a typical English sentence (i.e., 15-20 words). However, accurate and complete commit messages summarizing software changes are important to support a number of development and maintenance tasks. In this paper we present an approach, coined as Change Scribe, which is designed to generate commit messages automatically from change sets. Change Scribe generates natural language commit messages by taking into account commit stereotype, the type of changes (e.g., files rename, changes done only to property files), as well as the impact set of the underlying changes. We evaluated Change Scribe in a survey involving 23 developers in which the participants analyzed automatically generated commit messages from real changes and compared them with commit messages written by the original developers of six open source systems. The results demonstrate that automatically generated messages by Change Scribe are preferred in about 62% of the cases for large commits, and about 54% for small commits.<br/> &copy; 2014 IEEE.},
key={Open systems},
keywords={Codes (symbols);Natural language processing systems;Open source software;Surveys;},
note={Automatically generated;Code changes;commit message;Maintenance tasks;Open source system;Source code changes;summarization;Version control system;},
URL={http://dx.doi.org/10.1109/SCAM.2014.14},
}


@article{20171203489545,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Unravel programming sessions with THRESHER: Identifying coherent and complete sets of fine-granular source code changes},
journal={Computer Software},
author={Taeumel, Marcel and Platz, Stephanie and Steinert, Bastian and Hirschfeld, Robert and Masuhara, Hidehiko},
volume={34},
number={1},
year={2017},
pages={103 - 118},
issn={02896540},
abstract={Development teams benefit from version control systems, which manage shared access to code repositories and persist entire project histories for analysis or recovery. Such systems will be efficient if developers commit coherent and complete change sets. These best practices, however, are difficult to follow because multiple activities often interleave without notice and existing tools impede unraveling changes before committing them. We propose an interactive, graphical tool, called Thresher, that employs adaptable scripts to support developers to group and commit changes-especially for fine-granular change tracking where numerous changes are logged even in short programming sessions. We implemented our tool in Squeak/Smalltalk and derived a foundation of scripts from five refactoring sessions. We evaluated those scripts' precision and recall, which indicate a reduced manual effort because developers can focus on project-specific adjustments. Having such an interactive approach, they can easily intervene to accurately reconstruct activities and thus follow best practices.<br/> &copy; 2017, Japan Society for Software Science and Technology. All rights reserved.},
key={Control system analysis},
keywords={Computer software;Software engineering;},
note={Best practices;Change tracking;Development teams;Graphical tools;Interactive approach;Precision and recall;Source code changes;Version control system;},
URL={http://dx.doi.org/10.11309/jssst.34.1_103},
}


@inproceedings{20170803366605,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using topic model to suggest fine-grained source code changes},
journal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
author={Nguyen, Hoan Anh and Nguyen, Anh Tuan and Nguyen, Tien N.},
year={2016},
pages={200 - 210},
address={Raleigh, NC, United states},
abstract={Prior research has shown that source code and its changes are repetitive. Several approaches have leveraged that phenomenon to detect and recommend change/fix patterns. In this paper, we propose TasC, a model that leverages the context of change tasks in development history to suggest fine-grained code change/fix at the program statement level. We use Latent Dirichlet Allocation (LDA) to capture the change task context via co-occurring program elements in the changes in a context. We also propose a novel technique for measuring the similarity of code fragments and code changes using the task context. We conducted an empirical evaluation on a large dataset of 88 open-source Java projects containing more than 200 thousand source files and 3.5 million source lines of code in their last revisions with 423 thousand changed methods. Our result shows that TasC relatively improves recommendation accuracy up to 130%-250% in comparison with the base models that do not use task context. Compared with other types of contexts, TasC outperforms the models using structural and co-change contexts.<br/> &copy; 2016 IEEE.},
key={Open systems},
keywords={Codes (symbols);Computer software maintenance;Open source software;Statistics;},
note={Development history;Empirical evaluations;Fine-grained source code changes;Latent dirichlet allocations;Novel techniques;Program statements;Recommendation accuracy;Source lines of codes;},
URL={http://dx.doi.org/10.1109/ICSME.2016.40},
}


@inproceedings{20151900829151,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Detecting and investigating the source code changes using logical rules},
journal={2014 International Conference on Circuits, Power and Computing Technologies, ICCPCT 2014},
author={Kodhai, E. and Dhivya, B.},
year={2014},
pages={1603 - 1608},
address={Nagercoil, Tamil Nadu, India},
abstract={Software developers often need to examine program differences between two versions and reason about the changes. Analyzing the changes is the task. To facilitate the programmers to represent the high level source code changes, this proposed system introduces the rule-based program differencing approach to represent the changes as logical rules. This approach is instantiated with three levels: first level describes the changes in method header names and signature; second level captures change in the code level and structural dependences; and third level identifies the same set of function with different name. This approach concisely represents the systematic changes and helps the software engineers to recognize the program differences. This approach can be applied in open source project to examine the difference among program version.<br/> &copy; 2014 IEEE.},
key={Open systems},
keywords={Codes (symbols);Computer programming languages;Open source software;},
note={Logical rules;Open source projects;Open sources;Rule based programs;Software developer;Source code changes;Structural dependence;Systematic changes;},
URL={http://dx.doi.org/10.1109/ICCPCT.2014.7054763},
}


@article{20144900301289,
language={Chinese},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Bug prediction method for fine-grained source code changes},
journal={Ruan Jian Xue Bao/Journal of Software},
author={Yuan, Zi and Yu, Li-Li and Liu, Chao},
volume={25},
number={11},
year={2014},
pages={2499 - 2517},
issn={10009825},
abstract={Software changes constantly in its lifecycle to adapt to the changing requirements and environments. In order to predict whether each change will introduce any bugs timely, various bug prediction methods for software source code changes have been proposed by researchers. However, there are three deficiencies in existing methods: 1) The prediction granularities are limited at the coarse-grained levels (i.e. transaction or file levels); 2) As vector space model is used to represent software changes, abundant information in software repositories, such as program structure, natural language semantic and history information, can not be mined sufficiently; 3) Only short-time prediction is explored without considering the concept drift caused by new requirements, team restructuring or other external factors during the long time software evolution process. In order to overcome the shortcomings of existing methods, a bug prediction method for source code changes is proposed. It makes prediction for fine-grained (i.e. statement level) changes, which reduces the quality assurance cost effectively. By in-depth mining software repositories with static program analysis and natural language semantic inference technologies, feature sets of changes are constructed in four aspects (i.e. context, content, time, and developer) and key factors that lead to bug injection are revealed. Characteristics of concept drift in software evolution process are analyzed by using matrix of feature entropy difference, and an algorithm of adaptive window with concept reviewing is proposed to achieve stability of long-time prediction. Experiments on six famous open source projects demonstrate effectiveness of the proposed method.<br/> &copy; 2014 ISCAS.},
key={Open source software},
keywords={Codes (symbols);Computer programming languages;Cost effectiveness;Forecasting;Program debugging;Quality assurance;Semantics;Vector spaces;},
note={Bug predictions;Concept drifts;Fine-grained changes;Fine-grained source code changes;Mining software repositories;Natural language semantics;Software Evolution;Software evolution process;},
URL={http://dx.doi.org/10.13328/j.cnki.jos.004559},
}


@inproceedings{20124815720878,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The storyteller version control system tackling version control, code comments, and team learning},
journal={SPLASH'12 - Proceedings of the 2012 ACM Conference on Systems, Programming, and Applications: Software for Humanity},
author={Mahoney, Mark},
year={2012},
pages={17 - 18},
address={Tucson, AZ, United states},
abstract={This demonstration shows the Storyteller version control system. The tool aims to change the way software developers learn by opening up for examination how they do their work. The tool has traditional version control functionality (branching and merging) but in addition it records how development work is done, organizes it, and allows it to be played back for others. Most importantly, the tool allows developers to tell stories about what they did and why. It captures and organizes institutional knowledge that would otherwise be lost.<br/>},
key={Computer systems programming},
keywords={Application programs;Control systems;Information management;},
note={Institutional knowledge;Software developer;Software Evolution;Team learning;Version control;Version control functionalities;Version control system;},
URL={http://dx.doi.org/10.1145/2384716.2384725},
}


@article{20161002069058,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Slicing fine-grained code change history},
journal={IEICE Transactions on Information and Systems},
author={Maruyama, Katsuhisa and Omori, Takayuki and Hayashi, Shinpei},
volume={E99D},
number={3},
year={2016},
pages={671 - 687},
issn={09168532},
abstract={Change-aware development environments can automatically record fine-grained code changes on a program and allow programmers to replay the recorded changes in chronological order. However, since they do not always need to replay all the code changes to investigate how a particular entity of the program has been changed, they often eliminate several code changes of no interest by manually skipping them in replaying. This skipping action is an obstacle that makes many programmers hesitate when they use existing replaying tools. This paper proposes a slicing mechanism that automatically removes manually skipped code changes from the whole history of past code changes and extracts only those necessary to build a particular class member of a Java program. In this mechanism, fine-grained code changes are represented by edit operations recorded on the source code of a program and dependencies among edit operations are formalized. The paper also presents a running tool that slices the operation history and replays its resulting slices. With this tool, programmers can avoid replaying nonessential edit operations for the construction of class members that they want to understand. Experimental results show that the tool offered improvements over conventional replaying tools with respect to the reduction of the number of edit operations needed to be examined and over history filtering tools with respect to the accuracy of edit operations to be replayed.<br/> Copyright &copy; 2016 The Institute of Electronics, Information and Communication Engineers.},
key={Codes (symbols)},
keywords={Computer programming;Computer software;},
note={Code changes;Integrated development environment;Program comprehension;Program slicing;Software maintenance and evolution;},
URL={http://dx.doi.org/10.1587/transinf.2015EDP7282},
}


@inproceedings{20131816291341,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Slicing and replaying code change history},
journal={Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012},
author={Maruyama, Katsuhisa and Kitsu, Eijiro and Omori, Takayuki and Hayashi, Shinpei},
year={2012},
pages={246 - 249},
address={Essen, Germany},
abstract={Change-aware development environments have recently become feasible and reasonable. These environments can automatically record fine-grained code changes on a program and allow programmers to replay the recorded changes in chronological order. However, they do not always need to replay all the code changes to investigate how a particular entity of the program has been changed. Therefore, they often skip several code changes of no interest. This skipping action is an obstacle that makes many programmers hesitate in using existing replaying tools. This paper proposes a slicing mechanism that can extract only code changes necessary to construct a particular class member of a Java program from the whole history of past code changes. In this mechanism, fine-grained code changes are represented by edit operations recorded on source code of a program. The paper also presents a running tool that implements the proposed slicing and replays its resulting slices. With this tool, programmers can avoid replaying edit operations nonessential to the construction of class members they want to understand. &copy; 2012 ACM.<br/>},
key={Codes (symbols)},
keywords={Computer programming;Computer software;},
note={Code changes;Integrated development environment;Program comprehension;Program slicing;Software maintenance and evolution;},
}


@inproceedings{20124015532198,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Slicing and replaying code change history},
journal={2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings},
author={Maruyama, Katsuhisa and Kitsu, Eijiro and Omori, Takayuki and Hayashi, Shinpei},
year={2012},
pages={246 - 249},
address={Essen, Germany},
abstract={Change-aware development environments have recently become feasible and reasonable. These environments can automatically record fine-grained code changes on a program and allow programmers to replay the recorded changes in chronological order. However, they do not always need to replay all the code changes to investigate how a particular entity of the program has been changed. Therefore, they often skip several code changes of no interest. This skipping action is an obstacle that makes many programmers hesitate in using existing replaying tools. This paper proposes a slicing mechanism that can extract only code changes necessary to construct a particular class member of a Java program from the whole history of past code changes. In this mechanism, fine-grained code changes are represented by edit operations recorded on source code of a program. The paper also presents a running tool that implements the proposed slicing and replays its resulting slices. With this tool, programmers can avoid replaying edit operations nonessential to the construction of class members they want to understand. Copyright 2012 ACM.<br/>},
key={Codes (symbols)},
keywords={Computer programming;Computer software;},
note={Code changes;Integrated development environment;Program comprehension;Program slicing;Software maintenance and evolution;},
URL={http://dx.doi.org/10.1145/2351676.2351713},
}


@inproceedings{20122615174176,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Clustering source code files to predict change propagation during software maintenance},
journal={Proceedings of the Annual Southeast Conference},
author={Bailey, Megan and Lin, King-Ip and Sherrell, Linda},
year={2012},
pages={106 - 111},
address={Tuscaloosa, AL, United states},
abstract={This paper addresses the question that is frequently considered by software developers performing maintenance tasks on large systems: "If I make a change in this file, are there other files that need to change too?" If a development tool could automatically answer this question, then time and money could be saved during software maintenance. The proposed solution follows trends from past research results in using data mining techniques and information extracted from the CVS change repository. We define a distance measure using both the revision history of files and text-based information, cluster change sets of files, and then calculate a membership value of each file to each cluster to create groupings of files that are likely to be changed together in the future. Our approach predicts files that may need to be modified based on these clusters, and we evaluate these predictions on portions of the open-source Eclipse project. &copy; 2012 ACM.<br/>},
key={Data mining},
keywords={Computer software maintenance;Forecasting;Machinery;Open source software;},
note={Change prediction;Change propagation;Development tools;Membership values;Mining software repositories;Performing maintenance;Software developer;Text-based information;},
URL={http://dx.doi.org/10.1145/2184512.2184538},
}


@inproceedings{20183105617355,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning to accelerate symbolic execution via code transformation},
journal={Leibniz International Proceedings in Informatics, LIPIcs},
author={Chen, Junjie and Hu, Wenxiang and Zhang, Lingming and Hao, Dan and Khurshid, Sarfraz and Zhang, Lu},
volume={109},
year={2018},
pages={AITO - },
issn={18688969},
address={Amsterdam, Netherlands},
abstract={Symbolic execution is an effective but expensive technique for automated test generation. Over the years, a large number of refined symbolic execution techniques have been proposed to improve its efficiency. However, the symbolic execution efficiency problem remains, and largely limits the application of symbolic execution in practice. Orthogonal to refined symbolic execution, in this paper we propose to accelerate symbolic execution through semantic-preserving code transformation on the target programs. During the initial stage of this direction, we adopt a particular code transformation, compiler optimization, which is initially proposed to accelerate program concrete execution by transforming the source program into another semantic-preserving target program with increased efficiency (e.g., faster or smaller). However, compiler optimizations are mostly designed to accelerate program concrete execution rather than symbolic execution. Recent work also reported that unified settings on compiler optimizations that can accelerate symbolic execution for any program do not exist at all. Therefore, in this work we propose a machine-learning based approach to tuning compiler optimizations to accelerate symbolic execution, whose results may also aid further design of specific code transformations for symbolic execution. In particular, the proposed approach LEO separates source-code functions and libraries through our program-splitter, and predicts individual compiler optimization (i.e., whether a type of code transformation is chosen) separately through analyzing the performance of existing symbolic execution. Finally, LEO applies symbolic execution on the code transformed by compiler optimization (through our local-optimizer). We conduct an empirical study on GNU Coreutils programs using the KLEE symbolic execution engine. The results show that LEO significantly accelerates symbolic execution, outperforming the default KLEE configurations (i.e., turning on/off all compiler optimizations) in various settings, e.g., with the default training/testing time, LEO achieves the highest line coverage in 50/68 programs, and its average improvement rate on all programs is 46.48%/88.92% in terms of line coverage compared with turning on/off all compiler optimizations.<br/> &copy; Junjie Chen, Wenxiang Hu, Lingming Zhang, Dan Hao, Sarfraz Khurshid, and Lu Zhang.},
key={Program compilers},
keywords={Artificial intelligence;Codes (symbols);Concretes;Cosine transforms;Efficiency;Learning algorithms;Learning systems;Model checking;Object oriented programming;Open source software;Orbits;Semantics;Software testing;},
note={Automated test generations;Code transformation;Compiler optimizations;Empirical studies;Its efficiencies;Local optimizers;Source codes;Symbolic execution;},
URL={http://dx.doi.org/10.4230/LIPIcs.ECOOP.2018.6},
}


@inproceedings{20151800813921,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Historef: A tool for edit history refactoring},
journal={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
author={Hayashi, Shinpei and Hoshino, Daiki and Matsuda, Jumpei and Saeki, Motoshi and Omori, Takayuki and Maruyama, Katsuhisa},
year={2015},
pages={469 - 473},
address={Montreal, QC, Canada},
abstract={This paper presents Historef, a tool for automating edit history refactoring on Eclipse IDE for Java programs. The aim of our history refactorings is to improve the understandability and/or usability of the history without changing its whole effect. Historef enables us to apply history refactorings to the recorded edit history in the middle of the source code editing process by a developer. By using our integrated tool, developers can commit the refactored edits into underlying SCM repository after applying edit history refactorings so that they are easy to manage their changes based on the performed edits.<br/> &copy; 2015 IEEE.},
key={Computer software},
note={Integrated tools;Java program;Refactorings;Software Evolution;Source codes;tangled changes;Understandability;},
URL={http://dx.doi.org/10.1109/SANER.2015.7081858},
}


@inproceedings{20151100637332,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Identifying change patterns in software history},
journal={CEUR Workshop Proceedings},
author={Dagit, Jason and Sottile, Matthew},
volume={1008},
year={2013},
issn={16130073},
address={Florence, Italy},
abstract={Traditional algorithms for detecting differences in source code focus on differences between lines. As such, little can be learned about abstract changes that occur over time within a project. Structural differencing on the program's abstract syntax tree reveals changes at the syntactic level within code, which allows us to further process the differences to understand their meaning. We propose that grouping of changes by some metric of similarity, followed by pattern extraction via antiunification will allow us to identify patterns of change within a software project from the sequence of changes contained within a Version Control System (VCS). Tree similarity metrics such as a tree edit distance can be used to group changes in order to identify groupings that may represent a single class of change (e.g., adding a parameter to a function call). By applying antiunification within each group we are able to generalize from families of concrete changes to patterns of structural change. Studying patterns of change at the structural level, instead of line-by-line, allows us to gain insight into the evolution of software.<br/>},
key={Syntactics},
keywords={Trees (mathematics);Visualization;XML;},
note={Abstract Syntax Trees;Anti-unification;Pattern extraction;Soft ware evolutions;Structural difierencing;Tree edit distance;Version control;Version control system;},
}


@inproceedings{20135017066975,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Identifying implicit architectural dependencies using measures of source code change waves},
journal={Proceedings - 39th Euromicro Conference Series on Software Engineering and Advanced Applications, SEAA 2013},
author={Staron, Miroslaw and Meding, Wilhelm and Hoglund, Christoffer and Eriksson, Peter and Nilsson, Jimmy and Hansson, Jorgen},
year={2013},
pages={325 - 332},
address={Santander, Spain},
abstract={The principles of Agile software development are increasingly used in large software development projects, e.g. using Scrum of Scrums or combining Agile and Lean development methods. When large software products are developed by self-organized, usually feature-oriented teams, there is a risk that architectural dependencies between software components become uncontrolled. In particular there is a risk that the prescriptive architecture models in form of diagrams are outdated and implicit architectural dependencies may become more frequent than the explicit ones. In this paper we present a method for automated discovery of potential dependencies between software components based on analyzing revision history of software repositories. The result of this method is a map of implicit dependencies which is used by architects in decisions on the evolution of the architecture. The software architects can assess the validity of the dependencies and can prevent unwanted component couplings and design erosion hence minimizing the risk of post-release quality problems. Our method was evaluated in a case study at one large product at Saab Electronic Defense Systems (Saab EDS) and one large software product at Ericsson AB. &copy; 2013 IEEE.<br/>},
key={Software design},
keywords={Application programs;Architecture;Image quality;Industry;Risk assessment;Software architecture;},
note={Agile software development;dependency;Electronic defense systems;measure;metric;Mining software repositories;Software development projects;Software repositories;},
URL={http://dx.doi.org/10.1109/SEAA.2013.9},
}


@inproceedings{20132016323410,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A history querying tool and its application to detect multi-version refactorings},
journal={Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
author={Stevens, Reinout and De Roover, Coen and Noguera, Carlos and Jonckers, Viviane},
year={2013},
pages={335 - 338},
issn={15345351},
address={Genova, Italy},
abstract={Version Control Systems (VCS) have become indispensable in developing software. In order to provide support for change management, they track the history of software projects. Tool builders can exploit this latent historical information to provide insights in the evolution of the project. For example, the information needed to identify when and where a particular refactoring was applied is implicitly present in the VCS. However, tool support for eliciting this information is lacking. So far, no general-purpose history querying tool capable of answering a wide variety of questions about the evolution of software exists. Therefore, we generalize the idea of a program querying tool to a history querying tool. A program querying tool reifies the program's code into a knowledge base, from which it retrieves elements that exhibit characteristics specified through a user-provided program query. Our history querying tool, QwalKeko, enables specifying the evolution of source code characteristics across multiple versions of Java projects versioned in Git. We apply QwalKeko to the problem of detecting refactorings, specified as the code changes induced by each refactoring. These specifications stem from the literature, but are limited to changes between two successive versions. We demonstrate the expressiveness of our tool by generalizing the specifications such that refactorings can span multiple versions. &copy; 2013 IEEE.<br/>},
key={Computer software maintenance},
keywords={Codes (symbols);Knowledge based systems;Reengineering;Specifications;},
note={Change management;Historical information;ITS applications;Program comprehension;Refactorings;Software project;Software repositories;Version control system;},
URL={http://dx.doi.org/10.1109/CSMR.2013.44},
}


@inproceedings{20144600189252,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Fine-grained and accurate source code differencing},
journal={ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
author={Falleri, Jean-Remy and Morandat, Floreal and Blanc, Xavier and Martinez, Matias and Monperrus, Martin},
year={2014},
pages={313 - 323},
address={Vasteras, Sweden},
abstract={At the heart of software evolution is a sequence of edit actions, called an edit script, made to a source code file. Since software systems are stored version by version, the edit script has to be computed from these versions, which is known as a complex task. Existing approaches usually compute edit scripts at the text granularity with only add line and delete line actions. However, inferring syntactic changes from such an edit script is hard. Since moving code is a frequent action performed when editing code, it should also be taken into account. In this paper, we tackle these issues by introducing an algorithm computing edit scripts at the abstract syntax tree granularity including move actions. Our objective is to compute edit scripts that are short and close to the original developer intent. Our algorithm is implemented in a freely-available and extensible tool that has been intensively validated.<br/> &copy; 2014 ACM.},
key={Trees (mathematics)},
keywords={Abstracting;Codes (symbols);Software engineering;Syntactics;},
note={Abstract Syntax Trees;Algorithm computing;Complex task;Fine grained;Program comprehension;Software Evolution;Software systems;Tree differencing;},
URL={http://dx.doi.org/10.1145/2642937.2642982},
}


@inproceedings{20125015800507,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={A source code recommender system to support newcomers},
journal={Proceedings - International Computer Software and Applications Conference},
author={Malheiros, Yuri and Moraes, Alan and Trindade, Cleyton and Meira, Silvio},
year={2012},
pages={19 - 24},
issn={07303157},
address={Izmir, Turkey},
abstract={Newcomers in a software development project often need assistance to complete their first tasks. Then a mentor, an experienced member of the team, usually teaches the newcomers what they need to complete their tasks. But, to allocate an experienced member of a team to teach a newcomer during a long time is neither always possible nor desirable, because the mentor could be more helpful doing more important tasks. During the development the team interacts with a version control system, bug tracking and mailing lists, and all these tools record data creating the project memory. Recommender systems can use the project memory to help newcomers in some tasks answering their questions, thus in some cases the developers do not need a mentor. In this paper we present Mentor, a recommender system to help newcomers to solve change requests. Mentor uses the Prediction by Partial Matching (PPM) algorithm and some heuristics to analyze the change requests, and the version control data, and recommend potentially relevant source code that will help the developer in the change request solution. We did three experiments to compare the PPM algorithm with the Latent Semantic Indexing (LSI). Using PPM we achieved results for recall rate between 37% and 66.8%, and using LSI the results were between 20.3% and 51.6%. &copy; 2012 IEEE.<br/>},
key={Recommender systems},
keywords={Application programs;Computer software maintenance;Information management;Information theory;Semantics;Software design;Software engineering;},
note={Bug tracking;Latent Semantic Indexing;Mailing lists;Prediction by partial matching;Project memory;Software development projects;Version control;Version control system;},
URL={http://dx.doi.org/10.1109/COMPSAC.2012.11},
}


@article{20184305993998,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={CodeAttention: translating source code to comments by exploiting the code constructs},
journal={Frontiers of Computer Science},
author={Zheng, Wenhao and Zhou, Hongyu and Li, Ming and Wu, Jianxin},
year={2018},
issn={20952228},
abstract={Appropriate comments of code snippets provide insight for code functionality, which are helpful for program comprehension. However, due to the great cost of authoring with the comments, many code projects do not contain adequate comments. Automatic comment generation techniques have been proposed to generate comments from pieces of code in order to alleviate the human efforts in annotating the code.Most existing approaches attempt to exploit certain correlations (usually manually given) between code and generated comments, which could be easily violated if coding patterns change and hence the performance of comment generation declines. In addition, recent approaches ignore exploiting the code constructs and leveraging the code snippets like plain text. Furthermore, previous datasets are also too small to validate the methods and show their advantage. In this paper, we propose a new attention mechanism called CodeAttention to translate code to comments, which is able to utilize the code constructs, such as critical statements, symbols and keywords. By focusing on these specific points, CodeAttention could understand the semantic meanings of code better than previous methods. To verify our approach in wider coding patterns, we build a large dataset from open projects in GitHub. Experimental results in this large dataset demonstrate that the proposed method has better performance over existing approaches in both objective and subjective evaluation. We also perform ablation studies to determine effects of different parts in CodeAttention.<br/> &copy; 2018, Higher Education Press and Springer-Verlag GmbH Germany, part of Springer Nature.},
key={Automatic programming},
keywords={Codes (symbols);Learning systems;Recurrent neural networks;Semantics;},
note={Attention mechanisms;code comment generation;Code constructs;Coding patterns;Generation techniques;Objective and subjective evaluations;Program comprehension;Software minings;},
URL={http://dx.doi.org/10.1007/s11704-018-7457-6},
}


@inproceedings{20185106252827,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Generating accurate and compact edit scripts using tree differencing},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
author={Frick, Veit and Grassauer, Thomas and Beck, Fabian and Pinzger, Martin},
year={2018},
pages={264 - 274},
address={Madrid, Spain},
abstract={For analyzing changes in source code, edit scriptsare used to describe the differences between two versions of afile. These scripts consist of a list of actions that, applied to thesource file, result in the new version of the file. In contrast toline-based source code differencing, tree-based approaches suchas GumTree, MTDIFF, or ChangeDistiller extract changes bycomparing the abstract syntax trees (AST) of two versions of asource file. One benefit of tree-based approaches is their abilityto capture moved (sub) trees in the AST. Our approach, theIterative Java Matcher (IJM), builds upon GumTree and aims atgenerating more accurate and compact edit scripts that capturethe developer's intent. This is achieved by improving the qualityof the generated move and update actions, which are the mainsource of inaccurate actions generated by previous approaches. To evaluate our approach, we conducted a study with 11 external experts and manually analyzed the accuracy of 2400 randomly selected editactions. Comparing IJM to GumTree and MTDIFF, the resultsshow that IJM provides better accuracy for move and updateactions and is more beneficial to understanding the changes.<br/> &copy; 2018 IEEE.},
key={Computer software maintenance},
keywords={Syntactics;Trees (mathematics);},
note={Abstract Syntax Trees;Change extractions;External experts;Software Evolution;Source codes;Tree differencing;Tree-based approach;},
URL={http://dx.doi.org/10.1109/ICSME.2018.00036},
}


@inproceedings{20160601908398,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Co-evolution of infrastructure and source code - An empirical study},
journal={IEEE International Working Conference on Mining Software Repositories},
author={Jiang, Yujuan and Adams, Bram},
volume={2015-August},
year={2015},
pages={45 - 55},
issn={21601852},
address={Florence, Italy},
abstract={Infrastructure-as-code automates the process of configuring and setting up the environment (e.g., servers, VMs and databases) in which a software system will be tested and/or deployed, through textual specification files in a language like Puppet or Chef. Since the environment is instantiated automatically by the infrastructure languages' tools, no manual intervention is necessary apart from maintaining the infrastructure specification files. The amount of work involved with such maintenance, as well as the size and complexity of infrastructure specification files, have not yet been studied empirically. Through an empirical study of the version control system of 265 Open Stack projects, we find that infrastructure files are large and churn frequently, which could indicate a potential of introducing bugs. Furthermore, we found that the infrastructure code files are coupled tightly with the other files in a project, especially test files, which implies that testers often need to change infrastructure specifications when making changes to the test framework and tests.<br/> &copy; 2015 IEEE.},
key={Program debugging},
keywords={Association rules;Cloud computing;Codes (symbols);Couplings;Maintainability;Measurement;Servers;Software testing;Specifications;},
note={Co-evolution;Empirical studies;Manual intervention;Open stacks;Software systems;Source codes;Test framework;Version control system;},
URL={http://dx.doi.org/10.1109/MSR.2015.12},
}


@inproceedings{20140117155356,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Visualization of fine-grained code change history},
journal={Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},
author={Yoon, YoungSeok and Myers, Brad A. and Koo, Sebon},
year={2013},
pages={119 - 126},
issn={19436092},
address={San Jose, CA, United states},
abstract={Conventional version control systems save code changes at each check-in. Recently, some development environments retain more fine-grain changes. However, providing tools for developers to use those histories is not a trivial task, due to the difficulties in visualizing the history. We present two visualizations of fine-grained code change history, which actively interact with the code editor: a timeline visualization, and a code history diff view. Our timeline and filtering options allow developers to navigate through the history and easily focus on the information they need. The code history diff view shows the history of any particular code fragment, allowing developers to move through the history simply by dragging the marker back and forth through the timeline to instantly see the code that was in the snippet at any point in the past. We augment the usefulness of these visualizations with richer editor commands including selective undo and search, which are all implemented in an Eclipse plug-in called "Azurite". Azurite helps developers with answering common questions developers ask about the code change history that have been identified by prior research. In addition, many of users' backtracking tasks can be achieved using Azurite, which would be tedious or error-prone otherwise. &copy; 2013 IEEE.<br/>},
key={FORTH (programming language)},
keywords={Codes (symbols);Information filtering;Visual languages;Visualization;},
note={Code fragments;Development environment;Integrated development environment;Program comprehension;selective undo;Software visualization;Timeline visualizations;Version control system;},
URL={http://dx.doi.org/10.1109/VLHCC.2013.6645254},
}


@inproceedings{20143318061949,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={The effect of IMPORT change in software change history},
journal={Proceedings of the ACM Symposium on Applied Computing},
author={Kim, Jungil and Lee, Eunjoo},
year={2014},
pages={1753 - 1754},
address={Gyeongju, Korea, Republic of},
abstract={Source code change analysis in the project history is one of main issues in mining software repositories, which incorporates change prediction, API evolution and refactoring, etc. In the previous studies, fine-grained source code changes, that is, code level changes, were used to find source code change patterns. In this paper, we closely investigate IMPORT change type, which has not been unnoticed. At first, we performed modifying the existing change extraction tool, change distiller [3], to extract IMPORT change history. And then, we extracted commit history data from project repository of eclipse CDT and IDT. Change types for each change in commit history data have been determined using change types in [3] and IMPORT change types defined in this work. Finally, we analyzed the effect of IMPORT change using the frequency of each change type occurred in the commit history. Experimental result shows that the IMPORT change meaningfully affects other changes and it would be better to consider IMPORT change types in change analysis work. Copyright 2014 ACM.<br/>},
key={Data mining},
keywords={Codes (symbols);Computer programming languages;},
note={Change analysis;Change couplings;Change extractions;Change prediction;Fine-grained source code changes;Mining software repositories;Software change;Source code changes;},
URL={http://dx.doi.org/10.1145/2554850.2559925},
}


@inproceedings{20121915000165,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Enhancing history-based concern mining with fine-grained change analysis},
journal={Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
author={Hashimoto, Masatomo and Mori, Akira},
year={2012},
pages={75 - 84},
issn={15345351},
address={Szeged, Hungary},
abstract={Maintenance of large software projects is often hindered by cross-cutting concerns scattered over multiple modules. History-based mining techniques have been proposed to mitigate the difficultly by examining changes related to methods/functions in development history to suggest potential concerns. However, the techniques do not cope well with renamed entities and may lead to irrelevant information about concerns. The intricate procedures of the methods also make the results difficult for others to reproduce, utilize or improve. In this paper, we reinforce history-based concern mining techniques with fine-grained change analysis based on tree differencing on abstract syntax trees. Source code changes are recorded as facts over source code regions according to the RDF (Resource Description Framework) data model so that the analysis can be performed in terms of factbase queries. To show the capability of the method, we report on an experiment that emulates the state-of-the-art concern mining technique called COMMIT using our own change analysis tool called Diff/TS. A comparative case study on several open source projects written in C and Java shows that our technique improves results and overcomes the language barrier in the analysis. &copy; 2012 IEEE.<br/>},
key={C (programming language)},
keywords={Computer software maintenance;Open source software;Reengineering;Trees (mathematics);},
note={Abstract Syntax Trees;Cross-cutting concerns;Development history;Fine-grained changes;Mining techniques;Open source projects;Resource description framework;Source code changes;},
URL={http://dx.doi.org/10.1109/CSMR.2012.18},
}


@inproceedings{20160601908430,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using developer-interaction trails to triage change requests},
journal={IEEE International Working Conference on Mining Software Repositories},
author={Zanjani, Motahareh Bahrami and Kagdi, Huzefa and Bird, Christian},
volume={2015-August},
year={2015},
pages={88 - 98},
issn={21601852},
address={Florence, Italy},
abstract={The paper presents an approach, namely iHDev, to recommend developers who are most likely to implement incoming change requests. The basic premise of iHDev is that the developers who interacted with the source code relevant to a given change request are most likely to best assist with its resolution. A machine-learning technique is first used to locate source code entities relevant to the textual description of a given change request. Ihdev then mines interaction trails (i.e., Mylyn sessions) associated with these source code entities to recommend a ranked list of developers. Ihdev integrates the interaction trails in a unique way to perform its task, which was not investigated previously. An empirical study on open source systems Mylyn and Eclipse Project was conducted to assess the effectiveness of iHDev. A number of change requests were used in the evaluated bench-mark. Recall for top one to five recommended developers and Mean Reciprocal Rank (MRR) values are reported. Furthermore, a comparative study with two previous approaches that use commit histories and/or the source code authorship information for developer recommendation was performed. Results show that iHDev could provide a recall gain of up to 127.27% with equivalent or improved MRR values by up to 112.5%.<br/> &copy; 2015 IEEE.},
key={Open systems},
keywords={Codes (symbols);Computer programming languages;Computer software;Data mining;History;Learning systems;Mathematical models;Open source software;Program debugging;XML;},
note={Comparative studies;Computer bugs;Context;Developer recommendations;Machine learning techniques;Mean reciprocal ranks;Source code entities;Textual description;},
URL={http://dx.doi.org/10.1109/MSR.2015.16},
}


@inproceedings{20160601908460,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Extracting facts from performance tuning history of scientific applications for predicting effective optimization patterns},
journal={IEEE International Working Conference on Mining Software Repositories},
author={Hashimoto, Masatomo and Terai, Masaaki and Maeda, Toshiyuki and Minami, Kazuo},
volume={2015-August},
year={2015},
pages={13 - 23},
issn={21601852},
address={Florence, Italy},
abstract={To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.<br/> &copy; 2015 IEEE.},
key={Application programs},
keywords={Data mining;Learning systems;Program compilers;Semantic Web;Supercomputers;Syntactics;Trees (mathematics);},
note={Abstract Syntax Trees;Application performance;Combinatorial explosion;Pattern identification;Performance evaluations;Program transformations;Scientific applications;Transformation patterns;},
URL={http://dx.doi.org/10.1109/MSR.2015.9},
}


@inproceedings{20161102091004,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Semantic zooming of code change history},
journal={Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},
author={Yoon, Youngseok S. and Myers, Brad A.},
volume={2015-December},
year={2015},
pages={95 - 99},
issn={19436092},
address={Atlanta, GA, United states},
abstract={Previously, we presented our technique for visualizing fine-grained code changes in a timeline view, designed to facilitate reviewing and interacting with the code change history. During user evaluations, it became evident that users often wanted to see the code changes at a higher level of abstraction. Therefore, we developed a novel approach to automatically summarize fine-grained code changes into more conceptual, higher-level changes in real time. Our system provides four collapse levels, which are integrated with the timeline via semantic zooming: raw level (no collapsing), statement level, method level, and type level. Compared to the raw level, the number of code changes shown in the timeline at each level is reduced by 55%, 77%, and 83%, respectively. This implies that the semantic zooming would help users better understand and interact with the history by minimizing the potential information overload.<br/> &copy; 2015 IEEE.},
key={Visual languages},
keywords={Codes (symbols);Semantics;Visualization;},
note={Azurite;edit collapsing;Program comprehension;Semantic zooming;Software visualization;Timeline visualizations;},
URL={http://dx.doi.org/10.1109/VLHCC.2015.7357203},
}


@article{20190206361490,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Machine learning techniques for code smell detection: A Systematic literature review and meta-Analysis},
journal={Information and Software Technology},
author={Azeem, Muhammad Ilyas and Palomba, Fabio and Shi, Lin and Wang, Qing},
year={2019},
issn={09505849},
abstract={Background: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of machine learning techniques represents an ever increasing research area. Objective: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how machine learning approaches have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of machine learning approaches in the field of code smells. Method: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far. Results: The analyses performed show that God Class, Long Method, Functional Decomposition, and Spaghetti Code have been heavily considered in the literature. DECISION TREES and SUPPORT VECTOR MACHINES are the most commonly used machine learning algorithms for code smell detection. Models based on a large set of independent variables have performed well. JRIP and RANDOM FOREST are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future. Conclusion: Based on our findings, we argue that there is still room for the improvement of machine learning techniques in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.<br/> &copy; 2019 Elsevier B.V.},
key={Learning algorithms},
keywords={Artificial intelligence;Codes (symbols);Decision trees;Heuristic methods;Learning systems;Odors;},
note={Code smell;Evaluation strategies;Functional decomposition;Machine learning approaches;Machine learning techniques;Sources of informations;Systematic literature review;Systematic literature review (SLR);},
URL={http://dx.doi.org/10.1016/j.infsof.2018.12.009},
}


@inproceedings{20152901040607,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using edit distance and junction feature to detect and recognize arrow road marking},
journal={2014 17th IEEE International Conference on Intelligent Transportation Systems, ITSC 2014},
author={He, Uhang and Chen, Hi and Pan, Ifeng and Ni, Ai},
year={2014},
pages={2317 - 2323},
address={Qingdao, China},
abstract={Arrow road markings usually appear on freeway surface and they convey important navigation information to autonomous driving. But detecting and recognizing them is a tough task because they suffer from numerous deviations like objects' interference and themselves' abrasion, etc. We therefore propose a novel local junction feature (L-junction) to describe each road marking as a junction string, different deviation is dispersed into different junction. We encode those junctions within a range as the same code. To measure the similarity between detected junction string and ground truth junction string, we design a weighted edit distance strategy and assign different deviation with different weight so that our framework is robust enough to deviations in arrow road marking but sensitive to non- arrow road markings' deviations. To test our framework, we collect three freeway datasets with our self-driving car: clean/dirty arrow road marking images (300 images respectively), a video dataset (arrow road marking and non-road marking images (670 images)). Another deep learning framework (Boosting+Convolutional Deep Neural Network (CDNN)) is also implemented for comparison. Extensive experimental results well demonstrate the superior performance of our framework.<br/> &copy; 2014 IEEE.},
key={Road and street markings},
keywords={Deep neural networks;Feature extraction;Highway markings;Intelligent systems;Intelligent vehicle highway systems;Roads and streets;Statistical tests;},
note={Autonomous driving;Edit distance;Learning frameworks;Navigation in formation;Road marking;Self drivings;Video dataset;Weighted edit distance;},
URL={http://dx.doi.org/10.1109/ITSC.2014.6958061},
}


@inproceedings{20185106254786,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018},
journal={Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018},
year={2018},
pages={IEEE Computer Society; IEEE Technical Council on Software Engineering (TCSE) - },
address={Madrid, Spain},
abstract={The proceedings contain 27 papers. The topics discussed include: combining obfuscation and optimizations in the real world; obfuscating java programs by translating selected portions of bytecode to native libraries; enabling the continuous analysis of security vulnerabilities with VulData7; towards anticipation of architectural smells using link prediction techniques; periodic developer metrics in software defect prediction; which method-stereotype changes are indicators of code smells?; semantics-based code search using input/output examples; detecting evolutionary coupling using transitive association rules; the case for adaptive change recommendation; and on the use of machine learning techniques towards the design of cloud based automatic code clone validation tools.<br/>},
}


@inproceedings{20184305978190,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={TemPerf: Temporal correlation between performance metrics and source code},
journal={QUDOS 2016 - Proceedings of the 2nd International Workshop on Quality-Aware DevOps, co-located with ISSTA 2016},
author={Cito, Jurgen and Mazlami, Genc and Leitner, Philipp},
year={2016},
pages={46 - 47},
address={Saarbrucken, Germany},
abstract={Today's rapidly evolving software systems continuously introduce new changes that can potentially degrade performance. Large-scale load testing prior to deployment is sup- posed to avoid performance regressions in production. How- ever, due to the large input space in parameterized load testing, not all performance regressions can be prevented in practice. To support developers in identifying the change sets that had an impact on performance, we present TemPerf, a tool that correlates performance regressions with change sets by exploiting temporal constraints. It is implemented as an Eclipse IDE plugin that allows developers to visualize performance developments over time and dis- play temporally correlated change sets retrieved from version control and continuous integration platforms.<br/> &copy; 2016 ACM.},
key={Quality control},
keywords={Regression analysis;Software testing;},
note={Continuous integrations;Devops;Parameterized load;Performance;Performance metrics;Software systems;Temporal constraints;Temporal correlations;},
URL={http://dx.doi.org/10.1145/2945408.2945420},
}


@article{20145100339001,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Capturing software evolution and change through code repository smells},
journal={Lecture Notes in Business Information Processing},
author={Fontana, Francesca Arcelli and Rolla, Matteo and Zanoni, Marco},
volume={199},
year={2014},
pages={148 - 165},
issn={18651348},
abstract={In the last years we have seen the rise and the fall of many version control systems. These systems collect a large amount of data spanning from the path of the files involved in changes to the exact text changed in every file. This data can be exploited to produce an overview about how the system changed over time and evolved. We have developed a tool, called VCS-Analyzer, to use this information, both for data retrieval and analysis tasks. Currently, VCS-Analyzer implements six different analyses: two based on source code for the computation of metrics and the detection of code smells, and four original analysis based on repositories metadata, which are based on the concepts of Repository Metrics and Code Repository Smells. In this paper, we describe one smell and two metrics we have defined for source code repositories analysis.<br/> &copy; Springer International Publishing Switzerland 2014.},
key={Codes (symbols)},
keywords={Odors;},
note={Code changes;Code Repository smells;Data retrieval;Repository analysis;Repository Metrics;Software Evolution;Source code repositories;Version control system;},
}


@inproceedings{20165303204332,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Predicting software change-proneness with code smells and class imbalance learning},
journal={2016 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2016},
author={Kaur, Arvinder and Kaur, Kamaldeep and Jain, Shilpi},
year={2016},
pages={746 - 754},
address={Jaipur, India},
abstract={The objective of this paper is to study the relationship between different types of object-oriented software metrics, code smells and actual changes in software code that occur during maintenance period. It is hypothesized that code smells are indicators of maintenance problems. To understand the relationship between code smells and maintenance problems, we extract code smells in a Java based mobile application called MOBAC. Four versions of MOBAC are studied. Machine learning techniques are applied to predict software change-proneness with code smells as predictor variables. The results of this paper indicate that codes smells are more accurate predictors of change-proneness than static code metrics for all machine learning methods. However, class imbalance techniques did not outperform class balance machine learning techniques in change-proneness prediction. The results of this paper are based on accuracy measures such as F-measure and area under ROC curve.<br/> &copy; 2016 IEEE.},
key={Object oriented programming},
keywords={Artificial intelligence;Codes (symbols);Forecasting;Learning algorithms;Learning systems;Maintenance;Odors;},
note={Class imbalance learning;Code smell;Exception handling;Machine learning techniques;Software change;},
URL={http://dx.doi.org/10.1109/ICACCI.2016.7732136},
}


@inproceedings{20171203487936,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Evaluation of machine learning approaches for change-proneness prediction using code smells},
journal={Advances in Intelligent Systems and Computing},
author={Kaur, Kamaldeep and Jain, Shilpi},
volume={515},
year={2017},
pages={561 - 572},
issn={21945357},
address={Bhubaneswar, Odisha, India},
abstract={In the field of technology, software is an essential driver of business and industry. Software undergoes changes due to maintenance activities initiated by bug fixing, improved documentation, and new requirements of users. In software, code smells are indicators of a system which may give maintenance problem in future. This paper evaluates six types of machine learning algorithms to predict change-proneness using code smells as predictors for various versions of four Java-coded applications. Two approaches are used: method 1-random undersampling is done before Feature selection; method 2-feature selection is done prior to random undersampling. This paper concludes that gene expression programming (GEP) gives maximum AUC value, whereas cascade correlation network (CCR), treeboost, and PNN\GRNN algorithms are among top algorithms to predict F-measure, precision, recall, and accuracy. Also, GOD and L_M code smells are good predictors of software change-proneness. Results show that method 1 outperforms method 2.<br/> &copy; Springer Nature Singapore Pte Ltd. 2017.},
key={Learning algorithms},
keywords={Codes (symbols);Computation theory;Computer software maintenance;Feature extraction;Forecasting;Gene expression;Intelligent computing;Learning systems;Odors;},
note={Cascade correlation network;Code smell;Feature subset selection;Gene expression programming;Machine learning approaches;Random under samplings;Software change;Under-sampling;},
URL={http://dx.doi.org/10.1007/978-981-10-3153-3_56},
}


@inproceedings{20130716015588,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Review code evolution history in OSS universe},
journal={4th Asia-Pacific Symposium on Internetware, Internetware 2012},
author={Zhu, Jiaxin and Lin, Hongwu and Zhou, Minghui and Mei, Hong},
year={2012},
pages={CCF-TSE; CCF-TSS - },
address={Qingdao, China},
abstract={Software evolves all the time because of the changing requirements, in particular, in the diverse Internet environment. Evolution history recorded in software repositories, e.g., Version Control Systems, reflects people's software development practice. Exploring this history could help practitioners to reuse the best practices therefore improve productivity and software quality. Because of the difficulty of collecting and standardizing data, most existing work could only utilize small project set. In this study, we target the open source software universe to build a universal code evolution model for large-scale data. We consider code evolution from two aspects: code version changing history in a single project and code reuse history in the whole universe. In the model, files/modules are built as nodes, and relations (version change or reuse) between files/modules are built as connections. Based on the model, we design and implement a code evolution review framework, i.e., Code Evolution Reviewer (CER), which provides a series of data interfaces to review code evolution history, in particular, code version changing in single project and code reuse among projects. Further, CER could be utilized to explore best practices across large-scale project set. Copyright 2012 ACM.<br/>},
key={Open systems},
keywords={Codes (symbols);Computer software reusability;Computer software selection and evaluation;Open source software;Productivity;Software design;},
note={Best practices;Code evolution;Code reuse;Design and implements;Oss universe;Software development practices;Software repositories;Version control system;},
URL={http://dx.doi.org/10.1145/2430475.2430488},
}


@inproceedings{20160801977192,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Hierarchical categorization of edit operations for separately committing large refactoring results},
journal={International Workshop on Principles of Software Evolution (IWPSE)},
author={Matsuda, Jumpei and Hayashi, Shinpei and Saeki, Motoshi},
volume={30-Aug-2015},
year={2015},
pages={19 - 27},
address={Bergamo, Italy},
abstract={In software configuration management using a version control system, developers have to follow the commit policy of the project. However, preparing changes according to the policy are sometimes cumbersome and time-consuming, in particular when applying large refactoring consisting of multiple primitive refactoring instances. In this paper, we propose a technique for re-organizing changes by recording editing operations of source code. Editing operations including refactoring operations are hierarchically managed based on their types provided by an integrated development environment. Using the obtained hierarchy, developers can easily configure the granularity of changes and obtain the resulting changes based on the configured granularity. We confirmed the feasibility of the technique by applying it to the recorded changes in a large refactoring process.<br/>},
note={Editing operations;Hierarchical categorization;Integrated development environment;Refactorings;Software configuration management;Source codes;Tangled changes;Version control system;},
URL={http://dx.doi.org/10.1145/2804360.2804363},
}


@inproceedings{20174104262721,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Recommender system for model driven software development},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
author={Kogel, Stefan},
volume={Part F130154},
year={2017},
pages={1026 - 1029},
address={Paderborn, Germany},
abstract={Models are key artifacts in model driven software engineering, similar to source code in traditional software engineering. Integrated development environments help users while writing source code, e.g. with typed auto completions, quick fixes, or automatic refactorings. Similar integrated features are rare for modeling IDEs. The above source code IDE features can be seen as a recommender system. A recommender system for model driven software engineering can combine data from different sources in order to infer a list of relevant and actionable model changes in real time. These recommendations can speed up working on models by automating repetitive tasks and preventing errors when the changes are atypical for the changed models. Recommendations can be based on common model transformations that are taken from the literature or learned from models in version control systems. Further information can be taken from instance- to meta-model relationships, modeling related artifacts (e.g. correctness constraints), and versions histories of models under version control. We created a prototype recommender that analyses the change history of a single model. We computed its accuracy via crossvalidation and found that it was between 0.43 and 0.82 for models from an open source project. In order to have a bigger data set for the evaluation and the learning of model transformation, we also mined repositories from Eclipse projects for Ecore meta models and their versions.We found 4374 meta models with 17249 versions. 244 of these meta models were changed at least ten times and are candidates for learning common model transformations. We plan to evaluate our recommender system in two ways: (1) In off-line evaluations with data sets of models from the literature, created by us, or taken from industry partners. (2) In on-line user studies with participants from academia and industry, performed as case studies and controlled experiments.<br/> &copy; 2017 Association for Computing Machinery.},
key={Recommender systems},
keywords={Codes (symbols);Computer programming languages;Data mining;Engineering education;Heuristic algorithms;Information management;Learning systems;Metadata;Open source software;Search engines;Software design;},
note={Controlled experiment;Heuristic search algorithms;Integrated development environment;Model driven software engineering;Model transformation;Model-Driven Software Development;Open source projects;Version control system;},
URL={http://dx.doi.org/10.1145/3106237.3119874},
}


@article{20184305975630,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={How Well Do Change Sequences Predict Defects? Sequence Learning from Software Changes},
journal={IEEE Transactions on Software Engineering},
author={Wen, Ming and Wu, Rongxin and Cheung, S.C.},
year={2018},
issn={00985589},
abstract={Software defect prediction, which aims to identify defective modules, can assist developers in finding bugs and prioritizing limited quality assurance resources. Various features to build defect prediction models have been proposed and evaluated. Among them, process metrics are one important category. Yet, existing process metrics are mainly encoded manually from change histories and ignore the sequential information arising from the changes during software evolution. Unlike traditional process metrics used for existing defect prediction models, change sequences are mostly vectors of variable length. This makes it difficult to apply such sequences directly in prediction models that are driven by conventional classifiers. To resolve this challenge, we utilize Recurrent Neural Network (RNN), which is a deep learning technique, to encode features from sequence data automatically. In this paper, we propose a novel approach called Fences, which extracts six types of change sequences covering different aspects of software changes via fine-grained change analysis. It approaches defects prediction by mapping it to a sequence labeling problem solvable by RNN. Our evaluations on 10 open source projects show that Fences can predict defects with high performance. Fences also outperforms the state-of-the-art technique which learns semantic features automatically from static code via deep learning.<br/> IEEE},
key={Deep learning},
keywords={Computer software;Defects;Feature extraction;Fences;Forecasting;History;Learning systems;Measurement;Open source software;Program debugging;Quality assurance;Recurrent neural networks;Semantics;},
note={Defect prediction;Defect prediction models;Predictive models;Process metrics;Recurrent neural network (RNN);Sequence learning;Software defect prediction;State-of-the-art techniques;},
URL={http://dx.doi.org/10.1109/TSE.2018.2876256},
}


@inproceedings{20184606055887,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Applying Machine Learning to Predict Software Fault Proneness Using Change Metrics, Static Code Metrics, and a Combination of Them},
journal={Conference Proceedings - IEEE SOUTHEASTCON},
author={Alshehri, Yasser Ali and Goseva-Popstojanova, Katerina and Dzielski, Dale G. and Devine, Thomas},
volume={2018-April},
year={2018},
issn={07347502},
address={St. Petersburg, FL, United states},
abstract={Predicting software fault proneness is very important as the process of fixing these faults after the release is very costly and time-consuming. In order to predict software fault proneness, many machine learning algorithms (e.g., Logistic regression, Naive Bayes, and J48) were used on several datasets, using different metrics as features. The question is what algorithm is the best under which circumstance and what metrics should be applied. Related works suggested that using change metrics leads to the highest accuracy in prediction. In addition, some algorithms perform better than others in certain circumstances. In this work, we use three machine learning algorithms (i.e., logistic regression, Naive Bayes, and J48) on three Eclipse releases (i.e., 2.0, 2.1, 3.0). The results showed that accuracy is slightly better and false positive rates are lower, when we use the reduced set of metrics compared to all change metrics set. However, the recall and the G score are better when we use the complete set of change metrics. Furthermore, J48 outperformed the other classifiers with respect to the G score for the reduced set of change metrics, as well as in almost all cases when the complete set of change metrics, static code metrics, and the combination of both were used.<br/> &copy; 2018 IEEE.},
key={Learning algorithms},
keywords={Artificial intelligence;Classifiers;Codes (symbols);Data mining;Decision trees;Forecasting;Learning systems;Regression analysis;Reliability;Software engineering;Software reliability;},
note={Change metrics;Logistic regressions;Naive bayes;Software fault;Static code metrics;},
URL={http://dx.doi.org/10.1109/SECON.2018.8478911},
}


@inproceedings{20185106251313,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Enriching API documentation by relevant API methods recommendation based on version history},
journal={Proceedings - 3rd International Workshop on Dynamic Software Documentation, DySDoc3 2018},
author={Arimatsu, Yuu and Ishida, Yoshiya and Noda, Kunihiro and Kobayashi, Takashi},
year={2018},
pages={15 - 16},
address={Madrid, Spain},
abstract={Application programming interfaces (APIs) enable developers to increase their productivity; however, developers have to expend much of their time to search API usages because API documentation often lacks some types of information such as co-use relationships between API methods or code examples. In this paper, we propose a technique to automatically generate enriched API documentation. Our technique identifies groups of relevant API methods by analyzing a software change history and inserts those into standard API documentation. Using our document, developers can easily find relevant API methods interactively according to their various development purposes.<br/> &copy; 2018 IEEE.},
key={Application programming interfaces (API)},
keywords={Computer software maintenance;},
note={Document generation;Mining software;Search API;Software change;},
URL={http://dx.doi.org/10.1109/DySDoc3.2018.00014},
}


@inbook{20172703885631,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Change Coupling Between Software Artifacts: Learning from Past Changes},
journal={The Art and Science of Analyzing Software Data},
author={Oliva, Gustavo Ansaldi and Gerosa, Marco Aurelio},
year={2015},
pages={285 - 323},
abstract={While mining version control systems, researchers noticed that some artifacts frequently change together throughout software development. When a certain artifact co-changes repeatedly with another, we say that the former is change coupledto the latter. Researchers have found a series of applications for change coupling in software engineering. For instance, building on the idea that artifacts that changed together in the past are likely to change together in the future, researchers developed effective change prediction mechanisms. In this chapter, we describe the concept of change coupling in more detail and present different techniques to detect it. In particular, we provide ready-to-use code you can leverage as a starting step to detect change couplings in your own projects. In the last part of the chapter, we describe some of the main applications of change coupling analysis.<br/> &copy; 2015 Elsevier Inc. All rights reserved.},
key={Software design},
keywords={Application programs;Control systems;Information management;},
note={Change couplings;Change dependencies;Change history;Evolutionary dependencies;Historical co-changes;Logical dependencies;Revision control systems;Software change;Version control system;},
URL={http://dx.doi.org/10.1016/B978-0-12-411519-4.00011-2},
}


@inproceedings{20140717297291,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Mining A change history to quickly identify bug locations: A case study of the Eclipse project},
journal={2013 IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2013},
author={Tantithamthavorn, Chakkrit and Teekavanich, Rattamont and Ihara, Akinori and Matsumoto, Ken-Ichi},
year={2013},
pages={108 - 113},
address={Pasadena, CA, United states},
abstract={In this study, we proposed an approach to mine a change history to improve the bug localization performance. The key idea is that a recently fixed file may be fixed in the near future. We used a combination of textual feature and mining the change history to recommend source code files that are likely to be fixed for a given bug report. First, we adopted the Vector Space Model (VSM) to find relevant source code files that are textually similar to the bug report. Second, we analyzed the change history to identify previously fixed files. We then estimated the fault proneness of these files. Finally, we combined the two scores, from textual similarity and fault proneness, for every source code file. We then recommend developers examine source code files with higher scores. We evaluated our approach based on 1,212 bug reports from the Eclipse Platform and Eclipse JDT. The experimental results show that our proposed approach can improve the bug localization performance and effectively identify buggy files. &copy; 2013 IEEE.<br/>},
key={Software reliability},
keywords={Codes (symbols);Computer programming languages;Information retrieval;Program debugging;Technical presentations;Vector spaces;},
note={Bug localizations;Change history;Fault proneness;Software debugging;Source codes;Textual features;Textual similarities;Vector space model (VSM);},
URL={http://dx.doi.org/10.1109/ISSREW.2013.6688888},
}


@article{20131516184808,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Reducing features to improve code change-based bug prediction},
journal={IEEE Transactions on Software Engineering},
author={Shivaji, Shivkumar and James Whitehead Jr., E. and Akella, Ram and Kim, Sunghun},
volume={39},
number={4},
year={2013},
pages={552 - 569},
issn={00985589},
abstract={Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine (SVM) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The SVM's improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features. &copy; 1976-2012 IEEE.<br/>},
key={Feature extraction},
keywords={Artificial intelligence;Classifiers;Forecasting;Learning systems;Reliability;Support vector machines;},
note={Analysis of performance;Bug predictions;Important features;Multiple features;Optimal classification;Practical use;Software history;Software project;},
URL={http://dx.doi.org/10.1109/TSE.2012.43},
}


@inproceedings{20154001326498,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={On mapping releases to commits in open source systems},
journal={22nd International Conference on Program Comprehension, ICPC 2014 - Proceedings},
author={Shobe, Joseph F. and Karim, M. Yasser and Zanjani, Motahareh Bahrami and Kagdi, Huzefa},
year={2014},
pages={68 - 71},
address={Hyderabad, India},
abstract={The paper presents an empirical study on the release naming and structure in three open source projects: Google Chrome, GNU gcc, and Subversion. Their commonality and variability are discussed. An approach is developed that establishes the mapping from a particular release (major or minor) to the specific earliest and latest revisions, i.e., a commit window of a release, in the source control repository. For example, the major release 25.0 in Chrome is mapped to the earliest revision 157687 and latest revision 165096 in the trunk. This mapping between releases and commits would facilitate a systematic choice of history in units of the project evolution scale (i.e., commits that constitute a software release). A projected application is in forming a training set for a source-code change prediction model, e.g., using the association rule mining or machine learning techniques, commits from the source code history are needed.<br/> Copyright &copy; 2014 ACM.},
key={Open source software},
keywords={Computer programming;Learning systems;Mapping;Open systems;},
note={Commit history;Commonality and variability;Empirical studies;Machine learning techniques;Mining software repositories;Open source projects;Software release;Source code changes;},
URL={http://dx.doi.org/10.1145/2597008.2597792},
}


@inproceedings{20185006229615,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Bug localization by learning to rank and represent bug inducing changes},
journal={International Conference on Information and Knowledge Management, Proceedings},
author={Loyola, Pablo and Gajananan, Kugamoorthy and Satoh, Fumiko},
year={2018},
pages={657 - 665},
address={Torino, Italy},
abstract={In software development, bug localization is the process finding portions of source code associated to a submitted bug report. This task has been modeled as an information retrieval task at source code file, where the report is the query. In this work, we propose a model that, instead of working at file level, learns feature representations from source changes extracted from the project history at both syntactic and code change dependency perspectives to support bug localization. To that end, we structured an end-to-end architecture able to integrate feature learning and ranking between sets of bug reports and source code changes. We evaluated our model against the state of the art of bug localization on several real world software projects obtaining competitive results in both intra-project and cross-project settings. Besides the positive results in terms of model accuracy, as we are giving the developer not only the location of the bug associated to the report, but also the change that introduced, we believe this could give a broader context for supporting fixing tasks.<br/> &copy; 2018 Association for Computing Machinery.},
key={Codes (symbols)},
keywords={Computer programming languages;Information retrieval;Knowledge management;Software design;},
note={Bug localizations;Feature learning;Feature representation;Learning to rank;Model accuracy;Software project;Source code changes;State of the art;},
URL={http://dx.doi.org/10.1145/3269206.3271811},
}


@inproceedings{20161402185195,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={On the Conceptual Cohesion of Co-Change Clusters},
journal={Proceedings - 29th Brazilian Symposium on Software Engineering, SBES 2015},
author={De Oliveira, Marcos Cesar and Bonifacio, Rodrigo and Ramos, Guilherme N. and Ribeiro, Marcio},
year={2015},
pages={120 - 129},
address={Belo Horizonte - MG, Brazil},
abstract={The analysis of co-change clusters as an alternative software decomposition can provide insights on different perspectives of modularity. But the usual approach using coarse-grained entities does not provide relevant information, like the conceptual cohesion of the modular abstractions that emerge from co-change clusters. This work presents a novel approach to analyze the conceptual cohesion of the source-code associated with co-change clusters of fine-grained entities. We obtain from the change history information found in version control systems. We describe the use of our approach to analyze six well established and currently active open-source projects from different domains and one of the most relevant systems of the Brazilian Government for the financial domain. The results show that co-change clusters offer a new perspective on the code based on groups with high conceptual cohesion between its entities (up to 69% more than the original package decomposition), and, thus, are suited to detect concepts pervaded on codebases, opening new possibilities of comprehension of source-code by means of the concepts embodied in the co-change clusters.<br/> &copy; 2015 IEEE.},
key={Open systems},
keywords={Adhesion;Codes (symbols);Open source software;},
note={Change history;Coarse-grained;Different domains;Financial domains;Fine grained;Open source projects;Software decomposition;Version control system;},
URL={http://dx.doi.org/10.1109/SBES.2015.16},
}


@inproceedings{20122615174226,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Modulo-X: A simple transformation language for HPC programs},
journal={Proceedings of the Annual Southeast Conference},
author={Jacob, Ferosh and Gray, Jeff and Bangalore, Purushotham},
year={2012},
pages={351 - 352},
address={Tuscaloosa, AL, United states},
abstract={While creating a parallel version of a sequential program, some code sections may be duplicated in the translated version, which can hinder the evolution of the newly created program. This can be prevented if parallel sections of a program can be separated from the sequential sections. In this paper, we introduce a transformation language, called Modulo-X, which can make the parallel to sequential conversion task easier without modifying the original source code. A case study is included to show how sequential code can be converted to two leading parallel programming models (i.e., OpenMP and MPI) using Modulo-X. &copy; 2012 Authors.<br/>},
key={C (programming language)},
keywords={Application programming interfaces (API);Cesium;Codes (symbols);Cosine transforms;FORTRAN (programming language);Machinery;Parallel programming;},
note={Code sections;Parallel programming model;Parallel version;Sequential programs;Source code transformation;Source codes;Transformation languages;},
URL={http://dx.doi.org/10.1145/2184512.2184600},
}


@article{20164302932519,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Version 3.0 of code Java for 3D simulation of the CCA model},
journal={Computer Physics Communications},
author={Zhang, Kebo and Zuo, Junsen and Dou, Yifeng and Li, Chao and Xiong, Hailing},
volume={207},
year={2016},
pages={547 - 548},
issn={00104655},
abstract={In this paper we provide a new version of program for replacing the previous version. The frequency of traversing the clusters-list was reduced, and some code blocks were optimized properly; in addition, we appended and revised the comments of the source code for some methods or attributes. The compared experimental results show that new version has better time efficiency than the previous version. New version program summary Program title: CCA v03 Program Files doi:10.17632/gzncs28f95.1 Licensing provisions: Apache-2.0 License Programming language: Java Journal Reference of previous version: Computer Physics Communications 204 (2016) 214&ndash;215. Does the new version supersede the previous version?: Yes Nature of problem: There is poor running efficiency in the previous version. In addition, some methods in Java entity classes are short of necessary optimization. Solution method: A number of redundant steps, and running frequency of loop statements, are optimized into a more reasonable range. Object oriented solution, easy to reuse, extend and customize, in any development environment which supports Java JDK. Reasons for the new version: 1. In the previous version [2], for a random selected cluster moving a unit length, it must firstly obtain the maximum diffusion coefficient D<inf>max</inf>[1] which is searched from the clusters-list. In fact, it is not necessary that the program traverse the clusters-list for every diffusion step. If there is no aggregation between two clusters in the process of diffusion, the program does not have to traverse the clusters-list to get the D<inf>max</inf>in this step, because of no change for every cluster's property around the system. D<inf>max</inf>just be saved as a global variable which is used to store the latest maximum value. When aggregation takes place, we just update the current maximum value of D<inf>max</inf>, so greatly reduce loop steps. Although contrary to the principles of object-oriented programming to some extent, it is worthy to make such sacrifice. 2. Some methods in Java entity classes are short of necessary optimization, containing redundant codes which consume extra hardware resources. 3. The source code of old program lacks necessary explanations for some methods or attributes. Moreover, some code comments, which are incorrect or inexplicable, need revise. Summary of revisions: 1. Cut down the frequency of traversing the clusters-list. 2. Optimize some code blocks. 3. Append and revise the comments of the source code for some methods or attributes, so as to easily for users to understand, debug and maintain. Additional comments:[Figure presented] We provide a referential log&ndash;log plot of simulation time versus number of particles as shown in Fig. 1 by a general PC. In this simulation, constant concentration C is set to 0.01, diffusion exponent is set to 0.5, sticking probability exponent is set to 0, and absolute temperature is set to 298K, but the side length L of cube varies in 30, 40, 50, 60 and 70 (these parameters come from literature [1]). The number of particles N varies with the side length L(N=C&lowast;L<sup>3</sup>), so we just adjust the side length to get different particle numbers. As we can see from Fig. 1, the coefficient of x is 2.2413 of the new version, which is near to quadratic complexity, the coefficient of x is 2.9367 of the previous version, which is very close to cubic complexity. Obviously, the new version has a better running efficiency than the previous version. The new program brings a high running speed, but cause another problem that one user cannot promptly click the &ldquo;Suspend&rdquo; button to see a snapshot what he or she wanted to see. For this fault, we append a &ldquo;Slow down&rdquo; checkbox which can reduce the running speed for users&rsquo; choice, so it improves user-experience. Running time: Determined by the initial parameters. References: [1] C. Li, H. Xiong, Computer Physics Communications 185 (2014) 3424&ndash;3429.[2] K. Zhang, H. Xiong, C. Li, Computer Physics Communications 204 (2016) 214&ndash;215.<br/> &copy; 2016 Elsevier B.V.},
key={Java programming language},
keywords={Agglomeration;Codes (symbols);Computer software;Diffusion;Efficiency;Object oriented programming;Optimization;Problem oriented languages;},
note={Development environment;Diffusion exponent;Hardware resources;Quadratic complexity;Running efficiency;Simplification;Sticking probability;Time efficiencies;},
URL={http://dx.doi.org/10.1016/j.cpc.2016.05.031},
}


@inproceedings{20151800813949,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={SQA-Profiles: Rule-based activity profiles for Continuous Integration environments},
journal={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
author={Brandtner, Martin and Muller, Sebastian C. and Leitner, Philipp and Gall, Harald C.},
year={2015},
pages={301 - 310},
address={Montreal, QC, Canada},
abstract={Continuous Integration (CI) environments cope with the repeated integration of source code changes and provide rapid feedback about the status of a software project. However, as the integration cycles become shorter, the amount of data increases, and the effort to find information in CI environments becomes substantial. In modern CI environments, the selection of measurements (e.g., build status, quality metrics) listed in a dashboard does only change with the intervention of a stakeholder (e.g., a project manager). In this paper, we want to address the shortcoming of static views with so-called Software Quality Assessment (SQA) profiles. SQA-Profiles are defined as rule-sets and enable a dynamic composition of CI dashboards based on stakeholder activities in tools of a CI environment (e.g., version control system). We present a set of SQA-Profiles for project management committee (PMC) members: Bandleader, Integrator, Gatekeeper, and Onlooker. For this, we mined the commit and issue management activities of PMC members from 20 Apache projects. We implemented a framework to evaluate the performance of our rule-based SQA-Profiles in comparison to a machine learning approach. The results showed that project-independent SQA-Profiles can be used to automatically extract the profiles of PMC members with a precision of 0.92 and a recall of 0.78.<br/> &copy; 2015 IEEE.},
key={Integration},
keywords={Computer software selection and evaluation;Learning systems;Project management;},
note={Activity profile;Continuous integrations;Dynamic composition;Issue managements;Machine learning approaches;Software quality assessment;Source code changes;Version control system;},
URL={http://dx.doi.org/10.1109/SANER.2015.7081840},
}


@inproceedings{20164603010879,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Code Change History and Software Vulnerabilities},
journal={Proceedings - 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN-W 2016},
author={Pianco, Marcus and Fonseca, Baldoino and Antunes, Nuno},
year={2016},
pages={6 - 9},
address={Toulouse, France},
abstract={Usually, the most critical modules of the system receive extra attention. But even these modules might be too large to be thoroughly inspected so it is useful to know where to apply the majority of the efforts. Thus, knowing which code changes are more prone to contain vulnerabilities may allow security experts to concentrate on a smaller subset of submitted code changes. In this paper we discuss the change history of functions and its impact on the existence of vulnerabilities. For this, we analyzed the commit history of two software projects widely exposed to attacks (Mozilla and Linux Kernel). Starting from security bugs, we analyzed more than 95k functions (with and without vulnerabilities), and systematized the changes in each function according to a subset of the patterns described in the Orthogonal Defects Classification. The results show that the frequency of changes can allow to distinguish functions more prone to have vulnerabilities.<br/> &copy; 2016 IEEE.},
key={Security of data},
keywords={Codes (symbols);Computer operating systems;Orthogonal functions;},
note={Change history;Commit history;Critical modules;Defects classification;Security bugs;Security experts;Software project;Software vulnerabilities;},
URL={http://dx.doi.org/10.1109/DSN-W.2016.50},
}


@article{20164703042653,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Improved bug localization based on code change histories and bug reports},
journal={Information and Software Technology},
author={Youm, Klaus Changsun and Ahn, June and Lee, Eunseok},
volume={82},
year={2017},
pages={177 - 192},
issn={09505849},
abstract={Context Several issues or defects in released software during the maintenance phase are reported to the development team. It is costly and time-consuming for developers to precisely localize bugs. Bug reports and the code change history are frequently used and provide information for identifying fault locations during the software maintenance phase. Objective It is difficult to standardize the style of bug reports written in natural languages to improve the accuracy of bug localization. The objective of this paper is to propose an effective information retrieval-based bug localization method to find suspicious files and methods for resolving bugs. Method In this paper, we propose a novel information retrieval-based bug localization approach, termed Bug Localization using Integrated Analysis (BLIA). Our proposed BLIA integrates analyzed data by utilizing texts, stack traces and comments in bug reports, structured information of source files, and the source code change history. We improved the granularity of bug localization from the file level to the method level by extending previous bug repository data. Results We evaluated the effectiveness of our approach based on experiments using three open-source projects, namely AspectJ, SWT, and ZXing. In terms of the mean average precision, on average our approach improves the metric of BugLocator, BLUiR, BRTracer, AmaLgam and the preliminary version of BLIA by 54%, 42%, 30%, 25% and 15%, respectively, at the file level of bug localization. Conclusion Compared with prior tools, the results showed that BLIA outperforms these other methods. We analyzed the influence of each score of BLIA from various combinations based on the analyzed information. Our proposed enhancement significantly improved the accuracy. To improve the granularity level of bug localization, a new approach at the method level is proposed and its potential is evaluated.<br/> &copy; 2016 Elsevier B.V.},
key={Codes (symbols)},
keywords={Computer software maintenance;Information retrieval;Open source software;},
note={Bug localizations;Bug reports;Code changes;Method analysis;Stack traces;},
URL={http://dx.doi.org/10.1016/j.infsof.2016.11.002},
}


@article{20171403532732,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using discriminative feature in software entities for relevance identification of code changes},
journal={Journal of Software: Evolution and Process},
author={Huang, Yuan and Chen, Xiangping and Liu, Zhiyong and Luo, Xiaonan and Zheng, Zibin},
volume={29},
number={7},
year={2017},
issn={20477481},
abstract={Developers often bundle unrelated changes (eg, bug fix and feature addition) in a single commit and then submit a &ldquo;poor cohesive&rdquo; commit to version control system. Such a commit consists of multiple independent code changes and makes review of code changes harder. If the code changes before commit can be identified as related and unrelated ones, the &ldquo;cohesiveness&rdquo; of a commit can be guaranteed. Inspired by the effectiveness of machine learning techniques in classification field, we model the relevance identification of code changes as a binary classification problem (ie, related and unrelated changes) and propose discriminative feature in software entities to characterize the relevance of code changes. In particular, to quantify the discriminative feature, 21 coupling rules and 4 cochanged type relationships are elaborately extracted from software entities to construct related changes vector (RCV). Twenty-one coupling rules at granularities of class, attribute, and method can capture the relevance of code changes from structural coupling dimension, and 4 cochanged type relationships are defined to capture the change type combinations of software entities that may cause related changes. Based on RCV, machine learning algorithms are applied to identify the relevance of code changes. The experiment results show that probabilistic neural network and general regression neural network provide statistically significant improvements in accuracy of relevance identification of code changes over the other 4 machine learning algorithms. Related changes vector with 72 dimensions (RCV<inf>72</inf>) outperforms other 2RCVs with less dimensions.<br/> Copyright &copy; 2017 John Wiley & Sons, Ltd.},
key={Learning algorithms},
keywords={Codes (symbols);Couplings;Intelligent systems;Learning systems;Neural networks;},
note={Binary classification problems;cochanged types;Code changes;Discriminative features;General regression neural network;Machine learning techniques;Probabilistic neural networks;Version control system;},
URL={http://dx.doi.org/10.1002/smr.1859},
}


@inproceedings{20180304660009,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Confusion detection in code reviews},
journal={Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017},
author={Ebert, Felipe and Castor, Fernando and Novielli, Nicole and Serebrenik, Alexander},
year={2017},
pages={549 - 553},
address={Shanghai, China},
abstract={Code reviews are an important mechanism for assuring quality of source code changes. Reviewers can either add general comments pertaining to the entire change or pinpoint concerns or shortcomings about a specific part of the change using inline comments. Recent studies show that reviewers often do not understand the change being reviewed and its context. Our ultimate goal is to identify the factors that confuse code reviewers and understand how confusion impacts the efficiency and effectiveness of code review(er)s. As the first step towards this goal we focus on the identification of confusion in developers' comments. Based on an existing theoretical framework categorizing expressions of confusion, we manually classify 800 comments from code reviews of the Android project. We observe that confusion can be reasonably well-identified by humans: raters achieve moderate agreement (Fleiss' kappa 0.59 for the general comments and 0.49 for the inline ones). Then, for each kind of comment we build a series of automatic classifiers that, depending on the goals of the further analysis, can be trained to achieve high precision (0.875 for the general comments and 0.615 for the inline ones), high recall (0.944 for the general comments and 0.988 for the inline ones), or substantial precision and recall (0.696 and 0.542 for the general comments and 0.434 and 0.583 for the inline ones, respectively). These results motivate further research on the impact of confusion on the code review process. Moreover, other researchers can employ the proposed classifiers to analyze confusion in other contexts where software development-related discussions occur, such as mailing lists.<br/> &copy; 2017 IEEE.},
key={Codes (symbols)},
keywords={Computer software maintenance;Learning systems;Software design;Taxonomies;},
note={Automatic classifiers;Code review;Confusion;Fleiss' kappas;High-precision;Precision and recall;Source code changes;Theoretical framework;},
URL={http://dx.doi.org/10.1109/ICSME.2017.40},
}


@inproceedings{20182005187541,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={When Do Changes Induce Software Vulnerabilities?},
journal={Proceedings - 2017 IEEE 3rd International Conference on Collaboration and Internet Computing, CIC 2017},
author={Alohaly, Manar and Takabi, Hassan},
volume={2017-January},
year={2017},
pages={59 - 66},
address={San Jose, CA, United states},
abstract={Version control systems (VCSs) have almost become the de facto standard for the management of open-source projects and the development of their source code. In VCSs, source code which can potentially be vulnerable is introduced to a system through what are so called commits. Vulnerable commits force the system into an insecure state. The farreaching impact of vulnerabilities attests to the importance of identifying and understanding the characteristics of prior vulnerable changes (or commits), in order to detect future similar ones. The concept of change classification was previously studied in the literature of bug detection to identify commits with defects. In this paper, we borrow the notion of change classification from the literature of defect detection to further investigate its applicability to vulnerability detection problem using semi-supervised learning. In addition, we also experiment with new vulnerability predictors, and compare the predictive power of our proposed features with vulnerability prediction techniques based on text mining. The experimental results show that our semi-supervised approach holds promise in improving change classification effectiveness by leveraging unlabeled data.<br/> &copy; 2017 IEEE.},
key={Open systems},
keywords={Codes (symbols);Computer programming languages;Data mining;Defects;Open source software;Program debugging;Supervised learning;},
note={Open source projects;Prediction techniques;Semi- supervised learning;Software security;Software vulnerabilities;Source codes;Version control system;Vulnerability detection;},
URL={http://dx.doi.org/10.1109/CIC.2017.00020},
}


@inproceedings{20151800813935,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Cross-project build co-change prediction},
journal={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
author={Xia, Xin and Lo, David and Mcintosh, Shane and Shihab, Emad and Hassan, Ahmed E.},
year={2015},
pages={311 - 320},
address={Montreal, QC, Canada},
abstract={Build systems orchestrate how human-readable source code is translated into executable programs. In a software project, source code changes can induce changes in the build system (aka. build co-changes). It is difficult for developers to identify when build co-changes are necessary due to the complexity of build systems. Prediction of build co-changes works well if there is a sufficient amount of training data to build a model. However, in practice, for new projects, there exists a limited number of changes. Using training data from other projects to predict the build co-changes in a new project can help improve the performance of the build co-change prediction. We refer to this problem as cross-project build co-change prediction. In this paper, we propose CroBuild, a novel cross-project build co-change prediction approach that iteratively learns new classifiers. CroBuild constructs an ensemble of classifiers by iteratively building classifiers and assigning them weights according to its prediction error rate. Given that only a small proportion of code changes are build co-changing, we also propose an imbalance-aware approach that learns a threshold boundary between those code changes that are build co-changing and those that are not in order to construct classifiers in each iteration. To examine the benefits of CroBuild, we perform experiments on 4 large datasets including Mozilla, Eclipse-core, Lucene, and Jazz, comprising a total of 50,884 changes. On average, across the 4 datasets, CroBuild achieves a F1-score of up to 0.408. We also compare CroBuild with other approaches such as a basic model, AdaBoost proposed by Freund et al., and TrAdaBoost proposed by Dai et al. On average, across the 4 datasets, the CroBuild approach yields an improvement in F1-scores of 41.54%, 36.63%, and 36.97% over the basic model, AdaBoost, and TrAdaBoost, respectively.<br/> &copy; 2015 IEEE.},
key={Forecasting},
keywords={Adaptive boosting;Codes (symbols);Iterative methods;},
note={Change prediction;Cross-project;Ensemble of classifiers;Executable programs;Imbalance datum;Prediction error rates;Source code changes;Transfer learning;},
URL={http://dx.doi.org/10.1109/SANER.2015.7081841},
}


@inproceedings{20163802810634,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Learning to rank relevant files for bug reports using domain knowledge},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
author={Ye, Xin and Bunescu, Razvan and Liu, Chang},
volume={16-21-November-2014},
year={2014},
pages={689 - 699},
address={Hong Kong, China},
abstract={When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files of a project with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and potentially could lead to a substantial increase in productivity. This paper introduces an adaptive ranking approach that leverages domain knowledge through functional decompositions of source code files into methods, API descriptions of library components used in the code, the bug-fixing history, and the code change history. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features encoding domain knowledge, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluated our system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the newly introduced learning-to-rank approach significantly outperforms two recent state-of-the-art methods in recommending relevant files for bug reports. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70% of the bug reports in the Eclipse Platform and Tomcat projects.<br/>},
key={Program debugging},
keywords={Codes (symbols);Computer software maintenance;Open source software;Open systems;},
note={Adaptive rankings;Bug reports;Domain knowledge;Functional decomposition;Learning to rank;Library components;Source codes;Source files;},
URL={http://dx.doi.org/10.1145/2635868.2635874},
}


@article{20133316606876,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Integrating conceptual and logical couplings for change impact analysis in software},
journal={Empirical Software Engineering},
author={Kagdi, Huzefa and Gethers, Malcom and Poshyvanyk, Denys},
volume={18},
number={5},
year={2013},
pages={933 - 969},
issn={13823256},
abstract={The paper presents an approach that combines conceptual and evolutionary techniques to support change impact analysis in source code. Conceptual couplings capture the extent to which domain concepts and software artifacts are related to each other. This information is derived using Information Retrieval based analysis of textual software artifacts that are found in a single version of software (e.g., comments and identifiers in a single snapshot of source code). Evolutionary couplings capture the extent to which software artifacts were co-changed. This information is derived from analyzing patterns, relationships, and relevant information of source code changes mined from multiple versions in software repositories. The premise is that such combined methods provide improvements to the accuracy of impact sets compared to the two individual approaches. A rigorous empirical assessment on the changes of the open source systems Apache httpd, ArgoUML, iBatis, KOffice, and jEdit is also reported. The impact sets are evaluated at the file and method levels of granularity for all the software systems considered in the empirical evaluation. The results show that a combination of conceptual and evolutionary techniques, across several cut-off points and periods of history, provides statistically significant improvements in accuracy over either of the two techniques used independently. Improvements in F-measure values of up to 14% (from 3% to 17%) over the conceptual technique in ArgoUML at the method granularity, and up to 21% over the evolutionary technique in iBatis (from 9% to 30%) at the file granularity were reported. &copy; 2012 Springer Science+Business Media New York.<br/>},
key={Open source software},
keywords={Codes (symbols);Computer programming languages;Couplings;Information retrieval;Information use;Open systems;Reverse engineering;},
note={Change impact analysis;Empirical assessment;Empirical evaluations;Evolutionary techniques;Mining software repositories;Software evolution and maintenances;Software repositories;Source code changes;},
URL={http://dx.doi.org/10.1007/s10664-012-9233-9},
}


@inproceedings{20173003980977,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Change prediction through coding rules violations},
journal={ACM International Conference Proceeding Series},
author={Tollin, Irene and Fontana, Francesca Arcelli and Zanoni, Marco and Roveda, Riccardo},
volume={Part F128635},
year={2017},
pages={61 - 64},
address={Karlskrona, Sweden},
abstract={Static source code analysis is an increasingly important activity to manage software project quality, and is often found as a part of the development process. A widely adopted way of checking code quality is through the detection of violations to specific sets of rules addressing good programming practices. SonarQube is a platform able to detect these violations, called Issues. In this paper we described an empirical study performend on two industrial projects, where we used Issues extracted on different versions of the projects to predict changes in code through a set of machine learning models. We achieved good detection performances, especially when predicting changes in the next version. This result paves the way for future investigations of the interest in an industrial setting towards the prioritization of Issues management according to their impact on change-proneness.<br/> &copy; 2017 Copyright held by the owner/author(s).},
key={Quality control},
keywords={Artificial intelligence;Codes (symbols);Computer software selection and evaluation;Forecasting;Learning systems;},
note={Change prediction;Detection performance;Industrial projects;Industrial settings;Issues;Machine learning models;Programming practices;Software Quality;},
URL={http://dx.doi.org/10.1145/3084226.3084282},
}


@inproceedings{20161702283604,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Development history granularity transformations},
journal={Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},
author={Mulu, Kivanc and Swart, Luke and Brun, Yuriy and Ernst, Michael D.},
year={2015},
pages={697 - 702},
address={Lincoln, NE, United states},
abstract={Development histories can simplify some software engineering tasks, butdifferent tasks require different history granularities. For example, ahistory that includes every edit that resulted in compiling code is neededwhen searching for the cause of a regression, whereas a history that containsonly changes relevant to a feature is needed for understanding the evolutionof the feature. Unfortunately, today, both manual and automated historygeneration result in a single-granularity history. This paper introduces theconcept of multi-grained development history views and the architecture ofCodebase Manipulation, a tool that automatically records a fine-grainedhistory and manages its granularity by applying granularity transformations.<br/> &copy; 2015 IEEE.},
key={Software engineering},
keywords={Automation;},
note={Codebase Manipulation;Development history;Version control;},
URL={http://dx.doi.org/10.1109/ASE.2015.53},
}


@inproceedings{20152801026409,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Detecting program changes from edit history of source code},
journal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
author={Kitsu, Eijirou and Omori, Takayuki and Maruyama, Katsuhisa},
volume={1},
year={2013},
pages={299 - 306},
issn={15301362},
address={Bangkok, Thailand},
abstract={Detecting program changes helps maintainers to figure out the evolution of the changed program. For this, several line-based difference tools have been proposed, which extract differences between two versions of the program. Unfortunately, these tools do not provide enough support to program comprehension since a single commitment stored in a version control system contains multiple changes that are intermingled with each other. Therefore, the maintainers have to untangle them by hand. This work is troublesome and time-consuming. This paper proposes a novel mechanism that automatically detects individual program changes. For this, it restores snapshots of the program from the history of edit operations for the target source code and compares class members that result from syntax analysis for respective snapshots. In addition, the mechanism provides several options of aggregating fine-grained changes detected based on the edit history. The maintainers can select their suitable levels of summarization of program changes. The paper also shows experimental results with a running implementation of the change detection tool. Through the experiment, the detection mechanism presents various kinds of summarized information on program changes, which might facilitate maintainers' activities for program comprehension.<br/> &copy; 2013 IEEE.},
key={Software engineering},
keywords={Computer programming;Syntactics;},
note={Detection mechanism;Development environment;Fine-grained changes;Program analysis;Program comprehension;Software change;Software Evolution;Version control system;},
URL={http://dx.doi.org/10.1109/APSEC.2013.48},
}


@inproceedings{20132016323406,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using version control history to follow the changes of source code elements},
journal={Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
author={Toth, Zoltan and Novak, Gabor and Ferenc, Rudolf and Siket, Istvan},
year={2013},
pages={319 - 322},
issn={15345351},
address={Genova, Italy},
abstract={Version control systems store the whole history of the source code. Since the source code of a system is organized into files and folders, the history tells us the concerned files and their changed lines only but software engineers are also interested in which source code elements (e.g. classes or methods) are affected by a change. Unfortunately, in most programming languages source code elements do not follow the file system hierarchy, which means that a file can contain more classes and methods and a class can be stored in more files, which makes it difficult to determine the changes of classes by using the changes of files. To solve this problem we developed an algorithm, which is able to follow the changes of the source code elements by using the changes of files and we successfully applied it on the Web Kit open source system. &copy; 2013 IEEE.<br/>},
key={Open systems},
keywords={Codes (symbols);Computer programming languages;Computer software maintenance;Control systems;Information management;Open source software;Reengineering;Static analysis;},
note={File systems;Open source system;Repository mining;Source codes;Version control;Version control system;},
URL={http://dx.doi.org/10.1109/CSMR.2013.40},
}


@article{20122715198588,
language={Japanese},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Detecting program changes based on the edit history of source code},
journal={Computer Software},
author={Kitsu, Eijirou and Omori, Takayuki and Maruyama, Katsuhisa},
volume={29},
number={2},
year={2012},
pages={168 - 173},
issn={02896540},
abstract={To detect the changes of a program, the line-based difference between two versions of the program have been frequently used. In a single commitment performed on a version control system, however, multiple changes are usually intermingled. In this case, it is troublesome to untangle them. The proposed method can detect such individual changes. For this, it restores snapshots of the program from the history of editing operations for the target program and compares information on class members that results from syntax analysis for respective snapshots. Experimental results with a running tool implementing are also shown.<br/>},
key={Syntactics},
keywords={Computer software;Software engineering;},
note={Editing operations;Multiple changes;Source codes;Syntax analysis;Version control system;},
}


@inproceedings{20130615994005,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Refactoring edit history of source code},
journal={IEEE International Conference on Software Maintenance, ICSM},
author={Hayashi, Shinpei and Omori, Takayuki and Zenmyo, Teruyoshi and Maruyama, Katsuhisa and Saeki, Motoshi},
year={2012},
pages={617 - 620},
address={Riva del Garda, Trento, Italy},
abstract={This paper proposes a concept for refactoring an edit history of source code and a technique for its automation. The aim of our history refactoring is to improve the clarity and usefulness of the history without changing its overall effect. We have defined primitive history refactorings including their preconditions and procedures, and large refactorings composed of these primitives. Moreover, we have implemented a supporting tool that automates the application of history refactorings in the middle of a source code editing process. Our tool enables developers to pursue some useful applications using history refactorings such as task level commit from an entangled edit history and selective undo of past edit operations. &copy; 2012 IEEE.<br/>},
key={Computer software maintenance},
keywords={Codes (symbols);Computer programming languages;},
note={Refactorings;Software configuration management;Source codes;Supporting tool;Task levels;},
URL={http://dx.doi.org/10.1109/ICSM.2012.6405336},
}


@inproceedings{20183205663896,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Integrating source code search into git client for effective retrieving of change history},
journal={2018 IEEE 1st International Workshop on Mining and Analyzing Interaction Histories, MAINT 2018 - Proceedings},
author={Sasaki, Miwa and Matsumoto, Shinsuke and Kusumoto, Shinji},
volume={2018-January},
year={2018},
pages={12 - 16},
address={Campobasso, Italy},
abstract={In order to achieve effective development management, it is important to manipulate and understand the change histories of source code in a repository. Although general version control systems provide change history manipulation, these systems are restricted to line-based and textual operations such as grep and diff. As such, these systems cannot follow the syntax/semantics of the source code. While various studies have examined querying and searching source codes, these methods cannot follow historical changes. The key concept of this paper is the integration of a source code search technique into Git commands that manipulate historical data in a repository. This paper presents MJgit, a prototype tool for achieving the above goal. In order to evaluate the proposed tool, we conducted a performance experiment using actual software repositories.<br/> &copy; 2018 IEEE.},
key={Codes (symbols)},
keywords={Computer programming languages;Syntactics;Trees (mathematics);},
note={Abstract Syntax Trees;Code changes;Development management;Historical changes;MJgit;Performance experiment;Software repositories;Source code searches;},
URL={http://dx.doi.org/10.1109/MAINT.2018.8323089},
}


@inproceedings{20160501868942,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={How often does a source code unit change within a release window?},
journal={ACM International Conference Proceeding Series},
author={Shobe, Joseph F. and Karim, Md Yasser and Kagdi, Huzefa},
volume={18-20-February-2015},
year={2015},
pages={166 - 175},
address={Bangalore, India},
abstract={To form a training set for a source-code change prediction model, e.g., using the association rule mining or machine learning techniques, commits from the source code history are needed. The traceability between releases and commits would facilitate a systematic choice of history in units of the project evolution scale (i.e., commits that constitute a software release). For example, the major release 25.0 in Chrome is mapped to the earliest revision 157687 and latest revision 165096 in the trunk. Using this traceability, an empirical study is reported on the frequency distribution of file changes for different release windows. In Chrome, the majority (50%) of the committed files change only once between a pair of consecutive releases. This trend is reversed after expanding the window size to at least 10. That is, the majority (50%) of the files change multiple times when commits constituting 10 or greater releases are considered. These results suggest that a training set of at least 10 releases is needed to provide a prediction coverage for majority of the files.<br/> Copyright 2015 ACM.},
key={Software engineering},
keywords={Codes (symbols);Computer programming languages;Learning systems;},
note={Commit history;Empirical studies;Frequency distributions;Machine learning techniques;Mining software repositories;Release window;Software release;Source code changes;},
URL={http://dx.doi.org/10.1145/2723742.2723759},
}


@inproceedings{20140717317073,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Detecting bad smells in source code using change history information},
journal={2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings},
author={Palomba, Fabio and Bavota, Gabriele and Di Penta, Massimiliano and Oliveto, Rocco and De Lucia, Andrea and Poshyvanyk, Denys},
year={2013},
pages={268 - 278},
address={Palo Alto, CA, United states},
abstract={Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately. We propose an approach to detect five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy, by exploiting change history information mined from versioning systems. We applied approach, coined as HIST (Historical Information for Smell deTection), to eight software projects written in Java, and wherever possible compared with existing state-of-the-art smell detectors based on source code analysis. The results indicate that HIST's precision ranges between 61% and 80%, and its recall ranges between 61% and 100%. More importantly, the results confirm that HIST is able to identify code smells that cannot be identified through approaches solely based on code analysis. &copy; 2013 IEEE.<br/>},
key={Codes (symbols)},
keywords={Computer programming languages;Computer software;Odors;},
note={Change history;Code smell;Historical information;Software project;Source code analysis;State of the art;Structural information;Versioning systems;},
URL={http://dx.doi.org/10.1109/ASE.2013.6693086},
}


@article{20183805833694,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Cognitive complexity as a quantifier of version to version Java-based source code change: An empirical probe},
journal={Information and Software Technology},
author={Kaur, Loveleen and Mishra, Ashutosh},
volume={106},
year={2019},
pages={31 - 48},
issn={09505849},
abstract={Context: It has been often argued that it is challenging to modify code fragments from existing software that contains files that are difficult to comprehend. Since systematic software maintenance includes an extensive human activity, cognitive complexity is one of the intrinsic factors that could potentially contribute to or impede an efficient software maintenance practice, the empirical validation of which remains vastly unaddressed. Objective: This study conducts an experimental analysis in which the software developer's level of difficulty in comprehending the software: the cognitive complexity, is theoretically computed and empirically evaluated for estimating its relevance to actual software change. Method: For multiple successive releases of two Java-based software projects, where the source code of a previous release has been substantively used in a novel release, we calculate the change results and the values of the cognitive complexity for each of the version's source code Java files. We construct eight datasets and build predictive models using statistical analysis and machine learning techniques. Results: The pragmatic comparative examination of the estimated cognitive complexity against prevailing metrics of software change and software complexity clearly validates the cognitive complexity metric as a noteworthy measure of version to version source code change.<br/> &copy; 2018 Elsevier B.V.},
key={Java programming language},
keywords={Artificial intelligence;Codes (symbols);Computer software;Learning systems;Regression analysis;},
note={Cognitive complexity;Empirical validation;Experimental analysis;Level of difficulties;Logistic regression analysis;Machine learning techniques;Software change;Software metrics;},
URL={http://dx.doi.org/10.1016/j.infsof.2018.09.002},
}


@inproceedings{20153201154942,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Impact analysis of change requests on source code based on interaction and commit histories},
journal={11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings},
author={Zanjani, Motahareh Bahrami and Swartzendruber, George and Kagdi, Huzefa},
year={2014},
pages={162 - 171},
address={Hyderabad, India},
abstract={The paper presents an approach to perform impact analysis (IA) of an incoming change request on source code. The approach is based on a combination of interaction (e.g., Mylyn) and commit (e.g., CVS) histories. The source code entities (i.e., files and methods) that were interacted or changed in the resolution of past change requests (e.g., bug fixes) were used. Information retrieval, machine learning, and lightweight source code analysis techniques were employed to form a corpus from these source code entities. Additionally, the corpus was augmented with the textual descriptions of the previously resolved change requests and their associated commit messages. Given a textual description of a change request, this corpus is queried to obtain a ranked list of relevant source code entities that are most likely change prone. Such an approach that combines information from interactions and commits for IA at the change request level was not previously investigated. Furthermore, the approach requires only the entities that were interacted and/or committed in the past, which differs from the previous solutions that require indexing of a complete snapshot (e.g., a release). An empirical study on 3272 interactions and 5093 commits from Mylyn, an open source task management tool, was conducted. The results show that the combined approach outperforms an individual approach based on commits. Moreover, it also outperformed an approach based on indexing a single, complete snapshot of a software system. Copyright is held by the author/owner(s). Publication rights licensed to ACM.<br/>},
key={Open source software},
keywords={Artificial intelligence;Codes (symbols);Computer programming languages;Indexing (of information);Information retrieval;Learning systems;},
note={Empirical studies;Impact analysis;Interaction history;Software systems;Source code analysis;Source code entities;Task management;Textual description;},
URL={http://dx.doi.org/10.1145/2597073.2597096},
}


@inproceedings{20145200361650,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={ChangeChecker: A tool for defect prediction in source code changes based on incremental learning method},
journal={Proceedings of 2013 3rd International Conference on Computer Science and Network Technology, ICCSNT 2013},
author={Yuan, Zi and Liu, Chao and Yu, Lili and Zhang, Linghua},
year={2014},
pages={349 - 354},
address={Dalian, China},
abstract={In software development process, software developers may introduce defects as they make changes to software projects. Being aware of introduced defects immediately upon the completion of the change would allow software developers or testers to allocate more resources of testing and inspecting on the current risky change timely, which can shorten the process of defect finding and fixing effectively. In this paper, we propose a software tool called ChangeChecker to help software developers predict whether current source code change has any defects or not during the software development process. This tool infers the existence of defect by dynamically mining patterns of the source code changes in the revision history of the software project. It mainly consists of three components: (1) incremental feature collection and transformation, (2) real-time defect prediction for source code changes, and (3) dynamic update of the learning model. The tool has been evaluated in a large famous open source project Eclipse and applied to a real software development scenario.<br/> &copy; 2013 IEEE.},
key={Open source software},
keywords={Codes (symbols);Computer programming languages;Defects;Engineering education;Forecasting;Open systems;Software design;Software engineering;Software testing;},
note={Current sources;Defect prediction;Incremental learning;Open source projects;Software developer;Software development process;Software project;Source code changes;},
URL={http://dx.doi.org/10.1109/ICCSNT.2013.6967127},
}


@article{20160501864245,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Variance of source code quality change caused by version control operations},
journal={Acta Cybernetica},
author={Farago, Csaba},
volume={22},
number={1},
year={2015},
pages={35 - 56},
issn={0324721X},
abstract={Software maintenance consumes huge efforts. Its cost strongly depends on the quality of the source code: an easy-to-maintain code needs much less effort than the maintenance of a more problematic one. Based on experiences, the maintainability of the source code tends to decrease during its lifetime. However, in most of the cases, this decrease is not a smooth linear one, but there are larger and smaller ups and downs, and the net root of these changes generally results in a negative tendency. Detecting common development patterns which similarly influence the maintainability could help to stop or even turn back source code erosion. In this research the scale of the ups and downs are investigated, namely that which version control operations cause bigger and which smaller changes in the maintainability. We calculated the maintainability and collected the cardinality of each version control operation for every revision of four inspected software systems. With the help of these data we checked which version control operation causes higher absolute code quality changes and which lower. We found clear connection between version control operations and the variance of the maintainability changes. File Additions and file Deletions caused significantly higher variance in maintainability changes compared to file Updates. Commits containing higher number of operations - regardless of the type of the operation - caused higher variance in maintainability changes than those commits containing lower number of operations. As a practical conclusion, it is recommended to pay special attention to the quality of commits containing new file additions, e.g. with the help of a mandatory code review.<br/>},
key={Quality control},
keywords={Codes (symbols);Computer programming languages;Erosion;Information management;Maintainability;Software testing;},
note={Cardinalities;Development patterns;ISO/IEC 9126;Software maintainability;Software systems;Source code qualities;Source codes;Version control;},
URL={http://dx.doi.org/10.14232/actacyb.22.1.2015.4},
}


@inproceedings{20142817914863,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Cookbook: In situ code completion using edit recipes learned from examples},
journal={36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings},
author={Jacobellis, John and Meng, Na and Kim, Miryung},
year={2014},
pages={584 - 587},
address={Hyderabad, India},
abstract={Existing code completion engines leverage only pre-defined templates or match a set of user-defined APIs to complete the rest of changes. We propose a new code completion technique, called Cookbook, where developers can define custom edit recipes|a reusable template of complex edit operations|by specifying change examples. It generates an abstract edit recipe that describes the most specific generalization of the demonstrated example program transformations. Given a library of edit recipes, it matches a developer's edit stream to recommend a suitable recipe that is capable of filling out the rest of change customized to the target. We evaluate Cookbook using 68 systematic changed methods drawn from the version history of Eclipse SWT. Cookbook is able to narrow down to the most suitable recipe in 75% of the cases. It takes 120 milliseconds to find the correct suitable recipe on average, and the edits produced by the selected recipe are on average 82% similar to developers' hand edit. This shows Cookbook's potential to speed up manual editing and to minimize developers' errors. Our demo video is available at https://www.youtube.com/ watch?v=y4BNc8FT4RU. Copyright &copy; 2014 ACM.<br/>},
key={Software engineering},
keywords={Codes (symbols);},
note={Code completions;Edit recipe;Program transformations;Reusable templates;Speed up;},
URL={http://dx.doi.org/10.1145/2591062.2591076},
}


@inproceedings{20184005905913,
language={English},
copyright={Compilation and indexing terms, Copyright 2019 Elsevier Inc.},
copyright={Compendex},
title={Using structured text source code metrics and artificial neural networks to predict change proneness at code tab and program organization level},
journal={ACM International Conference Proceeding Series},
author={Kumar, Lov and Sureka, Ashish},
year={2017},
pages={172 - 180},
address={Jaipur, India},
abstract={Structured Text (ST) is a high-level text-based programming language which is part of the IEC 61131-3 standard. ST is widely used in the domain of industrial automation engineering to create Programmable Logic Controller (PLC) programs. ST is a Domain Specific Language (DSL) which is specialized to the Automation Engineering (AE) application domain. ST has specialized features and programming constructs which are different than general purpose programming languages. We define, develop a tool and compute 10 source code metrics and their correlation with each-other at the Code Tab (CT) and Program Organization Unit (POU) level for two real-world industrial projects at a leading automation engineering company. We study the correlation between the 10 ST source code metrics and their relationship with change proneness at the CT and POU level by creating experimental dataset consisting of different versions of the system. We build predictive models using Artificial Neural Network (ANN) based techniques to predict change proneness of the software. We conduct a series of experiments using various training algorithms and measure the performance of our approach using accuracy and F-measure metrics. We also apply two feature selection techniques to select optimal features aiming to improve the overall accuracy of the classifier.<br/> &copy; 2017 ACM.},
key={Application programs},
keywords={Automation;Codes (symbols);Computer circuits;Controllers;Engineering education;Forecasting;Learning systems;Network coding;Neural networks;Object oriented programming;Problem oriented languages;Programmable logic controllers;Programmed control systems;},
note={Change proneness;Machine learning applications;Programmable Logic Controller (PLC);Source code analysis;Source code metrics;Structured text;},
URL={http://dx.doi.org/10.1145/3021460.3021481},
}



