@article{MULJATI20171019,
title = "Performance of Direct Displacement Based Design on Regular Concrete Building Against Indonesian Response Spectrum",
journal = "Procedia Engineering",
volume = "171",
pages = "1019 - 1024",
year = "2017",
note = "The 3rd International Conference on Sustainable Civil Engineering Structures and Construction Materials - Sustainable Structures for Future Generations",
issn = "1877-7058",
doi = "https://doi.org/10.1016/j.proeng.2017.01.438",
url = "http://www.sciencedirect.com/science/article/pii/S1877705817304484",
author = "Ima Muljati and Benjamin Lumantarna and Reynaldo P. Intan and Arygianny Valentino",
keywords = "Direct displacement based design, non-linier time history analysis, performance based design, regular plan building",
abstract = "The renewal of Indonesian seismic code from SNI 1726-2002 into SNI 1726-2012 brings significant change in the design spectrum. Focused on several regular plan concrete building which have been design using displacement based design method, the aim of this study is to verify their performance using nonlinear time history analysis based on parameters: drift, damage indices, and plastic mechanism determined by FEMA 356. The excitation is spectrum consistent accelerogram based on El-Centro 1940 N-S, to match with the new Indonesian response spectrum for soft soil in low- and high intensity area. It is found that the code-designed buildings are not suitable for the targeted design of level-2 with maximum drift of 2.5% due to major. This is caused by improper selection of SNI spectrum as the design major earthquake. In fact, it is only equivalent to small earthquake. Although buildings survive up to a very rare earthquake without collapse but they suffer excessive damage and rotation due to small- to major-earthquake. The capacity design procedure is able to maintain ductile mechanism, but some columns experience yielding at prohibited locations."
}
@article{CHEN2015e45,
title = "Quality improvement of International Classification of Diseases, 9th revision, diagnosis coding in radiation oncology: Single-institution prospective study at University of California, San Francisco",
journal = "Practical Radiation Oncology",
volume = "5",
number = "1",
pages = "e45 - e51",
year = "2015",
issn = "1879-8500",
doi = "https://doi.org/10.1016/j.prro.2014.03.007",
url = "http://www.sciencedirect.com/science/article/pii/S1879850014000678",
author = "Chien P. Chen and Steve Braunstein and Michelle Mourad and I-Chow J. Hsu and Daphne Haas-Kogan and Mack Roach and Shannon E. Fogh",
abstract = "Purpose
Accurate International Classification of Diseases (ICD) diagnosis coding is critical for patient care, billing purposes, and research endeavors. In this single-institution study, we evaluated our baseline ICD-9 (9th revision) diagnosis coding accuracy, identified the most common errors contributing to inaccurate coding, and implemented a multimodality strategy to improve radiation oncology coding.
Methods and materials
We prospectively studied ICD-9 coding accuracy in our radiation therapy--specific electronic medical record system. Baseline ICD-9 coding accuracy was obtained from chart review targeting ICD-9 coding accuracy of all patients treated at our institution between March and June of 2010. To improve performance an educational session highlighted common coding errors, and a user-friendly software tool, RadOnc ICD Search, version 1.0, for coding radiation oncology specific diagnoses was implemented. We then prospectively analyzed ICD-9 coding accuracy for all patients treated from July 2010 to June 2011, with the goal of maintaining 80% or higher coding accuracy. Data on coding accuracy were analyzed and fed back monthly to individual providers.
Results
Baseline coding accuracy for physicians was 463 of 661 (70%) cases. Only 46% of physicians had coding accuracy above 80%. The most common errors involved metastatic cases, whereby primary or secondary site ICD-9 codes were either incorrect or missing, and special procedures such as stereotactic radiosurgery cases. After implementing our project, overall coding accuracy rose to 92% (range, 86%-96%). The median accuracy for all physicians was 93% (range, 77%‐100%) with only 1 attending having accuracy below 80%. Incorrect primary and secondary ICD-9 codes in metastatic cases showed the most significant improvement (10% vs 2% after intervention).
Conclusions
Identifying common coding errors and implementing both education and systems changes led to significantly improved coding accuracy. This quality assurance project highlights the potential problem of ICD-9 coding accuracy by physicians and offers an approach to effectively address this shortcoming."
}
@article{KOTLYAR2017236,
title = "A perturbation-based susbtep method for coupled depletion Monte-Carlo codes",
journal = "Annals of Nuclear Energy",
volume = "102",
pages = "236 - 244",
year = "2017",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2016.12.022",
url = "http://www.sciencedirect.com/science/article/pii/S0306454916308945",
author = "Dan Kotlyar and Manuele Aufiero and Eugene Shwageraus and Massimiliano Fratoni",
keywords = "Monte Carlo, Coupled codes, Depletion, General perturbation theory, Serpent",
abstract = "Coupled Monte Carlo (MC) methods are becoming widely used in reactor physics analysis and design. Many research groups therefore, developed their own coupled MC depletion codes. Typically, in such coupled code systems, neutron fluxes and cross sections are provided to the depletion module by solving a static neutron transport problem. These fluxes and cross sections are representative only of a specific time-point. In reality however, both quantities would change through the depletion time interval. Recently, Generalized Perturbation Theory (GPT) equivalent method that relies on collision history approach was implemented in Serpent MC code. This method was used here to calculate the sensitivity of each nuclide and reaction cross section due to the change in concentration of every isotope in the system. The coupling method proposed in this study also uses the substep approach, which incorporates these sensitivity coefficients to account for temporal changes in cross sections. As a result, a notable improvement in time dependent cross section behavior was obtained. The method was implemented in a wrapper script that couples Serpent with an external depletion solver. The performance of this method was compared with other existing methods. The results indicate that the proposed method requires substantially less MC transport solutions to achieve the same accuracy."
}
@article{PARNELL2016111,
title = "Joint palaeoclimate reconstruction from pollen data via forward models and climate histories",
journal = "Quaternary Science Reviews",
volume = "151",
pages = "111 - 126",
year = "2016",
issn = "0277-3791",
doi = "https://doi.org/10.1016/j.quascirev.2016.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S0277379116303511",
author = "Andrew C. Parnell and John Haslett and James Sweeney and Thinh K. Doan and Judy R.M. Allen and Brian Huntley",
keywords = "Palaeoclimate reconstruction, Statistical modelling, Forward models, Climate histories, Joint inference, Palynology, Chronological uncertainty",
abstract = "We present a method and software for reconstructing palaeoclimate from pollen data with a focus on accounting for and reducing uncertainty. The tools we use include: forward models, which enable us to account for the data generating process and hence the complex relationship between pollen and climate; joint inference, which reduces uncertainty by borrowing strength between aspects of climate and slices of the core; and dynamic climate histories, which allow for a far richer gamut of inferential possibilities. Through a Monte Carlo approach we generate numerous equally probable joint climate histories, each of which is represented by a sequence of values of three climate dimensions in discrete time, i.e. a multivariate time series. All histories are consistent with the uncertainties in the forward model and the natural temporal variability in climate. Once generated, these histories can provide most probable climate estimates with uncertainty intervals. This is particularly important as attention moves to the dynamics of past climate changes. For example, such methods allow us to identify, with realistic uncertainty, the past century that exhibited the greatest warming. We illustrate our method with two data sets: Laguna de la Roya, with a radiocarbon dated chronology and hence timing uncertainty; and Lago Grande di Monticchio, which contains laminated sediment and extends back to the penultimate glacial stage. The procedure is made available via an open source R package, Bclim, for which we provide code and instructions."
}
@article{XHONNEUX2014361,
title = "Development of an integrated fission product release and transport code for spatially resolved full-core calculations of V/HTRs",
journal = "Nuclear Engineering and Design",
volume = "271",
pages = "361 - 369",
year = "2014",
note = "SI : HTR 2012",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2013.11.063",
url = "http://www.sciencedirect.com/science/article/pii/S0029549313006481",
author = "Andre Xhonneux and Hans-Josef Allelein",
abstract = "The computer codes FRESCO-I, FRESCO-II, PANAMA and SPATRA developed at Forschungszentrum Jülich in Germany in the early 1980s are essential tools to predict the fission product release from spherical fuel elements and the TRISO fuel performance, respectively, under given normal or accidental conditions. These codes are able to calculate a conservative estimation of the source term, i.e. quantity and duration of radionuclide release. Recently, these codes have been reversed engineered, modernized (FORTRAN 95/2003) and combined to form a consistent code named STACY (Source Term Analysis Code System). STACY will later become a module of the V/HTR Code Package (HCP). In addition, further improvements have been implemented to enable more detailed calculations. For example the distinct temperature profile along the pebble radius is now taken into account and coated particle failure rates can be calculated under normal operating conditions. In addition, the absolute fission product release of an V/HTR pebble bed core can be calculated by using the newly developed burnup code Topological Nuclide Transformation (TNT) replacing the former rudimentary approach. As a new functionality, spatially resolved fission product release calculations for normal operating conditions as well as accident conditions can be performed. In case of a full-core calculation, a large number of individual pebbles which follow a random path through the reactor core can be simulated. The history of the individual pebble is recorded, too. Main input data such as spatially resolved neutron fluxes and fluid dynamics data are provided by the VSOP code. Capabilities of the FRESCO-I and SPATRA code which allow for the simulation of the redistribution of fission products within the primary circuit and the deposition of fission products on graphitic and metallic surfaces are also available in STACY. In this paper, details of the STACY model and first results for its application to the 200MW(th) HTR-Module are presented."
}
@article{MICKEVICIUS2013409,
title = "Revised calculation of four-particle harmonic-oscillator transformation brackets matrix",
journal = "Computer Physics Communications",
volume = "184",
number = "2",
pages = "409 - 413",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2012.09.021",
url = "http://www.sciencedirect.com/science/article/pii/S0010465512003074",
author = "S. Mickevičius and D. Germanas and R.K. Kalinauskas",
keywords = "Mathematical methods in physics, Algebraic methods, Nuclear shell model",
abstract = "In this article we present a new, considerably enhanced and more rapid method for calculation of the matrix of four-particle harmonic-oscillator transformation brackets (4HOB). The new method is an improved version of 4HOB matrix calculations which facilitates the matrix calculation by finding the eigenvectors of the 4HOB matrix explicitly. Using this idea the new Fortran code for fast and 4HOB matrix calculation is presented. The calculation time decreases more than a few hundred times for large matrices. As many problems of nuclear and hadron physics structure are modeled on the harmonic oscillator (HO) basis our presented method can be useful for large-scale nuclear structure and many-particle identical fermion systems calculations.
Program summary
Title of program: HOTB_M Catalogue identifier: AEFQ_v3_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEFQ_v3_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License version 3 No. of lines in distributed program, including test data, etc.: 2149 No. of bytes in distributed program, including test data, etc.: 17576 Distribution format: tar.gz Programming language: Fortran 90. Computer: Any computer with Fortran 90 compiler. Operating system: Windows, Linux, FreeBSD, True64 Unix. RAM: Up to a few Gigabytes (see Table 1, Table 2 included in the distribution package) Classification: 17.16, 17.17. Catalogue identifier of previous version: AEFQ_v2_0 Journal reference of previous version: Comput. Phys. Comm. 182(2011)1377 Does the new version supersede the previous version?: Yes Nature of problem: Calculation of the matrix of the 4HOB in a more effective way, which allows us to calculate the matrix of the brackets up to a few hundred times more rapidly than in a previous version. Solution method: The method is based on compact expressions of 4HOB, presented in [1] and its simplifications presented in this paper. Reasons for new version: We facilitated the calculation of the 4HOB, based on the method presented in the section ’Theoretical aspects’. The new program version gives shorter calculation times for the 4HOB Summary of revisions: New subroutines for calculation of the matrix of the 4HOB. For theoretical issues of revision see the section ’Theoretical aspects’. Restrictions: The 4HOB matrices up to e=28. Running time: Depends on the dimension of the 4HOB matrix (see Table 1, Table 2 included in the distribution file). References: [1] D. Germanas, S. Mickevicius, R.K. Kalinauskas, Calculation of four-particle harmonic-oscillator transformation brackets, Computer Physics Communications 181, 420–425 (2010)."
}
@article{BAIER2014431,
title = "Extension and application of the reactor dynamics code DYN3D for Block-type High Temperature Reactors",
journal = "Nuclear Engineering and Design",
volume = "271",
pages = "431 - 436",
year = "2014",
note = "SI : HTR 2012",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2013.12.013",
url = "http://www.sciencedirect.com/science/article/pii/S0029549313006882",
author = "Silvio Baier and Emil Fridman and Soeren Kliem and Ulrich Rohde",
abstract = "The reactor code DYN3D was developed at the Helmholtz-Zentrum Dresden-Rossendorf to study steady state and transient behavior of Light Water Reactors. Concerning the neutronics part, the multigroup diffusion or SP3 transport equation based on nodal expansion methods is solved both for hexagonal and square fuel element geometry. To deal with Block-type High Temperature Reactor cores DYN3D was extended to a version DYN3D-HTR. A 3D heat conduction model was introduced to include 3D effects of heat transfer and heat conduction and the detailed structure of the fuel element. Homogenized neutronic cross sections were generated by applying a Monte Carlo approach with resolution of each individual TRISO fuel particle. Results of coupled steady state and transient calculations with 12 energy groups are presented. Transient case studies are control rod insertion, a change of the inlet coolant temperature and a change of the coolant gas mass flow rate. It is shown that DYN3D-HTR is an appropriate code system to simulate steady states and short time transients. Furthermore the necessity of the 3D heat conduction model is demonstrated."
}
@article{GARVERT2015418,
title = "Learning-Induced Plasticity in Medial Prefrontal Cortex Predicts Preference Malleability",
journal = "Neuron",
volume = "85",
number = "2",
pages = "418 - 428",
year = "2015",
issn = "0896-6273",
doi = "https://doi.org/10.1016/j.neuron.2014.12.033",
url = "http://www.sciencedirect.com/science/article/pii/S0896627314011428",
author = "Mona M. Garvert and Michael Moutoussis and Zeb Kurth-Nelson and Timothy E.J. Behrens and Raymond J. Dolan",
abstract = "Summary
Learning induces plasticity in neuronal networks. As neuronal populations contribute to multiple representations, we reasoned plasticity in one representation might influence others. We used human fMRI repetition suppression to show that plasticity induced by learning another individual’s values impacts upon a value representation for oneself in medial prefrontal cortex (mPFC), a plasticity also evident behaviorally in a preference shift. We show this plasticity is driven by a striatal “prediction error,” signaling the discrepancy between the other’s choice and a subject’s own preferences. Thus, our data highlight that mPFC encodes agent-independent representations of subjective value, such that prediction errors simultaneously update multiple agents’ value representations. As the resulting change in representational similarity predicts interindividual differences in the malleability of subjective preferences, our findings shed mechanistic light on complex human processes such as the powerful influence of social interaction on beliefs and preferences."
}
@article{LIN2018118,
title = "Change in 1-year hospitalization of overall and older patients with major depressive disorder after second-generation antipsychotics augmentation treatment",
journal = "Journal of Affective Disorders",
volume = "230",
pages = "118 - 124",
year = "2018",
issn = "0165-0327",
doi = "https://doi.org/10.1016/j.jad.2018.01.011",
url = "http://www.sciencedirect.com/science/article/pii/S0165032717318670",
author = "Chun-Yuan Lin and Te-Jen Lai and Yu-Hsin Wu and Ping-Kun Chen and Yuan-Fu Lin and I.-Chia Chien",
keywords = "Geriatric/Aging/Elderly, Depression, Pharmacoepidemiology, Pharmacotherapy, Treatment resistance",
abstract = "Background
Studies on second-generation antipsychotics (SGA) augmentation treatment for older adults with major depressive disorder (MDD) remain limited. We aimed to investigate the effectiveness of SGA augmentation for overall and older patients with MDD inpatient history by assessing the change in 1-year hospitalization before and after SGA augmentation using the latest National Health Insurance Research Database (NHIRD) in Taiwan.
Methods
The samples were MDD patients (ICD-9 CM code: 296.2 and 296.3) who had psychiatric inpatient history. A total of 2602 MDD patients including 430 elderly subjects (age ≥ 60 years) who received SGA augmentation for 8 weeks between January 1998 and December 2012 were included in this 1-year mirror-image study. Outcome measures included number and length of psychiatric and all-cause hospitalizations.
Results
After 8-week continuous SGA augmentation in the study subjects, the total number and days of psychiatric hospitalizations among overall patients reduced by 33.57% (p < .0001) and 18.24% (p < .0001), respectively; the total number and days of psychiatric hospitalizations among older patients (age ≥ 60) reduced by 44.52% (p < .0001) and 27.95% (p < .0001), respectively. Similarly, the total number and days of all-cause hospitalizations were significantly reduced.
Limitations
MDD patients without inpatient history were not included due to data limitation; hence, the results may not be generalized to all patients.
Conclusions
The results support that SGA may be effective in reducing psychiatric and all-cause hospitalization among overall and elderly MDD patients. More studies focusing on the safety of SGA among older MDD patients is warranted."
}
@article{TODD2018123,
title = "Time as context: The influence of hierarchical patterning on sensory inference",
journal = "Schizophrenia Research",
volume = "191",
pages = "123 - 131",
year = "2018",
note = "Mismatch Negativity",
issn = "0920-9964",
doi = "https://doi.org/10.1016/j.schres.2017.03.033",
url = "http://www.sciencedirect.com/science/article/pii/S092099641730172X",
author = "Juanita Todd and Anne Petherbridge and Bronte Speirs and Alexander Provost and Bryan Paton",
keywords = "Mismatch negativity, Temporal processing, Context, Hierarchical inference, Predictive coding",
abstract = "Time, or more specifically temporal structure, is a critical variable in understanding how the auditory system uses acoustic patterns to predict input, and to filter events based on their relevance. A key index of this filtering process is the auditory evoked potential component known as mismatch negativity or MMN. In this paper we review findings of smaller MMN in schizophrenia through the lens of time as an influential contextual variable. More specifically, we review studies that show how MMN to a locally rare pattern-deviation is modulated by the longer-term context in which it occurs. Empirical data is presented from a non-clinical sample confirming that the absence of a stable higher-order structure to sound sequences alters the way MMN amplitude changes over time. This result is discussed in relation to how hierarchical pattern learning might enrich our understanding of how and why MMN amplitude modulation is disrupted in schizophrenia."
}
@article{STEFANES201843,
title = "Property size drives differences in forest code compliance in the Brazilian Cerrado",
journal = "Land Use Policy",
volume = "75",
pages = "43 - 49",
year = "2018",
issn = "0264-8377",
doi = "https://doi.org/10.1016/j.landusepol.2018.03.022",
url = "http://www.sciencedirect.com/science/article/pii/S0264837716313047",
author = "Mauricio Stefanes and Fabio de Oliveira Roque and Reinaldo Lourival and Isabel Melo and Pierre Cyril Renaud and Jose Manuel Ochoa Quintero",
keywords = "Cerrado land use, Farm size, Rural environmental registry (CAR), Deforestation, Savannah",
abstract = "The Rural Environmental Registry (CAR) dataset opens a new window for spatially explicit studies of the rural landscape of Brazil, enabling analysis with an accurate representation of land use and land cover change dynamics at the property level. Here, we evaluated farm compliance with the Brazilian Forest Code (revised in 2012) in Mato Grosso do Sul state, where agribusiness activities have already converted more than 70% of native vegetation, Cerrado. We analysed the most recent version of the CAR dataset, using geographic information system analytical tools. We observed a positive relationship between compliance with the 20% compulsory Legal Reserves and farm size class. We showed that larger, rather than smaller, farms have important effects on biodiversity conservation at the landscape scale. Large farms (> than 1000 ha), comprising 74.2% of the study area, tended to show better compliance levels (51%) than smaller properties (33%). At the same time, they contain huge amount of land with native vegetation that lies outside Legal Reserves, and so may pose a risk for legal deforestation of near 2 million ha. We argue that a portfolio of socioeconomic incentives for restoration, protected areas, and no-net-loss components in agricultural programmes, are essential measures to increase compliance and halt deforestation in the Cerrado of Central Brazil. Moreover, we argue that considering property size improves the likelihood of success of such initiatives. Although acknowledging that landscape management can help address socioeconomic conflicts and improve food production, it must be accompanied by a strong “anti-deforestation” policy to guarantee the maintenance of existing native vegetation remnants. We also highlight the importance of investigating the role of property size in maintaining remaining vegetation in this region, instead of merely focused on the number of compliant farms."
}
@article{SINGH2017338,
title = "A numerical methodology for estimation of volatile fission products release from nuclear fuel",
journal = "Nuclear Engineering and Design",
volume = "323",
pages = "338 - 344",
year = "2017",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2017.02.036",
url = "http://www.sciencedirect.com/science/article/pii/S002954931730105X",
author = "Mahender Singh and Deb Mukhopadhyay and D. Datta",
keywords = "Fission product, Diffusion equation, Fractional release, Fuel grain",
abstract = "In this paper we have developed a numerical methodology which is capable of estimating volatile fission product release from nuclear fuel under changing irradiation conditions with incorporation of all physical phenomena’s. The present study bridges the gap between previous analytical studies by providing estimation of unstable fission product release under accidental condition with and without incorporation of precursor’s history effect. Based on the present methodology a computer model “FIPRAP-Fission Product Release Analysis Program” has been developed and validated with standard fission product release problems and benchmarked against internationally accepted CORSOR-BOOTH model as well as Accident Source Term Evaluation Code (ASTEC). The result of the proposed model lies within 15% of internationally accepted models."
}
@article{LARAPRIETO201559,
title = "An Innovative Self-learning Approach to 3D Printing Using Multimedia and Augmented Reality on Mobile Devices",
journal = "Procedia Computer Science",
volume = "75",
pages = "59 - 65",
year = "2015",
note = "2015 International Conference Virtual and Augmented Reality in Education",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.12.206",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915036674",
author = "Vianney Lara-Prieto and Efraín Bravo-Quirino and Miguel Ángel Rivera-Campa and José Enrique Gutiérrez-Arredondo",
keywords = "3D Printing, Augmented Reality, Self-learning",
abstract = "Technology is evolving rapidly and it is becoming harder to keep up its pace. At a university environment, important investments are required so that students and professors can have access to novel technology. However, it is also fundamental to know how to use this novel technology to exploit its benefits. Tecnológico de Monterrey, Campus Monterrey, recently acquired several 3D printers so that the students could get familiar with this rapid prototyping technology and use them to deliver better quality projects. Nevertheless, the new challenge was to administrate the 3D printing process including 3D model verification, STL file generation, printing time and raw material. Since students were not familiar with 3D printing, the expert had to spend a lot of time with them explaining the whole process from the modelling to the printing stage, and verifying their work to make an efficient use of 3D printing time and materials. To overcome this situation, it was decided to make use of augmented reality and multimedia applications to generate tutorials for self-learning the whole process of 3D printing. Nowadays, the wide spread of mobile devices and wireless technologies brings a huge potential to e-learning changing dramatically the traditional instructor-oriented scheme. The learning process can take place in an informal setting having the tutorials available on mobile devices by just scanning a quick response code, usually known as QR code. The first QR code enables a link to download the free augmented reality Layar app. Then, several images can be scanned using Layar to open each of the video tutorials. These videos explain graphically step by step of all the processes involved and give different options of software to be used. The tutorials are easy to follow so that any engineering or design student can learn from them. This case-study offers an innovative learning approach that fosters self-learning and a more efficient use of technology resources."
}
@article{URAKAWA201792,
title = "Exogenously-driven perceptual alternation of a bistable image: From the perspective of the visual change detection process",
journal = "Neuroscience Letters",
volume = "653",
pages = "92 - 96",
year = "2017",
issn = "0304-3940",
doi = "https://doi.org/10.1016/j.neulet.2017.05.041",
url = "http://www.sciencedirect.com/science/article/pii/S0304394017304378",
author = "Tomokazu Urakawa and Tomoya Aragaki and Osamu Araki",
keywords = "Bistable, Perceptual alternation, Predictive coding, Visual change detection, vMMN",
abstract = "Based on the predictive coding framework, the present behavioral study focused on the automatic visual change detection process, which yields a concomitant prediction error, as one of the visual processes relevant to the exogenously-driven perceptual alternation of a bistable image. According to this perspective, we speculated that the automatic visual change detection process with an enhanced prediction error is relevant to the greater induction of exogenously-driven perceptual alternation and attempted to test this hypothesis. A modified version of the oddball paradigm was used based on previous electroencephalographic studies on visual change detection, in which the deviant and standard defined by the bar’s orientation were symmetrically presented around a continuously presented Necker cube (a bistable image). By manipulating inter-stimulus intervals and the number of standard repetitions, we set three experimental blocks: HM, IM, and LM blocks, in which the strength of the prediction error to the deviant relative to the standard was expected to gradually decrease in that order. The results obtained showed that the deviant significantly increased perceptual alternation of the Necker cube over that by the standard from before to after the presentation of the deviant. Furthermore, the differential proportion of the deviant relative to the standard significantly decreased from the HM block to the IM and LM blocks. These results are consistent with our hypothesis, supporting the involvement of the automatic visual change detection process in the induction of exogenously-driven perceptual alternation."
}
@article{FU201240,
title = "Modification and application of the system analysis code ATHLET to trans-critical simulations",
journal = "Annals of Nuclear Energy",
volume = "44",
pages = "40 - 49",
year = "2012",
issn = "0306-4549",
doi = "https://doi.org/10.1016/j.anucene.2012.02.005",
url = "http://www.sciencedirect.com/science/article/pii/S0306454912000382",
author = "S.W. Fu and X.J. Liu and C. Zhou and Z.H. Xu and Y.H. Yang and X. Cheng",
keywords = "Supercritical pressure, Pseudo two-phase method, Trans-critical transient, ATHLET-SC",
abstract = "During the loss of coolant accident (LOCA) of supercritical water cooled reactor (SCWR), the pressure in the reactor system will undergo a rapid decrease from supercritical to subcritical condition. This process is called trans-critical transients, which is of crucial importance for the LOCA analysis of SCWR. Using the current version of system code (e.g. ATHLET, REALP), calculation will be terminated due to the abrupt change of void fraction across the critical point (22.064MPa). To solve this problem, a pseudo two-phase method is proposed by introducing a fictitious region of latent heat (enthalpy of vaporization hfg∗) at pseudo-critical temperatures. A smooth transition of void fraction can be realized by using liquid-field conservation equations at temperatures lower than the pseudo-critical temperature, and vapor-field conservation equations at temperatures higher than the pseudo-critical temperature. Adopting this method, the system code ATHLET is modified to ATHLET-SC mod 2 on the basic of the previous version ATHLET-SC mod 1 modified by Shanghai Jiao Tong University. When the fictitious region of latent heat is kept as a small region, the code can achieve an acceptable accuracy. Moreover, the ATHLET-SC mod 2 code is applied to simulate the blowdown process of a simplified model. The results achieved so far indicate a good applicability of the new modified code for the trans-critical transient."
}
@article{ANGELOPOULOS2016103,
title = "Advances in integrative statistics for logic programming",
journal = "International Journal of Approximate Reasoning",
volume = "78",
pages = "103 - 115",
year = "2016",
issn = "0888-613X",
doi = "https://doi.org/10.1016/j.ijar.2016.06.008",
url = "http://www.sciencedirect.com/science/article/pii/S0888613X16300937",
author = "Nicos Angelopoulos and Samer Abdallah and Georgios Giamas",
keywords = " statistical computing, Logic programming, Visualisation, Machine learning, Graph drawing, bioinformatics",
abstract = "We present recent developments on the syntax of Real, a library for interfacing two Prolog systems to the statistical language R. We focus on the changes in Prolog syntax within SWI-Prolog that accommodate greater syntactic integration, enhanced user experience and improved features for web-services. We recount the full syntax and functionality of Real as well as presenting a full application and sister packages which include Prolog code interfacing a number of common and useful tasks that can be delegated to R. We argue that Real is a powerful extension to logic programming, providing access to a popular statistical system that has complementary strengths in areas such as machine learning, statistical inference and visualisation. Furthermore, Real has a central role to play in the uptake of semantic web, computational biology and bioinformatics as application areas for research in logic programming."
}
@article{HUANG2018416,
title = "Tensor Discriminant Analysis with Partial Label",
journal = "Procedia Computer Science",
volume = "131",
pages = "416 - 424",
year = "2018",
note = "Recent Advancement in Information and Communication Technology:",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2018.04.225",
url = "http://www.sciencedirect.com/science/article/pii/S1877050918306057",
author = "Kai Huang and Di Zhou and Yingna Cong and Wen Xu and Wei Wang and Zhiguo Que",
keywords = "learning, dimension reduction, electrocardiograms (ECGs) analysis, sparse coding, tensor analysis, multilinear analysis",
abstract = "Small Sample Size (SSS) and overfitting problems will be encountered if using vector space methods by vectorize high order tensor data. In addition, the structure information will be lost, So thensor based methods or multilinear algorithm will be more approriate for this type of data. For medical data expecially electrocardiograms (ECGs), achieving the diagnosed disease label is time sonsuming and expensive. To overcome this type of problem, this paper propose a algorithm for data with partial label, in other words, our algorithm take labeled data and unlabel data as input data, Anotoher issue is the spaseity of effective features. Here we propose a Sparse Semisupervised Sparse Multilinear Discriminant Analysis (SSSMDA) for electrocardiograms (ECGs), our method consider the distirbution of unlabeled and labeled data, we also consider the label achieved by label propagation algorithm. We convert original 12 lead electrocardiograms (ECGs) to 3-order tensor by Short Time Fourier Transformation (STFT). The effectiveness of our approach is proved in the experiment section."
}
@article{SMITH201658,
title = "Trunk–pelvis coordination during turning: A cross sectional study of young adults with and without a history of low back pain",
journal = "Clinical Biomechanics",
volume = "36",
pages = "58 - 64",
year = "2016",
issn = "0268-0033",
doi = "https://doi.org/10.1016/j.clinbiomech.2016.05.011",
url = "http://www.sciencedirect.com/science/article/pii/S0268003316300687",
author = "Jo Armour Smith and Kornelia Kulig",
keywords = "Walking turns, Trunk, Pelvis, Coordination, Low back pain",
abstract = "Background
During steady-state locomotion, symptomatic individuals with low back pain demonstrate reduced ability to modulate coordination between the trunk and the pelvis in the axial plane. It is unclear if this is also true during functional locomotor perturbations such as changing direction, or if this change in coordination adaptability persists between symptomatic episodes. The purpose of this study was to compare trunk–pelvis coordination during walking turns in healthy individuals and asymptomatic individuals with a history of low back pain.
Methods
Participants performed multiple ipsilateral turns. Axial plane inter-segmental coordination and stride-to-stride coordination variability were quantified using the vector coding technique. Frequency of coordination mode and amplitude of coordination variability was compared between groups using Wilcoxon signed-rank tests and paired t-tests respectively.
Findings
During stance phase of the turn, there was no significant difference in either inter-segmental coordination or coordination variability between groups. Inter-segmental coordination between the trunk and the pelvis was predominantly inphase during this part of the turn. During swing phase, patterns of coordination were more diversified, and individuals with a history of low back pain had significantly greater trunk phase coordination than healthy controls. Coordination variability was the same in both groups.
Interpretation
Changes in trunk–pelvis coordination are evident between symptomatic episodes in individuals with a history of low back pain. However, previously demonstrated decreases in coordination variability were not found between symptomatic episodes in individuals with recurrent low back pain and therefore may represent a response to concurrent pain rather than a persistent change in motor control."
}
@article{PINHEIROCHAGAS2018,
title = "Decoding the processing stages of mental arithmetic with magnetoencephalography",
journal = "Cortex",
year = "2018",
issn = "0010-9452",
doi = "https://doi.org/10.1016/j.cortex.2018.07.018",
url = "http://www.sciencedirect.com/science/article/pii/S0010945218302351",
author = "Pedro Pinheiro-Chagas and Manuela Piazza and Stanislas Dehaene",
keywords = "Mental arithmetic, Magnetoencephalography, Decoding, Representational similarity analysis",
abstract = "Elementary arithmetic is highly prevalent in our daily lives. However, despite decades of research, we are only beginning to understand how the brain solves simple calculations. Here, we applied machine learning techniques to magnetoencephalography (MEG) signals in an effort to decompose the successive processing stages and mental transformations underlying elementary arithmetic. Adults subjects verified single-digit addition and subtraction problems such as 3 + 2 = 9 in which each successive symbol was presented sequentially. MEG signals revealed a cascade of partially overlapping brain states. While the first operand could be transiently decoded above chance level, primarily based on its visual properties, the decoding of the second operand was more accurate and lasted longer. Representational similarity analyses suggested that this decoding rested on both visual and magnitude codes. We were also able to decode the operation type (additions vs. subtraction) during practically the entire trial after the presentation of the operation sign. At the decision stage, MEG indicated a fast and highly overlapping temporal dynamics for (1) identifying the proposed result, (2) judging whether it was correct or incorrect, and (3) pressing the response button. Surprisingly, however, the internally computed result could not be decoded. Our results provide a first comprehensive picture of the unfolding processing stages underlying arithmetic calculations at a single-trial level, and suggest that externally and internally generated neural codes may have different neural substrates."
}
@article{REZAEI2016137,
title = "Analysis of the effect of climate change on the reliability of overhead transmission lines",
journal = "Sustainable Cities and Society",
volume = "27",
pages = "137 - 144",
year = "2016",
issn = "2210-6707",
doi = "https://doi.org/10.1016/j.scs.2016.01.007",
url = "http://www.sciencedirect.com/science/article/pii/S2210670716300075",
author = "Seyedeh Nasim Rezaei and Luc Chouinard and Sébastien Langlois and Frédéric Légeron",
keywords = "Structural reliability analysis, Transmission lines, Climate change, Statistical learning theory",
abstract = "Climate change is anticipated to influence the reliability of overhead transmission and distribution lines through impacts on extreme weather events. Changes in the frequency and intensity of wind and ice storms may have a considerable effect on applied loads and can consequently affect the probability of structural failure of different components of the line. This study examines the reliability of transmission lines under a range of assumed changes in the mean and standard deviation of climatic variables affecting transmission lines such as annual extreme wind speed and ice thickness. The methodology used for the reliability analysis of transmission lines under current and future climatic conditions is based on the concepts of statistical learning theory. The sensitivity study provides the information required to improve the capacity of transmission lines and mitigate long-term risks from the effects of a changing climate. The results indicate that climate change as predicted by many researchers can significantly affect the reliability of existing transmission line systems. Hence, relying on the historic climatic data may not be sufficient to ensure an adequate reliability of transmission line systems in the future. The specification of design loads for the evaluation of existing lines or the design of new lines should consider both future climate models and historical climate data."
}
@article{GARCIAGALLARDO201653,
title = "Effects of variable sequences of food availability on interval time-place learning by pigeons",
journal = "Behavioural Processes",
volume = "130",
pages = "53 - 64",
year = "2016",
issn = "0376-6357",
doi = "https://doi.org/10.1016/j.beproc.2016.07.008",
url = "http://www.sciencedirect.com/science/article/pii/S0376635716301619",
author = "Daniel García-Gallardo and Claudio Carpio",
keywords = "Time-place learning, Timing, Pigeon",
abstract = "The effects of within session variability of the sequences of food availability in a 16 period Time Place Learning (TPL) task on the performance of pigeons were assessed. Two groups of birds were exposed to two conditions. For group 1 (N=3), the first condition consisted of a TPL task in which food could be obtained according to a Random Interval (RI) 25s schedule of reinforcement in one of four feeders, the correct feeder changed every 3min. The same sequence was repeated four times within every training session (Fixed Sequence). The second condition was exactly the same as the first one with the exception that the sequence in which the correct feeder changed was randomized, yielding a total of four randomized sequences of food availability each session (Variable Sequence). An Open Hopper Test (OHT) was conducted at the end of each condition. Birds in group 2 (N=3) experienced the same conditions but in the reverse order. Results showed high percent correct responses for both group of birds under both conditions. However, birds were able to time the availability period’s duration only under the Fixed Sequence condition, as shown by anticipation, anticipation of depletion and persistence of visiting patterns on the OHT. The implications of these results to Gallistels (1990) tripartite time-place-event memory code model are discussed, pointing out that these results are in line with previous findings about the important role that spatial parameters of a TPL task can play, for accurate timing was precluded when a variable sequence was employed."
}
@article{NORDAHLHANSEN201619,
title = "Relations between specific and global outcome measures in a social-communication intervention for children with autism spectrum disorder",
journal = "Research in Autism Spectrum Disorders",
volume = "29-30",
pages = "19 - 29",
year = "2016",
issn = "1750-9467",
doi = "https://doi.org/10.1016/j.rasd.2016.05.005",
url = "http://www.sciencedirect.com/science/article/pii/S1750946716300575",
author = "Anders Nordahl-Hansen and Sue Fletcher-Watson and Helen McConachie and Anett Kaale",
keywords = "Autism spectrum disorders, Social communication, Treatment, Intervention, Outcome measures, Brief observation of social communication change (BOSCC), Children, Joint engagement",
abstract = "Assessment of relevant outcomes is a key challenge in evaluating effects of social-communication interventions. However, few studies have investigated in what ways specific and more global measures may influence reported results of social-communication interventions for children with autism spectrum disorder (ASD). In this study both a specific and a global, more global autism symptom measure were used to assess effects of a brief social-communication intervention. Fifty-nine children (2–4 years) diagnosed with autistic disorder were assessed with the Joint Engagement (JE) states coding procedure and a preliminary version of the Brief Observation of Social Communication Change (BOSCC). A statistically significant difference was found between intervention and control groups from baseline to intervention endpoint on JE but not on BOSCC. Degree of change on the measures was moderately related, and both were independent of language level and non-verbal mental age. This study adds to the knowledge of what may be expected of different outcome measures and provides suggestions to how measures may be deployed to investigate underlying mechanisms and developmental pathways."
}
@article{JEZEK2015129,
title = "How Java APIs break – An empirical study",
journal = "Information and Software Technology",
volume = "65",
pages = "129 - 146",
year = "2015",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2015.02.014",
url = "http://www.sciencedirect.com/science/article/pii/S0950584915000506",
author = "Kamil Jezek and Jens Dietrich and Premek Brada",
keywords = "Binary compatibility, API evolution, Backward compatibility, Byte-code, Java",
abstract = "Context
It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs.
Objective
We have studied the extent of the problem in real world programs. We were interested in two aspects: the compatibility of API changes as libraries evolve, and the impact this has on programs using these libraries.
Method
This study is based on the qualitas corpus version 20120401. A data set consisting of 109 Java open-source programs and 564 program versions was used from this corpus. We have investigated two types of library dependencies: explicit dependencies to embedded libraries, and dependencies defined by symbolic references in Maven build files that are resolved at build time. We have used JaCC for API analysis, this tool is based on the popular ASM byte code analysis library.
Results
We found that for most of the programs we investigated, APIs are unstable as incompatible changes are common. Surprisingly, there are more compatibility problems in projects that use automated dependency resolution. However, we found only a few cases where this has an actual impact on other programs using such an API.
Conclusion
It is concluded that API instability is common and causes problems for programs using these APIs. Therefore, better tools and methods are needed to safeguard library evolution."
}
@article{ADEEL2017336,
title = "Random neural network based cognitive engines for adaptive modulation and coding in LTE downlink systems",
journal = "Computers & Electrical Engineering",
volume = "57",
pages = "336 - 350",
year = "2017",
issn = "0045-7906",
doi = "https://doi.org/10.1016/j.compeleceng.2016.11.005",
url = "http://www.sciencedirect.com/science/article/pii/S0045790616307066",
author = "Ahsan Adeel and Hadi Larijani and Ali Ahmadinia",
keywords = "Random neural network, Context-aware decision making, Adaptive modulation and coding, Long-term evolution, Hybrid cognitive engine, Genetic algorithm",
abstract = "This paper presents two random neural network (RNN) based context-aware decision making frameworks to improve adaptive modulation and coding (AMC) in long-term evolution (LTE) downlink systems. In the first framework, AMC is modelled as a traditional classification problem with the aim to maximize the probability of correct classification. The second framework seeks to optimize the throughput as opposed to simply maximizing the probability of the correct classification. To model the second framework, we developed a hybrid cognitive engine (CE) architecture by integrating an RNN based learning algorithm with genetic algorithm (GA) based reasoning. RNN inherent properties help CE to comply with the essential CE design requirement (i.e. concurrent long-term-learning, low computational complexity, and fast decision making). The performance of RNN is compared with artificial neural networks (ANN) and state-of-the-art effective exponential SINR mapping (EESM) algorithm. A comprehensive analysis of the proposed RNN based AMC scheme is presented by jointly incorporating the effect of different schedulers, feedback delays, and multi-antenna diversity on the throughput of an orthogonal frequency-division multiple access (OFDMA) system. The critical analysis of the first framework revealed that RNN based CE can achieve comparable results with faster adaptation, even in severe environment changes without the need of retraining compared to ANN. The analysis of the second approach demonstrated RNNs faster adaptation as compared to ANN and showed upto 253% gain in user throughput. RNN based CE efficiently exploited the channel quality information feedback delay to improve system throughput and helped cell-edge and cell-centre users to experience much better services in terms of achieved throughput as compared to EESM."
}
@article{BROWN2013S7,
title = "Designing Adult Code Simulations for Antepartum and Postpartum Nurses",
journal = "Journal of Obstetric, Gynecologic & Neonatal Nursing",
volume = "42",
pages = "S7 - S8",
year = "2013",
note = "Proceedings of the AWHONN 2013 Convention",
issn = "0884-2175",
doi = "https://doi.org/10.1111/1552-6909.12056",
url = "http://www.sciencedirect.com/science/article/pii/S0884217515314805",
author = "Judy P. Brown and Claire Zaya",
keywords = "simulation, maternal code, antepartum, postpartum",
abstract = "Poster Presentation
Purpose for the Program
To improve the ability of nurses in the postpartum and antepartum units to respond in the event of a maternal code. Even very experienced nurses expressed that though they knew how to perform cardiopulmonary resuscitation, the maternal code experience was overwhelming and they felt unprepared.
Proposed Change
The leadership of the antepartum and postpartum units and the nursing simulation faculty at the hospital collaborated to propose a simulation‐based learning experience for the registered nurse (RN) staff of these units. Simulation‐based learning provides an opportunity to practice skills in a safe environment while helping the nurse develop critical thinking skills, promote effective communication, and work collaboratively with other members of the team.
Implementation, Outcomes, and Evaluation
Over a period of months, the program faculty (simulation staff, nurse educators, and staff nurses) created two maternal code simulations. The final program involved three stages: skills sessions, simulated code scenarios, and debriefings. During the skill sessions the staff reviewed skills, such as use of the defibrillator, contents of the code cart, medications used in a code, and communication techniques. The participants toured the simulation lab before the actual simulation to see how the room was set up and how the manikin worked. When it was time for each scenario, participants were given role cards to remind them of critical activities during a code. The primary nurse received a report on the mock patient and the simulation started. One half of the group participated in each simulation whereas the other half watched the scenario on a live feed in a conference room. Immediately after each scenario a debriefing session took place. Consistent themes during debriefing included discussions about the role of the nurse in a code situation, effective communication in an emergency, and the value of effective team work. Without exception, results of the 196 written evaluations indicated that staff nurses felt more knowledgeable and confident about adult codes after the simulation.
Implications for Nursing Practice
Based on the overwhelming positive feedback of simulation‐based learning, the program was offered again the following year to all RN staff. Anecdotally, many nurses in the second year of the simulation program commented that they also felt more confident in other emergencies in the units after having participated in the maternal code simulation the previous year. The current plan is to offer simulation‐based learning experiences to all RNs in the antepartum and postpartum units annually."
}
@article{TOUTAOUI20121,
title = "Effects of physics change in Monte Carlo code on electron pencil beam dose distributions",
journal = "Radiation Physics and Chemistry",
volume = "81",
number = "1",
pages = "1 - 8",
year = "2012",
issn = "0969-806X",
doi = "https://doi.org/10.1016/j.radphyschem.2011.08.009",
url = "http://www.sciencedirect.com/science/article/pii/S0969806X11002830",
author = "Abdelkader Toutaoui and Nadia Khelassi-Toutaoui and Zakia Brahimi and Ahmed Chafik Chami",
keywords = "Pencil beam kernels, Monte Carlo, Electron beams",
abstract = "Pencil beam algorithms used in computerized electron beam dose planning are usually described using the small angle multiple scattering theory. Alternatively, the pencil beams can be generated by Monte Carlo simulation of electron transport. In a previous work, the 4th version of the Electron Gamma Shower (EGS) Monte Carlo code was used to obtain dose distributions from monoenergetic electron pencil beam, with incident energy between 1MeV and 50MeV, interacting at the surface of a large cylindrical homogeneous water phantom. In 2000, a new version of this Monte Carlo code has been made available by the National Research Council of Canada (NRC), which includes various improvements in its electron-transport algorithms. In the present work, we were interested to see if the new physics in this version produces pencil beam dose distributions very different from those calculated with oldest one. The purpose of this study is to quantify as well as to understand these differences. We have compared a series of pencil beam dose distributions scored in cylindrical geometry, for electron energies between 1MeV and 50MeV calculated with two versions of the Electron Gamma Shower Monte Carlo Code. Data calculated and compared include isodose distributions, radial dose distributions and fractions of energy deposition. Our results for radial dose distributions show agreement within 10% between doses calculated by the two codes for voxels closer to the pencil beam central axis, while the differences are up to 30% for longer distances. For fractions of energy deposition, the results of the EGS4 are in good agreement (within 2%) with those calculated by EGSnrc at shallow depths for all energies, whereas a slightly worse agreement (15%) is observed at deeper distances. These differences may be mainly attributed to the different multiple scattering for electron transport adopted in these two codes and the inclusion of spin effect, which produces an increase of the effective range of electrons."
}
@article{HADJIDOUKAS20142217,
title = "NDL-v2.0: A new version of the numerical differentiation library for parallel architectures",
journal = "Computer Physics Communications",
volume = "185",
number = "7",
pages = "2217 - 2219",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2014.04.002",
url = "http://www.sciencedirect.com/science/article/pii/S0010465514001258",
author = "P.E. Hadjidoukas and P. Angelikopoulos and C. Voglis and D.G. Papageorgiou and I.E. Lagaris",
keywords = "Numerical differentiation, Finite differences, Optimization, Nonlinear equations, OpenMP, MPI, Python, Parallel processing",
abstract = "We present a new version of the numerical differentiation library (NDL) used for the numerical estimation of first and second order partial derivatives of a function by finite differencing. In this version we have restructured the serial implementation of the code so as to achieve optimal task-based parallelization. The pure shared-memory parallelization of the library has been based on the lightweight OpenMP tasking model allowing for the full extraction of the available parallelism and efficient scheduling of multiple concurrent library calls. On multicore clusters, parallelism is exploited by means of TORC, an MPI-based multi-threaded tasking library. The new MPI implementation of NDL provides optimal performance in terms of function calls and, furthermore, supports asynchronous execution of multiple library calls within legacy MPI programs. In addition, a Python interface has been implemented for all cases, exporting the functionality of our library to sequential Python codes.
New version program summary
Program title: NDL-v2.0 Catalog identifier: AEDG_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDG_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 63036 No. of bytes in distributed program, including test data, etc.: 801872 Distribution format: tar.gz Programming language: ANSI Fortran-77, ANSI C, Python. Computer: Distributed systems (clusters), shared memory systems. Operating system: Linux, Unix. Has the code been vectorized or parallelized?: Yes. RAM: The library uses O(N) internal storage, N being the dimension of the problem. It can use up to O(N2) internal storage for Hessian calculations, if a task throttling factor has not been set by the user. Classification: 4.9, 4.14, 6.5. Catalog identifier of previous version: AEDG_v1_0 Journal reference of previous version: Comput. Phys. Comm. 180(2009)1404 Does the new version supersede the previous version?: Yes Nature of problem: The numerical estimation of derivatives at several accuracy levels is a common requirement in many computational tasks, such as optimization, solution of nonlinear systems, and sensitivity analysis. For a large number of scientific and engineering applications, the underlying functions correspond to simulation codes for which analytical estimation of derivatives is difficult or almost impossible. A parallel implementation that exploits systems with multiple CPUs is very important for large scale and computationally expensive problems. Solution method: Finite differencing is used with a carefully chosen step that minimizes the sum of the truncation and round-off errors. The parallel versions employ both OpenMP and MPI libraries. Reasons for new version: The updated version was motivated by our endeavors to extend a parallel Bayesian uncertainty quantification framework [1], by incorporating higher order derivative information as in most state-of-the-art stochastic simulation methods such as Stochastic Newton MCMC [2] and Riemannian Manifold Hamiltonian MC [3]. The function evaluations are simulations with significant time-to-solution, which also varies with the input parameters such as in [1, 4]. The runtime of the N-body-type of problem changes considerably with the introduction of a longer cut-off between the bodies. In the first version of the library, the OpenMP-parallel subroutines spawn a new team of threads and distribute the function evaluations with a PARALLEL DO directive. This limits the functionality of the library as multiple concurrent calls require nested parallelism support from the OpenMP environment. Therefore, either their function evaluations will be serialized or processor oversubscription is likely to occur due to the increased number of OpenMP threads. In addition, the Hessian calculations include two explicit parallel regions that compute first the diagonal and then the off-diagonal elements of the array. Due to the barrier between the two regions, the parallelism of the calculations is not fully exploited. These issues have been addressed in the new version by first restructuring the serial code and then running the function evaluations in parallel using OpenMP tasks. Although the MPI-parallel implementation of the first version is capable of fully exploiting the task parallelism of the PNDL routines, it does not utilize the caching mechanism of the serial code and, therefore, performs some redundant function evaluations in the Hessian and Jacobian calculations. This can lead to: (a) higher execution times if the number of available processors is lower than the total number of tasks, and (b) significant energy consumption due to wasted processor cycles. Overcoming these drawbacks, which become critical as the time of a single function evaluation increases, was the primary goal of this new version. Due to the code restructure, the MPI-parallel implementation (and the OpenMP-parallel in accordance) avoids redundant calls, providing optimal performance in terms of the number of function evaluations. Another limitation of the library was that the library subroutines were collective and synchronous calls. In the new version, each MPI process can issue any number of subroutines for asynchronous execution. We introduce two library calls that provide global and local task synchronizations, similarly to the BARRIER and TASKWAIT directives of OpenMP. The new MPI-implementation is based on TORC, a new tasking library for multicore clusters [5–7]. TORC improves the portability of the software, as it relies exclusively on the POSIX-Threads and MPI programming interfaces. It allows MPI processes to utilize multiple worker threads, offering a hybrid programming and execution environment similar to MPI+OpenMP, in a completely transparent way. Finally, to further improve the usability of our software, a Python interface has been implemented on top of both the OpenMP and MPI versions of the library. This allows sequential Python codes to exploit shared and distributed memory systems. Summary of revisions: The revised code improves the performance of both parallel (OpenMP and MPI) implementations. The functionality and the user-interface of the MPI-parallel version have been extended to support the asynchronous execution of multiple PNDL calls, issued by one or multiple MPI processes. A new underlying tasking library increases portability and allows MPI processes to have multiple worker threads. For both implementations, an interface to the Python programming language has been added. Restrictions: The library uses only double precision arithmetic. The MPI implementation assumes the homogeneity of the execution environment provided by the operating system. Specifically, the processes of a single MPI application must have identical address space and a user function resides at the same virtual address. In addition, address space layout randomization should not be used for the application. Unusual features: The software takes into account bound constraints, in the sense that only feasible points are used to evaluate the derivatives, and given the level of the desired accuracy, the proper formula is automatically employed. Running time: Running time depends on the function’s complexity. The test run took 23 ms for the serial distribution, 25 ms for the OpenMP with 2 threads, 53 ms and 1.01 s for the MPI parallel distribution using 2 threads and 2 processes respectively and yield-time for idle workers equal to 10 ms. References: [1] P. Angelikopoulos, C. Paradimitriou, P. Koumoutsakos, Bayesian uncertainty quantification and propagation in molecular dynamics simulations: a high performance computing framework, J. Chem. Phys 137 (14). [2] H.P. Flath, L.C. Wilcox, V. Akcelik, J. Hill, B. van Bloemen Waanders, O. Ghattas, Fast algorithms for Bayesian uncertainty quantification in large-scale linear inverse problems based on low-rank partial Hessian approximations, SIAM J. Sci. Comput. 33 (1) (2011) 407–432. [3] M. Girolami, B. Calderhead, Riemann manifold Langevin and Hamiltonian Monte Carlo methods, J. R. Stat. Soc. Ser. B (Stat. Methodol.) 73 (2) (2011) 123–214. [4] P. Angelikopoulos, C. Paradimitriou, P. Koumoutsakos, Data driven, predictive molecular dynamics for nanoscale flow simulations under uncertainty, J. Phys. Chem. B 117 (47) (2013) 14808–14816. [5] P.E. Hadjidoukas, E. Lappas, V.V. Dimakopoulos, A runtime library for platform-independent task parallelism, in: PDP, IEEE, 2012, pp. 229–236. [6] C. Voglis, P.E. Hadjidoukas, D.G. Papageorgiou, I. Lagaris, A parallel hybrid optimization algorithm for fitting interatomic potentials, Appl. Soft Comput. 13 (12) (2013) 4481–4492. [7] P.E. Hadjidoukas, C. Voglis, V.V. Dimakopoulos, I. Lagaris, D.G. Papageorgiou, Supporting adaptive and irregular parallelism for non-linear numerical optimization, Appl. Math. Comput. 231 (2014) 544–559."
}
@article{CEVIDANES2015S195,
title = "Incorporating 3-dimensional models in online articles",
journal = "American Journal of Orthodontics and Dentofacial Orthopedics",
volume = "147",
number = "5, Supplement ",
pages = "S195 - S204",
year = "2015",
note = "2015 Centennial Supplement",
issn = "0889-5406",
doi = "https://doi.org/10.1016/j.ajodo.2015.02.002",
url = "http://www.sciencedirect.com/science/article/pii/S0889540615000992",
author = "Lucia H.S. Cevidanes and Antonio C.O. Ruellas and Julien Jomier and Tung Nguyen and Steve Pieper and Francois Budin and Martin Styner and Beatriz Paniagua",
abstract = "Introduction
The aims of this article are to introduce the capability to view and interact with 3-dimensional (3D) surface models in online publications, and to describe how to prepare surface models for such online 3D visualizations.
Methods
Three-dimensional image analysis methods include image acquisition, construction of surface models, registration in a common coordinate system, visualization of overlays, and quantification of changes. Cone-beam computed tomography scans were acquired as volumetric images that can be visualized as 3D projected images or used to construct polygonal meshes or surfaces of specific anatomic structures of interest. The anatomic structures of interest in the scans can be labeled with color (3D volumetric label maps), and then the scans are registered in a common coordinate system using a target region as the reference. The registered 3D volumetric label maps can be saved in .obj, .ply, .stl, or .vtk file formats and used for overlays, quantification of differences in each of the 3 planes of space, or color-coded graphic displays of 3D surface distances.
Results
All registered 3D surface models in this study were saved in .vtk file format and loaded in the Elsevier 3D viewer. In this study, we describe possible ways to visualize the surface models constructed from cone-beam computed tomography images using 2D and 3D figures. The 3D surface models are available in the article's online version for viewing and downloading using the reader's software of choice. These 3D graphic displays are represented in the print version as 2D snapshots. Overlays and color-coded distance maps can be displayed using the reader's software of choice, allowing graphic assessment of the location and direction of changes or morphologic differences relative to the structure of reference. The interpretation of 3D overlays and quantitative color-coded maps requires basic knowledge of 3D image analysis.
Conclusions
When submitting manuscripts, authors can now upload 3D models that will allow readers to interact with or download them. Such interaction with 3D models in online articles now will give readers and authors better understanding and visualization of the results."
}
@article{YUAN2018116,
title = "Mitochondrial phylogeny, divergence history and high-altitude adaptation of grassland caterpillars (Lepidoptera: Lymantriinae: Gynaephora) inhabiting the Tibetan Plateau",
journal = "Molecular Phylogenetics and Evolution",
volume = "122",
pages = "116 - 124",
year = "2018",
issn = "1055-7903",
doi = "https://doi.org/10.1016/j.ympev.2018.01.016",
url = "http://www.sciencedirect.com/science/article/pii/S1055790317307078",
author = "Ming-Long Yuan and Qi-Lin Zhang and Li Zhang and Cheng-Lin Jia and Xiao-Peng Li and Xing-Zhuo Yang and Run-Qiu Feng",
keywords = "Mitochondrial genomics, Erebidae, Divergence time, Accelerated evolution, High-altitude adaptation, Positive selection",
abstract = "Grassland caterpillars (Lepidoptera: Lymantriinae: Gynaephora) are the most important pests in alpine meadows of the Tibetan Plateau (TP) and have well adapted to high-altitude environments. To further understand the evolutionary history and their adaptation to the TP, we newly determined seven complete TP Gynaephora mitogenomes. Compared to single genes, whole mitogenomes provided the best phylogenetic signals and obtained robust results, supporting the monophyly of the TP Gynaephora species and a phylogeny of Arctiinae + (Aganainae + Lymantriinae). Incongruent phylogenetic signals were found among single mitochondrial genes, none of which recovered the same phylogeny as the whole mitogenome. We identified six best-performing single genes using Shimodaira-Hasegawa tests and found that the combinations of rrnS and either cox1 or cox3 generated the same phylogeny as the whole mitogenome, indicating the phylogenetic potential of these three genes for future evolutionary studies of Gynaephora. The TP Gynaephora species were estimated to radiate on the TP during the Pliocene and Quaternary, supporting an association of the diversification and speciation of the TP Gynaephora species with the TP uplifts and associated climate changes during this time. Selection analyses revealed accelerated evolutionary rates of the mitochondrial protein-coding genes in the TP Gynaephora species, suggesting that they accumulated more nonsynonymous substitutions that may benefit their adaptation to high altitudes. Furthermore, signals of positive selection were detected in nad5 of two Gynaephora species with the highest altitude-distributions, indicating that this gene may contribute to Gynaephora’s adaptation to divergent altitudes. This study adds to the understanding of the TP Gynaephora evolutionary relationships and suggests a link between mitogenome evolution and ecological adaptation to high-altitude environments in grassland caterpillars."
}
@article{VARACALLO2017794,
title = "Improving Orthopedic Resident Knowledge of Documentation, Coding, and Medicare Fraud",
journal = "Journal of Surgical Education",
volume = "74",
number = "5",
pages = "794 - 798",
year = "2017",
issn = "1931-7204",
doi = "https://doi.org/10.1016/j.jsurg.2017.02.003",
url = "http://www.sciencedirect.com/science/article/pii/S1931720417300703",
author = "Matthew A. Varacallo and Michael Wolf and Martin J. Herman",
keywords = "resident documentation, resident coding, core competencies, Medicare fraud, orthopedic residency, residency curriculum, Patient Care, Professionalism, Practice-Based Learning and Improvement",
abstract = "Background
Most residency programs still lack formal education and training on the basic clinical documentation and coding principles. Today’s physicians are continuously being held to increasing standards for correct coding and documentation, yet little has changed in the residency training curricula to keep pace with these increasing standards. Although there are many barriers to implementing these topics formally, the main concern has been the lack of time and resources. Thus, simple models may have the best chance for success at widespread implementation.
Purpose
The first goal of the study was to assess a group of orthopedic residents’ fund of knowledge regarding basic clinical documentation guidelines, coding principles, and their ability to appropriately identify cases of Medicare fraud. The second goal was to analyze a single, high-yield educational session’s effect on overall resident knowledge acquisition and awareness of these concepts.
Subject Selection and Study Protocol
Orthopedic residents belonging to 1 of 2 separate residency programs voluntarily and anonymously participated. All were asked to complete a baseline assessment examination, followed by attending a 45-minute lecture given by the same orthopedic faculty member who remained blinded to the test questions. Each resident then completed a postsession examination. Each resident was also asked to self-rate his or her documentation and coding level of comfort on a Likert scale (1-5). Statistical significance was set at p < 0.05.
Main Findings
A total of 32 orthopedic residents were participated. Increasing postgraduate year-level of training correlated with higher Likert-scale ratings for self-perceived comfort levels with documentation and coding. However, the baseline examination scores were no different between senior and junior residents (p > 0.20). The high-yield teaching session significantly improved the average total examination scores at both sites (p < 0.01), with overall improvement being similar between the 2 groups (p > 0.10).
Principal Conclusions
The current healthcare environment necessitates better physician awareness regarding clinical documentation guidelines and coding principles. Very few adjustments to incorporate these teachings have been made to most residency training curricula, and the lack of time and resources remains the concern of many surgical programs. We have demonstrated that orthopedic resident knowledge in these important areas drastically improves after a single, high-yield 45-minute teaching session."
}
@article{DSOUZA20131929,
title = "Resilient Dynamic Data Driven Application Systems (rDDDAS)",
journal = "Procedia Computer Science",
volume = "18",
pages = "1929 - 1938",
year = "2013",
note = "2013 International Conference on Computational Science",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2013.05.362",
url = "http://www.sciencedirect.com/science/article/pii/S187705091300505X",
author = "Glynis Dsouza and Salim Hariri and Youssif Al-Nashif and Gabriel Rodriguez",
keywords = "DDDAS, Moving Target Defense, Software Behavior Encryption, Cloud Computing, Resilience Applications",
abstract = "There is a growing interest in Cloud Computing for delivering computing as a utility. Security in Cloud Computing is a challenging research problem because it involves many interdependent tasks including vulnerability scanning, application layer firewalls, configuration management, alert monitoring and analysis, source code analysis, and user identity management. It is widely accepted that we cannot build software and computing systems that are free from vulnerabilities and cannot be penetrated or attacked. Consequently, there is a strong interest in resilience approach because of its potential to address the cybersecurity challenges. Our is based on using the Dynamic Data Driven Application System (DDDAS) and Moving Target Defence (MTD) strategies to develop resilient DDDAS. The Resilient Applications utilize the following capabilities: Software Behaviour Encryption (SBE), Replication, Diversity, Automated Checkpointing and Recovery. Software Behaviour Encryption employs spatiotemporal behaviour encryption and a moving target defence to make active software components change their implementations and their resources randomly and consequently evade attackers. Diversity and random execution is achieved by “hot” shuffling multiple functionally- equivalent, behaviourally-different software versions at runtime (This encryption of the execution environment will make it extremely difficult for an attack to disrupt the normal operations of a cloud application. Also, the dynamic change in the execution environment will hide the software flaws that would otherwise be exploited by a cyberattacker. Checkpointing is used to save the current state of the task to a reliable storage and thus enabling rollback recovery if it is required to tolerate cyberattacks and mitigate their impacts. We use the Compiler for Portable Checkpointing (CPPC), a tool for automatically inserting portable checkpoints into the code. We also evaluate the performance and overhead of running three applications in our rDDDAS environment. Our experimental results show that the rDDDAS environment can be used to develop resilient cloud applications are resilient against attacks with around 7% in execution time overhead."
}
@article{OPLETAL20131946,
title = "HRMC_2.0: Hybrid Reverse Monte Carlo method with silicon, carbon and germanium potentials",
journal = "Computer Physics Communications",
volume = "184",
number = "8",
pages = "1946 - 1957",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.03.004",
url = "http://www.sciencedirect.com/science/article/pii/S001046551300101X",
author = "G. Opletal and T.C. Petersen and I.K. Snook and S.P. Russo",
keywords = "RMC, HRMC, Amorphous, Carbon, Silicon, Germanium, Metropolis",
abstract = "The Hybrid Reverse Monte Carlo (HRMC) code models the atomic structure of materials via the use of a combination of constraints including experimental diffraction data and an empirical energy potential. In this version update, germanium potential parameters are introduced and constraints based on the coordination, average coordination and the total bond angle distribution are implemented. Other additional changes include a constraint on three member ring formation, a constraint on porosity and an extension to handle systems with up to three different elements.
Program Summary
Program title: HRMC version 2.0 Catalogue identifier: AEAO_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEAO_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 94759 No. of bytes in distributed program, including test data, etc.: 751023 Distribution format: tar.gz Programming language: Fortran 90. Computer: Any computer capable of running executables produced by the Fortran 90 compiler. For example, the code runs in Windows 7, once compiled with the GNU Fortan 95 compiler. Operating system: Unix, Windows. RAM: Depends on the type of empirical potential used, number of atoms and which constraints are employed. Typically below 2 GB for a system with a few thousand atoms. Classification: 7.7. Does the new version supersede the previous version?: Yes Catalogue identifier of previous version: AEAO_v1_1 Journal reference of previous version: Comput. Phys. Comm. 182 (2011) 542 Nature of problem: Atomic modeling using a combination of empirical potentials, fits to experimental data and other chemically or physically motivated constraints. Solution method: Single move Metropolis Monte Carlo method used to minimize total energy and discrepancy between simulation and experimental data. Reasons for new version: Extension of capabilities from old version. Summary of revisions: Inclusion of Stillinger–Weber parameters for germanium, inclusion of a bond angle distribution constraint, inclusion of more general coordination, average coordination and porosity/volume constraints. Variable step sizes are now supported. Extension to systems containing up to three elements. New quench schemes to control constraint weighting throughout the simulation have been included. Constraints to forbid three member ring formation have been developed. There are changes to the input/output structure. Running time: 1000 s for a test run on a Intel Xeon 2.93 GHz—Nehalem series processor."
}
@article{SILVA2018,
title = "Reciprocal White Matter Changes Associated With Copy Number Variation at 15q11.2 BP1-BP2: A Diffusion Tensor Imaging Study",
journal = "Biological Psychiatry",
year = "2018",
issn = "0006-3223",
doi = "https://doi.org/10.1016/j.biopsych.2018.11.004",
url = "http://www.sciencedirect.com/science/article/pii/S0006322318320092",
author = "Ana I. Silva and Magnus O. Ulfarsson and Hreinn Stefansson and Omar Gustafsson and G. Bragi Walters and David E.J. Linden and Lawrence S. Wilkinson and Mark Drakesmith and Michael J. Owen and Jeremy Hall and Kari Stefansson",
keywords = "15q11.2 BP1-BP2, Copy number variant, CYFIP1, Diffusion tensor imaging, Fragile X syndrome, Genetics",
abstract = "Background
The 15q11.2 BP1-BP2 cytogenetic region has been associated with learning and motor delays, autism, and schizophrenia. This region includes a gene that codes for the cytoplasmic FMR1 interacting protein 1 (CYFIP1). The CYFIP1 protein is involved in actin cytoskeletal dynamics and interacts with the fragile X mental retardation protein. Absence of fragile X mental retardation protein causes fragile X syndrome. Because abnormal white matter microstructure has been reported in both fragile X syndrome and psychiatric disorders, we looked at the impact of 15q11.2 BP1-BP2 dosage on white matter microstructure.
Methods
Combining a brain-wide voxel-based approach and a regional-based analysis, we analyzed diffusion tensor imaging data from healthy individuals with the deletion (n = 30), healthy individuals with the reciprocal duplication (n = 27), and IQ-matched control subjects with no large copy number variants (n = 19), recruited from a large genotyped population sample.
Results
We found global mirror effects (deletion > control > duplication) on fractional anisotropy. The deletion group showed widespread increased fractional anisotropy when compared with duplication. Regional analyses revealed a greater effect size in the posterior limb of the internal capsule and a tendency for decreased fractional anisotropy in duplication.
Conclusions
These results show a reciprocal effect of 15q11.2 BP1-BP2 on white matter microstructure, suggesting that reciprocal chromosomal imbalances may lead to opposite changes in brain structure. Findings in the deletion overlap with previous white matter differences reported in fragile X syndrome patients, suggesting common pathogenic mechanisms derived from disruptions of cytoplasmic CYFIP1-fragile X mental retardation protein complexes. Our data begin to identify specific components of the 15q11.2 BP1-BP2 phenotype and neurobiological mechanisms of potential relevance to the increased risk for disorder."
}
@article{ZHOU20171100,
title = "Numerical simulation of metal jet breakup, cooling and solidification in water",
journal = "International Journal of Heat and Mass Transfer",
volume = "109",
pages = "1100 - 1109",
year = "2017",
issn = "0017-9310",
doi = "https://doi.org/10.1016/j.ijheatmasstransfer.2017.02.083",
url = "http://www.sciencedirect.com/science/article/pii/S0017931016329933",
author = "Yuan Zhou and Jingtan Chen and Mingjun Zhong and Junfeng Wang and Meng Lv",
keywords = "Molten jet, Numerical simulation, FCI, Solidification, Breakup",
abstract = "During transient intrusion of molten metal into water, metal go through cooling, breakup before fully solidified. This paper describes a numerical code which combines cooling, solidification and breakup in a single computation. In the code free surface of jet is tracked by Volume of Fluid Method (VOF), both the heat transfer and viscosity variation during liquid-solid phase change are taken into account. The simulation results of melt jet pattern, front position history, jet breakup length and breakup time are in good agreement with the experimental results. The effects of interfacial temperature and jet velocity are also determined. The molten jet thermal history and solidification, droplet generation rate at different penetration times, which are difficult to observe in experiment, are presented to gain an insight into this complicated process. Solidified metal proportion increases with jet penetration depth. Melt jet breakup with surface solidification can be divided into three zones in space: (1) liquid core, (2) solidifying zone, (3) solid droplets. These simulation data are helpful to substantiate the understanding of the phenomena during molten melt jet interactions with water."
}
@article{SENGUPTAIRVING2016210,
title = "Doing things: Organizing for agency in mathematical learning",
journal = "The Journal of Mathematical Behavior",
volume = "41",
pages = "210 - 218",
year = "2016",
issn = "0732-3123",
doi = "https://doi.org/10.1016/j.jmathb.2015.10.001",
url = "http://www.sciencedirect.com/science/article/pii/S0732312315300018",
author = "Tesha Sengupta-Irving",
keywords = "Agency, Curriculum, Learning",
abstract = "In the United States, school mathematics generally fails to help students see themselves as capable of impacting their world – a perspective Freire argues defines human agency. This analysis draws from a five-week Algebra intervention for middle school students (n=46) designed to promote agency through collaborative mathematical activity. Typically, students identified as underperforming (as most in this intervention were), teachers revert to procedural, low-level instruction. In contrast, this intervention was designed around tasks of high cognitive demand that required visual or symbolic representation of algebraic concepts. Qualitative coding of student interviews (n=46) confirm the design principles of authority, agency and collaboration were positively impactful for students. In particular, interviews evidence a changing perspective from math as boring to the possibility of math as comingling intellectual challenge and personal enjoyment. These results are traced to the design principles and in particular, the focus on organizing for agency."
}
@article{CHETAN201513,
title = "An efficient and secure robust watermarking scheme for document images using Integer wavelets and block coding of binary watermarks",
journal = "Journal of Information Security and Applications",
volume = "24-25",
pages = "13 - 24",
year = "2015",
issn = "2214-2126",
doi = "https://doi.org/10.1016/j.jisa.2015.07.002",
url = "http://www.sciencedirect.com/science/article/pii/S2214212615000423",
author = "K.R. Chetan and S. Nirmala",
keywords = "Document image, Robust watermarking, Integer wavelets, Quantization based embedding, Watermark extraction, Binary block coding",
abstract = "A novel, efficient and robust watermarking scheme for protection of document image contents is proposed in this work. An integer wavelet-based watermarking scheme for embedding the compressed version of the binary watermark logo has been developed for robust watermarking. At the sender side, the source document image is divided into empty and non-empty segments depending on the absence or presence of the information. Watermarking is applied for non-empty segments and thus the amount of embedding capacity is reduced. A binary watermark logo is compressed using binary block coding technique of appropriate block-size. A level-2 integer wavelet transformation is applied on the non-empty segment of the source document image. LL-sub-band of level-2 of the transformed image is subdivided into blocks of uniform size and compressed watermark bitstream is embedded into it. The compressed watermark is redundantly embedded into blocks using quantization technique. Thus, multiple copies of compressed watermark are available and each block of the source document image segment need not include the entire compressed watermark stream. At the receiver side, the extracted segments from each set of blocks are merged to obtain a single extracted bitstream. The bitstream is further decoded to get the binary watermark. The extracted and embedded watermarks are compared and authentication decision is taken based on majority voting technique. Based on the quantization step size, size of the logo and the level of wavelet transform, the watermarks are extracted without accessing the original image. The experimental results show that the proposed technique is highly robust. The performance of the proposed approach is measured in parameters Peak Signal to Noise Ratio (PSNR) and Normalized Correlation Coefficient (NCC). Results show that the proposed approach is better than the existing methods. In the proposed scheme for decompression of watermark, the level of block coding technique is the key, which provides an additional layer of security."
}
@article{TANG201469,
title = "Numerical modelling of the tailored tempering process applied to 22MnB5 sheets",
journal = "Finite Elements in Analysis and Design",
volume = "81",
pages = "69 - 81",
year = "2014",
issn = "0168-874X",
doi = "https://doi.org/10.1016/j.finel.2013.11.009",
url = "http://www.sciencedirect.com/science/article/pii/S0168874X13002011",
author = "B.T. Tang and S. Bruschi and A. Ghiotti and P.F. Bariani",
keywords = "Tailored tempering, Boron steel, Phase transformation kinetics",
abstract = "In order to enhance the crash characteristics and geometrical accuracy, components hot formed in a fully martensitic state have gained in the last few years more and more importance. However, the very high strength exhibited by these components makes subsequent operations such as cutting difficult due to the high process forces and associated high wear of the cutting tools. Moreover, for some applications, such as B-pillars and other automotive components that may undergo impact loading, it may be desirable to create regions of the part with softer and more ductile microstructures. The novel process called the tailored tempering process allows doing this by suppressing the martensitic transformation in those zones of the sheet located under heated parts of the tools. In the paper, a numerical model of the tailored tempering process was developed, accurately calibrated and validated through a laboratory-scale hot forming process. Using the commercial FE code Forge™ a fully coupled thermo-mechanical-metallurgical model of the process was set up. The influence of the phase transformation kinetics was taken into account by implementing in the model phase transformation data, namely the shift of the TTT curves due to the applied stress and the transformation plasticity coefficients, gained from an extensive dilatometric experimental campaign and analysis. A laboratory-scale hot-formed U-channel was produced using segmented tools with heated and cooled zones so that the cooling rate of the blank can be locally controlled during the hot forming process. The part Vickers hardness distribution and microstructural evolution predicted by FORGE™ were then compared with the experimental results, proving the validation of the numerical model by taking into account the influence of the transformation plasticity and deformation history on the phase transformation kinetics."
}
@article{SINGH201622,
title = "Direction-adaptive fixed length discrete cosine transform framework for efficient H.264/AVC video coding",
journal = "Signal Processing: Image Communication",
volume = "48",
pages = "22 - 37",
year = "2016",
issn = "0923-5965",
doi = "https://doi.org/10.1016/j.image.2016.09.002",
url = "http://www.sciencedirect.com/science/article/pii/S0923596516301187",
author = "Deepak Singh and Sukadev Meher",
keywords = "Discrete cosine transform, Direction adaptive transform, Directional transform framework, Video coding, Video compression",
abstract = "The 2D-discrete cosine transform (2D-DCT) is one of the popular transformation for video coding. Yet, 2D-DCT may not be able to efficiently represent video data with fewer coefficients for oblique featured blocks. To further improve the compression gain for such oblique featured video data, this paper presents a directional transform framework based on direction-adaptive fixed length discrete cosine transform (DAFL-DCT) for intra-, and inter-frame. The proposed framework selects the best suitable transform mode from eight proposed directional transform modes for each block, and modified zigzag scanning pattern rearranges these transformed coefficients into a 1D-array, suitable for entropy encoding. The proposed scheme is analysed on JM 18.6 of H.264/AVC platform. Performance comparisons have been made with respect to rate-distortion (RD), Bjontegaard metrics, encoding time etc. The proposed transform scheme outperforms the conventional 2D-DCT and other state-of-art techniques in terms of compression gain and subjective quality."
}
@article{DAMODAR20181017,
title = "A higher altitude is an independent risk factor for venous thromboembolisms following total shoulder arthroplasty",
journal = "Journal of Orthopaedics",
volume = "15",
number = "4",
pages = "1017 - 1021",
year = "2018",
issn = "0972-978X",
doi = "https://doi.org/10.1016/j.jor.2018.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S0972978X18303465",
author = "Dhanur Damodar and Rushabh Vakharia and Ajit Vakharia and Jon Sheu and Chester J. Donnally and Jonathan C. Levy and Lee Kaplan and Julianne Munoz",
keywords = "altitude, Elevation, Deep vein thrombosis, Pulmonary embolism, Total shoulder arthroplasty",
abstract = "Introduction
High altitudes lead to physiological changes that may predispose to venous thromboembolisms (VTE) including deep vein thrombosis (DVT) and pulmonary embolism (PE). No prior study has evaluated if there is also a higher risk of VTEs after total shoulder arthroplasties (TSAs) performed at higher elevations compared to lower elevations. The purpose of this study was to identify if undergoing TSA at a higher altitude center (>4000 feet above sea level) is an independent risk factor for a postoperative VTE.
Methods
A retrospective review was performed from 2005 to 2014 using the Medicare Standard Analytical Files of the Pearl Diver database (Pearl Diver Technologies, West Conshohocken, PA, USA). The inclusion criteria for the study group consisted of all patients in the database undergoing primary TSAs at an altitude above 4000 feet. Patients were queried using the International Classification of Disease 9th revision codes (ICD-9). All patients undergoing primary TSA were queried using ICD-9 procedure code 81.80. Patients were filtered using the zip codes of the hospitals where the procedure occurred and were separated into high (>4,000 ft) and low (<100 ft) altitudes. Patients undergoing TSA in altitudes <100 ft represented the control group. Patients with a history of VTE, DVT, PE, and coagulation disorders were excluded from the study. Patients in the study group were randomly matched 1:1 according to age, gender, and comorbidities. Two mutually exclusive cohorts were formed and rates of VTE, DVT, and PE were analyzed and compared. Statistical analysis was performed using the programming language R (University of Auckland, New Zealand). An alpha value less than 0.05 was considered statistically significant.
Results
In the first 30 postoperative days, patients undergoing TSA at a higher altitude experienced a significantly higher rate of PEs (odds ratio [OR], 39.5; P = <0.001) when compared to similar patients at lower altitudes. This trend was also present for PE (OR, 2.02; P < 0.03) at 90 days postoperatively.
Conclusion
TSAs performed at higher altitudes (>4000 feet) have a higher rate of acute postoperative PEs in the first 30 days and 90 days postoperatively when compared to matched patients receiving the same surgery at a lower altitude (<100 feet). TSA patients at high altitude should be counseled on these increased risks."
}
@article{SOUTO2018733,
title = "Time-space efficient regression testing for configurable systems",
journal = "Journal of Systems and Software",
volume = "137",
pages = "733 - 746",
year = "2018",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2017.08.010",
url = "http://www.sciencedirect.com/science/article/pii/S0164121217301656",
author = "Sabrina Souto and Marcelo d’Amorim",
keywords = "Regression testing, Configurable systems",
abstract = "Configurable systems are those that can be adapted from a set of input options, reflected in code in form of variations. Testing these systems is challenging because of the vast array of configuration possibilities where bugs can hide. In the context of evolution, testing becomes even more challenging — not only code but also the set of plausible configurations can change across versions. This paper proposes EvoSPLat, a regression testing technique for configurable systems that explores all dynamically reachable configurations from a test. EvoSPLat supports two important application scenarios of regression testing. In the RCS scenario EvoSPLat prunes configurations (not tests) that are not impacted by changes. In the RTS scenario EvoSPLat prunes tests (not configurations) which are not impacted by changes. To evaluate EvoSPLat under the RCS scenario we used a selection of configurable Java programs. Results indicate that EvoSPLat reduced time by  ∼22% and reduced the number of configurations tested by  ∼45%. To evaluate EvoSPLat under the RTS scenario we used GCC. Results indicate that EvoSPLat reduced time to run tests by  ∼35%. Overall, results suggest that EvoSPLat is a promising technique to test configurable systems in the prevalent scenario of evolution."
}
@article{CONROY201733,
title = "Using Behavior Change Techniques to Guide Selections of Mobile Applications to Promote Fluid Consumption",
journal = "Urology",
volume = "99",
pages = "33 - 37",
year = "2017",
issn = "0090-4295",
doi = "https://doi.org/10.1016/j.urology.2016.09.015",
url = "http://www.sciencedirect.com/science/article/pii/S0090429516306197",
author = "David E. Conroy and Alexandra Dubansky and Joshua Remillard and Robert Murray and Christine A. Pellegrini and Siobhan M. Phillips and Necole M. Streeper",
abstract = "Objective
To determine the extent to which validated techniques for behavior change have been infused in commercially available fluid consumption applications (apps).
Materials and Methods
Coders evaluated behavior change techniques represented in online descriptions for 50 fluid consumption apps and the latest version of each app.
Results
Apps incorporated a limited range of behavior change techniques (<20% of taxonomy). The number of techniques varied by operating system but not as a function of whether apps were free or paid. Limitations include the lack of experimental evidence establishing the efficacy of these apps.
Conclusion
Patients with urolithiasis can choose from many apps to support the recommended increase in fluid intake. Apps for iOS devices incorporate more behavior change techniques compared to apps for the Android operating system. Free apps are likely to expose patients to a similar number of techniques as paid apps. Physicians and patients should screen app descriptions for features to promote self-monitoring and provide feedback on discrepancies between behavior and a fluid consumption goal."
}
@article{WURTHINGER2013481,
title = "Unrestricted and safe dynamic code evolution for Java",
journal = "Science of Computer Programming",
volume = "78",
number = "5",
pages = "481 - 498",
year = "2013",
note = "Special section: Principles and Practice of Programming in Java 2009/2010 & Special section: Self-Organizing Coordination",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2011.06.005",
url = "http://www.sciencedirect.com/science/article/pii/S0167642311001456",
author = "Thomas Würthinger and Christian Wimmer and Lukas Stadler",
keywords = "Java, Virtual machine, Class hierarchy, Run-time evolution, Dynamic software updating, Safe dynamic updates",
abstract = "Dynamic code evolution is a technique to update a program while it is running. In an object-oriented language such as Java, this can be seen as replacing a set of classes by new versions. We modified an existing high-performance virtual machine to allow arbitrary changes to the definition of loaded classes. Besides adding and deleting fields and methods, we also allow any kind of changes to the class and interface hierarchy. Our approach focuses on increasing developer productivity during debugging, but can also be applied for updating of long-running applications. Changes can be applied at any point at which a Java program can be suspended. Our virtual machine is able to continue execution of old changed or deleted methods and also to access deleted static fields. A dynamic verification of the current state of the program ensures type safety of complex class hierarchy changes. However, the programmer still has to ensure that the semantics of the modified program are correct and that the new program version can start running from the state left behind by the old program version. The evaluation section shows that our modifications to the virtual machine have no negative performance impact on normal program execution. The in-place instance update algorithm is in many cases faster than a full garbage collection. Standard Java development environments automatically use the code evolution features of our modified virtual machine, so no additional tools are required."
}
@article{LEE201889,
title = "Coelacanth-specific adaptive genes give insights into primitive evolution for water-to-land transition of tetrapods",
journal = "Marine Genomics",
volume = "38",
pages = "89 - 95",
year = "2018",
issn = "1874-7787",
doi = "https://doi.org/10.1016/j.margen.2017.12.004",
url = "http://www.sciencedirect.com/science/article/pii/S1874778716302033",
author = "Chul Lee and Heesu Jeong and DongAhn Yoo and Eun Bae Kim and Bo-Hye Nam and Heebal Kim",
keywords = "Coelacanth, Limb emergence, Comparative genomics, Positively selected genes, dN/dS analysis, TAAS analysis",
abstract = "Coelacanth is a group of extant lobe-finned fishes in Sarcopterygii that provides evolutionary information for the missing link between ray-finned fish and tetrapod vertebrates. Its phenotypes, different from actinopterygian fishes, have been considered as primitive terrestrial traits such as cartilages in their fatty fins which are homologous with the humerus and femur. To investigate molecular evolution of coelacanth which led to its divergence into Sarcopterygii, we compared its protein coding sequences with 11 actinopterygian fishes. We identified 47 genes under positive selection specific to coelacanth, when compared to Holostei and Teleostei. Out of these, NCDN and 14 genes were associated with spatial learning and nitrogen metabolism, respectively. In homeobox gene superfamily, we identified coelacanth-specific amino acid substitutions, and also observed that one of replacements in SHOX was shared with extant tetrapods. Such molecular changes may cause primordial morphological change in the common ancestor of sarcopterygians. These results suggest that certain genes such as NCDN, MMS19, TRMT1, ALX1, DLX5 and SHOX might have played a role in the evolutionary transition between aquatic and terrestrial vertebrates."
}
@article{TALBOT2018684,
title = "Normal CA1 Place Fields but Discoordinated Network Discharge in a Fmr1-Null Mouse Model of Fragile X Syndrome",
journal = "Neuron",
volume = "97",
number = "3",
pages = "684 - 697.e4",
year = "2018",
issn = "0896-6273",
doi = "https://doi.org/10.1016/j.neuron.2017.12.043",
url = "http://www.sciencedirect.com/science/article/pii/S0896627317312114",
author = "Zoe Nicole Talbot and Fraser Todd Sparks and Dino Dvorak and Bridget Mary Curran and Juan Marcos Alarcon and André Antonio Fenton",
keywords = "fragile X syndrome, , FMRP, place cell, learning, memory, synaptic plasticity, neural coordination, intellectual disability, autism",
abstract = "Summary
Silence of FMR1 causes loss of fragile X mental retardation protein (FMRP) and dysregulated translation at synapses, resulting in the intellectual disability and autistic symptoms of fragile X syndrome (FXS). Synaptic dysfunction hypotheses for how intellectual disabilities like cognitive inflexibility arise in FXS predict impaired neural coding in the absence of FMRP. We tested the prediction by comparing hippocampus place cells in wild-type and FXS-model mice. Experience-driven CA1 synaptic function and synaptic plasticity changes are excessive in Fmr1-null mice, but CA1 place fields are normal. However, Fmr1-null discharge relationships to local field potential oscillations are abnormally weak, stereotyped, and homogeneous; also, discharge coordination within Fmr1-null place cell networks is weaker and less reliable than wild-type. Rather than disruption of single-cell neural codes, these findings point to invariant tuning of single-cell responses and inadequate discharge coordination within neural ensembles as a pathophysiological basis of cognitive inflexibility in FXS.
Video Abstract
"
}
@article{SHAO20181,
title = "Complete sequence of the tumor-inducing plasmid pTiChry5 from the hypervirulent Agrobacterium tumefaciens strain Chry5",
journal = "Plasmid",
volume = "96-97",
pages = "1 - 6",
year = "2018",
issn = "0147-619X",
doi = "https://doi.org/10.1016/j.plasmid.2018.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S0147619X1730121X",
author = "Shuai Shao and Xiaorong Zhang and G. Paul H. van Heusden and Paul J.J. Hooykaas",
keywords = ", Chry5, Chrysopine, Ti-plasmid, Opine, Plant vector",
abstract = "Agrobacterium tumefaciens strain Chry5 is hypervirulent on many plants including soybean that are poorly transformed by other A. tumefaciens strains. Therefore, it is considered as a preferred vector for genetic transformation of plants. Here we report the complete nucleotide sequence of its chrysopine-type Ti-plasmid pTiChry5. It is comprised of 197,268 bp with an overall GC content of 54.5%. Two T-DNA regions are present and 219 putative protein-coding sequences could be identified in pTiChry5. Roughly one half of the plasmid is highly similar to the agropine-type Ti plasmid pTiBo542, including the virulence genes with an identical virG gene, which is responsible for the supervirulence caused by pTiBo542. The remaining part of pTiChry5 is less related to that of pTiBo542 and embraces the trb operon of conjugation genes, genes involved in the catabolism of Amadori opines and the gene for chrysopine synthase, which replaces the gene for agropine synthase in pTiBo542. With the exception of an insertion of IS869, these Ti plasmids differ completely in the set of transposable elements present, reflecting a different evolutionary history from a common ancestor."
}
@article{LUPYAN2015117,
title = "Object knowledge changes visual appearance: Semantic effects on color afterimages",
journal = "Acta Psychologica",
volume = "161",
pages = "117 - 130",
year = "2015",
issn = "0001-6918",
doi = "https://doi.org/10.1016/j.actpsy.2015.08.006",
url = "http://www.sciencedirect.com/science/article/pii/S0001691815300421",
author = "Gary Lupyan",
keywords = "Perception, Top-down effects, Visual knowledge, Predictive coding, Afterimages",
abstract = "According to predictive coding models of perception, what we see is determined jointly by the current input and the priors established by previous experience, expectations, and other contextual factors. The same input can thus be perceived differently depending on the priors that are brought to bear during viewing. Here, I show that expected (diagnostic) colors are perceived more vividly than arbitrary or unexpected colors, particularly when color input is unreliable. Participants were tested on a version of the ‘Spanish Castle Illusion’ in which viewing a hue-inverted image renders a subsequently shown achromatic version of the image in vivid color. Adapting to objects with intrinsic colors (e.g., a pumpkin) led to stronger afterimages than adapting to arbitrarily colored objects (e.g., a pumpkin-colored car). Considerably stronger afterimages were also produced by scenes containing intrinsically colored elements (grass, sky) compared to scenes with arbitrarily colored objects (books). The differences between images with diagnostic and arbitrary colors disappeared when the association between the image and color priors was weakened by, e.g., presenting the image upside-down, consistent with the prediction that color appearance is being modulated by color knowledge. Visual inputs that conflict with prior knowledge appear to be phenomenologically discounted, but this discounting is moderated by input certainty, as shown by the final study which uses conventional images rather than afterimages. As input certainty is increased, unexpected colors can become easier to detect than expected ones, a result consistent with predictive-coding models."
}
@article{ZIMMERMANN20131320,
title = "Face learning and the emergence of view-independent face recognition: An event-related brain potential study",
journal = "Neuropsychologia",
volume = "51",
number = "7",
pages = "1320 - 1329",
year = "2013",
issn = "0028-3932",
doi = "https://doi.org/10.1016/j.neuropsychologia.2013.03.028",
url = "http://www.sciencedirect.com/science/article/pii/S0028393213001012",
author = "Friederike G.S. Zimmermann and Martin Eimer",
keywords = "Face perception, Face recognition, Visual face memory, Event-related brain potentials",
abstract = "Recognizing unfamiliar faces is more difficult than familiar face recognition, and this has been attributed to qualitative differences in the processing of familiar and unfamiliar faces. Familiar faces are assumed to be represented by view-independent codes, whereas unfamiliar face recognition depends mainly on view-dependent low-level pictorial representations. We employed an electrophysiological marker of visual face recognition processes in order to track the emergence of view-independence during the learning of previously unfamiliar faces. Two face images showing either the same or two different individuals in the same or two different views were presented in rapid succession, and participants had to perform an identity-matching task. On trials where both faces showed the same view, repeating the face of the same individual triggered an N250r component at occipito-temporal electrodes, reflecting the rapid activation of visual face memory. A reliable N250r component was also observed on view-change trials. Crucially, this view-independence emerged as a result of face learning. In the first half of the experiment, N250r components were present only on view-repetition trials but were absent on view-change trials, demonstrating that matching unfamiliar faces was initially based on strictly view-dependent codes. In the second half, the N250r was triggered not only on view-repetition trials but also on view-change trials, indicating that face recognition had now become more view-independent. This transition may be due to the acquisition of abstract structural codes of individual faces during face learning, but could also reflect the formation of associative links between sets of view-specific pictorial representations of individual faces."
}
@article{SHARP2013385,
title = "Cognitions, Emotions, and Applications: Participants' Experiences of Learning about Strengths in an Academic Library",
journal = "The Journal of Academic Librarianship",
volume = "39",
number = "5",
pages = "385 - 391",
year = "2013",
issn = "0099-1333",
doi = "https://doi.org/10.1016/j.acalib.2013.02.008",
url = "http://www.sciencedirect.com/science/article/pii/S0099133313000232",
author = "Allison Sharp and Jeanine Williamson",
keywords = "Positive psychology, Academic libraries management, Strengths-based interventions",
abstract = "This study examined academic library employees' experiences during “strengths education,” a process of learning about individual strengths during a positive psychology intervention. Participants took the Clifton StrengthsFinder test, attended a workshop, and then were interviewed about what they considered to be the effects of the strengths training. The focus of the qualitative analysis was the interviewees' statements about the intrapersonal and interpersonal effects of learning about their strengths. We categorized and coded these statements as cognitions formed, emotions experienced, and applications envisioned. Our findings raise interesting implications for job satisfaction and employee self-esteem, especially during times of change."
}
@article{ORTEGA201881,
title = "Rising sea levels and sinking property values: Hurricane Sandy and New York’s housing market",
journal = "Journal of Urban Economics",
volume = "106",
pages = "81 - 100",
year = "2018",
issn = "0094-1190",
doi = "https://doi.org/10.1016/j.jue.2018.06.005",
url = "http://www.sciencedirect.com/science/article/pii/S0094119018300354",
author = "Francesc Ortega and Süleyman Taṣpınar",
keywords = "Climate change, Real estate, Cities, Hurricane Sandy",
abstract = "This paper analyzes the effects of hurricane Sandy on the New York City housing market using a large parcel-level dataset that contains all housing sales for 2003–2017. The dataset also contains geo-coded FEMA data on which building structures were damaged by the hurricane and to what degree. Our estimates provide robust evidence of a persistent negative impact on flood zone housing values. We show the gradual emergence of a price penalty among flood zone properties that were not damaged by Sandy, reaching 8% in year 2017 and showing no signs of recovery. In contrast, damaged properties suffered a large immediate drop in value following the storm (17–22%), followed by a partial recovery and convergence toward a similar penalty as non-damaged properties. The partial recovery in the prices of damaged properties likely reflects their gradual restoration. However, the persistent price reduction affecting all flood-zone properties is more consistent with a learning mechanism. Hurricane Sandy may have increased the perceived risk of large-scale flooding episodes in that area."
}
@article{RIVAS20161653,
title = "Safety studies of plasma-wall events with AINA code for Japanese DEMO",
journal = "Fusion Engineering and Design",
volume = "109-111",
pages = "1653 - 1657",
year = "2016",
note = "Proceedings of the 12th International Symposium on Fusion Nuclear Technology-12 (ISFNT-12)",
issn = "0920-3796",
doi = "https://doi.org/10.1016/j.fusengdes.2015.10.037",
url = "http://www.sciencedirect.com/science/article/pii/S0920379615303240",
author = "J.C. Rivas and M. Nakamura and Y. Someya and K. Hoshino and N. Asakura and H. Takase and Y. Miyoshi and H. Utoh and K. Tobita and J. Dies and A. de Blas and A. Riego and M. Fabbri",
keywords = "Fusion safety, DEMO, Plasma transients, Pedestal sol model, Loss of plasma control, Ex-vessel LOCA",
abstract = "In this contribution, the work done in AINA code during 2014 and 2015 at IFERC is presented. The main motivation of this work was to adapt the code and to perform safety studies for a Japanese DEMO design. Related to AINA code, the work has supposed major changes in plasma models. Significant is the addition of an integrated SOL-pedestal model that allows the estimation of heat loads at divertor. Also, a thermal model for a WCPB (water cooled pebble bed) breeding blanket has been developed based in parametric input data from neutronics calculations. Related to safety studies, a major breakthrough in the study of LOPC (loss of plasma control) transients has been the use of an optimization method to determine the most severe transients in terms of the shortest melting times. The results of the safety study show that LOPC transients are not likely to be severe for breeding blanket, but for the case of divertor can induce severe melting. For ex-vessel LOCA (loss of coolant accident) analysis, it is severe for both blanket and divertor, but in the first case the transient time until melting is nearly two orders of magnitude higher. The results point out that the recovery time for plasma control system should be at least one order of magnitude lower than confinement time to avoid melting of divertor targets."
}
@article{DAQI2014789,
title = "Integrated Fisher linear discriminants: An empirical study",
journal = "Pattern Recognition",
volume = "47",
number = "2",
pages = "789 - 805",
year = "2014",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2013.07.021",
url = "http://www.sciencedirect.com/science/article/pii/S0031320313003208",
author = "Gao Daqi and Ding Jun and Zhu Changming",
keywords = "Fisher linear discriminants, Imbalanced datasets, Empirical thresholds, Neighborhood-preserving transformations, Iterative learning",
abstract = "This paper studies Fisher linear discriminants (FLDs) based on classification accuracies for imbalanced datasets. An optimal threshold is found out from a series of empirical formulas developed, which is related not only to sample sizes but also to distribution regions. A mixed binary–decimal coding system is suggested to make the very dense datasets sparse and enlarge the class margins on condition that the neighborhood relationships of samples are nearly preserved. The within-class scatter matrices being or approximately singular should be moderately reduced in dimensionality but not added with tiny perturbations. The weight vectors can be further updated by a kind of epoch-limited (three at most) iterative learning strategy provided that the current training error rates come down accordingly. Putting the above ideas together, this paper proposes a type of integrated FLDs. The extensive experimental results over real-world datasets have demonstrated that the integrated FLDs have obvious advantages over the conventional FLDs in the aspects of learning and generalization performances for the imbalanced datasets."
}
@article{SHARMA2018333,
title = "Sparse coding based features for speech units classification",
journal = "Computer Speech & Language",
volume = "47",
pages = "333 - 350",
year = "2018",
issn = "0885-2308",
doi = "https://doi.org/10.1016/j.csl.2017.08.004",
url = "http://www.sciencedirect.com/science/article/pii/S0885230816301565",
author = "Pulkit Sharma and Vinayak Abrol and A.D. Dileep and Anil Kumar Sao",
keywords = "Sparse representation, Dictionary learning, Speech recognition",
abstract = "In this work, we propose sparse representation based features for speech units classification tasks. In order to effectively capture the variations in a speech unit, the proposed method employs multiple class specific dictionaries. Here, the training data belonging to each class is clustered into multiple clusters, and a principal component analysis (PCA) based dictionary is learnt for each cluster. It has been observed that coefficients corresponding to middle principal components can effectively discriminate among different speech units. Exploiting this observation, we propose to use a transformation function known as weighted decomposition (WD) of principal components, which is used to emphasize the discriminative information present in the PCA-based dictionary. In this paper, both raw speech samples and mel frequency cepstral coefficients (MFCC) are used as an initial representation for feature extraction. For comparison, various popular dictionary learning techniques such as K-singular value decomposition (KSVD), simultaneous codeword optimization (SimCO) and greedy adaptive dictionary (GAD) are also employed in the proposed framework. The effectiveness of the proposed features is demonstrated using continuous density hidden Markov model (CDHMM) based classifiers for (i) classification of isolated utterances of E-set of English alphabet, (ii) classification of consonant-vowel (CV) segments in Hindi language and (iii) classification of phoneme from TIMIT phonetic corpus."
}
@article{HAUSLER201353,
title = "Natural image sequences constrain dynamic receptive fields and imply a sparse code",
journal = "Brain Research",
volume = "1536",
pages = "53 - 67",
year = "2013",
note = "Selected papers presented at the Tenth International Neural Coding Workshop, Prague, Czech Republic, 2012",
issn = "0006-8993",
doi = "https://doi.org/10.1016/j.brainres.2013.07.056",
url = "http://www.sciencedirect.com/science/article/pii/S0006899313010792",
author = "Chris Häusler and Alex Susemihl and Martin P. Nawrot",
keywords = "Autoencoding, Lifetime sparseness, Machine learning, Population sparseness, Restricted Boltzmann Machine, Visual cortex",
abstract = "In their natural environment, animals experience a complex and dynamic visual scenery. Under such natural stimulus conditions, neurons in the visual cortex employ a spatially and temporally sparse code. For the input scenario of natural still images, previous work demonstrated that unsupervised feature learning combined with the constraint of sparse coding can predict physiologically measured receptive fields of simple cells in the primary visual cortex. This convincingly indicated that the mammalian visual system is adapted to the natural spatial input statistics. Here, we extend this approach to the time domain in order to predict dynamic receptive fields that can account for both spatial and temporal sparse activation in biological neurons. We rely on temporal restricted Boltzmann machines and suggest a novel temporal autoencoding training procedure. When tested on a dynamic multi-variate benchmark dataset this method outperformed existing models of this class. Learning features on a large dataset of natural movies allowed us to model spatio-temporal receptive fields for single neurons. They resemble temporally smooth transformations of previously obtained static receptive fields and are thus consistent with existing theories. A neuronal spike response model demonstrates how the dynamic receptive field facilitates temporal and population sparseness. We discuss the potential mechanisms and benefits of a spatially and temporally sparse representation of natural visual input."
}
@article{FRUCCI2016148,
title = "WIRE: Watershed based iris recognition",
journal = "Pattern Recognition",
volume = "52",
pages = "148 - 159",
year = "2016",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2015.08.017",
url = "http://www.sciencedirect.com/science/article/pii/S0031320315003052",
author = "Maria Frucci and Michele Nappi and Daniel Riccio and Gabriella Sanniti di Baja",
keywords = "Iris segmentation, Iris detection, Iris recognition, Watershed transformation, Circle fitting",
abstract = "A Watershed transform based Iris REcognition system (WIRE) for noisy images acquired in visible wavelength is presented. Key points of the system are: the color/illumination correction pre-processing step, which is crucial for darkly pigmented irises whose albedo would be dominated by corneal specular reflections; the criteria used for the binarization of the watershed transform, leading to a preliminary segmentation which is refined by taking into account the watershed regions at least partially included in the best iris fitting circle; the introduction of a new cost function to score the circles detected as potentially delimiting limbus and pupil. The advantage offered by the high precision of WIRE in iris segmentation has a positive impact as regards the iris code, which results to be more accurately computed, so that the performance of iris recognition is also improved. To assess the performance of WIRE and to compare it with the performance of other available methods, two well known databases have been used, specifically UBIRIS version 1 session 2 and the subset of UBIRIS version 2 that has been used as training set for the international challenge NICE II."
}
@article{MENZEL201484,
title = "The insect mushroom body, an experience-dependent recoding device",
journal = "Journal of Physiology-Paris",
volume = "108",
number = "2",
pages = "84 - 95",
year = "2014",
note = "Neuroethology: A Tribute to Hector Maldonado",
issn = "0928-4257",
doi = "https://doi.org/10.1016/j.jphysparis.2014.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S092842571400031X",
author = "Randolf Menzel",
keywords = "Honeybee, Apis mellifera, Mushroom body, Kenyon cells, Matrix memory, Microcircuit, Mushroom body extrinsic neurons, Recurrent inhibition, PE1 neuron, PCT neurons",
abstract = "The insect mushroom body is a higher order integration center involved in cross-sensory integration and memory formation. The relatively large mushroom bodies of social Hymenoptera (e.g. bees) have been related to the demands of a social system and the neural processes required to allow the animal to navigate in an ever-changing environment. Here I review studies aiming to elucidate the neural processes that take place at the input and the output sites of the mushroom bodies and that underlie cross-sensory integration, associative learning, memory storage and retrieval. Highly processed sensory information is received at modality-specific compartments of the input site, the calyx. The large number of intrinsic neurons of the mushroom body receive multiple sensory inputs establishing combinations of processed sensory stimuli. A matrix-like memory structure characterizes the dendritic area of the intrinsic neurons allowing storage of rich combinations of sensory information. The rather small number of extrinsic neurons read out from multiple intrinsic neurons, thereby losing their sensory coding properties. The response properties of these neurons change according to the value of stimulus combinations experienced. It is concluded that the mushroom bodies transform the highly dimensional sensory coding space into a low dimensional coding space of value-based information. A model of such an experience-dependent recoding device is presented and compared with the available data."
}
@article{ZHANG2016407,
title = "Development of a three-zone transport model for activated corrosion products analysis of Tokamak Cooling Water System",
journal = "Fusion Engineering and Design",
volume = "109-111",
pages = "407 - 410",
year = "2016",
note = "Proceedings of the 12th International Symposium on Fusion Nuclear Technology-12 (ISFNT-12)",
issn = "0920-3796",
doi = "https://doi.org/10.1016/j.fusengdes.2016.02.091",
url = "http://www.sciencedirect.com/science/article/pii/S0920379616301831",
author = "Jingyu Zhang and Lu Li and Shuxiang He and Wen Song and Yu Fu and Bin Zhang and Yixue Chen",
keywords = "Activated corrosion products, Three-zone transport model, CATE 2.0 code, Radioactivity",
abstract = "In the Tokamak Cooling Water System (TCWS), the activated corrosion products (ACPs) play as an important radioactive source, which have impact on reactor inspection and maintenance. A three-zone transport model of ACPs was elaborated in this paper, which is based on the theory that the main driving force for ACPs transport is the temperature change of the coolant throughout the loop and the resulting change in metal ion solubility in the coolant. The three-zone transport model was used to replace the loop-homogenization model in the ACPs evaluation code CATE 1.0, developing CATE to give the capability to calculate spatial distribution of ACPs. As a result, CATE was upgraded to version 2.0. For code testing, a FW/BLK cooling loop of ITER was simulated using CATE 2.0, and the composition and radioactivity of ACPs were calculated. The results showed that the major contributors came from the short-life nuclides, especially Mn-56, which can influence material choice in reactor design and shutdown time before reactor maintenance."
}
@article{GERMANAS2017259,
title = "HOTB update: Parallel code for calculation of three- and four-particle harmonic oscillator transformation brackets and their matrices using OpenMP",
journal = "Computer Physics Communications",
volume = "215",
pages = "259 - 264",
year = "2017",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2017.01.028",
url = "http://www.sciencedirect.com/science/article/pii/S0010465517300401",
author = "D. Germanas and A. Stepšys and S. Mickevičius and R.K. Kalinauskas",
keywords = "Mathematical methods in physics, Algebraic methods, Nuclear shell model",
abstract = "This is a new version of the HOTB code designed to calculate three and four particle harmonic oscillator (HO) transformation brackets and their matrices. The new version uses the OpenMP parallel communication standard for calculations of harmonic oscillator transformation brackets. A package of Fortran code is presented. Calculation time of large matrices, orthogonality conditions and array of coefficients can be significantly reduced using effective parallel code. Other functionalities of the original code (for example calculation of single harmonic oscillator brackets) have not been modified.
New version program summary
Program Title: HOTB_OpenMP Program Files doi:http://dx.doi.org/10.17632/ddjxc7rkpr.1 Licensing provisions: GPLv3 Programming language: Fortran 90 Journal reference of previous version:  [2], [3], [4] Does the new version supersede the previous version?: Yes Reasons for the new version: Calculation of huge amount of HOB’s, using single processor, takes much time. Speed-up is needed. The new program version allows to perform calculations on multiple processors using shared memory OpenMP API that reduces time needed for calculations and is easily implemented by end user. Nature of problem: Often solving nuclear structure and other problems huge amount of HOB’s is needed. Calculation of matrices, orthogonality conditions and arrays of three-particle harmonic oscillator brackets (3HOB) and four-particle harmonic oscillator brackets (4HOB) using single processor for high values of harmonic oscillator quanta e takes much time. Solution method: Using OpenMP parallelization standard for the three and four-particle harmonics oscillator brackets 3HOB and 4HOB, based on methods, is presented in [2], [3], [4]. Summary of revisions: Subroutines for calculation arrays of HOB’s, their matrices and orthogonality conditions are rewritten to use OpenMP parallel programming standard. 1.Additional features of new code HOTB_OpenMP: (1)Calculation time. As the code of previous version of program was redone using parallelism paradigma, it is now possible to reduce the calculation time of transformation matrices significantly, depending on the number of processors (cores), as the dimensions of matrices are growing very rapidly according to the energy and momenta values.2.Modifications or corrections to previous versions:Updated program HOTB_OpenMP is written in the FORTRAN 90 language, according to formulas described in [1], [2], [3], [4]. There is one file: HOTB_OpenMP.f90.Detailed descriptions of internal parameters used by subroutines are represented in [1] for 3HOB, [2] for 4HOB, [3] for 4HOB for special cases of the parameters and [4] for 4HOB for matrix M method and also are located in files read.me and appendix.pdf. File read.me also contains description of example input and output files.Main computational subroutines using OpenMP: (1)subroutineall_3HOB_OpenMPdimens bus_3HOB array_3HOB.Performs parallel calculations of all 3HOB brackets for given e (total harmonic oscillator energy). Other input parameters: dimens—dimension of the problem, bus_3HOB—array of quantum numbers. The return value is array_3HOB—array of 3HOB brackets.(2)subroutineall_4HOB_OpenMPdimens bus_4HOB array_4HOB.Performs parallel calculations of all 4HOB brackets for given e (total HO energy). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is array_4HOB—array of 4HOB brackets.(3)subroutineorthogonality_3HOB_OpenMPpaklaida nk dimens bus_3HOB.Performs parallel calculations of orthogonality of 3HOB matrix using OpenMP for given e (total HO energy). Other input parameters: dimens—dimension of the problem, bus_3HOB—array of quantum numbers. The return values are paklaida—error of the problem and nk—amount of coefficients (equation (43) in [1]).(4)subroutineorthogonality_4HOB_OpenMPpaklaida nk dimens bus_4HOB.Performs parallel calculations of orthogonality of 4HOB matrix using OpenMP for given e (total HO energy). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return values are paklaida—error of the problem and nk—amount of coefficients (equation (12) in [2]).(5)subroutinematrix_3HOB_OpenMPl dimens bus_3HOB matr_3HOB.Performs parallel calculations of 3HOB matrix using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_3HOB—array of quantum numbers. The return value is matr_3HOB—matrix of 3HOB coefficients.(6)subroutinematrix_4HOB_OpenMPl dimens bus_4HOB matr_4HOB.Performs parallel calculations of 4HOB matrix using OpenMP for given e (total HO energy) and l (total angular momentum).Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.(7)subroutinematrix_4HOB_d_0_OpenMPl dimens bus_4HOB matr_4HOB.Performs parallel calculations of 4HOB matrix with parameter d = 0 using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.(8)subroutinematrix_4HOB_d_infinity_OpenMPl dimens bus_4HOB matr_4HOB.Performs parallel calculations of 4HOB matrix with parameter d = infinity using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.(9)subroutinematrix_4HOB_d1_0_OpenMPl dimens bus_4HOB matr_4HOB.Performs parallel calculations of 4HOB matrix with parameter d1 = 0 using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.(10)subroutinematrix_M_OpenMPl dimens bus_4HOB matr_M.Performs parallel calculations of M matrix using OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.(11)subroutinematrix_4HOB_M_OpenMPl dimens bus_4HOB matr_4HOB_M.Performs parallel calculations of 4HOB matrix using M technique and OpenMP for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem, bus_4HOB—array of quantum numbers. The return value is matr_4HOB—matrix of 4HOB coefficients.3.Additional non-parallel subroutines, needed main computational subroutines to run: (1)subroutineall_3HOB_dimensione dimens.Outputs number of all 3HOB brackets for given e (total HO energy). The return value is dimens—dimension of the problem.(2)subroutinearray_quantum_numbers_all_3HOBe dimens bus_3HOB.Fills array of quantum numbers for all 3HOB brackets for given e (total HO energy). Other input parameters: dimens—dimension of the problem. The return value is bus_3HOB—array of quantum numbers.(3)subroutineall_4HOB_dimensione dimens.Outputs number of all 4HOB brackets for given e (total HO energy). The return value is dimens—dimension of the problem.(4)subroutinearray_quantum_numbers_all_4HOBe dimens bus_4HOB.Fills array of quantum numbers for all 4HOB brackets for given e (total HO energy). Other input parameters: dimens—dimension of the problem. The return value is bus_4HOB—array of quantum numbers.(5)subroutineort_3HOB_dimensione dimens.Calculates number of 3HOB coefficients for orthogonality test for given e (total HO energy). The return value is dimens—dimension of the problem.(6)subroutinearray_quantum_numbers_ort_3HOBe dimens bus_3HOB.Fills array of quantum numbers for 3HOB orthogonality test for given e (total HO energy). Other input parameters: dimens—dimension of the problem. The return value is bus_3HOB—array of quantum numbers.(7)subroutineort_4HOB_dimensione dimens.Calculates number of 4HOB coefficients for orthogonality test for given e (total HO energy). The return value is dimens—dimension of the problem.(8)subroutinearray_quantum_numbers_ort_4HOBe dimens bus_4HOB.Fills array of quantum numbers for 4HOB orthogonality test for given e (total HO energy). Other input parameters: dimens—dimension of the problem. The return value is bus_4HOB—array of quantum numbers.(9)subroutinematrix_3HOB_dimensione l dimens.Calculates dimension of 3HOB matrix for given e (total HO energy) and l (total angular momentum). The return value is dimens—dimension of the problem.(10)subroutinearray_quantum_numbers_matrix_3HOBe l dimens bus_3HOB.Fills array of quantum numbers for matrix 3HOB for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem. The return value is bus_3HOB—array of quantum numbers.(11)subroutinematrix_4HOB_dimensione l dimens.Calculates dimension of 4HOB matrix for given e (total HO energy) and l (total angular momentum). The return value is dimens—dimension of the problem.(12)subroutinearray_quantum_numbers_matrix_4HOBe l dimens bus_4HOB.Fills array of quantum numbers for matrix 4HOB for given e (total HO energy) and l (total angular momentum). Other input parameters: dimens—dimension of the problem. The return value is bus_4HOB—array of quantum numbers.(13)subroutinehob_output_to_filearray_xHOB dimens name bus_xHOB n e.Writes HOB’s array array_xHOB to file. Other input parameters: dimens—dimension of the problem,  bus_xHOB—array of quantum numbers, n—parameter, that equals 9 in case of 3HOB and equals 17 in case of 4HOB, e—harmonic oscillator quanta. The return value is text file that name is value of the variable name.(14)subroutineort_output_to_filepaklaida dimens nk name n e.Writes orthogonality test results to file. Input parameters: paklaida—error of the problem, dimens—dimension of the problem, nk—amount of coefficients, n—parameter, that equals 1 in case of 3HOB and equals 2 in case of 4HOB, e—harmonic oscillator quanta. The return value is text file that name is value of the variable name.(15)subroutinematrix_output_to_filematrix dimens name bus_xHOB n e l.Writes HOB’s matrix matrix to file. Other input parameters: dimens—dimension of the problem, bus_xHOB—array of quantum numbers, n—parameter, that equals 4 in case of 3HOB and equals 8 in case of 4HOB, e—harmonic oscillator quanta, l—total angular momentum. The return value is text file that name is value of the variable name.4.Additional subroutines for user conveniency: (1)subroutinesingle_3HOBComputes single 3HOB for given set of quantum numbers.(2)subroutinesingle_4HOBComputes single 4HOB for given set of quantum numbers.(3)subroutinesingle_4HOB_d_0Computes single 4HOB for given set of quantum numbers, case d = 0.(4)subroutinesingle_4HOB_d_infinityComputes single 4HOB for given set of quantum numbers, case d = infinity.(5)subroutinesingle_4HOB_d1_0Computes single 4HOB for given set of quantum numbers, case d1 = 0. For illustrational purposes we have made some calculations for all HOB’s, their orthogonality conditions and matrices. Calculations were done on 12 cores 96 GB RAM computer. Calculation time is OpenMP wall-time function omp_get_wtime. Results are presented in Table 1, Table 2, Table 3, Table 4, Table 5, Table 6. Additional comments including Restrictions and Unusual features: Calculations can be done up to harmonic oscillator (HO) energy quanta e = 28 for double precision (presented here) version. Acknowledgments Computations were performed using resources at the High Performance Computing Center “HPC Saultekis” in Vilnius University’s Faculty of Physics. [1]G.P. Kamuntavičius, R.K. Kalinauskas, B.R. Barrett, S. Mickevičius, D. Germanas, The general harmonic-oscillator brackets: compact expression, symmetries, sums and Fortran code, Nucl. Phys. A 695(2001) 191-201.[2]D. Germanas, R.K. Kalinauskas, S. Mickevičius, Calculation of four-particle harmonic-oscillator transformation brackets, Computer Physics Communications 181(2010) 420-425.[3]S. Mickevičius, E. Brazauskas, D. Germanas, R.K. Kalinauskas, The four-particle harmonic-oscillator brackets: Compact expressions and updated Fortran program, Computer Physics Communications 182(2011) 1377-1381.[4]S. Mickevičius, D. Germanas, R.K. Kalinauskas, Revised calculation of four-particle harmonic-oscillator transformation brackets matrix, Computer Physics Communications 184(2013) 409-413."
}
@article{STERPENICH2014608,
title = "Sleep sharpens sensory stimulus coding in human visual cortex after fear conditioning",
journal = "NeuroImage",
volume = "100",
pages = "608 - 618",
year = "2014",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2014.06.003",
url = "http://www.sciencedirect.com/science/article/pii/S1053811914004807",
author = "Virginie Sterpenich and Camille Piguet and Martin Desseilles and Leonardo Ceravolo and Markus Gschwind and Dimitri Van De Ville and Patrik Vuilleumier and Sophie Schwartz",
keywords = "Conditioning, Emotion, Perceptual learning, Memory consolidation, Sleep, Functional MRI, Amygdala, Fusiform cortex",
abstract = "Efficient perceptual identification of emotionally-relevant stimuli requires optimized neural coding. Because sleep contributes to neural plasticity mechanisms, we asked whether the perceptual representation of emotionally-relevant stimuli within sensory cortices is modified after a period of sleep. We show combined effects of sleep and aversive conditioning on subsequent discrimination of face identity information, with parallel plasticity in the amygdala and visual cortex. After one night of sleep (but neither immediately nor after an equal waking interval), a fear-conditioned face was better detected when morphed with another identity. This behavioral change was accompanied by increased selectivity of the amygdala and face-responsive fusiform regions. Overnight neural changes can thus sharpen the representation of threat-related stimuli in cortical sensory areas, in order to improve detection in impoverished or ambiguous situations. These findings reveal an important role of sleep in shaping cortical selectivity to emotionally-relevant cues and thus promoting adaptive responses to new dangers."
}
@article{LUNDVALL2015231,
title = "How do technical improvements change radiographers' practice – A practice theory perspective",
journal = "Radiography",
volume = "21",
number = "3",
pages = "231 - 235",
year = "2015",
issn = "1078-8174",
doi = "https://doi.org/10.1016/j.radi.2014.12.002",
url = "http://www.sciencedirect.com/science/article/pii/S1078817414001552",
author = "L.-L. Lundvall and M. Abrandt-Dahlgren and S. Wirell",
keywords = "Practice, Radiographer, Radiography, Practice theory",
abstract = "Introduction
The two plane imaging techniques are gradually being replaced by multidimensional imaging. How it affects radiographers' professional practice has not been investigated.
Aim
To explore how technical development affects the relations between different actors and their actions in the practice of Computer Tomography.
Method
A qualitative design with data collection by open interviews (n = 8) and open observations (n = 10) of radiographers during their work with Computer Tomography. Data was first analyzed inductively resulting in seven codes. Secondly abduction was carried out by interpreting the content in the codes with a practice theory. This resulted in four themes.
Result
First theme: Changed materiality makes the practical action easier. The actual image production has become practically easier. Second theme: Changed machines cause conflict between the arrangements of the work and the patients' needs. The time for the machine to carry out image production is easy to foresee, but information about the patient's individual status and needs is missing and this leads to difficulties in giving individual planned care. Third theme: Changing materiality prefigure learning. The different apparatus in use and the continuously changing methods of image production is co-constitutive of the practitioners' activities and learning. Fourth theme: Radiography is arranged for patient safety in relation to radiation doses and medical security risks. But the radiographers, who meet the patients, have to check the accuracy of the planned examination in relation to the clinical observed information about patient safety risks with the examination."
}
@article{HOU2013418,
title = "A modified Delphi translation strategy and challenges of International Classification for Nursing Practice (ICNP®)",
journal = "International Journal of Medical Informatics",
volume = "82",
number = "5",
pages = "418 - 426",
year = "2013",
issn = "1386-5056",
doi = "https://doi.org/10.1016/j.ijmedinf.2012.08.002",
url = "http://www.sciencedirect.com/science/article/pii/S1386505612001530",
author = "I-Ching Hou and Polun Chang and Hui-Ya Chan and Patricia C. Dykes",
keywords = "Nursing, ICNP, Translation, Terminology, Delphi",
abstract = "Background and objectives
Standardized terminology is an important infrastructure component of the electronic health record. ICNP® is a systemic coding system that can support the development of nursing information systems. Translation of the standardized terminology preferred terms into local terms is an important first step in the translation process. The purpose of this case report is to describe the translation strategy used and challenges faced in translating ICNP® Version 2 preferred terms from English to traditional Chinese.
Methods
A modified Delphi strategy using forward translation and expert consensus was conducted to facilitate semantic and cultural translation and validation of the ICNP® and to make the process generalizable. A nursing informatics expert completed the initial forward translation. Five nursing experts with rich clinical and academic experiences joined this process and validated the initial translation. The nursing experts’ consensus was then used to finalize the traditional Chinese terms.
Results
A total of 1863 preferred terms from the ICNP® Version 2 were translated from English into traditional Chinese. Majority agreement from two or more nursing experts was achieved for 98.3% (n=1832) of the preferred term translations. Less than 2% (n=31) of terms had no majority agreement. Translation challenges include the following: (1) changes in code structure of preferred terms from the ICNP® β2 version to Verson 2, (2) inability to identify resources to complete the translation that fully met ICNP recommendations for terminology translators, (3) ambiguous preferred term descriptions, and (4) ambiguous preferred term names.
Conclusions
Most of the ICNP® Version 2 preferred terms were translated from English into traditional Chinese with majority consensus. For the terms without consensus, we recommend that all synonyms be included in the ICNP® translation. In countries like Taiwan where nursing education occurs in English, it is recommended that English terms are displayed along with the translated official language to help nurses to interpret and use the terminology correctly."
}
@article{COSTE201618,
title = "Two-phase CFD validation: TOPFLOW-PTS steady-state steam-water tests 3–16, 3–17, 3–18 and 3–19",
journal = "Nuclear Engineering and Design",
volume = "299",
pages = "18 - 27",
year = "2016",
note = "CFD4NRS-5",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2015.08.006",
url = "http://www.sciencedirect.com/science/article/pii/S0029549315003507",
author = "P. Coste and N. Mérigoux",
abstract = "For the Pressurized Thermal Shock (PTS) issue, the NEPTUNE_CFD code solves the Eulerian two-fluid model with specific models for liquid/gas interfaces, which are much larger than the computational cells size. The CFD validation database dedicated to PTS includes the TOPFLOW-PTS experiment, which represents condensation phenomena in a PWR cold leg, with the Emergency Core Cooling system (ECC) and a downcomer. The present study deals with NEPTUNE_CFD calculations of steady-state steam-water tests 3–16, 3–17, 3–18 and 3–19, which differ each other by the ECC liquid inlet flow rate (m˙L,ECC). So the liquid turbulence, which is the main input of the condensation models, is changed from one test to the other, firstly in the ECC region. The direct and first order effect of the ECC flow on the liquid temperature is shown with sensitive two-phase flow regime transitions, which require a careful meshing. This condition being fulfilled, satisfactory NEPTUNE_CFD results mesh-independency on refinement is shown. Present CFD is able to calculate the effect of m˙L,ECC on condensation and temperatures. CFD versions with k-ɛ and Rij-ɛ SSG turbulence modeling are compared and the improvement brought by the new version with Rij-ɛ SSG is shown."
}
@article{AHUSBORDE20151,
title = "Mercer's spectral decomposition for the characterization of thermal parameters",
journal = "Journal of Computational Physics",
volume = "294",
pages = "1 - 19",
year = "2015",
issn = "0021-9991",
doi = "https://doi.org/10.1016/j.jcp.2015.03.037",
url = "http://www.sciencedirect.com/science/article/pii/S0021999115001928",
author = "E. Ahusborde and M. Azaïez and F. Ben Belgacem and E. Palomo Del Barrio",
keywords = "Singular value decomposition, Spectral expansion of Mercer kernels, Identification",
abstract = "We investigate a tractable Singular Value Decomposition (SVD) method used in thermography for the characterization of thermal parameters. The inverse problem to solve is based on the model of transient heat transfer. The most significant advantage is the transformation of the dynamic identification problem into a steady identification equation. The time dependence is accounted for by the SVD in a performing way. We lay down a mathematical foundation well fitted to this approach, which relies on the spectral expansion of Mercer kernels. This enables us to shed more light on most of its important features. Given its potentialities, the analysis we propose here might help users understanding the way the SVD algorithm, or the TSVD, its truncated version, operate in the thermal parameters estimation and why it is relevant and attractive. When useful, the study is complemented by some analytical and numerical illustrations realized within matlab's code."
}
@article{LEWIS2014150,
title = "Leveraging e-Learning in Medical Education",
journal = "Current Problems in Pediatric and Adolescent Health Care",
volume = "44",
number = "6",
pages = "150 - 163",
year = "2014",
note = "Adult Education: New Concepts and Effective Methods for Today’s Learners",
issn = "1538-5442",
doi = "https://doi.org/10.1016/j.cppeds.2014.01.004",
url = "http://www.sciencedirect.com/science/article/pii/S1538544214000145",
author = "Kadriye O. Lewis and Michal J. Cidon and Teresa L. Seto and Haiqin Chen and John D. Mahan",
abstract = "e-Learning has become a popular medium for delivering instruction in medical education. This innovative method of teaching offers unique learning opportunities for medical trainees. The purpose of this article is to define the present state of e-learning in pediatrics and how to best leverage e-learning for educational effectiveness and change in medical education. Through addressing under-examined and neglected areas in implementation strategies for e-learning, its usefulness in medical education can be expanded. This study used a systematic database review of published studies in the field of e-learning in pediatric training between 2003 and 2013. The search was conducted using educational and health databases: Scopus, ERIC, PubMed, and search engines Google and Hakia. A total of 72 reference articles were suitable for analysis. This review is supplemented by the use of e-Learning Design Screening Questions to define e-learning design and development in 10 randomly selected articles. Data analysis used template-based coding themes and counting of the categories using descriptive statistics.Our search for pediatric e-learning (using Google and Hakia) resulted in six well-defined resources designed to support the professional development of doctors, residents, and medical students. The majority of studies focused on instructional effectiveness and satisfaction. There were few studies about e-learning development, implementation, and needs assessments used to identify the institutional and learners׳ needs. Reviewed studies used various study designs, measurement tools, instructional time, and materials for e-learning interventions. e-Learning is a viable solution for medical educators faced with many challenges, including (1) promoting self-directed learning, (2) providing flexible learning opportunities that would offer continuous (24h/day/7 days a week) availability for learners, and (3) engaging learners through collaborative learning communities to gain significant learning and augment continuous professional development. Several important recommendations for faculty instructors interested in providing and/or improving e-learning activities for today׳s learners are detailed."
}
@article{WRIGHT2014177,
title = "Raising the standard: changes to the Australian Code of Good Manufacturing Practice (cGMP) for Human Blood and Blood Components, Human Tissues and Human Cellular Therapy Products",
journal = "Pathology",
volume = "46",
number = "3",
pages = "177 - 183",
year = "2014",
issn = "0031-3025",
doi = "https://doi.org/10.1097/PAT.0000000000000067",
url = "http://www.sciencedirect.com/science/article/pii/S0031302516306006",
author = "Craig Wright and Zlatibor Velickovic and Ross Brown and Stephen Larsen and Janet L. Macpherson and John Gibson and John E.J. Rasko",
keywords = "Blood, cellular therapy, cGMP, code, manufacture, standard, tissue",
abstract = "Summary
In Australia, manufacture of blood, tissues and biologicals must comply with the federal laws and meet the requirements of the Therapeutic Goods Administration (TGA) Manufacturing Principles as outlined in the current Code of Good Manufacturing Practice (cGMP). The Therapeutic Goods Order (TGO) No. 88 was announced concurrently with the new cGMP, as a new standard for therapeutic goods. This order constitutes a minimum standard for human blood, tissues and cellular therapeutic goods aimed at minimising the risk of infectious disease transmission. The order sets out specific requirements relating to donor selection, donor testing and minimisation of infectious disease transmission from collection and manufacture of these products. The Therapeutic Goods Manufacturing Principles Determination No. 1 of 2013 references the human blood and blood components, human tissues and human cellular therapy products 2013 (2013 cGMP). The name change for the 2013 cGMP has allowed a broadening of the scope of products to include human cellular therapy products. It is difficult to directly compare versions of the code as deletion of some clauses has not changed the requirements to be met, as they are found elsewhere amongst the various guidelines provided. Many sections that were specific for blood and blood components are now less prescriptive and apply to a wider range of cellular therapies, but the general overall intent remains the same. Use of ’should’ throughout the document instead of ’must’ allows flexibility for alternative processes, but these systems will still require justification by relevant logical argument and validation data to be acceptable to TGA. The cGMP has seemingly evolved so that specific issues identified at audit over the last decade have now been formalised in the new version. There is a notable risk management approach applied to most areas that refer to process justification and decision making. These requirements commenced on 31 May 2013 and a 12 month transition period applies for implementation by manufacturers. The cGMP and TGO update follows the implementation of the TGA regulatory biologicals framework for cell and tissue based therapies announced in 2011. One implication for licenced TGA facilities is that they must implement the 2013 cGMP, TGO 88 and other relevant TGOs together, as they are intricately linked. This review is intended to assist manufacturers by comparing the 2000 version of the cGMP, to the new 2013 cGMP, noting that the new Code extends to include human cellular therapy products."
}
@article{HORVATH201475,
title = "Genetic comparisons yield insight into the evolution of enamel thickness during human evolution",
journal = "Journal of Human Evolution",
volume = "73",
pages = "75 - 87",
year = "2014",
issn = "0047-2484",
doi = "https://doi.org/10.1016/j.jhevol.2014.01.005",
url = "http://www.sciencedirect.com/science/article/pii/S0047248414000888",
author = "Julie E. Horvath and Gowri L. Ramachandran and Olivier Fedrigo and William J. Nielsen and Courtney C. Babbitt and Elizabeth M. St. Clair and Lisa W. Pfefferle and Jukka Jernvall and Gregory A. Wray and Christine E. Wall",
keywords = "Primate comparative genomics, , , , ",
abstract = "Enamel thickness varies substantially among extant hominoids and is a key trait with significance for interpreting dietary adaptation, life history trajectory, and phylogenetic relationships. There is a strong link in humans between enamel formation and mutations in the exons of the four genes that code for the enamel matrix proteins and the associated protease. The evolution of thick enamel in humans may have included changes in the regulation of these genes during tooth development. The cis-regulatory region in the 5′ flank (upstream non-coding region) of MMP20, which codes for enamelysin, the predominant protease active during enamel secretion, has previously been shown to be under strong positive selection in the lineages leading to both humans and chimpanzees. Here we examine evidence for positive selection in the 5′ flank and 3′ flank of AMELX, AMBN, ENAM, and MMP20. We contrast the human sequence changes with other hominoids (chimpanzees, gorillas, orangutans, gibbons) and rhesus macaques (outgroup), a sample comprising a range of enamel thickness. We find no evidence for positive selection in the protein-coding regions of any of these genes. In contrast, we find strong evidence for positive selection in the 5′ flank region of MMP20 and ENAM along the lineage leading to humans, and in both the 5′ flank and 3′ flank regions of MMP20 along the lineage leading to chimpanzees. We also identify putative transcription factor binding sites overlapping some of the species-specific nucleotide sites and we refine which sections of the up- and downstream putative regulatory regions are most likely to harbor important changes. These non-coding changes and their potential for differential regulation by transcription factors known to regulate tooth development may offer insight into the mechanisms that allow for rapid evolutionary changes in enamel thickness across closely-related species, and contribute to our understanding of the enamel phenotype in hominoids."
}
@article{ASLANI2017732,
title = "Adaptive traffic signal control with actor-critic methods in a real-world traffic network with different traffic disruption events",
journal = "Transportation Research Part C: Emerging Technologies",
volume = "85",
pages = "732 - 752",
year = "2017",
issn = "0968-090X",
doi = "https://doi.org/10.1016/j.trc.2017.09.020",
url = "http://www.sciencedirect.com/science/article/pii/S0968090X17302681",
author = "Mohammad Aslani and Mohammad Saadi Mesgari and Marco Wiering",
keywords = "Adaptive traffic signal controller, Reinforcement learning, Actor-critic, Pedestrians, Traffic disruptions",
abstract = "The transportation demand is rapidly growing in metropolises, resulting in chronic traffic congestions in dense downtown areas. Adaptive traffic signal control as the principle part of intelligent transportation systems has a primary role to effectively reduce traffic congestion by making a real-time adaptation in response to the changing traffic network dynamics. Reinforcement learning (RL) is an effective approach in machine learning that has been applied for designing adaptive traffic signal controllers. One of the most efficient and robust type of RL algorithms are continuous state actor-critic algorithms that have the advantage of fast learning and the ability to generalize to new and unseen traffic conditions. These algorithms are utilized in this paper to design adaptive traffic signal controllers called actor-critic adaptive traffic signal controllers (A-CATs controllers). The contribution of the present work rests on the integration of three threads: (a) showing performance comparisons of both discrete and continuous A-CATs controllers in a traffic network with recurring congestion (24-h traffic demand) in the upper downtown core of Tehran city, (b) analyzing the effects of different traffic disruptions including opportunistic pedestrians crossing, parking lane, non-recurring congestion, and different levels of sensor noise on the performance of A-CATS controllers, and (c) comparing the performance of different function approximators (tile coding and radial basis function) on the learning of A-CATs controllers. To this end, first an agent-based traffic simulation of the study area is carried out. Then six different scenarios are conducted to find the best A-CATs controller that is robust enough against different traffic disruptions. We observe that the A-CATs controller based on radial basis function networks (RBF (5)) outperforms others. This controller is benchmarked against controllers of discrete state Q-learning, Bayesian Q-learning, fixed time and actuated controllers; and the results reveal that it consistently outperforms them."
}
@article{MODOLO2018385,
title = "Distributed formation control using robust asynchronous and broadcast-based optimization schemes",
journal = "IFAC-PapersOnLine",
volume = "51",
number = "23",
pages = "385 - 390",
year = "2018",
note = "7th IFAC Workshop on Distributed Estimation and Control in Networked Systems NECSYS 2018",
issn = "2405-8963",
doi = "https://doi.org/10.1016/j.ifacol.2018.12.066",
url = "http://www.sciencedirect.com/science/article/pii/S2405896318336012",
author = "Vinicio Modolo and Damiano Varagnolo and Ruggero Carli",
keywords = "Newton Raphson Consensus, packet losses, target tracking",
abstract = "We consider the problem of letting a network of mobile agents distributedly track and maintain a formation while using communication schemes that are asynchronous, broadcasts based, and prone to packet losses. To this purpose we revisit and modify an existing distributed optimization algorithm that corresponds to a distributed version of the Newton Raphson (NR) algorithm. The proposed scheme uses then robust asynchronous ratio consensus algorithms as building blocks, and employs opportune definitions for the local cost functions to achieve the desired coordination objective. In our algorithm, indeed, we code the position of the to-be-followed target as the minimum of a shared global cost, and capture the desired inter-robots behaviors through dedicated distances-based potential barriers. We then check the effectiveness of the strategy using field tests, and verify that the scheme achieves the desired goal of introducing robustness to changes in the agents positions due to unexpected disturbances. More precisely, if an agent breaks the formation, then the update mechanism embedded in our scheme make that agent move back to a meaningful position as soon as some packets are successfully received by the misplaced agent."
}
@article{SOARES20131006,
title = "Comparing approaches to analyze refactoring activity on software repositories",
journal = "Journal of Systems and Software",
volume = "86",
number = "4",
pages = "1006 - 1022",
year = "2013",
note = "SI : Software Engineering in Brazil: Retrospective and Prospective Views",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2012.10.040",
url = "http://www.sciencedirect.com/science/article/pii/S016412121200297X",
author = "Gustavo Soares and Rohit Gheyi and Emerson Murphy-Hill and Brittany Johnson",
keywords = "Refactoring, Repository, Manual analysis, Automated analysis",
abstract = "Some approaches have been used to investigate evidence on how developers refactor their code, whether refactorings activities may decrease the number of bugs, or improve developers’ productivity. However, there are some contradicting evidence in previous studies. For instance, some investigations found evidence that if the number of refactoring changes increases in the preceding time period the number of defects decreases, different from other studies. They have used different approaches to evaluate refactoring activities. Some of them identify committed behavior-preserving transformations in software repositories by using manual analysis, commit messages, or dynamic analysis. Others focus on identifying which refactorings are applied between two programs by using manual inspection or static analysis. In this work, we compare three different approaches based on manual analysis, commit message (Ratzinger's approach) and dynamic analysis (SafeRefactor's approach) to detect whether a pair of versions determines a refactoring, in terms of behavioral preservation. Additionally, we compare two approaches (manual analysis and Ref-Finder) to identify which refactorings are performed in each pair of versions. We perform both comparisons by evaluating their accuracy, precision, and recall in a randomly selected sample of 40 pairs of versions of JHotDraw, and 20 pairs of versions of Apache Common Collections. While the manual analysis presents the best results in both comparisons, it is not as scalable as the automated approaches. Ratzinger's approach is simple and fast, but presents a low recall; differently, SafeRefactor is able to detect most applied refactorings, although limitations in its test generation backend results for some kinds of subjects in low precision values. Ref-Finder presented a low precision and recall in our evaluation."
}
@article{BAZAVOV20131075,
title = "Program package for multicanonical simulations of U(1) lattice gauge theory—Second version",
journal = "Computer Physics Communications",
volume = "184",
number = "3",
pages = "1075 - 1076",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2012.10.027",
url = "http://www.sciencedirect.com/science/article/pii/S0010465512003700",
author = "Alexei Bazavov and Bernd A. Berg",
abstract = "A new version STMCMUCA_V1_1 of our program package is available. It eliminates compatibility problems of our Fortran 77 code, originally developed for the g77 compiler, with Fortran 90 and 95 compilers.
New version program summary
Program title: STMC_U1MUCA_v1_1 Catalogue identifier: AEET_v1_1 Licensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html Programming language: Fortran 77 compatible with Fortran 90 and 95 Computers: Any capable of compiling and executing Fortran code Operating systems: Any capable of compiling and executing Fortran code RAM: 10 MB and up depending on lattice size used No. of lines in distributed program, including test data, etc.: 15059 No. of bytes in distributed program, including test data, etc.: 215733 Keywords: Markov chain Monte Carlo, multicanonical, Wang–Landau recursion, Fortran, lattice gauge theory, U(1) gauge group, phase transitions of continuous systems Classification: 11.5 Catalogue identifier of previous version: AEET_v1_0 Journal Reference of previous version: Computer Physics Communications 180 (2009) 2339–2347 Does the new version supersede the previous version?: Yes Nature of problem: Efficient Markov chain Monte Carlo simulation of U(1) lattice gauge theory (or other continuous systems) close to its phase transition. Measurements and analysis of the action per plaquette, the specific heat, Polyakov loops and their structure factors. Solution method: Multicanonical simulations with an initial Wang–Landau recursion to determine suitable weight factors. Reweighting to physical values using logarithmic coding and calculating jackknife error bars. Reasons for the new version: The previous version was developed for the g77 compiler Fortran 77 version. Compiler errors were encountered with Fortran 90 and Fortran 95 compilers (specified below). Summary of revisions: epsilon=one/10**10 is replaced by epsilon/10.0D10 in the parameter statements of the subroutines u1_bmha.f, u1_mucabmha.f, u1wl_backup.f, u1wlread_backup.f of the folder Libs/U1_par. For the tested compilers script files are added in the folder ExampleRuns and readme.txt files are now provided in all subfolders of ExampleRuns. The gnuplot driver files produced by the routine hist_gnu.f of Libs/Fortran are adapted to syntax required by gnuplot version 4.0 and higher. Restrictions: Due to the use of explicit real*8 initialization the conversion into real*4 will require extra changes besides replacing the implicit.sta file by its real*4 version. Unusual features: The programs have to be compiled the script files like those contained in the folder ExampleRuns as explained in the original paper. Running time: The prepared test runs took up to 74 minutes to execute on a 2 GHz PC."
}
@article{DUFF2016354,
title = "MEAMfit: A reference-free modified embedded atom method (RF-MEAM) energy and force-fitting code",
journal = "Computer Physics Communications",
volume = "203",
pages = "354 - 355",
year = "2016",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2016.02.015",
url = "http://www.sciencedirect.com/science/article/pii/S0010465516300303",
author = "Andrew Ian Duff",
keywords = "Interatomic potential, Fitting, Lattice vibrations",
abstract = "MEAMfit v1.02. Changes: various bug fixes; speed of single-shot energy and force calculations (not optimization) increased by ×10; elements up to Cn (Z=112) now correctly read from vasprun.xml files; EAM fits now produce Camelion output files; changed max number of vasprun.xml files to 10,000 (an unnecessary lower limit of 10 was allowed in the previous version).
New version program summary
Program title: MEAMfit Catalogue identifier: AEWY_v1_1 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEWY_v1_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: BSD 2-Clause License No. of lines in distributed program, including test data, etc.: 5202334 No. of bytes in distributed program, including test data, etc.: 66637698 Distribution format: tar.gz Programming language: Fortran. Computer: Linux based workstations. Operating system: Linux. RAM: 120 MB Classification: 16.1. Catalogue identifier of previous version: AEWY_v1_0 Journal reference of previous version: Comput. Phys. Comm. 196 (2015) 439 External routines: TOMS611 Unconstrained Minimization [1] included in the MEAMfit code. Does the new version supersede the previous version?: Yes Nature of problem: Fitting embedded atom method (EAM) and reference-free modified embedded atom method (RF-MEAM) potentials [2–3] to energies and forces produced by VASP [4–7]. Solution method: A computer program is presented which uses a conjugate-gradient minimizer paired with a genetic algorithm to fit EAM and RF-MEAM potentials to energies and/or atomic forces read directly from VASP output files. Potentials produced by the code are directly usable with the LAMMPS [8] or Camelion [9] molecular-dynamics packages. Reasons for new version: Bugs fixed; improvements made. Summary of revisions: Various bug fixes; speed of single-shot energy and force calculations (not optimization) increased by ×10; elements up to Cn (Z=112) now correctly read from vasprun.xml files; EAM fits now produce Camelion output files; changed max number of vasprun.xml files to 10,000 (an unnecessary lower limit of 10 was allowed in the previous version). Additional comments: User manual provided. !!!!! The distribution file for this program is over 65 MB and therefore is not delivered directly when download or Email is requested. Instead a html file giving details of how the program can be obtained is sent. !!!!! Running time: The run-time depends on the required level of accuracy of the final potential. For an EAM potential fit to 670 energies, a few hours on a single core is usually sufficient to produce a potential with R=12%–13% (see Equation. 9 in main-text for definition). To ensure a maximally optimized potential however (R=12%), a run-time of 24 h is recommended. To optimize a RF-MEAM potential, a further 24 h should be allowed. One will already find an improvement over the EAM using just a single core, however to ensure a maximally optimized potential, one should run several instances of MEAMfit in parallel. References:[1]J. E. Dennis, D. Gay and R. E. Welsch, ACM Trans. on Math. Soft., 7 (1981) 348–368.[2]M. I. Baskes, Materials Science and Engineering A, 261 (1999), 165.[3]M. Timonova and B. J. Thijsse, Modelling Simul. Mater. Sci. Eng., 19 (2011) 015003[4]G. Kresse, J. Hafner, Phys. Rev. B 47 (1993) 558.[5]G. Kresse, J. Hafner, Phys. Rev. B 49 (1994) 14251.[6]G. Kresse, J. Furthmüller, Comput. Mat. Sci. 6 (1996) 15.[7]G. Kresse, J. Furthmüller, Phys. Rev. B 54 (1996) 11169.[8]S. Plimpton, J. Comp. Phys. 117 (1995)[9]http//://tinyurl.com/camelion11-53"
}
@article{ENGLAND201760,
title = "Combining dynamic modelling codes with medium energy ion scattering measurements to characterise plasma doping",
journal = "Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms",
volume = "409",
pages = "60 - 64",
year = "2017",
note = "Proceedings of the 20th International Conference on Ion Beam Modification of Materials (IBMM 2016)",
issn = "0168-583X",
doi = "https://doi.org/10.1016/j.nimb.2017.05.043",
url = "http://www.sciencedirect.com/science/article/pii/S0168583X17306249",
author = "J. England and W. Möller and J.A. van den Berg and A. Rossall and W.J. Min and J. Kim",
keywords = "Ion-implantation, Ion beam modelling, TRIDYN, TRI3DYN, Plasma doping, PLAD, FinFET",
abstract = "Plasma doping ion implantation (PLAD) is becoming increasingly important in the manufacture of advanced semiconductor device structures but a fundamental understanding of PLAD is complicated. A model of PLAD into planar substrates has been constructed using the one dimensional computer code TRIDYN to predict collision cascades and hence substrate compositional changes during implantation. Medium Energy Ion Scattering (MEIS) measurements of dopant profiles in PLAD processed samples were used to calibrate the input ion and neutral fluxes to the model. Rules could then be proposed for how post implant profiles should be modified by a cleaning step. This learning was applied to a three dimensional TRI3DYN based model for PLAD implants into FinFET like structures. Comparison of the model to dopant profile measurements made by time of flight (TOF)-MEIS revealed the angular distributions of neutral species and doping mechanisms acting in three dimensional structures."
}
@article{SOUMYA2016140,
title = "Structural Analysis of a Historical Dam",
journal = "Procedia Engineering",
volume = "144",
pages = "140 - 147",
year = "2016",
note = "International Conference on Vibration Problems 2015",
issn = "1877-7058",
doi = "https://doi.org/10.1016/j.proeng.2016.05.017",
url = "http://www.sciencedirect.com/science/article/pii/S1877705816302326",
author = " Soumya and A.D. Pandey and R. Das and M.J. Mahesh and S. Anvesh and P. Saini",
keywords = "Composite dam, Time history analysis",
abstract = "Studies pertaining to earthquakes began only around the 17th century and those related to their effects on structures began even later in the 19th century. Hence structures made during or prior to this period are often not designed for seismic forces. India has many historical structures that were constructed long before the codes for seismic resistant design came in practice and, in many regions the seismic activities too have changed over time. It is therefore necessary that these monuments are analysed for safety during seismic activity. This paper aims to analyse the probable failure patterns of a composite dam about 120 years old using time history analysis. 2D modelling and analysis has been carried out using ANSYS14.0 to estimate stresses in the dam for three time histories with varying PGA values. Stress results show that failure of weaker materials at joints may cause internal crack formation in the dam."
}
@article{STAIKOV2016e591,
title = "Diagnostic value of color-coded duplex sonography in patients with ischemic stroke and congenital changes in the circle of Willis",
journal = "Cor et Vasa",
volume = "58",
number = "6",
pages = "e591 - e599",
year = "2016",
issn = "0010-8650",
doi = "https://doi.org/10.1016/j.crvasa.2015.11.008",
url = "http://www.sciencedirect.com/science/article/pii/S0010865015001241",
author = "Ivan Staikov and Ivan Stoyanov and Milena Staneva and Neyko Neykov and Galina Kirova and Petar Polomski and Ivo Petrov",
keywords = "Congenital abnormalities of the circle of Willis, Ischemic stroke, Color-coded duplex sonography, Magnetic resonance angiography",
abstract = "The circle of Willis (CoW) forms the main circulatory system in the human brain. A large number of variations of the CoW is known, and also their association with ischemic stroke. Three cases of young patients with combination of ischemic stroke and anomalies in the CoW are presented, and the value of the color-coded duplex sonography (CCDS) is compared to other imaging diagnostics such as magnetic resonance angiography (MRA) and digital subtraction angiography (DSA). In these patients we found multiple risk factors such as stenosis or thrombosis of intracranial brain vessels, mechanical compression of vessels, a genetic mutation associated with an increased risk of thrombosis, and intake of oral contraceptives. For clinical evaluation several methods were used: detailed medical history, neurological status, laboratory examinations (complete blood count, biochemistry, lipid profile, HIV1/2, Syphilis RPR test), screening for markers associated with an increased risk of thrombosis, chest X-ray, spinal fluid study, CCDS, DSA, MRA. A full conformity in the data from CCDS and other imaging methods was found. The authors discuss the pathogenetic role of congenital anomalies of CoW, incidence of ischemic stroke and the high diagnostic value of CCDS for finding such anomalies."
}
@article{JOHNSON201772,
title = "PFLOTRAN-E4D: A parallel open source PFLOTRAN module for simulating time-lapse electrical resistivity data",
journal = "Computers & Geosciences",
volume = "99",
pages = "72 - 80",
year = "2017",
issn = "0098-3004",
doi = "https://doi.org/10.1016/j.cageo.2016.09.006",
url = "http://www.sciencedirect.com/science/article/pii/S0098300416304289",
author = "Timothy C. Johnson and Glenn E. Hammond and Xingyuan Chen",
keywords = "Hydrogeophysics, Time-lapse geophysics, Electrical resistivity tomography, Groundwater, Simulation, Multi-physics, Parallel, Open-source",
abstract = "Time-lapse electrical resistivity tomography (ERT) is finding increased application for remotely monitoring processes occurring in the near subsurface in three-dimensions (i.e. 4D monitoring). However, there are few codes capable of simulating the evolution of subsurface resistivity and corresponding tomographic measurements arising from a particular process, particularly in parallel and with an open source license. Herein we describe and demonstrate an electrical resistivity tomography module for the PFLOTRAN subsurface flow and reactive transport simulation code, named PFLOTRAN-E4D. The PFLOTRAN-E4D module operates in parallel using a dedicated set of compute cores in a master-slave configuration. At each time step, the master processes receives subsurface states from PFLOTRAN, converts those states to bulk electrical conductivity, and instructs the slave processes to simulate a tomographic data set. The resulting multi-physics simulation capability enables accurate feasibility studies for ERT imaging, the identification of the ERT signatures that are unique to a given process, and facilitates the joint inversion of ERT data with hydrogeological data for subsurface characterization. PFLOTRAN-E4D is demonstrated herein using a field study of stage-driven groundwater/river water interaction ERT monitoring along the Columbia River, Washington, USA. Results demonstrate the complex nature of subsurface electrical conductivity changes, in both the saturated and unsaturated zones, arising from river stage fluctuations and associated river water intrusion into the aquifer. The results also demonstrate the sensitivity of surface based ERT measurements to those changes over time. PFLOTRAN-E4D is available with the PFLOTRAN development version with an open-source license at https://bitbucket.org/pflotran/pflotran-dev."
}
@article{NAVADOMINGUEZ2014122,
title = "Assessment of subchannel code ASSERT-PV for flow-distribution predictions",
journal = "Nuclear Engineering and Design",
volume = "275",
pages = "122 - 132",
year = "2014",
issn = "0029-5493",
doi = "https://doi.org/10.1016/j.nucengdes.2014.05.001",
url = "http://www.sciencedirect.com/science/article/pii/S0029549314002623",
author = "A. Nava-Dominguez and Y.F. Rao and G.M. Waddington",
abstract = "This paper reports an assessment of the recently released subchannel code ASSERT-PV 3.2 for the prediction of flow-distribution in fuel bundles, including subchannel void fraction, quality and mass fluxes. Experimental data from open literature and from in-house tests are used to assess the flow-distribution models in ASSERT-PV 3.2. The prediction statistics using the recommended model set of ASSERT-PV 3.2 are compared to those from previous code versions. Separate-effects sensitivity studies are performed to quantify the contribution of each flow-distribution model change or enhancement to the improvement in flow-distribution prediction. The assessment demonstrates significant improvement in the prediction of flow-distribution in horizontal fuel channels containing CANDU bundles."
}
@article{ZACATE2016180,
title = "Stochastic hyperfine interactions modeling library—Version 2",
journal = "Computer Physics Communications",
volume = "199",
pages = "180 - 181",
year = "2016",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2015.10.013",
url = "http://www.sciencedirect.com/science/article/pii/S0010465515003938",
author = "Matthew O. Zacate and William E. Evenson",
keywords = "Perturbed angular correlation spectroscopy, Mössbauer effect, TDPAC, Stochastic models, Hyperfine methods",
abstract = "The stochastic hyperfine interactions modeling library (SHIML) provides a set of routines to assist in the development and application of stochastic models of hyperfine interactions. The library provides routines written in the C programming language that (1) read a text description of a model for fluctuating hyperfine fields, (2) set up the Blume matrix, upon which the evolution operator of the system depends, and (3) find the eigenvalues and eigenvectors of the Blume matrix so that theoretical spectra of experimental techniques that measure hyperfine interactions can be calculated. The optimized vector and matrix operations of the BLAS and LAPACK libraries are utilized. The original version of SHIML constructed and solved Blume matrices for methods that measure hyperfine interactions of nuclear probes in a single spin state. Version 2 provides additional support for methods that measure interactions on two different spin states such as Mössbauer spectroscopy and nuclear resonant scattering of synchrotron radiation. Example codes are provided to illustrate the use of SHIML to (1) generate perturbed angular correlation spectra for the special case of polycrystalline samples when anisotropy terms of higher order than A22 can be neglected and (2) generate Mössbauer spectra for polycrystalline samples for pure dipole or pure quadrupole transitions.
New version program summary
Program title: SHIML Catalogue identifier: AEIF_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIF_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License version 3 with supplemental citation provision No. of lines in distributed program, including test data, etc.: 88510 No. of bytes in distributed program, including test data, etc.: 3311047 Distribution format: tar.gz Programming language: C. Computer: Any. Operating system: LINUX, OS X. RAM: Variable Catalogue identifier of previous version: AEIF_v1_0 Journal reference of previous version: Comput. Phys. Comm. 182(2011)1061 Classification: 7.4. External routines: TAPP [1], BLAS [2], a C-interface to BLAS [3], and LAPACK [4]. Additionally, GSL [3] is needed to compile the example code that simulates Mössbauer spectra. Does the new version supersede the previous version?: No Nature of problem: In condensed matter systems, hyperfine methods such as nuclear magnetic resonance (NMR), Mössbauer effect (ME), muon spin rotation (μSR), and perturbed angular correlation spectroscopy (PAC) measure electromagnetic fields due to electronic and magnetic structure within Angstroms of nuclear probes through the hyperfine interaction. When interactions fluctuate at rates comparable to the time scale of a hyperfine method, there is a loss in signal coherence, and spectra in the time domain are damped while spectra in the frequency domain are broadened. The degree of damping or broadening can be used to determine fluctuation rates, provided that theoretical expressions for spectra can be derived for relevant physical models of the fluctuations. SHIML provides routines to help researchers quickly develop code to incorporate stochastic models of fluctuating hyperfine interactions in calculations of hyperfine spectra. Solution method: Calculations are based on the method for modeling stochastic hyperfine interactions for PAC by Winkler and Gerdau [5]. The method is extended to include other hyperfine methods following the work of Dattagupta [6]. The code provides routines for reading model information from text files, allowing researchers to develop new models quickly without the need to modify computer code for each new model to be considered. Reasons for new version: The original version of the library provided support only for those methods that measure hyperfine interactions on one spin state of the nuclear probe. As such, it excluded important hyperfine methods that measure the interactions on two spin states such as Mössbauer spectroscopy and nuclear resonant scattering of synchrotron radiation. The present version of SHIML provides the necessary support for such double spin-state methods while maintaining backward compatibility for code already developed using the original version. Summary of revisions: Routines now check that values representing nuclear spins are positive integers or positive half-integers. Additional utility functions are provided to make it easier for code developers to calculate Hamiltonians of electric quadrupole interactions. A correction was made to the portion of code responsible for calculating the Blume matrix of single spin-state methods; however, this change will not alter results obtained from single spin-state simulations using version 1 of the library. The remaining revisions support calculations for double spin-state methods. (1) Model-file syntax is expanded in order to allow users to specify different hyperfine interactions for ground and excited spin states and to input isomer shifts. (2) New routines for initialization and for Blume-matrix calculations are included for the double spin-state case. (3) New example code is provided to illustrate how SHIML can be used to simulate Mössbauer spectra of polycrystalline samples for pure dipole or pure quadrupole transitions; background information about the Mössbauer examples can be found in Ref. [7]. Finally, updated software documentation is included in a User’s Guide as a PDF file in the code distribution. Running time: Variable References:[1]M. O. Zacate, The Adjustable Parameter Package, Technical Report 2, CINSAM Grant 2006-R7 (unpublished); available for download at http://tapp.nku.edu/.[2]L. S. Blackford et al., ACM Trans. Math. Soft. 28 (2002) 135; J. Dongarra, International Journal of High Performance Applications and Supercomputing 16 (2002) 1; http://www.netlib.org/blas/.[3]M. Galassi et al., GNU Scientific Library Reference Manual, third edition (2009); available for download at http://www.gnu.org/software/gsl/.[4]E. Anderson et al., LAPACK Users’ Guide, third ed. (Society for Industrial and Applied Mathematics, Philadelphia, PA, 1999); http://www.netlib.org/lapack/.[5]H. Winkler, E. Gerdau, Z. Phys. 262 (1973) 363.[6]S. Dattagupta, Hyperfine Interact. 11 (1981) 77.[7]M. O. Zacate, W. E. Evenson, Hyperfine Interact. 231 (2015) 143."
}
@article{DUPONT201520,
title = "Comparison of different numerical approaches to the 1D sea-ice thermodynamics problem",
journal = "Ocean Modelling",
volume = "87",
pages = "20 - 29",
year = "2015",
issn = "1463-5003",
doi = "https://doi.org/10.1016/j.ocemod.2014.12.006",
url = "http://www.sciencedirect.com/science/article/pii/S1463500314001930",
author = "Frederic Dupont and Martin Vancoppenolle and Louis-Bruno Tremblay and Hendrik Huwald",
keywords = "1D sea-ice thermodynamics, Sigma vertical coordinate, Enthalpy conserving numerical schemes, Mode comparison",
abstract = "The vertical one-dimensional sea-ice thermodynamic problem using the principle of conservation of enthalpy is revisited here using (1) the Bitz and Lipscomb (1999) finite-difference approach (FD), (2) a reformulation of the sigma-level transformation of Huwald et al. (2005b) (FV) and (3) a Finite Element approach also in sigma coordinates (FE). These three formulations are compared in terms of physics, numerics, and performance, in order to identify the best choice for large-scale climate models. The BL99 formulation sequentially treats the diffusion of heat and the changes in the vertical position of the ice-snow layers. In contrast, the FV sigma-level transformation elegantly treats both simultaneously. The original FV formulation suffers however from slow convergence. The convergence can nonetheless be improved significantly with a few simple modifications to the original code. The three formulations are compared following the experimental protocol of the Sea Ice Model Intercomparison Project for ice thermodynamics (SIMIP2). It is found that all formulations converge to the same solution. The FD approach, however, suffers from the added cost of the remapping step at large number of ice layers (we include in the appendix an optimized version of the FD code–written by one of the reviewer–that resolves this issue). Finally the FE formulation results in a sub-surface temperature over-estimation at low resolution, a problem which disappears at high resolution. Hence, only FD and FV are found suitable for climate models."
}
@article{PEREZ2017363,
title = "Axially deformed solution of the Skyrme–Hartree–Fock–Bogolyubov equations using the transformed harmonic oscillator basis (III) hfbtho (v3.00): A new version of the program",
journal = "Computer Physics Communications",
volume = "220",
pages = "363 - 375",
year = "2017",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2017.06.022",
url = "http://www.sciencedirect.com/science/article/pii/S0010465517302047",
author = "R. Navarro Perez and N. Schunck and R.-D. Lasseri and C. Zhang and J. Sarich",
keywords = "Energy density functional theory, Hartree–Fock–Bogoliubov theory, Skyrme force, Gogny force, Axial harmonic oscillator basis, Transformed harmonic oscillator, Collective inertia, Pairing regularization",
abstract = "We describe the new version 3.00 of the code hfbtho that solves the nuclear Hartree–Fock (HF) or Hartree–Fock–Bogolyubov (HFB) problem by using the cylindrical transformed deformed harmonic oscillator basis. In the new version, we have implemented the following features: (i) the full Gogny force in both particle–hole and particle–particle channels, (ii) the calculation of the nuclear collective inertia at the perturbative cranking approximation, (iii) the calculation of fission fragment charge, mass and deformations based on the determination of the neck, (iv) the regularization of zero-range pairing forces, (v) the calculation of localization functions, (vi) a MPI interface for large-scale mass table calculations.
PROGRAM SUMMARY
Program title:hfbtho v3.00 Program Files doi:http://dx.doi.org/10.17632/c5g2f92by3.1 Licensing provisions: GPL v3 Programming language: FORTRAN-95 Journal reference of previous version: M.V. Stoitsov, N. Schunck, M. Kortelainen, N. Michel, H. Nam, E. Olsen, J. Sarich, and S. Wild, Comput. Phys. Commun. 184 (2013). Does the new version supersede the previous one: Yes Summary of revisions: 1. the Gogny force in both particle–hole and particle–particle channels was implemented; 2. the nuclear collective inertia at the perturbative cranking approximation was implemented; 3. fission fragment charge, mass and deformations were implemented based on the determination of the position of the neck between nascent fragments; 4. the regularization method of zero-range pairing forces was implemented; 5. the localization functions of the HFB solution were implemented; 6. a MPI interface for large-scale mass table calculations was implemented. Nature of problem:hfbtho is a physics computer code that is used to model the structure of the nucleus. It is an implementation of the energy density functional (EDF) approach to atomic nuclei, where the energy of the nucleus is obtained by integration over space of some phenomenological energy density, which is itself a functional of the neutron and proton intrinsic densities. In the present version of hfbtho, the energy density derives either from the zero-range Skyrme or the finite-range Gogny effective two-body interaction between nucleons. Nuclear super-fluidity is treated at the Hartree–Fock–Bogolyubov (HFB) approximation. Constraints on the nuclear shape allows probing the potential energy surface of the nucleus as needed e.g., for the description of shape isomers or fission. The implementation of a local scale transformation of the single-particle basis in which the HFB solutions are expanded provide a tool to properly compute the structure of weakly-bound nuclei. Solution method: The program uses the axial Transformed Harmonic Oscillator (THO) single-particle basis to expand quasiparticle wave functions. It iteratively diagonalizes the Hartree–Fock–Bogolyubov Hamiltonian based on generalized Skyrme-like energy densities and zero-range pairing interactions or the finite-range Gogny force until a self-consistent solution is found. A previous version of the program was presented in M.V. Stoitsov, N. Schunck, M. Kortelainen, N. Michel, H. Nam, E. Olsen, J. Sarich, and S. Wild, Comput. Phys. Commun. 184 (2013) 1592–1604 with much of the formalism presented in the original paper M.V. Stoitsov, J. Dobaczewski, W. Nazarewicz, P. Ring, Comput. Phys. Commun. 167 (2005) 43–63. Additional comments: The user must have access to (i) the LAPACK subroutines dsyeevr, dsyevd, dsytrf and dsytri, and their dependencies, which compute eigenvalues and eigenfunctions of real symmetric matrices, (ii) the LAPACK subroutines dgetri and dgetrf, which invert arbitrary real matrices, and (iii) the BLAS routines dcopy, dscal, dgemm and dgemv for double-precision linear algebra (or provide another set of subroutines that can perform such tasks). The BLAS and LAPACK subroutines can be obtained from the Netlib Repository at the University of Tennessee, Knoxville: http://netlib2.cs.utk.edu/."
}
@article{ZHOU2018,
title = "Geomechanical responses during depressurization of hydrate-bearing sediment formation over a long methane gas production period",
journal = "Geomechanics for Energy and the Environment",
year = "2018",
issn = "2352-3808",
doi = "https://doi.org/10.1016/j.gete.2018.12.002",
url = "http://www.sciencedirect.com/science/article/pii/S2352380818300649",
author = "Mingliang Zhou and Kenichi Soga and Koji Yamamoto and Hongwei Huang",
keywords = "Hydrate-bearing sediments, Long period gas production, Geomechanical responses, Stress and strain profiles",
abstract = "The geomechanical behaviour of hydrate-bearing sediments during methane gas production is complex due to the spatial and temporal changes in stress, pore pressure, temperature, and phase change. In order to evaluate the geomechanical risks during methane gas production, it is necessary to understand the thermo-hydro-mechanical (THM) responses in the production region that recovers methane gas from the hydrate-bearing sediments. In this study, a fully coupled THM numerical simulator code was used to examine the reservoir scale field behaviour observed during the gas production trial conducted at the Eastern Nankai Trough, Japan in March, 2013. Using the available field investigation data, history matching of the gas production test was conducted to evaluate the methane gas production process. The fully coupled model allowed examination of the mechanical response using the critical state based constitutive model proposed by Uchida et al. (2012). The model parameters were determined from the results of triaxial compression tests conducted on recovered core samples. Based on the reservoir scale simulation results, this paper investigated the mechanical responses of five selected elements at different locations in the hydrate gas production region. The mechanical responses of hydrate-bearing sediments at specific locations within the production region are related to their hydrate dissociation status, which typically can be divided into before, during and after stages of hydrate dissociation. The 260 days gas production simulation suggests an increase in effective stresses accompanied by shearing deformation, which makes the soil more plastic. Potential geomechanical risks (such as wellbore stability and formation compaction) associated with the observed changes in stress/strain were also identified."
}
@article{ZHANG2016246,
title = "Real-time tracking-by-learning with high-order regularization fusion for big video abstraction",
journal = "Signal Processing",
volume = "124",
pages = "246 - 258",
year = "2016",
note = "Big Data Meets Multimedia Analytics",
issn = "0165-1684",
doi = "https://doi.org/10.1016/j.sigpro.2015.07.021",
url = "http://www.sciencedirect.com/science/article/pii/S016516841500256X",
author = "Peng Zhang and Tao Zhuo and Yanning Zhang and Lei Xie and Dapeng Tao",
keywords = "Video abstraction, Learning, Tracking, High-order, Regularization",
abstract = "Visual tracking is a key technique used by video abstraction to achieve efficient post-analysis for big video surveillance. In order to tackle the problem of constantly changing scenarios during online tracking, additional factors such as motion can be incorporated by utilizing a fusion strategy to improve the final performance. Unfortunately, straightforward output fusion is difficult to be synchronized due to the diversity in model regression. Therefore, a widely cited problem for learning based fusion is to incorporate regularizers for label assignment of unlabeled samples, which is one of the major research focuses on semi-supervised learning. In this paper, a novel tracking strategy based on semi-supervised learning with high order regularization fusion has been proposed. It employs two different types of regularizers to achieve more accurate label assignment based on kernelized confidence prediction and graph-based bi-directional trace from motion. The computation of the proposed tracker takes advantage of the unique feature of circulant matrix in Fourier domain and integral patterns, and thus can be readily implemented for real-time processing, even without any code optimization. Via a dynamic budget maintenance for model updating, the proposed tracking method demonstrated to outperform most state-of-art trackers on challenging benchmark videos with a fixed parameter configuration."
}
@article{MIZUNO201758,
title = "p53 Signaling Pathway Polymorphisms Associated With Emphysematous Changes in Patients With COPD",
journal = "Chest",
volume = "152",
number = "1",
pages = "58 - 69",
year = "2017",
issn = "0012-3692",
doi = "https://doi.org/10.1016/j.chest.2017.03.012",
url = "http://www.sciencedirect.com/science/article/pii/S001236921730377X",
author = "Shiro Mizuno and Takeshi Ishizaki and Maiko Kadowaki and Masaya Akai and Kohei Shiozaki and Masaharu Iguchi and Taku Oikawa and Ken Nakagawa and Kazuhiro Osanai and Hirohisa Toga and Jose Gomez-Arroyo and Donatas Kraskauskas and Carlyne D. Cool and Herman J. Bogaard and Norbert F. Voelkel",
keywords = "cigarette smoke extract, low attenuation area, lung fibroblast, mouse double minute 2 homolog",
abstract = "Background
The p53 signaling pathway may be important for the pathogenesis of emphysematous changes in the lungs of smokers. Polymorphism of p53 at codon 72 is known to affect apoptotic effector proteins, and the polymorphism of mouse double minute 2 homolog (MDM2) single nucleotide polymorphism (SNP)309 is known to increase MDM2 expression. The aim of this study was to assess polymorphisms of the p53 and MDM2 genes in smokers and confirm the role of SNPs in these genes in the pathogenesis of pulmonary emphysema.
Methods
This study included 365 patients with a smoking history, and the polymorphisms of p53 and MDM2 genes were identified. The degree of pulmonary emphysema was determined by means of CT scanning. SNPs, MDM2 mRNA, and p53 protein levels were assessed in human lung tissues from smokers. Plasmids encoding p53 and MDM2 SNPs were used to transfect human lung fibroblasts (HLFs) with or without cigarette smoke extract (CSE), and the effects on cell proliferation and MDM2 promoter activity were measured.
Results
The polymorphisms of the p53 and MDM2 genes were associated with emphysematous changes in the lung and were also associated with p53 protein and MDM2 mRNA expression in the lung tissue samples. Transfection with a p53 gene-coding plasmid regulated HLF proliferation, and the analysis of P2 promoter activity in MDM2 SNP309-coding HLFs showed the promoter activity was altered by CSE.
Conclusions
Our data demonstrated that p53 and MDM2 gene polymorphisms are associated with apoptotic signaling and smoking-related emphysematous changes in lungs from smokers."
}
@article{XIE201681,
title = "Cross-Modal Self-Taught Hashing for large-scale image retrieval",
journal = "Signal Processing",
volume = "124",
pages = "81 - 92",
year = "2016",
note = "Big Data Meets Multimedia Analytics",
issn = "0165-1684",
doi = "https://doi.org/10.1016/j.sigpro.2015.10.010",
url = "http://www.sciencedirect.com/science/article/pii/S0165168415003539",
author = "Liang Xie and Lei Zhu and Peng Pan and Yansheng Lu",
keywords = "Image retreival, Cross-modal hashing, Self-taught learning, Semantic correlation",
abstract = "Cross-modal hashing integrates the advantages of traditional cross-modal retrieval and hashing, it can solve large-scale cross-modal retrieval effectively and efficiently. However, existing cross-modal hashing methods rely on either labeled training data, or lack semantic analysis. In this paper, we propose Cross-Modal Self-Taught Hashing (CMSTH) for large-scale cross-modal and unimodal image retrieval. CMSTH can effectively capture the semantic correlation from unlabeled training data. Its learning process contains three steps: first we propose Hierarchical Multi-Modal Topic Learning (HMMTL) to detect multi-modal topics with semantic information. Then we use Robust Matrix Factorization (RMF) to transfer the multi-modal topics to hash codes which are more suited to quantization, and these codes form a unified hash space. Finally we learn hash functions to project all modalities into the unified hash space. Experimental results on two web image datasets demonstrate the effectiveness of CMSTH compared to representative cross-modal and unimodal hashing methods."
}
@article{SZILAGYI2016914,
title = "Testing the generalized complementary relationship of evaporation with continental-scale long-term water-balance data",
journal = "Journal of Hydrology",
volume = "540",
pages = "914 - 922",
year = "2016",
issn = "0022-1694",
doi = "https://doi.org/10.1016/j.jhydrol.2016.07.001",
url = "http://www.sciencedirect.com/science/article/pii/S0022169416304310",
author = "Jozsef Szilagyi and Richard Crago and Russell J. Qualls",
keywords = "Complementary relationship, Evaporation, Water balance",
abstract = "The original and revised versions of the generalized complementary relationship (GCR) of evaporation (ET) were tested with six-digit Hydrologic Unit Code (HUC6) level long-term (1981–2010) water-balance data (sample size of 334). The two versions of the GCR were calibrated with Parameter-Elevation Regressions on Independent Slopes Model (PRISM) mean annual precipitation (P) data and validated against water-balance ET (ETwb) as the difference of mean annual HUC6-averaged P and United States Geological Survey HUC6 runoff (Q) rates. The original GCR overestimates P in about 18% of the PRISM grid points covering the contiguous United States in contrast with 12% of the revised version. With HUC6-averaged data the original version has a bias of −25mmyr−1 vs the revised version’s −17mmyr−1, and it tends to more significantly underestimate ETwb at high values than the revised one (slope of the best fit line is 0.78 vs 0.91). At the same time it slightly outperforms the revised version in terms of the linear correlation coefficient (0.94 vs 0.93) and the root-mean-square error (90 vs 92mmyr−1)."
}
@article{RAHIMIAN2015S146,
title = "BinComp: A stratified approach to compiler provenance Attribution",
journal = "Digital Investigation",
volume = "14",
pages = "S146 - S155",
year = "2015",
note = "The Proceedings of the Fifteenth Annual DFRWS Conference",
issn = "1742-2876",
doi = "https://doi.org/10.1016/j.diin.2015.05.015",
url = "http://www.sciencedirect.com/science/article/pii/S1742287615000602",
author = "Ashkan Rahimian and Paria Shirani and Saed Alrbaee and Lingyu Wang and Mourad Debbabi",
keywords = "Compiler provenance, Reverse engineering, Binary program analysis, Digital forensics, Programming analysis",
abstract = "Compiler provenance encompasses numerous pieces of information, such as the compiler family, compiler version, optimization level, and compiler-related functions. The extraction of such information is imperative for various binary analysis applications, such as function fingerprinting, clone detection, and authorship attribution. It is thus important to develop an efficient and automated approach for extracting compiler provenance. In this study, we present BinComp, a practical approach which, analyzes the syntax, structure, and semantics of disassembled functions to extract compiler provenance. BinComp has a stratified architecture with three layers. The first layer applies a supervised compilation process to a set of known programs to model the default code transformation of compilers. The second layer employs an intersection process that disassembles functions across compiled binaries to extract statistical features (e.g., numerical values) from common compiler/linker-inserted functions. This layer labels the compiler-related functions. The third layer extracts semantic features from the labeled compiler-related functions to identify the compiler version and the optimization level. Our experimental results demonstrate that BinComp is efficient in terms of both computational resources and time."
}
@article{ABUAISHA201610,
title = "Influence of hydraulic fracturing on impedance and efficiency of thermal recovery from HDR reservoirs",
journal = "Geomechanics for Energy and the Environment",
volume = "7",
pages = "10 - 25",
year = "2016",
issn = "2352-3808",
doi = "https://doi.org/10.1016/j.gete.2016.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S2352380816300016",
author = "Murad AbuAisha and Benjamin Loret",
keywords = "Hydraulic fracturing, Thermo-poroelastic framework, Thermal recovery, Boundary conditions, Viscosity-temperature change, Impedance and efficiency",
abstract = "Impedance and efficiency are key characteristics in regard to the economic viability of geothermal sites. The influence of hydraulic fracturing process on the efficiency of thermal recovery from HDR reservoirs is addressed in a totally coupled thermo-poroelastic framework. A fracturing model (HFM) is integrated into a domestic Fortran 90 finite element code. At any time and any geometrical point, the state of fracture is embodied in a fabric: the later includes both the actual fracture length and the actual fracture width in all directions of space. The local current anisotropic permeability tensor, which describes the evolving hydraulic connectivity of the fractured medium, is obtained by directional integration of the updated fracture fabric. A modified version of the model which accounts for the effect of deviatoric stresses on the fracturing criterion is shown to have stronger effects on the enhancement of the permeability. To gain confidence in the numerical approach, simulations are correlated to field data of the Soultz-sous-Forêts geothermal site that are available in the early times of the injection process. Cooling facilitates significantly the fracturing process close to the injection well. The model is next applied to simulate a hydraulic fracturing test over a long period of time. Hydraulic fracturing is shown to decrease the impedance of the reservoir, but it reduces the duration of an efficient exploitation. The nature of the mechanical, hydraulic and thermal boundary conditions is also investigated. The composition of the working fluid and the change of its viscosity with temperature are shown to affect fluid and heat transports in the poroelastic medium and hence the efficiency of hydraulic fracturing."
}
@article{STAUB20131792,
title = "SARAH 3.2: Dirac gauginos, UFO output, and more",
journal = "Computer Physics Communications",
volume = "184",
number = "7",
pages = "1792 - 1809",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.02.019",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513000763",
author = "Florian Staub",
keywords = ", MadGraph, Model files, Supersymmetry, Dirac gauginos",
abstract = "SARAH is a Mathematica package optimized for the fast, efficient and precise study of supersymmetric models beyond the MSSM: a new model can be defined in a short form and all vertices are derived. This allows SARAH to create model files for FeynArts/FormCalc, CalcHep/CompHep and WHIZARD/O’Mega. The newest version of SARAH now provides the possibility to create model files in the UFO format which is supported by MadGraph 5, MadAnalysis 5, GoSam, and soon by Herwig++. Furthermore, SARAH also calculates the mass matrices, RGEs and 1-loop corrections to the mass spectrum. This information is used to write source code for SPheno in order to create a precision spectrum generator for the given model. This spectrum-generator–generator functionality as well as the output of WHIZARD and CalcHep model files has seen further improvement in this version. Also models including Dirac gauginos are supported with the new version of SARAH, and additional checks for the consistency of the implementation of new models have been created.
Program summary
Program title:SARAH Catalogue identifier: AEIB_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIB_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 322411 No. of bytes in distributed program, including test data, etc.: 3629206 Distribution format: tar.gz Programming language: Mathematica. Computer: All for which Mathematica is available. Operating system: All for which Mathematica is available. Classification: 11.1, 11.6. Catalogue identifier of previous version: AEIB_v1_0 Journal reference of previous version: Comput. Phys. Comm. 182 (2011) 808 Does the new version supersede the previous version?: Yes, the new version includes all known features of the previous version but also provides the new features mentioned below. Nature of problem: To use Madgraph for new models it is necessary to provide the corresponding model files which include all information about the interactions of the model. However, the derivation of the vertices for a given model and putting those into model files which can be used with Madgraph is usually very time consuming. Dirac gauginos are not present in the minimal supersymmetric standard model (MSSM) or many extensions of it. Dirac mass terms for vector superfields lead to new structures in the supersymmetric (SUSY) Lagrangian (bilinear mass term between gaugino and matter fermion as well as new D-terms) and modify also the SUSY renormalization group equations (RGEs). The Dirac character of gauginos can change the collider phenomenology. In addition, they come with an extended Higgs sector for which a precise calculation of the 1-loop masses has not happened so far. Solution method: SARAH calculates the complete Lagrangian for a given model whose gauge sector can be any direct product of SU(N) gauge groups. The chiral superfields can transform as any, irreducible representation with respect to these gauge groups and it is possible to handle an arbitrary number of symmetry breakings or particle rotations. Also the gauge fixing is automatically added. Using this information, SARAH derives all vertices for a model. These vertices can be exported to model files in the UFO which is supported by Madgraph and other codes like GoSam, MadAnalysis or ALOHA. The user can also study models with Dirac gauginos. In that case SARAH includes all possible terms in the Lagrangian stemming from the new structures and can also calculate the RGEs. The entire impact of these terms is then taken into account in the output of SARAH to UFO, CalcHep, WHIZARD, FeynArts and SPheno. Reasons for new version: SARAH provides, with this version, the possibility of creating model files in the UFO format. The UFO format is supposed to become a standard format for model files which should be supported by many different tools in the future. Also models with Dirac gauginos were not supported in earlier versions. Summary of revisions: Support of models with Dirac gauginos. Output of model files in the UFO format, speed improvement in the output of WHIZARD model files, CalcHep output supports the internal diagonalization of mass matrices, output of control files for LHPC spectrum plotter, support of generalized PDG numbering scheme PDG.IX, improvement of the calculation of the decay widths and branching ratios with SPheno, the calculation of new low energy observables are added to the SPheno output, the handling of gauge fixing terms has been significantly simplified. Restrictions: SARAH can only derive the Lagrangian in an automatized way for N=1 SUSY models, but not for those with more SUSY generators. Furthermore, SARAH supports only renormalizable operators in the output of model files in the UFO format and also for CalcHep, FeynArts and WHIZARD. Also color sextets are not yet included in the model files for Monte Carlo tools. Dimension 5 operators are only supported in the calculation of the RGEs and mass matrices. Unusual features: SARAH does not need the Lagrangian of a model as input to calculate the vertices. The gauge structure, particle and content and superpotential as well as rotations stemming from gauge symmetry breaking are sufficient. All further information is derived by SARAH on its own. Therefore, the model files are very short and the implementation of new models is fast and easy. In addition, the implementation of a model can be checked for physical and formal consistency. In addition, SARAH can generate Fortran code for a full 1-loop analysis of the mass spectrum in the context for Dirac gauginos. Running time: Measured CPU time for the evaluation of the MSSM using a Lenovo Thinkpad X220 with i7 processor (2.53 GHz). Calculating the complete Lagrangian: 9 s. Calculating all vertices: 51 s. Output of the UFO model files: 49 s."
}
@article{THRAMBOULIDIS201592,
title = "A cyber–physical system-based approach for industrial automation systems",
journal = "Computers in Industry",
volume = "72",
pages = "92 - 102",
year = "2015",
issn = "0166-3615",
doi = "https://doi.org/10.1016/j.compind.2015.04.006",
url = "http://www.sciencedirect.com/science/article/pii/S0166361515000962",
author = "Kleanthis Thramboulidis",
keywords = "Industrial automation systems, Cyber–physical systems, System-based approach, Mechatronics, UML/SysML, IEC 61131, Java, IoT",
abstract = "Industrial automation systems (IASs) are commonly developed using the languages defined by the IEC 61131 standard and are executed on programmable logic controllers (PLCs). Their software part is commonly considered only after the development and integration of mechanics and electronics. However, this approach narrows the solution space for software; thus, it is considered inadequate to address the complexity of today's systems. In this paper, we adopt a system-based approach for the development of IASs. Based on this, the UML model of the software part of the system is extracted from the SysML system model and it is then refined to get the implementation code. Two implementation alternatives are considered to exploit both PLCs and the recent deluge of embedded boards in the market. For PLC targets, the new version of IEC 61131 that supports object-orientation is adopted, while Java is used for embedded boards. The case study used to illustrate our approach was developed as a lab exercise, which aims to introduce to students a number of technologies used to address challenges in the domain of cyber–physical systems and highlights the role of the Internet of Things (IoT) as a glue for their cyber interfaces."
}
@article{RAJTER2018534,
title = "Selection and paucity of phylogenetic signal challenge the utility of alpha-tubulin in reconstruction of evolutionary history of free-living litostomateans (Protista, Ciliophora)",
journal = "Molecular Phylogenetics and Evolution",
volume = "127",
pages = "534 - 544",
year = "2018",
issn = "1055-7903",
doi = "https://doi.org/10.1016/j.ympev.2018.05.011",
url = "http://www.sciencedirect.com/science/article/pii/S1055790318301611",
author = "Ľubomír Rajter and Peter Vďačný",
keywords = "D/d ratio, Haptoria, Molecular homoplasies, Rhynchostomatia, Ribosomal RNAs, Substitution rate",
abstract = "The class Litostomatea represents a highly diverse but monophyletic group, uniting both free-living and endosymbiotic ciliates. Ribosomal RNA genes and ITS-region sequences helped to recognize and define the main litostomatean lineages, but did not provide enough phylogenetic signal to unambiguously resolve their interrelationships. In this study, we attempted to improve the resolution among main free-living predatory lineages by adding the gene coding for alpha-tubulin. However, our phylogenetic analyses challenged the performance of alpha-tubulin in reconstruction of evolutionary history of free-living litostomateans. We identified several mutually interconnected problems associated with the ciliate alpha-tubulin gene: the paucity of phylogenetic signal, molecular homoplasies and non-neutral evolution. Positive selection may generate molecular homoplasies (parallel evolution), while negative selection may cause a small number of changes and hence little phylogenetic informativness. Both problems were encountered in nucleotide and amino acid alpha-tubulin alignments, indicating an action of various selective pressures. Taking into account the involvement of alpha-tubulin in many essential biological processes, this protein could be so strongly affected by purifying selection that it even might have become an inappropriate molecular marker for reconstruction of phylogenetic relationships. Therefore, a great caution should be paid when tubulin genes are included in phylogenetic and/or phylogenomic analyses."
}
@article{HOWARD201866,
title = "Contrasting prediction methods for early warning systems at undergraduate level",
journal = "The Internet and Higher Education",
volume = "37",
pages = "66 - 75",
year = "2018",
issn = "1096-7516",
doi = "https://doi.org/10.1016/j.iheduc.2018.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S1096751617303974",
author = "Emma Howard and Maria Meehan and Andrew Parnell",
keywords = "Learning analytics, Early warning systems, Undergraduate education, Prediction modelling",
abstract = "Recent studies have provided evidence in favour of adopting early warning systems as a means of identifying at-risk students. Our study examines eight prediction methods, and investigates the optimal time in a course to apply such a system. We present findings from a statistics university course which has weekly continuous assessment and a large proportion of resources on the Learning Management System Blackboard. We identify weeks 5–6 (half way through the semester) as an optimal time to implement an early warning system, as it allows time for the students to make changes to their study patterns while retaining reasonable prediction accuracy. Using detailed variables, clustering and our final prediction method of BART (Bayesian Additive Regressive Trees) we can predict students' final mark by week 6 based on mean absolute error to 6.5 percentage points. We provide our R code for implementation of the prediction methods used in a GitHub repository11Abbreviations: Bayesian Additive Regressive Trees (BART); Random Forests (RF); Principal Components Regression (PCR); Multivariate Adaptive Regression Splines (Splines); K-Nearest Neighbours (KNN); Neural Networks (NN) and; Support Vector Machine (SVM). Abbreviations: Bayesian Additive Regressive Trees (BART); Random Forests (RF); Principal Components Regression (PCR); Multivariate Adaptive Regression Splines (Splines); K-Nearest Neighbours (KNN); Neural Networks (NN) and; Support Vector Machine (SVM)"
}
@article{PATTANAIK2018594,
title = "The changing face of anaphylaxis in adults and adolescents",
journal = "Annals of Allergy, Asthma & Immunology",
volume = "121",
number = "5",
pages = "594 - 597",
year = "2018",
issn = "1081-1206",
doi = "https://doi.org/10.1016/j.anai.2018.07.017",
url = "http://www.sciencedirect.com/science/article/pii/S1081120618305805",
author = "Debendra Pattanaik and Phil Lieberman and Jay Lieberman and Thanai Pongdee and Alexandria Tran Keene",
abstract = "Background
Our institution has published serial studies of adults and adolescents with anaphylactic events. The first series was published in 1993 and the last was published in 2006. It was our perception that the nature of anaphylactic episodes had changed over the 2 decades since the last review.
Objective
To determine whether the etiologies and presentations of anaphylaxis have changed during the past decade in our population.
Methods
Patient charts were identified based on International Classification of Diseases, Ninth Revision codes for anaphylactic shock. Charts identified were analyzed for clinical symptoms reported, comorbidities, etiology, investigative testing, and subsequent treatment. These cases were categorized as definitive, probable, or idiopathic based on history and results from testing, similar to our prior reports.
Results
We identified 281 possible cases, of which 218 met criteria for anaphylaxis. Of these cases, median age was 42 years (range 9–78) and 64% were female. In the review of cases, 85 (39%) were determined to have a definitive etiology, 57 were determined to have a probable etiology (26%), and 76 (35%) were idiopathic. Interestingly, of those with a definitive cause, the most common etiology identified was galactose-α-1,3-galactose, accounting for 28 cases (33%). Foods were the second leading cause, accounting for 24 cases (28%).
Conclusion
In this follow-up report on anaphylaxis etiology from a single center, the most common etiology was galactose-α-1,3-galactose. This differs greatly from prior reports from our center. Interestingly, the percentage of cases attributed to idiopathic anaphylaxis decreased from 59% in our previous report to 35% in the present report, which could largely be explained by the number of galactose-α-1,3-galactose cases."
}
@article{PEKTAS201791,
title = "Classification of malware families based on runtime behaviors",
journal = "Journal of Information Security and Applications",
volume = "37",
pages = "91 - 100",
year = "2017",
issn = "2214-2126",
doi = "https://doi.org/10.1016/j.jisa.2017.10.005",
url = "http://www.sciencedirect.com/science/article/pii/S2214212617301643",
author = "Abdurrahman Pektaş and Tankut Acarman",
keywords = "Behavior analysis, Dynamic analysis, Malware classification, Machine learning",
abstract = "Classification of malware samples plays a crucial role in building and maintaining security. Design of a malware classification system capable of supporting a large set of samples and adaptable to model changes at runtime is required to identify the high number of malware variants. In this paper, file system, network, registry activities observed during the execution traces and n-gram modeling over API-call sequences are used to represent behavior based features of a malware. We present a methodology to build the feature vector by using run-time behaviors by applying online machine learning algorithms for classification of malware samples in a distributed and scalable architecture. To validate the effectiveness and scalability, we evaluate our method on 17,900 recent malign codes such as viruses, trojans, backdoors, worms. Our experimental results show that the presented malware classification’s training and testing accuracy is reached at 94% and 92.5%, respectively."
}
@article{HAN201914,
title = "A photometric study of the contact binary AR Bootis",
journal = "New Astronomy",
volume = "66",
pages = "14 - 19",
year = "2019",
issn = "1384-1076",
doi = "https://doi.org/10.1016/j.newast.2018.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S138410761830143X",
author = "Quan-wang Han and Li-fang Li and Xiao-yang Kong and Jian-sha Li and Deng-kai Jiang",
keywords = "Stars: Binaries: Close, Stars: Binaries: Eclipsing, Stars: Individuals: AR Bootis",
abstract = "New complete BVR light curves of the contact binary AR Bootis (AR Boo) are presented. They are analyzed through the 2013 version of the Wilson–Devinney (W–D) code. Our new photometric solution shows that AR Boo is a W-subtype W UMa contact binary system with a mass ratio of q=m2/m1=1.865 and a fill-out factor of 12.7%. The changes in the orbital period of AR Boo are analyzed. It is found that the orbital period of this binary has a continuous increase at a rate of 2.04×10−7daysyr−1, together with a cyclic variation (with a period of 39.05 yr and an amplitude of 0.0154 days). We discussed the mechanism responsible for the changes in the orbital period of AR Boo. It is found that the long-term increase in its orbital period is caused by mass transfer from the less massive component to the more massive one, and the periodic variation in its orbital period might be a result of the light-travel-time effect owing to a third body or the magnetic activity in the more massive component of AR Boo."
}
@article{HOLLIS2014695,
title = "tau: A 1D radiative transfer code for transmission spectroscopy of extrasolar planet atmospheres",
journal = "Computer Physics Communications",
volume = "185",
number = "2",
pages = "695",
year = "2014",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.09.017",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513003184",
author = "M.D.J. Hollis and M. Tessenyi and G. Tinetti",
keywords = "Extrasolar planets, Atmospheric characterization, Spectroscopy, Radiative transfer",
abstract = "The tau code is a 1D line-by-line radiative transfer code, which is generally applicable for modeling transmission spectra of close-in extrasolar planets. The inputs are the assumed temperature–pressure profile of the planetary atmosphere, the continuum absorption coefficients and the absorption cross-sections for the trace molecular absorbers present in the model, as well as the fundamental system parameters taken from the published literature. The program then calculates the optical path through the planetary atmosphere of the radiation from the host star, and quantifies the absorption due to the modeled composition in a transmission spectrum of transit depth as a function of wavelength. The code is written in C++, parallelized using OpenMP, and is available for public download and use from http://www.ucl.ac.uk/exoplanets/.
New version program summary
Program title: tau Catalogue identifier: AEPN_v1_1 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEPN_v1_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 56203 No. of bytes in distributed program, including test data, etc.: 1679104 Distribution format: tar.gz Programming language: C++. Computer: Non-specific. Operating system: Linux, Mac. Has the code been vectorized or parallelized?: Yes RAM: 300 MB Classification: 1.3. External routines: OpenMP (http://openmp.org/wp) Does the new version supersede the previous version?: Yes Nature of problem: Calculation of molecular absorption, and hence transit depths, as a function of wavelength for stellar radiation passing through planetary atmospheres. Solution method: Line-by-line calculation of wavelength-dependent optical depths using absorption cross-sections for various trace molecular absorbers. Reasons for new version: Bug found in previous version whereby some values, defined as floats, took values too small to be represented by this datatype, and so were set to zero. Summary of revisions: All ‘float’ datatypes changed to ‘double’. Additional comments: The distribution file contains, •./doc/: readme and User Guide.•./run/: sample input data files•./out/sample_tau_output.dat: sample output file for run mode ‘9’ (H2O at χ=10−5 for HD189733b at Tatm=1500K).Running time: From 0.5 to 500 s, depending on run parameters."
}
@article{MORAN2014109,
title = "Mental disorder and criminality in Canada",
journal = "International Journal of Law and Psychiatry",
volume = "37",
number = "1",
pages = "109 - 116",
year = "2014",
note = "Historical Perspectives on Forensic Psychiatry",
issn = "0160-2527",
doi = "https://doi.org/10.1016/j.ijlp.2013.09.010",
url = "http://www.sciencedirect.com/science/article/pii/S016025271300085X",
author = "James E. Moran",
keywords = "Psychiatry, Criminal law, History, Canada, Criminal code, Reform",
abstract = "This article examines the relationship between mental disorder and criminality in Canada from the colonial period to the landmark 1992 Mental Disorder Amendments that followed the passing of Bill C-30. The history of this relationship has been shaped by longstanding formal and informal systems of social regulation, by the contests of federal–provincial jurisdiction, by changing trends in the legal and psychiatric professions, and by amendments to the federal Criminal Code. A study of these longer-term features demonstrates that there has been no linear path of progress in Canada's response to mentally unwell offenders. Those caught in the web of crime and mental disorder have been cast and recast over the past 150years by the changing dynamics of criminal law, psychiatry, and politics. A long historical perspective suggests how earlier and more contemporary struggles over mental disorder and criminality are connected, how these struggles are bound by historical circumstance, and how a few relatively progressive historical moments emerging from these struggles might be recovered, and theorized to advantage."
}
@article{AGOSTINI2017187,
title = "Efficient interactive decision-making framework for robotic applications",
journal = "Artificial Intelligence",
volume = "247",
pages = "187 - 212",
year = "2017",
note = "Special Issue on AI and Robotics",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2015.04.004",
url = "http://www.sciencedirect.com/science/article/pii/S0004370215000661",
author = "Alejandro Agostini and Carme Torras and Florentin Wörgötter",
keywords = "Decision making, Human-like task, Logic-based planning, Online learning, Robotics",
abstract = "The inclusion of robots in our society is imminent, such as service robots. Robots are now capable of reliably manipulating objects in our daily lives but only when combined with artificial intelligence (AI) techniques for planning and decision-making, which allow a machine to determine how a task can be completed successfully. To perform decision making, AI planning methods use a set of planning operators to code the state changes in the environment produced by a robotic action. Given a specific goal, the planner then searches for the best sequence of planning operators, i.e., the best plan that leads through the state space to satisfy the goal. In principle, planning operators can be hand-coded, but this is impractical for applications that involve many possible state transitions. An alternative is to learn them automatically from experience, which is most efficient when there is a human teacher. In this study, we propose a simple and efficient decision-making framework for this purpose. The robot executes its plan in a step-wise manner and any planning impasse produced by missing operators is resolved online by asking a human teacher for the next action to execute. Based on the observed state transitions, this approach rapidly generates the missing operators by evaluating the relevance of several cause–effect alternatives in parallel using a probability estimate, which compensates for the high uncertainty that is inherent when learning from a small number of samples. We evaluated the validity of our approach in simulated and real environments, where it was benchmarked against previous methods. Humans learn in the same incremental manner, so we consider that our approach may be a better alternative to existing learning paradigms, which require offline learning, a significant amount of previous knowledge, or a large number of samples."
}
@article{SCHEFFER2013S22,
title = "All Clear: Improving the Code Process",
journal = "Journal of Obstetric, Gynecologic & Neonatal Nursing",
volume = "42",
pages = "S22",
year = "2013",
note = "Proceedings of the AWHONN 2013 Convention",
issn = "0884-2175",
doi = "https://doi.org/10.1111/1552-6909.12078",
url = "http://www.sciencedirect.com/science/article/pii/S0884217515313496",
author = "Kristin Scheffer and Christine Renfro",
keywords = "simulation, maternal code, safety, anaphylactic syndrome of pregnancy, competencies",
abstract = "Childbearing Poster Presentation
Purpose for the Program
The simulation facilitators at Baylor University Medical Center recognized the need to create best practice surrounding maternal codes; therefore, a new process for annual Mock Code was developed that used the concept of simulation‐based learning.
Proposed Change
For the past 5 years our facility used simulation‐based learning that focused on high‐risk obstetric events; however, we still performed Mock Codes on the unit in a didactic type format with skills check‐off. Following a maternal code on our unit, we incorporated simulation concepts with a hands‐on approach, which focused on documentation, communication, full use of the defibrillator, and crash cart knowledge.
Implementation, Outcomes, and Evaluation
Because maternal codes are rare, staff were not comfortable with the different aspects of the defibrillator/automated external defibrillator (AED). In addition, staff were not accustomed to using the code documentation sheet. The Perinatal Simulation Team developed an anaphylactic syndrome of pregnancy or amniotic fluid embolism (AFE) scenario and recorded it. Using this video, we were able to debrief our entire staff regarding the processes they viewed. Initially, we made them serve as the documenter and record what they observed as the video played. We then had them discuss and compare documentation to gain an understanding of the importance of speaking up to ensure all necessary information is gathered during a code. Then we reviewed the tape again, this time debriefing the scenario and addressing teamwork, communication, and technical skills. Following this discussion, we ran a simulation scenario that mimicked what they just watched. They had to perform CPR, place the pads from the defibrillator/AED and deliver shock as advised, print the EKG strip from the defibrillator, change from AED mode to defibrillator mode, increase joules as requested, draw up code medications, retrieve any necessary equipment or supplies from the crash cart, prepare for a STAT bedside delivery, document the code events, and communicate appropriately to the medical team. Our training increased staff confidence, knowledge of the crash cart and defibrillator, and awareness of communication needs during the maternal code.
Implications for Nursing Practice
When applying the science of simulation by creating a unique learning experience, nurses’ confidence level, knowledge, and skills in recognizing the signs and symptoms of AFE can be improved. The use of simulation also improves the nurses’ ability to use the defibrillator, draw up code medications, serve as documenter, and effectively use closed‐loop communication in a code situation."
}
@article{LU20132618,
title = "AFMPB: An adaptive fast multipole Poisson–Boltzmann solver for calculating electrostatics in biomolecular systems",
journal = "Computer Physics Communications",
volume = "184",
number = "11",
pages = "2618 - 2619",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2013.05.012",
url = "http://www.sciencedirect.com/science/article/pii/S0010465513001720",
author = "Benzhuo Lu and Xiaolin Cheng and Jingfang Huang and J. Andrew McCammon",
keywords = "Poisson–Boltzmann equation, Boundary integral equation, Node-patch method, Krylov subspace methods, Fast multipole methods, Diagonal translations",
abstract = "A Fortran program package is introduced for rapid evaluation of the electrostatic potentials and forces in biomolecular systems modeled by the linearized Poisson–Boltzmann equation. The numerical solver utilizes a well-conditioned boundary integral equation (BIE) formulation, a node-patch discretization scheme, a Krylov subspace iterative solver package with reverse communication protocols, and an adaptive new version of the fast multipole method in which the exponential expansions are used to diagonalize the multipole-to-local translations. The program and its full description, as well as several closely related libraries and utility tools are available at http://lsec.cc.ac.cn/~lubz/afmpb.html and a mirror site at http://mccammon.ucsd.edu/. This paper is a brief summary of the program: the algorithms, the implementation and the usage.
New version program summary
Program title: AFMPB Catalogue identifier: AEGB_v1_1 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEGB_v1_1.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 2 No. of lines in distributed program, including test data, etc.: 440784 No. of bytes in distributed program, including test data, etc.: 8187139 Distribution format: tar.gz Programming language: Fortran Computer: Any Operating system: Any RAM: Depends on the size of the discretized biomolecular system Classification: 3 External routines: Pre- and post-processing tools are required for generating the boundary elements and for visualization. Users can use MSMS (http://www.scripps.edu/sanner/html/msmshome.html) for pre-processing, and VMD (http://www.ks.uiuc.edu/Research/vmd/) for visualization. Sub-programs included: An iterative Krylov subspace solvers package from SPARSKIT by Yousef Saad (http://www-users.cs.umn.edu/saad/software/SPARSKIT/sparskit.html), and the fast multipole methods subroutines from FMMSuite (http://www.fastmultipole.org/). Catalogue identifier of previous version: AEGB_v1_0 Journal reference of previous version: Comput. Phys. Comm. 181 (2010) 1150 Does the new version supersede the previous version?: Yes Nature of problem: Numerical solution of the linearized Poisson–Boltzmann equation that describes electrostatic interactions of molecular systems in ionic solutions. Solution method: A novel node-patch scheme is used to discretize the well-conditioned boundary integral equation formulation of the linearized Poisson–Boltzmann equation. Various Krylov subspace solvers can be subsequently applied to solve the resulting linear system, with a bounded number of iterations independent of the number of discretized unknowns. The matrix–vector multiplication at each iteration is accelerated by the adaptive new versions of fast multipole methods. The AFMPB solver requires other stand-alone pre-processing tools for boundary mesh generation, post-processing tools for data analysis and visualization, and can be conveniently coupled with different time stepping methods for dynamics simulation. Reasons for new version: Some bugs are fixed in the new version. Summary of revisions:•The type definition of ippt1 in line 88 of FBEM/bempb.f and line 32 of FBEM/closecoef.f is changed from real *8 to integer*4, and a similar change is made for ippt in line 105 of FBEM/solvpb.f and in line 32 of FBEM/closecoef.f.•In FBEM/elmgeom.f, line 239 “ELSEIF (meshfmt.EQ.1.OR. meshfmt.EQ. 4.OR. meshfmt.EQ.5) THEN” is changed to “ELSEIF (meshfmt.EQ.1.OR. meshfmt.EQ. 4) THEN”, line 478 “KJ=IDFCL(K)+J-1” is changed to “i=IDFCL(K)+J-1”, line 479 “KJ=NE(KJ)” is changed to “KJ=NE(i)”, line 480 “KJ1=NE(KJ+1)” is changed to “KJ1=NE(i+1)”, and line 647 “     STOP” is changed to “c     STOP”.•Five subroutines in FMM part (syukadap.f, syukdn.f, slapadap.f, slapdn.f, and treeadap.f) are substituted with the new ones in the new version.Restrictions: Only three or six significant digits options are provided in this version. Unusual features: Most of the codes are in Fortran77 style. Memory allocation functions from Fortran90 and above are used in a few subroutines. Additional comments: The current version of the codes is designed and written for single core/processor desktop machines. Check http://lsec.cc.ac.cn/lubz/afmpb.html for updates and changes. Running time: The running time varies with the number of discretized elements (N) in the system and their distributions. In most cases, it scales linearly as a function of N."
}
@article{FRISTON2015129,
title = "Active inference, communication and hermeneutics",
journal = "Cortex",
volume = "68",
pages = "129 - 143",
year = "2015",
note = "Special issue: Prediction in speech and language processing",
issn = "0010-9452",
doi = "https://doi.org/10.1016/j.cortex.2015.03.025",
url = "http://www.sciencedirect.com/science/article/pii/S0010945215001240",
author = "Karl J. Friston and Christopher D. Frith",
keywords = "Communication, Neuronal, Hermeneutics, Theory of mind, Active inference, Predictive coding, Bayesian, Synchronisation of chaos",
abstract = "Hermeneutics refers to interpretation and translation of text (typically ancient scriptures) but also applies to verbal and non-verbal communication. In a psychological setting it nicely frames the problem of inferring the intended content of a communication. In this paper, we offer a solution to the problem of neural hermeneutics based upon active inference. In active inference, action fulfils predictions about how we will behave (e.g., predicting we will speak). Crucially, these predictions can be used to predict both self and others – during speaking and listening respectively. Active inference mandates the suppression of prediction errors by updating an internal model that generates predictions – both at fast timescales (through perceptual inference) and slower timescales (through perceptual learning). If two agents adopt the same model, then – in principle – they can predict each other and minimise their mutual prediction errors. Heuristically, this ensures they are singing from the same hymn sheet. This paper builds upon recent work on active inference and communication to illustrate perceptual learning using simulated birdsongs. Our focus here is the neural hermeneutics implicit in learning, where communication facilitates long-term changes in generative models that are trying to predict each other. In other words, communication induces perceptual learning and enables others to (literally) change our minds and vice versa."
}
@article{CURTIS20182560,
title = "Resection Arthroplasty Compared With Total Hip Arthroplasty in Treating Chronic Hip Pain of Patients With a History of Substance Abuse",
journal = "The Journal of Arthroplasty",
volume = "33",
number = "8",
pages = "2560 - 2565",
year = "2018",
issn = "0883-5403",
doi = "https://doi.org/10.1016/j.arth.2018.03.016",
url = "http://www.sciencedirect.com/science/article/pii/S0883540318302596",
author = "William Curtis and Meir Marmor",
keywords = "resection arthroplasty, substance abuse, hip arthroplasty, pain management, sobriety pathway, hip arthritis",
abstract = "Background
Retrospective comparison of surgical management of severe hip pain in patients with a history of substance abuse treated by modified Girdlestone resection arthroplasty (RA) vs delayed total hip arthroplasty (THA) following yearlong sobriety pathway.
Methods
Patients were identified using charts, current procedural terminology (CPT) code query, and THA sobriety pathway registry. The primary outcome was adequate pain control following surgery, defined as visual analog scale ≤ 5 or verbal description of “moderate” or lower pain. RA patients with infectious arthritis were analyzed separately. The secondary outcome was the level of mobility after surgery.
Results
In the THA pathway, 15 of 28 (53.6%) proved sobriety, 11 (39.3%) underwent THA, and 9 (32.1%) achieved adequate pain control (median 77 days). After RA, 19 (76%) achieved adequate pain control (median 119.5 days). Preoperative infection did not significantly affect time to pain control after RA (P = .94). Time to adequate pain control was not significantly different between RA and THA patients (P = .19). Three patients (30%) experienced improved level of mobility after THA and 7 (70%) experienced no change. After RA, 7 patients (29.1%) experienced improved level of mobility, 3 (13.6%) lost mobility, and 14 (63.6%) experienced no change. Three RA patients were later converted to THA without complication.
Conclusion
Yearlong sobriety pathway leading to THA leads to successful pain control in less than one-third of enrolled patients. Compared to delayed THA, RA enables more patients with substance abuse to be treated sooner and results in successful reduction of pain in a similar proportion of patients. RA may be an effective pain-reducing procedure for these patients."
}
@article{PINTO20132472,
title = "Operating System from the Scratch: A Problem-based Learning Approach for the Emerging Demands on OS Development",
journal = "Procedia Computer Science",
volume = "18",
pages = "2472 - 2481",
year = "2013",
note = "2013 International Conference on Computational Science",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2013.05.424",
url = "http://www.sciencedirect.com/science/article/pii/S187705091300567X",
author = "Renê S. Pinto and Pedro Nobile and Edwin Mamani and Lourenço P. Júnior and Helder J.F. Luz and Francisco J. Monaco",
keywords = "Operating Systems, Computer Science Education, Problem-based learning",
abstract = "In recent past history of computer systems industry, for decades, the hegemony of a few de facto standards dictated by major proprietary commercial products dominated the Operating Systems (OS) field. In such technological context, conso- nantly to this trend, the knowledge objective focused by academical and training courses on OS-related disciplines has often been addressed more from the stand point of essential theoretical background than of the technical skills for actuation on de- sign and development field. Emerging paradigms, nevertheless, have been rapidly changing this scenario. Among them, the establishment of Open Source concept is boosting the growing diversity of new operating systems; concomitantly, evolution of embedded hardware architectures has make it possible to run sophisticated operating systems where only bare rudimentary, ad hoc system-software were once practical. Aligned along this perspective, this paper introduces a new platform for teaching and training programs on OS development founded on a project-based approach which guides the student throughout the process of designing and programming a sufficiently simple, but yet realistic and fully functional, OS from the scratch. The differential of the present proposal regarding related works is that, instead of either merely inspecting example-code or experimenting with simulators, the apprentice is guided across the challenge of coding an entire new instance of a didactic system specification. A comparison of the companion OS-example with existing alternatives brings out a less complex implementation structure which maps conceptual modules with implementation blocks in an intuitive correspondence and with reduced function cou- pling. Moreover, the learning platform comes with a courseware material consistently linked to the laboratory practices, and aimed at the systemic comprehension of the many related multidisciplinary aspects."
}