@article{soetens2017changes,
  title={Changes as first-class citizens: a research perspective on modern software tooling},
  author={Soetens, Quinten David and Robbes, Romain and Demeyer, Serge},
  journal={ACM Computing Surveys (CSUR)},
  volume={50},
  number={2},
  pages={18},
  year={2017},
  publisher={ACM}
}

@article{omori2018comparative,
  title={Comparative Study between Two Approaches Using Edit Operations and Code Differences to Detect Past Refactorings},
  author={Omori, Takayuki and Maruyama, Katsuhisa},
  journal={IEICE Transactions on Information and Systems},
  volume={101},
  number={3},
  pages={644--658},
  year={2018},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}

@inproceedings{LeBenich:2017:RSC:3155562.3155631,
 author = {Le\ssenich, Olaf and Apel, Sven and K\"{a}stner, Christian and Seibt, Georg and Siegmund, Janet},
 title = {Renaming and Shifted Code in Structured Merging: Looking Ahead for Precision and Performance},
 booktitle = {Proceedings of the 32Nd IEEE/ACM International Conference on Automated Software Engineering},
 series = {ASE 2017},
 year = {2017},
 isbn = {978-1-5386-2684-9},
 location = {Urbana-Champaign, IL, USA},
 pages = {543--553},
 numpages = {11},
 url = {http://dl.acm.org/citation.cfm?id=3155562.3155631},
 acmid = {3155631},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {JDime, Software Merging, Structured Merge, Version Control},
} 

@inproceedings{delaTorre:2018:IDS:3196398.3196404,
 author = {de la Torre, Guillermo and Robbes, Romain and Bergel, Alexandre},
 title = {Imprecisions Diagnostic in Source Code Deltas},
 booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
 series = {MSR '18},
 year = {2018},
 isbn = {978-1-4503-5716-6},
 location = {Gothenburg, Sweden},
 pages = {492--502},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3196398.3196404},
 doi = {10.1145/3196398.3196404},
 acmid = {3196404},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {differencing, gumtree, quality, source code change detection},
} 

@inproceedings{Dotzler:2017:MAR:3106237.3106276,
 author = {Dotzler, Georg and Kamp, Marius and Kreutzer, Patrick and Philippsen, Michael},
 title = {More Accurate Recommendations for Method-level Changes},
 booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
 series = {ESEC/FSE 2017},
 year = {2017},
 isbn = {978-1-4503-5105-8},
 location = {Paderborn, Germany},
 pages = {798--808},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3106237.3106276},
 doi = {10.1145/3106237.3106276},
 acmid = {3106276},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Program transformation, recommendation system, refactoring},
} 

@inproceedings{Macho:2017:EBC:3104188.3104233,
 author = {Macho, Christian and Mcintosh, Shane and Pinzger, Martin},
 title = {Extracting Build Changes with BuildDiff},
 booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
 series = {MSR '17},
 year = {2017},
 isbn = {978-1-5386-1544-7},
 location = {Buenos Aires, Argentina},
 pages = {368--378},
 numpages = {11},
 url = {https://doi.org/10.1109/MSR.2017.65},
 doi = {10.1109/MSR.2017.65},
 acmid = {3104233},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {build systems, maintenance, software quality},
} 

@inproceedings{malloy2017quantifying,
  title={Quantifying the transition from Python 2 to 3: an empirical study of Python applications},
  author={Malloy, Brian A and Power, James F},
  booktitle={2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
  pages={314--323},
  year={2017},
  organization={IEEE}
}

@article{chen2018study,
  title={A study on the changes of dynamic feature code when fixing bugs: towards the benefits and costs of Python dynamic features},
  author={Chen, Zhifei and Ma, Wanwangying and Lin, Wei and Chen, Lin and Li, Yanhui and Xu, Baowen},
  journal={Science China Information Sciences},
  volume={61},
  number={1},
  pages={012107},
  year={2018},
  publisher={Springer}
}

@inproceedings{kaur2017evaluation,
  title={Evaluation of sampling techniques in software fault prediction using metrics and code smells},
  author={Kaur, Kamaldeep and Kaur, Parmeet},
  booktitle={2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},
  pages={1377--1387},
  year={2017},
  organization={IEEE}
}

@inproceedings{tsantalis2018ten,
  title={Ten years of JDeodorant: Lessons learned from the hunt for smells},
  author={Tsantalis, Nikolaos and Chaikalis, Theodoros and Chatzigeorgiou, Alexander},
  booktitle={2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  pages={4--14},
  year={2018},
  organization={IEEE}
}

@inproceedings{
        author={Faujdar,N. and Srivastav,K. and Gupta,M. and Saraswat,S.},
        editor={ },
        year={2018},
        title={Detection Strategies of Bad Smells in Highly Configurable Software},
        booktitle={Proceedings of the 8th International Conference Confluence 2018 on Cloud Computing, Data Science and Engineering, Confluence 2018},
        pages={31-35},
        language={English},
        url={www.scopus.com},
}

@inproceedings{frick2018diffviz,
  title={DiffViz: A Diff Algorithm Independent Visualization Tool for Edit Scripts},
  author={Frick, Veit and Wedenig, Christoph and Pinzger, Martin},
  booktitle={2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={705--709},
  year={2018},
  organization={IEEE}
}

@inproceedings{santos2017recommending,
  title={Recommending source code locations for system specific transformations},
  author={Santos, Gustavo and Paix{\~a}o, Kl{\'e}risson VR and Anquetil, Nicolas and Etien, Anne and de Almeida Maia, Marcelo and Ducasse, St{\'e}phane},
  booktitle={2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  pages={160--170},
  year={2017},
  organization={IEEE}
}

@inproceedings{nguyen2016large,
  title={A large-scale study on repetitiveness, containment, and composability of routines in open-source projects},
  author={Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N},
  booktitle={2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)},
  pages={362--373},
  year={2016},
  organization={IEEE}
}

@inproceedings{kreutzer2016automatic,
  title={Automatic clustering of code changes},
  author={Kreutzer, Patrick and Dotzler, Georg and Ring, Matthias and Eskofier, Bjoern M and Philippsen, Michael},
  booktitle={Proceedings of the 13th International Conference on Mining Software Repositories},
  pages={61--72},
  year={2016},
  organization={ACM}
}

@inproceedings{zhong2015empirical,
  title={An empirical study on real bug fixes},
  author={Zhong, Hao and Su, Zhendong},
  booktitle={Proceedings of the 37th International Conference on Software Engineering-Volume 1},
  pages={913--923},
  year={2015},
  organization={IEEE Press}
}

@inproceedings{liu2018closer,
  title={A closer look at real-world patches},
  author={Liu, Kui and Kim, Dongsun and Koyuncu, Anil and Li, Li and Bissyand{\'e}, Tegawend{\'e} F and Le Traon, Yves},
  booktitle={2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={275--286},
  year={2018},
  organization={IEEE}
}

@inproceedings{wang2018empirical,
  title={An empirical study of multi-entity changes in real bug fixes},
  author={Wang, Ye and Meng, Na and Zhong, Hao},
  booktitle={2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={287--298},
  year={2018},
  organization={IEEE}
}

@inproceedings{chen2018approach,
  title={An Approach to Identifying Error Patterns for Infrastructure as Code},
  author={Chen, Wei and Wu, Guoquan and Wei, Jun},
  booktitle={2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
  pages={124--129},
  year={2018},
  organization={IEEE}
}


@article{zhong2018towards,
  title={Towards reusing hints from past fixes},
  author={Zhong, Hao and Meng, Na},
  journal={Empirical Software Engineering},
  volume={23},
  number={5},
  pages={2521--2549},
  year={2018},
  publisher={Springer}
}

@article{azad2017generating,
  title={Generating API call rules from version history and stack overflow posts},
  author={Azad, Shams and Rigby, Peter C and Guerrouj, Latifa},
  journal={ACM Transactions on Software Engineering and Methodology (TOSEM)},
  volume={25},
  number={4},
  pages={29},
  year={2017},
  publisher={ACM}
}

@article{alexandru2018redundancy,
  title={Redundancy-free analysis of multi-revision software artifacts},
  author={Alexandru, Carol V and Panichella, Sebastiano and Proksch, Sebastian and Gall, Harald C},
  journal={Empirical Software Engineering},
  pages={1--49},
  year={2018},
  publisher={Springer}
}

@article{gharehyazie2018cross,
  title={Cross-project code clones in GitHub},
  author={Gharehyazie, Mohammad and Ray, Baishakhi and Keshani, Mehdi and Zavosht, Masoumeh Soleimani and Heydarnoori, Abbas and Filkov, Vladimir},
  journal={Empirical Software Engineering},
  pages={1--36},
  year={2018},
  publisher={Springer}
}

@inproceedings{zhong2017boosting,
  title={Boosting complete-code tool for partial program},
  author={Zhong, Hao and Wang, Xiaoyin},
  booktitle={Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
  pages={671--681},
  year={2017},
  organization={IEEE Press}
}

@inproceedings{chen2017tool,
  title={Tool support for managing clone refactorings to facilitate code review in evolving software},
  author={Chen, Zhiyuan and Mohanavilasam, Maneesh and Kwon, Young-Woo and Song, Myoungkyu},
  booktitle={2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)},
  volume={1},
  pages={288--297},
  year={2017},
  organization={IEEE}
}

@inproceedings{nguyen2015graph,
  title={Graph-based statistical language model for code},
  author={Nguyen, Anh Tuan and Nguyen, Tien N},
  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
  volume={1},
  pages={858--868},
  year={2015},
  organization={IEEE}
}

@inproceedings{nguyen2016using,
  title={Using Topic Model to Suggest Fine-Grained Source Code Changes},
  author={Nguyen, Hoan Anh and Nguyen, Anh Tuan and Nguyen, Tien N},
  booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={200--210},
  year={2016},
  organization={IEEE}
}

@inproceedings{yue2017characterization,
  title={A characterization study of repeated bug fixes},
  author={Yue, Ruru and Meng, Na and Wang, Qianxiang},
  booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={422--432},
  year={2017},
  organization={IEEE}
}

@inproceedings{kong2015experience,
  title={Experience report: how do techniques, programs, and tests impact automated program repair?},
  author={Kong, Xianglong and Zhang, Lingming and Wong, W Eric and Li, Bixin},
  booktitle={2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)},
  pages={194--204},
  year={2015},
  organization={IEEE}
}

@inproceedings{ray2015uniqueness,
  title={The uniqueness of changes: Characteristics and applications},
  author={Ray, Baishakhi and Nagappan, Meiyappan and Bird, Christian and Nagappan, Nachiappan and Zimmermann, Thomas},
  booktitle={Proceedings of the 12th Working Conference on Mining Software Repositories},
  pages={34--44},
  year={2015},
  organization={IEEE Press}
}

@inproceedings{higo2015toward,
  title={Toward reusing code changes},
  author={Higo, Yoshiki and Ohtani, Akio and Hayashi, Shinpei and Hata, Hideaki and Shinji, Kusumoto},
  booktitle={Proceedings of the 12th Working Conference on Mining Software Repositories},
  pages={372--376},
  year={2015},
  organization={IEEE Press}
}

@inproceedings{gharehyazie2017some,
  title={Some from here, some from there: Cross-project code reuse in github},
  author={Gharehyazie, Mohammad and Ray, Baishakhi and Filkov, Vladimir},
  booktitle={2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)},
  pages={291--301},
  year={2017},
  organization={IEEE}
}

@inproceedings{jiang2015summarizing,
  title={Summarizing evolutionary trajectory by grouping and aggregating relevant code changes},
  author={Jiang, Qingtao and Peng, Xin and Wang, Hai and Xing, Zhenchang and Zhao, Wenyun},
  booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},
  pages={361--370},
  year={2015},
  organization={IEEE}
}

@inproceedings{mondal2016empirical,
  title={An empirical study on ranking change recommendations retrieved using code similarity},
  author={Mondal, Manishankar and Roy, Chanchal K and Schneider, Kevin A},
  booktitle={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},
  volume={3},
  pages={44--50},
  year={2016},
  organization={IEEE}
}


@article{
        author={Shippey,T. and Bowes,D. and Hall,T.},
        year={2019},
        title={Automatically identifying code features for software defect prediction: Using AST N-grams},
        journal={Information and Software Technology},
        volume={106},
        pages={142-160},
        language={English},
        url={www.scopus.com},
}


@inproceedings{
        author={Feng,Q. and Cai,Y. and Kazman,R. and Mo,R.},
        editor={ },
        year={2018},
        title={The birth, growth, death and rejuvenation of software maintenance communities},
        booktitle={International Symposium on Empirical Software Engineering and Measurement},
        language={English},
        url={www.scopus.com},
}


@article{
        author={Chen,Z. and Kwon,Y. -. and Song,M.},
        year={2018},
        title={Clone refactoring inspection by summarizing clone refactorings and detecting inconsistent changes during software evolution},
        journal={Journal of Software: Evolution and Process},
        volume={30},
        number={10},
        language={English},
        url={www.scopus.com},
}


@article{
        author={Zhong,H. and Mei,H.},
        year={2018},
        title={Mining repair model for exception-related bug},
        journal={Journal of Systems and Software},
        volume={141},
        pages={16-31},
        note={Cited By :1},
        language={English},
        url={www.scopus.com},
}


@article{
        author={Kong,X. and Zhang,L. and Wong,W. E. and Li,B.},
        year={2018},
        title={The impacts of techniques, programs and tests on automated program repair: An empirical study},
        journal={Journal of Systems and Software},
        volume={137},
        pages={480-496},
        language={English},
        url={www.scopus.com},
}

@article{
        author={Jiang,Q. and Peng,X. and Wang,H. and Xing,Z. and Zhao,W.},
        year={2017},
        title={Understanding systematic and collaborative code changes by mining evolutionary trajectory patterns},
        journal={Journal of Software: Evolution and Process},
        volume={29},
        number={3},
        note={Cited By :2},
        language={English},
        url={www.scopus.com},
}

@inproceedings{
        author={Rolim,R.},
        editor={ },
        year={2016},
        title={Automating repetitive code changes using examples},
        booktitle={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
        volume={13-18-November-2016},
        pages={1063-1065},
        language={English},
        url={www.scopus.com},
}

@inproceedings{
        author={Fowkes,J. and Sutton,C.},
        editor={ },
        year={2016},
        title={Parameter-Free Probabilistic API Mining across GitHub},
        booktitle={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
        volume={13-18-November-2016},
        pages={254-265},
        note={Cited By :11},
        language={English},
        url={www.scopus.com},
}

@inproceedings{
        author={Hanam,Q. and Brito,F. S. D. M. and Mesbah,A.},
        editor={ },
        year={2016},
        title={Discovering bug patterns in Javascript},
        booktitle={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
        volume={13-18-November-2016},
        pages={144-156},
        note={Cited By :11},
        language={English},
        url={www.scopus.com},
}

@inproceedings{
        author={Nguyen,A. T. and Nguyen,H. A. and Nguyen,T. N.},
        editor={ },
        year={2016},
        title={A large-scale study on repetitiveness, containment, and composability of routines in open-source projects},
        booktitle={Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},
        pages={362-373},
        note={Cited By :4},
        language={English},
        url={www.scopus.com},
}

@inproceedings{
        author={Barr,E. T. and Brun,Y. and Devanbu,P. and Harman,M. and Sarro,F.},
        editor={ },
        year={2014},
        title={The plastic surgery hypothesis},
        booktitle={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
        volume={16-21-November-2014},
        pages={306-317},
        note={Cited By :60},
        language={English},
        url={www.scopus.com},
}

@inproceedings{
        author={Negara,S. and Codoban,M. and Dig,D. and Johnson,R. E.},
        editor={ },
        year={2014},
        title={Mining fine-grained code changes to detect unknown change patterns},
        booktitle={Proceedings - International Conference on Software Engineering},
        chapter={1},
        pages={803-813},
        note={Cited By :44},
        language={English},
        url={www.scopus.com},
}

@inproceedings{
        author={Martinezy,M. and Weimerz,W. and Monperrusy,M.},
        editor={ },
        year={2014},
        title={Do the fix ingredients already exist? An empirical inquiry into the redundancy assumptions of program repair approaches},
        booktitle={36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings},
        pages={492-495},
        note={Cited By :25},
        language={English},
        url={www.scopus.com},
}

@inproceedings{goonetilleke2017graph,
  title={Graph data management of evolving dependency graphs for multi-versioned codebases},
  author={Goonetilleke, Oshini and Meibusch, David and Barham, Ben},
  booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={574--583},
  year={2017},
  organization={IEEE}
}

@inproceedings{muylaert2018untangling,
  title={Untangling Composite Commits Using Program Slicing},
  author={Muylaert, Ward and De Roover, Coen},
  booktitle={2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)},
  pages={193--202},
  year={2018},
  organization={IEEE}
}

@inproceedings{mahmoudi2018android,
  title={The Android update problem: an empirical study},
  author={Mahmoudi, Mehran and Nadi, Sarah},
  booktitle={Proceedings of the 15th International Conference on Mining Software Repositories},
  pages={220--230},
  year={2018},
  organization={ACM}
}

@article{stevens2019querying,
  title={Querying distilled code changes to extract executable transformations},
  author={Stevens, Reinout and Molderez, Tim and De Roover, Coen},
  journal={Empirical Software Engineering},
  volume={24},
  number={1},
  pages={491--535},
  year={2019},
  publisher={Springer}
}

@inproceedings{osman2014mining,
  title={Mining frequent bug-fix code changes},
  author={Osman, Haidar and Lungu, Mircea and Nierstrasz, Oscar},
  booktitle={2014 Software Evolution Week-IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)},
  pages={343--347},
  year={2014},
  organization={IEEE}
}

@inproceedings{tan2017codeflaws,
  title={Codeflaws: a programming competition benchmark for evaluating automated program repair tools},
  author={Tan, Shin Hwei and Yi, Jooyong and Mechtaev, Sergey and Roychoudhury, Abhik and others},
  booktitle={Proceedings of the 39th International Conference on Software Engineering Companion},
  pages={180--182},
  year={2017},
  organization={IEEE Press}
}

@inproceedings{tan2015relifix,
  title={relifix: Automated repair of software regressions},
  author={Tan, Shin Hwei and Roychoudhury, Abhik},
  booktitle={Proceedings of the 37th International Conference on Software Engineering-Volume 1},
  pages={471--482},
  year={2015},
  organization={IEEE Press}
}

@inproceedings{oumarou2015identifying,
  title={Identifying the exact fixing actions of static rule violation},
  author={Oumarou, Hayatou and Anquetil, Nicolas and Etien, Anne and Ducasse, St{\'e}phane and Taiwe, Kolyang Dina},
  booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},
  pages={371--379},
  year={2015},
  organization={IEEE}
}

@inproceedings{ueda2017if,
  title={How is IF Statement Fixed Through Code Review? A Case Study of Qt Project},
  author={Ueda, Yuki and Ihara, Akinori and Hirao, Toshiki and Ishio, Takashi and Matsumoto, Kenichi},
  booktitle={2017 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
  pages={207--213},
  year={2017},
  organization={IEEE}
}

@inproceedings{martinez2017towards,
  title={Towards the quality improvement of cross-platform mobile applications},
  author={Martinez, Matias and Lecomte, Sylvain},
  booktitle={2017 IEEE/ACM 4th International Conference on Mobile Software Engineering and Systems (MOBILESoft)},
  pages={184--188},
  year={2017},
  organization={IEEE}
}

@inproceedings{levin2016using,
  title={Using temporal and semantic developer-level information to predict maintenance activity profiles},
  author={Levin, Stanislav and Yehudai, Amiram},
  booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={463--467},
  year={2016},
  organization={IEEE}
}

@article{stevens2018querying,
  title={Querying distilled code changes to extract executable transformations},
  author={Stevens, Reinout and Molderez, Tim and De Roover, Coen},
  journal={Empirical Software Engineering},
  pages={1--45},
  year={2018},
  publisher={Springer}
}

@article{ueda2018if,
  title={How are IF-Conditional Statements Fixed Through Peer CodeReview?},
  author={Ueda, Yuki and Ihara, Akinori and Ishio, Takashi and Hirao, Toshiki and Matsumoto, Kenichi},
  journal={IEICE TRANSACTIONS on Information and Systems},
  volume={101},
  number={11},
  pages={2720--2729},
  year={2018},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}


@ARTICLE{Meqdadi201980,
author={Meqdadi, O. and Alhindawi, N. and Alsakran, J. and Saifan, A. and Migdadi, H.},
title={Mining software repositories for adaptive change commits using machine learning techniques},
journal={Information and Software Technology},
year={2019},
volume={109},
pages={80-91},
doi={10.1016/j.infsof.2019.01.008},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061095706&doi=10.1016%2fj.infsof.2019.01.008&partnerID=40&md5=62711a485ee5912c0cd26f79e7ff9368},
abstract={Context: Version Control Systems, such as Subversion, are standard repositories that preserve all of the maintenance changes undertaken to source code artifacts during the evolution of a software system. The documented data of the version history are organized as commits; however, these commits do not keep a tag that would identify the purpose of the relevant undertaken change of a commit, thus, there is rarely enough detail to clearly direct developers to the changes associated with a specific type of maintenance. Objective: This work examines the version histories of an open source system to automatically classify version commits into one of two categories, namely adaptive commits and non-adaptive commits. Method: We collected the commits from the version history of three open source systems, then we obtained eight different code change metrics related to, for example, the number of changed statements, methods, hunks, and files. Based on these change metrics, we built a machine learning approach to classify whether a commit was adaptive or not. Results: It is observed that code change metrics can be indicative of adaptive maintenance activities. Also, the classification findings show that the machine learning classifier developed has approximately 75% prediction accuracy within labeled change histories. Conclusion: The proposed method automates the process of examining the version history of a software system and identifies which commits to the system are related to an adaptive maintenance task. The evaluation of the method supports its applicability and efficiency. Although the evaluation of the proposed classifier on unlabeled change histories shows that it is not much better than the random guessing in terms of F-measure, we feel that our classifier would serve as a better basis for developing advanced classifiers that have predictive power of adaptive commits without the need of manual efforts. © 2019 Elsevier B.V.},
keywords={Codes (symbols);  Computer software maintenance;  Learning systems;  Machine learning;  Open source software, Adaptive maintenance;  Code changes;  Commit types;  Machine learning approaches;  Machine learning techniques;  Mining software repositories;  Prediction accuracy;  Version control system, Open systems},
document_type={Article},
source={Scopus},
}

@ARTICLE{Stevens2019491,
author={Stevens, R. and Molderez, T. and De Roover, C.},
title={Querying distilled code changes to extract executable transformations},
journal={Empirical Software Engineering},
year={2019},
volume={24},
number={1},
pages={491-535},
doi={10.1007/s10664-018-9644-3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053271297&doi=10.1007%2fs10664-018-9644-3&partnerID=40&md5=e63534634789c9c6f417132721c3f25a},
abstract={Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. We show that our approach is able to recall different implementation variants of the same source code evolution in histories of different software projects. We also evaluate the expressiveness and ease-of-use of our approach in a user study. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
keywords={Computer programming languages, Change distilling;  Change querying;  Evolution patterns;  Fine-grained changes;  Intermediate state;  Logic meta programming;  Mining software repositories;  Software project, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Frick2018264,
author={Frick, V. and Grassauer, T. and Beck, F. and Pinzger, M.},
title={Generating accurate and compact edit scripts using tree differencing},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
year={2018},
pages={264-274},
doi={10.1109/ICSME.2018.00036},
art_number={8530035},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058289624&doi=10.1109%2fICSME.2018.00036&partnerID=40&md5=5a0874c1dbad89a7cc6e6e231df79d78},
abstract={For analyzing changes in source code, edit scriptsare used to describe the differences between two versions of afile. These scripts consist of a list of actions that, applied to thesource file, result in the new version of the file. In contrast toline-based source code differencing, tree-based approaches suchas GumTree, MTDIFF, or ChangeDistiller extract changes bycomparing the abstract syntax trees (AST) of two versions of asource file. One benefit of tree-based approaches is their abilityto capture moved (sub) trees in the AST. Our approach, theIterative Java Matcher (IJM), builds upon GumTree and aims atgenerating more accurate and compact edit scripts that capturethe developer's intent. This is achieved by improving the qualityof the generated move and update actions, which are the mainsource of inaccurate actions generated by previous approaches. To evaluate our approach, we conducted a study with 11 external experts and manually analyzed the accuracy of 2400 randomly selected editactions. Comparing IJM to GumTree and MTDIFF, the resultsshow that IJM provides better accuracy for move and updateactions and is more beneficial to understanding the changes. © 2018 IEEE.},
keywords={Syntactics;  Trees (mathematics), Abstract Syntax Trees;  Change extractions;  External experts;  Software Evolution;  Source codes;  Tree differencing;  Tree-based approach, Computer software maintenance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2018287,
author={Wang, Y. and Meng, N. and Zhong, H.},
title={An empirical study of multi-entity changes in real bug fixes},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
year={2018},
pages={287-298},
doi={10.1109/ICSME.2018.00038},
art_number={8530037},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057470499&doi=10.1109%2fICSME.2018.00038&partnerID=40&md5=3e54a06b41738a4716cb9b51363baf12},
abstract={Prior studies showed that developers applied repeated bug fixes-similar or identical code changes-To multiple locations. According to the observation, researchers built tools to automatically generate candidate patches from the repeated bug-fixing patterns. However, all such research focuses on the recurring change patterns within single methods. We are curious whether there are also repeated bug fixes that change multiple program entities (e.g., classes, methods, and fields); and if so, how we can leverage such recurring change patterns to further help developers fix bugs. In this paper, we present a comprehensive empirical study on multi-entity bug fixes in terms of their frequency, composition, and semantic meanings. Specifically for each bug fix, we first used our approach InterPart to perform static inter-procedural analysis on partial programs (i.e., the old and new versions of changed Java files), and to extract change dependency graphs (CDGs)-graphs that connect multiple changed entities based on their syntactic dependencies. By extracting common subgraphs from the CDGs of different fixes, we identified the recurring change patterns. Our study on Aries, Cassandra, Derby, and Mahout shows that (1) 52-58% of bug fixes involved multi-entity changes; (2) 6 recurring change patterns commonly exist in all projects; and (3) 19-210 entity pairs were repetitively co-changed mainly because the pairs invoked the same methods, accessed the same fields, or contained similar content. These results helped us better understand the gap between the fixes generated by existing automatic program repair (APR) approaches and the real fixes. Our observations will shed light on the follow-up research of automatic program comprehension and modification. © 2018 IEEE.},
keywords={Computer software;  Computer software maintenance;  Semantics, Automatic programs;  Change dependencies;  Change patterns;  Empirical studies;  Inter-procedural analysis;  Multiple program;  Software entities;  Syntactic dependencies, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Huang2018679,
author={Huang, K. and Zhou, D. and Chen, B. and Wang, Y. and Zhao, W. and Peng, X. and Liu, Y.},
title={Cldiff: Generating concise linked code differences},
journal={ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
year={2018},
pages={679-690},
doi={10.1145/3238147.3238219},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056521780&doi=10.1145%2f3238147.3238219&partnerID=40&md5=adbe44e4f4489bfda3a269e289f2618f},
abstract={Analyzing and understanding source code changes is important in a variety of software maintenance tasks. To this end, many code differencing and code change summarization methods have been proposed. For some tasks (e.g. code review and software merging), however, those differencing methods generate too fine-grained a representation of code changes, and those summarization methods generate too coarse-grained a representation of code changes. Moreover, they do not consider the relationships among code changes. Therefore, the generated differences or summaries make it not easy to analyze and understand code changes in some software maintenance tasks. In this paper, we propose a code differencing approach, named ClDiff, to generate concise linked code differences whose granularity is in between the existing code differencing and code change summarization methods. The goal of ClDiff is to generate more easily understandable code differences. ClDiff takes source code files before and after changes as inputs, and consists of three steps. First, it pre-processes the source code files by pruning unchanged declarations from the parsed abstract syntax trees. Second, it generates concise code differences by grouping fine-grained code differences at or above the statement level and describing high-level changes in each group. Third, it links the related concise code differences according to five pre-defined links. Experiments with 12 Java projects (74,387 commits) and a human study with 10 participants have indicated the accuracy, conciseness, performance and usefulness of ClDiff. © 2018 Association for Computing Machinery.},
keywords={Abstracting;  Computer programming languages;  Computer software maintenance;  Software engineering;  Trees (mathematics), Abstract Syntax Trees;  Coarse-grained;  Code differencing;  Program comprehension;  Software merging;  Software-maintenance tasks;  Source code changes;  Source codes, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhen2018,
author={Zhen, Y. and Zhang, W. and Dai, Z. and Mao, J. and Chen, Y.},
title={Is it possible to automatically port kernel modules?},
journal={Proceedings of the 9th Asia-Pacific Workshop on Systems, APSys 2018},
year={2018},
doi={10.1145/3265723.3265732},
art_number={15},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055879107&doi=10.1145%2f3265723.3265732&partnerID=40&md5=b912d146f563ceb35c7582ac86d5dfe2},
abstract={As essential components in Linux kernel, kernel modules (kmods) account for over 70% of Linux source code and are heavily dependent on fast evolving and non-stable kernel internal interfaces. Forward and back porting kmods to target versions of Linux kernel is hard but necessary. We conducted a comprehensive study to investigate the characteristics of kernel internal interface changes by analyzing 256 representative patches selected from Linux development history in last 7 years. We gained some new insights into challenges and opportunities on automatic porting of kernel modules. The study allows us a beter understanding of the problem and it is useful for designing automated tools to assist in porting kmods. © 2018 Association for Computing Machinery.},
keywords={Automated tools;  Automatic porting;  Internal interfaces;  Kernel modules;  Linux Development;  Linux kernel;  Non-stable;  Source codes, Linux},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Niu20181887,
author={Niu, X. and Li, S. and Jia, Z. and Zhou, S. and Li, W. and Liao, X.},
title={Understanding the similarity of log revision behaviors in open source software},
journal={International Journal of Performability Engineering},
year={2018},
volume={14},
number={8},
pages={1887-1895},
doi={10.23940/ijpe.18.08.p27.18871895},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054725435&doi=10.23940%2fijpe.18.08.p27.18871895&partnerID=40&md5=c13d5d5c8e13deefefd3135f685095a6},
abstract={As logging code evolves with bug fixes and feature updates, developers may miss some log revisions due to a lack of general specifications and attention from developers. This makes it more troublesome to achieve good logging practices. In this paper, we try to study log revision behaviors from evolutionary history. Motivated by similar edits of clone codes, we assume there also exist similar log revisions that implicated log revision behaviors. Based on this assumption, we study the similarity of log revision behaviors and answer six research questions. Specifically, we find that 54.14% of log revisions belong to groups of similar log revisions and 64.4% of groups contain log revisions that are missed by developers. We stress the importance of branch statements on learning from similar log revisions since 53.51% of sampled similar log revisions are related to the semantics of branch statements. © 2018 Totem Publisher, Inc. All rights reserved.},
keywords={Open systems;  Semantics, Bug fixes;  Evolutionary history;  Failure diagnose;  General specification;  Log revision;  Research questions;  Software Evolution, Open source software},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jiang2018298,
author={Jiang, J. and Xiong, Y. and Zhang, H. and Gao, Q. and Chen, X.},
title={Shaping program repair space with existing patches and similar code},
journal={ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2018},
pages={298-309},
doi={10.1145/3213846.3213871},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051477803&doi=10.1145%2f3213846.3213871&partnerID=40&md5=eff0c7ae75fe5569b9e1190f5f2993a7},
abstract={Automated program repair (APR) has great potential to reduce bugfixing effort and many approaches have been proposed in recent years. APRs are often treated as a search problem where the search space consists of all the possible patches and the goal is to identify the correct patch in the space. Many techniques take a data-driven approach and analyze data sources such as existing patches and similar source code to help identify the correct patch. However, while existing patches and similar code provide complementary information, existing techniques analyze only a single source and cannot be easily extended to analyze both. In this paper, we propose a novel automatic program repair approach that utilizes both existing patches and similar code. Our approach mines an abstract search space from existing patches and obtains a concrete search space by differencing with similar code snippets. Then we search within the intersection of the two search spaces.We have implemented our approach as a tool called SimFix, and evaluated it on the Defects4J benchmark. Our tool successfully fixed 34 bugs. To our best knowledge, this is the largest number of bugs fixed by a single technology on the Defects4J benchmark. Furthermore, as far as we know, 13 bugs fixed by our approach have never been fixed by the current approaches. © 2018 Association for Computing Machinery.},
keywords={Codes (symbols);  Defects;  Repair;  Software testing, Automatic programs;  Code adaptation;  Code differencing;  Data-driven approach;  Data-sources;  Search problem;  Search spaces;  Single source, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Petke2018415,
author={Petke, J. and Haraldsson, S.O. and Harman, M. and Langdon, W.B. and White, D.R. and Woodward, J.R.},
title={Genetic Improvement of Software: A Comprehensive Survey},
journal={IEEE Transactions on Evolutionary Computation},
year={2018},
volume={22},
number={3},
pages={415-432},
doi={10.1109/TEVC.2017.2693219},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026851053&doi=10.1109%2fTEVC.2017.2693219&partnerID=40&md5=cc2ec8d4506e62ebdf30433caef7ed23},
abstract={Genetic improvement (GI) uses automated search to find improved versions of existing software. We present a comprehensive survey of this nascent field of research with a focus on the core papers in the area published between 1995 and 2015. We identified core publications including empirical studies, 96% of which use evolutionary algorithms (genetic programming in particular). Although we can trace the foundations of GI back to the origins of computer science itself, our analysis reveals a significant upsurge in activity since 2012. GI has resulted in dramatic performance improvements for a diverse set of properties such as execution time, energy and memory consumption, as well as results for fixing and extending existing system functionality. Moreover, we present examples of research work that lies on the boundary between GI and other areas, such as program transformation, approximate computing, and software repair, with the intention of encouraging further exchange of ideas between researchers in these fields. © 1997-2012 IEEE.},
keywords={Computer software;  Engineering research;  Genetic algorithms;  History;  Software engineering;  Software testing;  Surveying;  Surveys, Approximate computing;  Automated searches;  Empirical studies;  Existing systems;  Genetic improvements;  Memory consumption;  Performance improvements;  Program transformations, Genetic programming},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li2018178,
author={Li, S. and Niu, X. and Jia, Z. and Wang, J. and He, H. and Wang, T.},
title={Logtracker: Learning log revision behaviors proactively from software evolution history},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={178-188},
doi={10.1145/3196321.3196328},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051637000&doi=10.1145%2f3196321.3196328&partnerID=40&md5=d0215d35ac181445405a174136daefba},
abstract={Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted. © 2018 ACM.},
keywords={Codes (symbols);  Computer programming, Automatic tools;  Design and implements;  Empirical studies;  Failure diagnose;  log revision;  Open source projects;  Software Evolution;  Systematic process, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{An2018180,
author={An, K. and Meng, N. and Tilevich, E.},
title={Automatic inference of Java-to-swift translation rules for porting mobile applications},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={180-190},
doi={10.1145/3197231.3197240},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051632117&doi=10.1145%2f3197231.3197240&partnerID=40&md5=8a4fa9c806a76d64581564d9b2f10ec4},
abstract={A native cross-platform mobile app has multiple platform-specific implementations. Typically, an app is developed for one platform and then ported to the remaining ones. Translating an app from one language (e.g., Java) to another (e.g., Swift) by hand is tedious and error-prone, while automated translators either require manually defined translation rules or focus on translating APIs. To automate the translation of native cross-platform apps, we present J2SINFERER, a novel approach that iteratively infers syntactic transformation rules and API mappings from Java to Swift. Given a software corpus in both languages, J2SLNFERER first identifies the syntactically equivalent code based on braces and string similarity. For each pair of similar code segments, J2SLNFERER then creates syntax trees of both languages, leveraging the minimalist domain knowledge of language correspondence (e.g., operators and markers) to iteratively align syntax tree nodes, and to infer both syntax and API mapping rules. J2SLNFERER represents inferred rules as string templates, stored in a database, to translate code from Java to Swift. We evaluated J2SLNFERER with four applications, using one part of the data to infer translation rules, and the other part to apply the rules. With 76% in-project accuracy and 65% cross-project accuracy, J2SLNFERER outperforms in accuracy j2swift, a state-of-the-art Java-to-Swift conversion tool. As native cross-platform mobile apps grow in popularity, J2SLNFERER can shorten their time to market by automating the tedious and error prone task of source-to-source translation. © 2018 ACM.},
keywords={Codes (symbols);  Iterative methods;  Knowledge management;  Mapping;  Software engineering;  Syntactics;  Translation (languages);  Trees (mathematics), Automatic inference;  Error prone tasks;  Mobile applications;  Multiple platforms;  Source-to-source translations;  String similarity;  Syntactic transformations;  Translation rules, Java programming language},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Song2018112,
author={Song, M. and Tilevich, E.},
title={Systematic adaptation of dynamically generated source code via domain-specific examples},
journal={IET Software},
year={2018},
volume={12},
number={2},
pages={112-119},
doi={10.1049/iet-sen.2016.0211},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045114635&doi=10.1049%2fiet-sen.2016.0211&partnerID=40&md5=00823ac3428dc19bd8e154d3d7570340},
abstract={In modern web-based applications, an increasing amount of source code is generated dynamically at runtime. Web applications commonly execute dynamically generated code (DGC) emitted by third-party, black-box generators, run at remote sites. Web developers often need to adapt DGC before it can be executed: embedded HTML can be vulnerable to cross-site scripting attacks; an API may be incompatible with some browsers; and the program's state created by DGC may not be persisting. Lacking any systematic approaches for adapting DGC, web developers resort to ad-hoc techniques that are unsafe and error-prone. This study presents an approach for adapting DGC systematically that follows the program-transformation-by-example paradigm. The proposed approach provides predefined, domain-specific before/after examples that capture the variability of commonly used adaptations. By approving or rejecting these examples, web developers determine the required adaptation transformations, which are encoded in an adaptation script operating on the generated code's abstract syntax tree. The proposed approach is a suite of practical JavaScript program adaptations and their corresponding before/after examples. The authors have successfully applied the approach to real web applications to adapt third-party generated JavaScript code for security, browser compatibility, and persistence. © The Institution of Engineering and Technology 2017.},
keywords={High level languages;  Trees (mathematics), Abstract Syntax Trees;  Ad-hoc techniques;  Cross Site Scripting Attacks;  Domain specific;  JavaScript programs;  Program transformations;  WEB application;  Web-based applications, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Brito2018306,
author={Brito, G. and Hora, A. and Valente, M.T. and Robbes, R.},
title={On the use of replacement messages in API deprecation: An empirical study},
journal={Journal of Systems and Software},
year={2018},
volume={137},
pages={306-321},
doi={10.1016/j.jss.2017.12.007},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042222030&doi=10.1016%2fj.jss.2017.12.007&partnerID=40&md5=9eaf775ca7768ea3eac148d44cb169c6},
abstract={Libraries are commonly used to support code reuse and increase productivity. As any other system, they evolve over time, and so do their APIs. Consequently, client applications should be updated to benefit from better APIs. To facilitate this task, API elements should always be deprecated with replacement messages. However, in practice, there are evidences that API elements are deprecated without these messages. In this paper, we study questions regarding the adoption of deprecation messages. Our goal is twofold: to measure the real usage of deprecation messages and to investigate whether a tool is needed to recommend them. We assess (i) the frequency of deprecated elements with replacement messages, (ii) the impact of software evolution on this frequency, and (iii) the characteristics of systems that deprecate API elements in a correct way. Our analysis on 622 Java and 229 C# systems shows that: (i) on the median, 66.7% and 77.8% of the API elements are deprecated with replacement messages per project, (ii) there is no major effort to improve deprecation messages, and (iii) systems that deprecated API elements with messages are different in terms of size and community. As a result, we provide the basis for creating a tool to support clients detecting missing deprecation messages. © 2017 Elsevier Inc.},
keywords={Software engineering, API deprecation;  Client applications;  Code reuse;  Empirical Software Engineering;  Empirical studies;  Mining software repositories;  Software Evolution, Application programming interfaces (API)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Monperrus2018,
author={Monperrus, M.},
title={Automatic software repair: A bibliography},
journal={ACM Computing Surveys},
year={2018},
volume={51},
number={1},
doi={10.1145/3105906},
art_number={17},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042496901&doi=10.1145%2f3105906&partnerID=40&md5=f6f626f4680e93ec7c8a84c467982798},
abstract={This article presents a survey on automatic software repair. Automatic software repair consists of automatically finding a solution to software bugs without human intervention. This article considers all kinds of repairs. First, it discusses behavioral repair where test suites, contracts, models, and crashing inputs are taken as oracle. Second, it discusses state repair, also known as runtime repair or runtime recovery, with techniques such as checkpoint and restart, reconfiguration, and invariant restoration. The uniqueness of this article is that it spans the research communities that contribute to this body of knowledge: software engineering, dependability, operating systems, programming languages, and security. It provides a novel and structured overview of the diversity of bug oracles and repair operators used in the literature. © 2017 ACM.},
keywords={Program debugging;  Repair;  Software engineering, Body of knowledge;  Checkpoint-and-restart;  Human intervention;  Repair operator;  Research communities;  Self-healing;  Software bug;  Software repair, Restoration},
document_type={Review},
source={Scopus},
}

@ARTICLE{Huang20181597,
author={Huang, Q. and Huang, B. and Fang, Z. and Xiao, M. and Yu, Y.},
title={Exploiting abstract change pattern from code changes},
journal={Journal of Intelligent and Fuzzy Systems},
year={2018},
volume={35},
number={2},
pages={1597-1608},
doi={10.3233/JIFS-169698},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053287644&doi=10.3233%2fJIFS-169698&partnerID=40&md5=71cae02a72d80ec138bab3761c6344dc},
abstract={Benefited on the open source software movement, many code search tools are proposed to retrieve source code over the internet. However, the retrieved source code rarely meets user needs perfectly so that it has to be changed manually. This is because the retrieved source code is concretely over-specific to some particular context. To solve this problem, we propose an Abstract Change Pattern Model (ACPM) to ensure the context-specific source code general for various contexts. This model consists of the ACP abstracting and the ACP concretizing algorithms. The former exploits the abstractly context-aware change pattern from the code changes. Based on the change pattern, the latter transforms the context-specific source code into the correct one meeting different user needs. To evaluate ACPM, we extract 7 topics and collect 5-6 code snippets per topic from the Github, while performing 5 different experiments where we explore 2 sensitivity-related rules and use them to raise the accuracy gradually. Our experimental results show that ACPM is feasible and practical with 73.84% accuracy. © 2018 - IOS Press and the authors. All rights reserved.},
keywords={Codes (symbols);  Computer programming languages;  Data mining;  Open source software, Change patterns;  Code changes;  Code search;  Context-Aware;  Program transformations;  Source codes;  User need, Open systems},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2018137,
author={Wang, Y. and Meng, N. and Zhong, H.},
title={CMSuggester: Method Change Suggestion to Complement Multi-entity Edits},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={11293 LNCS},
pages={137-153},
doi={10.1007/978-3-030-04272-1_9},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057504932&doi=10.1007%2f978-3-030-04272-1_9&partnerID=40&md5=99ed5873fd4e160cd5bb523e88d2f47d},
abstract={Developers spend significant time and effort in maintaining software. In a maintenance task, developers sometimes have to simultaneously modify multiple program entities (i.e., classes, methods, and fields). We refer to such complex changes as multi-entity edits. It is challenging for developers to apply multi-entity edits consistently and completely. Existing tools provide limited support for such edits, mainly because the co-changed entities usually contain diverse program contexts and experience different changes. This paper introduces CMSuggester, an automatic approach that suggests complementary changes for multi-entity edits. Given a multi-entity edit that adds a field and modifies one or more methods to access the field, CMSuggester suggests other methods to co-change for the new field access. CMSuggester is inspired by our previous empirical study, which reveals that the methods co-changed to access a new field usually commonly access the same set of fields declared in the same class. By extracting the fields accessed by the given changed method(s), CMSuggester identifies and recommends any unchanged method that also accesses those fields. Our evaluation shows that CMSuggester recommends changes for 279 out of 408 suggestion tasks. With the recommended methods, CMSuggester achieves 73% F-score on average, while the widely used tool ROSE achieves 48% F-score. In most cases, as shown in our evaluation results, CMSuggester are useful for developers, since it recommend complete and correct multi-entity edits. © 2018, Springer Nature Switzerland AG.},
keywords={Artificial intelligence;  Computer science;  Computers, Automatic approaches;  Change suggestion;  Common field access;  Empirical studies;  Evaluation results;  Maintenance tasks;  Multi-entity edit;  Multiple program, Software testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gharehyazie2018,
author={Gharehyazie, M. and Ray, B. and Keshani, M. and Zavosht, M.S. and Heydarnoori, A. and Filkov, V.},
title={Cross-project code clones in GitHub},
journal={Empirical Software Engineering},
year={2018},
doi={10.1007/s10664-018-9648-z},
note={cited By 0; Article in Press},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053470784&doi=10.1007%2fs10664-018-9648-z&partnerID=40&md5=27dab325aad12a4b2d61c5f022f30cbf},
abstract={Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others’. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one’s needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts. Our contribution is two fold. First, to understand cross-project code reuse, here we present an in-depth empirical study of cloning in GitHub. Using Deckard, a popular clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different analysis methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Second, we utilized these results to develop a novel tool named CLONE-HUNTRESS that streamlines finding and tracking code clones in GitHub. The tool is GitHub integrated, built around a user-friendly interface and runs efficiently over a modern database system. We describe the tool and make it publicly available at http://clone-det.ictic.sharif.edu/. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
keywords={Cloning;  Codes (symbols);  Computer software reusability;  Open source software, Clone detection;  Coding efficiency;  Deckard;  Different domains;  GitHub;  Multiple-case study;  Social programming;  User friendly interface, Open systems},
document_type={Article in Press},
source={Scopus},
}

@CONFERENCE{Xin2017660,
author={Xin, Q. and Reiss, S.P.},
title={Leveraging syntax-related code for automated program repair},
journal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
year={2017},
pages={660-670},
doi={10.1109/ASE.2017.8115676},
art_number={8115676},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041434410&doi=10.1109%2fASE.2017.8115676&partnerID=40&md5=d6fd3994375177e5397fddaadf7c1d01},
abstract={We present our automated program repair technique ssFix which leverages existing code (from a code database) that is syntax-related to the context of a bug to produce patches for its repair. Given a faulty program and a fault-exposing test suite, ssFix does fault localization to identify suspicious statements that are likely to be faulty. For each such statement, ssFix identifies a code chunk (or target chunk) including the statement and its local context. ssFix works on the target chunk to produce patches. To do so, it first performs syntactic code search to find candidate code chunks that are syntax-related, i.e., structurally similar and conceptually related, to the target chunk from a code database (or codebase) consisting of the local faulty program and an external code repository. ssFix assumes the correct fix to be contained in the candidate chunks, and it leverages each candidate chunk to produce patches for the target chunk. To do so, ssFix translates the candidate chunk by unifying the names used in the candidate chunk with those in the target chunk; matches the chunk components (expressions and statements) between the translated candidate chunk and the target chunk; and produces patches for the target chunk based on the syntactic differences that exist between the matched components and in the unmatched components. ssFix finally validates the patched programs generated against the test suite and reports the first one that passes the test suite. We evaluated ssFix on 357 bugs in the Defects4J bug dataset. Our results show that ssFix successfully repaired 20 bugs with valid patches generated and that it outperformed five other repair techniques for Java. © 2017 IEEE.},
keywords={Automation;  Repair;  Software engineering;  Software testing;  Syntactics, Code database;  Code search;  code transfer;  External code;  Fault localization;  Local contexts;  Repair techniques, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fish201748,
author={Fish, A. and Nguyen, T.L. and Song, M.},
title={A code inspection tool by mining recurring changes in evolving software},
journal={SoftwareMining 2017 - Proceedings of the 2017 6th IEEE/ACM International Workshop on Software Mining, co-located with ASE 2017},
year={2017},
pages={48-51},
doi={10.1109/SOFTWAREMINING.2017.8100853},
art_number={8100853},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040762340&doi=10.1109%2fSOFTWAREMINING.2017.8100853&partnerID=40&md5=4fc7038021b5ef1af07631d5236c03cd},
abstract={Mining software repositories have frequently been investigated in recent research. Software modification in repositories are often recurring changes, similar but different changes across multiple locations. It is not easy for developers to find all the relevant locations to maintain such changes, including bug-fixes, new feature addition, and refactorings. Performing recurring changes is tedious and error-prone, resulting in in-consistent and missing updates. To address this problem, we present CloneMap, a clone-aware code inspection tool that helps developers ensure correctness of recurring changes to multiple locations in an evolving software. CloneMap allows developers to specify the old and new versions of a program. It then applies a clone detection technique to (1) mine repositories for extracting differences of recurring changes, (2) visualize the clone evolution, and (3) help developers focus their attention to potential anomalies, such as inconsistent and missing updates. © 2017 IEEE.},
keywords={Inspection equipment;  Location;  Machine tools, Bug fixes;  Clone detection techniques;  Code inspections;  Error prones;  Mining software repositories;  Recent researches;  Refactorings;  Software modification, Cloning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dotzler2017798,
author={Dotzler, G. and Kamp, M. and Kreutzer, P. and Philippsen, M.},
title={More accurate recommendations for method-level changes},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={798-808},
doi={10.1145/3106237.3106276},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030764759&doi=10.1145%2f3106237.3106276&partnerID=40&md5=8d50b7e1dff7e2bf9f396e921934cae8},
abstract={During the life span of large software projects, developers often apply the same code changes to different code locations in slight variations. Since the application of these changes to all locations is time-consuming and error-prone, tools exist that learn change patterns from input examples, search for possible pattern applications, and generate corresponding recommendations. In many cases, the generated recommendations are syntactically or semantically wrong due to code movements in the input examples. Thus, they are of low accuracy and developers cannot directly copy them into their projects without adjustments. We present the Accurate REcommendation System (ARES) that achieves a higher accuracy than other tools because its algorithms take care of code movements when creating patterns and recommendations. On average, the recommendations by ARES have an accuracy of 96% with respect to code changes that developers have manually performed in commits of source code archives. At the same time ARES achieves precision and recall values that are on par with other tools. © 2017 Copyright held by the owner/author(s).},
keywords={Codes (symbols);  Recommender systems, Change patterns;  Code changes;  Error prones;  Level change;  Precision and recall;  Program transformations;  Refactorings;  Software project, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yi2017740,
author={Yi, J. and Ahmed, U.Z. and Karkare, A. and Tan, S.H. and Roychoudhury, A.},
title={A feasibility study of using automated program repair for introductory programming assignments},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={740-751},
doi={10.1145/3106237.3106262},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030782216&doi=10.1145%2f3106237.3106262&partnerID=40&md5=12bd9d8ae4955e88d48f136a1f811852},
abstract={Despite the fact that an intelligent tutoring system for programming (ITSP) has long attracted interest, its widespread use has been hindered by the difficulty of generating personalized feedback automatically. Meanwhile, automated program repair (APR) is an emerging new technology that automatically fixes software bugs, and it has been shown that APR can fix the bugs of large real-world software. In this paper, we study the feasibility of marrying an ITSP and APR. We perform our feasibility study with four stateof- the-art APR tools (GenProg, AE, Angelix, and Prophet), and 661 programs written by the students taking an introductory programming course. We found that when APR tools are used out of the box, only about 30% of the programs in our dataset are repaired. This low repair rate is largely due to the student programs often being significantly incorrect - in contrast, professional software for which APR was successfully applied typically fails only a small portion of tests. To bridge this gap, we adopt in APR a new repair policy akin to the hint generation policy employed in the existing ITSP. This new repair policy admits partial repairs that address part of failing tests, which results in 84% improvement of repair rate. We also performed a user study with 263 novice students and 37 graders, and identified an understudied problem; while the graders seem to gain benefits from repairs, novice students do not seem to know how to effectively make use of generated repairs as hints. © 2017 Association for Computing Machinery.},
keywords={Computer aided instruction;  Education computing;  Planning;  Repair;  Software engineering;  Software testing;  Students;  Teaching;  Technology transfer, Feasibility studies;  Intelligent tutoring system;  Introductory programming;  Introductory programming course;  Personalized feedback;  Professional software;  State of the art;  Student project, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Long2017727,
author={Long, F. and Amidon, P. and Rinard, M.},
title={Automatic inference of code transforms for patch generation},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={727-739},
doi={10.1145/3106237.3106253},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030788844&doi=10.1145%2f3106237.3106253&partnerID=40&md5=5cd20407333b20227c6a027cb3fc15c5},
abstract={We present a new system, Genesis, that processes human patches to automatically infer code transforms for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the complete Genesis patch generation system working with real-world patches and defects collected from 372 Java projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from previous successful patches. © 2017 Association for Computing Machinery.},
keywords={Codes (symbols);  Inference engines;  Software engineering, Automatic inference;  First systems;  Generation systems;  Inference algorithm;  Patch generation;  Real-world;  Search spaces, Automatic programming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rolim2017404,
author={Rolim, R. and Soares, G. and D'Antoni, L. and Polozov, O. and Gulwani, S. and Gheyi, R. and Suzuki, R. and Hartmann, B.},
title={Learning syntactic program transformations from examples},
journal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017},
year={2017},
pages={404-415},
doi={10.1109/ICSE.2017.44},
art_number={7985680},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019549570&doi=10.1109%2fICSE.2017.44&partnerID=40&md5=b3a45063807a0c115be86bd635b81949},
abstract={Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-The-Art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average. © 2017 IEEE.},
keywords={Codes (symbols);  Computer programming;  Computer programming languages;  Computer systems programming;  Costs;  Digital subscriber lines;  Open source software;  Problem oriented languages;  Software engineering;  Students, Computer science students;  Open source projects;  Program synthesis;  Program transformations;  Programming assignments;  Programming by Example;  Refactorings;  Tutoring system, Education},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Koyuncu2017237,
author={Koyuncu, A. and Bissyandé, T.F. and Kim, D. and Klein, J. and Monperrus, M. and Traon, Y.L.},
title={Impact of tool support in patch construction},
journal={ISSTA 2017 - Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2017},
pages={237-248},
doi={10.1145/3092703:3092713},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026625753&doi=10.1145%2f3092703%3a3092713&partnerID=40&md5=c5e665b8c6bf9d1a26a98e87ee2ae41f},
abstract={In this work, we investigate the practice of patch construction in the Linux kernel development, focusing on the differences between three patching processes: (1) patches crafted entirely manually to fix bugs, (2) those that are derived from warnings of bug detection tools, and (3) those that are automatically generated based on fix patterns. With this study, we provide to the research community concrete insights on the practice of patching as well as how the development community is currently embracing research and commercial patching tools to improve productivity in repair. The result of our study shows that tool-supported patches are increasingly adopted by the developer community while manually-written patches are accepted more quickly. Patch application tools enable developers to remain committed to contributing patches to the code base. Our findings also include that, in actual development processes, patches generally implement several change operations spread over the code, even for patches fixing warnings by bug detection tools. Finally, this study has shown that there is an opportunity to directly leverage the output of bug detection tools to readily generate patches that are appropriate for fixing the problem, and that are consistent with manually-written patches. © 2017 ACM.},
keywords={Automation;  Computer debugging;  Computer operating systems;  Inspection equipment;  Linux;  Open systems;  Productivity;  Repair;  Software testing;  Tools, Automatically generated;  Bug detection;  Change operations;  Development community;  Development process;  Empirical;  Patch;  Research communities, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhong2017144,
author={Zhong, H. and Meng, N.},
title={An empirical study on using hints from past fixes},
journal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
year={2017},
pages={144-145},
doi={10.1109/ICSE-C.2017.88},
art_number={7965283},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026755401&doi=10.1109%2fICSE-C.2017.88&partnerID=40&md5=e56812c9b9192ba1782ebbd39b3064e8},
abstract={With the usage of version control systems, many bugfixes have accumulated over the years. Researchers have proposedvarious approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new bug fixes can beconstructed from old fixes, have not been investigated. When anapproach reuses past fixes to fix a new bug, the new bug fixshould overlap with past fixes in terms of code structures and/orcode names. Based on this intuition, we systematically design sixoverlap metrics, and conduct an empirical study on 5,735 bugfixes to investigate the usefulness of past fixes. For each bug fix, we create delta dependency graphs, and identify how bug fixesoverlap with each other by detecting isomorphic subgraphs. Ourresults show Besides that above two major findings, we haveadditional ten findings, which can deepen the understanding onautomatic program repair. © 2017 IEEE.},
keywords={Costs;  Repair;  Software engineering, Automatic programs;  Bug fixes;  Code structure;  Dependency graphs;  Empirical studies;  Isomorphic subgraphs;  Version control system, C (programming language)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Molderez2017248,
author={Molderez, T. and Stevens, R. and De Roover, C.},
title={Mining Change Histories for Unknown Systematic Edits},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2017},
pages={248-256},
doi={10.1109/MSR.2017.12},
art_number={7962375},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026513536&doi=10.1109%2fMSR.2017.12&partnerID=40&md5=0f73ceace2f21e830f52db18f6587a08},
abstract={Software developers often need to repeat similar modifications in multiple different locations of a system's source code. These repeated similar modifications, or systematic edits, can be both tedious and error-prone to perform manually. While there are tools that can be used to assist in automating systematic edits, it is not straightforward to find out where the occurrences of a systematic edit are located in an existing system. This knowledge is valuable to help decide whether refactoring is needed, or whether future occurrences of an existing systematic edit should be automated. In this paper, we tackle the problem of finding unknown systematic edits using a closed frequent itemset mining algorithm, operating on sets of distilled source code changes. This approach has been implemented for Java programs in a tool called SysEdMiner. To evaluate the tool's precision and scalability, we have applied it to an industrial use case. © 2017 IEEE.},
keywords={Change distilling;  Closed frequent itemset;  Existing systems;  Frequent itemset mining;  Industrial use case;  Software developer;  Source code changes;  Systematic edits, Computer software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gharehyazie2017291,
author={Gharehyazie, M. and Ray, B. and Filkov, V.},
title={Some from Here, Some from There: Cross-Project Code Reuse in GitHub},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2017},
pages={291-301},
doi={10.1109/MSR.2017.15},
art_number={7962379},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026520340&doi=10.1109%2fMSR.2017.15&partnerID=40&md5=889f5e4fec932c0467a48a34aca0713e},
abstract={Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others'. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one's needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts. To understand cross-project code reuse, here we present an in-depth study of cloning in GitHub. Using Deckard, a clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Our results show directions for new tools that can facilitate code foraging and sharing within GitHub. © 2017 IEEE.},
keywords={Cloning;  Codes (symbols);  Computer software reusability;  Genetic engineering;  Open source software;  Software engineering, Code reuse;  Coding efficiency;  Different domains;  Emerging patterns;  GitHub;  Multiple-case study;  Search behavior;  Social programming, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stevens2017171,
author={Stevens, R. and De Roover, C.},
title={Extracting executable transformations from distilled code changes},
journal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
year={2017},
pages={171-181},
doi={10.1109/SANER.2017.7884619},
art_number={7884619},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018416465&doi=10.1109%2fSANER.2017.7884619&partnerID=40&md5=ee0189b8cc4dd9b95dc6ef1fa7877eaa},
abstract={Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. Using examples, we demonstrate the expressiveness of specifying source code evolutions through intermediate ASTs. We also show that our approach is able to recall different implementation variants of the same source code evolution in open-source histories. © 2017 IEEE.},
keywords={Codes (symbols);  Computer programming languages;  Open source software;  Reengineering, Code changes;  Evolution patterns;  Fine-grained changes;  Intermediate state;  Mining software repositories;  Multiple changes;  Open sources;  Source codes, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Santos2017160,
author={Santos, G. and Paixao, K.V.R. and Anquetil, N. and Etien, A. and De Almeida Maia, M. and Ducasse, S.},
title={Recommending source code locations for system specific transformations},
journal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
year={2017},
pages={160-170},
doi={10.1109/SANER.2017.7884618},
art_number={7884618},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018376376&doi=10.1109%2fSANER.2017.7884618&partnerID=40&md5=2c8bab49fecd81753c4919caaa216628},
abstract={From time to time, developers perform sequences of code transformations in a systematic and repetitive way. This may happen, for example, when introducing a design pattern in a legacy system: similar classes have to be introduced, containing similar methods that are called in a similar way. Automation of these sequences of transformations has been proposed in the literature to avoid errors due to their repetitive nature. However, developers still need support to identify all the relevant code locations that are candidate for transformation. Past research showed that these kinds of transformation can lag for years with forgotten instances popping out from time to time as other evolutions bring them into light. In this paper, we evaluate three distinct code search approaches ('structural', based on Information Retrieval, and AST based algorithm) to find code locations that would require similar transformations. We validate the resulting candidate locations from these approaches on real cases identified previously in literature. The results show that looking for code with similar roles, e.g., classes in the same hierarchy, provides interesting results with an average recall of 87% and in some cases the precision up to 70%. © 2017 IEEE.},
keywords={Cosine transforms;  Legacy systems;  Location;  Reengineering, Candidate locations;  Code search;  Code transformation;  Design Patterns;  Real case;  Source codes;  System specific, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Park2017436,
author={Park, J. and Kim, M. and Bae, D.-H.},
title={An empirical study of supplementary patches in open source projects},
journal={Empirical Software Engineering},
year={2017},
volume={22},
number={1},
pages={436-473},
doi={10.1007/s10664-016-9432-x},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966430205&doi=10.1007%2fs10664-016-9432-x&partnerID=40&md5=6acc9bac416ceeb6c49a59d2f8ad4733},
abstract={Developers occasionally make more than one patch to fix a bug. The related patches sometimes are intentionally separated, but unintended omission errors require supplementary patches. Several change recommendation systems have been suggested based on clone analysis, structural dependency, and historical change coupling in order to reduce or prevent incomplete patches. However, very few studies have examined the reason that incomplete patches occur and how real-world omission errors could be reduced. This paper systematically studies a group of bugs that were fixed more than once in open source projects in order to understand the characteristics of incomplete patches. Our study on Eclipse JDT core, Eclipse SWT, Mozilla, and Equinox p2 showed that a significant portion of the resolved bugs require more than one attempt to fix. Compared to single-fix bugs, the multi-fix bugs did not have a lower quality of bug reports, but more attribute changes (i.e., cc’ed developers or title) were made to the multi-fix bugs than to the single-fix bugs. Multi-fix bugs are more likely to have high severity levels than single-fix bugs. Hence, more developers have participated in discussions about multi-fix bugs compared to single-fix bugs. Multi-fix bugs take more time to resolve than single-fix bugs do. Incomplete patches are longer and more scattered, and they are related to more files than regular patches are. Our manual inspection showed that the causes of incomplete patches were diverse, including missed porting updates, incorrect handling of conditional statements, and incomplete refactoring. Our investigation showed that only 7 % to 17 % of supplementary patches had content similar to their initial patches, which implies that supplementary patch locations cannot be predicted by code clone analysis alone. Furthermore, 16 % to 46 % of supplementary patches were beyond the scope of the immediate structural dependency of their initial patch locations. Historical co-change patterns also showed low precision in predicting supplementary patch locations. Code clones, structural dependencies, and historical co-change analyses predicted different supplementary patch locations, and there was little overlap between them. Combining these analyses did not cover all supplementary patch locations. The present study investigates the characteristics of incomplete patches and multi-fix bugs, which have not been systematically examined in previous research. We reveal that predicting supplementary patch is a difficult problem that existing change recommendation approaches could not cover. New type of approaches should be developed and validated on a supplementary patch data set, which developers failed to make the complete patches at once in practice. © 2016, Springer Science+Business Media New York.},
keywords={Cloning;  Location;  Open source software;  Open systems;  Program debugging, Bug fixes;  Change patterns;  Empirical studies;  Historical changes;  Manual inspection;  Open source projects;  Patches;  Software Evolution, Costs},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Newman2017604,
author={Newman, C.D. and Maletic, J.I. and Collard, M.L.},
title={SrcType: A tool for efficient static type resolution},
journal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
year={2017},
pages={604-606},
doi={10.1109/ICSME.2016.38},
art_number={7816517},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013135300&doi=10.1109%2fICSME.2016.38&partnerID=40&md5=1b9e6a4cc24aa6aa56c4bbb8ef842210},
abstract={An efficient, static type resolution tool is presented. The tool is implemented on top of srcML; an XML representation of source code and abstract syntax. The approach computes the type of every identifier (i.e., function names and variable names) within the provided body of code. The result is a dictionary that can be used to lookup the type of each name. Type information includes metadata such as constness, class membership, aliasing, line number, file, and namespace. The approach is highly scalable and can generate a dictionary for Linux (13 MLOC) in less than 7 minutes. The tool is open source under a GPL license and available for download at srcML.org. © 2016 IEEE.},
keywords={Computer operating systems;  Computer software maintenance;  Static analysis, Abstract syntax;  Aliasing;  Namespaces;  Open sources;  Source codes;  SrcML;  Type information;  XML representation, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nguyen2017200,
author={Nguyen, H.A. and Nguyen, A.T. and Nguyen, T.N.},
title={Using topic model to suggest fine-grained source code changes},
journal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
year={2017},
pages={200-210},
doi={10.1109/ICSME.2016.40},
art_number={7816467},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013124289&doi=10.1109%2fICSME.2016.40&partnerID=40&md5=09306020f4e5f855f93b77c1c95cb44e},
abstract={Prior research has shown that source code and its changes are repetitive. Several approaches have leveraged that phenomenon to detect and recommend change/fix patterns. In this paper, we propose TasC, a model that leverages the context of change tasks in development history to suggest fine-grained code change/fix at the program statement level. We use Latent Dirichlet Allocation (LDA) to capture the change task context via co-occurring program elements in the changes in a context. We also propose a novel technique for measuring the similarity of code fragments and code changes using the task context. We conducted an empirical evaluation on a large dataset of 88 open-source Java projects containing more than 200 thousand source files and 3.5 million source lines of code in their last revisions with 423 thousand changed methods. Our result shows that TasC relatively improves recommendation accuracy up to 130%-250% in comparison with the base models that do not use task context. Compared with other types of contexts, TasC outperforms the models using structural and co-change contexts. © 2016 IEEE.},
keywords={Codes (symbols);  Computer software maintenance;  Open source software;  Statistics, Development history;  Empirical evaluations;  Fine-grained source code changes;  Latent dirichlet allocations;  Novel techniques;  Program statements;  Recommendation accuracy;  Source lines of codes, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ma2017229,
author={Ma, S. and Thung, F. and Lo, D. and Sun, C. and Deng, R.H.},
title={VuRLE: Automatic vulnerability detection and repair by learning from examples},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10493 LNCS},
pages={229-246},
doi={10.1007/978-3-319-66399-9_13},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029517506&doi=10.1007%2f978-3-319-66399-9_13&partnerID=40&md5=8b63e1a9313f5c42720fddd3bf5adfff},
abstract={Vulnerability becomes a major threat to the security of many systems. Attackers can steal private information and perform harmful actions by exploiting unpatched vulnerabilities. Vulnerabilities often remain undetected for a long time as they may not affect typical systems’ functionalities. Furthermore, it is often difficult for a developer to fix a vulnerability correctly if he/she is not a security expert. To assist developers to deal with multiple types of vulnerabilities, we propose a new tool, called VuRLE, for automatic detection and repair of vulnerabilities. VuRLE (1) learns transformative edits and their contexts (i.e., code characterizing edit locations) from examples of vulnerable codes and their corresponding repaired codes; (2) clusters similar transformative edits; (3) extracts edit patterns and context patterns to create several repair templates for each cluster. VuRLE uses the context patterns to detect vulnerabilities, and customizes the corresponding edit patterns to repair them. We evaluate VuRLE on 279 vulnerabilities from 48 real-world applications. Under 10-fold cross validation, we compare VuRLE with another automatic repair tool, LASE. Our experiment shows that VuRLE successfully detects 183 out of 279 vulnerabilities, and repairs 101 of them, while LASE can only detect 58 vulnerabilities and repair 21 of them. © 2017, Springer International Publishing AG.},
keywords={Codes (symbols);  Security of data;  Security systems, 10-fold cross-validation;  Automatic Detection;  Context patterns;  Learning from examples;  Private information;  Security experts;  Template generation;  Vulnerability detection, Repair},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rolim20161063,
author={Rolim, R.},
title={Automating repetitive code changes using examples},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={1063-1065},
doi={10.1145/2950290.2983944},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997497946&doi=10.1145%2f2950290.2983944&partnerID=40&md5=ccedf65ba5689744cdb731e68ae50b0c},
abstract={While adding features, fixing bugs, or refactoring the code, developers may perform repetitive code edits. Although Integrated Development Environments (IDEs) automate some transformations such as renaming, many repetitive edits are performed manually, which is error-prone and time-consuming. To help developers to apply these edits, we propose a technique to perform repetitive edits using examples. The technique receives as input the source code before and after the developer edits some target locations of the change and produces as output the top-ranked program transformation that can be applied to edit the remaining target locations in the codebase. The technique uses a state-of-The-Art program synthesis methodology and has three main components: A) a DSL for describing program transformations; b) synthesis algorithms to learn program transformations in this DSL; c) ranking algorithms to select the program transformation with the higher probability of performing the desired repetitive edit. In our preliminary evaluation, in a dataset of 59 repetitive edit cases taken from real C# source code repositories, the technique performed, in 83% of the cases, the intended transformation using only 2.8 examples. © 2016 ACM.},
keywords={Digital subscriber lines;  Software engineering, Integrated development environment;  Program synthesis;  Program transformations;  Programming by Example;  Ranking algorithm;  Software Evolution;  State of the art;  Synthesis algorithms, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nguyen2016511,
author={Nguyen, A.T. and Hilton, M. and Codoban, M. and Nguyen, H.A. and Mast, L. and Rademacher, E. and Nguyen, T.N. and Dig, D.},
title={API code recommendation using statistical learning from fine-grained changes},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={511-522},
doi={10.1145/2950290.2950333},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997525166&doi=10.1145%2f2950290.2950333&partnerID=40&md5=3288ad1884b2a99e3b94e5d624ac7441},
abstract={Learning and remembering how to use APIs is difficult. While codecompletion tools can recommend API methods, browsing a long list of API method names and their documentation is tedious. Moreover, users can easily be overwhelmed with too much information. We present a novel API recommendation approach that taps into the predictive power of repetitive code changes to provide relevant API recommendations for developers. Our approach and tool, APIREC, is based on statistical learning from fine-grained code changes and from the context in which those changes were made. Our empirical evaluation shows that APIREC correctly recommends an API call in the first position 59% of the time, and it recommends the correct API call in the top 5 positions 77% of the time. This is a significant improvement over the state-of-The-Art approaches by 30-160% for top-1 accuracy, and 10-30% for top-5 accuracy, respectively. Our result shows that APIREC performs well even with a one-Time, minimal training dataset of 50 publicly available projects. © 2016 ACM.},
keywords={Software engineering, API Recommendation;  Code changes;  Empirical evaluations;  Fine-grained changes;  Minimal training;  Predictive power;  State-of-the-art approach;  Statistical learning, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hanam2016144,
author={Hanam, Q. and Brito, F.S.D.M. and Mesbah, A.},
title={Discovering bug patterns in Javascript},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={144-156},
doi={10.1145/2950290.2950308},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997194354&doi=10.1145%2f2950290.2950308&partnerID=40&md5=62585e7f8af13c465525ad608bdb900b},
abstract={JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug findings tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-Automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using languageconstruct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript. © 2016 ACM.},
keywords={Artificial intelligence;  Data mining;  Learning systems;  Software engineering;  Static analysis, Bug Patterns;  Javascript;  Large-scale studies;  Multiple projects;  Node.Js;  Semi-automatics;  Tools and techniques;  Unsupervised machine learning, High level languages},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2016393,
author={Wang, S. and Lo, D. and Jiang, L.},
title={AutoQuery: automatic construction of dependency queries for code search},
journal={Automated Software Engineering},
year={2016},
volume={23},
number={3},
pages={393-425},
doi={10.1007/s10515-014-0170-2},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908519669&doi=10.1007%2fs10515-014-0170-2&partnerID=40&md5=63c09f676ed6bd7b5d7824c48db980c3},
abstract={Many code search techniques have been proposed to return relevant code for a user query expressed as textual descriptions. However, source code is not mere text. It contains dependency relations among various program elements. To leverage these dependencies for more accurate code search results, techniques have been proposed to allow user queries to be expressed as control and data dependency relationships among program elements. Although such techniques have been shown to be effective for finding relevant code, it remains a question whether appropriate queries can be generated by average users. In this work, we address this concern by proposing a technique, AutoQuery, that can automatically construct dependency queries from a set of code snippets. We realize AutoQuery by the following major steps: firstly, code snippets (that are not necessarily compilable) are converted into program dependence graphs (PDGs); secondly, a new graph mining solution is built to return common structures in the PDGs; thirdly, the common structures are converted to dependency queries, which are used to retrieve results by using a dependence-based code search technique. We have evaluated AutoQuery on real systems with 47 different code search tasks. The results show that the automatically constructed dependency queries retrieve relevant code with a precision, recall, and F-measure of 68.4, 72.1, and 70.2 %, respectively. We have also performed a user study to compare the effectiveness of AutoQuery with that of human generated queries. The results show that queries constructed by AutoQuery on average help to retrieve code fragments with comparable F-measures to those retrieved by human constructed queries. © 2014, Springer Science+Business Media New York.},
keywords={Codes (symbols);  Structures (built objects), Automatic construction;  Code search;  Dependency query;  Dependency relation;  Graph mining;  Program dependence graph;  Query construction;  Textual description, Query processing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Dotzler2016660,
author={Dotzler, G. and Philippsen, M.},
title={Move-optimized source code tree differencing},
journal={ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year={2016},
pages={660-671},
doi={10.1145/2970276.2970315},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989170976&doi=10.1145%2f2970276.2970315&partnerID=40&md5=708e077a12335b0c8c20f6be72c81707},
abstract={When it is necessary to express changes between two source code files as a list of edit actions (an edit script), modern tree differencing algorithms are superior to most text-based approaches because they take code movements into account and express source code changes more accurately. We present 5 general optimizations that can be added to state-of-the-art tree differencing algorithms to shorten the resulting edit scripts. Applied to Gumtree, RTED, JSync, and ChangeDistiller, they lead to shorter scripts for 18{ 98% of the changes in the histories of 9 open-source software repositories. These optimizations also are parts of our novel Move-optimized Tree DIFFerencing algorithm (MTDIFF) that has a higher accuracy in detecting moved code parts. MTDIFF (which is based on the ideas of ChangeDistiller) further shortens the edit script for another 20% of the changes in the repositories. MTDIFF and all the benchmarks are available under an open-source license. © 2016 ACM.},
keywords={Codes (symbols);  Computer programming languages;  Forestry;  Open systems;  Optimization;  Software engineering;  Trees (mathematics), Differencing algorithm;  General optimizations;  Open source license;  Source code changes;  Source codes;  State of the art;  Text-based approach;  Tree Differencing, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cheng2016,
author={Cheng, X. and Zhong, H. and Chen, Y. and Hu, Z. and Zhao, J.},
title={Rule-directed code clone synchronization},
journal={IEEE International Conference on Program Comprehension},
year={2016},
volume={2016-July},
doi={10.1109/ICPC.2016.7503722},
art_number={7503722},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979771503&doi=10.1109%2fICPC.2016.7503722&partnerID=40&md5=fd90ba24b000c19011e5153fdb1a7657},
abstract={Code clones are prevalent in software systems due to many factors in software development. Detecting code clones and managing consistency between them along code evolution can be very useful for reducing clone-related bugs and maintenance costs. Despite some early attempts at detecting code clones and managing the consistency between them, the state-of-the-art tool can only handle simple code clones whose structures are identical or quite similar. However, existing empirical studies show that clones can have quite different structures with their evolution, which can easily go beyond the capability of the state-of-the-art tool. In this paper, we propose CCSync, a novel, rule-directed approach, which paves the structure differences between the code clones and synchronizes them even when code clones become quite different in their structures. The key steps of this approach are, given two code clones, to (1) extract a synchronization rule from the relationship between the clones, and (2) once one code fragment is updated, propagate the modifications to the other following the synchronization rule. We have implemented a tool for CCSync and evaluated its effectiveness on five Java projects. Our results shows that there are many code clones suitable for synchronization, and our tool achieves precisions of up to 92% and recalls of up to 84%. In particular, more than 76% of our generated revisions are identical with manual revisions. © 2016 IEEE.},
keywords={Codes (symbols);  Computer programming;  Software design;  Synchronization, Code clone;  Code fragments;  Different structure;  Empirical studies;  Maintenance cost;  Software systems;  State of the art;  Structure difference, Cloning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chugh2016781,
author={Chugh, R.},
title={Prodirect manipulation: Bidirectional programming for the masses},
journal={Proceedings - International Conference on Software Engineering},
year={2016},
pages={781-784},
doi={10.1145/2889160.2889210},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975843943&doi=10.1145%2f2889160.2889210&partnerID=40&md5=f73cb929f8a53369cadc6ca98e99ca61},
abstract={Software interfaces today generally fall at either end of a spectrum. On one end are programmable systems, which allow expert users (i.e. programmers) to write software artifacts that describe complex abstractions, but programs are disconnected from their eventual output. On the other end are domain-specific graphical user interfaces (GUIs), which allow end users (i.e. non-programmers) to easily create varied content but present insurmountable walls when a desired feature is not built-in. Both programmatic and direct manipulation have distinct strengths, but users must typically choose one over the other or use some ad-hoc combination of systems. Our goal, put simply, is to bridge this divide. We envision novel software systems that tightly couple programmatic and direct manipulation - - a combination we dub prodirect manipulation - - for a variety of use cases. This will require advances in a broad range of software engineering disciplines, from program analysis and program synthesis technology to user interface design and evaluation. In this extended abstract, we propose two general strategies - - real-time program synthesis and domain-specific synthesis of general-purpose programs - - that may prove fruitful for overcoming the technical challenges. We also discuss metrics that will be important in evaluating the usability and utility of prodirect manipulation systems. © 2016 ACM.},
keywords={Domain walls;  Graphical user interfaces;  Human computer interaction;  Software engineering;  User interfaces, Bi-directional programming;  End user programming;  Engineering disciplines;  Graphical user interface (GUIs);  Prodirect manipulation;  Program synthesis;  Programmable systems;  User interface designs, Computer programming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Selakovic201661,
author={Selakovic, M. and Pradel, M.},
title={Performance issues and optimizations in Java script: An empirical study},
journal={Proceedings - International Conference on Software Engineering},
year={2016},
volume={14-22-May-2016},
pages={61-72},
doi={10.1145/2884781.2884829},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971483495&doi=10.1145%2f2884781.2884829&partnerID=40&md5=6fe6b730ce029b2e0572c4fb441821fe},
abstract={As JavaScript is becoming increasingly popular, the performance of JavaScript programs is crucial to ensure the responsiveness and energy-effciency of thousands of programs. Yet, little is known about performance issues that developers face in practice and they address these issues. This paper presents an empirical study of 98 fixed performance issues from 16 popular client-side and server-side JavaScript projects. We identify eight root causes of issues and show that ineffcient usage of APIs is the most prevalent root cause. Furthermore, we find that most issues are addressed by optimizations that modify only a few lines of code, without significantly affecting the complexity of the source code. By studying the performance impact of optimizations on several versions of the SpiderMonkey and V8 engines, we find that only 42.68% of all optimizations improve performance consistently across all versions of both engines. Finally, we observe that many optimizations are instances of patterns applicable across projects, as evidenced by 139 previously unknown optimization opportunities that we find based on the patterns identified during the study. The results of the study help application developers to avoid common mistakes, researchers to develop performance-related techniques that address relevant problems, and engine developers to address prevalent bottleneck patterns. © 2016 ACM.},
keywords={Engines;  High level languages, Application developers;  Empirical studies;  Improve performance;  JavaScript programs;  Lines of code;  Performance impact;  Performance issues;  Source codes, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kreutzer201661,
author={Kreutzer, P. and Dotzler, G. and Ring, M. and Eskofier, B.M. and Philippsen, M.},
title={Automatic clustering of code changes},
journal={Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},
year={2016},
pages={61-72},
doi={10.1145/2901739.2901749},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974623425&doi=10.1145%2f2901739.2901749&partnerID=40&md5=5ceb185eb95befc365edaab3d81d6045},
abstract={Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes. To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes. We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes. © 2016 ACM.},
keywords={Codes (symbols);  Open source software;  Open systems, Automatic clustering;  Bug finding tools;  Clustering;  Code changes;  Open source projects;  Research tools;  Similarity metrics;  Software repositories, Clustering algorithms},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Gulwani2016137,
author={Gulwani, S.},
title={Programming by examples (and its applications in data wrangling)},
journal={Dependable Software Systems Engineering},
year={2016},
volume={45},
pages={137-158},
doi={10.3233/978-1-61499-627-9-137},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980006611&doi=10.3233%2f978-1-61499-627-9-137&partnerID=40&md5=bd32ebd0cd9f9c2c5cb779e705d80859},
abstract={Programming by Examples (PBE) has the potential to revolutionize enduser programming by enabling end users, most of whom are non-programmers, to create scripts for automating repetitive tasks. PBE involves synthesizing intended programs in an underlying domain-specific language (DSL) from example based specifications (Ispec).We formalize the notion of Ispec and discuss some principles behind designing useful DSLs for synthesis. A key technical challenge in PBE is to search for programs that are consistent with the Ispec provided by the user. We present a divide-and-conquer based search paradigm that leverages deductive rules and version space algebras for manipulating sets of programs. Another technical challenge in PBE is to resolve the ambiguity that is inherent in the Ispec. We show how machine learning based ranking techniques can be used to predict an intended program within a set of programs that are consistent with the Ispec. We also present some user interaction models including program navigation and active-learning based conversational clarification that communicate actionable information to the user to help resolve ambiguity in the Ispec. The above-mentioned concepts are illustrated using practical PBE systems for data wrangling (including FlashFill, FlashExtract, FlashRelate), several of which have already been deployed in the real world. © 2016 The authors and IOS Press. All rights reserved.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Rodriguez2016132,
author={Rodriguez, L.R. and Lawall, J.},
title={Increasing Automation in the Backporting of Linux Drivers Using Coccinelle},
journal={Proceedings - 2015 11th European Dependable Computing Conference, EDCC 2015},
year={2016},
pages={132-143},
doi={10.1109/EDCC.2015.23},
art_number={7371961},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966388987&doi=10.1109%2fEDCC.2015.23&partnerID=40&md5=65eb9e0e1331de3a0f888de84ac6ada4},
abstract={Software is continually evolving, to fix bugs and add new features. Industry users, however, often value stability, and thus may not be able to update their code base to the latest versions. This raises the need to selectively backport new features to older software versions. Traditionally, backporting has been done by cluttering the backported code with preprocessor directives, to replace behaviors that are unsupported in an earlier version by appropriate workarounds. This approach however, involves writing a lot of error-prone backporting code, and results in implementations that are hard to read and maintain. We consider this issue in the context of the Linux kernel, for whicholder versions are in wide use. We present a new backporting strategy that relies on the use of a backporting compatability library and on code that is automatically generated using the program transformation tool Coccinelle. This approach reduces the amount of code that must be manually written, and thus can help the Linux kernel backporting effort scale while maintainingthe dependability of the backporting process. © 2015 IEEE.},
keywords={Codes (symbols);  Computer operating systems;  Program debugging, Automatically generated;  backports;  Compatability;  Error prones;  Linux drivers;  Linux kernel;  Program transformations;  Software versions, Linux},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gao2016307,
author={Gao, Q. and Zhang, H. and Wang, J. and Xiong, Y. and Zhang, L. and Mei, H.},
title={Fixing recurring crash bugs via analyzing Q&A sites},
journal={Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},
year={2016},
pages={307-318},
doi={10.1109/ASE.2015.81},
art_number={7372020},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963852339&doi=10.1109%2fASE.2015.81&partnerID=40&md5=075a4269b366c7b5059eecd63deb39c7},
abstract={Recurring bugs are common in software systems, especially in client programs that depend on the same framework. Existing research uses human-written templates, and is limited to certain types of bugs. In this paper, we propose a fully automatic approach to fixing recurring crash bugs via analyzing Q&A sites. By extracting queries from crash traces and retrieving a list of Q&A pages, we analyze the pages and generate edit scripts. Then we apply these scripts to target source code and filter out the incorrect patches. The empirical results show that our approach is accurate in fixing real-world crash bugs, and can complement existing bug-fixing approaches. © 2015 IEEE.},
keywords={Software engineering, And filters;  Automatic approaches;  Bug-fixing;  Client programs;  Real-world;  Software systems;  Target source, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kögel201614,
author={Kögel, S. and Groner, R. and Tichy, M.},
title={Automatic change recommendation of models and meta models based on change histories},
journal={CEUR Workshop Proceedings},
year={2016},
volume={1706},
pages={14-19},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996799553&partnerID=40&md5=5adf5f9259609fe9fd4bedd87fc420f2},
abstract={Model-driven software engineering uses models and meta models as key artefacts in the software development process. Typically, changes in the models (or meta models) do not come in isolation but are part of more complex change sets where a single change depends on other changes, e.g., a component is added to an architectural model and thereafter ports and connectors connect this component to other components. Furthermore, these sets of related and depending changes are often recurring, e.g., always when a component is added to an architecture, it is highly likely that ports are added to that component, too. This is similar for changes in meta models. Our goal is to help engineers by (1) automatically identifying clusters of related changes on model histories and (2) recommending corresponding changes after the engineer performs a single change. In this position paper, we present an initial technique to achieve our goal. We evaluate our technique with models from the Eclipse GMF project and present our recommendations as well as the recommendation quality. Our evaluation found an average precision between 0:43 and 0:82 for our recommendations.},
keywords={Quality control;  Software engineering, Architectural modeling;  Change history;  Change recommendation;  History minings;  Model driven development;  Model driven software engineering;  Position papers;  Software development process, Software design},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Molderez2016192,
author={Molderez, T. and de Roover, C.},
title={Search-based generalization and refinement of code templates},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9962 LNCS},
pages={192-208},
doi={10.1007/978-3-319-47106-8_13},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989880911&doi=10.1007%2f978-3-319-47106-8_13&partnerID=40&md5=246d2f112af4470291c1d69fae993ba4},
abstract={Several tools support code templates as a means to specify searches within a program’s source code. Despite their ubiquity, code templates can often prove difficult to specify, and may produce too many or too few match results. In this paper, we present a search-based approach to support developers in specifying templates. This approach uses a suite of mutation operators to recommend changes to a given template, such that it matches with a desired set of code snippets. We evaluate our approach on the problem of inferring a code template that matches all instances of a design pattern, given one instance as a starting template. © Springer International Publishing AG 2016.},
keywords={Codes (symbols);  Evolutionary algorithms;  Recommender systems;  Software engineering, Design Patterns;  Mutation operators;  Search-based;  Source codes;  Templates, Template matching},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gulwani20169,
author={Gulwani, S.},
title={Programming by examples: Applications, algorithms, and ambiguity resolution},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9706},
pages={9-14},
doi={10.1007/978-3-319-40229-1_2},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976614459&doi=10.1007%2f978-3-319-40229-1_2&partnerID=40&md5=cdb4b6460ace33a89696533595840a75},
abstract={99% of computer end users do not know programming, and struggle with repetitive tasks. Programming by Examples (PBE) can revolutionize this landscape by enabling users to synthesize intended programs from example based specifications. A key technical challenge in PBE is to search for programs that are consistent with the examples provided by the user. Our efficient search methodology is based on two key ideas: (i) Restriction of the search space to an appropriate domainspecific language that offers balanced expressivity and readability (ii) A divide-and-conquer based deductive search paradigm that inductively reduces the problem of synthesizing a program of a certain kind that satisfies a given specification into sub-problems that refer to sub-programs or sub-specifications. Another challenge in PBE is to resolve the ambiguity in the example based specification. We will discuss two complementary approaches: (a) machine learning based ranking techniques that can pick an intended program from among those that satisfy the specification, and (b) active-learning based user interaction models. The above concepts will be illustrated using FlashFill, FlashExtract, and FlashRelate— PBE technologies for data manipulation domains. These technologies, which have been released inside various Microsoft products, are useful for data scientists who spend 80% of their time wrangling with data. The Microsoft PROSE SDK allows easy construction of such technologies. © Springer International Publishing Switzerland 2016.},
keywords={Algorithms;  Artificial intelligence;  Learning systems;  Specifications;  Web browsers, Ambiguity resolution;  Data manipulations;  Divide and conquer;  Domain specific languages;  Programming by Example;  Ranking technique;  Technical challenges;  User interaction, Computer programming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Singh2016816,
author={Singh, R.},
title={BlinkFill: Semisupervised programming by example for syntactic string transformations},
journal={Proceedings of the VLDB Endowment},
year={2016},
volume={9},
number={10},
pages={816-827},
doi={10.14778/2977797.2977807},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979581533&doi=10.14778%2f2977797.2977807&partnerID=40&md5=66f5e5e705c159427fc948b4fa02c8b6},
abstract={The recent Programming By Example (PBE) techniques such as FlashFill have shown great promise for enabling end-users to perform data transformation tasks using inputoutput examples. Since examples are inherently an underspecification, there are typically a large number of hypotheses conforming to the examples, and the PBE techniques suffer from scalability issues for finding the intended program amongst the large space. We present a semi-supervised learning technique to significantly reduce this ambiguity by using the logical information present in the input data to guide the synthesis algorithm. We develop a data structure InputDataGraph to succinctly represent a large set of logical patterns that are shared across the input data, and use this graph to effciently learn substring expressions in a new PBE system BlinkFill. We evaluate BlinkFill on 207 real-world benchmarks and show that BlinkFill is significantly faster (on average 41×) and requires fewer input-output examples (1.27 vs 1.53) to learn the desired transformations in comparison to FlashFill. © 2016 VLDB Endowment.},
keywords={Human computer interaction;  Input output programs;  Supervised learning, Data transformation;  Programming by Example;  Scalability issue;  Semi-supervised;  Semi-supervised learning techniques;  String transformation;  Synthesis algorithms;  Underspecification, Metadata},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Santos2015221,
author={Santos, G. and Etien, A. and Anquetil, N. and Ducasse, S. and Valente, M.T.},
title={Recording and replaying system specific, source code transformations},
journal={2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings},
year={2015},
pages={221-230},
doi={10.1109/SCAM.2015.7335418},
art_number={7335418},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963579544&doi=10.1109%2fSCAM.2015.7335418&partnerID=40&md5=caab743909e7534391f231ce998796bd},
abstract={During its lifetime, a software system is under continuous maintenance to remain useful. Maintenance can be achieved in activities such as adding new features, fixing bugs, improving the system's structure, or adapting to new APIs. In such cases, developers sometimes perform sequences of code changes in a systematic way. These sequences consist of small code changes (e.g., create a class, then extract a method to this class), which are applied to groups of related code entities (e.g., some of the methods of a class). This paper presents the design and proof-of-concept implementation of a tool called MacroRecorder. This tool records a sequence of code changes, then it allows the developer to generalize this sequence in order to apply it in other code locations. In this paper, we discuss MACRORECORDER's approach that is independent of both development and transformation tools. The evaluation is based on previous work on repetitive code changes related to rearchitecting. MacroRecorder was able to replay 92% of the examples, which consisted in up to seven code entities modified up to 66 times. The generation of a customizable, large-scale transformation operator has the potential to efficiently assist code maintenance. © 2015 IEEE.},
keywords={Computer programming languages;  Computer software maintenance;  Cosine transforms;  Maintenance;  Program debugging, Automated code;  Continuous maintenance;  Programming by demon-stration;  Refactorings;  Scale transformation;  Software Evolution;  Source code transformation;  Transformation tools, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Santos2015221,
author={Santos, G. and Anquetil, N. and Etien, A. and Ducasse, S. and Valente, M.T.},
title={System specific, source code transformations},
journal={2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings},
year={2015},
pages={221-230},
doi={10.1109/ICSM.2015.7332468},
art_number={7332468},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961588842&doi=10.1109%2fICSM.2015.7332468&partnerID=40&md5=c7da4702e7797779c2fe047863c830a2},
abstract={During its lifetime, a software system might undergo a major transformation effort in its structure, for example to migrate to a new architecture or bring some drastic improvements to the system. Particularly in this context, we found evidences that some sequences of code changes are made in a systematic way. These sequences are composed of small code transformations (e.g., create a class, move a method) which are repeatedly applied to groups of related entities (e.g., a class and some of its methods). A typical example consists in the systematic introduction of a Factory design pattern on the classes of a package. We define these sequences as transformation patterns. In this paper, we identify examples of transformation patterns in real world software systems and study their properties: (i) they are specific to a system; (ii) they were applied manually; (iii) they were not always applied to all the software entities which could have been transformed; (iv) they were sometimes complex; and (v) they were not always applied in one shot but over several releases. These results suggest that transformation patterns could benefit from automated support in their application. From this study, we propose as future work to develop a macro recorder, a tool with which a developer records a sequence of code transformations and then automatically applies them in other parts of the system as a customizable, large-scale transformation operator. © 2015 IEEE.},
keywords={Codes (symbols);  Computer software;  Cosine transforms;  Maintenance, Code transformation;  Rearchitecting;  Refactoring tools;  Restructuring;  Scale transformation;  Source code transformation;  Transformation effort;  Transformation patterns, Computer software maintenance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lin2015520,
author={Lin, Y. and Peng, X. and Xing, Z. and Zheng, D. and Zhao, W.},
title={Clone-based and interactive recommendation for modifying pasted code},
journal={2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings},
year={2015},
pages={520-531},
doi={10.1145/2786805.2786871},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960403747&doi=10.1145%2f2786805.2786871&partnerID=40&md5=3deb374eda483a956e83cfea41b7cb04},
abstract={Developers often need to modify pasted code when programming with copy-and-paste practice. Some modifications on pasted code could involve lots of editing efforts, and any missing or wrong edit could incur bugs. In this paper, we propose a clone-based and interactive approach to recommending where and how to modify the pasted code. In our approach, we regard clones of the pasted code as the results of historical copy-and-paste operations and their differences as historical modifications on the same piece of code. Our approach first retrieves clones of the pasted code from a clone repository and detects syntactically complete differences among them. Then our approach transfers each clone difference into a modification slot on the pasted code, suggests options for each slot, and further mines modifying regulations from the clone differences. Based on the mined modifying regulations, our approach dynamically updates the suggested options and their ranking in each slot according to developer's modifications on the pasted code. We implement a proof-of-concept tool CCDemon based on our approach and evaluate its effectiveness based on code clones detected from five open source projects. The results show that our approach can identify 96.9% of the to-be-modified positions in pasted code and suggest 75.0% of the required modifications. Our human study further confirms that CCDemon can help developers to accomplish their modifications of pasted code more efficiently. © 2015 ACM.},
keywords={Cloning;  Codes (symbols);  Copying;  Open source software;  Software engineering, Code clone;  Copy-and-paste;  Differencing;  Recommendation;  Reuse, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2015320,
author={Chen, F. and Kim, S.},
title={Crowd debugging},
journal={2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings},
year={2015},
pages={320-332},
doi={10.1145/2786805.2786819},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960393115&doi=10.1145%2f2786805.2786819&partnerID=40&md5=b989d9a2a1cb36798ea196dae432e413},
abstract={Research shows that, in general, many people turn to QA sites to solicit answers to their problems. We observe in Stack Overflow a huge number of recurring questions, 1,632,590, despite mechanisms having been put into place to prevent these recurring questions. Recurring questions imply developers are facing similar issues in their source code. However, limitations exist in the QA sites. Developers need to visit them frequently and/or should be familiar with all the content to take advantage of the crowd's knowledge. Due to the large and rapid growth of QA data, it is difficult, if not impossible for developers to catch up. To address these limitations, we propose mining the QA site, Stack Overflow, to leverage the huge mass of crowd knowledge to help developers debug their code. Our approach reveals 189 warnings and 171 (90.5%) of them are confirmed by developers from eight high-quality and well-maintained projects. Developers appreciate these findings because the crowd provides solutions and comprehensive explanations to the issues. We compared the confirmed bugs with three popular static analysis tools (FindBugs, JLint and PMD). Of the 171 bugs identified by our approach, only FindBugs detected six of them whereas JLint and PMD detected none. © 2015 ACM.},
keywords={Computer debugging;  Knowledge management;  Software engineering, High quality;  Rapid growth;  Source codes;  Stack overflow, Static analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2015111,
author={Zhang, T. and Song, M. and Pinedo, J. and Kim, M.},
title={Interactive code review for systematic changes},
journal={Proceedings - International Conference on Software Engineering},
year={2015},
volume={1},
pages={111-122},
doi={10.1109/ICSE.2015.33},
art_number={7194566},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951825931&doi=10.1109%2fICSE.2015.33&partnerID=40&md5=0d25c8e6c882b9d18bad5a2a67b001e6},
abstract={Developers often inspect a diff patch during peer code reviews. Diff patches show low-level program differences per file without summarizing systematic changes-similar, related changes to multiple contexts. We present CRITICS, an interactive approach for inspecting systematic changes. When a developer specifies code change within a diff patch, CRITICS allows developers to customize the change template by iteratively generalizing change content and context. By matching a generalized template against the codebase, it summarizes similar changes and detects potential mistakes. We evaluated CRITICS using two methods. First, we conducted a user study at Salesforce.com, where professional engineers used CRITICS to investigate diff patches authored by their own team. After using CRITICS, all six participants indicated that they would like CRITICS to be integrated into their current code review environment. This also attests to the fact that CRITICS scales to an industry-scale project and can be easily adopted by professional engineers. Second, we conducted a user study where twelve participants reviewed diff patches using CRITICS and Eclipse diff. The results show that human subjects using CRITICS answer questions about systematic changes 47.3% more correctly with 31.9% saving in time during code review tasks, in comparison to the baseline use of Eclipse diff. These results show that CRITICS should improve developer productivity in inspecting systematic changes during peer code reviews. © 2015 IEEE.},
keywords={Inspection;  Iterative methods;  Software engineering, Current codes;  Human subjects;  Interactive approach;  Low-level programs;  Multiple contexts;  Peer code review;  Professional engineer;  Systematic changes, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nistor2015902,
author={Nistor, A. and Chang, P.-C. and Radoi, C. and Lu, S.},
title={CARAMEL: Detecting and fixing performance problems that have non-intrusive fixes},
journal={Proceedings - International Conference on Software Engineering},
year={2015},
volume={1},
pages={902-912},
doi={10.1109/ICSE.2015.100},
art_number={7194636},
note={cited By 45},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951840695&doi=10.1109%2fICSE.2015.100&partnerID=40&md5=9c0a889a5a1ffb4b6395f7ed47280ea6},
abstract={Performance bugs are programming errors that slow down program execution. While existing techniques can detect various types of performance bugs, a crucial and practical aspect of performance bugs has not received the attention it deserves: how likely are developers to fix a performance bug? In practice, fixing a performance bug can have both benefits and drawbacks, and developers fix a performance bug only when the benefits outweigh the drawbacks. Unfortunately, for many performance bugs, the benefits and drawbacks are difficult to assess accurately. This paper presents CARAMEL, a novel static technique that detects and fixes performance bugs that have non-intrusive fixes likely to be adopted by developers. Each performance bug detected by CARAMEL is associated with a loop and a condition. When the condition becomes true during the loop execution, all the remaining computation performed by the loop is wasted. Developers typically fix such performance bugs because these bugs waste computation in loops and have nonintrusive fixes: when some condition becomes true dynamically, just break out of the loop. Given a program, CARAMEL detects such bugs statically and gives developers a potential sourcelevel fix for each bug. We evaluate CARAMEL on real-world applications, including 11 popular Java applications (e.g., Groovy, Log4J, Lucene, Struts, Tomcat, etc) and 4 widely used C/C++ applications (Chromium, GCC, Mozilla, and MySQL). CARAMEL finds 61 new performance bugs in the Java applications and 89 new performance bugs in the C/C++ applications. Based on our bug reports, developers so far have fixed 51 and 65 performance bugs in the Java and C/C++ applications, respectively. Most of the remaining bugs are still under consideration by developers. © 2015 IEEE.},
keywords={C++ (programming language);  Costs;  Java programming language;  Software engineering, Bug reports;  Java applications;  Non-intrusive;  Performance bugs;  Performance problems;  Program execution;  Programming errors;  Static techniques, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Higo2015372,
author={Higo, Y. and Ohtani, A. and Hayashi, S. and Hata, H. and Shinji, K.},
title={Toward reusing code changes},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2015},
volume={2015-August},
pages={372-376},
doi={10.1109/MSR.2015.43},
art_number={7180097},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957095130&doi=10.1109%2fMSR.2015.43&partnerID=40&md5=ed2480f1173bf95555bbddc09bf3f076},
abstract={Existing techniques have succeeded to help developers implement new code. However, they are insufficient to help to change existing code. Previous studies have proposed techniques to support bug fixes but other kinds of code changes such as function enhancements and refactorings are not supported by them. In this paper, we propose a novel system that helps developers change existing code. Unlike existing techniques, our system can support any kinds of code changes if similar code changes occurred in the past. Our research is still on very early stage and we have not have any implementation or any prototype yet. This paper introduces our research purpose, an outline of our system, and how our system is different from existing techniques. © 2015 IEEE.},
keywords={Bug fixes;  Change reuse;  Code changes;  Code clone;  Refactorings;  Research purpose;  Source code analysis, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ray201534,
author={Ray, B. and Nagappan, M. and Bird, C. and Nagappan, N. and Zimmermann, T.},
title={The uniqueness of changes: Characteristics and applications},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2015},
volume={2015-August},
pages={34-43},
doi={10.1109/MSR.2015.11},
art_number={7180065},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957093115&doi=10.1109%2fMSR.2015.11&partnerID=40&md5=814eba66618d7721892d78e3a15fa1db},
abstract={Changes in software development come in many forms. Some changes are frequent, idiomatic, or repetitive (e.g. Adding checks for nulls or logging important values) while others are unique. We hypothesize that unique changes are different from the more common similar (or non-unique) changes in important ways, they may require more expertise or represent code that is more complex or prone to mistakes. As such, these unique changes are worthy of study. In this paper, we present a definition of unique changes and provide a method for identifying them in software project history. Based on the results of applying our technique on the Linux kernel and two large projects at Microsoft, we present an empirical study of unique changes. We explore how prevalent unique changes are and investigate where they occur along the architecture of the project. We further investigate developers' contribution towards uniqueness of changes. We also describe potential applications of leveraging the uniqueness of change and implement two of those applications, evaluating the risk of changes based on uniqueness and providing change recommendations for non-unique changes. © 2015 IEEE.},
keywords={Buildings;  Cloning;  Computer operating systems;  Computer software;  History;  Linux;  Syntactics, Context;  Empirical studies;  Important value;  Large project;  Linux kernel;  MicroSoft;  Software project, Software design},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sidiroglou-Douskos201543,
author={Sidiroglou-Douskos, S. and Lahtinen, E. and Long, F. and Rinard, M.},
title={Automatic error elimination by horizontal code transfer across multiple applications},
journal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
year={2015},
volume={2015-June},
pages={43-54},
doi={10.1145/2737924.2737988},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951734323&doi=10.1145%2f2737924.2737988&partnerID=40&md5=21a6401abeff08cc6411dda0eb9d0be0},
abstract={We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications that process the same inputs to successfully eliminate errors in the recipient. Experimental results using seven donor applications to eliminate ten errors in seven recipient applications highlight the ability of CP to transfer code across applications to eliminate out of bounds access, integer overflow, and divide by zero errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to automatically transfer code across multiple applications.},
keywords={Computational linguistics;  Computer programming;  Computer programming languages;  Errors;  Program translators, Automatic codes;  Correct code;  Error elimination;  First systems;  Integer overflow;  Multiple applications;  Source codes;  Zero errors, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sidiroglou-Douskos201543,
author={Sidiroglou-Douskos, S. and Lahtinen, E. and Long, F. and Rinard, M.},
title={Automatic error elimination by horizontal code transfer across multiple applications},
journal={ACM SIGPLAN Notices},
year={2015},
volume={50},
number={6},
pages={43-54},
doi={10.1145/2737924.2737988},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951014939&doi=10.1145%2f2737924.2737988&partnerID=40&md5=2cedfff3535e3140739b6fbd4db4d84c},
abstract={We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications that process the same inputs to successfully eliminate errors in the recipient. Experimental results using seven donor applications to eliminate ten errors in seven recipient applications highlight the ability of CP to transfer code across applications to eliminate out of bounds access, integer overflow, and divide by zero errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to automatically transfer code across multiple applications. © 2015 ACM.},
keywords={Errors;  Program translators, Automatic codes;  Correct code;  Error elimination;  First systems;  Integer overflow;  Multiple applications;  Source codes;  Zero errors, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hora2015192,
author={Hora, A. and Anquetil, N. and Etien, A. and Ducasse, S. and Valente, M.T.},
title={Automatic detection of system-specific conventions unknown to developers},
journal={Journal of Systems and Software},
year={2015},
volume={109},
pages={192-204},
doi={10.1016/j.jss.2015.08.007},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941268844&doi=10.1016%2fj.jss.2015.08.007&partnerID=40&md5=1ada0ab398c789efb37cf4d12e6fbcee},
abstract={In Apache Ant, a convention to improve maintenance was introduced in 2004 stating a new way to close files instead of the Java generic InputStream.close(). Yet, six years after its introduction, this convention was still not generally known to the developers. Two existing solutions could help in these cases. First, one can deprecate entities, but, in our example, one can hardly deprecate Java's method. Second, one can create a system-specific rule to be automatically enforced. In a preceding publication, we showed that system-specific rules are more likely to be noticed by developers than generic ones. However, in practice, developers rarely create specific rules. We therefore propose to free the developers from the need to create rules by automatically detecting such conventions from source code repositories. This is done by mining the change history of the system to discover similar changes being applied over several revisions. The proposed approach is applied to a real-world system, and the extracted rules are validated with the help of experts. The results show that many rules are in fact relevant for the experts. © 2015 ElsevierInc.Allrightsreserved.},
keywords={Hardware, Automatic Detection;  Change history;  Java generics;  Mining software repositories;  Real-world system;  Software Evolution;  Source code repositories;  System specific, Software engineering},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhang2014755,
author={Zhang, T. and Song, M. and Kim, M.},
title={Critics: An interactive code review tool for searching and inspecting systematic changes},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2014},
volume={16-21-November-2014},
pages={755-758},
doi={10.1145/2635868.2661675},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986910764&doi=10.1145%2f2635868.2661675&partnerID=40&md5=48f9f2fd8365c31b51688fc097c0635a},
abstract={During peer code reviews, developers often examine program differences. When using existing program differencing tools, it is difficult for developers to inspect systematic changes|similar, related changes that are scattered across multiple files. Developers cannot easily answer questions such as "what other code locations changed similar to this change?" and "are there any other locations that are similar to this code but are not updated?" In this paper, we demonstrate CRITICS, an Eclipse plug-in that assists developers in inspecting systematic changes. It (1) allows developers to customize a context-aware change template, (2) searches for systematic changes using the template, and (3) detects missing or inconsistent edits. Developers can interactively refine the customized change template to see corresponding search results. CRITICS has potential to improve developer productivity in inspecting large, scattered edits during code reviews. The tool's demonstration video is available at https://www.youtube.com/watch?v=F2D7t-Z5rhk. Copyright 2014 ACM.},
keywords={Inspection;  Software engineering, Code review;  Context-Aware;  Peer code review;  Plug-ins;  Program differencing;  Software Evolution;  Systematic changes, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ying2014460,
author={Ying, A.T.T. and Robillard, M.P.},
title={Selection and presentation practices for code example summarization},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2014},
volume={16-21-November-2014},
pages={460-471},
doi={10.1145/2635868.2635877},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986893030&doi=10.1145%2f2635868.2635877&partnerID=40&md5=465229eebfd86615ec13bb389f8ae142},
abstract={Code examples are an important source for answering questions about software libraries and applications. Many usage contexts for code examples require them to be distilled to their essence: e.g., when serving as cues to longer documents, or for reminding developers of a previously known idiom. We conducted a study to discover how code can be summarized and why. As part of the study, we collected 156 pairs of code examples and their summaries from 16 participants, along with over 26 hours of think-aloud verbalizations detailing the decisions of the participants during their summarization activities. Based on a qualitative analysis of this data we elicited a list of practices followed by the participants to summarize code examples and propose empirically-supported hypotheses justifying the use of specific practices. One main finding was that none of the participants exclusively extracted code verbatim for the summaries, motivating abstractive summarization. The results provide a grounded basis for the development of code example summarization and presentation technology. Copyright 2014 ACM.},
keywords={Application programs;  Software engineering, Code examples;  Qualitative analysis;  Software libraries;  Summarization;  Think aloud;  Usage context, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Barr2014306,
author={Barr, E.T. and Brun, Y. and Devanbu, P. and Harman, M. and Sarro, F.},
title={The plastic surgery hypothesis},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2014},
volume={16-21-November-2014},
pages={306-317},
doi={10.1145/2635868.2635898},
note={cited By 60},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986922150&doi=10.1145%2f2635868.2635898&partnerID=40&md5=afe74e3801caaac4782e5479cedb51f3},
abstract={Recent work on genetic-programming-based approaches to automatic program patching have relied on the insight that the content of new code can often be assembled out of fragments of code that already exist in the code base. This insight has been dubbed the plastic surgery hypothesis; successful, well-known automatic repair tools such as GenProg rest on this hypothesis, but it has never been validated. We formalize and validate the plastic surgery hypothesis and empirically measure the extent to which raw material for changes actually already exists in projects. In this paper, we mount a large-scale study of several large Java projects, and examine a history of 15,723 commits to determine the extent to which these commits are graftable, i.e., can be reconstituted from existing code, and find an encouraging degree of graftability, surprisingly independent of commit size and type of commit. For example, we find that changes are 43% graftable from the exact version of the software being changed. With a view to investigating the difficulty of finding these grafts, we study the abundance of such grafts in three possible sources: the immediately previous version, prior history, and other projects. We also examine the contiguity or chunking of these grafts, and the degree to which grafts can be found in the same file. Our results are quite promising and suggest an optimistic future for automatic program patching methods that search for raw material in already extant code in the project being patched.},
keywords={Codes (symbols);  Genetic algorithms;  Genetic programming;  Repair;  Software engineering;  Surgery, Automatic programs;  Code reuse;  Empirical Software Engineering;  Large-scale studies;  Mining software repositories;  Plastic surgery;  Repair tools, Computer software reusability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Milea201486,
author={Milea, N.A. and Jiang, L. and Khoo, S.-C.},
title={Vector abstraction and concretization for scalable detection of refactorings},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2014},
volume={16-21-November-2014},
pages={86-97},
doi={10.1145/2635868.2635926},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986893423&doi=10.1145%2f2635868.2635926&partnerID=40&md5=aad22b5f0d8b7e64e6ca55f681522ec4},
abstract={Automated techniques have been proposed to either identify refactoring opportunities (i.e., code fragments that can be but have not yet been restructured in a program), or reconstruct historical refactorings (i.e., code restructuring operations that have happened between different versions of a program). In this paper, we propose a new technique that can detect both refactoring opportunities and historical refactorings in large code bases. The key of our technique is the design of vector abstraction and concretization operations that can encode code changes induced by certain refactorings as characteristic vectors. Thus, the problem of identifying refactorings can be reduced to the problem of identifying matching vectors, which can be solved efficiently. We have implemented our technique for Java. The prototype is applied to 200 bundle projects from the Eclipse ecosystem containing 4.5 million lines of code, and reports in total more than 32K instances of 17 types of refactoring opportunities, taking 25 minutes on average for each type. The prototype is also applied to 14 versions of 3 smaller programs (JMeter, Ant, XML-Security), and detects (1) more than 2.8K refactoring opportunities within individual versions with a precision of about 87%, and (2) more than 190 historical refactorings across consecutive versions of the programs with a precision of about 92%. Copyright 2014 ACM.},
keywords={Abstracting;  Software engineering;  Vectors, Automated techniques;  Characteristic vectors;  Code representation;  Code restructuring;  Large code basis;  Matching vectors;  Refactorings;  Software Evolution, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hora2014420,
author={Hora, A. and Etien, A. and Anquetil, N. and Ducasse, S. and Valente, M.T.},
title={APIEvolutionMiner: Keeping API evolution under control},
journal={2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014 - Proceedings},
year={2014},
pages={420-424},
doi={10.1109/CSMR-WCRE.2014.6747209},
art_number={6747209},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898402098&doi=10.1109%2fCSMR-WCRE.2014.6747209&partnerID=40&md5=19fe4f6cf69154138788d5750fbaf75a},
abstract={During software evolution, source code is constantly refactored. In real-world migrations, many methods in the newer version are not present in the old version (e.g.,60% of the methods in Eclipse 2.0 were not in version 1.0). This requires changes to be consistently applied to reflect the new API and avoid further maintenance problems. In this paper, we propose a tool to extract rules by monitoring API changes applied in source code during system evolution. In this process, changes are mined at revision level in code history. Our tool focuses on mining invocation changes to keep track of how they are evolving. We also provide three case studies in order to evaluate the tool. © 2014 IEEE.},
keywords={Reengineering;  Reverse engineering;  Tools, Keep track of;  Maintenance Problem;  Real-world;  Software Evolution;  Source codes;  System evolution, Computer software maintenance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yang2014311,
author={Yang, Y. and Washizaki, H. and Fukazawa, Y.},
title={A tool to suggest similar program element modifications},
journal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
year={2014},
volume={1},
pages={311-318},
doi={10.1109/APSEC.2014.54},
art_number={7091325},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951280534&doi=10.1109%2fAPSEC.2014.54&partnerID=40&md5=5b480adc08ddd54e7e53ff7563d062b9},
abstract={Many program tasks require continuous modification of similar program elements, which is burdensome on programmers because continuous modifications are time consuming and some modifications are easily overlooked. To resolve this issue, we developed a tool, named SimilarHighlight, which extracted all possible matching elements via similarity patterns from recently modified elements using a sub syntax tree comparison. SimilarHighlight suggests similar program elements that may be modified during the next modification. Potential elements are highlighted and their text can be immediately selected by shortcut keys. Evaluations indicate that SimilarHighlight can improve programming productivity. Currently, SimilarHighlight supports C, C#, JAVA, JavaScript, and PHP, but in the future we will expand it to other languages. © 2014 IEEE.},
keywords={Productivity;  Software engineering;  Syntactics;  Trees (mathematics), Continuous modification;  Matching elements;  Minimal Keystrokes;  Modification;  Program elements;  Similar elements;  Similarity patterns;  Syntax tree, Java programming language},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Milea2014138,
author={Milea, N.A. and Jiang, L. and Khoo, S.-C.},
title={Scalable detection of missed cross-function refactorings},
journal={2014 International Symposium on Software Testing and Analysis, ISSTA 2014 - Proceedings},
year={2014},
pages={138-148},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942772713&partnerID=40&md5=9ab2cbfb7609e69b67c329607b80eab7},
abstract={Refactoring is an important way to improve the design of existing code. Identifying refactoring opportunities (i.e., code fragments that can be refactored) in large code bases is a challenging task. In this paper, we propose a novel, automated and scalable technique for identifying cross-function refactoring opportunities that span more than one function (e.g., Extract Method and Inline Method). The key of our technique is the design of efficient vector inlining operations that emulate the effect of method inlining among code fragments, so that the problem of identifying cross-function refactoring can be reduced to the problem of finding similar vectors before and after inlining. We have implemented our technique in a prototype tool named ReDex which encodes Java programs to particular vectors. We have applied the tool to a large code base, 4.5 million lines of code, comprising of 200 bundle projects in the Eclipse ecosystem (e.g., Eclipse JDT, Eclipse PDE, Apache Commons, Hamcrest, etc.). Also, different from many other studies on detecting refactoring, ReDex only searches for code fragments that can be, but have not yet been, refactored in a way similar to some refactoring that happened in the code base. Our results show that ReDex can find 277 cross-function refactoring opportunities in 2 minutes, and 223 cases were labelled as true opportunities by users, and cover many categories of cross-function refactoring operations in classical refactoring books, such as Self Encapsulate Field, Decompose Conditional Expression, Hide Delegate, Preserve Whole Object, etc. Copyright 2014 ACM.},
keywords={Computer software;  Java programming language;  Software testing;  Vectors, Code fragments;  Conditional expressions;  Large code basis;  Lines of code;  Prototype tools;  Refactorings;  Software Evolution;  Vector-based representations, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Kim2014421,
author={Kim, M. and Meng, N.},
title={Recommending program transformations: Automating repetitive software changes},
journal={Recommendation Systems in Software Engineering},
year={2014},
pages={421-453},
doi={10.1007/978-3-642-45135-5_16},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948100298&doi=10.1007%2f978-3-642-45135-5_16&partnerID=40&md5=29ecc97fbf6b0618eeda20f374f55fec},
abstract={Adding features and fixing bugs in software often require systematic edits which are similar but not identical changes to multiple code locations. Finding all relevant locations and making the correct edits is a tedious and error-prone process. This chapter presents several state-of-the art approaches to recommending program transformation in order to automate repetitive software changes. First, it discusses programming-by-demonstration (PBD) approaches that automate repetitive tasks by inferring a generalized action script from a user’s recorded actions. Second, it presents edit location suggestion approaches that only recommend candidate edit locations but do not apply necessary code transformations. Finally, it describes program transformation approaches that take code examples or version histories as input, automatically identify candidate edit locations, and apply context awareness, customization program transformations to generate a new program version. In particular, this chapter describes two concrete example-based program transformation approaches in detail, Sydit and Lase. These two approaches are selected for an in-depth discussion, because they handle the issue of both recommending change locations and applying transformations, and they are specifically designed to update programs as opposed to regular text documents. The chapter is then concluded with open issues and challenges of recommending program transformations. © Springer-Verlag Berlin Heidelberg 2014.},
keywords={Codes (symbols);  Cosine transforms;  Location, Code transformation;  Context- awareness;  Error-prone process;  Issues and challenges;  Program transformation approach;  Program transformations;  Programming by demonstration;  State-of-the-art approach, Program debugging},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Ray2013367,
author={Ray, B. and Kim, M. and Person, S. and Rungta, N.},
title={Detecting and characterizing semantic inconsistencies in ported code},
journal={2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings},
year={2013},
pages={367-377},
doi={10.1109/ASE.2013.6693095},
art_number={6693095},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893600713&doi=10.1109%2fASE.2013.6693095&partnerID=40&md5=7404c91fba52752da40871d741d0151f},
abstract={Adding similar features and bug fixes often requires porting program patches from reference implementations and adapting them to target implementations. Porting errors may result from faulty adaptations or inconsistent updates. This paper investigates (1) the types of porting errors found in practice, and (2) how to detect and characterize potential porting errors. Analyzing version histories, we define five categories of porting errors, including incorrect control- and data-flow, code redundancy, inconsistent identifier renamings, etc. Leveraging this categorization, we design a static control- and data-dependence analysis technique, SPA, to detect and characterize porting inconsistencies. Our evaluation on code from four open-source projects shows that SPA can detect porting inconsistencies with 65% to 73% precision and 90% recall, and identify inconsistency types with 58% to 63% precision and 92% to 100% recall. In a comparison with two existing error detection tools, SPA improves precision by 14 to 17 percentage points. © 2013 IEEE.},
keywords={Analysis techniques;  Code redundancy;  Error-detection tools;  Open source projects;  Percentage points;  Reference implementation;  Semantic inconsistencies;  Static control, Errors;  Semantics, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nguyen2013180,
author={Nguyen, H.A. and Nguyen, A.T. and Nguyen, T.T. and Nguyen, T.N. and Rajan, H.},
title={A study of repetitiveness of code changes in software evolution},
journal={2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings},
year={2013},
pages={180-190},
doi={10.1109/ASE.2013.6693078},
art_number={6693078},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893627452&doi=10.1109%2fASE.2013.6693078&partnerID=40&md5=54f8384431ff0c89eb07bf6727ac7c9a},
abstract={In this paper, we present a large-scale study of repetitiveness of code changes in software evolution. We collected a large data set of 2,841 Java projects, with 1.7 billion source lines of code (SLOC) at the latest revisions, 1.8 million code change revisions (0.4 million fixes), 6.2 million changed files, and 2.5 billion changed SLOCs. A change is considered repeated within or cross-project if it matches another change having occurred in the history of the project or another project, respectively. We report the following important findings. First, repetitiveness of changes could be as high as 70-100% at small sizes and decreases exponentially as size increases. Second, repetitiveness is higher and more stable in the cross-project setting than in the within-project one. Third, fixing changes repeat similarly to general changes. Importantly, learning code changes and recommending them in software evolution is beneficial with accuracy for top-1 recommendation of over 30% and top-3 of nearly 35%. Repeated fixing changes could also be useful for automatic program repair. © 2013 IEEE.},
keywords={Automatic programs;  Code changes;  Large datasets;  Large-scale studies;  Software Evolution;  Source lines of codes, Automatic programming;  Repair, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kim2013802,
author={Kim, D. and Nam, J. and Song, J. and Kim, S.},
title={Automatic patch generation learned from human-written patches},
journal={Proceedings - International Conference on Software Engineering},
year={2013},
pages={802-811},
doi={10.1109/ICSE.2013.6606626},
art_number={6606626},
note={cited By 203},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886385527&doi=10.1109%2fICSE.2013.6606626&partnerID=40&md5=685b153c2c003f703ede67ec8ccdf3a6},
abstract={Patch generation is an essential software maintenance task because most software systems inevitably have bugs that need to be fixed. Unfortunately, human resources are often insufficient to fix all reported and known bugs. To address this issue, several automated patch generation techniques have been proposed. In particular, a genetic-programming-based patch generation technique, GenProg, proposed by Weimer et al., has shown promising results. However, these techniques can generate nonsensical patches due to the randomness of their mutation operations. To address this limitation, we propose a novel patch generation approach, Pattern-based Automatic program Repair (Par), using fix patterns learned from existing human-written patches. We manually inspected more than 60,000 human-written patches and found there are several common fix patterns. Our approach leverages these fix patterns to generate program patches automatically. We experimentally evaluated Par on 119 real bugs. In addition, a user study involving 89 students and 164 developers confirmed that patches generated by our approach are more acceptable than those generated by GenProg. Par successfully generated patches for 27 out of 119 bugs, while GenProg was successful for only 16 bugs. © 2013 IEEE.},
keywords={Automatic programs;  Generation techniques;  Mutation operations;  Software systems;  Software-maintenance tasks;  User study, Genetic programming;  Repair;  Software engineering, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

Scopus
EXPORT DATE: 12 March 2019

@CONFERENCE{Tan2019100,
author={Tan, L. and Bockisch, C.},
title={A Survey of Refactoring Detection Tools},
journal={CEUR Workshop Proceedings},
year={2019},
volume={2308},
pages={100-105},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061841732&partnerID=40&md5=612dba5f487183e718d91ddb10f440d4},
abstract={Several tools for detecting refactorings in the code exist and have been evaluated in the literature. However, we found that the benchmarks used for the evaluation so far are incomplete and therefore, the validity of the previous evaluations is at stake. While our completed benchmark largely confirmed the previous results, in particular confirming that RefactoringMiner generally outperforms its competitors, we also identified a weak spot of RefactoringMiner that was not noted before: Refactorings of the type Move Class and Rename Package are frequently classified falsely. In this paper we discuss the reasons for this wrong classification and outline a possible fix, which potentially boosts the overall precision and recall of RefactoringMiner to over 95%. © 2019 CEUR-WS.},
keywords={Detection tools;  Move method;  Precision and recall;  Refactoringminer;  Refactorings;  Reproduction study, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang201848,
author={Wang, Y. and Hu, S. and Yin, L. and Zhou, X.},
title={Using Code Evolution Information to Improve the Quality of Labels in Code Smell Datasets},
journal={Proceedings - International Computer Software and Applications Conference},
year={2018},
volume={1},
pages={48-53},
doi={10.1109/COMPSAC.2018.00015},
art_number={8377639},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055446660&doi=10.1109%2fCOMPSAC.2018.00015&partnerID=40&md5=1f214cb0dd32cce58b9061f6e84b8e89},
abstract={Several approaches are proposed to detect code smells. A set of important approaches are based on machine learning algorithms, which require the code smells have been labeled in source codes as training data firstly. The common labeling approaches are based on manual or tools, but it is difficult for current approaches to get reliable large-scale datasets. In this paper, an approach using the evolution information of source codes is proposed to get large-scale and more reliable training datasets for detecting code smells based on machine learning algorithms. Our approach analyzes the evolving of the source code smells firstly labeled by a tool from the baseline version into the contrastive version of a software system, and then constructs training datasets based on those 'changed smells'. Experiments conducted on three open source software projects for detecting four types of code smells(which are Data Class, God Class, Brain Class and Brain Method) show that the models obtained by changed smells datasets have better performance on code smell detection than those obtained by unchanged smells datasets (with an average improvement rate of 7.8% and a maximum increase of 30%). The experiments results indicate that using the evolution information of source codes can construct more reliable training datasets for detecting code smells based on machine learning algorithms. © 2018 IEEE.},
keywords={Application programs;  Artificial intelligence;  Codes (symbols);  Computer programming languages;  Information use;  Learning systems;  Odors;  Open source software;  Open systems, Code smell;  Large-scale datasets;  Open source software projects;  Refactorings;  Software systems;  Training data;  Training data sets;  Training dataset, Learning algorithms},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tavares201881,
author={Tavares, C.S. and Ferreira, F. and Figueiredo, E.},
title={A systematic mapping of literature on software refactoring tools [Um Mapeamento Sistemático da Literatura sobre Ferramentas de Refatoração de Software]},
journal={ACM International Conference Proceeding Series},
year={2018},
pages={81-88},
doi={10.1145/3229345.3229357},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060041285&doi=10.1145%2f3229345.3229357&partnerID=40&md5=5bc897f0e4336f4df00a3a2cbc4112f4},
abstract={Refactoring consists of improving the internal structure of the code without changing the external behavior of a software system. However, the task of refactoring is very costly in the development of an information system. Thus, many tools have been proposed to support refactoring the source code. In order to find tools cited in the literature, this work presents a Systematic Literature Mapping about refactoring. As a result, this paper summarizes the refactoring tools that have been published in the last 5 years in terms of the tool profiles developed, which programming languages have support for refactoring and which are the main refactoring strategies that are handled by tools. It has been identified that publications on refactoring have remained constant over the past 5 years. Also, most of the refactoring works describe tools, being they for systems written in the Java language, that perform code refactoring automatically and the main refactorings are: Move Method, Pull Up Method, Extract Class and Code Clone. Finally, we performed an analysis of the data returned by the DBLP library. As a result, it was observed that the papers returned by the DBLP have a high level of similarity with the other research bases studied. © 2018 Association for Computing Machinery.},
keywords={Information systems;  Information use;  Mapping, Code re-factoring;  External behavior;  Internal structure;  Java language;  Refactoring tools;  Software refactoring;  Software systems;  Systematic mapping, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeLaTorre2018492,
author={De La Torre, G. and Robbes, R. and Bergel, A.},
title={Imprecisions diagnostic in source code deltas},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={492-502},
doi={10.1145/3196398.3196404},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051659811&doi=10.1145%2f3196398.3196404&partnerID=40&md5=6134dc01de9588886846ec348c75840c},
abstract={Beyond a practical use in code review, source code change detection (SCCD) is an important component of many mining software repositories (MSR) approaches. As such, any error or imprecision in the detection may result in a wrong conclusion while mining repositories. We identified, analyzed, and characterized impressions in GumTree, which is the most advanced algorithm for SCCD. After analyzing its detection accuracy over a curated corpus of 107 C# projects, we diagnosed several imprecisions. Many of our findings confirm that a more language-aware perspective of GumTree can be helpful in reporting more precise changes. © 2018 ACM.},
keywords={Codes (symbols);  Computer programming languages;  Image quality, Detection accuracy;  differencing;  gumtree;  Mining repositories;  Mining software repository (MSR);  Practical use;  Source code changes;  Source codes, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mahmoudi2018220,
author={Mahmoudi, M. and Nadi, S.},
title={The Android update problem: An empirical study},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={220-230},
doi={10.1145/3196398.3196434},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051663029&doi=10.1145%2f3196398.3196434&partnerID=40&md5=fce9bbf8fef60b369bb445d5cdf3c6cc},
abstract={Many phone vendors use Android as their underlying OS, but often extend it to add new functionality and to make it compatible with their specific phones. When a new version of Android is released, phone vendors need to merge or re-apply their customizations and changes to the new release. This is a difficult and time-consuming process, which often leads to late adoption of new versions. In this paper, we perform an empirical study to understand the nature of changes that phone vendors make, versus changes made in the original development of Android. By investigating the overlap of different changes, we also determine the possibility of having automated support for merging them. We develop a publicly available tool chain, based on a combination of existing tools, to study such changes and their overlap. As a proxy case study, we analyze the changes in the popular community-based variant of Android, LineageOS, and its corresponding Android versions. We investigate and report the common types of changes that occur in practice. Our findings show that 83% of subsystems modified by LineageOS are also modified in the next release of Android. By taking the nature of overlapping changes into account, we assess the feasibility of having automated tool support to help phone vendors with the Android update problem. Our results show that 56% of the changes in LineageOS have the potential to be safely automated. © 2018 ACM.},
keywords={Automation;  Merging;  Telephone sets, Android;  Automated support;  Automated tool support;  Community-based;  Empirical studies;  Software Evolution;  Software merging, Android (operating system)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hora20181102,
author={Hora, A. and Silva, D. and Valente, M.T. and Robbes, R.},
title={Assessing the threat of untracked changes in software evolution},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={1102-1113},
doi={10.1145/3180155.3180212},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049384092&doi=10.1145%2f3180155.3180212&partnerID=40&md5=07ba3737c8a737f50be2574980302c3e},
abstract={While refactoring is extensively performed by practitioners, many Mining Software Repositories (MSR) approaches do not detect nor keep track of refactorings when performing source code evolution analysis. In the best case, keeping track of refactorings could be unnecessary work; in the worst case, these untracked changes could significantly affect the performance of MSR approaches. Since the extent of the threat is unknown, the goal of this paper is to assess whether it is significant. Based on an extensive empirical study, we answer positively: we found that between 10 and 21% of changes at the method level in 15 large Java systems are untracked. This results in a large proportion (25%) of entities that may have their histories split by these changes, and a measurable effect on at least two MSR approaches. We conclude that handling untracked changes should be systematically considered by MSR studies. © 2018 ACM.},
keywords={Computer software, Empirical studies;  Evolution analysis;  Keep track of;  Mining software repositories;  Mining software repository (MSR);  Refactorings;  Software Evolution;  Source codes, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brito2018507,
author={Brito, A. and Xavier, L. and Hora, A. and Valente, M.T.},
title={APIDiff: Detecting API breaking changes},
journal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
year={2018},
volume={2018-March},
pages={507-511},
doi={10.1109/SANER.2018.8330249},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050975257&doi=10.1109%2fSANER.2018.8330249&partnerID=40&md5=83d5929cd83f6627dea22a5e51149fae},
abstract={Libraries are commonly used to increase productivity. As most software systems, they evolve over time and changes are required. However, this process may involve breaking compatibility with previous versions, leading clients to fail. In this context, it is important that libraries creators and clients frequently assess API stability in order to better support their maintenance practices. In this paper, we introduce APIDIFF, a tool to identify API breaking and non-breaking changes between two versions of a Java library. The tool detects changes on three API elements: types, methods, and fields. We also report usage scenarios of APIDIFF with four real-world Java libraries. © 2018 IEEE.},
keywords={Java programming language;  Libraries;  Reengineering, API Evolution;  Breaking Changes;  Java library;  Maintenance practices;  Mining software repositories;  Real-world;  Software systems;  Usage scenarios, Application programming interfaces (API)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brito2018255,
author={Brito, A. and Xavier, L. and Hora, A. and Valente, M.T.},
title={Why and how Java developers break APIs},
journal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
year={2018},
volume={2018-March},
pages={255-265},
doi={10.1109/SANER.2018.8330214},
art_number={8330214},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050964632&doi=10.1109%2fSANER.2018.8330214&partnerID=40&md5=3b5660a7a94702544d95b57772bc2b57},
abstract={Modern software development depends on APIs to reuse code and increase productivity. As most software systems, these libraries and frameworks also evolve, which may break existing clients. However, the main reasons to introduce breaking changes in APIs are unclear. Therefore, in this paper, we report the results of an almost 4-month long field study with the developers of 400 popular Java libraries and frameworks. We configured an infrastructure to observe all changes in these libraries and to detect breaking changes shortly after their introduction in the code. After identifying breaking changes, we asked the developers to explain the reasons behind their decision to change the APIs. During the study, we identified 59 breaking changes, confirmed by the developers of 19 projects. By analyzing the developers' answers, we report that breaking changes are mostly motivated by the need to implement new features, by the desire to make the APIs simpler and with fewer elements, and to improve maintainability. We conclude by providing suggestions to language designers, tool builders, software engineering researchers and API developers. © 2018 IEEE.},
keywords={Computer software reusability;  Libraries;  Reengineering;  Software design, API Evolution;  Breaking Changes;  Field studies;  Java developers;  Java library;  Software systems, Java programming language},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Neto2018380,
author={Neto, E.C. and Da Costa, D.A. and Kulesza, U.},
title={The impact of refactoring changes on the SZZ algorithm: An empirical study},
journal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
year={2018},
volume={2018-March},
pages={380-390},
doi={10.1109/SANER.2018.8330225},
art_number={8330225},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051002915&doi=10.1109%2fSANER.2018.8330225&partnerID=40&md5=03fcd181ea85716c750da79718e3dfb6},
abstract={SZZ is a widely used algorithm in the software engineering community to identify changes that are likely to introduce bugs (i.e., bug-introducing changes). Despite its wide adoption, SZZ still has room for improvements. For example, current SZZ implementations may still flag refactoring changes as bug-introducing. Refactorings should be disregarded as bug-introducing because they do not change the system behaviour. In this paper, we empirically investigate how refactorings impact both the input (bug-fix changes) and the output (bug-introducing changes) of the SZZ algorithm. We analyse 31,518 issues of ten Apache projects with 20,298 bug-introducing changes. We use an existing tool that automatically detects refactorings in code changes. We observe that 6.5% of lines that are flagged as bug-introducing changes by SZZ are in fact refactoring changes. Regarding bug-fix changes, we observe that 19.9% of lines that are removed during a fix are related to refactorings and, therefore, their respective inducing changes are false positives. We then incorporate the refactoring-detection tool in our Refactoring Aware SZZ Implementation (RA-SZZ). Our results reveal that RA-SZZ reduces 20.8% of the lines that are flagged as bug-introducing changes compared to the state-of-the-art SZZ implementations. Finally, we perform a manual analysis to identify change patterns that are not captured by the refactoring identification tool used in our study. Our results reveal that 47.95% of the analyzed bug-introducing changes contain additional change patterns that RA-SZZ should not flag as bug-introducing. © 2018 IEEE.},
keywords={Reengineering;  Software engineering, Bug fixes;  bug-introducing change;  Change patterns;  Empirical studies;  Engineering community;  Identification tools;  Refactorings;  State of the art, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Honsel2018161,
author={Honsel, D. and Fiekas, N. and Herbold, V. and Welter, M. and Ahlbrecht, T. and Waack, S. and Dix, J. and Grabowski, J.},
title={Simulating software refactorings based on graph transformations},
journal={Communications in Computer and Information Science},
year={2018},
volume={889},
pages={161-175},
doi={10.1007/978-3-319-96271-9_10},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052070893&doi=10.1007%2f978-3-319-96271-9_10&partnerID=40&md5=a7c2e2098b93048b455a2aaf8c0bf1e9},
abstract={We aim to simulate software processes in order to predict the structural evolution of software graphs and assure higher software quality. To make our simulation and therefore the results more accurate, we need to model real world practices. In this paper, we consider the specific problem of including software refactorings in our simulation. We describe these refactorings as graph transformations and apply parameters we collected from open source projects. © Springer Nature Switzerland AG 2018.},
keywords={Computer software selection and evaluation;  Graph theory, Graph Transformation;  Open source projects;  Real-world practice;  Refactorings;  Software process;  Software Quality;  Specific problems;  Structural evolution, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Oo201876025,
author={Oo, T. and Liu, H. and Nyirongo, B.},
title={Dynamic Ranking of Refactoring Menu Items for Integrated Development Environment},
journal={IEEE Access},
year={2018},
volume={6},
pages={76025-76035},
doi={10.1109/ACCESS.2018.2883769},
art_number={8552339},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057887721&doi=10.1109%2fACCESS.2018.2883769&partnerID=40&md5=ab8d1015283043b0ce46f6f453d4d8c0},
abstract={Software refactoring is popular and thus most mainstream IDEs, e.g., Eclipse, provide a top level menu, especially for refactoring activities. The refactoring menu is designed to facilitate refactorings, and it has become one of the most commonly used menus. However, to support a large number of refactoring types, the refactoring menu contains a long list of menu items. As a result, it is tedious to select the intended menu item from the lengthy menu. To facilitate the menu selection, in this paper, we propose an approach to dynamic ranking of refactoring menu items for IDE. We put the most likely refactoring menu item on the top of the refactoring menu according to developers' source code selection and code smells associated with the selected source code. The ranking is dynamic because it changes frequently according to the context. First, we collect the refactoring history of the open source applications and detect the code smells. Based on the refactoring history, we design questionnaires and analyze the responses from developers to discover the source code selection patterns for different refactoring types. Subsequently, we analyze the relationship between code smells associated with the refactoring software entities and the corresponding refactoring types. Finally, based on the preceding analysis, we calculate the likelihood of different refactoring types to be applied when a specific part of source code is selected, and rank the menu items according to the resulting likelihood. We conduct a case study to evaluate the proposed approach. Evaluation results suggest that the proposed approach is accurate, and in most cases (95.69%), it can put the intended refactoring menu item on the top of the menu. © 2013 IEEE.},
keywords={Codes (symbols);  Computer programming languages;  History;  Integrodifferential equations;  Learning systems;  Odors;  Open systems;  Software design;  Software engineering;  Surveys;  Tools;  User interfaces, Crawlers;  Evaluation results;  Integrated development environment;  Java;  Menu Ranking;  Open source application;  Software entities;  Software refactoring, Open source software},
document_type={Article},
source={Scopus},
}

Scopus
EXPORT DATE: 12 March 2019

@ARTICLE{Parsai2019419,
author={Parsai, A. and Demeyer, S.},
title={Do null-type mutation operators help prevent null-type faults?},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11376 LNCS},
pages={419-434},
doi={10.1007/978-3-030-10801-4_33},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062322203&doi=10.1007%2f978-3-030-10801-4_33&partnerID=40&md5=0fd6c2a43cf9ed71388bf04ec228b189},
abstract={The null-type is a major source of faults in Java programs, and its overuse has a severe impact on software maintenance. Unfortunately traditional mutation testing operators do not cover null-type faults by default, hence cannot be used as a preventive measure. We address this problem by designing four new mutation operators which model null-type faults explicitly. We show how these mutation operators are capable of revealing the missing tests, and we demonstrate that these mutation operators are useful in practice. For the latter, we analyze the test suites of 15 open-source projects to describe the trade-offs related to the adoption of these operators to strengthen the test suite. © 2019, Springer Nature Switzerland AG.},
keywords={Computer software maintenance;  Economic and social effects;  Open source software, Java program;  Mutation operators;  Mutation testing;  Null type;  Open source projects;  Preventive measures;  Test quality;  Trade off, Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Islam201823,
author={Islam, M.R. and Zibran, M.F.},
title={On the characteristics of buggy code clones: A code quality perspective},
journal={2018 IEEE 12th International Workshop on Software Clones, IWSC 2018 - Proceedings},
year={2018},
volume={2018-January},
pages={23-29},
doi={10.1109/IWSC.2018.8327315},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049689512&doi=10.1109%2fIWSC.2018.8327315&partnerID=40&md5=6a1aa84c107419a2b856199395e2ae17},
abstract={Code clone is an immensely studied code smell. Not all the clones in a software system are equally harmful. Earlier work studied various traits of clones including their stability and relationships with program faults against non-cloned code. This paper presents a comparative study on the characteristics of buggy and non-buggy clones from a code quality perspective. In the light of 29 code quality metrics, we study buggy and non-buggy clones in 2,077 revisions of three software systems written in Java. The findings from this work add to the characterization of buggy clones. Such a characterization will be useful in cost-effective clone management and clone-aware software development. © 2018 IEEE.},
keywords={Codes (symbols);  Computer software;  Cost effectiveness;  Software design, Clone management;  Code clone;  Code quality;  Code smell;  Comparative studies;  Cost effective;  Software systems, Cloning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nielebock20171010,
author={Nielebock, S.},
title={Towards API-specific automatic program repair},
journal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
year={2017},
pages={1010-1013},
doi={10.1109/ASE.2017.8115721},
art_number={8115721},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041446664&doi=10.1109%2fASE.2017.8115721&partnerID=40&md5=e0a1bc4e46a709a9cf9b2e1534377cb7},
abstract={The domain of Automatic Program Repair (APR) had many research contributions in recent years. So far, most approaches target fixing generic bugs in programs (e.g., off-by-one errors). Nevertheless, recent studies reveal that about 50% of real bugs require API-specific fixes (e.g., adding missing API method calls or correcting method ordering), for which existing APR approaches are not designed. In this paper, we address this problem and introduce the notion of an API-specific program repair mechanism. This mechanism detects erroneous code in a similar way to existing APR approaches. However, to fix such bugs, it uses API-specific information from the erroneous code to search for API usage patterns in other software, with which we could fix the bug. We provide first insights on the applicability of this mechanism and discuss upcoming research challenges. © 2017 IEEE.},
keywords={Application programming interfaces (API);  Repair;  Software engineering, API-specific Bugs;  Automatic programs;  Repair mechanism;  Research challenges;  Specific information;  Specification mining;  Usage patterns, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Leuenberger201771,
author={Leuenberger, M. and Osman, H. and Ghafari, M. and Nierstrasz, O.},
title={Harvesting the Wisdom of the Crowd to Infer Method Nullness in Java},
journal={Proceedings - 2017 IEEE 17th International Working Conference on Source Code Analysis and Manipulation, SCAM 2017},
year={2017},
volume={2017-October},
pages={71-80},
doi={10.1109/SCAM.2017.22},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040622329&doi=10.1109%2fSCAM.2017.22&partnerID=40&md5=ee3e32b92c05b0c439c4553f2aec5160},
abstract={Null pointer exceptions are common bugs in Java projects. Previous research has shown that dereferencing the results of method calls is the main source of these bugs, as developers do not anticipate that some methods return null. To make matters worse, we find that whether a method returns null or not (nullness), is rarely documented. We argue that method nullness is a vital piece of information that can help developers avoid this category of bugs. This is especially important for external APIs where developers may not even have access to the code.,In this paper, we study the method nullness of Apache Lucene, the de facto standard library for text processing in Java. Particularly, we investigate how often the result of each Lucene method is checked against null in Lucene clients. We call this measure method nullability, which can serve as a proxy for method nullness. Analyzing Lucene internal and external usage, we find that most methods are never checked for null. External clients check more methods than Lucene checks internally. Manually inspecting our dataset reveals that some null checks are unnecessary. We present an IDE plugin that complements existing documentation and makes up for missing documentation regarding method nullness and generates nullness annotations, so that static analysis can pinpoint potentially missing or unnecessary null checks. © 2017 IEEE.},
keywords={Application programming interfaces (API);  Codes (symbols);  Static analysis;  Text processing, Apache Lucene;  De facto standard;  Management systems;  null pointer exceptions;  Plug-ins;  Usage analysis;  Wisdom of the crowds, Java programming language},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tan2017180,
author={Tan, S.H. and Yi, J. and Yulis and Mechtaev, S. and Roychoudhury, A.},
title={Codeflaws: A programming competition benchmark for evaluating automated program repair tools},
journal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
year={2017},
pages={180-182},
doi={10.1109/ICSE-C.2017.76},
art_number={7965296},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026737226&doi=10.1109%2fICSE-C.2017.76&partnerID=40&md5=3b93ea710a3d0cae0fd27eb47a7438a8},
abstract={Several automated program repair techniques have been proposed to reduce the time and effort spent in bug-fixing. While these repair tools are designed to be generic such that they could address many software faults, different repair tools may fix certain types of faults more effectively than other tools. Therefore, it is important to compare more objectively the effectiveness of different repair tools on various fault types. However, existing benchmarks on automated program repairs do not allow thorough investigation of the relationship between fault types and the effectiveness of repair tools. We present Codeflaws, a set of 3902 defects from 7436 programs automatically classified across 39 defect classes (we refer to different types of fault as defect classes derived from the syntactic differences between a buggy program and a patched program). © 2017 IEEE.},
keywords={Automation;  Benchmarking;  Defects;  Repair;  Software engineering, Bug-fixing;  Defect class;  Empirical evaluations;  Fault types;  Repair techniques;  Repair tools;  Software fault, C (programming language)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lin2017188,
author={Lin, W. and Chen, Z. and Ma, W. and Chen, L. and Xu, L. and Xu, B.},
title={An empirical study on the characteristics of python fine-grained source code change types},
journal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
year={2017},
pages={188-199},
doi={10.1109/ICSME.2016.25},
art_number={7816466},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013070479&doi=10.1109%2fICSME.2016.25&partnerID=40&md5=13e81e45a92cfd374cac5781e1df8e3b},
abstract={Software has been changing during its whole life cycle. Therefore, identification of source code changes becomes a key issue in software evolution analysis. However, few current change analysis research focus on dynamic language software. In this paper, we pay attention to the fine-grained source code changes of Python software. We implement an automatic tool named PyCT to extract 77 kinds of fine-grained source code change types from commit history information. We conduct an empirical study on ten popular Python projects from five domains, with 132294 commits, to investigate the characteristics of dynamic software source code changes. Analyzing the source code changes in four aspects, we distill 11 findings, which are summarized into two insights on software evolution: change prediction and fault code fix. In addition, we provide direct evidence on how developers use and change dynamic features. Our results provide useful guidance and insights for improving the understanding of source code evolution of dynamic language software. © 2016 IEEE.},
keywords={Codes (symbols);  Computer programming languages;  Computer software maintenance;  High level languages;  Life cycle, Characteristics of dynamics;  Fine-grained changes;  Fine-grained source code changes;  Identification of sources;  Python;  Software Evolution;  Software evolution analysis;  Source code changes, Computer software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Parsai2017148,
author={Parsai, A. and Murgia, A. and Demeyer, S.},
title={LittleDarwin: A Feature-Rich and Extensible Mutation Testing Framework for Large and Complex Java Systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10522 LNCS},
pages={148-163},
doi={10.1007/978-3-319-68972-2_10},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032862315&doi=10.1007%2f978-3-319-68972-2_10&partnerID=40&md5=806a7b502a1d5e2d0e5847f8465040d4},
abstract={Mutation testing is a well-studied method for increasing the quality of a test suite. We designed LittleDarwin as a mutation testing framework able to cope with large and complex Java software systems, while still being easily extensible with new experimental components. LittleDarwin addresses two existing problems in the domain of mutation testing: having a tool able to work within an industrial setting, and yet, be open to extension for cutting edge techniques provided by academia. LittleDarwin already offers higher-order mutation, null type mutants, mutant sampling, manual mutation, and mutant subsumption analysis. There is no tool today available with all these features that is able to work with typical industrial software systems. © 2017, IFIP International Federation for Information Processing.},
keywords={Computer software;  Java programming language;  Software engineering, Cutting edges;  Existing problems;  Higher-order;  Industrial settings;  Industrial software;  Java software;  Java system;  Mutation testing, Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Oumarou201651,
author={Oumarou, H. and Anquetil, N. and Etien, A. and Ducasse, S. and Taiwe, K.D.},
title={Identifying the Exact Bug Fixing Actions},
journal={Proceedings - 7th International Workshop on Empirical Software Engineering in Practice, IWESEP 2016},
year={2016},
pages={51-56},
doi={10.1109/IWESEP.2016.13},
art_number={7464553},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971472391&doi=10.1109%2fIWESEP.2016.13&partnerID=40&md5=1598198c00a2a9c4f6b0c190cedd31c8},
abstract={In-spite of the principle of good programming practice which stipulates that a commit should include only modifications belonging to one task, programmers submit tangled commits consisting of modifications related to two or several distinct tasks. Some researches show that between 11 and 39% of bug fix commits are tangled and at least 16.6% of all the commits are incorrectly associated to bug reports. Tangled commits make historical analysis of the project less reliable, and impede the extraction of bug fix pattern because most mining software repository techniques are designed with the assumption that each commit includes only modifications for a single task. In this paper, we are proposing a method that identifies precisely the modifications that are related to a bug fix. We validate our solution on a real world system with real bugs and bug-fixes. © 2016 IEEE.},
keywords={Costs;  Software engineering, Bug fixes;  Bug reports;  Bug-fixing;  Historical analysis;  Mining software repositories;  Programming practices;  Real-world system, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Alharbi2015515,
author={Alharbi, K. and Yeh, T.},
title={Collect, decompile, extract, stats, and diff: Mining design pattern changes in android apps},
journal={MobileHCI 2015 - Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services},
year={2015},
pages={515-524},
doi={10.1145/2785830.2785892},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959419451&doi=10.1145%2f2785830.2785892&partnerID=40&md5=df2828957d8450e1c6a7c91a9a5ca925},
abstract={Mobile user interface design patterns have been widely used across different mobile platforms. UI design patterns have evolved and changed significantly as new trends emerge and fade at different times. This paper presents a data-mining approach to analyzing design pattern changes in Android apps. Over a period of 18 months, we tracked 24,436 apps and collected their versions. In total, our sample consists of 56,349 unique app versions, more than 5 million source files, and more than 25 million UI elements. We developed a dedicated infrastructure based on modern big data technologies to support our differential analyses regarding design pattern changes. Some highlights of our findings include a) some apps would switch to a design pattern even after it was deprecated, b) the adoption rate of newly introduced design patterns (e.g., Fragment) is relatively low, c) some apps would update their listing details to reflect changes in design patterns. © 2015 ACM.},
keywords={Application programs;  Big data;  Data mining;  Design;  Human computer interaction;  Mobile devices;  User interfaces, Analysis;  Android;  Data technologies;  Design Patterns;  Differential analysis;  Mobile platform;  Mobile user interface;  Pattern, Android (operating system)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Inozemtseva2015843,
author={Inozemtseva, L.},
title={Understanding the Software Fault Introduction Process},
journal={Proceedings - International Conference on Software Engineering},
year={2015},
volume={2},
pages={843-846},
doi={10.1109/ICSE.2015.274},
art_number={7203095},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951777712&doi=10.1109%2fICSE.2015.274&partnerID=40&md5=2c7fdeb4baf234ad6b4f8ab8756b76a3},
abstract={Testing and debugging research revolves around faults, yet we have a limited understanding of the processes by which faults are introduced and removed. Previous work in this area has focused on describing faults rather than explaining the introduction and removal processes, meaning that a great deal of testing and debugging research depends on assumptions that have not been empirically validated. We propose a three-phase project to develop an explanatory theory of the fault introduction process and describe how the project will be completed. © 2015 IEEE.},
keywords={Computer debugging;  Computer software;  Engineering research;  Software engineering;  Software testing;  Testing, Fault introduction;  Removal process;  Software fault;  Testing and debugging;  Three-phase projects, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sharma2015235,
author={Sharma, M. and Kumari, M. and Singh, V.B.},
title={Post release versions based code change quality metrics},
journal={ACM International Conference Proceeding Series},
year={2015},
volume={10-13-August-2015},
pages={235-243},
doi={10.1145/2791405.2791466},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960953021&doi=10.1145%2f2791405.2791466&partnerID=40&md5=b9a6aa190d8476f27419ade2ea8abe01},
abstract={Software Metric is a quantitative measure of the degree to which a system, component or process possesses a given attribute. Bug fixing, new features (NFs) introduction and feature improvements (IMPs) are the key factors in deciding the next version of software. For fixing an issue (bug/new feature/feature improvement), a lot of changes have to be incorporated into the source code of the software. These code changes need to be understood by software engineers and managers when performing their daily development and maintenance tasks. In this paper, we have proposed four new metrics namely code change quality, code change density, file change quality and file change density to understand the quality of code changes across the different versions of five open source software products, namely Avro, Pig, Hive, jUDDI and Whirr of Apache project. Results show that all the products get better code change quality over a period of time. We have also observed that all the five products follow the similar code change trend. © 2015 ACM.},
keywords={Codes (symbols);  Computer software;  Entropy;  Information science;  Open source software;  Software engineering, Code changes;  Feature improvement;  Maintenance tasks;  New feature;  Quality metrics;  Quantitative measures;  Software metrices;  Software repositories, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chauhan201535203,
author={Chauhan, P.},
title={Recurrent bug fixing: Keshmesh and Naive Bayes},
journal={International Journal of Applied Engineering Research},
year={2015},
volume={10},
number={15},
pages={35203-35208},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941131005&partnerID=40&md5=5cde4b0996ad145482ff10a112cf925c},
abstract={The bug-free programming has unendingly been a dream to chase for programmers and developers thus inescapable is the presence of bugs which advances with development of the software. Problems being faced by programmers are analyzed by characterizing the matter followed by frequency analysis of bugs being similar over different frameworks which is another significant issue to unwind. Only then will bugs be positioned in hierarchal order of commonest to least common. We worked on recurring Concurrent Bug patterns. Concurrency bug patterns (CBPs) are a classification of bug examples specific to concurrent software. More and more software engineers are developing concurrent programming software to acquire effectiveness of multi core architecture. However another obstacle in creating concurrent programming is the presence of practically identical bugs(aka recurrent bugs) showing up over and over. In spite of the fact that frequency of a bugs does not generally indicate errors in programming but rather it leads to future bugs as programming grows. This build up a need to develop tools and techniques to automatically extract and use the already available information within the software revision histories and code to make future bug correction predictions. In this paper we propose the Naïve Bayes classifier to extend the functionality provided by Keshmesh, a tool to detect and fix the concurrent bugs. Naïve Bayes plays a major role in identifying more number of bugs as found by Keshmesh alone. Finding and repairing bugs in software is extremely crucial for the software to keep it stable and least vulnerable to future bugs. © Research India Publications.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Beller2014202,
author={Beller, M. and Bacchelli, A. and Zaidman, A. and Juergens, E.},
title={Modern code reviews in open-source projects: Which problems do they fix?},
journal={11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings},
year={2014},
pages={202-211},
doi={10.1145/2597073.2597082},
note={cited By 57},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928665241&doi=10.1145%2f2597073.2597082&partnerID=40&md5=d794b26c0bc19222b32a9d28449d8db2},
abstract={Code review is the manual assessment of source code by humans, mainly intended to identify defects and quality problems. Modern Code Review (MCR), a lightweight variant of the code inspections investigated since the 1970s, prevails today both in industry and open-source software (OSS) systems. The objective of this paper is to increase our understanding of the practical benefits that the MCR process produces on reviewed source code. To that end, we empirically explore the problems fixed through MCR in OSS systems. We manually classified over 1,400 changes taking place in reviewed code from two OSS projects into a validated categorization scheme. Surprisingly, results show that the types of changes due to the MCR process in OSS are strikingly similar to those in the industry and academic systems from literature, featuring the similar 75:25 ratio of maintainability-related to functional problems. We also reveal that 7-35% of review comments are discarded and that 10-22% of the changes are not triggered by an explicit review comment. Patterns emerged in the review data; we investigated them revealing the technical factors that influence the number of changes due to the MCR process. We found that bug-fixing tasks lead to fewer changes and tasks with more altered files and a higher code churn have more changes. Contrary to intuition, the person of the reviewer had no impact on the number of changes. Copyright is held by the author/owner(s). Publication rights licensed to ACM.},
keywords={Codes (symbols);  Computer software;  Defects;  Information dissemination;  Open source software;  Software engineering, Academic system;  Code inspections;  Code review;  Functional problems;  Lightweight variants;  Open source projects;  Quality problems;  Technical factors, Open systems},
document_type={Conference Paper},
source={Scopus},
}

Scopus
EXPORT DATE: 12 March 2019

@CONFERENCE{Hashimoto2018598,
author={Hashimoto, M. and Mori, A. and Izumida, T.},
title={Automated patch extraction via syntax- and semantics-aware delta debugging on source code changes},
journal={ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year={2018},
pages={598-609},
doi={10.1145/3236024.3236047},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058312685&doi=10.1145%2f3236024.3236047&partnerID=40&md5=65094eb3a1979539d3e2ddab3304133b},
abstract={Delta debugging (DD) is an approach to automating debugging activities based on systematic testing. DD algorithms find the cause of a regression of a program by minimizing changes between a working version and a faulty version of the program. However, it is still an open problem to minimize a huge set of changes while avoiding any invalid subsets that do not result in testable programs, especially in case that no software configuration management system is available. In this paper, we propose a rule-based approach to syntactic and semantic decomposition of changes into independent components to facilitate DD on source code changes, and hence to extract patches automatically. For analyzing changes, we make use of tree differencing on abstract syntax trees instead of common differencing on plain texts. We have developed an experimental implementation for Java programs and applied it to 194 bug fixes from Defects4J and 8 real-life regression bugs from 6 open source Java projects. Compared to a DD tool based on plain text differencing, it extracted patches whose size is reduced by 50% at the cost of 5% more test executions for the former dataset and by 73% at the cost of 40% more test executions for the latter, both on average. © 2018 Association for Computing Machinery.},
keywords={Abstracting;  Java programming language;  Open source software;  Regression analysis;  Semantics;  Statistical tests;  Syntactics;  Trees (mathematics), Abstract Syntax Trees;  Delta debugging;  Independent components;  Rule-based approach;  Software configuration management systems;  Source code changes;  Systematic testing;  tree differencing, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mechtaev2018,
author={Mechtaev, S. and Gao, X. and Tan, S.H. and Roychoudhury, A.},
title={Test-equivalence analysis for automatic patch generation},
journal={ACM Transactions on Software Engineering and Methodology},
year={2018},
volume={27},
number={4},
doi={10.1145/3241980},
art_number={15},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055834590&doi=10.1145%2f3241980&partnerID=40&md5=8b652172ab80df3bad5917d0e9e5e6c8},
abstract={Automated program repair is a problem of finding a transformation (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading student assignments, and patching security vulnerabilities. A common challenge faced by existing repair techniques is scalability to large patch spaces, since there are many candidate patches that these techniques explicitly or implicitly consider. The correctness criteria for program repair is often given as a suite of tests. Current repair techniques do not scale due to the large number of test executions performed by the underlying search algorithms. In this work, we address this problem by introducing a methodology of patch generation based on a test-equivalence relation (if two programs are “test-equivalent” for a given test, they produce indistinguishable results on this test). We propose two test-equivalence relations based on runtime values and dependencies, respectively, and present an algorithm that performs on-the-fly partitioning of patches into test-equivalence classes. Our experiments on real-world programs reveal that the proposed methodology drastically reduces the number of test executions and therefore provides an order of magnitude efficiency improvement over existing repair techniques, without sacrificing patch quality. © 2018 Association for Computing Machinery.},
keywords={Equivalence classes;  Grading;  Program debugging;  Repair;  Set theory, Correctness criterion;  Dynamic program analysis;  Efficiency improvement;  Equivalence relations;  Program synthesis;  Real world projects;  Security vulnerabilities;  Student assignments, Software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhong20182521,
author={Zhong, H. and Meng, N.},
title={Towards reusing hints from past fixes: An exploratory study on thousands of real samples},
journal={Empirical Software Engineering},
year={2018},
volume={23},
number={5},
pages={2521-2549},
doi={10.1007/s10664-017-9584-3},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038808079&doi=10.1007%2fs10664-017-9584-3&partnerID=40&md5=1d4291d45443069beb65daf6399cf884},
abstract={With the usage of version control systems, many bug fixes have accumulated over the years. Researchers have proposed various automatic program repair (APR) approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new fixes overlap with old fixes, have not been investigated. Intuitively, the overlap between old and new fixes decides how APR approaches can construct new fixes with old ones. Based on this intuition, we systematically designed six overlap metrics, and performed an empirical study on 5,735 bug fixes to investigate the usefulness of past fixes when composing new fixes. For each bug fix, we created delta graphs (i.e., program dependency graphs for code changes), and identified how bug fixes overlap with each other in terms of the content, code structures, and identifier names of fixes. Our results show that if an APR approach knows all code name changes and composes new fixes by fully or partially reusing the content of past fixes, only 2.1% and 3.2% new fixes can be created from single or multiple past fixes in the same project, compared with 0.9% and 1.2% fixes created from past fixes across projects. However, if an APR approach knows all code name changes and composes new fixes by fully or partially reusing the code structures of past fixes, up to 41.3% and 29.7% new fixes can be created. By making the above observations and revealing other ten findings, we investigated the upper bound of reusable past fixes and composable new fixes, exploring the potential of existing and future APR approaches. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.},
keywords={Computer software reusability;  Program debugging, Automatic programs;  Code changes;  Code structure;  Empirical studies;  Exploratory studies;  Program dependency graphs;  Real samples;  Version control system, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Niu20181887,
author={Niu, X. and Li, S. and Jia, Z. and Zhou, S. and Li, W. and Liao, X.},
title={Understanding the similarity of log revision behaviors in open source software},
journal={International Journal of Performability Engineering},
year={2018},
volume={14},
number={8},
pages={1887-1895},
doi={10.23940/ijpe.18.08.p27.18871895},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054725435&doi=10.23940%2fijpe.18.08.p27.18871895&partnerID=40&md5=c13d5d5c8e13deefefd3135f685095a6},
abstract={As logging code evolves with bug fixes and feature updates, developers may miss some log revisions due to a lack of general specifications and attention from developers. This makes it more troublesome to achieve good logging practices. In this paper, we try to study log revision behaviors from evolutionary history. Motivated by similar edits of clone codes, we assume there also exist similar log revisions that implicated log revision behaviors. Based on this assumption, we study the similarity of log revision behaviors and answer six research questions. Specifically, we find that 54.14% of log revisions belong to groups of similar log revisions and 64.4% of groups contain log revisions that are missed by developers. We stress the importance of branch statements on learning from similar log revisions since 53.51% of sampled similar log revisions are related to the semantics of branch statements. © 2018 Totem Publisher, Inc. All rights reserved.},
keywords={Open systems;  Semantics, Bug fixes;  Evolutionary history;  Failure diagnose;  General specification;  Log revision;  Research questions;  Software Evolution, Open source software},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jiang2018298,
author={Jiang, J. and Xiong, Y. and Zhang, H. and Gao, Q. and Chen, X.},
title={Shaping program repair space with existing patches and similar code},
journal={ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2018},
pages={298-309},
doi={10.1145/3213846.3213871},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051477803&doi=10.1145%2f3213846.3213871&partnerID=40&md5=eff0c7ae75fe5569b9e1190f5f2993a7},
abstract={Automated program repair (APR) has great potential to reduce bugfixing effort and many approaches have been proposed in recent years. APRs are often treated as a search problem where the search space consists of all the possible patches and the goal is to identify the correct patch in the space. Many techniques take a data-driven approach and analyze data sources such as existing patches and similar source code to help identify the correct patch. However, while existing patches and similar code provide complementary information, existing techniques analyze only a single source and cannot be easily extended to analyze both. In this paper, we propose a novel automatic program repair approach that utilizes both existing patches and similar code. Our approach mines an abstract search space from existing patches and obtains a concrete search space by differencing with similar code snippets. Then we search within the intersection of the two search spaces.We have implemented our approach as a tool called SimFix, and evaluated it on the Defects4J benchmark. Our tool successfully fixed 34 bugs. To our best knowledge, this is the largest number of bugs fixed by a single technology on the Defects4J benchmark. Furthermore, as far as we know, 13 bugs fixed by our approach have never been fixed by the current approaches. © 2018 Association for Computing Machinery.},
keywords={Codes (symbols);  Defects;  Repair;  Software testing, Automatic programs;  Code adaptation;  Code differencing;  Data-driven approach;  Data-sources;  Search problem;  Search spaces;  Single source, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhong201816,
author={Zhong, H. and Mei, H.},
title={Mining repair model for exception-related bug},
journal={Journal of Systems and Software},
year={2018},
volume={141},
pages={16-31},
doi={10.1016/j.jss.2018.03.046},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044617398&doi=10.1016%2fj.jss.2018.03.046&partnerID=40&md5=5ff81feb82e40849366596153ec91a55},
abstract={It has long been a hot research topic to detect and to repair bugs automatically. As a common practice, researchers propose approaches for specific bugs, and their approaches typically are limited in handling the variety among bugs. Recently, researchers start to explore automatic program repair. With predefined repair operators and test cases, test-based repair approaches use search algorithms to generate patches for a bug, until a patch passes all the test cases. To improve the effectiveness to generate patches, Martinez and Monperrus (2013b) proposed an approach that mines repair models from past fixes. Although their approach produces positive results, we argue that it can be feasible to further improve their approach, if we mine repair models for bug categories, instead of all bugs. However, the benefits are still unclear, since existing benchmarks do not classify bugs into categories and existing approaches cannot mine repair models for bug categories. In this paper, we implement a tool, called EXFI, that classifies bugs into categories based on their related exceptions. With its support, we construct a benchmark, in which bug categories are marked. Furthermore, we propose an approach, called MIMO, that mines a repair model for each exception. We compared the general repair model with our mined repair models. Our results show that our mined models are all significantly different from the general model. Outside of the projects where our models are mined, we selected 59 real bugs. For each bug, we used our models and the general model to generate correct repair shapes for these bugs. The results show that for 43 out of 59 bugs, our models found faster a correct shape than the general repair model (Martinez and Monperrus, 2013b), and for 5 bugs, our models were able to find correct shapes that were not found by the compared model. © 2018 Elsevier Inc.},
keywords={Benchmarking;  Program debugging, Automatic programs;  Common practices;  Exception-related bugs;  General repair;  Hot research topics;  Repair models;  Repair operator;  Search Algorithms, Repair},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wang2018481,
author={Wang, K. and Singh, R. and Su, Z.},
title={Search, align, and repair: Data-driven feedback generation for introductory programming exercises},
journal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
year={2018},
pages={481-495},
doi={10.1145/3192366.3192384},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049555511&doi=10.1145%2f3192366.3192384&partnerID=40&md5=a16d350c7cd9b1be04456c59cf7ca95d},
abstract={This paper introduces the 'Search, Align, and Repair' data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or MOOC-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the Sarfgen system and evaluated it on thousands of real student attempts from the Microsoft-DEV204.1x edX course and the Microsoft CodeHunt platform. Our results show that Sarfgen can, within two seconds on average, generate concise, useful feedback for 89.7% of the incorrect student submissions. It has been integrated with the Microsoft-DEV204.1X edX class and deployed for production use. © 2018 Association for Computing Machinery.},
keywords={Computer aided analysis;  Computer programming languages;  Grading;  Repair;  Students, Automatic grading;  Computer-aided education;  Data driven;  Fully automated;  Introductory programming;  Introductory programming course;  Large amounts;  Program analysis, Computer aided instruction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gulwani2018465,
author={Gulwani, S. and Radiček, I. and Zuleger, F.},
title={Automated clustering and program repair for introductory programming assignments},
journal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
year={2018},
pages={465-480},
doi={10.1145/3192366.3192387},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049565185&doi=10.1145%2f3192366.3192387&partnerID=40&md5=1ff2247f025dd3d8e783ea1e5f38ddd4},
abstract={Providing feedback on programming assignments is a tedious task for the instructor, and even impossible in large Massive Open Online Courses with thousands of students. Previous research has suggested that program repair techniques can be used to generate feedback in programming education. In this paper, we present a novel fully automated program repair algorithm for introductory programming assignments. The key idea of the technique, which enables automation and scalability, is to use the existing correct student solutions to repair the incorrect attempts. We evaluate the approach in two experiments: (I) We evaluate the number, size and quality of the generated repairs on 4,293 incorrect student attempts from an existing MOOC. We find that our approach can repair 97% of student attempts, while 81% of those are small repairs of good quality. (II) We conduct a preliminary user study on performance and repair usefulness in an interactive teaching setting. We obtain promising initial results (the average usefulness grade 3.4 on a scale from 1 to 5), and conclude that our approach can be used in an interactive setting. © 2018 Copyright held by the owner/author(s).},
keywords={Automation;  Computer programming languages;  Dynamic analysis;  Quality control;  Repair;  Teaching, Automated clustering;  Clustering;  Introductory programming;  Massive open online course;  MOOC;  Programming assignments;  Programming education;  Teaching settings, Students},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2018178,
author={Li, S. and Niu, X. and Jia, Z. and Wang, J. and He, H. and Wang, T.},
title={Logtracker: Learning log revision behaviors proactively from software evolution history},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={178-188},
doi={10.1145/3196321.3196328},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051637000&doi=10.1145%2f3196321.3196328&partnerID=40&md5=d0215d35ac181445405a174136daefba},
abstract={Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted. © 2018 ACM.},
keywords={Codes (symbols);  Computer programming, Automatic tools;  Design and implements;  Empirical studies;  Failure diagnose;  log revision;  Open source projects;  Software Evolution;  Systematic process, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{An2018180,
author={An, K. and Meng, N. and Tilevich, E.},
title={Automatic inference of Java-to-swift translation rules for porting mobile applications},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={180-190},
doi={10.1145/3197231.3197240},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051632117&doi=10.1145%2f3197231.3197240&partnerID=40&md5=8a4fa9c806a76d64581564d9b2f10ec4},
abstract={A native cross-platform mobile app has multiple platform-specific implementations. Typically, an app is developed for one platform and then ported to the remaining ones. Translating an app from one language (e.g., Java) to another (e.g., Swift) by hand is tedious and error-prone, while automated translators either require manually defined translation rules or focus on translating APIs. To automate the translation of native cross-platform apps, we present J2SINFERER, a novel approach that iteratively infers syntactic transformation rules and API mappings from Java to Swift. Given a software corpus in both languages, J2SLNFERER first identifies the syntactically equivalent code based on braces and string similarity. For each pair of similar code segments, J2SLNFERER then creates syntax trees of both languages, leveraging the minimalist domain knowledge of language correspondence (e.g., operators and markers) to iteratively align syntax tree nodes, and to infer both syntax and API mapping rules. J2SLNFERER represents inferred rules as string templates, stored in a database, to translate code from Java to Swift. We evaluated J2SLNFERER with four applications, using one part of the data to infer translation rules, and the other part to apply the rules. With 76% in-project accuracy and 65% cross-project accuracy, J2SLNFERER outperforms in accuracy j2swift, a state-of-the-art Java-to-Swift conversion tool. As native cross-platform mobile apps grow in popularity, J2SLNFERER can shorten their time to market by automating the tedious and error prone task of source-to-source translation. © 2018 ACM.},
keywords={Codes (symbols);  Iterative methods;  Knowledge management;  Mapping;  Software engineering;  Syntactics;  Translation (languages);  Trees (mathematics), Automatic inference;  Error prone tasks;  Mobile applications;  Multiple platforms;  Source-to-source translations;  String similarity;  Syntactic transformations;  Translation rules, Java programming language},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Feldman2018,
author={Feldman, M.Q. and Cho, J.Y. and Ong, M. and Gulwani, S. and Popovic, Z. and Andersen, E.},
title={Automatic diagnosis of students' misconceptions in K-8 mathematics},
journal={Conference on Human Factors in Computing Systems - Proceedings},
year={2018},
volume={2018-April},
doi={10.1145/3173574.3173838},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046976960&doi=10.1145%2f3173574.3173838&partnerID=40&md5=8202873f8061d2f3b0859ec330907431},
abstract={K-8 mathematics students must learn many procedures, such as addition and subtraction. Students frequently learn "buggy" variations of these procedures, which we ideally could identify automatically. This is challenging because there are many possible variations that reflect deep compositions of procedural thought. Existing approaches for K-8 math use manually specified variations which do not scale to new math algorithms or previously unseen misconceptions. Our system examines students' answers and infers how they incorrectly combine basic skills into complex procedures. We evaluate this approach on data from approximately 300 students. Our system replicates 86% of the answers that contain clear systematic mistakes (13%). Investigating further, we found 77% at least partially replicate a known misconception, with 53% matching exactly. We also present data from 29 participants showing that our system can demonstrate inferred incorrect procedures to an educator as successfully as a human expert. © 2018 Association for Computing Machinery.},
keywords={Human computer interaction;  Human engineering, Automatic diagnosis;  Complex procedure;  Elementary education;  Human expert;  Programming by demon-stration, Students},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2018118,
author={Liu, X. and Zhong, H.},
title={Mining stackoverflow for program repair},
journal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
year={2018},
volume={2018-March},
pages={118-129},
doi={10.1109/SANER.2018.8330202},
art_number={8330202},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044591182&doi=10.1109%2fSANER.2018.8330202&partnerID=40&md5=7754c6864ba2ccc911d697af4efe0457},
abstract={In recent years, automatic program repair has been a hot research topic in the software engineering community, and many approaches have been proposed. Although these approaches produce promising results, some researchers criticize that existing approaches are still limited in their repair capability, due to their limited repair templates. Indeed, it is quite difficult to design effective repair templates. An award-wining paper analyzes thousands of manual bug fixes, but summarizes only ten repair templates. Although more bugs are thus repaired, recent studies show such repair templates are still insufficient. We notice that programmers often refer to Stack Overflow, when they repair bugs. With years of accumulation, Stack Overflow has millions of posts that are potentially useful to repair many bugs. The observation motives our work towards mining repair templates from Stack Overflow. In this paper, we propose a novel approach, called SOFix, that extracts code samples from Stack Overflow, and mines repair patterns from extracted code samples. Based on our mined repair patterns, we derived 13 repair templates. We implemented these repair templates in SOFix, and conducted evaluations on the widely used benchmark, Defects4J. Our results show that SOFix repaired 23 bugs, which are more than existing approaches. After comparing repaired bugs and templates, we find that SOFix repaired more bugs, since it has more repair templates. In addition, our results also reveal the urgent need for better fault localization techniques. © 2018 IEEE.},
keywords={Reengineering;  Software engineering, Automatic programs;  Bug fixes;  Engineering community;  Fault localization;  Hot research topics;  Limited repairs;  Stack overflow, Repair},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hieu2018174,
author={Hieu, B.T. and Mustapha, S.M.F.D.S.},
title={Automated data-driven hint generation in intelligent tutoring systems for code-writing: On the road of future research},
journal={International Journal of Emerging Technologies in Learning},
year={2018},
volume={13},
number={9},
pages={174-189},
doi={10.3991/ijet.v13i09.8023},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057574543&doi=10.3991%2fijet.v13i09.8023&partnerID=40&md5=349e90c4f60f6f19a18e4cd048a7246b},
abstract={Introductory programming is an essential part of the curriculum in any engineering discipline in universities. However, for many beginning students, it is very difficult to learn. In particular, these students often get stuck and frustrated when attempting to solve programming exercises. One way to assist beginning programmers to overcome difficulties in learning to program is to use intelligent tutoring systems (ITSs) for programming, which can provide students with personalized hints of students' solving process in programming exercises. Currently, mostly these systems manually construct the domain models. They take much time to construct, especially for exercises with very large solution spaces. One of the major challenges associated with handling ITSs for programming comes from the diversity of possible code solutions that a student can write. The use of data-driven approaches to develop these ITSs is just starting to be explored in the field. Given that this is still a relatively new research field, many challenges are still remained unsolved. Our goal in this paper is to review and classify analysis techniques that are requested to generate datadriven hints in ITSs for programming. This work also aims equally to identify the possible future directions in this research field. © 2018, Kassel University Press GmbH.},
keywords={Computer aided instruction;  Students, Analysis techniques;  Data driven;  Data-driven approach;  Engineering disciplines;  Intelligent tutoring system;  Intelligent tutoring system (ITSs);  Introductory programming;  Programming exercise, Engineering research},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Xin2017660,
author={Xin, Q. and Reiss, S.P.},
title={Leveraging syntax-related code for automated program repair},
journal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
year={2017},
pages={660-670},
doi={10.1109/ASE.2017.8115676},
art_number={8115676},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041434410&doi=10.1109%2fASE.2017.8115676&partnerID=40&md5=d6fd3994375177e5397fddaadf7c1d01},
abstract={We present our automated program repair technique ssFix which leverages existing code (from a code database) that is syntax-related to the context of a bug to produce patches for its repair. Given a faulty program and a fault-exposing test suite, ssFix does fault localization to identify suspicious statements that are likely to be faulty. For each such statement, ssFix identifies a code chunk (or target chunk) including the statement and its local context. ssFix works on the target chunk to produce patches. To do so, it first performs syntactic code search to find candidate code chunks that are syntax-related, i.e., structurally similar and conceptually related, to the target chunk from a code database (or codebase) consisting of the local faulty program and an external code repository. ssFix assumes the correct fix to be contained in the candidate chunks, and it leverages each candidate chunk to produce patches for the target chunk. To do so, ssFix translates the candidate chunk by unifying the names used in the candidate chunk with those in the target chunk; matches the chunk components (expressions and statements) between the translated candidate chunk and the target chunk; and produces patches for the target chunk based on the syntactic differences that exist between the matched components and in the unmatched components. ssFix finally validates the patched programs generated against the test suite and reports the first one that passes the test suite. We evaluated ssFix on 357 bugs in the Defects4J bug dataset. Our results show that ssFix successfully repaired 20 bugs with valid patches generated and that it outperformed five other repair techniques for Java. © 2017 IEEE.},
keywords={Automation;  Repair;  Software engineering;  Software testing;  Syntactics, Code database;  Code search;  code transfer;  External code;  Fault localization;  Local contexts;  Repair techniques, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Suzuki2017107,
author={Suzuki, R. and Soares, G. and Head, A. and Glassman, E. and Reis, R. and Mongiovi, M. and D'Antoni, L. and Hartmann, B.},
title={TraceDiff: Debugging unexpected code behavior using trace divergences},
journal={Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},
year={2017},
volume={2017-October},
pages={107-115},
doi={10.1109/VLHCC.2017.8103457},
art_number={8103457},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041002633&doi=10.1109%2fVLHCC.2017.8103457&partnerID=40&md5=28f50d6d73e7bb7f88973ff590bfa785},
abstract={Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers' hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool. © 2017 IEEE.},
keywords={Codes (symbols);  Students;  Teaching;  Visual languages, Code execution;  Feed-back designs;  Feedback systems;  Personalized feedback;  Program synthesis;  Programming assignments;  Visual debugging;  Visualization technique, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zaytsev2017212,
author={Zaytsev, V.},
title={Parser Generation by Example for Legacy Pattern Languages},
journal={GPCE 2017 - Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences, co-located with SPLASH 2017},
year={2017},
pages={212-218},
doi={10.1145/3136040.3136058},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041182217&doi=10.1145%2f3136040.3136058&partnerID=40&md5=8d8111e2ad9dfc69526caaa4f4ec8ce7},
abstract={Most modern software languages enjoy relatively free and relaxed concrete syntax, with significant flexibility of formatting of the program/model/sheet text. Yet, in the dark legacy corners of software engineering there are still languages with a strict fixed column-based structure—the compromises of times long gone, attempting to combine some human readability with some ease of machine processing. In this paper, we consider an industrial case study for retirement of a legacy domain-specific language, completed under extreme circumstances: absolute lack of documentation, varying line structure, hierarchical blocks within one file, scalability demands for millions of lines of code, performance demands for manipulating tens of thousands multi-megabyte files, etc. However, the regularity of the language allowed to infer its structure from the available examples, automatically, and produce highly efficient parsers for it. © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.},
keywords={Computer programming languages;  Problem oriented languages;  Software engineering;  Syntactics, Grammar inference;  Language acquisition;  Legacy software;  Parser generation;  Pattern languages, Computational linguistics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{D'Antoni2017582,
author={D'Antoni, L. and Singh, R. and Vaughn, M.},
title={NoFAQ: Synthesizing command repairs from examples},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={582-592},
doi={10.1145/3106237.3106241},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030779381&doi=10.1145%2f3106237.3106241&partnerID=40&md5=d88c0cefc995e65243cedba1951f22a5},
abstract={Command-line tools are confusing and hard to use due to their cryptic error messages and lack of documentation. Novice users often resort to online help-forums for finding corrections to their buggy commands, but have a hard time in searching precisely for posts that are relevant to their problem and then applying the suggested solutions to their buggy command. We present NoFAQ, a tool that uses a set of rules to suggest possible fixes when users write buggy commands that trigger commonly occurring errors. The rules are expressed in a language called Fixit and each rule pattern-matches against the user's buggy command and corresponding error message, and uses these inputs to produce a possible fixed command. NoFAQ automatically learns Fixit rules from examples of buggy and repaired commands. We evaluate NoFAQ on two fronts. First, we use 92 benchmark problems drawn from an existing tool and show that NoFAQ is able to synthesize rules for 81 benchmark problems in real time using just 2 to 5 input-output examples for each rule. Second, we run our learning algorithm on the examples obtained through a crowd-sourcing interface and show that the learning algorithm scales to large sets of examples. © 2017 Copyright held by the owner/author(s).},
keywords={Benchmarking;  Computer programming languages;  Errors;  Learning algorithms;  Problem oriented languages, Bench-mark problems;  Command line interface;  Domain specific languages;  Error messages;  Input-output;  Program synthesis;  Programming by Example;  Set of rules, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dotzler2017798,
author={Dotzler, G. and Kamp, M. and Kreutzer, P. and Philippsen, M.},
title={More accurate recommendations for method-level changes},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={798-808},
doi={10.1145/3106237.3106276},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030764759&doi=10.1145%2f3106237.3106276&partnerID=40&md5=8d50b7e1dff7e2bf9f396e921934cae8},
abstract={During the life span of large software projects, developers often apply the same code changes to different code locations in slight variations. Since the application of these changes to all locations is time-consuming and error-prone, tools exist that learn change patterns from input examples, search for possible pattern applications, and generate corresponding recommendations. In many cases, the generated recommendations are syntactically or semantically wrong due to code movements in the input examples. Thus, they are of low accuracy and developers cannot directly copy them into their projects without adjustments. We present the Accurate REcommendation System (ARES) that achieves a higher accuracy than other tools because its algorithms take care of code movements when creating patterns and recommendations. On average, the recommendations by ARES have an accuracy of 96% with respect to code changes that developers have manually performed in commits of source code archives. At the same time ARES achieves precision and recall values that are on par with other tools. © 2017 Copyright held by the owner/author(s).},
keywords={Codes (symbols);  Recommender systems, Change patterns;  Code changes;  Error prones;  Level change;  Precision and recall;  Program transformations;  Refactorings;  Software project, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yi2017740,
author={Yi, J. and Ahmed, U.Z. and Karkare, A. and Tan, S.H. and Roychoudhury, A.},
title={A feasibility study of using automated program repair for introductory programming assignments},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={740-751},
doi={10.1145/3106237.3106262},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030782216&doi=10.1145%2f3106237.3106262&partnerID=40&md5=12bd9d8ae4955e88d48f136a1f811852},
abstract={Despite the fact that an intelligent tutoring system for programming (ITSP) has long attracted interest, its widespread use has been hindered by the difficulty of generating personalized feedback automatically. Meanwhile, automated program repair (APR) is an emerging new technology that automatically fixes software bugs, and it has been shown that APR can fix the bugs of large real-world software. In this paper, we study the feasibility of marrying an ITSP and APR. We perform our feasibility study with four stateof- the-art APR tools (GenProg, AE, Angelix, and Prophet), and 661 programs written by the students taking an introductory programming course. We found that when APR tools are used out of the box, only about 30% of the programs in our dataset are repaired. This low repair rate is largely due to the student programs often being significantly incorrect - in contrast, professional software for which APR was successfully applied typically fails only a small portion of tests. To bridge this gap, we adopt in APR a new repair policy akin to the hint generation policy employed in the existing ITSP. This new repair policy admits partial repairs that address part of failing tests, which results in 84% improvement of repair rate. We also performed a user study with 263 novice students and 37 graders, and identified an understudied problem; while the graders seem to gain benefits from repairs, novice students do not seem to know how to effectively make use of generated repairs as hints. © 2017 Association for Computing Machinery.},
keywords={Computer aided instruction;  Education computing;  Planning;  Repair;  Software engineering;  Software testing;  Students;  Teaching;  Technology transfer, Feasibility studies;  Intelligent tutoring system;  Introductory programming;  Introductory programming course;  Personalized feedback;  Professional software;  State of the art;  Student project, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Long2017727,
author={Long, F. and Amidon, P. and Rinard, M.},
title={Automatic inference of code transforms for patch generation},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={727-739},
doi={10.1145/3106237.3106253},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030788844&doi=10.1145%2f3106237.3106253&partnerID=40&md5=5cd20407333b20227c6a027cb3fc15c5},
abstract={We present a new system, Genesis, that processes human patches to automatically infer code transforms for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the complete Genesis patch generation system working with real-world patches and defects collected from 372 Java projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from previous successful patches. © 2017 Association for Computing Machinery.},
keywords={Codes (symbols);  Inference engines;  Software engineering, Automatic inference;  First systems;  Generation systems;  Inference algorithm;  Patch generation;  Real-world;  Search spaces, Automatic programming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mottin20171977,
author={Mottin, D. and Lissandrini, M. and Velegrakis, Y. and Palpanas, T.},
title={New trends on exploratory methods for data analytics},
journal={Proceedings of the VLDB Endowment},
year={2017},
volume={10},
number={12},
pages={1977-1980},
doi={10.14778/3137765.3137824},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036654672&doi=10.14778%2f3137765.3137824&partnerID=40&md5=90d86b7ee7fb1259483ac733a3a9c3aa},
abstract={Data usually comes in a plethora of formats and dimensions, rendering the exploration and information extraction processes cumbersome. Thus, being able to cast exploratory queries in the data with the intent of having an immediate glimpse on some of the data properties is becoming crucial. An exploratory query should be simple enough to avoid complicate declarative languages (such as SQL) and mechanisms, and at the same time retain the exibility and expressiveness of such languages. Recently, we have witnessed a rediscovery of the so called example-based methods, in which the user, or the analyst circumvent query languages by using examples as input. An example is a representative of the intended results, or in other words, an item from the result set. Example-based methods exploit inherent characteristics of the data to infer the results that the user has in mind, but may not able to (easily) express. They can be useful both in cases where a user is looking for information in an unfamiliar dataset, or simply when she is exploring the data without knowing what to find in there. In this tutorial, we present an excursus over the main methods for exploratory analysis, with a particular focus on examplebased methods. We show how different data types require different techniques, and present algorithms that are specifically designed for relational, textual, and graph data. © 2017 VLDB.},
keywords={Query languages, Data analytics;  Data properties;  Data type;  Declarative Languages;  Example-based methods;  Exploratory analysis;  Graph data;  Inherent characteristics, Data mining},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Phothilimthana2017182,
author={Phothilimthana, P.M. and Sridhara, S.},
title={High-coverage hint generation for massive courses: Do automated hints help CS1 students?},
journal={Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE},
year={2017},
volume={Part F128680},
pages={182-187},
doi={10.1145/3059009.3059058},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029501992&doi=10.1145%2f3059009.3059058&partnerID=40&md5=4b7156222608027b2be15013517361e5},
abstract={In massive programming courses, automated hint generation offers the promise of zero-cost, zero-latency assistance for students who are struggling to make progress on solving a program. While a more robust hint generation approach based on path construction requires tremendous engineering effort to build, another easier-to-build approach based on program mutations suffers from low coverage. This paper describes a robust hint generation system that extends the coverage of the mutation-based approach using two complementary techniques. A syntax checker detects common syntax misconception errors in individual subexpressions to guide students to partial solutions that can be evaluated for the semantic correctness. A mutation-based approach is then used to generate hints for almost-correct programs. If the mutation-based approach fails, a case analyzer detects missing program branches to guide students to partial solutions with reasonable structures. After analyzing over 75, 000 program submissions and 8, 789 hint requests, we found that using all three techniques together could offer hints for any program, no matter how far it was from a correct solution. Furthermore, our analysis shows that hints contributed to students' progress while still encouraging the students to solve problems by themselves. © 2017 ACM.},
keywords={Automation;  Computer aided analysis;  Computer aided instruction;  Education;  Education computing;  Engineering education;  Engineering research;  Semantics;  Syntactics;  Teaching, Automated tutor;  Complementary techniques;  Computer-aided education;  Generation systems;  Path construction;  Program analysis;  Program synthesis;  Programming course, Students},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Suzuki20172951,
author={Suzuki, R. and Soares, G. and Glassman, E. and Head, A. and D'Antoni, L. and Hartmann, B.},
title={Exploring the design space of automatically synthesized hints for introductory programming assignments},
journal={Conference on Human Factors in Computing Systems - Proceedings},
year={2017},
volume={Part F127655},
pages={2951-2958},
doi={10.1145/3027063.3053187},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019558441&doi=10.1145%2f3027063.3053187&partnerID=40&md5=4939483d8199b0c855668f61a42e3821},
abstract={For massive programming classrooms, recent advances in program synthesis offer means to automatically grade and debug student submissions, and generate feedback at scale. A key challenge for synthesis-based autograders is how to design personalized feedback for students that is as effective as manual feedback given by teachers today. To understand the state of hint-giving practice, we analyzed 132 online Q&A posts and conducted a semi-structured interview with a teacher from a local massive programming class. We identified five types of teacher hints that can also be generated by program synthesis. These hints describe transformations, locations, data, behavior, and examples. We describe our implementation of three of these hint types. This work paves the way for future deployments of automatic, pedagogically-useful programming hints driven by program synthesis. Copyright © 2017 by the Association for Computing Machinery, Inc. (ACM).},
keywords={Education;  Human engineering;  Program debugging;  Students, Automated feedback;  Design spaces;  Introductory programming;  Personalized feedback;  Program synthesis;  Programming class;  Programming education;  Semi structured interviews, Teaching},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Head201789,
author={Head, A. and Glassman, E. and Soares, G. and Suzuki, R. and Figueredo, L. and D'Antoni, L. and Hartmann, B.},
title={Writing reusable code feedback at scale with mixed-initiative program synthesis},
journal={L@S 2017 - Proceedings of the 4th (2017) ACM Conference on Learning at Scale},
year={2017},
pages={89-98},
doi={10.1145/3051457.3051467},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018384491&doi=10.1145%2f3051457.3051467&partnerID=40&md5=619fcab6eeb9b34a9db9501eb3af2e9d},
abstract={In large introductory programming classes, teacher feedback on individual incorrect student submissions is often infeasible. Program synthesis techniques are capable of fixing student bugs and generating hints automatically, but they lack the deep domain knowledge of a teacher and can generate functionally correct but stylistically poor fixes. We introduce a mixedinitiative approach which combines teacher expertise with data-driven program synthesis techniques. We demonstrate our novel approach in two systems that use different interaction mechanisms. Our systems use program synthesis to learn bug-fixing code transformations and then cluster incorrect submissions by the transformations that correct them. The MISTAKEBROWSER system learns transformations from examples of students fixing bugs in their own submissions. The FIXPROPAGATOR system learns transformations from teachers fixing bugs in incorrect student submissions. Teachers can write feedback about a single submission or a cluster of submissions and propagate the feedback to all other submissions that can be fixed by the same transformation. Two studies suggest this approach helps teachers better understand student bugs and write reusable feedback that scales to a massive introductory programming classroom. © 2017 ACM.},
keywords={Computer software reusability;  Cosine transforms;  Education;  Students;  Teaching, Code transformation;  Domain knowledge;  Interaction mechanisms;  Introductory programming;  Mixed-initiative;  Program synthesis;  Programming education;  Teacher feedback, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gulwani20173,
author={Gulwani, S. and Jain, P.},
title={Programming by examples: PL Meets ML},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10695 LNCS},
pages={3-20},
doi={10.1007/978-3-319-71237-6_1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035036817&doi=10.1007%2f978-3-319-71237-6_1&partnerID=40&md5=d7a830a9231a66768fc21ac7a34d3cad},
abstract={Programming by Examples (PBE) involves synthesizing intended programs in an underlying domain-specific language from example-based specifications. PBE systems are already revolutionizing the application domain of data wrangling and are set to significantly impact several other domains including code refactoring. There are three key components in a PBE system. (i) A search algorithm that can efficiently search for programs that are consistent with the examples provided by the user. We leverage a divide-and-conquer-based deductive search paradigm that inductively reduces the problem of synthesizing a program expression of a certain kind that satisfies a given specification into sub-problems that refer to sub-expressions or sub-specifications. (ii) Program ranking techniques to pick an intended program from among the many that satisfy the examples provided by the user. We leverage features of the program structure as well of the outputs generated by the program on test inputs. (iii) User interaction models to facilitate usability and debuggability. We leverage active-learning techniques based on clustering inputs and synthesizing multiple programs. Each of these PBE components leverage both symbolic reasoning and heuristics. We make the case for synthesizing these heuristics from training data using appropriate machine learning methods. This can not only lead to better heuristics, but can also enable easier development, maintenance, and even personalization of a PBE system. © Springer International Publishing AG 2017.},
keywords={Artificial intelligence;  Computer programming languages;  Graphical user interfaces;  Heuristic methods;  Learning algorithms;  Learning systems;  Object oriented programming;  Problem oriented languages;  Specifications, Code re-factoring;  Divide and conquer;  Domain specific languages;  Machine learning methods;  Program structures;  Programming by Example;  Search Algorithms;  Symbolic reasoning, Software testing},
document_type={Conference Paper},
source={Scopus},
}



@CONFERENCE{Wang2018415,
author={Wang, C. and Li, Y. and Xu, B.},
title={How many versions does a bug live in? an empirical study on text features for bug lifecycle prediction},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2018},
volume={2018-July},
pages={415-420},
doi={10.18293/SEKE2018-176},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056862953&doi=10.18293%2fSEKE2018-176&partnerID=40&md5=a7272695d95a2df2f5c8a4617e84d198},
abstract={During the software system's maintenance and evolution, finding and removing software bugs is a very important part that consumes a large amount of money and effort. To analyze different bugs' character, it is very essential to know how long or which period of versions does the bug live in. In this study, we define version-based bug lifecycle and propose a text features based classification model to predict the versionlength of bug lifecycle. We collect 57000+ bugs from 10 well-know Apache Software Foundation projects to construct our dataset, and use the tf-idf method to collect our text features from bug report's summary and description. Our experimental results show that the text feature based method performs better than other baseline methods on 10 projects. The text feature based Naive Bayes classifiers outperforms all other methods with different features and classifiers. © 2018 Universitat zu Koln. All rights reserved.},
keywords={Knowledge engineering;  Life cycle;  Program debugging;  Software engineering;  Technology transfer;  Text processing, Apache software foundations;  Baseline methods;  Classification models;  Empirical studies;  Large amounts;  Naive Bayes classifiers;  Software bug;  Software systems, Classification (of information)},
document_type={Conference Paper},
source={Scopus},
}



@CONFERENCE{Dotzler2017798,
author={Dotzler, G. and Kamp, M. and Kreutzer, P. and Philippsen, M.},
title={More accurate recommendations for method-level changes},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={798-808},
doi={10.1145/3106237.3106276},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030764759&doi=10.1145%2f3106237.3106276&partnerID=40&md5=8d50b7e1dff7e2bf9f396e921934cae8},
abstract={During the life span of large software projects, developers often apply the same code changes to different code locations in slight variations. Since the application of these changes to all locations is time-consuming and error-prone, tools exist that learn change patterns from input examples, search for possible pattern applications, and generate corresponding recommendations. In many cases, the generated recommendations are syntactically or semantically wrong due to code movements in the input examples. Thus, they are of low accuracy and developers cannot directly copy them into their projects without adjustments. We present the Accurate REcommendation System (ARES) that achieves a higher accuracy than other tools because its algorithms take care of code movements when creating patterns and recommendations. On average, the recommendations by ARES have an accuracy of 96% with respect to code changes that developers have manually performed in commits of source code archives. At the same time ARES achieves precision and recall values that are on par with other tools. © 2017 Copyright held by the owner/author(s).},
keywords={Codes (symbols);  Recommender systems, Change patterns;  Code changes;  Error prones;  Level change;  Precision and recall;  Program transformations;  Refactorings;  Software project, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Asaduzzaman2016512,
author={Asaduzzaman, M. and Roy, C.K. and Schneider, K.A. and Hou, D.},
title={A Simple, Efficient, Context-sensitive Approach for Code Completion},
journal={Journal of Software: Evolution and Process},
year={2016},
volume={28},
number={7},
pages={512-541},
doi={10.1002/smr.1791},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977540582&doi=10.1002%2fsmr.1791&partnerID=40&md5=d385d6c1e31bc5f299d78135d8608574},
abstract={Code completion helps developers use application programming interfaces (APIs) and frees them from remembering every detail. In this paper, we first describe a novel technique called Context-sensitive Code Completion (CSCC) for improving the performance of API method call completion. CSCC is context sensitive in that it uses new sources of information as the context of a target method call. CSCC indexes method calls in code examples by their context. To recommend completion proposals, CSCC ranks candidate methods by the similarities between their contexts and the context of the target call. Evaluation using a set of subject systems and five popular state-of-the-art techniques suggests that CSCC performs better than existing type or example-based code completion systems. We conduct experiments to find how different contextual elements of the target call benefit CSCC. Next, we investigate the adaptability of the technique to support another form of code completion, i.e., field completion. Evaluation with eight different subject systems suggests that CSCC can easily support field completion with high accuracy. Finally, we compare CSCC with four popular statistical language models that support code completion. Results of statistical tests from our study suggest that CSCC not only outperforms those techniques that are based on token level language models, but also in most cases performs better or equally well with GraLan, the state-of-the-art graph-based language model. © 2016 John Wiley & Sons, Ltd.},
keywords={Application programming interfaces (API);  Codes (symbols);  Computational linguistics;  Graphic methods;  Natural language processing systems, API methods;  Code completions;  Context sensitive;  Language model;  Simhash, Context sensitive languages},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kreutzer201661,
author={Kreutzer, P. and Dotzler, G. and Ring, M. and Eskofier, B.M. and Philippsen, M.},
title={Automatic clustering of code changes},
journal={Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},
year={2016},
pages={61-72},
doi={10.1145/2901739.2901749},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974623425&doi=10.1145%2f2901739.2901749&partnerID=40&md5=5ceb185eb95befc365edaab3d81d6045},
abstract={Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes. To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes. We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes. © 2016 ACM.},
keywords={Codes (symbols);  Open source software;  Open systems, Automatic clustering;  Bug finding tools;  Clustering;  Code changes;  Open source projects;  Research tools;  Similarity metrics;  Software repositories, Clustering algorithms},
document_type={Conference Paper},
source={Scopus},
}

Scopus
EXPORT DATE: 12 March 2019

@ARTICLE{Stevens2019491,
author={Stevens, R. and Molderez, T. and De Roover, C.},
title={Querying distilled code changes to extract executable transformations},
journal={Empirical Software Engineering},
year={2019},
volume={24},
number={1},
pages={491-535},
doi={10.1007/s10664-018-9644-3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053271297&doi=10.1007%2fs10664-018-9644-3&partnerID=40&md5=e63534634789c9c6f417132721c3f25a},
abstract={Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. We show that our approach is able to recall different implementation variants of the same source code evolution in histories of different software projects. We also evaluate the expressiveness and ease-of-use of our approach in a user study. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
keywords={Computer programming languages, Change distilling;  Change querying;  Evolution patterns;  Fine-grained changes;  Intermediate state;  Logic meta programming;  Mining software repositories;  Software project, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Somogyi201951,
author={Somogyi, F.A. and Asztalos, M.},
title={A survey on text-based modeling in model evolution and management},
journal={Periodica polytechnica Electrical engineering and computer science},
year={2019},
volume={63},
number={1},
pages={51-65},
doi={10.3311/PPee.12305},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061730571&doi=10.3311%2fPPee.12305&partnerID=40&md5=28ec18c1a37328a04dba247cf0260499},
abstract={Model-driven software engineering methodologies like model-driven engineering aim to improve the productivity of software development by using graph-based models as the main artifacts during development, and generating the source code from these models. The models are usually displayed and edited using a graphical notation. However, they can also be described using a textual notation. This has some advantages and disadvantages compared to the graphical approach. For example, while editing the model, we can better focus on the details instead of a broad overview. Similarly to source code, models evolve rapidly during development. Handling and managing the evolution of models is an important task in model-driven methodologies and is an active research area today. However, there exist few research on text-based modeling approaches, compared to graph-based ones. This paper introduces the text-based modeling research field based on existing literature, and presents the state-of-the-art of the field related to model evolution and management. Our goal is to identify challenges and directions for future research in this field. The main topics covered are model differencing and merging, and the synchronization of the textual and graphical notations. © 2018 Budapest University of Technology and Economics. All Rights Reserved.},
keywords={Graphic methods;  Reviews;  Surveying;  Surveys, Domain specific modeling;  Model driven development;  Model evolution;  Model management;  Model-based engineering;  Model-driven Engineering, Software design},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yang201892,
author={Yang, C. and James WhiteHead, E.},
title={A taxonomy of code changes occurring within a statement or a signature},
journal={Proceedings - 2018 12th International Symposium on Theoretical Aspects of Software Engineering, TASE 2018},
year={2018},
volume={2018-January},
pages={92-99},
doi={10.1109/TASE.2018.00020},
art_number={8560738},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062324469&doi=10.1109%2fTASE.2018.00020&partnerID=40&md5=7ce389dad8d7f517237a49395111f7b1},
abstract={We propose a taxonomy of code changes at a granularity finer than the statement level. It classifies changes that occur within a statement or signature. We firstly define the changes based on the proposed operations on the tree of the statement or signature. Then, we classify the changes according to action type, entity kind, element kind and pattern kind. Based on the current implementation, we applied it to four open Java projects. Through the case study, we validated that the taxonomy can classify changes in all the modified statements and signatures. And, we checked the proportions of change patterns. Furthermore, we demonstrated that it is easy to find out the rename-induced statement updates with the help of the taxonomy. As a result, this taxonomy can be used for further change understanding and change analysis. © 2018 IEEE.},
keywords={Codes (symbols);  Software engineering, Change analysis;  Change patterns;  Code changes;  Element kinds;  Software Evolution;  Source code changes, Taxonomies},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Frick2018705,
author={Frick, V. and Wedenig, C. and Pinzger, M.},
title={DiffViz: A diff algorithm independent visualization tool for edit scripts},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
year={2018},
pages={705-709},
doi={10.1109/ICSME.2018.00081},
art_number={8530084},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058307098&doi=10.1109%2fICSME.2018.00081&partnerID=40&md5=215cc7fd14aa3544b5c3d9a72845b216},
abstract={A number of approaches and tools exist that extract and visualize the changes between two versions of a file and thereby help developers to understand them. DiffViz is an interactive visualization tool that visualizes the changes independent from the differencing algorithm. It supports, but is not limited to, a granularity on the level of abstract syntax trees. Furthermore, it provides several new features, such as node matching and the mini-map, to navigate and analyze the changes. A demo of the installation and example usage of the tool is available here: https://youtu.be/RF93ey9GYoc. © 2018 IEEE.},
keywords={Computer software maintenance;  Flow visualization;  Trees (mathematics), Abstract Syntax Trees;  Differencing algorithm;  Edit scripts;  Interactive visualization tool;  It supports;  Visualization tools, Visualization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Frick2018264,
author={Frick, V. and Grassauer, T. and Beck, F. and Pinzger, M.},
title={Generating accurate and compact edit scripts using tree differencing},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
year={2018},
pages={264-274},
doi={10.1109/ICSME.2018.00036},
art_number={8530035},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058289624&doi=10.1109%2fICSME.2018.00036&partnerID=40&md5=5a0874c1dbad89a7cc6e6e231df79d78},
abstract={For analyzing changes in source code, edit scriptsare used to describe the differences between two versions of afile. These scripts consist of a list of actions that, applied to thesource file, result in the new version of the file. In contrast toline-based source code differencing, tree-based approaches suchas GumTree, MTDIFF, or ChangeDistiller extract changes bycomparing the abstract syntax trees (AST) of two versions of asource file. One benefit of tree-based approaches is their abilityto capture moved (sub) trees in the AST. Our approach, theIterative Java Matcher (IJM), builds upon GumTree and aims atgenerating more accurate and compact edit scripts that capturethe developer's intent. This is achieved by improving the qualityof the generated move and update actions, which are the mainsource of inaccurate actions generated by previous approaches. To evaluate our approach, we conducted a study with 11 external experts and manually analyzed the accuracy of 2400 randomly selected editactions. Comparing IJM to GumTree and MTDIFF, the resultsshow that IJM provides better accuracy for move and updateactions and is more beneficial to understanding the changes. © 2018 IEEE.},
keywords={Syntactics;  Trees (mathematics), Abstract Syntax Trees;  Change extractions;  External experts;  Software Evolution;  Source codes;  Tree differencing;  Tree-based approach, Computer software maintenance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ponta2018449,
author={Ponta, S.E. and Plate, H. and Sabetta, A.},
title={Beyond metadata: code-centric and usage-based analysis of known vulnerabilities in open-source software},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
year={2018},
pages={449-460},
doi={10.1109/ICSME.2018.00054},
art_number={8530051},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058302988&doi=10.1109%2fICSME.2018.00054&partnerID=40&md5=b4b3fb60996781520d12d6f2cf9d72fd},
abstract={The use of open-source software (OSS) is ever-increasing, and so is the number of open-source vulnerabilities being discovered and publicly disclosed. The gains obtained from the reuse of community-developed libraries may be offset by the cost of detecting, assessing, and mitigating their vulnerabilities in a timely manner. In this paper we present a novel method to detect, assess and mitigate OSS vulnerabilities that improves on state-of-The-Art approaches, which commonly depend on metadata to identify vulnerable OSS dependencies. Our solution instead is code-centric and combines static and dynamic analysis to determine the reachability of the vulnerable portion of libraries used (directly or transitively) by an application. Taking this usage into account, our approach then supports developers in choosing among the existing non-vulnerable library versions. Vulas, the tool implementing our code-centric and usage-based approach, is officially recommended by SAP to scan its Java software, and has been successfully used to perform more than 250000 scans of about 500 applications since December 2016. We report on our experience and on the lessons we learned when maturing the tool from a research prototype to an industrial-grade solution. © 2018 IEEE.},
keywords={Application programs;  Codes (symbols);  Computer software maintenance;  Industrial research;  Information dissemination;  Libraries;  Metadata;  Open systems, Industrial grades;  Java software;  Known vulnerabilities;  Open sources;  Research prototype;  Static and dynamic analysis;  Update supports;  Vulnerability analysis, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2018275,
author={Liu, K. and Kim, D. and Koyuncu, A. and Li, L. and Bissyande, T.F. and Le Traon, Y.},
title={A closer look at real-world patches},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
year={2018},
pages={275-286},
doi={10.1109/ICSME.2018.00037},
art_number={8530036},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058333823&doi=10.1109%2fICSME.2018.00037&partnerID=40&md5=b591978c95259da9ce8bbfbc17bbb7ec},
abstract={Bug fixing is a time-consuming and tedious task. To reduce the manual efforts in bug fixing, researchers have presented automated approaches to software repair. Unfortunately, recent studies have shown that the state-of-The-Art techniques in automated repair tend to generate patches only for a small number of bugs even with quality issues (e.g., incorrect behavior and nonsensical changes). To improve automated program repair (APR) techniques, the community should deepen its knowledge on repair actions from real-world patches since most of the techniques rely on patches written by human developers. Previous investigations on real-world patches are limited to statement level that is not sufficiently fine-grained to build this knowledge. In this work, we contribute to building this knowledge via a systematic and fine-grained study of 16,450 bug fix commits from seven Java open-source projects. We find that there are opportunities for APR techniques to improve their effectiveness by looking at code elements that have not yet been investigated. We also discuss nine insights into tuning automated repair tools. For example, a small number of statement and expression types are recurrently impacted by real-world patches, and expression-level granularity could reduce search space of finding fix ingredients, where previous studies never explored. © 2018 IEEE.},
keywords={Automation;  Computer software maintenance;  Open systems;  Repair;  Trees (mathematics), Abstract Syntax Trees;  Automated approach;  Expression levels;  Fix pattern;  Open source projects;  Program patch;  Software repair;  State-of-the-art techniques, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2018287,
author={Wang, Y. and Meng, N. and Zhong, H.},
title={An empirical study of multi-entity changes in real bug fixes},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
year={2018},
pages={287-298},
doi={10.1109/ICSME.2018.00038},
art_number={8530037},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057470499&doi=10.1109%2fICSME.2018.00038&partnerID=40&md5=3e54a06b41738a4716cb9b51363baf12},
abstract={Prior studies showed that developers applied repeated bug fixes-similar or identical code changes-To multiple locations. According to the observation, researchers built tools to automatically generate candidate patches from the repeated bug-fixing patterns. However, all such research focuses on the recurring change patterns within single methods. We are curious whether there are also repeated bug fixes that change multiple program entities (e.g., classes, methods, and fields); and if so, how we can leverage such recurring change patterns to further help developers fix bugs. In this paper, we present a comprehensive empirical study on multi-entity bug fixes in terms of their frequency, composition, and semantic meanings. Specifically for each bug fix, we first used our approach InterPart to perform static inter-procedural analysis on partial programs (i.e., the old and new versions of changed Java files), and to extract change dependency graphs (CDGs)-graphs that connect multiple changed entities based on their syntactic dependencies. By extracting common subgraphs from the CDGs of different fixes, we identified the recurring change patterns. Our study on Aries, Cassandra, Derby, and Mahout shows that (1) 52-58% of bug fixes involved multi-entity changes; (2) 6 recurring change patterns commonly exist in all projects; and (3) 19-210 entity pairs were repetitively co-changed mainly because the pairs invoked the same methods, accessed the same fields, or contained similar content. These results helped us better understand the gap between the fixes generated by existing automatic program repair (APR) approaches and the real fixes. Our observations will shed light on the follow-up research of automatic program comprehension and modification. © 2018 IEEE.},
keywords={Computer software;  Computer software maintenance;  Semantics, Automatic programs;  Change dependencies;  Change patterns;  Empirical studies;  Inter-procedural analysis;  Multiple program;  Software entities;  Syntactic dependencies, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Foo2018791,
author={Foo, D. and Chua, H. and Yeo, J. and Ang, M.Y. and Sharma, A.},
title={Efficient static checking of library updates},
journal={ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year={2018},
pages={791-796},
doi={10.1145/3236024.3275535},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058317239&doi=10.1145%2f3236024.3275535&partnerID=40&md5=e72811ff0b7d5dcf5026c68a24eedb85},
abstract={Software engineering practices have evolved to the point where a developer writing a new application today doesn't start from scratch, but reuses a number of open source libraries and components. These third-party libraries evolve independently of the applications in which they are used, and may not maintain stable interfaces as bugs and vulnerabilities in them are fixed. This in turn causes API incompatibilities in downstream applications which must be manually resolved. Oversight here may manifest in many ways, from test failures to crashes at runtime. To address this problem, we present a static analysis for automatically and efficiently checking if a library upgrade introduces an API incompatibility. Our analysis does not rely on reported version information from library developers, and instead computes the actual differences between methods in libraries across different versions. The analysis is scalable, enabling real-time diff queries involving arbitrary pairs of library versions. It supports a vulnerability remediation product which suggests library upgrades automatically and is lightweight enough to be part of a continuous integration/delivery (CI/CD) pipeline. To evaluate the effectiveness of our approach, we determine semantic versioning adherence of a corpus of open source libraries taken from Maven Central, PyPI, and RubyGems. We find that on average, 26% of library versions are in violation of semantic versioning. We also analyze a collection of popular open source projects from GitHub to determine if we can automatically update libraries in them without causing API incompatibilities. Our results indicate that we can suggest upgrades automatically for 10% of the libraries. © 2018 Association for Computing Machinery.},
keywords={Application programs;  Libraries;  Open systems;  Program debugging;  Semantics;  Static analysis, api diffs;  Call graphs;  Continuous integrations;  Downstream applications;  Open-source libraries;  Software engineering practices;  Versioning;  Vulnerability remediations, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hashimoto2018598,
author={Hashimoto, M. and Mori, A. and Izumida, T.},
title={Automated patch extraction via syntax- and semantics-aware delta debugging on source code changes},
journal={ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year={2018},
pages={598-609},
doi={10.1145/3236024.3236047},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058312685&doi=10.1145%2f3236024.3236047&partnerID=40&md5=65094eb3a1979539d3e2ddab3304133b},
abstract={Delta debugging (DD) is an approach to automating debugging activities based on systematic testing. DD algorithms find the cause of a regression of a program by minimizing changes between a working version and a faulty version of the program. However, it is still an open problem to minimize a huge set of changes while avoiding any invalid subsets that do not result in testable programs, especially in case that no software configuration management system is available. In this paper, we propose a rule-based approach to syntactic and semantic decomposition of changes into independent components to facilitate DD on source code changes, and hence to extract patches automatically. For analyzing changes, we make use of tree differencing on abstract syntax trees instead of common differencing on plain texts. We have developed an experimental implementation for Java programs and applied it to 194 bug fixes from Defects4J and 8 real-life regression bugs from 6 open source Java projects. Compared to a DD tool based on plain text differencing, it extracted patches whose size is reduced by 50% at the cost of 5% more test executions for the former dataset and by 73% at the cost of 40% more test executions for the latter, both on average. © 2018 Association for Computing Machinery.},
keywords={Abstracting;  Java programming language;  Open source software;  Regression analysis;  Semantics;  Statistical tests;  Syntactics;  Trees (mathematics), Abstract Syntax Trees;  Delta debugging;  Independent components;  Rule-based approach;  Software configuration management systems;  Source code changes;  Systematic testing;  tree differencing, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu20181487,
author={Liu, H. and Li, B. and Yang, Y. and Ma, W. and Jia, R.},
title={Exploring the Impact of Code Smells on Fine-Grained Structural Change-Proneness},
journal={International Journal of Software Engineering and Knowledge Engineering},
year={2018},
volume={28},
number={10},
pages={1487-1516},
doi={10.1142/S0218194018500432},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054031457&doi=10.1142%2fS0218194018500432&partnerID=40&md5=577bccdc905f34ed6bd1ac86acf7bd2a},
abstract={Code smells are used to describe the bad structures in the source code, which could hinder software maintainability, understandability and changeability. Nowadays, scholars mainly focus on the impact of smell on textual change-proneness. However, in comparison to textual changes, structural changes could better reveal the change nature. In practice, not all code change types are equally important in terms of change risk severity levels, and software developers are more interested in particular changes relevant to their current tasks. Therefore, we investigate the relationship between smells and fine-grained structural change-proneness to solve these issues. Our experiment was conducted on 11 typical open source projects. We first employed Fishers exact test and Mann-Whitney test to explore whether smelly files (affected by at least one smell type) had higher structural change-proneness than other files, and whether files with more smell instances are more likely to undergo structural changes, respectively. Multivariate logistic regression model was built to study the relation between each kind of smell and change-proneness with respect to five change categories. Our results showed that: (1) in most cases, smelly files were more prone to structural changes and files with more smell instances tend to undergo higher structural changes; (2) quite a few smell types were related to structural change-proneness, particularly, Refused Parent Bequest (RPB), Message Chains (MCH), Divergent Change (DIVC), Feature Envy (FE) and Shotgun Surgery (SS) increased structural changes for some change categories. However, when controlling the file size Lines of Code (LOC), significant change-proneness of some smells disappeared or the magnitude of significance decreased more or less. © 2018 World Scientific Publishing Company.},
keywords={Odors;  Open source software;  Regression analysis, Change proneness;  Code changes;  Code smell;  Fine-grained changes;  Multivariate logistic regressions;  Open source projects;  Software developer;  Software maintainability, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tufano2018832,
author={Tufano, M. and Penta, M.D. and Watson, C. and White, M. and Bavota, G. and Poshyvanyk, D.},
title={An empirical investigation into learning bug-fixing patches in the wild via neural machine translation},
journal={ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
year={2018},
pages={832-837},
doi={10.1145/3238147.3240732},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056518629&doi=10.1145%2f3238147.3240732&partnerID=40&md5=23d977d10b5427473000d22d9c921ff9},
abstract={Millions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. We mine millions of bug-fixes from the change histories of GitHub repositories to extract meaningful examples of such bug-fixes. Then, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. Our model is able to fix hundreds of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9% of the cases. © 2018 Association for Computing Machinery.},
keywords={Codes (symbols);  Computational linguistics;  Computer aided language translation;  Open source software;  Program debugging;  Software design, Bug fixes;  Change history;  Development history;  Empirical investigation;  Empirical studies;  Encoder-decoder;  Machine translations;  Open source projects, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Huang2018679,
author={Huang, K. and Zhou, D. and Chen, B. and Wang, Y. and Zhao, W. and Peng, X. and Liu, Y.},
title={Cldiff: Generating concise linked code differences},
journal={ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
year={2018},
pages={679-690},
doi={10.1145/3238147.3238219},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056521780&doi=10.1145%2f3238147.3238219&partnerID=40&md5=adbe44e4f4489bfda3a269e289f2618f},
abstract={Analyzing and understanding source code changes is important in a variety of software maintenance tasks. To this end, many code differencing and code change summarization methods have been proposed. For some tasks (e.g. code review and software merging), however, those differencing methods generate too fine-grained a representation of code changes, and those summarization methods generate too coarse-grained a representation of code changes. Moreover, they do not consider the relationships among code changes. Therefore, the generated differences or summaries make it not easy to analyze and understand code changes in some software maintenance tasks. In this paper, we propose a code differencing approach, named ClDiff, to generate concise linked code differences whose granularity is in between the existing code differencing and code change summarization methods. The goal of ClDiff is to generate more easily understandable code differences. ClDiff takes source code files before and after changes as inputs, and consists of three steps. First, it pre-processes the source code files by pruning unchanged declarations from the parsed abstract syntax trees. Second, it generates concise code differences by grouping fine-grained code differences at or above the statement level and describing high-level changes in each group. Third, it links the related concise code differences according to five pre-defined links. Experiments with 12 Java projects (74,387 commits) and a human study with 10 participants have indicated the accuracy, conciseness, performance and usefulness of ClDiff. © 2018 Association for Computing Machinery.},
keywords={Abstracting;  Computer programming languages;  Computer software maintenance;  Software engineering;  Trees (mathematics), Abstract Syntax Trees;  Coarse-grained;  Code differencing;  Program comprehension;  Software merging;  Software-maintenance tasks;  Source code changes;  Source codes, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Niu20181887,
author={Niu, X. and Li, S. and Jia, Z. and Zhou, S. and Li, W. and Liao, X.},
title={Understanding the similarity of log revision behaviors in open source software},
journal={International Journal of Performability Engineering},
year={2018},
volume={14},
number={8},
pages={1887-1895},
doi={10.23940/ijpe.18.08.p27.18871895},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054725435&doi=10.23940%2fijpe.18.08.p27.18871895&partnerID=40&md5=c13d5d5c8e13deefefd3135f685095a6},
abstract={As logging code evolves with bug fixes and feature updates, developers may miss some log revisions due to a lack of general specifications and attention from developers. This makes it more troublesome to achieve good logging practices. In this paper, we try to study log revision behaviors from evolutionary history. Motivated by similar edits of clone codes, we assume there also exist similar log revisions that implicated log revision behaviors. Based on this assumption, we study the similarity of log revision behaviors and answer six research questions. Specifically, we find that 54.14% of log revisions belong to groups of similar log revisions and 64.4% of groups contain log revisions that are missed by developers. We stress the importance of branch statements on learning from similar log revisions since 53.51% of sampled similar log revisions are related to the semantics of branch statements. © 2018 Totem Publisher, Inc. All rights reserved.},
keywords={Open systems;  Semantics, Bug fixes;  Evolutionary history;  Failure diagnose;  General specification;  Log revision;  Research questions;  Software Evolution, Open source software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Accioly20182051,
author={Accioly, P. and Borba, P. and Cavalcanti, G.},
title={Understanding semi-structured merge conflict characteristics in open-source Java projects},
journal={Empirical Software Engineering},
year={2018},
volume={23},
number={4},
pages={2051-2085},
doi={10.1007/s10664-017-9586-1},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038628515&doi=10.1007%2fs10664-017-9586-1&partnerID=40&md5=0ff6c8798c2b0b941587337bc37fa534},
abstract={Empirical studies show that merge conflicts frequently occur, impairing developers’ productivity, since merging conflicting contributions might be a demanding and tedious task. However, the structure of changes that lead to conflicts has not been studied yet. Understanding the underlying structure of conflicts, and the involved syntactic language elements might shed light on how to better avoid merge conflicts. To this end, in this paper we derive a catalog of conflict patterns expressed in terms of the structure of code changes that lead to merge conflicts. We focus on conflicts reported by a semistructured merge tool that exploits knowledge about the underlying syntax of the artifacts. This way, we avoid analyzing a large number of spurious conflicts often reported by typical line based merge tools. To assess the occurrence of such patterns in different systems, we conduct an empirical study reproducing 70,047 merges from 123 GitHub Java projects. Our results show that most semistructured merge conflicts in our sample happen because developers independently edit the same or consecutive lines of the same method. However, the probability of creating a merge conflict is approximately the same when editing methods, class fields, and modifier lists. Furthermore, we noticed that most part of conflicting merge scenarios, and merge conflicts, involve more than two developers. Also, that copying and pasting pieces of code, or even entire files, across different repositories is a common practice and cause of conflicts. Finally, we discuss how our results reveal the need for new research studies and suggest potential improvements to tools supporting collaborative software development. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.},
keywords={Groupware;  Java programming language;  Open source software;  Software design;  Syntactics, Awareness tool;  Collaborative software development;  Common practices;  Empirical Software Engineering;  Empirical studies;  Research studies;  Semi-structured;  Syntactic languages, Mergers and acquisitions},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jiang2018298,
author={Jiang, J. and Xiong, Y. and Zhang, H. and Gao, Q. and Chen, X.},
title={Shaping program repair space with existing patches and similar code},
journal={ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2018},
pages={298-309},
doi={10.1145/3213846.3213871},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051477803&doi=10.1145%2f3213846.3213871&partnerID=40&md5=eff0c7ae75fe5569b9e1190f5f2993a7},
abstract={Automated program repair (APR) has great potential to reduce bugfixing effort and many approaches have been proposed in recent years. APRs are often treated as a search problem where the search space consists of all the possible patches and the goal is to identify the correct patch in the space. Many techniques take a data-driven approach and analyze data sources such as existing patches and similar source code to help identify the correct patch. However, while existing patches and similar code provide complementary information, existing techniques analyze only a single source and cannot be easily extended to analyze both. In this paper, we propose a novel automatic program repair approach that utilizes both existing patches and similar code. Our approach mines an abstract search space from existing patches and obtains a concrete search space by differencing with similar code snippets. Then we search within the intersection of the two search spaces.We have implemented our approach as a tool called SimFix, and evaluated it on the Defects4J benchmark. Our tool successfully fixed 34 bugs. To our best knowledge, this is the largest number of bugs fixed by a single technology on the Defects4J benchmark. Furthermore, as far as we know, 13 bugs fixed by our approach have never been fixed by the current approaches. © 2018 Association for Computing Machinery.},
keywords={Codes (symbols);  Defects;  Repair;  Software testing, Automatic programs;  Code adaptation;  Code differencing;  Data-driven approach;  Data-sources;  Search problem;  Search spaces;  Single source, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Murphy-Hill201814,
author={Murphy-Hill, E. and Sadowski, C. and Head, A. and Daughtry, J. and Macvean, A. and Jaspan, C. and Winter, C.},
title={Discovering API usability problems at scale},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={14-17},
doi={10.1145/3194793.3194795},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051867089&doi=10.1145%2f3194793.3194795&partnerID=40&md5=138e1a96c0b6a68d50768026352fcfb1},
abstract={Software developers' productivity can be negatively impacted by using APIs incorrectly. In this paper, we describe an analysis technique we designed to find API usability problems by comparing successive file-level changes made by individual software developers. We applied our tool, StopMotion, to the file histories of real developers doing real tasks at Google. The results reveal several API usability challenges including simple typos, conceptual API misalignments, and conflation of similar APIs. © 2018 Copyright held by the owner/author(s).},
keywords={Application programming interfaces (API), Analysis techniques;  File levels;  Software developer;  Usability problems, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeLaTorre2018492,
author={De La Torre, G. and Robbes, R. and Bergel, A.},
title={Imprecisions diagnostic in source code deltas},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={492-502},
doi={10.1145/3196398.3196404},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051659811&doi=10.1145%2f3196398.3196404&partnerID=40&md5=6134dc01de9588886846ec348c75840c},
abstract={Beyond a practical use in code review, source code change detection (SCCD) is an important component of many mining software repositories (MSR) approaches. As such, any error or imprecision in the detection may result in a wrong conclusion while mining repositories. We identified, analyzed, and characterized impressions in GumTree, which is the most advanced algorithm for SCCD. After analyzing its detection accuracy over a curated corpus of 107 C# projects, we diagnosed several imprecisions. Many of our findings confirm that a more language-aware perspective of GumTree can be helpful in reporting more precise changes. © 2018 ACM.},
keywords={Codes (symbols);  Computer programming languages;  Image quality, Detection accuracy;  differencing;  gumtree;  Mining repositories;  Mining software repository (MSR);  Practical use;  Source code changes;  Source codes, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zampetti2018526,
author={Zampetti, F. and Serebrenik, A. and Di Penta, M.},
title={Was self-admitted technical debt removal a real removal?: An in-depth perspective},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={526-536},
doi={10.1145/3196398.3196423},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051635603&doi=10.1145%2f3196398.3196423&partnerID=40&md5=55574cd1b42668435b7df6ea09d91098},
abstract={Technical Debt (TD) has been defined as "code being not quite right yet", and its presence is often self-admitted by developers through comments. The purpose of such comments is to keep track of TD and appropriately address it when possible. Building on a previous quantitative investigation by Maldonado et al. on the removal of self-admitted technical debt (SATD), in this paper we perform an in-depth quantitative and qualitative study of how SATD is addressed in five Java open source projects. On the one hand, we look at whether SATD is "accidentally" removed, and the extent to which the SATD removal is being documented. We found that that (i) between 20% and 50% of SATD comments are accidentally removed while entire classes or methods are dropped, (ii) 8% of the SATD removal is acknowledged in commit messages, and (iii) while most of the changes addressing SATD require complex source code changes, very often SATD is addressed by specific changes to method calls or conditionals. Our results can be used to better plan TD management or learn patterns for addressing certain kinds of TD and provide recommendations to developers. © 2018 ACM.},
keywords={Computer software, Complex sources;  Keep track of;  Open source projects;  Qualitative study;  Quantitative investigation;  Technical debts, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2018178,
author={Li, S. and Niu, X. and Jia, Z. and Wang, J. and He, H. and Wang, T.},
title={Logtracker: Learning log revision behaviors proactively from software evolution history},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={178-188},
doi={10.1145/3196321.3196328},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051637000&doi=10.1145%2f3196321.3196328&partnerID=40&md5=d0215d35ac181445405a174136daefba},
abstract={Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted. © 2018 ACM.},
keywords={Codes (symbols);  Computer programming, Automatic tools;  Design and implements;  Empirical studies;  Failure diagnose;  log revision;  Open source projects;  Software Evolution;  Systematic process, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hassan20181078,
author={Hassan, F. and Wang, X.},
title={HireBuild: An automatic approach to history-driven repair of build scripts},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={1078-1089},
doi={10.1145/3180155.3180181},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044602664&doi=10.1145%2f3180155.3180181&partnerID=40&md5=a134c928acf61339a13d30de789ea534},
abstract={Advancements in software build tools such as Maven reduce build management effort, but developers still need specialized knowledge and long time to maintain build scripts and resolve build failures. More recent build tools such as Gradle give developers greater extent of customization flexibility, but can be even more difficult to maintain. According to the TravisTorrent dataset of open-source software continuous integration, 22% of code commits include changes in build script files to maintain build scripts or to resolve build failures. Automated program repair techniques have great potential to reduce cost of resolving software failures, but the existing techniques mostly focus on repairing source code so that they cannot directly help resolving software build failures. To address this limitation, we propose HireBuild: <u>Hi</u>story-Driven <u>Rep</u>air of <u>Build</u> Scripts, the first approach to automatic patch generation for build scripts, using fix patterns automatically generated from existing build script fixes and recommending fix patterns based on build log similarity. From TravisTorrent dataset, we extracted 175 build failures and their corresponding fixes which revise Gradle build scripts. Among these 175 build failures, we used the 135 earlier build fixes for automatic fix-pattern generation and the more recent 40 build failures (fixes) for evaluation of our approach. Our experiment shows that our approach can fix 11 of 24 reproducible build failures, or 45% of the reproducible build failures, within comparable time of manual fixes. © 2018 ACM.},
keywords={Costs;  Open systems;  Repair;  Safety engineering, Automatic approaches;  Automatically generated;  Build logs;  Continuous integrations;  Management efforts;  Patch generation;  Pattern Generation;  Specialized knowledge, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Macho2018106,
author={Macho, C. and McIntosh, S. and Pinzger, M.},
title={Automatically repairing dependency-related build breakage},
journal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
year={2018},
volume={2018-March},
pages={106-117},
doi={10.1109/SANER.2018.8330201},
art_number={8330201},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049722884&doi=10.1109%2fSANER.2018.8330201&partnerID=40&md5=6b1cc80af70b9364d68283821d7dcfbd},
abstract={Build systems are widely used in today's software projects to automate integration and build processes. Similar to source code, build specifications need to be maintained to avoid outdated specifications, and build breakage as a consequence. Recent work indicates that neglected build maintenance is one of the most frequently occurring reasons why open source and proprietary builds break. In this paper, we propose BuildMedic, an approach to automatically repair Maven builds that break due to dependency-related issues. Based on a manual investigation of 37 broken Maven builds in 23 open source Java projects, we derive three repair strategies to automatically repair the build, namely Version Update, Delete Dependency, and Add Repository. We evaluate the three strategies on 84 additional broken builds from the 23 studied projects in order to demonstrate the applicability of our approach. The evaluation shows that BuildMedic can automatically repair 45 of these broken builds (54%). Furthermore, in 36% of the successfully repaired build breakages, BuildMedic outputs at least one repair candidate that is considered a correct repair. Moreover, 76% of them could be repaired with only a single dependency correction. © 2018 IEEE.},
keywords={Coal breakage;  Reengineering;  Repair;  Specifications, Build systems;  Open sources;  Repair strategy;  Software project;  Source codes, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Soto2018221,
author={Soto, M. and Le Goues, C.},
title={Using a probabilistic model to predict bug fixes},
journal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
year={2018},
volume={2018-March},
pages={221-231},
doi={10.1109/SANER.2018.8330211},
art_number={8330211},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049727696&doi=10.1109%2fSANER.2018.8330211&partnerID=40&md5=2ba4b267b83561bb55d53ba7024a0481},
abstract={Automatic Software Repair (APR) has significant potential to reduce software maintenance costs by reducing the human effort required to localize and fix bugs. State-of-the-art generate-and-validate APR techniques select between and instantiate various mutation operators to construct candidate patches, informed largely by heuristic probability distributions. This may reduce effectiveness in terms of both efficiency and output quality. In practice, human developers have many options in terms of how to edit code to fix bugs, some of which are far more common than others (e.g., deleting a line of code is more common than adding a new class). We mined the most recent 100 bug-fixing commits from each of the 500 most popular Java projects in GitHub (the largest dataset to date) to create a probabilistic model describing edit distributions. We categorize, compare and evaluate the different mutation operators used in state-of-the-art approaches. We find that a probabilistic modelbased APR approach patches bugs more quickly in the majority of bugs studied, and that the resulting patches are of higher quality than those produced by previous approaches. Finally, we mine association rules for multi-edit source code changes, an understudied but important problem. We validate the association rules by analyzing how much of our corpus can be built from them. Our evaluation indicates that 84.6% of the multi-edit patches from the corpus can be built from the association rules, while maintaining 90% confidence. © 2018 IEEE.},
keywords={Association rules;  Codes (symbols);  Program debugging;  Reengineering;  Repair, Mine association rules;  Mutation operators;  Probabilistic modeling;  Software maintenance costs;  Software repair;  Source code changes;  State of the art;  State-of-the-art approach, Probability distributions},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2018118,
author={Liu, X. and Zhong, H.},
title={Mining stackoverflow for program repair},
journal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
year={2018},
volume={2018-March},
pages={118-129},
doi={10.1109/SANER.2018.8330202},
art_number={8330202},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044591182&doi=10.1109%2fSANER.2018.8330202&partnerID=40&md5=7754c6864ba2ccc911d697af4efe0457},
abstract={In recent years, automatic program repair has been a hot research topic in the software engineering community, and many approaches have been proposed. Although these approaches produce promising results, some researchers criticize that existing approaches are still limited in their repair capability, due to their limited repair templates. Indeed, it is quite difficult to design effective repair templates. An award-wining paper analyzes thousands of manual bug fixes, but summarizes only ten repair templates. Although more bugs are thus repaired, recent studies show such repair templates are still insufficient. We notice that programmers often refer to Stack Overflow, when they repair bugs. With years of accumulation, Stack Overflow has millions of posts that are potentially useful to repair many bugs. The observation motives our work towards mining repair templates from Stack Overflow. In this paper, we propose a novel approach, called SOFix, that extracts code samples from Stack Overflow, and mines repair patterns from extracted code samples. Based on our mined repair patterns, we derived 13 repair templates. We implemented these repair templates in SOFix, and conducted evaluations on the widely used benchmark, Defects4J. Our results show that SOFix repaired 23 bugs, which are more than existing approaches. After comparing repaired bugs and templates, we find that SOFix repaired more bugs, since it has more repair templates. In addition, our results also reveal the urgent need for better fault localization techniques. © 2018 IEEE.},
keywords={Reengineering;  Software engineering, Automatic programs;  Bug fixes;  Engineering community;  Fault localization;  Hot research topics;  Limited repairs;  Stack overflow, Repair},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Song2018112,
author={Song, M. and Tilevich, E.},
title={Systematic adaptation of dynamically generated source code via domain-specific examples},
journal={IET Software},
year={2018},
volume={12},
number={2},
pages={112-119},
doi={10.1049/iet-sen.2016.0211},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045114635&doi=10.1049%2fiet-sen.2016.0211&partnerID=40&md5=00823ac3428dc19bd8e154d3d7570340},
abstract={In modern web-based applications, an increasing amount of source code is generated dynamically at runtime. Web applications commonly execute dynamically generated code (DGC) emitted by third-party, black-box generators, run at remote sites. Web developers often need to adapt DGC before it can be executed: embedded HTML can be vulnerable to cross-site scripting attacks; an API may be incompatible with some browsers; and the program's state created by DGC may not be persisting. Lacking any systematic approaches for adapting DGC, web developers resort to ad-hoc techniques that are unsafe and error-prone. This study presents an approach for adapting DGC systematically that follows the program-transformation-by-example paradigm. The proposed approach provides predefined, domain-specific before/after examples that capture the variability of commonly used adaptations. By approving or rejecting these examples, web developers determine the required adaptation transformations, which are encoded in an adaptation script operating on the generated code's abstract syntax tree. The proposed approach is a suite of practical JavaScript program adaptations and their corresponding before/after examples. The authors have successfully applied the approach to real web applications to adapt third-party generated JavaScript code for security, browser compatibility, and persistence. © The Institution of Engineering and Technology 2017.},
keywords={High level languages;  Trees (mathematics), Abstract Syntax Trees;  Ad-hoc techniques;  Cross Site Scripting Attacks;  Domain specific;  JavaScript programs;  Program transformations;  WEB application;  Web-based applications, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2018182,
author={Li, Y. and Zhu, C. and Rubin, J. and Chechik, M.},
title={Semantic Slicing of Software Version Histories},
journal={IEEE Transactions on Software Engineering},
year={2018},
volume={44},
number={2},
pages={182-201},
doi={10.1109/TSE.2017.2664824},
art_number={7843626},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042129677&doi=10.1109%2fTSE.2017.2664824&partnerID=40&md5=13645d66ce31b78e41975fade3fa7f89},
abstract={Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of the change history, 'inheriting' additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and propose techniques to minimize the produced slice. We then instantiate the overall approach, CSlicer, in a specific implementation for Java projects managed in Git and evaluate its correctness and effectiveness on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones. © 1976-2012 IEEE.},
keywords={Computer software;  Open systems;  Semantics;  Software engineering, Automated support;  Configuration management systems;  Configuration management tools;  dependency;  Program analysis;  Software change;  Software developer;  Version control, Open source software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Benni2018164,
author={Benni, B. and Mosser, S. and Moha, N. and Riveill, M.},
title={A Delta-Oriented Approach to Support the Safe Reuse of Black-Box Code Rewriters},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10826 LNCS},
pages={164-180},
doi={10.1007/978-3-319-90421-4_11},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047130758&doi=10.1007%2f978-3-319-90421-4_11&partnerID=40&md5=25a4f8703dd96be4566d0e358c9ca946},
abstract={The tedious process of corrective and perfective maintenance is often automated thanks to rewriting rules using tools such as Spoon or Coccinelle. These tools consider rules as black-boxes, and compose multiple rules by giving the output of a given rewriting as input to the next one. It is up to the developer to identify the right order (if it exists) among all the different rules. In this paper, we define a formal model compatible with the black-box assumption that reifies the modifications (Δs) made by each rule. Leveraging these Δs, we propose a way to safely compose multiple rules when applied to the same program by (i) ensuring the isolated application of the different rules and (ii) yield unexpected behaviors that were silently ignored before. We assess this approach by applying rewriting rules used to fix anti-patterns existing in Android applications to external pieces of software available on GitHub. © 2018, Springer International Publishing AG, part of Springer Nature.},
keywords={Application programs, Android applications;  Anti-patterns;  Black boxes;  Formal model;  Perfective maintenance;  Rewriting rules, Computer software reusability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Somogyi2018657,
author={Somogyi, F.A. and Asztalos, M.},
title={Formal description and verification of a text-based model differencing and merging method},
journal={MODELSWARD 2018 - Proceedings of the 6th International Conference on Model-Driven Engineering and Software Development},
year={2018},
volume={2018-January},
pages={657-667},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052012829&partnerID=40&md5=ae301dfac590a6501939c4176d965d23},
abstract={Version control is an integral part of teamwork in software development. Differencing and merging key artifacts (i.e. source code) is a key feature in version control systems. The concept of version control can also be applied to model-driven methodologies. The models are usually differenced and merged in their graph-based form. However, if supported, we can also use the textual representation of the models during this process. Text-based model differencing and merging methods have some useful use cases, like supporting the persistence of the model, or having a fallback plan should the differencing algorithm fail. Using the textual notation to display and edit models is relatively rare, as the visual (graph-based) representation of the model is more common. However, many believe that using them both would be the ideal solution. In this paper, we present the formal description of a text-based model differencing and merging method from previous work. We also verify our algorithm based on this formal description. The focus of the verification is the soundness and completeness of the method. The long term goal of our research is to develop a modeling environment-independent algorithm. This could be used in version control systems that support textual representations. Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.},
keywords={Algorithms;  Control systems;  Graphic methods;  Information management;  Merging;  Verification, Differencing algorithm;  Formal Description;  Model-driven methodology;  Modeling environments;  Soundness and completeness;  Textual representation;  Version control;  Version control system, Software design},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bach2017307,
author={Bach, T. and Andrzejak, A. and Pannemans, R. and Lo, D.},
title={The Impact of Coverage on Bug Density in a Large Industrial Software Project},
journal={International Symposium on Empirical Software Engineering and Measurement},
year={2017},
volume={2017-November},
pages={307-313},
doi={10.1109/ESEM.2017.44},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042384798&doi=10.1109%2fESEM.2017.44&partnerID=40&md5=926786df8be5dda473f3fd8671108953},
abstract={Measuring quality of test suites is one of the major challenges of software testing. Code coverage identifies tested and untested parts of code and is frequently used to approximate test suite quality. Multiple previous studies have investigated the relationship between coverage ratio and test suite quality, without a clear consent in the results. In this work we study whether covered code contains a smaller number of future bugs than uncovered code (assuming appropriate scaling). If this correlation holds and bug density is lower in covered code, coverage can be regarded as a meaningful metric to estimate the adequacy of testing. To this end we analyse 16000 internal bug reports and bug-fixes of SAP HANA, a large industrial software project. We found that the above-mentioned relationship indeed holds, and is statistically significant. Contrary to most previous works our study uses real bugs and real bug-fixes. Furthermore, our data is derived from a complex and large industrial project. © 2017 IEEE.},
keywords={Codes (symbols);  Computer software selection and evaluation;  Software engineering, bug densitiy;  coverage;  Empirical research;  Industry project;  Real world projects;  Software Quality, Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hassan2017157,
author={Hassan, F. and Wang, X.},
title={Change-Aware Build Prediction Model for Stall Avoidance in Continuous Integration},
journal={International Symposium on Empirical Software Engineering and Measurement},
year={2017},
volume={2017-November},
pages={157-162},
doi={10.1109/ESEM.2017.23},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042388336&doi=10.1109%2fESEM.2017.23&partnerID=40&md5=9f5e3568a539f3c139f23b6463f35865},
abstract={Continuous Integration(CI) is a widely used development practice where developers integrate their work after submitting code changes at central repository. CI servers usually monitor central repository for code change submission and automatically build software with changed code, perform unit testing, integration testing and provide test summary report. If build or test fails developers fix those issues and submit the code changes. Continuous submission of code modification by developers and build latency time creates stalls at CI server build pipeline and hence developers have to wait long time to get build outcome. In this paper, we proposed build prediction model that uses TravisTorrent data set with build error log clustering and AST level code change modification data to predict whether a build will be successful or not without attempting actual build so that developer can get early build outcome result. With the proposed model we can predict build outcome with an average F-Measure over 87% on all three build systems (Ant, Maven, Gradle) under the cross-project prediction scenario. © 2017 IEEE.},
keywords={Codes (symbols);  Forecasting;  Integration;  Pipeline codes;  Software engineering;  Software testing, Build systems;  Code modifications;  Continuous integrations;  Continuous submission;  Development practices;  Latency time;  Outcome prediction;  Prediction model, Integration testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ahmed201758,
author={Ahmed, I. and Brindescu, C. and Mannan, U.A. and Jensen, C. and Sarma, A.},
title={An Empirical Examination of the Relationship between Code Smells and Merge Conflicts},
journal={International Symposium on Empirical Software Engineering and Measurement},
year={2017},
volume={2017-November},
pages={58-67},
doi={10.1109/ESEM.2017.12},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042390923&doi=10.1109%2fESEM.2017.12&partnerID=40&md5=9ab6c329033b57a845b80a8fde98a836},
abstract={Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both 'smelly' and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality. © 2017 IEEE.},
keywords={Learning systems;  Mergers and acquisitions;  Odors;  Semantics;  Software design;  Software engineering;  Trees (mathematics), Abstract Syntax Trees;  Code smell;  Conflict Resolution;  Day-to-day operations;  Development workflow;  Empirical analysis;  Empirical examination;  Undesirable effects, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Higo2017532,
author={Higo, Y. and Ohtani, A. and Kusumoto, S.},
title={Generating simpler AST edit scripts by considering copy-and-paste},
journal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
year={2017},
pages={532-542},
doi={10.1109/ASE.2017.8115664},
art_number={8115664},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041440051&doi=10.1109%2fASE.2017.8115664&partnerID=40&md5=2866cb368a355e130c63327b292583b3},
abstract={In software development, there are many situations in which developers need to understand given source code changes in detail. Until now, a variety of techniques have been proposed to support understanding source code changes. Tree-based differencing techniques are expected to have better understandability than text-based ones, which are widely used nowadays (e.g., diff in Unix). In this paper, we propose to consider copy-and-paste as a kind of editing action forming tree-based edit script, which is an editing sequence that transforms a tree to another one. Software developers often perform copy- and-paste when they are writing source code. Introducing copy- and-paste action into edit script contributes to not only making simpler (more easily understandable) edit scripts but also making edit scripts closer to developers' actual editing sequences. We conducted experiments on an open dataset. As a result, we confirmed that our technique made edit scripts shorter for 18% of the code changes with a little more computational time. For the other 82% code changes, our technique generated the same edit scripts as an existing technique. We also confirmed that our technique provided more helpful visualizations. © 2017 IEEE.},
keywords={Codes (symbols);  Computer programming languages;  Copying;  Software design, Code changes;  Computational time;  Copy-and-paste;  Software developer;  Source code changes;  Source codes;  Tree-based;  Understandability, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lesenich2017543,
author={Lesenich, O. and Apel, S. and Kastner, C. and Seibt, G. and Siegmund, J.},
title={Renaming and shifted code in structured merging: Looking ahead for precision and performance},
journal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
year={2017},
pages={543-553},
doi={10.1109/ASE.2017.8115665},
art_number={8115665},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041447888&doi=10.1109%2fASE.2017.8115665&partnerID=40&md5=0cd5074ea7a3c39e1528eee9dd91692f},
abstract={Diffing and merging of source-code artifacts is an essential task when integrating changes in software versions. While state-of-the-art line-based merge tools (e.g., git merge) are fast and independent of the programming language used, they have only a low precision. Recently, it has been shown that the precision of merging can be substantially improved by using a language-aware, structured approach that works on abstract syntax trees. But, precise structured merging is NP hard, especially, when considering the notoriously difficult scenarios of renamings and shifted code. To address these scenarios without compromising scalability, we propose a syntax-aware, heuristic optimization for structured merging that employs a lookahead mechanism during tree matching. The key idea is that renamings and shifted code are not arbitrarily distributed, but their occurrence follows patterns, which we address with a syntax-specific lookahead. Our experiments with 48 real-world open-source projects (4,878 merge scenarios with over 400 million lines of code) demonstrate that we can significantly improve matching precision in 28 percent of cases while maintaining performance. © 2017 IEEE.},
keywords={Codes (symbols);  Mergers and acquisitions;  Merging;  Open source software;  Software engineering;  Syntactics;  Trees (mathematics), Abstract Syntax Trees;  Heuristic optimization;  Lines of code;  Open source projects;  Software versions;  State of the art;  Structured approach;  Tree-matching, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yue2017422,
author={Yue, R. and Meng, N. and Wang, Q.},
title={A characterization study of repeated bug fixes},
journal={Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017},
year={2017},
pages={422-432},
doi={10.1109/ICSME.2017.16},
art_number={8094441},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040541904&doi=10.1109%2fICSME.2017.16&partnerID=40&md5=0698d0c5f15325f7b7e093bfbd3ccb93},
abstract={Programmers always fix bugs when maintaining software. Previous studies showed that developers apply repeated bug fixes-similar or identical code changes-to multiple locations. Based on the observation, researchers built tools to identify code locations in need of similar changes, or to suggest similar bug fixes to multiple code fragments. However, some fundamental research questions, such as what are the characteristics of repeated bug fixes, are still unexplored. In this paper, we present a comprehensive empirical study with 341,856 bug fixes from 3 open source projects to investigate repeated fixes in terms of their frequency, edit locations, and semantic meanings. Specifically, we sampled bug reports and retrieved the corresponding fixing patches in version history. Then we chopped patches into smaller fixes (edit fragments). Among all the fixes related to a bug, we identified repeated fixes using clone detection, and put a fix and its repeated ones into one repeated-fix group. With these groups, we characterized the edit locations, and investigated the common bug patterns as well as common fixes. Our study on Eclipse JDT, Mozilla Firefox, and LibreOffice shows that (1) 15-20% of bugs involved repeated fixes; (2) 73-92% of repeated-fix groups were applied purely to code clones; and (3) 39% of manually examined groups focused on bugs relevant to additions or deletions of whole if-structures. These results deepened our understanding of repeated fixes. They enabled us to assess the effectiveness of existing tools, and will further provide insights for future research directions in automatic software maintenance and program repair. © 2017 IEEE.},
keywords={Cloning;  Codes (symbols);  Computer software maintenance;  Costs;  Location;  Maintenance;  Open source software;  Repair;  Semantics, Characterization studies;  Clone detection;  Empirical studies;  Fundamental research;  Future research directions;  Mozilla firefox;  Multiple codes;  Open source projects, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Miraldo20172,
author={Miraldo, V.C. and Dagand, P.-É. and Swierstra, W.},
title={Type-directed diffing of structured data},
journal={TyDe 2017 - Proceedings of the 2nd ACM SIGPLAN International Workshop on Type-Driven Development, co-located with ICFP 2017},
year={2017},
pages={2-15},
doi={10.1145/3122975.3122976},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030543474&doi=10.1145%2f3122975.3122976&partnerID=40&md5=431a92f7811f6759305c3d6d46de9955},
abstract={The Unix diff utility that compares lines of text is used pervasively by version control systems. Yet certain changes to a program may be difficult to describe accurately in terms of modifications to individual lines of code. As a result, observing changes at such a fixed granularity may lead to unnecessary conflicts between different edits. This paper presents a generic representation for describing transformations between algebraic data types and a non-deterministic algorithm for computing such representations. These representations can be used to give a more accurate account of modifications made to algebraic data structures - and the abstract syntax trees of computer programs in particular - as opposed to only considering modifications between their textual representations.},
keywords={Abstracting;  Algebra;  Information management, Agda;  Datatype-generic programming;  Dependently typed programming;  Generic representation;  Nondeterministic algorithms;  Textual representation;  Version control;  Version control system, Trees (mathematics)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yi2017740,
author={Yi, J. and Ahmed, U.Z. and Karkare, A. and Tan, S.H. and Roychoudhury, A.},
title={A feasibility study of using automated program repair for introductory programming assignments},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={740-751},
doi={10.1145/3106237.3106262},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030782216&doi=10.1145%2f3106237.3106262&partnerID=40&md5=12bd9d8ae4955e88d48f136a1f811852},
abstract={Despite the fact that an intelligent tutoring system for programming (ITSP) has long attracted interest, its widespread use has been hindered by the difficulty of generating personalized feedback automatically. Meanwhile, automated program repair (APR) is an emerging new technology that automatically fixes software bugs, and it has been shown that APR can fix the bugs of large real-world software. In this paper, we study the feasibility of marrying an ITSP and APR. We perform our feasibility study with four stateof- the-art APR tools (GenProg, AE, Angelix, and Prophet), and 661 programs written by the students taking an introductory programming course. We found that when APR tools are used out of the box, only about 30% of the programs in our dataset are repaired. This low repair rate is largely due to the student programs often being significantly incorrect - in contrast, professional software for which APR was successfully applied typically fails only a small portion of tests. To bridge this gap, we adopt in APR a new repair policy akin to the hint generation policy employed in the existing ITSP. This new repair policy admits partial repairs that address part of failing tests, which results in 84% improvement of repair rate. We also performed a user study with 263 novice students and 37 graders, and identified an understudied problem; while the graders seem to gain benefits from repairs, novice students do not seem to know how to effectively make use of generated repairs as hints. © 2017 Association for Computing Machinery.},
keywords={Computer aided instruction;  Education computing;  Planning;  Repair;  Software engineering;  Software testing;  Students;  Teaching;  Technology transfer, Feasibility studies;  Intelligent tutoring system;  Introductory programming;  Introductory programming course;  Personalized feedback;  Professional software;  State of the art;  Student project, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Le2017593,
author={Le, X.-B.D. and Chu, D.-H. and Lo, D. and Le Goues, C. and Visser, W.},
title={S3: Syntax- and semantic-guided repair synthesis via programming by examples},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={593-604},
doi={10.1145/3106237.3106309},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030757304&doi=10.1145%2f3106237.3106309&partnerID=40&md5=abfce350040c37286a28f28f99798ee8},
abstract={A notable class of techniques for automatic program repair is known as semantics-based. Such techniques, e.g., Angelix, infer semantic specifications via symbolic execution, and then use program synthesis to construct new code that satisfies those inferred specifications. However, the obtained specifications are naturally incomplete, leaving the synthesis engine with a difficult task of synthesizing a general solution from a sparse space of many possible solutions that are consistent with the provided specifications but that do not necessarily generalize. We present S3, a new repair synthesis engine that leverages programming-by-examples methodology to synthesize high-quality bug repairs. The novelty in S3 that allows it to tackle the sparse search space to create more general repairs is three-fold: (1) A systematic way to customize and constrain the syntactic search space via a domain-specific language, (2) An efficient enumerationbased search strategy over the constrained search space, and (3) A number of ranking features based on measures of the syntactic and semantic distances between candidate solutions and the original buggy program.We compare S3's repair effectiveness with state-ofthe- art synthesis engines Angelix, Enumerative, and CVC4. S3 can successfully and correctly fix at least three times more bugs than the best baseline on datasets of 52 bugs in small programs, and 100 bugs in real-world large programs. © 2017 Association for Computing Machinery.},
keywords={Computer programming languages;  Engines;  Model checking;  Problem oriented languages;  Repair;  Semantics;  Software engineering;  Specifications;  Syntactics, Automatic programs;  Domain specific languages;  General solutions;  Programming by Example;  Repair effectiveness;  Search strategies;  Semantic specification;  Symbolic execution, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gyori2017318,
author={Gyori, A. and Lahiri, S.K. and Partush, N.},
title={Refining interprocedural change-impact analysis using equivalence relations},
journal={ISSTA 2017 - Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2017},
pages={318-328},
doi={10.1145/3092703.3092719},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026626027&doi=10.1145%2f3092703.3092719&partnerID=40&md5=dc94e8d9a6180fb355cd7ef37b8dc8ca},
abstract={Change-impact analysis (CIA) is the task of determining the set of program elements impacted by a program change. Precise CIA has great potential to avoid expensive testing and code reviews for (parts of) changes that are refactorings (semantics-preserving). However most statement-level CIA techniques suffer from imprecision as they do not incorporate the semantics of the change. We formalize change impact in terms of the trace semantics of two program versions. We show how to leverage equivalence relations to make dataflow-based CIA aware of the change semantics, thereby improving precision in the presence of semanticspreserving changes. We propose an anytime algorithm that applies costly equivalence-relation inference incrementally to refine the set of impacted statements. We implemented a prototype and evaluated it on 322 real-world changes from open-source projects and benchmark programs used by prior research. The evaluation results show an average 35% improvement in the number of impacted statements compared to prior datafiow-based techniques. © 2017 Association for Computing Machinery.},
keywords={Computer software maintenance;  Inference engines;  Open source software;  Semantics;  Set theory, Any-time algorithms;  Benchmark programs;  Change impact analysis;  Equivalence;  Equivalence relations;  Evaluation results;  Impact analysis;  Open source projects, Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Koyuncu2017237,
author={Koyuncu, A. and Bissyandé, T.F. and Kim, D. and Klein, J. and Monperrus, M. and Traon, Y.L.},
title={Impact of tool support in patch construction},
journal={ISSTA 2017 - Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2017},
pages={237-248},
doi={10.1145/3092703:3092713},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026625753&doi=10.1145%2f3092703%3a3092713&partnerID=40&md5=c5e665b8c6bf9d1a26a98e87ee2ae41f},
abstract={In this work, we investigate the practice of patch construction in the Linux kernel development, focusing on the differences between three patching processes: (1) patches crafted entirely manually to fix bugs, (2) those that are derived from warnings of bug detection tools, and (3) those that are automatically generated based on fix patterns. With this study, we provide to the research community concrete insights on the practice of patching as well as how the development community is currently embracing research and commercial patching tools to improve productivity in repair. The result of our study shows that tool-supported patches are increasingly adopted by the developer community while manually-written patches are accepted more quickly. Patch application tools enable developers to remain committed to contributing patches to the code base. Our findings also include that, in actual development processes, patches generally implement several change operations spread over the code, even for patches fixing warnings by bug detection tools. Finally, this study has shown that there is an opportunity to directly leverage the output of bug detection tools to readily generate patches that are appropriate for fixing the problem, and that are consistent with manually-written patches. © 2017 ACM.},
keywords={Automation;  Computer debugging;  Computer operating systems;  Inspection equipment;  Linux;  Open systems;  Productivity;  Repair;  Software testing;  Tools, Automatically generated;  Bug detection;  Change operations;  Development community;  Development process;  Empirical;  Patch;  Research communities, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tan2017180,
author={Tan, S.H. and Yi, J. and Yulis and Mechtaev, S. and Roychoudhury, A.},
title={Codeflaws: A programming competition benchmark for evaluating automated program repair tools},
journal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
year={2017},
pages={180-182},
doi={10.1109/ICSE-C.2017.76},
art_number={7965296},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026737226&doi=10.1109%2fICSE-C.2017.76&partnerID=40&md5=3b93ea710a3d0cae0fd27eb47a7438a8},
abstract={Several automated program repair techniques have been proposed to reduce the time and effort spent in bug-fixing. While these repair tools are designed to be generic such that they could address many software faults, different repair tools may fix certain types of faults more effectively than other tools. Therefore, it is important to compare more objectively the effectiveness of different repair tools on various fault types. However, existing benchmarks on automated program repairs do not allow thorough investigation of the relationship between fault types and the effectiveness of repair tools. We present Codeflaws, a set of 3902 defects from 7436 programs automatically classified across 39 defect classes (we refer to different types of fault as defect classes derived from the syntactic differences between a buggy program and a patched program). © 2017 IEEE.},
keywords={Automation;  Benchmarking;  Defects;  Repair;  Software engineering, Bug-fixing;  Defect class;  Empirical evaluations;  Fault types;  Repair techniques;  Repair tools;  Software fault, C (programming language)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cito2017323,
author={Cito, J. and Schermann, G. and Wittern, J.E. and Leitner, P. and Zumberi, S. and Gall, H.C.},
title={An Empirical Analysis of the Docker Container Ecosystem on GitHub},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2017},
pages={323-333},
doi={10.1109/MSR.2017.67},
art_number={7962382},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026503858&doi=10.1109%2fMSR.2017.67&partnerID=40&md5=128b493003480ea0b80920271873af97},
abstract={Docker allows packaging an application with its dependencies into a standardized, self-contained unit (a so-called container), which can be used for software development and to run the application on any system. Dockerfiles are declarative definitions of an environment that aim to enable reproducible builds of the container. They can often be found in source code repositories and enable the hosted software to come to life in its execution environment. We conduct an exploratory empirical study with the goal of characterizing the Docker ecosystem, prevalent quality issues, and the evolution of Dockerfiles. We base our study on a data set of over 70000 Dockerfiles, and contrast this general population with samplings that contain the Top-100 and Top-1000 most popular Docker-using projects. We find that most quality issues (28.6%) arise from missing version pinning (i.e., specifying a concrete version for dependencies). Further, we were not able to build 34% of Dockerfiles from a representative sample of 560 projects. Integrating quality checks, e.g., to issue version pinning warnings, into the container build process could result into more reproducible builds. The most popular projects change more often than the rest of the Docker population, with 5.81 revisions per year and 5 lines of code changed on average. Most changes deal with dependencies, that are currently stored in a rather unstructured manner. We propose to introduce an abstraction that, for instance, could deal with the intricacies of different package managers and could improve migration to more light-weight images. © 2017 IEEE.},
keywords={Application programs;  Containers;  Ecology;  Ecosystems;  Population statistics;  Software engineering, Docker;  Empirical analysis;  Empirical Software Engineering;  Execution environments;  General population;  GitHub;  Representative sample;  Source code repositories, Software design},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Macho2017368,
author={Macho, C. and McIntosh, S. and Pinzger, M.},
title={Extracting Build Changes with BUILDDIFF},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2017},
pages={368-378},
doi={10.1109/MSR.2017.65},
art_number={7962386},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026499815&doi=10.1109%2fMSR.2017.65&partnerID=40&md5=5b3ee73c53acc98957daf9a2d9cce1d5},
abstract={Build systems are an essential part of modern software engineering projects. As software projects change continuously, it is crucial to understand how the build system changes because neglecting its maintenance can lead to expensive build breakage. Recent studies have investigated the (co-)evolution of build configurations and reasons for build breakage, but they did this only on a coarse grained level. In this paper, we present BUILDDIFF, an approach to extract detailed build changes from MAVEN build files and classify them into 95 change types. In a manual evaluation of 400 build changing commits, we show that BUILDDIFF can extract and classify build changes with an average precision and recall of 0.96 and 0.98, respectively. We then present two studies using the build changes extracted from 30 open source Java projects to study the frequency and time of build changes. The results show that the top 10 most frequent change types account for 73% of the build changes. Among them, changes to version numbers and changes to dependencies of the projects occur most frequently. Furthermore, our results show that build changes occur frequently around releases. With these results, we provide the basis for further research, such as for analyzing the (co-)evolution of build files with other artifacts or improving effort estimation approaches. Furthermore, our detailed change information enables improvements of refactoring approaches for build configurations and improvements of models to identify error-prone build files. © 2017 IEEE.},
keywords={Computer software maintenance;  Computer software selection and evaluation;  Maintenance;  Software engineering, Build systems;  Coarse-grained;  Effort Estimation;  Open sources;  Precision and recall;  Software engineering projects;  Software project;  Software Quality, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Soetens2017,
author={Soetens, Q.D. and Robbes, R. and Demeyer, S.},
title={Changes as first-class citizens: A research perspective on modern software tooling},
journal={ACM Computing Surveys},
year={2017},
volume={50},
number={2},
doi={10.1145/3038926},
art_number={18},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017462023&doi=10.1145%2f3038926&partnerID=40&md5=726931a538fb97503e862ace4373c80f},
abstract={Software must evolve to keep up with an ever-changing context, the real world. We discuss an emergent trend in software evolution research revolving around the central notion that drives evolution: Change. By reifying change, and by modelling it as a first-class entity, researchers can now analyse the complex phenomenon known as software evolution with an unprecedented degree of accuracy. We present a Systematic Mapping Study of 86 articles to give an overview on the state of the art in this area of research and present a roadmap with open issues and future directions. © 2017 ACM.},
keywords={Computer science;  Surveys, Atomic changes;  Change distilling;  Change recording;  Fine grained;  Fine-grained changes;  Systematic mapping studies, Mapping},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Stevens2017171,
author={Stevens, R. and De Roover, C.},
title={Extracting executable transformations from distilled code changes},
journal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
year={2017},
pages={171-181},
doi={10.1109/SANER.2017.7884619},
art_number={7884619},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018416465&doi=10.1109%2fSANER.2017.7884619&partnerID=40&md5=ee0189b8cc4dd9b95dc6ef1fa7877eaa},
abstract={Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. Using examples, we demonstrate the expressiveness of specifying source code evolutions through intermediate ASTs. We also show that our approach is able to recall different implementation variants of the same source code evolution in open-source histories. © 2017 IEEE.},
keywords={Codes (symbols);  Computer programming languages;  Open source software;  Reengineering, Code changes;  Evolution patterns;  Fine-grained changes;  Intermediate state;  Mining software repositories;  Multiple changes;  Open sources;  Source codes, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Laverdiere2017240,
author={Laverdiere, M.-A. and Merlo, E.},
title={Computing counter-examples for privilege protection losses using security models},
journal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
year={2017},
pages={240-249},
doi={10.1109/SANER.2017.7884625},
art_number={7884625},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018403590&doi=10.1109%2fSANER.2017.7884625&partnerID=40&md5=29331547ad71090979a1729384fe111b},
abstract={Role-Based Access Control (RBAC) is commonly used in web applications to protect information and restrict operations. Code changes may affect the security of the application and need to be validated, in order to avoid security vulnerabilities, which is a major undertaking. A statement suffers from privilege protection loss in a release pair when it was definitely protected on all execution paths in the previous release and is now reachable by some execution paths with an inferior privilege protection. Because the code change and the resulting privilege protection loss may be distant (e.g. in different functions or files), developers may find it difficult to diagnose and correct the issue. We use Pattern Traversal Flow Analysis (PTFA) to statically analyze code-derived formal models. Our analysis automatically computes counter-examples of definite protection properties and privilege protection losses. We computed privilege protections and their changes for 147 release pairs of WordPress. We computed counter-examples for a total of 14,116 privilege protection losses we found spread in 31 release pairs.We present the distribution of counter-examples' lengths, as well as their spread across function and file boundaries. Our results show that counter-examples are typically short and localized. The median example spans 88 statements, crosses a single function boundary, and is contained in the same file. The 90th centile example measures 174 statements and spans 3 function boundaries over 3 files. We believe that the privilege protection counter-examples' characteristics would be helpful to focus developers' attention for security reviews. These counter-examples are also a first step toward explanations. © 2017 IEEE.},
keywords={Access control;  Codes (symbols);  Computer software maintenance;  Model checking;  Reengineering, Counter examples;  Evolution;  Execution paths;  Protect information;  Role-based Access Control;  Security model;  Security vulnerabilities;  WEB application, Static analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Charpentier2017235,
author={Charpentier, A. and Falleri, J.-R. and Morandat, F. and Ben Hadj Yahia, E. and Réveillère, L.},
title={Raters’ reliability in clone benchmarks construction},
journal={Empirical Software Engineering},
year={2017},
volume={22},
number={1},
pages={235-258},
doi={10.1007/s10664-015-9419-z},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957652388&doi=10.1007%2fs10664-015-9419-z&partnerID=40&md5=93b2f8d4cecdb2e421ce4d6080c577c0},
abstract={Cloned code often complicates code maintenance and evolution and must therefore be effectively detected. One of the biggest challenges for clone detectors is to reduce the amount of irrelevant clones they found, called false positives. Several benchmarks of true and false positive clones have been introduced, enabling tool developers to compare, assess and fine-tune their tools. Manual inspection of clone candidates is performed by raters that do not have expertise on the underlying code. This way of building benchmarks might be unreliable when considering context-dependent clones i.e., clones valid for a specific purpose. Our goal is to investigate the reliability of rater judgments about context-dependent clones. We randomly select about 600 clones from two projects and ask several raters, including experts of the projects, to manually classify these clones. We observe that judgments of non expert raters are not always repeatable. We also observe that they seldomly agree with each others and with the expert. Finally, we find that the project and the fact that a clone is a true or false positive might have an influence on the agreement between the expert and non experts. Therefore, using non experts to produce clone benchmarks could be unreliable. © 2016, Springer Science+Business Media New York.},
keywords={Codes (symbols), Code clone;  Context dependent;  Duplication;  Empirical studies;  Enabling tools;  False positive;  Manual inspection;  Software metrics, Cloning},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Thung2017222,
author={Thung, F. and Le, X.-B.D. and Lo, D. and Lawall, J.},
title={Recommending code changes for automatic backporting of linux device drivers},
journal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
year={2017},
pages={222-232},
doi={10.1109/ICSME.2016.71},
art_number={7816469},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013076920&doi=10.1109%2fICSME.2016.71&partnerID=40&md5=5a83fbffd772ae154448e44782abd216},
abstract={Device drivers are essential components of any operating system (OS). They specify the communication protocol that allows the OS to interact with a device. However, drivers for new devices are usually created for a specific OS version. These drivers often need to be backported to the older versions to allow use of the new device. Backporting is often done manually, and is tedious and error prone. To alleviate this burden on developers, we propose an automatic recommendation system to guide the selection of backporting changes. Our approach analyzes the version history for cues to recommend candidate changes. We have performed an experiment on 100 Linux driver files and have shown that we can give a recommendation containing the correct backport for 68 of the drivers. For these 68 cases, 73.5%, 85.3%, and 88.2% of the correct recommendations are located in the Top-1, Top-2, and Top-5 positions of the recommendation lists respectively. The successful cases cover various kinds of changes including change of record access, deletion of function argument, change of a function name, change of constant, and change of if condition. Manual investigation of failed cases highlights limitations of our approach, including inability to infer complex changes, and unavailability of relevant cues in version history. © 2016 IEEE.},
keywords={Computer operating systems;  Computer software maintenance;  Linux, Backporting;  Code changes;  Device Driver;  Error prones;  Linux drivers;  New devices, Recommender systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{VanEs201731,
author={Van Es, N. and Vandercammen, M. and De Roover, C.},
title={Incrementalizing Abstract Interpretation},
journal={CEUR Workshop Proceedings},
year={2017},
volume={2047},
pages={31-35},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041675860&partnerID=40&md5=4fb618ed6a7de67f4bf7b275934aee9b},
abstract={A powerful approach to design static analysers is abstract interpretation, which reasons over an approximation of the program’s behaviour. The Abstracting Abstract Machines (AAM) technique, introduced by Van Horn & Might, presents a systematic approach to derive abstract interpreters. However, it is often not useable in the development and evolution of large-scale applications, as with the current state-of-the-art, an AAM analysis can not incrementally update its results upon changes in the program. That is, whenever minor modifications occur in the program’s source code, one needs to recompute the entire AAM analysis from scratch, which can easily get time-consuming. We therefore propose an incremental approach to abstract interpretation, more precisely the AAM technique. That is, we modify the technique so that the result of an AAM analysis, the abstract state graph, can incrementally be updated following a change in the program’s source code. Our algorithm tracks dependencies between the nodes in the abstract syntax tree (AST) of the program and the transitions in the abstract state graph to invalidate and recompute new transitions in the state graph upon a change in the AST. Our experiments using a set of Scheme micro-benchmarks reveal that in practice this approach is often limited, as only states that are identical in both state graphs are reusable. We therefore introduce an improvement to the original incremental algorithm, which we refer to as state adaptation. State adaptation also enables reusing states that are not identical, but similar. Both the original and improved algorithm are integrated and evaluated in the Scala-AM framework. Our current implementations already show good results in terms of incremental efficiency, although more optimization is required to achieve actual gains in run time performance with our approaches.},
keywords={Abstracting;  Application programs;  Model checking;  Optimization;  Static analysis, Abstract interpretations;  Abstract machines;  Abstract Syntax Trees;  Incremental algorithm;  Incremental approach;  Incremental computation;  Large-scale applications;  Run-time performance, Trees (mathematics)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Oumaziz201712,
author={Oumaziz, M.A. and Charpentier, A. and Falleri, J.-R. and Blanc, X.},
title={Documentation reuse: Hot or not? an empirical study},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10221 LNCS},
pages={12-27},
doi={10.1007/978-3-319-56856-0_2},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019252348&doi=10.1007%2f978-3-319-56856-0_2&partnerID=40&md5=0ed9e9ec000756d7558cec282305ab5e},
abstract={Having available a high quality documentation is critical for software projects. This is why documentation tools such as Javadoc are so popular. As for code, documentation should be reused when possible to increase developer productivity and simplify maintenance. In this paper, we perform an empirical study of duplications in JavaDoc documentation on a corpus of seven famous Java APIs. Our results show that copy-pastes of JavaDoc documentation tags are abundant in our corpus. We also show that these copy-pastes are caused by four different kinds of relations in the underlying source code. In addition, we show that popular documentation tools do not provide any reuse mechanism to cope with these relations. Finally, we make a proposal for a simple but efficient automatic reuse mechanism. © Springer International Publishing AG 2017.},
keywords={Reusability;  System program documentation, Documentation tools;  Empirical studies;  High quality;  Javadoc;  Reuse;  Reuse mechanism;  Software project;  Source codes, Computer software reusability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ma2017229,
author={Ma, S. and Thung, F. and Lo, D. and Sun, C. and Deng, R.H.},
title={VuRLE: Automatic vulnerability detection and repair by learning from examples},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10493 LNCS},
pages={229-246},
doi={10.1007/978-3-319-66399-9_13},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029517506&doi=10.1007%2f978-3-319-66399-9_13&partnerID=40&md5=8b63e1a9313f5c42720fddd3bf5adfff},
abstract={Vulnerability becomes a major threat to the security of many systems. Attackers can steal private information and perform harmful actions by exploiting unpatched vulnerabilities. Vulnerabilities often remain undetected for a long time as they may not affect typical systems’ functionalities. Furthermore, it is often difficult for a developer to fix a vulnerability correctly if he/she is not a security expert. To assist developers to deal with multiple types of vulnerabilities, we propose a new tool, called VuRLE, for automatic detection and repair of vulnerabilities. VuRLE (1) learns transformative edits and their contexts (i.e., code characterizing edit locations) from examples of vulnerable codes and their corresponding repaired codes; (2) clusters similar transformative edits; (3) extracts edit patterns and context patterns to create several repair templates for each cluster. VuRLE uses the context patterns to detect vulnerabilities, and customizes the corresponding edit patterns to repair them. We evaluate VuRLE on 279 vulnerabilities from 48 real-world applications. Under 10-fold cross validation, we compare VuRLE with another automatic repair tool, LASE. Our experiment shows that VuRLE successfully detects 183 out of 279 vulnerabilities, and repairs 101 of them, while LASE can only detect 58 vulnerabilities and repair 21 of them. © 2017, Springer International Publishing AG.},
keywords={Codes (symbols);  Security of data;  Security systems, 10-fold cross-validation;  Automatic Detection;  Context patterns;  Learning from examples;  Private information;  Security experts;  Template generation;  Vulnerability detection, Repair},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Asenov2017152,
author={Asenov, D. and Guenat, B. and Müller, P. and Otth, M.},
title={Precise version control of trees with line-based version control systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10202 LNCS},
pages={152-169},
doi={10.1007/978-3-662-54494-5_9},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016440185&doi=10.1007%2f978-3-662-54494-5_9&partnerID=40&md5=e6f98cec4495d91b2edfcbc02c1b17f6},
abstract={Version control of tree structures, ubiquitous in software engineering, is typically performed on a textual encoding of the trees, rather than the trees directly. Applying standard line-based diff and merge algorithms to such encodings leads to inaccurate diffs, unnecessary conflicts, and incorrect merges. To address these problems, we propose novel algorithms for computing precise diffs between two versions of a tree and for three-way merging of trees. Unlike most other approaches for version control of structured data, our approach integrates with mainstream version control systems. Our merge algorithm can be customized for specific application domains to further improve merge results. An evaluation of our approach on abstract syntax trees from popular Java projects shows substantially improved merge results compared to Git. © Springer-Verlag GmbH Germany 2017.},
keywords={Computation theory;  Control systems;  Encoding (symbols);  Information management;  Software engineering;  Trees (mathematics), Abstract Syntax Trees;  Novel algorithm;  Software Evolution;  Structured data;  Structured editors;  Trees;  Version control;  Version control system, Computer control},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xuan201734,
author={Xuan, J. and Martinez, M. and DeMarco, F. and Clement, M. and Marcote, S.L. and Durieux, T. and Le Berre, D. and Monperrus, M.},
title={Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs},
journal={IEEE Transactions on Software Engineering},
year={2017},
volume={43},
number={1},
pages={34-55},
doi={10.1109/TSE.2016.2560811},
art_number={7463060},
note={cited By 43},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009892148&doi=10.1109%2fTSE.2016.2560811&partnerID=40&md5=af3ff1d95f717354a64369a6660f37b0},
abstract={We propose Nopol, an approach to automatic repair of buggy conditional statements (i.e., if-then-else statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of Nopol consists of three major phases. First, Nopol employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, Nopol encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem; then a feasible solution to the SMT instance is translated back into a code patch. We evaluate Nopol on 22 real-world bugs (16 bugs with buggy if conditions and six bugs with missing preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with buggy if conditions and missing preconditions. We illustrate the capabilities and limitations of Nopol using case studies of real bug fixes. © 1976-2012 IEEE.},
keywords={Computer software;  Java programming language;  Open source software;  Repair;  Software testing;  Surface mount technology, Conditional expressions;  Empirical analysis;  Fault localization;  Feasible solution;  Open source projects;  Oriented features;  patch generation;  Satisfiability modulo Theories, Program debugging},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li2016201,
author={Li, Z. and Zou, D. and Xu, S. and Jin, H. and Qi, H. and Hu, J.},
title={VulPecker: An automated vulnerability detection system based on code similarity analysis},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={5-9-December-2016},
pages={201-213},
doi={10.1145/2991079.2991102},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007566205&doi=10.1145%2f2991079.2991102&partnerID=40&md5=674029c8181f7ee6b08613f2826d7a0f},
abstract={Software vulnerabilities are the fundamental cause of many attacks. Even with rapid vulnerability patching, the problem is more complicated than it looks. One reason is that instances of the same vulnerability may exist in multiple software copies that are difficult to track in real life (e.g., different versions of libraries and applications). This calls for tools that can automatically search for vulnerable software with respect to a given vulnerability. In this paper, we move a step forward in this direction by presenting Vulnerability Pecker (VulPecker), a system for automatically detecting whether a piece of software source code contains a given vulnerability or not. The key insight underlying VulPecker is to leverage (i) a set of features that we define to characterize patches, and (ii) code-similarity algorithms that have been proposed for various purposes, while noting that no single code-similarity algorithm is effective for all kinds of vulnerabilities. Experiments show that VulPecker detects 40 vulnerabilities that are not published in the National Vulnerability Database (NVD). Among these vulnerabilities, 18 are not known for their existence and have yet to be confirmed by vendors at the time of writing (these vulnerabilities are "anonymized" in the present paper for ethical reasons), and the other 22 vulnerabilities have been "silently" patched by the vendors in the later releases of the vulnerable products. © 2016 ACM.},
keywords={Codes (symbols);  Computer software;  Security of data;  Security systems, Code similarities;  National vulnerability database;  Software source codes;  Software vulnerabilities;  Vulnerability detection;  Vulnerability signature, Application programs},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cazzola2016332,
author={Cazzola, W. and Jalili, M.},
title={Dodging Unsafe Update Points in Java Dynamic Software Updating Systems},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
year={2016},
pages={332-341},
doi={10.1109/ISSRE.2016.17},
art_number={7774532},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013309485&doi=10.1109%2fISSRE.2016.17&partnerID=40&md5=0c81b0442624355c1ea631ccf3123eab},
abstract={Dynamic Software Updating (DSU) provides mechanisms to update a program without stopping its execution. An indiscriminate update, that does not consider the current state of the computation, potentially undermines the stability of the running application. To automatically determine a safe moment when to update the running system is still an open problem often neglected from the existing DSU systems. This paper proposes a mechanism to support the choice of a safe update point by marking which point can be considered unsafe and therefore dodged during the update. The method is based on decorating the code with some specific meta-data that can be used to find the right moment to do the update. The proposed approach has been implemented as an external component that can be plugged into every DSU system. The approach is demonstrated on the evolution of the HSQLDB system from two distinct versions to their next update. © 2016 IEEE.},
keywords={Computer software;  Dynamics, Dynamic software update;  Dynamic software updating;  Dynamic update;  External components;  JavAdaptor;  Running applications;  Running systems, Software reliability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ahmed2016547,
author={Ahmed, I. and Gopinath, R. and Brindescu, C. and Groce, A. and Jensen, C.},
title={Can testedness be effectively measured?},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={547-558},
doi={10.1145/2950290.2950324},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997418602&doi=10.1145%2f2950290.2950324&partnerID=40&md5=dda5f661050c3d8d817424775170a2d3},
abstract={Among the major questions that a practicing tester faces are deciding where to focus additional testing effort, and decid-ing when to stop testing. Test the least-Tested code, and stop when all code is well-Tested, is a reasonable answer. Many measures of "testedness" have been proposed; unfortunately, we do not know whether these are truly effective. In this paper we propose a novel evaluation of two of the most important and widely-used measures of test suite qual-ity. The first measure is statement coverage, the simplest and best-known code coverage measure. The second mea-sure is mutation score, a supposedly more powerful, though expensive, measure. We evaluate these measures using the actual criteria of interest: if a program element is (by these measures) well tested at a given point in time, it should require fewer fu-ture bug-fixes than a "poorly tested" element. If not, then it seems likely that we are not effectively measuring tested-ness. Using a large number of open source Java programs from Github and Apache, we show that both statement cov-erage and mutation score have only a weak negative corre-lation with bug-fixes. Despite the lack of strong correlation, there are statistically and practically significant differences between program elements for various binary criteria. Pro-gram elements (other than classes) covered by any test case see about half as many bug-fixes as those not covered, and a similar line can be drawn for mutation score thresholds. Our results have important implications for both software engineering practice and research evaluation.},
keywords={Codes (symbols);  Computer software;  Java programming language;  Open source software;  Software engineering, Coverage criteria;  Mutation testing;  Program elements;  Research evaluation;  Software engineering practices;  Sta-Tistical analysis;  Statement coverage;  Strong correlation, Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nguyen2016511,
author={Nguyen, A.T. and Hilton, M. and Codoban, M. and Nguyen, H.A. and Mast, L. and Rademacher, E. and Nguyen, T.N. and Dig, D.},
title={API code recommendation using statistical learning from fine-grained changes},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={511-522},
doi={10.1145/2950290.2950333},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997525166&doi=10.1145%2f2950290.2950333&partnerID=40&md5=3288ad1884b2a99e3b94e5d624ac7441},
abstract={Learning and remembering how to use APIs is difficult. While codecompletion tools can recommend API methods, browsing a long list of API method names and their documentation is tedious. Moreover, users can easily be overwhelmed with too much information. We present a novel API recommendation approach that taps into the predictive power of repetitive code changes to provide relevant API recommendations for developers. Our approach and tool, APIREC, is based on statistical learning from fine-grained code changes and from the context in which those changes were made. Our empirical evaluation shows that APIREC correctly recommends an API call in the first position 59% of the time, and it recommends the correct API call in the top 5 positions 77% of the time. This is a significant improvement over the state-of-The-Art approaches by 30-160% for top-1 accuracy, and 10-30% for top-5 accuracy, respectively. Our result shows that APIREC performs well even with a one-Time, minimal training dataset of 50 publicly available projects. © 2016 ACM.},
keywords={Software engineering, API Recommendation;  Code changes;  Empirical evaluations;  Fine-grained changes;  Minimal training;  Predictive power;  State-of-the-art approach;  Statistical learning, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hanam2016144,
author={Hanam, Q. and Brito, F.S.D.M. and Mesbah, A.},
title={Discovering bug patterns in Javascript},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={144-156},
doi={10.1145/2950290.2950308},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997194354&doi=10.1145%2f2950290.2950308&partnerID=40&md5=62585e7f8af13c465525ad608bdb900b},
abstract={JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug findings tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-Automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using languageconstruct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript. © 2016 ACM.},
keywords={Artificial intelligence;  Data mining;  Learning systems;  Software engineering;  Static analysis, Bug Patterns;  Javascript;  Large-scale studies;  Multiple projects;  Node.Js;  Semi-automatics;  Tools and techniques;  Unsupervised machine learning, High level languages},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Achten2016,
author={Achten, P. and Stutterheim, J. and Lijnse, B. and Plasmeijer, R.},
title={Towards the layout of things},
journal={ACM International Conference Proceeding Series},
year={2016},
doi={10.1145/3064899.3064905},
art_number={3},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019167069&doi=10.1145%2f3064899.3064905&partnerID=40&md5=a55779ee514a1e8177a11bea684c0ff2},
abstract={When writing a user interface (UI), the layout of its elements play an important role. Programmers should be able to specify the layout of UIs in an intuitive way, while being able to separate the concern of laying out the UI from the rest of the software implementation. Ideally, the same layout language can be used in multiple application domains, so the programmer only has to learn one set of layout concepts. In this paper we introduce such a general-purpose layout language. We obtain this language by abstracting from a layout language we have introduced in previous work for declaratively defining Scalable Vector Graphics (SVG). We show that this abstract layout language can be instantiated for multiple domains: the SVG library by which the language is inspired, ncurses-based text-based user interfaces, and iTasks. In all of these cases, a separation of concerns is maintained. © 2016 ACM.},
keywords={Abstracting;  Graphical user interfaces;  User interfaces, Multiple applications;  Multiple domains;  Scalable vector graphics;  Separation of concerns;  Software implementation;  Task-oriented programming, Functional programming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dotzler2016660,
author={Dotzler, G. and Philippsen, M.},
title={Move-optimized source code tree differencing},
journal={ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year={2016},
pages={660-671},
doi={10.1145/2970276.2970315},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989170976&doi=10.1145%2f2970276.2970315&partnerID=40&md5=708e077a12335b0c8c20f6be72c81707},
abstract={When it is necessary to express changes between two source code files as a list of edit actions (an edit script), modern tree differencing algorithms are superior to most text-based approaches because they take code movements into account and express source code changes more accurately. We present 5 general optimizations that can be added to state-of-the-art tree differencing algorithms to shorten the resulting edit scripts. Applied to Gumtree, RTED, JSync, and ChangeDistiller, they lead to shorter scripts for 18{ 98% of the changes in the histories of 9 open-source software repositories. These optimizations also are parts of our novel Move-optimized Tree DIFFerencing algorithm (MTDIFF) that has a higher accuracy in detecting moved code parts. MTDIFF (which is based on the ideas of ChangeDistiller) further shortens the edit script for another 20% of the changes in the repositories. MTDIFF and all the benchmarks are available under an open-source license. © 2016 ACM.},
keywords={Codes (symbols);  Computer programming languages;  Forestry;  Open systems;  Optimization;  Software engineering;  Trees (mathematics), Differencing algorithm;  General optimizations;  Open source license;  Source code changes;  Source codes;  State of the art;  Text-based approach;  Tree Differencing, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xuan201665,
author={Xuan, J. and Cornu, B. and Martinez, M. and Baudry, B. and Seinturier, L. and Monperrus, M.},
title={B-Refactoring: Automatic test code refactoring to improve dynamic analysis},
journal={Information and Software Technology},
year={2016},
volume={76},
pages={65-80},
doi={10.1016/j.infsof.2016.04.016},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965136564&doi=10.1016%2fj.infsof.2016.04.016&partnerID=40&md5=7f80f2bb795e4b7ba2c1f6d6e0978c89},
abstract={Context: Developers design test suites to verify that software meets its expected behaviors. Many dynamic analysis techniques are performed on the exploitation of execution traces from test cases. In practice, one test case may imply various behaviors. However, the execution of a test case only yields one trace, which can hide the others. Objective: In this article, we propose a new technique of test code refactoring, called B-Refactoring. The idea behind B-Refactoring is to split a test case into small test fragments, which cover a simpler part of the control flow to provide better support for dynamic analysis. Method: For a given dynamic analysis technique, B-Refactoring monitors the execution of test cases and constructs small test cases without loss of the testability. We apply B-Refactoring to assist two existing analysis tasks: automatic repair of if-condition bugs and automatic analysis of exception contracts. Results: Experimental results show that B-Refactoring can effectively improve the execution traces of the test suite. Real-world bugs that could not be previously fixed with the original test suites are fixed after applying B-Refactoring; meanwhile, exception contracts are better verified via applying B-Refactoring to original test suites. Conclusions: We conclude that applying B-Refactoring improves the execution traces of test cases for dynamic analysis. This improvement can enhance existing dynamic analysis tasks. © 2016 Elsevier B.V. All rights reserved.},
keywords={Information systems;  Software engineering, Automatic analysis;  Control flows;  Design tests;  Dynamic analysis techniques;  Execution trace;  Real-world;  Refactorings;  Testability, Software testing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kreutzer201661,
author={Kreutzer, P. and Dotzler, G. and Ring, M. and Eskofier, B.M. and Philippsen, M.},
title={Automatic clustering of code changes},
journal={Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},
year={2016},
pages={61-72},
doi={10.1145/2901739.2901749},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974623425&doi=10.1145%2f2901739.2901749&partnerID=40&md5=5ceb185eb95befc365edaab3d81d6045},
abstract={Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes. To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes. We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes. © 2016 ACM.},
keywords={Codes (symbols);  Open source software;  Open systems, Automatic clustering;  Bug finding tools;  Clustering;  Code changes;  Open source projects;  Research tools;  Similarity metrics;  Software repositories, Clustering algorithms},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pawlik2016157,
author={Pawlik, M. and Augsten, N.},
title={Tree edit distance: Robust and memory-efficient},
journal={Information Systems},
year={2016},
volume={56},
pages={157-173},
doi={10.1016/j.is.2015.08.004},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945588965&doi=10.1016%2fj.is.2015.08.004&partnerID=40&md5=5860933103c2bf5e9ff8cd90b7348d4b},
abstract={Hierarchical data are often modelled as trees. An interesting query identifies pairs of similar trees. The standard approach to tree similarity is the tree edit distance, which has successfully been applied in a wide range of applications. In terms of runtime, the state-of-the-art algorithm for the tree edit distance is RTED, which is guaranteed to be fast independent of the tree shape. Unfortunately, this algorithm requires up to twice the memory of its competitors. The memory is quadratic in the tree size and is a bottleneck for the tree edit distance computation. In this paper we present a new, memory efficient algorithm for the tree edit distance, AP-TED (All Path Tree Edit Distance). Our algorithm runs at least as fast as RTED without trading in memory efficiency. This is achieved by releasing memory early during the first step of the algorithm, which computes a decomposition strategy for the actual distance computation. We show the correctness of our approach and prove an upper bound for the memory usage. The strategy computed by AP-TED is optimal in the class of all-path strategies, which subsumes the class of LRH strategies used in RTED. We further present the AP-TED+ algorithm, which requires less computational effort for very small subtrees and improves the runtime of the distance computation. Our experimental evaluation confirms the low memory requirements and the runtime efficiency of our approach. © 2015 Elsevier Ltd.},
keywords={Algorithms;  Efficiency;  Forestry, Approximate matching;  Computational effort;  Decomposition strategy;  Experimental evaluation;  Memory-efficient algorithms;  Similarity search;  State-of-the-art algorithms;  Tree edit distance, Trees (mathematics)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zimmerman2016283,
author={Zimmerman, K. and Rupakheti, C.R.},
title={An automated framework for recommending program elements to novices},
journal={Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},
year={2016},
pages={283-288},
doi={10.1109/ASE.2015.54},
art_number={7372017},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963811311&doi=10.1109%2fASE.2015.54&partnerID=40&md5=69e6acbdc11855fcc18984dc6c6ab7a0},
abstract={Novice programmers often learn programming by implementing well-known algorithms. There are several challenges in the process. Recommendation systems in software currently focus on programmer productivity and ease of development. Teaching aides for such novice programmers based on recommendation systems still remain an underexplored area. In this paper, we present a general framework for recognizing the desired target for partially-written code and recommending a reliable series of edits to transform the input program into the target solution. Our code analysis is based on graph matching and tree edit algorithms. Our experimental results show that efficient graph comparison techniques can accurately match two portions of source code and produce an accurate set of source code edits. We provide details on implementation of our framework, which is developed as a plugin for Java in Eclipse IDE. © 2015 IEEE.},
keywords={Algorithms;  Automation;  Codes (symbols);  Computer programming;  Pattern matching;  Software engineering;  Trees (mathematics), Comparison techniques;  Graph matchings;  Input programs;  Novice programmer;  Program elements;  Programmer productivity;  Recommendation Framework;  Target solution, Recommender systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2016686,
author={Li, Y. and Rubin, J. and Chechik, M.},
title={Semantic slicing of software version histories},
journal={Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},
year={2016},
pages={686-696},
doi={10.1109/ASE.2015.47},
art_number={7372056},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963930546&doi=10.1109%2fASE.2015.47&partnerID=40&md5=1fba178ec894f35c270041b39bc4480f},
abstract={Software developers often need to transfer func-tionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a specific subset of the change history, "inheriting" additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We refer to our approach, CSLICER, as semantic slicing of version histories. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and instantiate it in a specific implementation for Java projects managed in Git. We evaluate the correctness and effectiveness of our approach on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones. © 2015 IEEE.},
keywords={Automation;  Open systems;  Semantics;  Software engineering, Automated support;  Configuration management systems;  Configuration management tools;  Dependency;  High level semantics;  Software change;  Software developer;  Software versions, Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gao2016307,
author={Gao, Q. and Zhang, H. and Wang, J. and Xiong, Y. and Zhang, L. and Mei, H.},
title={Fixing recurring crash bugs via analyzing Q&A sites},
journal={Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},
year={2016},
pages={307-318},
doi={10.1109/ASE.2015.81},
art_number={7372020},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963852339&doi=10.1109%2fASE.2015.81&partnerID=40&md5=075a4269b366c7b5059eecd63deb39c7},
abstract={Recurring bugs are common in software systems, especially in client programs that depend on the same framework. Existing research uses human-written templates, and is limited to certain types of bugs. In this paper, we propose a fully automatic approach to fixing recurring crash bugs via analyzing Q&A sites. By extracting queries from crash traces and retrieving a list of Q&A pages, we analyze the pages and generate edit scripts. Then we apply these scripts to target source code and filter out the incorrect patches. The empirical results show that our approach is accurate in fixing real-world crash bugs, and can complement existing bug-fixing approaches. © 2015 IEEE.},
keywords={Software engineering, And filters;  Automatic approaches;  Bug-fixing;  Client programs;  Real-world;  Software systems;  Target source, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hilton201653,
author={Hilton, M. and Nelson, N. and McDonald, H. and McDonald, S. and Metoyer, R. and Dig, D.},
title={TDDViz: Using software changes to understand conformance to Test Driven Development},
journal={Lecture Notes in Business Information Processing},
year={2016},
volume={251},
pages={53-65},
doi={10.1007/978-3-319-33515-5_5},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971575025&doi=10.1007%2f978-3-319-33515-5_5&partnerID=40&md5=68c5ff44024c0b8d7b776de4e4a20c1a},
abstract={A bad software development process leads to wasted effort and inferior products. In order to improve a software process, it must be first understood. Our unique approach in this paper uses code and test changes to understand conformance to the Test Driven Development (TDD) process. We designed and implemented TDDViz, a tool that supports developers in better understanding how they conform to TDD. TDDViz supports this understanding by providing novel visualizations of developers’ TDD process. To enable TDDViz’s visualizations, we developed a novel automatic inferencer that identifies the phases that make up the TDD process solely based on code and test changes. We evaluate TDDViz using two complementary methods: a controlled experiment with 35 participants to evaluate the visualization, and a case study with 2601 TDD Sessions to evaluate the inference algorithm. The controlled experiment shows that, in comparison to existing visualizations, participants performed significantly better when using TDDViz to answer questions about code evolution. In addition, the case study shows that the inferencing algorithm in TDDViz infers TDD phases with an accuracy (F-measure) of 87%. © The Author(s) 2016.},
keywords={Codes (symbols);  Computer programming;  Inference engines;  Software engineering;  Software testing;  Visualization, Complementary methods;  Controlled experiment;  Development process;  Inference algorithm;  Novel visualizations;  Software process;  Software visualization;  Test driven development, Software design},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Freeman2016139,
author={Freeman, P. and Watson, I. and Denny, P.},
title={Inferring student coding goals using abstract syntax trees},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9969 LNAI},
pages={139-153},
doi={10.1007/978-3-319-47096-2_10},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994876195&doi=10.1007%2f978-3-319-47096-2_10&partnerID=40&md5=242bcf984472bccc7cb93c5219678c03},
abstract={The rapidly growing demand for programming skills has driven improvements in the technologies delivering programming education to students. Intelligent tutoring systems will potentially contribute to solving this problem, but development of effective systems has been slow to take hold in this area. We present a novel alternative, abstract Syntax Tree Retrieval, which uses case-based reasoning to infer student goals from previous solutions to coding problems. Without requiring programmed expert knowledge, our system demonstrates that accurate retrieval is possible for basic problems. We expect that additional research will uncover more applications for this technology, including more effective intelligent tutoring systems. © Springer International Publishing AG 2016.},
abstract={A bad software development process leads to wasted effort and inferior products. In order to improve a software process, it must be first understood. Our unique approach in this paper uses code and test changes to understand conformance to the Test Driven Development (TDD) process. We designed and implemented TDDViz, a tool that supports developers in better understanding how they conform to TDD. TDDViz supports this understanding by providing novel visualizations of developers’ TDD process. To enable TDDViz’s visualizations, we developed a novel automatic inferencer that identifies the phases that make up the TDD process solely based on code and test changes. We evaluate TDDViz using two complementary methods: a controlled experiment with 35 participants to evaluate the visualization, and a case study with 2601 TDD Sessions to evaluate the inference algorithm. The controlled experiment shows that, in comparison to existing visualizations, participants performed significantly better when using TDDViz to answer questions about code evolution. In addition, the case study shows that the inferencing algorithm in TDDViz infers TDD phases with an accuracy (F-measure) of 87%. © The Author(s) 2016.},
keywords={Computer aided instruction;  Problem solving;  Students;  Syntactics;  Trees (mathematics);  XML, Abstract Syntax Trees;  Coding problems;  Effective systems;  Expert knowledge;  Growing demand;  Intelligent tutoring system;  Programming education;  Programming skills, Case based reasoning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Higo2015372,
author={Higo, Y. and Ohtani, A. and Hayashi, S. and Hata, H. and Shinji, K.},
title={Toward reusing code changes},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2015},
volume={2015-August},
pages={372-376},
doi={10.1109/MSR.2015.43},
art_number={7180097},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957095130&doi=10.1109%2fMSR.2015.43&partnerID=40&md5=ed2480f1173bf95555bbddc09bf3f076},
abstract={Existing techniques have succeeded to help developers implement new code. However, they are insufficient to help to change existing code. Previous studies have proposed techniques to support bug fixes but other kinds of code changes such as function enhancements and refactorings are not supported by them. In this paper, we propose a novel system that helps developers change existing code. Unlike existing techniques, our system can support any kinds of code changes if similar code changes occurred in the past. Our research is still on very early stage and we have not have any implementation or any prototype yet. This paper introduces our research purpose, an outline of our system, and how our system is different from existing techniques. © 2015 IEEE.},
keywords={Bug fixes;  Change reuse;  Code changes;  Code clone;  Refactorings;  Research purpose;  Source code analysis, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Palix201543,
author={Palix, N. and Falleri, J.-R. and Lawall, J.},
title={Improving pattern tracking with a language-aware tree differencing algorithm},
journal={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
year={2015},
pages={43-52},
doi={10.1109/SANER.2015.7081814},
art_number={7081814},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928689087&doi=10.1109%2fSANER.2015.7081814&partnerID=40&md5=b73a9cd4ad9daa13011bf3e039b8d2ff},
abstract={Tracking code fragments of interest is important in monitoring a software project over multiple versions. Various approaches, including our previous work on Herodotos, exploit the notion of Longest Common Subsequence, as computed by readily available tools such as GNU Diff, to map corresponding code fragments. Nevertheless, the efficient code differencing algorithms are typically line-based or word-based, and thus do not report changes at the level of language constructs. Furthermore, they identify only additions and removals, but not the moving of a block of code from one part of a file to another. Code fragments of interest that fall within the added and removed regions of code have to be manually correlated across versions, which is tedious and error-prone. When studying a very large code base over a long time, the number of manual correlations can become an obstacle to the success of a study. In this paper, we investigate the effect of replacing the current line-based algorithm used by Herodotos by tree-matching, as provided by the algorithm of the differencing tool GumTree. In contrast to the line-based approach, the tree-based approach does not generate any manual correlations, but it incurs a high execution time. To address the problem, we propose a hybrid strategy that gives the best of both approaches. © 2015 IEEE.},
keywords={Algorithms;  Codes (symbols);  Computational linguistics;  Image matching;  Open source software, Code metrics;  Code tracking;  Differencing algorithm;  Differencing tools;  Language constructs;  Longest common subsequences;  Tree-based approach;  Tree-matching, Trees (mathematics)},
document_type={Conference Paper},
source={Scopus},
}


@ARTICLE{Stevens2019491,
author={Stevens, R. and Molderez, T. and De Roover, C.},
title={Querying distilled code changes to extract executable transformations},
journal={Empirical Software Engineering},
year={2019},
volume={24},
number={1},
pages={491-535},
doi={10.1007/s10664-018-9644-3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053271297&doi=10.1007%2fs10664-018-9644-3&partnerID=40&md5=e63534634789c9c6f417132721c3f25a},
abstract={Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. We show that our approach is able to recall different implementation variants of the same source code evolution in histories of different software projects. We also evaluate the expressiveness and ease-of-use of our approach in a user study. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
keywords={Computer programming languages, Change distilling;  Change querying;  Evolution patterns;  Fine-grained changes;  Intermediate state;  Logic meta programming;  Mining software repositories;  Software project, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Im2018629,
author={Im, B. and Chen, A. and Wallach, D.S.},
title={An historical analysis of the seandroid policy evolution},
journal={ACM International Conference Proceeding Series},
year={2018},
pages={629-640},
doi={10.1145/3274694.3274709},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060064966&doi=10.1145%2f3274694.3274709&partnerID=40&md5=2227f51c419a3e0dd94ea30eca4d4183},
abstract={Android adopted SELinux's mandatory access control (MAC) mechanisms in 2013. Since then, billions of Android devices have benefited from mandatory access control security policies. These policies are expressed in a variety of rules, maintained by Google and extended by Android OEMs. Over the years, the rules have grown to be quite complex, making it challenging to properly understand or configure these policies. In this paper, we perform a measurement study on the SEAndroid repository to understand the evolution of these policies. We propose a new metric to measure the complexity of the policy by expanding policy rules, with their abstraction features such as macros and groups, into primitive “boxes”, which we then use to show that the complexity of the SEAndroid policies has been growing exponentially over time. By analyzing the Git commits, snapshot by snapshot, we are also able to analyze the “age” of policy rules, the trend of changes, and the contributor composition. We also look at hallmark events in Android's history, such as the “Stagefright” vulnerability in Android's media facilities, pointing out how these events led to changes in the MAC policies. The growing complexity of Android's mandatory policies suggests that we will eventually hit the limits of our ability to understand these policies, requiring new tools and techniques. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
keywords={Access control;  Android (operating system);  Security systems, Android;  Historical analysis;  Mandatory access control;  Measurement study;  SEAndroid;  Security;  SELinux;  Tools and techniques, Mobile security},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2018287,
author={Wang, Y. and Meng, N. and Zhong, H.},
title={An empirical study of multi-entity changes in real bug fixes},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
year={2018},
pages={287-298},
doi={10.1109/ICSME.2018.00038},
art_number={8530037},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057470499&doi=10.1109%2fICSME.2018.00038&partnerID=40&md5=3e54a06b41738a4716cb9b51363baf12},
abstract={Prior studies showed that developers applied repeated bug fixes-similar or identical code changes-To multiple locations. According to the observation, researchers built tools to automatically generate candidate patches from the repeated bug-fixing patterns. However, all such research focuses on the recurring change patterns within single methods. We are curious whether there are also repeated bug fixes that change multiple program entities (e.g., classes, methods, and fields); and if so, how we can leverage such recurring change patterns to further help developers fix bugs. In this paper, we present a comprehensive empirical study on multi-entity bug fixes in terms of their frequency, composition, and semantic meanings. Specifically for each bug fix, we first used our approach InterPart to perform static inter-procedural analysis on partial programs (i.e., the old and new versions of changed Java files), and to extract change dependency graphs (CDGs)-graphs that connect multiple changed entities based on their syntactic dependencies. By extracting common subgraphs from the CDGs of different fixes, we identified the recurring change patterns. Our study on Aries, Cassandra, Derby, and Mahout shows that (1) 52-58% of bug fixes involved multi-entity changes; (2) 6 recurring change patterns commonly exist in all projects; and (3) 19-210 entity pairs were repetitively co-changed mainly because the pairs invoked the same methods, accessed the same fields, or contained similar content. These results helped us better understand the gap between the fixes generated by existing automatic program repair (APR) approaches and the real fixes. Our observations will shed light on the follow-up research of automatic program comprehension and modification. © 2018 IEEE.},
keywords={Computer software;  Computer software maintenance;  Semantics, Automatic programs;  Change dependencies;  Change patterns;  Empirical studies;  Inter-procedural analysis;  Multiple program;  Software entities;  Syntactic dependencies, Program debugging},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Damevski20181100,
author={Damevski, K. and Chen, H. and Shepherd, D.C. and Kraft, N.A. and Pollock, L.},
title={Predicting future developer behavior in the IDE using topic models},
journal={IEEE Transactions on Software Engineering},
year={2018},
volume={44},
number={11},
pages={1100-1111},
doi={10.1109/TSE.2017.2748134},
art_number={8024001},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029172869&doi=10.1109%2fTSE.2017.2748134&partnerID=40&md5=09dd407f1e04978a96727f4f13ece9f2},
abstract={While early software command recommender systems drew negative user reaction, recent studies show that users of unusually complex applications will accept and utilize command recommendations. Given this new interest, more than a decade after first attempts, both the recommendation generation (backend) and the user experience (frontend) should be revisited. In this work, we focus on recommendation generation. One shortcoming of existing command recommenders is that algorithms focus primarily on mirroring the short-term past,-i.e., assuming that a developer who is currently debugging will continue to debug endlessly. We propose an approach to improve on the state of the art by modeling future task context to make better recommendations to developers. That is, the approach can predict that a developer who is currently debugging may continue to debug OR may edit their program. To predict future development commands, we applied Temporal Latent Dirichlet Allocation, a topic model used primarily for natural language, to software development interaction data (i.e., command streams). We evaluated this approach on two large interaction datasets for two different IDEs, Microsoft Visual Studio and ABB Robot Studio. Our evaluation shows that this is a promising approach for both predicting future IDE commands and producing empirically-interpretable observations. © 1976-2012 IEEE.},
keywords={Analytical models;  Application programs;  Data reduction;  Data structures;  Flow visualization;  Integrodifferential equations;  Recommender systems, Adaptation models;  Complex applications;  Developer behavior;  IDE interaction data;  Natural languages;  Predictive models;  Software commands;  User experience, Data visualization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2018,
author={Liu, X. and Huang, L. and Egyed, A. and Ge, J.},
title={Do code data sharing dependencies support an early prediction of software actual change impact set?},
journal={Journal of Software: Evolution and Process},
year={2018},
volume={30},
number={11},
doi={10.1002/smr.1960},
art_number={e1960},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050919825&doi=10.1002%2fsmr.1960&partnerID=40&md5=e13c43d3695fb9fdb2516dfdfac566fc},
abstract={Existing studies have shown that structural dependencies within code are good predictors for code actual change impact set—a set of entities that repeatedly changing together to ensure a consistent and complete change. However, the result is far from ideal, particularly when insufficient historical data are available at an early stage of software development. This paper demonstrates that a better understanding of data dependencies in addition to call dependencies greatly improves actual change impact set prediction. We propose a new approach and tool (namely, CHIP) to predict software actual change impact sets leveraging both call and data sharing dependencies. For this purpose, CHIP employs novel extensions (dependency frequency filtering and shared data type idf filtering) to reduce false positives. CHIP assumes that developers know initial places where to start making changes in the source code even though they may not know all changes. This approach has been empirically evaluated on 4 large-scale open source systems. Our evaluation demonstrates that data sharing dependencies have a complementary impact on software actual change impact set prediction as compared with predictions based on call dependencies only. CHIP improves the F2-score compared with the predictors using both Program Dependence Graph and evolutionary couplings. © 2018 John Wiley & Sons, Ltd.},
keywords={Codes (symbols);  Forecasting;  Open source software;  Software design, Data dependencies;  Data Sharing;  Early prediction;  Frequency filtering;  Impact analysis;  Open source system;  Program dependence graph;  Source codes, Open systems},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Feng2018,
author={Feng, Q. and Cai, Y. and Kazman, R. and Mo, R.},
title={The birth, growth, death and rejuvenation of software maintenance communities},
journal={International Symposium on Empirical Software Engineering and Measurement},
year={2018},
doi={10.1145/3239235.3239246},
art_number={a5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061484364&doi=10.1145%2f3239235.3239246&partnerID=40&md5=c980316559db7abf2dfb6c323baa9ff1},
abstract={Background: Though much research has been conducted to investigate software maintenance activities, there has been little work charactering maintenance files as a community and exploring the evolution of this community. Aims: The goal of our research is to identify maintenance communities and monitor their evolution-birth, growth, death and rejuvenation. Method: In this paper, we leveraged a social community detection algorithm - -clique prelocation method (CPM) - -to identify file communities. Then we implemented an algorithm to detect new communities, active communities, inactive communities and reactivated communities by cumulatively detecting and constantly comparing communities in time sequences. Results: Based on our analysis of 14 open-source projects, we found that new communities are mostly caused by bug and improvement issues. An active community can be vigorous, on and off, through the entire life of a system, and so does an inactive community. In addition, an inactive community can be reactivated again, mostly through bug issues. Conclusions: These findings add to our understanding of software maintenance communities and help us identify the most expensive maintenance spots by identifying constantly active communities. © 2018 ACM.},
keywords={Endocrinology;  Engineering research;  Open source software, Open source projects;  Social communities;  Software Evolution;  Software maintenance activity;  Time sequences, Computer software maintenance},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhong20182521,
author={Zhong, H. and Meng, N.},
title={Towards reusing hints from past fixes: An exploratory study on thousands of real samples},
journal={Empirical Software Engineering},
year={2018},
volume={23},
number={5},
pages={2521-2549},
doi={10.1007/s10664-017-9584-3},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038808079&doi=10.1007%2fs10664-017-9584-3&partnerID=40&md5=1d4291d45443069beb65daf6399cf884},
abstract={With the usage of version control systems, many bug fixes have accumulated over the years. Researchers have proposed various automatic program repair (APR) approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new fixes overlap with old fixes, have not been investigated. Intuitively, the overlap between old and new fixes decides how APR approaches can construct new fixes with old ones. Based on this intuition, we systematically designed six overlap metrics, and performed an empirical study on 5,735 bug fixes to investigate the usefulness of past fixes when composing new fixes. For each bug fix, we created delta graphs (i.e., program dependency graphs for code changes), and identified how bug fixes overlap with each other in terms of the content, code structures, and identifier names of fixes. Our results show that if an APR approach knows all code name changes and composes new fixes by fully or partially reusing the content of past fixes, only 2.1% and 3.2% new fixes can be created from single or multiple past fixes in the same project, compared with 0.9% and 1.2% fixes created from past fixes across projects. However, if an APR approach knows all code name changes and composes new fixes by fully or partially reusing the code structures of past fixes, up to 41.3% and 29.7% new fixes can be created. By making the above observations and revealing other ten findings, we investigated the upper bound of reusable past fixes and composable new fixes, exploring the potential of existing and future APR approaches. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.},
keywords={Computer software reusability;  Program debugging, Automatic programs;  Code changes;  Code structure;  Empirical studies;  Exploratory studies;  Program dependency graphs;  Real samples;  Version control system, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Huang20182226,
author={Huang, Y. and Jia, N. and Zhou, Q. and Chen, X.-P. and Xiong, Y.-F. and Luo, X.-N.},
title={Method Combining Structural and Semantic Features to Support Code Commenting Decision [融合结构与语义特征的代码注释决策支持方法]},
journal={Ruan Jian Xue Bao/Journal of Software},
year={2018},
volume={29},
number={8},
pages={2226-2242},
doi={10.13328/j.cnki.jos.005528},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055558530&doi=10.13328%2fj.cnki.jos.005528&partnerID=40&md5=e6eb6abe09383934546ff21dbdc471fb},
abstract={Code comment is quite important to help developer review and comprehend source code. Strategic comment decision is desired to cover core code snippets of software system without incurring unintended trivial comments. However, in current practice, there is a lack of rigorous specifications for developers to make their comment decisions. Commenting has become an important yet tough decision which mostly depends on the personal experience of developers. To reduce the effort on making comment decisions, this paper investigates a unified commenting regulation from a large number of commenting instances. A method, CommentAdviser, is proposed to guide developers in placing comments in source code. Since making comment is closely related to the context information of source code themselves, the method identifies this important factor for determining where to comment and extract them as structural context feature and semantic context feature. Next, machine learning techniques are applied to identify the possible commenting locations in source code. CommentAdviser is evaluated on 10 data sets from GitHub. The experimental results, as well as a user study, demonstrate the feasibility and effectiveness of CommentAdviser. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.},
keywords={Artificial intelligence;  Computer programming languages;  Learning systems;  Semantics, Code comment;  Comment decision;  Context information;  Current practices;  Machine learning techniques;  Personal experience;  Semantic features;  Structural feature, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cui2018776,
author={Cui, Z. and Chen, X. and Mu, Y. and Pan, M. and Wang, R.},
title={PSP-Finder: A defect detection method based on mining correlations from function call paths},
journal={Chinese Journal of Electronics},
year={2018},
volume={27},
number={4},
pages={776-782},
doi={10.1049/cje.2018.04.001},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051353929&doi=10.1049%2fcje.2018.04.001&partnerID=40&md5=d84f88abdd1d745d214486591b6fb3e7},
abstract={Large scale programs usually imply many programming rules, which are missing from the specification documents. However, if programmers violate these rules in the process of programming, it is possible to introduce software defects. Previous works on mining function call correlation patterns only use structural information of the program, while control flow, data flow or other semantic information of the program are not exploited in those approaches. As a result, the defect detecting ability is restricted and high false rate is caused. This paper proposes a defect detection method based on mining function call association rules from program paths, which can be provided by simple static analysis. Then, the programs are automatically checked against the function call association rules for detecting suspicious defects. Based on this approach, experiments are carried out on a group of open source projects. The experiment results show that this approach can improve the capability of detecting defects and find more bugs related to program execution path. In addition, the false positive function call patterns and the overhead for manually validating suspicious defects are reduced. © 2018 Chinese Institute of Electronics. All rights reserved.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Huang20181333,
author={Huang, Q. and Yang, Y. and Zhan, X. and Wan, H. and Wu, G.},
title={Query expansion based on statistical learning from code changes},
journal={Software - Practice and Experience},
year={2018},
volume={48},
number={7},
pages={1333-1351},
doi={10.1002/spe.2574},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048332093&doi=10.1002%2fspe.2574&partnerID=40&md5=d4d3c5887825b7582df9c7d2112ee924},
abstract={Thesaurus-based, code-related, and software-specific query expansion techniques are the main contributions in free-form query search. However, these techniques still could not put the most relevant query result in the first position because they lack the ability to infer the expansion words that represent the user needs based on a given query. In this paper, we discover that code changes can imply what users want and propose a novel query expansion technique with code changes (QECC). It exploits (changes, contexts) pairs from changed methods. On the basis of statistical learning from pairs, it can infer code changes for a given query. In this way, it expands a query with code changes and recommends the query results that meet actual needs perfectly. In addition, we implement InstaRec to perform QECC and evaluate it with 195 039 change commits from GitHub and our code tracker. The results show that QECC can improve the precision of 3 code search algorithms (ie, IR, Portfolio, and VF) by up to 52% to 62% and outperform the state-of-the-art query expansion techniques (ie, query expansion based on crowd knowledge and CodeHow) by 13% to 16% when the top 1 result is inspected. Copyright © 2018 John Wiley & Sons, Ltd.},
keywords={Computer software reusability;  Information retrieval, Code changes;  Code search;  Free form queries;  Query expansion;  Query expansion techniques;  Relevant query;  State of the art;  Statistical learning, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chi2018,
author={Chi, P.-Y. and Hu, S.-P. and Li, Y.},
title={Doppio: Tracking UI flows and code changes for app development},
journal={Conference on Human Factors in Computing Systems - Proceedings},
year={2018},
volume={2018-April},
doi={10.1145/3173574.3174029},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046933574&doi=10.1145%2f3173574.3174029&partnerID=40&md5=6eab8da51464030181ec1111f4db713d},
abstract={Developing interactive systems often involves a large set of callback functions for handling user interaction, which makes it challenging to manage UI behaviors, create descriptive documentation, and track code revisions. We developed Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code. For each input event listener of a widget, e.g., onClick of an Android View class, Doppio captures and associates its UI output from a program execution with its code snippet from the codebase. It automatically generates a screenflow diagram organized by the callback methods and interaction flow, where developers can review the code and UI revisions interactively. Doppio, as an IDE plugin, is seamlessly integrated into a common development workflow. Our studies show that our tool is able to generate quality visual documentation and helped participants understand unfamiliar source code and track changes. © 2018 Copyright is held by the owner/author(s).},
keywords={Android (operating system);  Demonstrations;  Human computer interaction;  Human engineering, Android;  IDEs;  Mobile apps;  Screencast videos;  Screenflow diagram;  Software documentation, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yamamori2017128,
author={Yamamori, A. and Hagward, A.M. and Kobayashi, T.},
title={Can Developers' Interaction Data Improve Change Recommendation?},
journal={Proceedings - International Computer Software and Applications Conference},
year={2017},
volume={1},
pages={128-137},
doi={10.1109/COMPSAC.2017.79},
art_number={8029600},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029840337&doi=10.1109%2fCOMPSAC.2017.79&partnerID=40&md5=d71b48ed13627c82fe4a85574a521862},
abstract={One of the most common causes of bugs is overlooking changes. To prevent bugs and improve the quality of the products, numerous studies have been undertaken on change guides based on logical couplings extracted from developers' past process histories, such as change history. While valuable change rules based on logical couplings can be gleaned found from the change history, these rules often fail to find appropriate candidates because the change histories in repositories only preserve a summary of changes between commits. We recently analyzed the interaction data produced by a developer in an integrated development environment. Such interaction data contains not only a detailed change history but also reference activities between commits. In this paper, we investigate whether logical couplings extracted from interaction data could improve change recommendation performance. We used the interaction data from actual open source development, not from the project only for this study. Experimental results obtained using the interaction data from actual open source development showed a significant improvement in the efficiency of the change recommendation process. The results also indicated improvement in the number of detected artifacts that the developer had forgot to change. © 2017 IEEE.},
keywords={Application programs;  Computer software maintenance;  Couplings;  Open source software, Change Guide;  Change impact analysis;  Integrated development environment;  Interaction Data;  Mining software repositories;  Open source development;  Process history;  Recommendation performance, Computer software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Molderez2017248,
author={Molderez, T. and Stevens, R. and De Roover, C.},
title={Mining Change Histories for Unknown Systematic Edits},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2017},
pages={248-256},
doi={10.1109/MSR.2017.12},
art_number={7962375},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026513536&doi=10.1109%2fMSR.2017.12&partnerID=40&md5=0f73ceace2f21e830f52db18f6587a08},
abstract={Software developers often need to repeat similar modifications in multiple different locations of a system's source code. These repeated similar modifications, or systematic edits, can be both tedious and error-prone to perform manually. While there are tools that can be used to assist in automating systematic edits, it is not straightforward to find out where the occurrences of a systematic edit are located in an existing system. This knowledge is valuable to help decide whether refactoring is needed, or whether future occurrences of an existing systematic edit should be automated. In this paper, we tackle the problem of finding unknown systematic edits using a closed frequent itemset mining algorithm, operating on sets of distilled source code changes. This approach has been implemented for Java programs in a tool called SysEdMiner. To evaluate the tool's precision and scalability, we have applied it to an industrial use case. © 2017 IEEE.},
keywords={Change distilling;  Closed frequent itemset;  Existing systems;  Frequent itemset mining;  Industrial use case;  Software developer;  Source code changes;  Systematic edits, Computer software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Damevski2017359,
author={Damevski, K. and Shepherd, D.C. and Schneider, J. and Pollock, L.},
title={Mining Sequences of Developer Interactions in Visual Studio for Usage Smells},
journal={IEEE Transactions on Software Engineering},
year={2017},
volume={43},
number={4},
pages={359-371},
doi={10.1109/TSE.2016.2592905},
art_number={7516714},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018977446&doi=10.1109%2fTSE.2016.2592905&partnerID=40&md5=b0fbdc5052c12dba5a5b044b5ac0f506},
abstract={In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional. © 2017 IEEE.},
keywords={Data mining;  Discovery wells;  Odors;  Program debugging;  Software design;  Studios, Developer behavior;  Development activity;  Large-scale dataset;  Pattern mining;  Performance problems;  Time-ordered sequences;  Usability analysis;  Usage data, Integrodifferential equations},
document_type={Article},
source={Scopus},
}

@ARTICLE{Soetens2017,
author={Soetens, Q.D. and Robbes, R. and Demeyer, S.},
title={Changes as first-class citizens: A research perspective on modern software tooling},
journal={ACM Computing Surveys},
year={2017},
volume={50},
number={2},
doi={10.1145/3038926},
art_number={18},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017462023&doi=10.1145%2f3038926&partnerID=40&md5=726931a538fb97503e862ace4373c80f},
abstract={Software must evolve to keep up with an ever-changing context, the real world. We discuss an emergent trend in software evolution research revolving around the central notion that drives evolution: Change. By reifying change, and by modelling it as a first-class entity, researchers can now analyse the complex phenomenon known as software evolution with an unprecedented degree of accuracy. We present a Systematic Mapping Study of 86 articles to give an overview on the state of the art in this area of research and present a roadmap with open issues and future directions. © 2017 ACM.},
keywords={Computer science;  Surveys, Atomic changes;  Change distilling;  Change recording;  Fine grained;  Fine-grained changes;  Systematic mapping studies, Mapping},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Stevens2017171,
author={Stevens, R. and De Roover, C.},
title={Extracting executable transformations from distilled code changes},
journal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
year={2017},
pages={171-181},
doi={10.1109/SANER.2017.7884619},
art_number={7884619},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018416465&doi=10.1109%2fSANER.2017.7884619&partnerID=40&md5=ee0189b8cc4dd9b95dc6ef1fa7877eaa},
abstract={Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. Using examples, we demonstrate the expressiveness of specifying source code evolutions through intermediate ASTs. We also show that our approach is able to recall different implementation variants of the same source code evolution in open-source histories. © 2017 IEEE.},
keywords={Codes (symbols);  Computer programming languages;  Open source software;  Reengineering, Code changes;  Evolution patterns;  Fine-grained changes;  Intermediate state;  Mining software repositories;  Multiple changes;  Open sources;  Source codes, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Santos2017160,
author={Santos, G. and Paixao, K.V.R. and Anquetil, N. and Etien, A. and De Almeida Maia, M. and Ducasse, S.},
title={Recommending source code locations for system specific transformations},
journal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
year={2017},
pages={160-170},
doi={10.1109/SANER.2017.7884618},
art_number={7884618},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018376376&doi=10.1109%2fSANER.2017.7884618&partnerID=40&md5=2c8bab49fecd81753c4919caaa216628},
abstract={From time to time, developers perform sequences of code transformations in a systematic and repetitive way. This may happen, for example, when introducing a design pattern in a legacy system: similar classes have to be introduced, containing similar methods that are called in a similar way. Automation of these sequences of transformations has been proposed in the literature to avoid errors due to their repetitive nature. However, developers still need support to identify all the relevant code locations that are candidate for transformation. Past research showed that these kinds of transformation can lag for years with forgotten instances popping out from time to time as other evolutions bring them into light. In this paper, we evaluate three distinct code search approaches ('structural', based on Information Retrieval, and AST based algorithm) to find code locations that would require similar transformations. We validate the resulting candidate locations from these approaches on real cases identified previously in literature. The results show that looking for code with similar roles, e.g., classes in the same hierarchy, provides interesting results with an average recall of 87% and in some cases the precision up to 70%. © 2017 IEEE.},
keywords={Cosine transforms;  Legacy systems;  Location;  Reengineering, Candidate locations;  Code search;  Code transformation;  Design Patterns;  Real case;  Source codes;  System specific, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jiang2017,
author={Jiang, Q. and Peng, X. and Wang, H. and Xing, Z. and Zhao, W.},
title={Understanding systematic and collaborative code changes by mining evolutionary trajectory patterns},
journal={Journal of Software: Evolution and Process},
year={2017},
volume={29},
number={3},
doi={10.1002/smr.1840},
art_number={e1840},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015706775&doi=10.1002%2fsmr.1840&partnerID=40&md5=5f27f9bc8c76ca91bf3b704470d38920},
abstract={The life cycle of a large-scale software system can undergo many releases. Each release often involves hundreds or thousands of revisions committed by many developers over time. Many code changes are made in a systematic and collaborative way. However, such systematic and collaborative code changes are often undocumented and hidden in the evolution history of a software system. It is desirable to recover commonalities and associations among dispersed code changes in the evolutionary trajectory of a software system. In this paper, we present Summarizing Evolutionary Trajectory by Grouping and Aggregation (SETGA), an approach to summarizing historical commit records as trajectory patterns by grouping and aggregating relevant code changes committed over time. The SETGA extracts change operations from a series of commit records from version control systems. It then groups extracted change operations by their common properties from different dimensions such as change operation types, developers, and change locations. After that, SETGA aggregates relevant change operation groups by mining various associations among them. We implement SETGA and conduct an empirical study with 3 open-source systems. We investigate underlying evolution rules and problems that can be revealed by the identified patterns and analyze the evolution of trajectory patterns in different periods. The results show that SETGA can identify various types of trajectory patterns that are useful for software evolution management and quality assurance. Copyright © 2017 John Wiley & Sons, Ltd.},
keywords={Codes (symbols);  Computer software;  Control systems;  Information management;  Life cycle;  Mining;  Open source software;  Quality assurance;  Trajectories, Code changes;  Collaborative codes;  evolution;  Large-scale software systems;  pattern;  Software Evolution;  Trajectory pattern;  Version control system, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lee20171234,
author={Lee, J. and Kim, K. and Lee, Y.-H. and Hong, J.-E. and Seo, Y.-H. and Yang, B.-D. and Jung, W.},
title={Extracting the source code context to predict import changes using GPES},
journal={KSII Transactions on Internet and Information Systems},
year={2017},
volume={11},
number={2},
pages={1234-1249},
doi={10.3837/tiis.2017.02.035},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014125055&doi=10.3837%2ftiis.2017.02.035&partnerID=40&md5=9435eb58b415c74fe66e287ad5889ea8},
abstract={One of the difficulties developers encounter in maintaining tasks of a large-scale software system is the updating of suitable libraries on time. Developers tend to miss or make mistakes when searching for and choosing libraries during the development process, or there may not be a stable library for the developers to use. We present a novel approach for helping developers modify software easily and on time and avoid software failures. Using a tool previously built by us called GPES, we collected information of projects, such as abstract syntax trees, tokens, software metrics, relations, and evolutions, for our experiments. We analyzed the contexts of source codes in existing projects to predict changes automatically and to recommend suitable libraries for the projects. The collected data show that researchers can reduce the overall cost of data analysis by transforming the extracted data into the required input formats with a simple query-based implementation. Also, we manually evaluated how the extracted contexts are similar to the description and we found that a sufficient number of the words in the contexts is similar and it might help developers grasp the domain of the source codes easily. © 2017 KSII.},
keywords={Computer programming languages;  Libraries;  Metadata;  Natural language processing systems;  Trees (mathematics), Abstract Syntax Trees;  Context analysis;  Development process;  Large-scale software systems;  Software failure;  Software metrics;  Software repositories;  Source code changes, Codes (symbols)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nguyen2017200,
author={Nguyen, H.A. and Nguyen, A.T. and Nguyen, T.N.},
title={Using topic model to suggest fine-grained source code changes},
journal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
year={2017},
pages={200-210},
doi={10.1109/ICSME.2016.40},
art_number={7816467},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013124289&doi=10.1109%2fICSME.2016.40&partnerID=40&md5=09306020f4e5f855f93b77c1c95cb44e},
abstract={Prior research has shown that source code and its changes are repetitive. Several approaches have leveraged that phenomenon to detect and recommend change/fix patterns. In this paper, we propose TasC, a model that leverages the context of change tasks in development history to suggest fine-grained code change/fix at the program statement level. We use Latent Dirichlet Allocation (LDA) to capture the change task context via co-occurring program elements in the changes in a context. We also propose a novel technique for measuring the similarity of code fragments and code changes using the task context. We conducted an empirical evaluation on a large dataset of 88 open-source Java projects containing more than 200 thousand source files and 3.5 million source lines of code in their last revisions with 423 thousand changed methods. Our result shows that TasC relatively improves recommendation accuracy up to 130%-250% in comparison with the base models that do not use task context. Compared with other types of contexts, TasC outperforms the models using structural and co-change contexts. © 2016 IEEE.},
keywords={Codes (symbols);  Computer software maintenance;  Open source software;  Statistics, Development history;  Empirical evaluations;  Fine-grained source code changes;  Latent dirichlet allocations;  Novel techniques;  Program statements;  Recommendation accuracy;  Source lines of codes, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Thung2017222,
author={Thung, F. and Le, X.-B.D. and Lo, D. and Lawall, J.},
title={Recommending code changes for automatic backporting of linux device drivers},
journal={Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016},
year={2017},
pages={222-232},
doi={10.1109/ICSME.2016.71},
art_number={7816469},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013076920&doi=10.1109%2fICSME.2016.71&partnerID=40&md5=5a83fbffd772ae154448e44782abd216},
abstract={Device drivers are essential components of any operating system (OS). They specify the communication protocol that allows the OS to interact with a device. However, drivers for new devices are usually created for a specific OS version. These drivers often need to be backported to the older versions to allow use of the new device. Backporting is often done manually, and is tedious and error prone. To alleviate this burden on developers, we propose an automatic recommendation system to guide the selection of backporting changes. Our approach analyzes the version history for cues to recommend candidate changes. We have performed an experiment on 100 Linux driver files and have shown that we can give a recommendation containing the correct backport for 68 of the drivers. For these 68 cases, 73.5%, 85.3%, and 88.2% of the correct recommendations are located in the Top-1, Top-2, and Top-5 positions of the recommendation lists respectively. The successful cases cover various kinds of changes including change of record access, deletion of function argument, change of a function name, change of constant, and change of if condition. Manual investigation of failed cases highlights limitations of our approach, including inability to infer complex changes, and unavailability of relevant cues in version history. © 2016 IEEE.},
keywords={Computer operating systems;  Computer software maintenance;  Linux, Backporting;  Code changes;  Device Driver;  Error prones;  Linux drivers;  New devices, Recommender systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rolim20161063,
author={Rolim, R.},
title={Automating repetitive code changes using examples},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={1063-1065},
doi={10.1145/2950290.2983944},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997497946&doi=10.1145%2f2950290.2983944&partnerID=40&md5=ccedf65ba5689744cdb731e68ae50b0c},
abstract={While adding features, fixing bugs, or refactoring the code, developers may perform repetitive code edits. Although Integrated Development Environments (IDEs) automate some transformations such as renaming, many repetitive edits are performed manually, which is error-prone and time-consuming. To help developers to apply these edits, we propose a technique to perform repetitive edits using examples. The technique receives as input the source code before and after the developer edits some target locations of the change and produces as output the top-ranked program transformation that can be applied to edit the remaining target locations in the codebase. The technique uses a state-of-The-Art program synthesis methodology and has three main components: A) a DSL for describing program transformations; b) synthesis algorithms to learn program transformations in this DSL; c) ranking algorithms to select the program transformation with the higher probability of performing the desired repetitive edit. In our preliminary evaluation, in a dataset of 59 repetitive edit cases taken from real C# source code repositories, the technique performed, in 83% of the cases, the intended transformation using only 2.8 examples. © 2016 ACM.},
keywords={Digital subscriber lines;  Software engineering, Integrated development environment;  Program synthesis;  Program transformations;  Programming by Example;  Ranking algorithm;  Software Evolution;  State of the art;  Synthesis algorithms, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Silva2016858,
author={Silva, D. and Tsantalis, N. and Valente, M.T.},
title={Why we Refactor? Confessions of Github contributors},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={858-870},
doi={10.1145/2950290.2950305},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997428839&doi=10.1145%2f2950290.2950305&partnerID=40&md5=29c96c43c7fbcc798c112eee9b56c1ac},
abstract={Refactoring is a widespread practice that helps developers to improve the maintainability and readability of their code. However, there is a limited number of studies empirically investigating the actual motivations behind specific refac-toring operations applied by developers. To fill this gap, we monitored Java projects hosted on GitHub to detect re-cently applied refactorings, and asked the developers to ex-plain the reasons behind their decision to refactor the code. By applying thematic analysis on the collected responses, we compiled a catalogue of 44 distinct motivations for 12 well-known refactoring types. We found that refactoring ac-tivity is mainly driven by changes in the requirements and much less by code smells. Extract Method is the most versatile refactoring operation serving 11 different purposes. Finally, we found evidence that the IDE used by the devel-opers affects the adoption of automated refactoring tools.},
keywords={Motivation;  Odors;  Software engineering, Code smell;  GitHub;  Refactoring tools;  Refactorings;  Software Evolution;  Thematic analysis, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nguyen2016511,
author={Nguyen, A.T. and Hilton, M. and Codoban, M. and Nguyen, H.A. and Mast, L. and Rademacher, E. and Nguyen, T.N. and Dig, D.},
title={API code recommendation using statistical learning from fine-grained changes},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={511-522},
doi={10.1145/2950290.2950333},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997525166&doi=10.1145%2f2950290.2950333&partnerID=40&md5=3288ad1884b2a99e3b94e5d624ac7441},
abstract={Learning and remembering how to use APIs is difficult. While codecompletion tools can recommend API methods, browsing a long list of API method names and their documentation is tedious. Moreover, users can easily be overwhelmed with too much information. We present a novel API recommendation approach that taps into the predictive power of repetitive code changes to provide relevant API recommendations for developers. Our approach and tool, APIREC, is based on statistical learning from fine-grained code changes and from the context in which those changes were made. Our empirical evaluation shows that APIREC correctly recommends an API call in the first position 59% of the time, and it recommends the correct API call in the top 5 positions 77% of the time. This is a significant improvement over the state-of-The-Art approaches by 30-160% for top-1 accuracy, and 10-30% for top-5 accuracy, respectively. Our result shows that APIREC performs well even with a one-Time, minimal training dataset of 50 publicly available projects. © 2016 ACM.},
keywords={Software engineering, API Recommendation;  Code changes;  Empirical evaluations;  Fine-grained changes;  Minimal training;  Predictive power;  State-of-the-art approach;  Statistical learning, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hanam2016144,
author={Hanam, Q. and Brito, F.S.D.M. and Mesbah, A.},
title={Discovering bug patterns in Javascript},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={144-156},
doi={10.1145/2950290.2950308},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997194354&doi=10.1145%2f2950290.2950308&partnerID=40&md5=62585e7f8af13c465525ad608bdb900b},
abstract={JavaScript has become the most popular language used by developers for client and server side programming. The language, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug findings tools in use today cover bug patterns that are discovered by reading best practices or through developer intuition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-Automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using languageconstruct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that occur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript. © 2016 ACM.},
keywords={Artificial intelligence;  Data mining;  Learning systems;  Software engineering;  Static analysis, Bug Patterns;  Javascript;  Large-scale studies;  Multiple projects;  Node.Js;  Semi-automatics;  Tools and techniques;  Unsupervised machine learning, High level languages},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Proksch2016111,
author={Proksch, S. and Amann, S. and Nadi, S. and Mezini, M.},
title={Evaluating the evaluations of code recommender systems: A reality check},
journal={ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year={2016},
pages={111-121},
doi={10.1145/2970276.2970330},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989170526&doi=10.1145%2f2970276.2970330&partnerID=40&md5=89470c873e1e038c1cd70392d1471069},
abstract={While researchers develop many new exciting code recommender systems, such as method-call completion, code-snippet completion, or code search, an accurate evaluation of such systems is always a challenge. We analyzed the current literature and found that most of the current evaluations rely on artificial queries extracted from released code, which begs the question: Do such evaluations reect real-life usages? To answer this question, we capture 6,189 fine-grained development histories from real IDE interactions. We use them as a ground truth and extract 7,157 real queries for a specific method-call recommender system. We compare the results of such real queries with different artificial evaluation strategies and check several assumptions that are repeatedly used in research, but never empirically evaluated. We find that an evolving context that is often observed in practice has a major effect on the prediction quality of recommender systems, but is not commonly reected in artificial evaluations. © 2016 ACM.},
keywords={Codes (symbols);  Integrodifferential equations;  Recommender systems;  Software engineering, Artificial Evaluation;  Development history;  Empirical studies;  Evaluation strategies;  Fine grained;  Ground truth;  IDE Interaction Data;  Prediction quality, Quality control},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{He20161227,
author={He, H. and Wang, J. and Xu, C. and Wang, H. and Ren, J.},
title={Mining high utility patterns from software executing traces},
journal={International Journal of Innovative Computing, Information and Control},
year={2016},
volume={12},
number={4},
pages={1227-1239},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982899565&partnerID=40&md5=0b041b1fae4e7cb412c1e67e7abc790e},
abstract={Software behavior mining is quite meaningful work. Finding meaningful patterns can help program maintainers detect the exception and improve work efficiency. These high utility software executing patterns shed light on software behavior and capture unique characteristic of software traces. In this paper, an efficient method called SHUP- Miner (Software High Utility Pattern Mining) is proposed to mine high utility patterns from software executing traces. In the algorithm, we firstly define the utility of each event in the software executing traces according to their importance of the software behavior. Secondly, a novel structure called improved utility list shorted as IUL is put forward. It stores patterns position and utility information which contributes to pruning space and extending patterns. Moreover, pattern extending strategy and IUCS (Improved Utility Cooccurrence Structure) are incorpora, ted into SHUP-Miner to improve the efficiency. At last, vie conduct experiments on both real and synthetic datasets. The results show that SHUP-Miner incorporating the efficiency-enhanced strategies demonstrates impressive performance. © 2016, IJICIC Editorial Office. All rights reserved.},
keywords={Data mining;  Efficiency;  Miners, High utility;  Meaningful works;  Novel structures;  Pattern mining;  Software behavior;  Synthetic datasets;  Utility software;  Work efficiency, Utility programs},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pollock20161,
author={Pollock, L.},
title={Experiences in scaling field studies of software developer behavior},
journal={Proceedings - 3rd International Workshop on Software Engineering Research and Industrial Practice, SER and IP 2016},
year={2016},
pages={1-2},
doi={10.1145/2897022.2897838},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974569541&doi=10.1145%2f2897022.2897838&partnerID=40&md5=22fd2d8603293561a7681e5620b35a90},
abstract={Most of our current understanding of how programmers perform various software maintenance and evolution tasks is based on controlled studies or interviews, which are inherently limited in size, scope, and realism. Replicating controlled studies in the field can both explore the findings of these studies in wider contexts and study new factors that have not been previously encountered in the laboratory setting. While replicating controlled studies in the field seems like an obvious next step in scientific progress, it is a step that has rarely been attempted, in part due to its complexity, which requires not only the industrial knowhow to implement a robust, scalable system, but the academic knowledge of how to design rigorous studies. In this talk, I will describe a few examples of successfully scaled studies, contrast them with less successful cases (including our own), and provide lessons learned. I will share the importance of collecting targeted information instead of generic logs, the insight that automated data collection paired with followup surveys is a powerful tool, and the nuances around what researchers can and cannot expect working developers to tolerate for the sake of research. © 2016 ACM.},
keywords={Engineering research;  Industrial management;  Software engineering;  Technology transfer, Automated data collection;  Field studies;  Scalable systems;  Scientific progress;  Software developer;  Software maintenance and evolution, Industrial research},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2016680,
author={Liu, H.},
title={Towards better program obfuscation: Optimization via language models},
journal={Proceedings - International Conference on Software Engineering},
year={2016},
pages={680-682},
doi={10.1145/2889160.2891040},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026665442&doi=10.1145%2f2889160.2891040&partnerID=40&md5=af16d582d793d1fff1c26ebf8dcd0f84},
abstract={As a common practice in software development, program obfuscation aims at deterring reverse engineering and malicious attacks on released source or binary code. Owning ample obfuscation techniques, we have relatively little knowledge on how to most effectively use them. The biggest challenge lies in identifying the most useful combination of these techniques. We propose a unified framework to automatically generate and optimize obfuscation based on an obscurity language model and a Monte Carlo Markov Chain (MCMC) based search algorithm. We further instantiate it for JavaScript programs and developed the Closure∗ tool. Compared to the well-known Google Closure Compiler, Closure∗ outperforms its default setting by 26%. For programs which have already been well obfuscated, Closure∗ can still outperform by 22%. © 2016 Author.},
keywords={Computational linguistics;  Markov processes;  Optimization;  Reverse engineering;  Software engineering, JavaScript programs;  Language model;  Monte Carlo Markov chain;  Obfuscation;  Program obfuscation;  Random searches;  Search Algorithms;  Unified framework, Software design},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Damevski2016126,
author={Damevski, K. and Chen, H. and Shepherd, D. and Pollock, L.},
title={Interactive exploration of developer interaction traces using a Hidden Markov Model},
journal={Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},
year={2016},
pages={126-136},
doi={10.1145/2901739.2901741},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974534250&doi=10.1145%2f2901739.2901741&partnerID=40&md5=f66cc7a27d811eec03bab028b42e429d},
abstract={Using IDE usage data to analyze the behavior of software developers in the field, during the course of their daily work, can lend support to (or dispute) laboratory studies of developers. This paper describes a technique that leverages Hidden Markov Models (HMMs) as a means of mining high-level developer behavior from low-level IDE interaction traces of many developers in the field. HMMs use dual stochastic processes to model higher-level hidden behavior using observable input sequences of events. We propose an interactive approach of mining interpretable HMMs, based on guiding a human expert in building a high quality HMM in an iterative, one state at a time, manner. The final result is a model that is both representative of the field data and captures the field phenomena of interest. We apply our HMM construction approach to study debugging behavior, using a large IDE interaction dataset collected from nearly 200 developers at ABB, Inc. Our results highlight the different modes and constituent actions in debugging, exhibited by the developers in our dataset. © 2016 ACM.},
keywords={Integrodifferential equations;  Iterative methods;  Markov processes;  Random processes;  Stochastic models;  Stochastic systems;  Trellis codes, Construction approaches;  Developer behavior;  Hidden markov models (HMMs);  Input sequence;  Interactive approach;  Interactive exploration;  Laboratory studies;  Software developer, Hidden Markov models},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kreutzer201661,
author={Kreutzer, P. and Dotzler, G. and Ring, M. and Eskofier, B.M. and Philippsen, M.},
title={Automatic clustering of code changes},
journal={Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016},
year={2016},
pages={61-72},
doi={10.1145/2901739.2901749},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974623425&doi=10.1145%2f2901739.2901749&partnerID=40&md5=5ceb185eb95befc365edaab3d81d6045},
abstract={Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes. To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes. We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes. © 2016 ACM.},
keywords={Codes (symbols);  Open source software;  Open systems, Automatic clustering;  Bug finding tools;  Clustering;  Code changes;  Open source projects;  Research tools;  Similarity metrics;  Software repositories, Clustering algorithms},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Muşlu2016697,
author={Muşlu, K. and Swart, L. and Brun, Y. and Ernst, M.D.},
title={Development history granularity transformations},
journal={Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015},
year={2016},
pages={697-702},
doi={10.1109/ASE.2015.53},
art_number={7372057},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963838659&doi=10.1109%2fASE.2015.53&partnerID=40&md5=fae9dc36945dd1d0d5baf7b2f7023fb6},
abstract={Development histories can simplify some software engineering tasks, butdifferent tasks require different history granularities. For example, ahistory that includes every edit that resulted in compiling code is neededwhen searching for the cause of a regression, whereas a history that containsonly changes relevant to a feature is needed for understanding the evolutionof the feature. Unfortunately, today, both manual and automated historygeneration result in a single-granularity history. This paper introduces theconcept of multi-grained development history views and the architecture ofCodebase Manipulation, a tool that automatically records a fine-grainedhistory and manages its granularity by applying granularity transformations. © 2015 IEEE.},
keywords={Automation, Codebase Manipulation;  Development history;  Version control, Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yamamori2016281,
author={Yamamori, A. and Kobayashi, T.},
title={A change guide method based on developers' interaction and past recommendation},
journal={Proceedings of the 13th IASTED International Conference on Parallel and Distributed Computing and Networks, PDCN 2016},
year={2016},
pages={281-288},
doi={10.2316/P.2016.835-012},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015873843&doi=10.2316%2fP.2016.835-012&partnerID=40&md5=5cfa9770f232aa6adbb0b7562c15af66},
abstract={In this paper, we propose a change guide method based on the past developers' activity that consists of read and write access records of artifacts. In our proposed method, we calculate candidates of next change recommendation considering the history of its recommendations. We define "cumulative likelihood" to enable the method to recommend the appropriate candidates when a change propagates more than one code elements. A case study using interaction history logs from 15 participants showed the improvement of the accuracy of the method-level change recommendation.},
keywords={Computer networks;  Computer software maintenance;  Hardware;  Information systems, Change guide;  Interaction history;  Level change;  Software repository mining, Distributed computer systems},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Dyer2015593,
author={Dyer, R. and Nguyen, H. and Rajan, H. and Nguyen, T.},
title={Boa: An Enabling Language and Infrastructure for Ultra-Large-Scale MSR Studies},
journal={The Art and Science of Analyzing Software Data},
year={2015},
pages={593-621},
doi={10.1016/B978-0-12-411519-4.00020-3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944068581&doi=10.1016%2fB978-0-12-411519-4.00020-3&partnerID=40&md5=cb68f450a14c974443b22e2e146af905},
abstract={Mining software repositories (MSR) on a large scale is important for more generalizable research results. Large collections of software artifacts are openly available (e.g., SourceForge has more than 350,000 projects, GitHub has more than 10 million projects, and Google Code has more than 250,000 projects), but capitalizing on this data is extremely difficult. This chapter serves as a reference guide for Boa, a language and infrastructure designed to decrease the barrier to entry for ultra-large-scale MSR studies. Boa consists of a domain-specific language, its compiler, a dataset that contains almost 700,000 open source projects as of this writing, a back end based on MapReduce to effectively analyze this dataset, and a Web-based front end for writing MSR programs. This chapter describes how researchers and software practitioners can start using this resource. © 2015 Elsevier Inc. All rights reserved.},
keywords={Problem oriented languages;  Program compilers, Domain specific languages;  Mining software repositories;  Mining software repository (MSR);  Open source projects;  Software practitioners;  Source-code mining;  Ultra large scale;  Web-based front end, Open source software},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Yoon2015223,
author={Yoon, Y.S. and Myers, B.A.},
title={Supporting selective undo in a code editor},
journal={Proceedings - International Conference on Software Engineering},
year={2015},
volume={1},
pages={223-233},
doi={10.1109/ICSE.2015.43},
art_number={7194576},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951869024&doi=10.1109%2fICSE.2015.43&partnerID=40&md5=67a9ad0aea42ad915b6b783dbb074f3f},
abstract={Programmers often need to revert some code to an earlier state, or restore a block of code that was deleted a while ago. However, support for this backtracking in modern programming environments is limited. Many of the backtracking tasks can be accomplished by having a selective undo feature in code editors, but this has major challenges: there can be conflicts among edit operations, and it is difficult to provide usable interfaces for selective undo. In this paper, we present AZURITE, an Eclipse plug-in that allows programmers to selectively undo finegrained code changes made in the code editor. With AZURITE, programmers can easily perform backtracking tasks, even when the desired code is not in the undo stack or a version control system. AZURITE also provides novel user interfaces specifically designed for selective undo, which were iteratively improved through user feedback gathered from actual users in a preliminary field trial. A formal lab study showed that programmers can successfully use AZURITE, and were twice as fast as when limited to conventional features. © 2015 IEEE.},
keywords={Codes (symbols);  Computer programming;  Software engineering, Backtracking;  Code changes;  Code editors;  Field trial;  Programming environment;  Selective undo;  User feedback;  Version control system, User interfaces},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sharma2015235,
author={Sharma, M. and Kumari, M. and Singh, V.B.},
title={Post release versions based code change quality metrics},
journal={ACM International Conference Proceeding Series},
year={2015},
volume={10-13-August-2015},
pages={235-243},
doi={10.1145/2791405.2791466},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960953021&doi=10.1145%2f2791405.2791466&partnerID=40&md5=b9a6aa190d8476f27419ade2ea8abe01},
abstract={Software Metric is a quantitative measure of the degree to which a system, component or process possesses a given attribute. Bug fixing, new features (NFs) introduction and feature improvements (IMPs) are the key factors in deciding the next version of software. For fixing an issue (bug/new feature/feature improvement), a lot of changes have to be incorporated into the source code of the software. These code changes need to be understood by software engineers and managers when performing their daily development and maintenance tasks. In this paper, we have proposed four new metrics namely code change quality, code change density, file change quality and file change density to understand the quality of code changes across the different versions of five open source software products, namely Avro, Pig, Hive, jUDDI and Whirr of Apache project. Results show that all the products get better code change quality over a period of time. We have also observed that all the five products follow the similar code change trend. © 2015 ACM.},
keywords={Codes (symbols);  Computer software;  Entropy;  Information science;  Open source software;  Software engineering, Code changes;  Feature improvement;  Maintenance tasks;  New feature;  Quality metrics;  Quantitative measures;  Software metrices;  Software repositories, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hashimoto201513,
author={Hashimoto, M. and Terai, M. and Maeda, T. and Minami, K.},
title={Extracting facts from performance tuning history of scientific applications for predicting effective optimization patterns},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2015},
volume={2015-August},
pages={13-23},
doi={10.1109/MSR.2015.9},
art_number={7180063},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957072232&doi=10.1109%2fMSR.2015.9&partnerID=40&md5=a6a3ef640aa523bb1b8eb2add9269540},
abstract={To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer. © 2015 IEEE.},
keywords={Artificial intelligence;  Data mining;  Learning systems;  Program compilers;  Semantic Web;  Supercomputers;  Syntactics;  Trees (mathematics), Abstract Syntax Trees;  Application performance;  Combinatorial explosion;  Pattern identification;  Program transformations;  Scientific applications;  Transformation patterns;  Version control system, Application programs},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Higo2015372,
author={Higo, Y. and Ohtani, A. and Hayashi, S. and Hata, H. and Shinji, K.},
title={Toward reusing code changes},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2015},
volume={2015-August},
pages={372-376},
doi={10.1109/MSR.2015.43},
art_number={7180097},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957095130&doi=10.1109%2fMSR.2015.43&partnerID=40&md5=ed2480f1173bf95555bbddc09bf3f076},
abstract={Existing techniques have succeeded to help developers implement new code. However, they are insufficient to help to change existing code. Previous studies have proposed techniques to support bug fixes but other kinds of code changes such as function enhancements and refactorings are not supported by them. In this paper, we propose a novel system that helps developers change existing code. Unlike existing techniques, our system can support any kinds of code changes if similar code changes occurred in the past. Our research is still on very early stage and we have not have any implementation or any prototype yet. This paper introduces our research purpose, an outline of our system, and how our system is different from existing techniques. © 2015 IEEE.},
keywords={Bug fixes;  Change reuse;  Code changes;  Code clone;  Refactorings;  Research purpose;  Source code analysis, Codes (symbols)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yoon201519,
author={Yoon, S. and Lee, C. and Park, S. and Hwang, M.},
title={A Trace Mechanism of Change History in the Presence of Refactoring},
journal={International Journal of Software Engineering and its Applications},
year={2015},
volume={9},
number={9},
pages={19-28},
doi={10.14257/ijseia.2015.9.9.03},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946898698&doi=10.14257%2fijseia.2015.9.9.03&partnerID=40&md5=cf44e5c86134004ee0003fb62b22def3},
abstract={Nowadays, changes in software development are inevitable. Therefore, understanding and analysis of changes considered more important than anything else. The existing version control systems, which are used for effective change management, are impossible to trace changes in the presence of refactoring. There have been a number of approaches proposed for trace changes using operation-based mechanisms or rule-based mechanisms. However, it has many limitations with excessive recording or not sufficient coverage. In this approach, we define four rules based on M.Flower's refactoring techniques and find an origin method matching the pattern of changed source code with the pre-defined rules. The case study shows that the proposed mechanism is effective Also, case study looking for open source project code changes about the origin method show the proposed mechanism's effectiveness. © 2015 SERSC.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jiang2015361,
author={Jiang, Q. and Peng, X. and Wang, H. and Xing, Z. and Zhao, W.},
title={Summarizing Evolutionary Trajectory by Grouping and Aggregating relevant code changes},
journal={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings},
year={2015},
pages={361-370},
doi={10.1109/SANER.2015.7081846},
art_number={7081846},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928678931&doi=10.1109%2fSANER.2015.7081846&partnerID=40&md5=b77c8dab88a2c23e2c085e26da613a1f},
abstract={The lifecycle of a large-scale software system can undergo many releases. Each release often involves hundreds or thousands of revisions committed by many developers over time. Many code changes are made in a systematic and collaborative way. However, such systematic and collaborative code changes are often undocumented and hidden in the evolution history of a software system. It is desirable to recover commonalities and associations among dispersed code changes in the evolutionary trajectory of a software system. In this paper, we present SETGA (Summarizing Evolutionary Trajectory by Grouping and Aggregation), an approach to summarizing historical commit records as trajectory patterns by grouping and aggregating relevant code changes committed over time. SETGA extracts change operations from a series of commit records from version control systems. It then groups extracted change operations by their common properties from different dimensions such as change operation types, developers and change locations. After that, SETGA aggregates relevant change operation groups by mining various associations among them. The proposed approach has been implemented and applied to three open-source systems. The results show that SETGA can identify various types of trajectory patterns that are useful for software evolution management and quality assurance. © 2015 IEEE.},
keywords={Codes (symbols);  Computer software;  Control systems;  Information management;  Mining;  Open source software;  Quality assurance;  Trajectories, Code changes;  Collaborative codes;  Evolution;  Large-scale software systems;  Open source system;  Pattern;  Trajectory pattern;  Version control system, Open systems},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Soni20151512,
author={Soni, M. and Singh, H. and Sethi, N.},
title={A state of the art survey of data mining techniques for software engineering data},
journal={International Journal of Applied Engineering Research},
year={2015},
volume={10},
number={55},
pages={1512-1522},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942920938&partnerID=40&md5=46bfa4bda0a0ba0ba691bcde685a3ede},
abstract={Source code plugins are the additional features provided by the software applications in addition to main functionality. Violating coding rules in the source code of these applications can lead to error patterns in the source code revision histories. Various data mining techniques have been used to mine software engineering data to classify the information about the software system. Mining software engineering data aims at removing irrelevant data and outliers from revision histories of various software applications. It improves the process of software debugging along with software maintenance leading in high quality software by reducing cost and effort. This paper discusses various data mining approaches which are used in mining different software engineering data. © Research India Publications.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Amann2015121,
author={Amann, S. and Beyer, S. and Kevic, K. and Gall, H.},
title={Software mining studies: Goals, approaches, artifacts, and replicability},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={8987},
pages={121-158},
doi={10.1007/978-3-319-28406-4_5},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957030143&doi=10.1007%2f978-3-319-28406-4_5&partnerID=40&md5=7d7486a64aa5349348c2c00c6f013e33},
abstract={The mining of software archives has enabled new ways for increasing the productivity in software development: Analyzing software quality, mining project evolution, investigating change patterns and evolution trends, mining models for development processes, developing methods of integrating mined data from various historical sources, or analyzing natural language artifacts in software repositories, are examples of research topics. Software repositories include various data, ranging from source control systems, issue tracking systems, artifact repositories such as requirements, design and architectural documentation, to archived communication between project members. Practitioners and researchers have recognized the potential of mining these sources to support the maintenance of software, to improve their design or architecture, and to empirically validate development techniques or processes. We revisited software mining studies that were published in recent years in the top venues of software engineering, such as ICSE, ESEC/FSE, and MSR. In analyzing these software mining studies, we highlight different viewpoints: pursued goals, state-of-the-art approaches, mined artifacts, and study replicability. To analyze the mining artifacts, we (lexically) analyzed research papers of more than a decade. In terms of replicability we looked at existing work in the field in mining approaches, tools, and platforms. We address issues of replicability and reproducibility to shed light onto challenges for large-scale mining studies that would enable a stronger conclusion stability. © Springer International Publishing Switzerland 2015.},
keywords={Big data;  Computer software selection and evaluation;  Data mining;  Software engineering;  Teaching, Artifact repositories;  Development process;  Development technique;  Large-scale mining;  Natural languages;  Software repositories;  Source control system;  State-of-the-art approach, Software design},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yoon2014101,
author={Yoon, Y.S. and Myers, B.A.},
title={A longitudinal study of programmers' backtracking},
journal={Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},
year={2014},
pages={101-108},
doi={10.1109/VLHCC.2014.6883030},
art_number={6883030},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907416780&doi=10.1109%2fVLHCC.2014.6883030&partnerID=40&md5=b182a1ecfc6a6a3a1318ec9cb4606e5e},
abstract={Programming often involves reverting source code to an earlier state, which we call backtracking. We performed a longitudinal study of programmers' backtracking, analyzing 1,460 hours of fine-grained code editing logs collected from 21 people. Our analysis method keeps track of the change history of each abstract syntax tree node and looks for backtracking instances within each node. Using this method, we detected a total of 15,095 backtracking instances, which gives an average backtracking rate of 10.3/hour. The size of backtracking varied con-siderably, ranging from a single character to thousands of char-acters. 34% of the backtracking was performed by manually deleting or typing the desired code, and 9.5% of all backtracking was selective, meaning that it could not have been performed using the conventional undo command present in the IDE. The study results show that programmers need better backtracking tools, and also provide design implications for such tools. © 2014 IEEE.},
keywords={Integrodifferential equations;  Visual languages, Abstract Syntax Trees;  Analysis method;  backtracking;  Design implications;  Empirical studies;  Interactive development environment;  Longitudinal study;  undo, Computer programming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gligoric2014361,
author={Gligoric, M. and Negara, S. and Legunsen, O. and Marinov, D.},
title={An empirical evaluation and comparison of manual and automated test selection},
journal={ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
year={2014},
pages={361-371},
doi={10.1145/2642937.2643019},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908627870&doi=10.1145%2f2642937.2643019&partnerID=40&md5=95ef2057c7ca840bbdce2c375ad00494},
abstract={Regression test selection speeds up regression testing by rerunning only the tests that can be affected by the most recent code changes. Much progress has been made on research in automated test selection over the last three decades, but it has not translated into practical tools that are widely adopted. Therefore, developers either re-run all tests after each change or perform manual test selection. Re-running all tests is expensive, while manual test selection is tedious and error-prone. Despite such a big trade-off, no study assessed how developers perform manual test selection and compared it to automated test selection. This paper reports on our study of manual test selection in practice and our comparison of manual and automated test selection. We are the first to conduct a study that (1) analyzes data from manual test selection, collected in real time from 14 developers during a three-month study and (2) compares manual test selection with an automated state-of-the-research test-selection tool for 450 test sessions. Almost all developers in our study performed manual test selection, and they did so in mostly ad-hoc ways. Comparing manual and automated test selection, we found the two approaches to select different tests in each and every one of the 450 test sessions investigated. Manual selection chose more tests than automated selection 73% of the time (potentially wasting time) and chose fewer tests 27% of the time (potentially missing bugs). These results show the need for better automated test-selection techniques that integrate well with developers' programming environments. © 2014 ACM.},
keywords={Economic and social effects;  Software engineering;  Software testing, Automated selection;  Automated test;  Empirical evaluations;  Manual tests;  Programming environment;  Regression test selection;  Regression testing;  Test selection, Automation},
document_type={Conference Paper},
source={Scopus},
}

@inproceedings{Haldeman:2018:PMF:3159450.3159502,
 author = {Haldeman, Georgiana and Tjang, Andrew and Babe\c{s}-Vroman, Monica and Bartos, Stephen and Shah, Jay and Yucht, Danielle and Nguyen, Thu D.},
 title = {Providing Meaningful Feedback for Autograding of Programming Assignments},
 booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
 series = {SIGCSE '18},
 year = {2018},
 isbn = {978-1-4503-5103-4},
 location = {Baltimore, Maryland, USA},
 pages = {278--283},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3159450.3159502},
 doi = {10.1145/3159450.3159502},
 acmid = {3159502},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autograding, concepts/skills-based hints, error categorization},
} 

@inproceedings{Joyner:2018:SLP:3231644.3231649,
 author = {Joyner, David},
 title = {Squeezing the Limeade: Policies and Workflows for Scalable Online Degrees},
 booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},
 series = {L@S '18},
 year = {2018},
 isbn = {978-1-4503-5886-6},
 location = {London, United Kingdom},
 pages = {53:1--53:10},
 articleno = {53},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3231644.3231649},
 doi = {10.1145/3231644.3231649},
 acmid = {3231649},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {academic integrity, accreditation, online degrees},
}

@inproceedings{Suzuki:2017:EDS:3027063.3053187,
 author = {Suzuki, Ryo and Soares, Gustavo and Glassman, Elena and Head, Andrew and D'Antoni, Loris and Hartmann, Bj\"{o}rn},
 title = {Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments},
 booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
 series = {CHI EA '17},
 year = {2017},
 isbn = {978-1-4503-4656-6},
 location = {Denver, Colorado, USA},
 pages = {2951--2958},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/3027063.3053187},
 doi = {10.1145/3027063.3053187},
 acmid = {3053187},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated feedback, program synthesis, programming education},
} 

@inproceedings{Cassidy:2018:GCH:3231644.3231680,
 author = {Cassidy, Caitlin and Goldman, Max and Miller, Robert C.},
 title = {Glanceable Code History: Visualizing Student Code for Better Instructor Feedback},
 booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},
 series = {L@S '18},
 year = {2018},
 isbn = {978-1-4503-5886-6},
 location = {London, United Kingdom},
 pages = {22:1--22:4},
 articleno = {22},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/3231644.3231680},
 doi = {10.1145/3231644.3231680},
 acmid = {3231680},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer programming, learning at scale, visualizations},
} 

@inproceedings{D\&\#039;Antoni:2017:NSC:3106237.3106241,
 author = {D\&\#039;Antoni, Loris and Singh, Rishabh and Vaughn, Michael},
 title = {NoFAQ: Synthesizing Command Repairs from Examples},
 booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
 series = {ESEC/FSE 2017},
 year = {2017},
 isbn = {978-1-4503-5105-8},
 location = {Paderborn, Germany},
 pages = {582--592},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3106237.3106241},
 doi = {10.1145/3106237.3106241},
 acmid = {3106241},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Command Line Interface, Domain Specific Languages, Program Repair, Program Synthesis, Programming by Example},
} 

@inproceedings{Ichinco:2018:SSG:3202185.3202762,
 author = {Ichinco, Michelle and Kelleher, Caitlin},
 title = {Semi-automatic Suggestion Generation for Young Novice Programmers in an Open-ended Context},
 booktitle = {Proceedings of the 17th ACM Conference on Interaction Design and Children},
 series = {IDC '18},
 year = {2018},
 isbn = {978-1-4503-5152-2},
 location = {Trondheim, Norway},
 pages = {405--412},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/3202185.3202762},
 doi = {10.1145/3202185.3202762},
 acmid = {3202762},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code examples, novice programming, recommender systems},
}

@inproceedings{Ngoon:2018:IGT:3173574.3173629,
 author = {Ngoon, Tricia J. and Fraser, C. Ailie and Weingarten, Ariel S. and Dontcheva, Mira and Klemmer, Scott},
 title = {Interactive Guidance Techniques for Improving Creative Feedback},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {55:1--55:11},
 articleno = {55},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3173574.3173629},
 doi = {10.1145/3173574.3173629},
 acmid = {3173629},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {creativity, critique, educational technology, feedback},
}

@inproceedings{Feldman:2018:ADS:3173574.3173838,
 author = {Feldman, Molly Q. and Cho, Ji Yong and Ong, Monica and Gulwani, Sumit and Popovi\'{c}, Zoran and Andersen, Erik},
 title = {Automatic Diagnosis of Students' Misconceptions in K-8 Mathematics},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {264:1--264:12},
 articleno = {264},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3173574.3173838},
 doi = {10.1145/3173574.3173838},
 acmid = {3173838},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {elementary education, programming by demonstration},
}

@inproceedings{Gulwani:2018:ACP:3192366.3192387,
 author = {Gulwani, Sumit and Radi\v{c}ek, Ivan and Zuleger, Florian},
 title = {Automated Clustering and Program Repair for Introductory Programming Assignments},
 booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
 series = {PLDI 2018},
 year = {2018},
 isbn = {978-1-4503-5698-5},
 location = {Philadelphia, PA, USA},
 pages = {465--480},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3192366.3192387},
 doi = {10.1145/3192366.3192387},
 acmid = {3192387},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MOOC, clustering, dynamic analysis, program repair, programming education},
}

@inproceedings{Jayatilaka:2018:PIL:3209811.3209871,
 author = {Jayatilaka, Lahiru and Sengeh, David M. and Herrmann, Charles and Bertuccelli, Luca and Antos, Dimitrios and Grosz, Barbara J. and Gajos, Krzysztof Z.},
 title = {PETALS: Improving Learning of Expert Skill in Humanitarian Demining},
 booktitle = {Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies},
 series = {COMPASS '18},
 year = {2018},
 isbn = {978-1-4503-5816-3},
 location = {Menlo Park and San Jose, CA, USA},
 pages = {33:1--33:11},
 articleno = {33},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3209811.3209871},
 doi = {10.1145/3209811.3209871},
 acmid = {3209871},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {demining, explosives detection, training systems},
} 

@inproceedings{Kross:2018:SSI:3231644.3231662,
 author = {Kross, Sean and Guo, Philip J.},
 title = {Students, Systems, and Interactions: Synthesizing the First Four Years of Learning@Scale and Charting the Future},
 booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},
 series = {L@S '18},
 year = {2018},
 isbn = {978-1-4503-5886-6},
 location = {London, United Kingdom},
 pages = {2:1--2:10},
 articleno = {2},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3231644.3231662},
 doi = {10.1145/3231644.3231662},
 acmid = {3231662},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context/synthesis paper, meta-analysis, survey paper},
} 

@inproceedings{Rolim:2017:LSP:3097368.3097417,
 author = {Rolim, Reudismam and Soares, Gustavo and D'Antoni, Loris and Polozov, Oleksandr and Gulwani, Sumit and Gheyi, Rohit and Suzuki, Ryo and Hartmann, Bj\"{o}rn},
 title = {Learning Syntactic Program Transformations from Examples},
 booktitle = {Proceedings of the 39th International Conference on Software Engineering},
 series = {ICSE '17},
 year = {2017},
 isbn = {978-1-5386-3868-2},
 location = {Buenos Aires, Argentina},
 pages = {404--415},
 numpages = {12},
 url = {https://doi.org/10.1109/ICSE.2017.44},
 doi = {10.1109/ICSE.2017.44},
 acmid = {3097417},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {program synthesis, program transformation, refactoring, tutoring systems},
} 

@inproceedings{Accioly:2018:ACP:3196398.3196437,
 author = {Accioly, Paola and Borba, Paulo and Silva, L{\'e}uson and Cavalcanti, Guilherme},
 title = {Analyzing Conflict Predictors in Open-source Java Projects},
 booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
 series = {MSR '18},
 year = {2018},
 isbn = {978-1-4503-5716-6},
 location = {Gothenburg, Sweden},
 pages = {576--586},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3196398.3196437},
 doi = {10.1145/3196398.3196437},
 acmid = {3196437},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {awareness tools, collaborative development, conflict predictors, precision and recall},
} 

@inproceedings{Hashimoto:2018:APE:3236024.3236047,
 author = {Hashimoto, Masatomo and Mori, Akira and Izumida, Tomonori},
 title = {Automated Patch Extraction via Syntax- and Semantics-aware Delta Debugging on Source Code Changes},
 booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
 series = {ESEC/FSE 2018},
 year = {2018},
 isbn = {978-1-4503-5573-5},
 location = {Lake Buena Vista, FL, USA},
 pages = {598--609},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3236024.3236047},
 doi = {10.1145/3236024.3236047},
 acmid = {3236047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {delta debugging, software regression, tree differencing},
} 

@inproceedings{Karaivanov:2014:PST:2661136.2661148,
 author = {Karaivanov, Svetoslav and Raychev, Veselin and Vechev, Martin},
 title = {Phrase-Based Statistical Translation of Programming Languages},
 booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming \& Software},
 series = {Onward! 2014},
 year = {2014},
 isbn = {978-1-4503-3210-1},
 location = {Portland, Oregon, USA},
 pages = {173--184},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2661136.2661148},
 doi = {10.1145/2661136.2661148},
 acmid = {2661148},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {programming language translation, statistical machine translation},
}

@inproceedings{Rolim:2016:ARC:2950290.2983944,
 author = {Rolim, Reudismam},
 title = {Automating Repetitive Code Changes Using Examples},
 booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
 series = {FSE 2016},
 year = {2016},
 isbn = {978-1-4503-4218-6},
 location = {Seattle, WA, USA},
 pages = {1063--1065},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/2950290.2983944},
 doi = {10.1145/2950290.2983944},
 acmid = {2983944},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Program synthesis, Programming by examples, Software evolution},
} 

@article{Singh:2016:TSD:2914770.2837668,
 author = {Singh, Rishabh and Gulwani, Sumit},
 title = {Transforming Spreadsheet Data Types Using Examples},
 journal = {SIGPLAN Not.},
 issue_date = {January 2016},
 volume = {51},
 number = {1},
 month = jan,
 year = {2016},
 issn = {0362-1340},
 pages = {343--356},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2914770.2837668},
 doi = {10.1145/2914770.2837668},
 acmid = {2837668},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Noisy Examples, Probabilistic Synthesis, Program Synthesis, Programming By Examples, Spreadsheet Programming},
} 

@inproceedings{Hempel:2018:DEL:3180155.3180165,
 author = {Hempel, Brian and Lubin, Justin and Lu, Grace and Chugh, Ravi},
 title = {Deuce: A Lightweight User Interface for Structured Editing},
 booktitle = {Proceedings of the 40th International Conference on Software Engineering},
 series = {ICSE '18},
 year = {2018},
 isbn = {978-1-4503-5638-1},
 location = {Gothenburg, Sweden},
 pages = {654--664},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3180155.3180165},
 doi = {10.1145/3180155.3180165},
 acmid = {3180165},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {direct manipulation, refactoring, structured editing},
} 

@inproceedings{Wang:2016:FFS:2983990.2984030,
 author = {Wang, Xinyu and Gulwani, Sumit and Singh, Rishabh},
 title = {FIDEX: Filtering Spreadsheet Data Using Examples},
 booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
 series = {OOPSLA 2016},
 year = {2016},
 isbn = {978-1-4503-4444-9},
 location = {Amsterdam, Netherlands},
 pages = {195--213},
 numpages = {19},
 url = {http://doi.acm.org/10.1145/2983990.2984030},
 doi = {10.1145/2983990.2984030},
 acmid = {2984030},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Data Filtering, Program Synthesis, Programming By Examples, Regular Expressions},
}

@inproceedings{Rolim:2017:LSP:3097368.3097417,
 author = {Rolim, Reudismam and Soares, Gustavo and D'Antoni, Loris and Polozov, Oleksandr and Gulwani, Sumit and Gheyi, Rohit and Suzuki, Ryo and Hartmann, Bj\"{o}rn},
 title = {Learning Syntactic Program Transformations from Examples},
 booktitle = {Proceedings of the 39th International Conference on Software Engineering},
 series = {ICSE '17},
 year = {2017},
 isbn = {978-1-5386-3868-2},
 location = {Buenos Aires, Argentina},
 pages = {404--415},
 numpages = {12},
 url = {https://doi.org/10.1109/ICSE.2017.44},
 doi = {10.1109/ICSE.2017.44},
 acmid = {3097417},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {program synthesis, program transformation, refactoring, tutoring systems},
} 

@inproceedings{Jin:2017:FTD:3035918.3064034,
 author = {Jin, Zhongjun and Anderson, Michael R. and Cafarella, Michael and Jagadish, H. V.},
 title = {Foofah: Transforming Data By Example},
 booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
 series = {SIGMOD '17},
 year = {2017},
 isbn = {978-1-4503-4197-4},
 location = {Chicago, Illinois, USA},
 pages = {683--698},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3035918.3064034},
 doi = {10.1145/3035918.3064034},
 acmid = {3064034},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {a* algorithm, data transformation, heuristic, program synthesis, programming by example},
} 

@article{Steimann:2018:CR:3173093.3156016,
 author = {Steimann, Friedrich},
 title = {Constraint-Based Refactoring},
 journal = {ACM Trans. Program. Lang. Syst.},
 issue_date = {January 2018},
 volume = {40},
 number = {1},
 month = jan,
 year = {2018},
 issn = {0164-0925},
 pages = {2:1--2:40},
 articleno = {2},
 numpages = {40},
 url = {http://doi.acm.org/10.1145/3156016},
 doi = {10.1145/3156016},
 acmid = {3156016},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Refactoring, constraint-based repair},
} 

@article{Gligoric:2014:AMB:2714064.2660239,
 author = {Gligoric, Milos and Schulte, Wolfram and Prasad, Chandra and van Velzen, Danny and Narasamdya, Iman and Livshits, Benjamin},
 title = {Automated Migration of Build Scripts Using Dynamic Analysis and Search-based Refactoring},
 journal = {SIGPLAN Not.},
 issue_date = {October 2014},
 volume = {49},
 number = {10},
 month = oct,
 year = {2014},
 issn = {0362-1340},
 pages = {599--616},
 numpages = {18},
 url = {http://doi.acm.org/10.1145/2714064.2660239},
 doi = {10.1145/2714064.2660239},
 acmid = {2660239},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {build system, migration, search-based refactoring},
}

@article{Vidal:2018:ARB:3208361.3191314,
 author = {Vidal, Santiago and berra, I\~{n}aki and Zulliani, Santiago and Marcos, Claudia and Pace, J. Andr{\'e}s D\'{\i}az},
 title = {Assessing the Refactoring of Brain Methods},
 journal = {ACM Trans. Softw. Eng. Methodol.},
 issue_date = {June 2018},
 volume = {27},
 number = {1},
 month = apr,
 year = {2018},
 issn = {1049-331X},
 pages = {2:1--2:43},
 articleno = {2},
 numpages = {43},
 url = {http://doi.acm.org/10.1145/3191314},
 doi = {10.1145/3191314},
 acmid = {3191314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Code smells, brain method, long method, refactoring},
} 

Scopus
EXPORT DATE: 12 March 2019

@CONFERENCE{Chen2018124,
author={Chen, W. and Wu, G. and Wei, J.},
title={An Approach to Identifying Error Patterns for Infrastructure as Code},
journal={Proceedings - 29th IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2018},
year={2018},
pages={124-129},
doi={10.1109/ISSREW.2018.00-19},
art_number={8539175},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059848832&doi=10.1109%2fISSREW.2018.00-19&partnerID=40&md5=b3936dbaa3aa4aca753d94d1c60cb8e6},
affiliation={Institute of Software, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Sciences, Beijing, China},
abstract={Infrastructure as Code (IaC), which specifies system configurations in an imperative or declarative way, automates environment set up, system deployment and configuration. Despite wide adoption, developing and maintaining high-quality IaC artifacts is still challenging. This paper proposes an approach to handling the fine-grained and frequently occurring IaC code errors. The approach extracts code changes from historical commits and clusters them into groups, by constructing a feature model of code changes and employing an unsupervised machine learning algorithm. It identifies error patterns from the clusters and proposes a set of inspection rules to check the potential IaC code errors. In practice, we take Puppet code artifacts as subject objects and perform a comprehensive study on 14 popular Puppet artifacts. In our experiment, we get 41 cross-artifact error patterns, covering 42% crawled code changes. Based on these patterns, 30 rules are proposed, covering 60% identified error patterns, to proactively check IaC artifacts. The approach would be helpful in improving code quality of IaC artifacts. © 2018 IEEE.},
author_keywords={Error pattern;  Infrastructure as Code;  Puppet artifact},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Durieux20181,
author={Durieux, T. and Hamadi, Y. and Monperrus, M.},
title={Fully Automated HTML and Javascript Rewriting for Constructing a Self-Healing Web Proxy},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
year={2018},
volume={2018-October},
pages={1-12},
doi={10.1109/ISSRE.2018.00012},
art_number={8539064},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059636843&doi=10.1109%2fISSRE.2018.00012&partnerID=40&md5=6d04568c2396ac72131373f2d84b5e61},
affiliation={Inria and University of Lille, Lille, France; Ecole Polytechnique, Paris, France; KTH Royal Institute of Technology, Stockholm, Sweden},
abstract={Over the last few years, the complexity of web applications has increased to provide more dynamic web applications to users. The drawback of this complexity is the growing number of errors in the front-end applications. In this paper, we present BikiniProxy, a novel technique to provide self-healing for the web. BikiniProxy is designed as an HTTP proxy that uses five self-healing strategies to rewrite the buggy HTML and Javascript code. We evaluate BikiniProxy with a new benchmark of 555 reproducible Javascript errors, DeadClick. We create DeadClick by randomly crawling the Internet and collect all web pages that contain Javascript errors. Then, we observe how BikiniProxy heals those errors by collecting and comparing the traces of the original and healed pages. To sum up, BikiniProxy is a novel fully-automated self-healing approach that is specific to the web, evaluated on 555 real Javascript errors, and based on original self-healing rewriting strategies for HTML and Javascript. © 2018 IEEE.},
author_keywords={Failure oblivious computing;  Javascript;  Repair proxy;  Self healing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Frick2018264,
author={Frick, V. and Grassauer, T. and Beck, F. and Pinzger, M.},
title={Generating accurate and compact edit scripts using tree differencing},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
year={2018},
pages={264-274},
doi={10.1109/ICSME.2018.00036},
art_number={8530035},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058289624&doi=10.1109%2fICSME.2018.00036&partnerID=40&md5=5a0874c1dbad89a7cc6e6e231df79d78},
affiliation={Alpen-Adria-Universität Klagenfurt, University of Duisburg-Essen, Austria},
abstract={For analyzing changes in source code, edit scriptsare used to describe the differences between two versions of afile. These scripts consist of a list of actions that, applied to thesource file, result in the new version of the file. In contrast toline-based source code differencing, tree-based approaches suchas GumTree, MTDIFF, or ChangeDistiller extract changes bycomparing the abstract syntax trees (AST) of two versions of asource file. One benefit of tree-based approaches is their abilityto capture moved (sub) trees in the AST. Our approach, theIterative Java Matcher (IJM), builds upon GumTree and aims atgenerating more accurate and compact edit scripts that capturethe developer's intent. This is achieved by improving the qualityof the generated move and update actions, which are the mainsource of inaccurate actions generated by previous approaches. To evaluate our approach, we conducted a study with 11 external experts and manually analyzed the accuracy of 2400 randomly selected editactions. Comparing IJM to GumTree and MTDIFF, the resultsshow that IJM provides better accuracy for move and updateactions and is more beneficial to understanding the changes. © 2018 IEEE.},
author_keywords={Abstract syntax trees;  change extraction;  Software evolution;  Tree differencing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu20182180,
author={Liu, B.-B. and Dong, W. and Wang, J.},
title={Survey on Intelligent Search and Construction Methods of Program [智能化的程序搜索与构造方法综述]},
journal={Ruan Jian Xue Bao/Journal of Software},
year={2018},
volume={29},
number={8},
pages={2180-2197},
doi={10.13328/j.cnki.jos.005529},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055533864&doi=10.13328%2fj.cnki.jos.005529&partnerID=40&md5=9bb00a7f37ea2f4cbdd83e7b75d24367},
affiliation={College of Computer, National University of Defense Technology, Changsha, 410073, China},
abstract={The rapid development of Internet, machine learning and artificial intelligence, as well as the appearance of a large number of open-source software and communities, has brought new opportunities and challenges to the development of software engineering. There are billions of lines of code on the Internet. These codes, especially those of high quality and widely used contains all kinds of knowledge, which has led to the new idea of intelligent software development. It tries to make full use of code resources, knowledge and collective intelligence on the Internet to effectively improve the efficiency and quality of software development. The key technology is program search and construction, providing great theoretical and practical value. At present, the research work of these areas mainly focuses on code search, program synthesis, code recommendation and completion, defect detection, code style improvement, and automatic program repair. This paper surveys the current main research work from the above aspects, sorts out the specific theoretical and technical approaches in detail and summarizes the challenges in the current research process. Several directions of research in the future are also proposed. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.},
author_keywords={Collective intelligence;  Intelligent software development;  Machine learning;  Program construction;  Program search},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Rodríguez-Baquero2018372,
author={Rodríguez-Baquero, D. and Linares-Vásquez, M.},
title={Mutode: Generic JavaScript and node.js mutation testing tool},
journal={ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2018},
pages={372-375},
doi={10.1145/3213846.3229504},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051542060&doi=10.1145%2f3213846.3229504&partnerID=40&md5=5dd1d9ff26b71cff00afe02abc382604},
affiliation={Universidad de Los Andes, Bogotá, Colombia},
abstract={Mutation testing is a technique in which faults (mutants) are injected into a program or application to assess its test suite effectiveness. It works by inserting mutants and running the application's test suite to identify if the mutants are detected (killed) or not (survived) by the tests. Although computationally expensive, it has proven to be an effective method to assess application test suites. Several mutation testing frameworks and tools have been built for the various programing languages, however, very few tools have been built for the JavaScript language, more specifically, there is a lack of mutation testing tools for the Node.js runtime and npm based applications. The npm Registry is a public collection of modules of open-source code for Node.js, front-end web applications, mobile applications, robots, routers, and countless other needs of the JavaScript community. The over 700,000 packages hosted in npm are downloaded more than 5 billion times per week. More and more software is published in npm every day, representing a huge opportunity to share code and solutions, but also to share bugs and faulty software. In this paper, we briefly describe prior work for mutation operators in JavaScript and Node.js, and propose Mutode, an open source tool which leverages the npm package ecosystem to perform mutation testing for JavaScript and Node.js applications. We empirically evaluated Mutode effectiveness by running it on 12 of the top 20 npm modules that have automated test suites. © 2018 Association for Computing Machinery.},
author_keywords={JavaScript;  Mutation testing;  Node.js;  Operators},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Patra2018741,
author={Patra, J. and Dixit, P.N. and Pradel, M.},
title={ConflictJS: Finding and understanding conflicts between JavaScript libraries},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={741-751},
doi={10.1145/3180155.3180184},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049388864&doi=10.1145%2f3180155.3180184&partnerID=40&md5=526a1366a1fcb43a7e1a425e6faf4916},
affiliation={TU Darmstadt, Germany},
abstract={It is a common practice for client-side web applications to build on various third-party JavaScript libraries. Due to the lack of namespaces in JavaScript, these libraries all share the same global namespace. As a result, one library may inadvertently modify or even delete the APIs of another library, causing unexpected behavior of library clients. Given the quickly increasing number of libraries, manually keeping track of such conflicts is practically impossible both for library developers and users. This paper presents ConflictJS, an automated and scalable approach to analyze libraries for conflicts. The key idea is to tackle the huge search space of possible conflicts in two phases. At first, a dynamic analysis of individual libraries identifies pairs of potentially conflicting libraries. Then, targeted test synthesis validates potential conflicts by creating a client application that suffers from a conflict. The overall approach is free of false positives, in the sense that it reports a problem only when such a client exists. We use ConflictJS to analyze and study conflicts among 951 real-world libraries. The results show that one out of four libraries is potentially conflicting and that 166 libraries are involved in at least one certain conflict. The detected conflicts cause crashes and other kinds of unexpected behavior. Our work helps library developers to prevent conflicts, library users to avoid combining conflicting libraries, and provides evidence that designing a language without explicit namespaces has undesirable effects. © 2018 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Paltoglou2018402,
author={Paltoglou, A. and Zafeiris, V.E. and Giakoumakis, E.A. and Diamantidis, N.A.},
title={Automated refactoring of client-side JavaScript code to ES6 modules},
journal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
year={2018},
volume={2018-March},
pages={402-412},
doi={10.1109/SANER.2018.8330227},
art_number={8330227},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051011470&doi=10.1109%2fSANER.2018.8330227&partnerID=40&md5=02b9b67e995071c34c80545babfa2598},
affiliation={Department of Informatics, Athens University of Economics and Business, 76 Patission Str., Athens, 104 34, Greece},
abstract={JavaScript (JS) is a dynamic, weakly-typed and object-based programming language that expanded its reach, in recent years, from the desktop web browser to a wide range of runtime platforms in embedded, mobile and server hosts. Moreover, the scope of functionality implemented in JS scaled from DOM manipulation in dynamic HTML pages to full-scale applications for various domains, stressing the need for code reusability and maintainability. Towards this direction, the ECMAScript 6 (ES6) revision of the language standardized the syntax for class and module definitions, streamlining the encapsulation of data and functionality at various levels of granularity. This work focuses on refactoring client-side web applications for the elimination of code smells, relevant to global variables and functions that are declared in JS files linked to a web page. These declarations 'pollute' the global namespace at runtime and often lead to name conflicts with undesired effects. We propose a method for the encapsulation of global declarations through automated refactoring to ES6 modules. Our approach transforms each linked JS script of a web application to an ES6 module with appropriate import and export declarations that are inferred through static analysis. A prototype implementation of the proposed method, based on WALA libraries, has been evaluated on a set of open source projects. The evaluation results support the applicability and runtime efficiency of the proposed method. © 2018 IEEE.},
author_keywords={Client-side JavaScript;  ES6 Modules;  Global variables;  Refactoring},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gallaba2017353,
author={Gallaba, K. and Hanam, Q. and Mesbah, A. and Beschastnikh, I.},
title={Refactoring asynchrony in JavaScript},
journal={Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017},
year={2017},
pages={353-363},
doi={10.1109/ICSME.2017.83},
art_number={8094435},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040586242&doi=10.1109%2fICSME.2017.83&partnerID=40&md5=d680204d2e1198bc9189ba1ee47c7b03},
affiliation={McGill University, Canada; University of British Columbia, Canada},
abstract={JavaScript is a widely used programming language that makes extensive use of asynchronous computation, particularly in the form of asynchronous callbacks. These callbacks are used to handle tasks, from GUI events to network messages, in a non-blocking fashion. Asynchronous callbacks present developers with two challenges. First, JavaScript's try/catch error-handling mechanism is not sufficient for proper error handling in asynchronous contexts. In response, the JavaScript community has come to rely on the error-first protocol, an informal programming idiom that is not enforced or checked by the runtime. Second, JavaScript callbacks are frequently nested, making them difficult to handle (also known as callback hell). Fortunately, a recent language extension called promises provides an alternative to asynchronous callbacks. The adoption of promises, however, has been slow as refactoring existing code to use promises is a complex task. We present a set of program analysis techniques to detect instances of asynchronous callbacks and to refactor such callbacks, including callbacks with the error-first protocol, into promises. We implement our techniques in a tool called PROMISESLAND. We perform a manual analysis of four JavaScript applications to evaluate the tool's precision and recall, which are, on average, 100% and 83%, respectively. We evaluate PROMISESLAND on 21 large JavaScript applications, and find that PROMISESLAND (1) correctly refactors callbacks to promises, (2) outperforms a recent related refactoring technique, and (3) runs in under three seconds on all of our evaluation targets. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Abdalkareem2017385,
author={Abdalkareem, R. and Nourry, O. and Wehaibi, S. and Mujahid, S. and Shihab, E.},
title={Why do developers use trivial packages? An empirical case study on npm},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2017},
volume={Part F130154},
pages={385-395},
doi={10.1145/3106237.3106267},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030792039&doi=10.1145%2f3106237.3106267&partnerID=40&md5=e085e81771c7ae706ab885b695ec2c41},
affiliation={Data-driven Analysis of Software (DAS) Lab., Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada},
abstract={Code reuse is traditionally seen as good practice. Recent trends have pushed the concept of code reuse to an extreme, by using packages that implement simple and trivial tasks, which we call trivial packages'. A recent incident where a trivial package led to the breakdown of some of the most popular web applications such as Facebook and Netflix made it imperative to question the growing use of trivial packages. Therefore, in this paper, we mine more than 230, 000 npm packages and 38, 000 JavaScript applications in order to study the prevalence of trivial packages. We found that trivial packages are common and are increasing in popularity, making up 16.8% of the studied npm packages. We performed a survey with 88 Node.js developers who use trivial packages to understand the reasons and drawbacks of their use. Our survey revealed that trivial packages are used because they are perceived to be well implemented and tested pieces of code. However, developers are concerned about maintaining and the risks of breakages due to the extra dependencies trivial packages introduce. To objectively verify the survey results, we empirically validate the most cited reason and drawback and find that, contrary to developers' beliefs, only 45.2% of trivial packages even have tests. However, trivial packages appear to be deployment tested' and to have similar test, usage and community interest as non-trivial packages. On the other hand, we found that 11.5% of the studied trivial packages have more than 20 dependencies. Hence, developers should be careful about which trivial packages they decide to use. © 2017 Association for Computing Machinery.},
author_keywords={Code reuse;  Empirical studies;  JavaScript;  Node.js},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gao2017758,
author={Gao, Z. and Bird, C. and Barr, E.T.},
title={To Type or Not to Type: Quantifying Detectable Bugs in JavaScript},
journal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017},
year={2017},
pages={758-769},
doi={10.1109/ICSE.2017.75},
art_number={7985711},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027701347&doi=10.1109%2fICSE.2017.75&partnerID=40&md5=154661c7a35d0c162b69145bc54cb554},
affiliation={University College London, London, United Kingdom; Microsoft Research, Redmond, United States},
abstract={JavaScript is growing explosively and is now used in large mature projects even outside the web domain. JavaScript is also a dynamically typed language for which static type systems, notably Facebook's Flow and Microsoft's TypeScript, have been written. What benefits do these static type systems provide? Leveraging JavaScript project histories, we select a fixed bug and check out the code just prior to the fix. We manually add type annotations to the buggy code and test whether Flow and TypeScript report an error on the buggy code, thereby possibly prompting a developer to fix the bug before its public release. We then report the proportion of bugs on which these type systems reported an error. Evaluating static type systems against public bugs, which have survived testing and review, is conservative: it understates their effectiveness at detecting bugs during private development, not to mention their other benefits such as facilitating code search/completion and serving as documentation. Despite this uneven playing field, our central finding is that both static type systems find an important percentage of public bugs: both Flow 0.30 and TypeScript 2.0 successfully detect 15%!. © 2017 IEEE.},
author_keywords={Flow;  JavaScript;  mining software repositories;  static type systems;  TypeScript},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ocariza2017128,
author={Ocariza, F.S., Jr. and Bajaj, K. and Pattabiraman, K. and Mesbah, A.},
title={A Study of Causes and Consequences of Client-Side JavaScript Bugs},
journal={IEEE Transactions on Software Engineering},
year={2017},
volume={43},
number={2},
pages={128-144},
doi={10.1109/TSE.2016.2586066},
art_number={7501855},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013096214&doi=10.1109%2fTSE.2016.2586066&partnerID=40&md5=06b73a13154c1d7eabb1f0b82266ea54},
affiliation={Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC  V6T 1Z4, Canada},
abstract={Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript. © 2016 IEEE.},
author_keywords={bug reports;  Document Object Model (DOM);  empirical study;  Faults;  JavaScript},
document_type={Article},
source={Scopus},
}

Scopus
EXPORT DATE: 12 March 2019

@ARTICLE{Stevens2019491,
author={Stevens, R. and Molderez, T. and De Roover, C.},
title={Querying distilled code changes to extract executable transformations},
journal={Empirical Software Engineering},
year={2019},
volume={24},
number={1},
pages={491-535},
doi={10.1007/s10664-018-9644-3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053271297&doi=10.1007%2fs10664-018-9644-3&partnerID=40&md5=e63534634789c9c6f417132721c3f25a},
affiliation={Maxflow BVBA, Leuven, Belgium; Software Languages Lab, Vrije Universiteit Brussel, Ixelles, Belgium},
abstract={Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. We show that our approach is able to recall different implementation variants of the same source code evolution in histories of different software projects. We also evaluate the expressiveness and ease-of-use of our approach in a user study. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Change distilling;  Change querying;  Logic meta-programming},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Maruyama2018537,
author={Maruyama, K. and Hayashi, S. and Omori, T.},
title={ChangeMacroRecorder: Recording fine-grained textual changes of source code},
journal={25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
year={2018},
volume={2018-March},
pages={537-541},
doi={10.1109/SANER.2018.8330255},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050918560&doi=10.1109%2fSANER.2018.8330255&partnerID=40&md5=dfd150d4eeecccd03be8e404a8a42b3b},
affiliation={Ritsumeikan University, Japan; Tokyo Institute of Technology, Japan},
abstract={Recording code changes comes to be well recognized as an effective means for understanding the evolution of existing programs and making their future changes efficient. Although fine-grained textual changes of source code are worth leveraging in various situations, there is no satisfactory tool that records such changes. This paper proposes a yet another tool, called ChangeMacroRecorder, which automatically records all textual changes of source code while a programmer writes and modifies it on the Eclipse's Java editor. Its capability has been improved with respect to both the accuracy of its recording and the convenience for its use. Tool developers can easily and cheaply create their new applications that utilize recorded changes by embedding our proposed recording tool into them. © 2018 IEEE.},
author_keywords={Change recording;  Fine-grained changes;  Integrated development environments},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Omori20187,
author={Omori, T.},
title={Privacy preservation in interaction history on integrated development environments},
journal={2018 IEEE 1st International Workshop on Mining and Analyzing Interaction Histories, MAINT 2018 - Proceedings},
year={2018},
volume={2018-January},
pages={7-11},
doi={10.1109/MAINT.2018.8323088},
art_number={8323088},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051052828&doi=10.1109%2fMAINT.2018.8323088&partnerID=40&md5=6b98d18b8ee667ad2dfa5a9fad60fb6b},
affiliation={Department of Computer Science, Ritsumeikan University, Japan},
abstract={The interaction history in a software development environment allows us to analyze how developers change source code and how they use tools on the integrated development environment. Sharing the interaction history with tool providers increases the chances that developers obtain better tools. However, the interaction history sometimes contains privacy-sensitive information, which is an obstacle in collecting and using the interaction history. As an attempt to tackle this issue, this paper proposes a technique to replace sensitive text in a recorded interaction history. This paper describes the proposed technique, its current implementation, the results of a preliminary survey on how potential privacy-sensitive information exists in recorded interaction histories, and how privacy issues in sharing interaction histories can be ameliorated. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Omori2018644,
author={Omori, T. and Maruyama, K.},
title={Comparative study between two approaches using edit operations and code differences to detect past refactorings},
journal={IEICE Transactions on Information and Systems},
year={2018},
volume={E101D},
number={3},
pages={644-658},
doi={10.1587/transinf.2017EDP7160},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042656892&doi=10.1587%2ftransinf.2017EDP7160&partnerID=40&md5=045c2ba1d0fdcdf64533a3a7e9cdea1a},
affiliation={Department of Computer Science, Ritsumeikan University, Kusatsu-shi, 525-8577, Japan; Department of Information Science and Engineering, Ritsumeikan University, Kusatsu-shi, 525-8577, Japan},
abstract={Understanding which refactoring transformations were performed is in demand in modern software constructions. Traditionally, many researchers have been tackling understanding code changes with history data derived from version control systems. In those studies, problems of the traditional approach are pointed out, such as entanglement of multiple changes. To alleviate the problems, operation histories on IDEs' code editors are available as a new source of software evolution data nowadays. By replaying such histories, we can investigate past code changes in a fine-grained level. However, the prior studies did not provide enough evidence of their effectiveness for detecting refactoring transformations. This paper describes an experiment in which participants detect refactoring transformations performed by other participants after investigating the code changes with an operation-replay tool and diff tools. The results show that both approaches have their respective factors that pose misunderstanding and overlooking of refactoring transformations. Two negative factors on divided operations and generated compound operations were observed in the operation-based approach, whereas all the negative factors resulted from three problems on tangling, shadowing, and out-of-order of code changes in the difference-based approach. This paper also shows seven concrete examples of participants' mistakes in both approaches. These findings give us hints for improving existing tools for understanding code changes and detecting refactoring transformations. © 2018 The Institute of Electronics, Information and Communication Engineers.},
author_keywords={Fine-grained code change;  Refactoring detection;  Software evolution;  Understanding code change},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Stevens2017171,
author={Stevens, R. and De Roover, C.},
title={Extracting executable transformations from distilled code changes},
journal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
year={2017},
pages={171-181},
doi={10.1109/SANER.2017.7884619},
art_number={7884619},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018416465&doi=10.1109%2fSANER.2017.7884619&partnerID=40&md5=ee0189b8cc4dd9b95dc6ef1fa7877eaa},
affiliation={Software Languages Lab, Vrije Universiteit Brussel, Belgium},
abstract={Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. Using examples, we demonstrate the expressiveness of specifying source code evolutions through intermediate ASTs. We also show that our approach is able to recall different implementation variants of the same source code evolution in open-source histories. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}
